<doc id="50513" url="http://en.wikipedia.org/wiki?curid=50513" title="Melanin">
Melanin

Melanin (Greek: μέλας - "melas", "black, dark") is a broad term for a group of natural pigments found in most organisms (arachnids are one of the few groups in which it has not been detected). Melanin is produced by the oxidation of the amino acid tyrosine, followed by polymerization. The pigment is produced in a specialized group of cells known as melanocytes.
There are three basic types of melanin: eumelanin, pheomelanin, and neuromelanin. The most common type of melanin is eumelanin. There are two types of eumelanin- brown eumelanin and black eumelanin. Pheomelanin is a cysteine-containing red polymer of benzothiazine units largely responsible for red hair, among other pigmentation. Neuromelanin is found in the brain, though its function remains obscure.
In the skin, melanogenesis occurs after exposure to UV radiation, causing the skin to visibly tan. Melanin is an effective absorber of light; the pigment is able to dissipate over 99.9% of absorbed UV radiation. Because of this property, melanin is thought to protect skin cells from UVB radiation damage, reducing the risk of cancer. Furthermore, though exposure to UV radiation is associated with increased risk of malignant melanoma, a cancer of the melanocytes, studies have shown a lower incidence for skin cancer in individuals with more concentrated melanin, i.e. darker skin tone. Nonetheless, the relationship between skin pigmentation and photoprotection is still being clarified.
Humans.
In humans, melanin is the primary determinant of skin color. It is also found in hair, the pigmented tissue underlying the iris of the eye, and the stria vascularis of the inner ear. In the brain, tissues with melanin include the medulla and pigment-bearing neurons within areas of the brainstem, such as the locus coeruleus and the substantia nigra. It also occurs in the zona reticularis of the adrenal gland.
The melanin in the skin is produced by melanocytes, which are found in the basal layer of the epidermis. Although, in general, human beings possess a similar concentration of melanocytes in their skin, the melanocytes in some individuals and ethnic groups produce variable amounts of melanin. Some humans have very little or no melanin synthesis in their bodies, a condition known as albinism.
Because melanin is an aggregate of smaller component molecules, there are many different types of melanin with differing proportions and bonding patterns of these component molecules. Both pheomelanin and eumelanin are found in human skin and hair, but eumelanin is the most abundant melanin in humans, as well as the form most likely to be deficient in albinism.
Eumelanin.
Eumelanin polymers have long been thought to comprise numerous cross-linked 5,6-dihydroxyindole (DHI) and 5,6-dihydroxyindole-2-carboxylic acid (DHICA) polymers.
There are two types of eumelanin- brown eumelanin and black eumelanin, which chemically differ from each other in their pattern of polymeric bonds. A small amount of black eumelanin in the absence of other pigments causes grey hair. A small amount of brown eumelanin in the absence of other pigments causes yellow (blond) color hair.
Pheomelanin.
Pheomelanins impart a pink to red hue, depending upon the concentration. Pheomelanins are particularly concentrated in the lips, nipples, glans of the penis, and vagina. When a small amount of brown eumelanin in hair, which would otherwise cause blond hair, is mixed with red pheomelanin, the result is red hair.
In chemical terms, pheomelanins differs from eumelanins in that the oligomer structure incorporates benzothiazine and benzothiazole units that are produced, instead of DHI and DHICA, when the amino acid L-cysteine is present.
Trichochromes.
Trichochromes (formerly called trichosiderins) are pigments produced from the same metabolic pathway as the eumelanins and pheomelanins, but unlike those molecules they have low molecular weight. They occur in some red human hair.
Neuromelanin.
Neuromelanin (NM) is a dark polymer pigment produced in specific populations of catecholaminergic neurons in the brain, or in the upper vocal cords. Humans have the largest amount of NM, while it is in lesser amount in other non-human primates, and totally absent in other species. However, the biological function remains unknown, although human NM has been shown to efficiently bind transition metals such as iron, as well as other potentially toxic molecules. Therefore, it may play crucial roles in apoptosis and the related Parkinson's disease.
Other organisms.
Melanins have very diverse roles and functions in various organisms. A form of melanin makes up the ink used by many cephalopods (see cephalopod ink) as a defense mechanism against predators. Melanins also protect microorganisms, such as bacteria and fungi, against stresses that involve cell damage such as UV radiation from the sun and reactive oxygen species. Melanin also protects against damage from high temperatures, chemical stresses (such as heavy metals and oxidizing agents), and biochemical threats (such as host defenses against invading microbes). Therefore, in many pathogenic microbes (for example, in "Cryptococcus neoformans", a fungus) melanins appear to play important roles in virulence and pathogenicity by protecting the microbe against immune responses of its host. In invertebrates, a major aspect of the innate immune defense system against invading pathogens involves melanin. Within minutes after infection, the microbe is encapsulated within melanin (melanization), and the generation of free radical byproducts during the formation of this capsule is thought to aid in killing them. Some types of fungi, called radiotrophic fungi, appear to be able to use melanin as a photosynthetic pigment that enables them to capture gamma rays and harness this energy for growth.
Melanin is important in mammalian pigmentation. The black feathers of birds owe their color to melanin; they are less readily degraded by bacteria than white feathers, or those containing other pigments such as carotenes. In a bird's eye, a specialized organ, rich in blood vessels, the pecten oculi, is also extremely rich in melanin, which has been considered to have role in absorption of light falling on optic disc and using it to warm up the eye. This, in turn may stimulate release of nutrients from pecten oculi to retina, via vitreous humor; it is plausible as bird retina is devoid of its own blood vessels. In pigment epithelium of retina, presence of high amounts of melanin granules may also minimize back-scatter of image light on the retina.
In some mice, melanin is used slightly differently. For instance, in Agouti mice, the hair appears brown because of alternation between black eumelanin production and a yellow variety of pheomelanin. The hairs are actually banded black and yellow, and the net effect is the brown color of most mice. Some genetic irregularities can produce either fully black or fully yellow mice.
Melanins produced by plants are sometimes referred to as 'Catechol Melanins' as they can yield catechol on alkali fusion. It is commonly seen in the enzymatic browning of fruits such as bananas. Biosynthesis involves the oxidation of indole-5,6-quinone by the tyrosinase type polyphenol oxidase from tyrosine and catecholamines leading to the formation of catechol melanin. Despite this many plants contain compounds which inhibit the production of melanins.
Biosynthetic pathways.
The first step of the biosynthetic pathway for both eumelanins and pheomelanins is catalysed by tyrosinase:
Dopaquinone can combine with cysteine by two pathways to benzothiazines and pheomelanins
Also, dopaquinone can be converted to leucodopachrome and follow two more pathways to the eumelanins
Detailed metabolic pathways can be found in the KEGG database (see External links).
Microscopic appearance.
Melanin is brown, non-refractile, and finely granular with individual granules having a diameter of less than 800 nanometers. This differentiates melanin from common blood breakdown pigments, which are larger, chunky, and refractile, and range in color from green to yellow or red-brown. In heavily pigmented lesions, dense aggregates of melanin can obscure histologic detail. A dilute solution of potassium permanganate is an effective melanin bleach.
Genetic disorders and disease states.
Melanin deficiency has been connected for some time with various genetic abnormalities and disease states.
There are approximately nine different types of oculocutaneous albinism, which is mostly an autosomal recessive disorder. Certain ethnicities have higher incidences of different forms. For example, the most common type, called oculocutaneous albinism type 2 (OCA2), is especially frequent among people of black African descent. It is an autosomal recessive disorder characterized by a congenital reduction or absence of melanin pigment in the skin, hair, and eyes. The estimated frequency of OCA2 among African-Americans is 1 in 10,000, which contrasts with a frequency of 1 in 36,000 in white Americans. In some African nations, the frequency of the disorder is even higher, ranging from 1 in 2,000 to 1 in 5,000. Another form of Albinism, the "yellow oculocutaneous albinism", appears to be more prevalent among the Amish, who are of primarily Swiss and German ancestry. People with this IB variant of the disorder commonly have white hair and skin at birth, but rapidly develop normal skin pigmentation in infancy.
Ocular albinism affects not only eye pigmentation but visual acuity, as well. People with albinism typically test poorly, within the 20/60 to 20/400 range. In addition, two forms of albinism, with approximately 1 in 2700 most prevalent among people of Puerto Rican origin, are associated with mortality beyond melanoma-related deaths.
Mortality also is increased in patients with Hermansky-Pudlak syndrome and Chediak-Higashi syndrome. Patients with Hermansky-Pudlak syndrome have a bleeding diathesis secondary to platelet dysfunction and also experience restrictive lung disease (pulmonary fibrosis), inflammatory bowel disease, cardiomyopathy, and renal disease. Patients with Chediak-Higashi syndrome are susceptible to infection and also can develop lymphofollicular malignancy.
The role that melanin deficiency plays in such disorders remains under study.
The connection between albinism and deafness is well known, though poorly understood. In his 1859 treatise "On the Origin of Species", Charles Darwin observed that "cats which are entirely white and have blue eyes are generally deaf". In humans, hypopigmentation and deafness occur together in the rare Waardenburg's syndrome, predominantly observed among the Hopi in North America. The incidence of albinism in Hopi Indians has been estimated as approximately 1 in 200 individuals. It is interesting to note that similar patterns of albinism and deafness have been found in other mammals, including dogs and rodents. However, a lack of melanin "per se" does not appear to be directly responsible for deafness associated with hypopigmentation, as most individuals lacking the enzymes required to synthesize melanin have normal auditory function. Instead the absence of melanocytes in the stria vascularis of the inner ear results in cochlear impairment, though why this is, is not fully understood.
In Parkinson's disease, a disorder that affects neuromotor functioning, there is decreased neuromelanin in the substantia nigra and locus coeruleus as consequence of specific dropping out of dopaminergic and noradrenergic pigmented neurons. This results in diminished dopamine and norepinephrine synthesis. While no correlation between race and the level of neuromelanin in the substantia nigra has been reported, the significantly lower incidence of Parkinson's in blacks than in whites has "prompt[ed] some to suggest that cutaneous melanin might somehow serve to protect the neuromelanin in substantia nigra from external toxins." Also see Nicolaus review article on the function of neuromelanins.
In addition to melanin deficiency, the molecular weight of the melanin polymer may be decreased by various factors such as oxidative stress, exposure to light, perturbation in its association with melanosomal matrix proteins, changes in pH, or in local concentrations of metal ions. A decreased molecular weight or a decrease in the degree of polymerization of ocular melanin has been proposed to turn the normally anti-oxidant polymer into a pro-oxidant. In its pro-oxidant state, melanin has been suggested to be involved in the causation and progression of macular degeneration and melanoma. Rasagiline, an important monotherapy drug in Parkinson's disease, has melanin binding properties, and melanoma tumor reducing properties.
Higher eumelanin levels also can be a disadvantage, however, beyond a higher disposition toward vitamin D deficiency. Dark skin is a complicating factor in the laser removal of port-wine stains. Effective in treating white skin, in general, lasers are less successful in removing port-wine stains in people of Asian or African descent. Higher concentrations of melanin in darker-skinned individuals simply diffuse and absorb the laser radiation, inhibiting light absorption by the targeted tissue. In similar manner, melanin can complicate laser treatment of other dermatological conditions in people with darker skin.
Freckles and moles are formed where there is a localized concentration of melanin in the skin. They are highly associated with pale skin.
Nicotine has an affinity for melanin-containing tissues because of its precursor function in melanin synthesis or its irreversible binding of melanin. This has been suggested to underlie the increased nicotine dependence and lower smoking cessation rates in darker pigmented individuals.
Human adaptation.
Melanocytes insert granules of melanin into specialized cellular vesicles called melanosomes. These are then transferred into the other skin cells of the human epidermis. The melanosomes in each recipient cell accumulate atop the cell nucleus, where they protect the nuclear DNA from mutations caused by the ionizing radiation of the sun's ultraviolet rays. In general, people whose ancestors lived for long periods in the regions of the globe near the equator have larger quantities of eumelanin in their skins. This makes their skins brown or black and protects them against high levels of exposure to the sun, which more frequently result in melanomas in lighter-skinned people.
With humans, exposure to sunlight stimulates the skin to produce vitamin D. Because high levels of cutaneous melanin act as a natural sun screen, dark skin can be a risk factor for vitamin D deficiency in regions of the Earth known as cool temperate zones, i.e., above 36 degrees latitude in the Northern hemisphere and below 36 degrees in the Southern hemisphere. As a result of this, health authorities in Canada and the USA have issued recommendations for people with darker complexions (including people of southern European descent) to consume between 1000-2000 IU (International Units) of vitamin D, daily, autumn through spring.
The most recent scientific evidence indicates that all humans originated in Africa, then populated the rest of the world through successive radiations. It seems likely that the first modern humans had relatively large numbers of eumelanin-producing melanocytes. In accordance, they had darker skin as with the indigenous people of Africa today. As some of these original peoples migrated and settled in areas of Asia and Europe, the selective pressure for eumelanin production decreased in climates where radiation from the sun was less intense. Of the two common gene variants known to be associated with pale human skin, "Mc1r" does not appear to have undergone positive selection, while "SLC24A5" has.
As with peoples having migrated northward, those with light skin migrating toward the equator acclimatize to the much stronger solar radiation. Most people's skin darkens when exposed to UV light, giving them more protection when it is needed. This is the physiological purpose of sun tanning. Dark-skinned people, who produce more skin-protecting eumelanin, have a greater protection against sunburn and the development of melanoma, a potentially deadly form of skin cancer, as well as other health problems related to exposure to strong solar radiation, including the photodegradation of certain vitamins such as riboflavins, carotenoids, tocopherol, and folate.
Melanin in the eyes, in the iris and choroid, helps protect them from ultraviolet and high-frequency visible light; people with gray, blue, and green eyes are more at risk for sun-related eye problems. Further, the ocular lens yellows with age, providing added protection. However, the lens also becomes more rigid with age, losing most of its accommodation — the ability to change shape to focus from far to near — a detriment due probably to protein crosslinking caused by UV exposure.
Recent research suggests that melanin may serve a protective role other than photoprotection. Melanin is able to effectively ligate metal ions through its carboxylate and phenolic hydroxyl groups, in many cases much more efficiently than the powerful chelating ligand ethylenediaminetetraacetate (EDTA). Thus, it may serve to sequester potentially toxic metal ions, protecting the rest of the cell. This hypothesis is supported by the fact that the loss of neuromelanin observed in Parkinson's disease is accompanied by an increase in iron levels in the brain.
Physical properties and technological applications.
Evidence exists in support of a highly cross-linked heteropolymer bound covalently to matrix scaffolding melanoproteins. It has been proposed that the ability of melanin to act as an antioxidant is directly proportional to its degree of polymerization or molecular weight. Suboptimal conditions for the effective polymerization of melanin monomers may lead to formation of lower-molecular-weight, pro-oxidant melanin that has been implicated in the causation and progression of macular degeneration and melanoma. Signaling pathways that upregulate melanization in the retinal pigment epithelium (RPE) also may be implicated in the downregulation of rod outer segment phagocytosis by the RPE. This phenomenon has been attributed in part to foveal sparing in macular degeneration.

</doc>
<doc id="50515" url="http://en.wikipedia.org/wiki?curid=50515" title="Akkadian language">
Akkadian language

Akkadian ("akkadû", 𒅎𒀝𒂵𒌈 ak.kADû) is an extinct east Semitic language (part of the greater Afroasiatic language family) that was spoken in ancient Mesopotamia. The earliest attested Semitic language, it used the cuneiform writing system, which was originally used to write ancient Sumerian, an unrelated language isolate. The language was named after the city of Akkad by linguists, a major center of Semitic Mesopotamian civilization during the Akkadian Empire (ca. 2334–2154 BC), although the language itself predates the founding of Akkad by many centuries.
The mutual influence between Sumerian and Akkadian had led scholars to describe the languages as a "sprachbund".
Akkadian proper names were first attested in Sumerian texts from ca. the late 29th century BC. From the second half of the third millennium BC (ca. 2500 BC), texts fully written in Akkadian begin to appear. Hundreds of thousands of texts and text fragments have been excavated to date, covering a vast textual tradition of mythological narrative, legal texts, scientific works, correspondence, political and military events, and many other examples. By the second millennium BC, two variant forms of the language were in use in Assyria and Babylonia, known as Assyrian and Babylonian respectively.
Akkadian had been for centuries the native language in Mesopotamian nations such as Assyria and Babylonia, and indeed became the lingua franca of much of the Ancient Near East due to the might of various Mesopotamian empires such as the Akkadian Empire, Old Assyrian Empire, Babylonian Empire and Middle Assyrian Empire}. However, it began to decline during the Neo Assyrian Empire around the 8th century BC, being marginalized by Aramaic during the reign of Tiglath-pileser III. By the Hellenistic period, the language was largely confined to scholars and priests working in temples in Assyria and Babylonia. The last Akkadian cuneiform document dates to the 1st century AD. A fair number of Akkadian loan words, together with the Akkadian grammatical structure, survive in the Mesopotamian Neo Aramaic dialects spoken in and around modern Iraq by the indigenous Assyrian (aka Chaldo-Assyrian) Christians and the Kurdish Jews of the region.
Classification.
Akkadian belongs with the other Semitic languages in the Near Eastnorth branch of the Afro-Asiatic family of languages, a language family native to Western Asia and Northern Africa.
Within the Near Eastern Semitic languages, Akkadian forms an East Semitic subgroup (with Eblaite). This group distinguishes itself from the Northwest and South Semitic languages by its SOV word order, while the other Semitic languages usually have either a VSO or SVO order. This novel word order is due to the influence of the Sumerian substratum, which has an SOV order.
Additionally Akkadian is the only Semitic language to use the prepositions "ina" and "ana" (locative, English "in"/"on"/"with", and dative-locative, "for"/"to", respectively). Other Semitic languages like Arabic and Aramaic have the prepositions "bi/bə" and "li/lə" (locative and dative, respectively). The origin of the Akkadian spatial prepositions is unknown. Arabic does in fact have ina and ana, meaning in order to, and for the purpose of, respectively.
In contrast to most other Semitic languages, Akkadian has only one non-sibilant fricative: ḫ [x]. Akkadian lost both the glottal and pharyngeal fricatives, which are characteristic of the other Semitic languages. Up until the Old Babylonian period, the Akkadian sibilants were exclusively affricate.
History and writing.
Writing.
Old Akkadian is preserved on clay tablets dating back to 2600 BC. It was written using cuneiform, a script adopted from the Sumerians using wedge-shaped symbols pressed in wet clay. As employed by Akkadian scribes, the adapted cuneiform script could represent either (a) Sumerian logograms ("i.e.", picture-based characters representing entire words), (b) Sumerian syllables, (c) Akkadian syllables, or (d) phonetic complements. However, in Akkadian the script practically became a fully fledged syllabic script, and the original logographic nature of cuneiform became secondary. However, logograms for frequent words such as 'god' and 'temple' were still used. For this reason, the sign "AN" can on the one hand be a logogram for the word "ilum" ('god') and on the other signify the god Anu or even the syllable "-an-". Additionally, the sign was used as a determinative for divine names.
Example 4 in the image on the right shows another peculiarity of Akkadian cuneiform. Many signs do not have a well-defined phonetic value. Certain signs, such as "AḪ", do not distinguish between the different vowel qualities. Nor is there any coordination in the other direction; the syllable "-ša-", for example, is rendered by the sign "ŠA", but also by the sign "NĪĜ". Both of these are often used for the same syllable in the same text.
Cuneiform was in many ways unsuited to Akkadian: among its flaws was its inability to represent important phonemes in Semitic, including a glottal stop, pharyngeals, and emphatic consonants. In addition, cuneiform was a syllabary writing system—"i.e.", a consonant plus vowel comprised one writing unit—frequently inappropriate for a Semitic language made up of triconsonantal roots ("i.e.", three consonants plus any vowels).
Development.
Akkadian is divided into several varieties based on geography and historical period:
The earliest known Akkadian inscription was found on a bowl at Ur, addressed to the very early pre-Sargonic king Meskiang-nuna of Ur by his queen Gan-saman, who is thought to have been from Akkad.
The Akkadian Empire, established by Sargon of Akkad, introduced the Akkadian language (the "language of Akkad") as a written language, adapting Sumerian cuneiform orthography for the purpose. During the Middle Bronze Age (Old Assyrian and Old Babylonian period), the language virtually displaced Sumerian, which is assumed to have been extinct as a living language by the 18th century BC.
Old Akkadian, which was used until the end of the 3rd millennium BC, differs from both Babylonian and Assyrian, and was displaced by these dialects. By the 21st century BC Babylonian and Assyrian, which were to become the primary dialects, were easily distinguishable. Old Babylonian, along with the closely related dialect Mariotic, is clearly more innovative than the Old Assyrian dialect and the more distantly related Eblaite language. For this reason, forms like "lu-prus" ('I will decide') are first encountered in Old Babylonian instead of the older "la-prus" (even though it was archaic compared to Akkadian). On the other hand, Assyrian developed certain innovations as well, such as the "Assyrian vowel harmony" (which is not comparable to that found in Turkish or Finnish). Eblaite is even more archaic, retaining a productive dual and a relative pronoun declined in case, number and gender. Both of these had already disappeared in Old Akkadian.
Old Babylonian was the language of king Hammurabi and his code, which is one of the oldest collections of laws in the world. (see Code of Ur-Nammu.)
The Middle Babylonian (or Assyrian) period started in the 16th century BC. The division is marked by the Kassite invasion of Babylonia around 1550 BC. The Kassites, who reigned for 300 years, gave up their own language in favor of Akkadian, but they had little influence on the language. At its apogee, Middle Babylonian was the written language of diplomacy of the entire ancient Orient, including Egypt. During this period, a large number of loan words were included in the language from North West Semitic languages and Hurrian; however, the use of these words was confined to the fringes of the Akkadian speaking territory.
Middle Assyrian served as a "lingua franca" in much of the Ancient Near East of the Late Bronze Age (Amarna Period). During the Neo-Assyrian Empire, Neo-Assyrian began to turn into a chancellery language, being marginalized by Old Aramaic. Under the Achaemenids, Aramaic continued to prosper, but Assyrian continued its decline. The language's final demise came about during the Hellenistic period when it was further marginalized by Koine Greek, even though Neo-Assyrian cuneiform remained in use in literary tradition well into Parthian times. The latest known text in cuneiform Babylonian is an astronomical text dated to 75 AD. The youngest texts written in Akkadian date from the 3rd century AD. A number of Akkadian words and many personal names survive to this day in the modern Assyrian (or Neo Aramaic) language spoken by ethnic Assyrians (aka Chaldo-Assyrians)in Iraq, Iran, Syria and Turkey.
Old Assyrian developed as well during the second millennium BC, but because it was a purely popular language — kings wrote in Babylonian — few long texts are preserved. From 1500 BC onwards, the language is termed Middle Assyrian.
During the first millennium BC, Akkadian progressively lost its status as a lingua franca. In the beginning, from around 1000 BC, Akkadian and Aramaic were of equal status, as can be seen in the number of copied texts: clay tablets were written in Akkadian, while scribes writing on papyrus and leather used Aramaic. From this period on, one speaks of Neo-Babylonian and Neo-Assyrian. Neo-Assyrian received an upswing in popularity in the 10th century BC when the Assyrian kingdom became a major power with the Neo Assyrian Empire, but texts written 'exclusively' in Neo-Assyrian disappear within 10 years of Nineveh's destruction in 612 BC.
After the end of the Mesopotamian kingdoms, which fell due to the Persian conquest of the area, Akkadian (which existed solely in the form of Late Babylonian) disappeared as a popular language. However, the language was still used in its written form; and even after the Greek invasion under Alexander the Great in the 4th century BC, Akkadian was still a contender as a written language, but spoken Akkadian was likely extinct by this time, or at least rarely used. The latest positively identified Akkadian text comes from the 1st century AD.
Decipherment.
The Akkadian language began to be rediscovered when Carsten Niebuhr in 1767 was able to make extensive copies of cuneiform texts and published them in Denmark. The deciphering of the texts started immediately, and bilinguals, in particular Old Persian-Akkadian bilinguals, were of great help. Since the texts contained several royal names, isolated signs could be identified, and were presented in 1802 by Georg Friedrich Grotefend. By this time it was already evident that Akkadian was a Semitic language, and the final breakthrough in deciphering the language came from Edward Hincks, Henry Rawlinson and Jules Oppert in the middle of the 19th century.
Dialects.
The following table summarises the dialects of Akkadian certainly identified so far.
Some researchers (such as W. Sommerfeld 2003) believe that the Old Akkadian variant used in the older texts is not an ancestor of the later Assyrian and Babylonian dialects, but rather a separate dialect that was replaced by these two dialects and which died out early.
Phonetics and phonology.
Because Akkadian as a spoken language is extinct and no contemporary descriptions of the pronunciation are known, little can be said with certainty about the phonetics and phonology of Akkadian. Some conclusions can be made, however, due to the relationship to the other Semitic languages and variant spellings of Akkadian words.
Consonants.
As far as can be told from the cuneiform orthography of Akkadian, several Proto-Semitic phonemes are lost in Akkadian. The Proto-Semitic glottal stop *ʾ, as well as the fricatives *ʿ, *h, *ḥ are lost as consonants, either by sound change or orthographically, but they gave rise to the vowel quality "e" not exhibited in Proto-Semitic. The interdental and the voiceless lateral fricatives (*ś, *ṣ́) merged with the sibilants as in Canaanite, leaving 19 consonantal phonemes.
The following table gives the consonant sounds distinguished in the Akkadian use of cuneiform, with the presumed pronunciation in IPA transcription according to Streck 2005. The parenthesised sign following is the transcription used in the literature, in the cases where that sign is different from the phonetic sign. This transcription has been suggested for all Semitic languages by the Deutsche Morgenländische Gesellschaft (DMG), and is therefore known as "DMG-umschrift".
The status of *š as postalveolar and of *z *s *ṣ as fricatives is contested, for instance by Kogan and Dolgopolsky, due to attested assimilations of voiceless coronal affricates to *s. For example, when the possessive suffix -šu is added to the root awat ('word'), it is written awassu ('his word') even though šš would be expected. What triggered the change from tš to ss is unclear, especially since a shift of š to s does not occur in other contexts.
According to Patrick R. Bennett's "Comparative Semitic Linguistics: a manual", the *š was a voiceless alveolo-palatal. In the pronunciation of an alveolo-palatal, the tongue approximates the teeth more closely.
An alternative approach to the phonology of these consonants is to treat *s *ṣ as voiceless coronal affricates [t͡s t͡sʼ], *š as a voiceless coronal fricative [s] and *z as a voiced coronal affricate or fricative [d͡z~z]. In this vein, an alternative transcription of *š is *s̠, with the macron below indicating a soft (lenis) articulation in Semitic transcription. The assimilation is then awat-su to [awat͡su], which is quite common across languages.
The following table shows Proto-Semitic phonemes and their correspondences among Akkadian, Arabic and Tiberian Hebrew:
Vowels.
Additionally, most researchers presume the existence of back mid vowel /o/, but the cuneiform writings give no good proof for this.
All consonants and vowels appear in long and short forms. Long consonants are represented in writing as double consonants, and long vowels are written with a macron (ā, ē, ī, ū). This distinction is phonemic, and is used in the grammar, for example "iprusu" ('that he decided') versus "iprusū" ('they decided').
Stress.
Nothing is known of Akkadian stress. There are however certain points of reference, such as the rule of vowel syncope (see the next paragraph), and some forms in the cuneiform that might represent the stressing of certain vowels; however, attempts at identifying a rule for stress have so far been unsuccessful.
A rule of Akkadian phonology is that certain short (and probably unstressed) vowels are dropped. The rule is that the last vowel of a succession of syllables that end in a short vowel is dropped, for example the declinational root of the verbal adjective of a root PRS is "PaRiS-". Thus the masculine singular nominative is "PaRS-um" (< "*PaRiS-um") but the feminine singular nominative is "PaRiStum" (< "*PaRiS-at-um"). Additionally there is a general tendency of syncope of short vowels in the later stages of Akkadian.
Grammar.
Morphology.
Overview.
Akkadian is an inflected language; and as a Semitic language, its grammatical features are highly similar to those found in Classical Arabic. And like all Semitic languages, Akkadian uses the system of consonantal roots. Most roots consist of three consonants (called the radicals), but some roots are composed of four consonants (so-called quadriradicals). The radicals are occasionally represented in transcription in upper-case letters, for example "PRS" (to decide). Between and around these radicals various infixes, suffixes and prefixes, having word generating or grammatical functions, are inserted. The resulting consonant-vowel pattern differentiates the original meaning of the root. Also, the middle radical can be geminated, which is represented by a doubled consonant in transcription (and sometimes in the cuneiform writing itself).
The consonants "ʔ", "w", "j" and "n" are termed "weak radicals" and roots containing these radicals give rise to irregular forms.
Case, number and gender.
Akkadian has two grammatical genders, masculine and feminine, with many feminine forms generated from masculine words by adding an "-at" suffix.
Formally, Akkadian has three numbers (singular, dual and plural) and three cases (nominative, accusative and genitive). However, even in the earlier stages of the language, the dual number is vestigial, and its use is largely confined to natural pairs (eyes, ears, etc.), and adjectives are never found in the dual. In the dual and plural, the accusative and genitive are merged into a single oblique case.
Akkadian, unlike Arabic, has mainly regular plurals (i.e. no broken plurals), although some masculine words take feminine plurals. In that respect, it is similar to Hebrew.
The nouns "šarrum" (king), "šarratum" (queen) and the adjective "dannum" (strong) will serve to illustrate the case system of Akkadian.
As is clear from the above table, the adjective and noun endings differ only in the masculine plural. Certain nouns, primarily those referring to geography, can also form a locative ending in "-um" in the singular and the resulting forms serve as adverbials. These forms are generally not productive, but in the Neo-Babylonian the "um"-locative replaces several constructions with the preposition "ina".
In the later stages of Akkadian the mimation (word-final "-m") - along with nunation (dual final "-n") - that occurs at the end of most case endings has disappeared, except in the locative. Later, the nominative and accusative singular of masculine nouns collapse to "-u" and in Neo-Babylonian most word-final short vowels are dropped. As a result case differentiation disappeared from all forms except masculine plural nouns. However many texts continued the practice of writing the case endings (although often sporadically and incorrectly). As the most important contact language throughout this period was Aramaic, which itself lacks case distinctions, it is possible that Akkadian's loss of cases was an areal as well as phonological phenomenon.
Noun States and Nominal Sentences.
As is also the case in other Semitic languages, Akkadian nouns may appear in a variety of "states" depending on their grammatical function in a sentence. The basic form of the noun is the "status rectus" (the Governed state), which is the form as described above, complete with case endings. In addition to this, Akkadian has the "status absolutus" (the Absolute state) and the "status constructus" (Construct state). The latter is found in all other Semitic languages, while the former appears only in Akkadian and some dialects of Aramaic.
The status absolutus is characterised by the loss of a noun's case ending (e.g. "awīl" < "awīlum", "šar" < "šarrum"). It is relatively uncommon, and is used chiefly to mark the predicate of a nominal sentence, in fixed adverbial expressions, and in expressions relating to measurements of length, weight, and the like.
(1) "Awīl-um šū šarrāq"
Translation: This man is a thief
(2) "šarrum lā šanān"
Translation: The king who cannot be rivaled
The Status Constructus is a great deal more common, and has a much wider range of applications. It is employed when a noun is followed by another noun in the genitive, a pronominal suffix, or a verbal clause in the subjunctive, and typically takes the "shortest form of the noun which is phonetically possible". In general, this amounts to the loss of case endings with short vowels, with the exception of the genitive -i in nouns preceding a pronominal suffix, hence:
(3) "māri-šu"
Translation: His son, its (masculine) son
but
(4) "mār šarr-im"
Translation: The king's son
There are numerous exceptions to this general rule, usually involving potential violations of the language's phonological limitations. Most obviously, Akkadian does not tolerate word final consonant clusters, so nouns like "kalbum" (dog) and "maḫrum" (front) would have illegal construct state forms "*kalb" and "*maḫr" unless modified. In many of these instances, the first vowel of the word is simply repeated (e.g. "kalab", "maḫar"). This rule, however, does not always hold true, especially in nouns where a short vowel has historically been elided (e.g. "šaknum" < "*šakinum" "governor"). In these cases, the lost vowel is restored in the construct state (so "šaknum" yields "šakin").
(5) "kalab belim"
Translation: The master's dog
(6) "sakin ālim"
A genitive relation can also be expressed with the relative preposition "ša", and the noun that the genitive phrase depends on appears in status rectus.
(7) salīmātum ša awīl Ešnunna
Translation: The alliances of the Ruler of Ešnunna (literally "Alliances which man of Ešnunna (has)")
The same preposition is also used to introduce true relative clauses, in which case the verb is placed in the subjunctive mood.
(7) "awīl-um ša māt-am i-kšud-Ø-u"
Translation: The man who conquered the land
Verbal morphology.
Verb aspects.
The Akkadian verb has six finite verb aspects (preterite, perfect, present, imperative, precative and vetitive) and three infinite forms (infinitive, participle and verbal adjective). The preterite is used for actions that are seen by the speaker as having occurred at a single point in time. The present is primarily imperfective in meaning and is used for concurrent and future actions as well as past actions with a temporal dimension. The final three finite forms are injunctive where the imperative and the precative together form a paradigm for positive commands and wishes, and the vetitive is used for negative wishes. Additionally the periphrastic prohibitive, formed by the present form of the verb and the negative adverb lā, is used to express negative commands. The infinitive of the Akkadian verb is a verbal noun, and in contrast to some other languages the Akkadian infinitive can be declined in case. The verbal adjective is an adjectival form and designates the state or the result of the action of the verb, and consequently the exact meaning of the verbal adjective is determined by the semantics of the verb itself. The participle, which can be active or passive, is another verbal adjective and its meaning is similar to the English gerund.
The following table shows the conjugation of the G-stem verbs derived from the root PRS ("to decide") in the various verb aspects of Akkadian:
The table below shows the different affixes attached to the preterite aspect of the verb root PRS "to decide"; and as can be seen, the grammatical genders differ only in the second person singular and third person plural.
Verb moods.
Akkadian verbs have 3 moods:
The following table demonstrates the verb moods of verbs derived from the root PRS ("to decide","to separate"):
Verb patterns.
Akkadian verbs have thirteen separate stems formed on each root. The basic, underived, stem is the G-stem (from the German Grundstamm, meaning "basic stem"). Causative or intensive forms are formed with the doubled D-stem, and it gets its name from the doubled middle radical that is characteristic of this form. The doubled middle radical is also characteristic of the present, but the forms of the D-stem use the secondary conjugational affixes, so a D-form will never be identical to a form in a different stem. The Š-stem is formed by adding a prefix "š-", and these forms are mostly causatives. Finally, the passive forms of the verb are in the N-stem, formed by adding a "n-" prefix. However the "n-" element is assimilated to a following consonant, so the original /n/ is only visible in a few forms.
Furthermore, reflexive and iterative verbal stems can be derived from each of the basic stems. The reflexive stem is formed with an infix "-ta", and the derived stems are therefore called Gt, Dt, Št and Nt, and the preterite forms of the Xt-stem are identical to the perfects of the X-stem. Iteratives are formed with the infix "-tan-", giving the Gtn, Dtn, Štn and Ntn. Because of the assimilation of "n", the /n/ is only seen in the present forms, and the Xtn preterite is identical to the Xt durative.
The final stem is the ŠD-stem, a form mostly attested only in poetic texts, and whose meaning is usually identical to either the Š-stem or the D-stem of the same verb. It is formed with the Š prefix (like the Š-stem) in addition to a doubled middle radical (like the D-stem).
An alternative to this naming system is a numerical system. The basic stems are numbered using Roman numerals so that G, D, Š and N become I, II, III and IV, respectively, and the infixes are numbered using Arabic numerals; 1 for the forms without an infix, 2 for the Xt, and 3 for the Xtn. The two numbers are separated using a solidus. As an example, the Štn-stem is called III/3. The most important user of this system is the Chicago Assyrian Dictionary.
There is mandatory congruence between the subject of the sentence and the verb, and this is expressed by prefixes and suffixes. There are two different sets of affixes, a primary set used for the forms of the G and N-stems, and a secondary set for the D and Š-stems.
The stems, their nomenclature and examples of the third-person masculine singular stative of the verb "parāsum" (root PRS: 'to decide, distinguish, separate') is shown below:
Stative.
A very often appearing form which can be formed by nouns, adjectives as well as by verbal adjectives is the stative. Nominal predicatives occur in the status absolutus and correspond to the verb "to be" in English. The stative in Akkadian corresponds to the Egyptian pseudo-participle. The following table contains an example of using the noun "šarrum" (king), the adjective "rapšum" (wide) and the verbal adjective "parsum" (decided).
Thus, the stative in Akkadian is used to convert simple stems into effective sentences, so that the form "šarr-āta" is equivalent to: "you were king", "you are king" and "you will be king". Hence, the stative is independent of time forms.
Derivation.
Beside the already explained possibility of derivation of different verb stems, Akkadian has numerous nominal formations derived from verb roots. A very frequently encountered form is the maPRaS form. It can express the location of an event, the person performing the act and many other meanings. If one of the root consonants is labial (p, b, m), the prefix becomes na- (maPRaS > naPRAS). Examples for this are: "maškanum" (place, location) from ŠKN (set, place, put), "mašraḫum" (splendour) from ŠRḪ (be splendid), "maṣṣarum" (guards) from NṢR (guard), "napḫarum" (sum) from PḪR (summarize).
A very similar formation is the maPRaSt form. The noun derived from this nominal formation is grammatically feminine. The same rules as for the maPRaS form apply, for example "maškattum" (deposit) from ŠKN (set, place, put), "narkabtum" (carriage) from RKB (ride, drive, mount).
The suffix - ūt is used to derive abstract nouns. The nouns which are formed with this suffix are grammatically feminine. The suffix can be attached to nouns, adjectives and verbs, e.g. "abūtum" (paternity) from "abum" (father), "rabutum" (size) from "rabum" (large), "waṣūtum" (leaving) from WṢY (leave).
Also derivatives of verbs from nouns, adjectives and numerals are numerous. For the most part, a D-stem is derived from the root of the noun or adjective. The derived verb then has the meaning of "make X do something" or "becoming X", for example: "duššûm" (let sprout) from "dišu" (grass), "šullušum" (to do something for the third time ) from "šalāš" (three).
Pronouns.
Personal pronouns.
Independent personal pronouns.
Independent personal pronouns in Akkadian are as follows:
Suffixed (or enclitic) pronouns.
Suffixed (or enclitic) pronouns (mainly denoting the genitive, accusative and dative) are as follows:
Demonstrative pronouns.
Demonstrative pronouns in Akkadian differ from the Western Semitic variety. The following table shows the Akkadian demonstrative pronouns according to near and far deixis:
Relative pronouns.
Relative pronouns in Akkadian are shown in the following table:
Unlike plural relative pronouns, singular relative pronouns in Akkadian exhibit full declension for case. However, only the form "ša" (originally accusative masculine singular) survived, while the other forms disappeared in time.
Interrogative pronouns.
The following table shows the Interrogative pronouns used in Akkadian:
Prepositions.
Akkadian has prepositions which consist mainly of only one word. For example: "ina" (in, on, out, through, under), "ana" (too, for, after, approximately), "adi" (to), "aššu" (because of), "eli" (up, over), "ištu/ultu" (of, since), "mala" (in accordance with), "itti" (also, with). There are, however, some compound prepositions which are combined with "ina" and "ana" (e.g. "ina maḫar" (forwards), "ina balu" (without), "ana ṣēr" (up to), "ana maḫar" (forwards). Regardless of the complexity of the preposition, the following noun is always in the genitive case.
Examples: "ina bītim" (in the house, from the house), "ana dummuqim" (to do good), "itti šarrim" (with the king), "ana ṣēr mārīšu" (up to his son).
Numerals.
Since numerals are written mostly as a number sign in the cuneiform script, the transliteration of many numerals is not well ascertained yet. Along with the counted noun, the cardinal numerals are in the status absolutus. Because other cases are very rare, the forms of the status rectus are known only by isolated numerals. The numerals 1 and 2 as well as 21–29, 31–39, 41–49 correspond with the counted in the grammatical gender, while the numerals 3–20, 30, 40 and 50 show gender polarity, i.e. if the counted noun is masculine, the numeral would be feminine and vice versa. This polarity is typical of the Semitic languages and appears also in classical Arabic for example. The numerals 60, 100 and 1000 do not change according to the gender of the counted noun. Counted nouns more than two appear in the plural form. However, body parts which occur in pairs appear in the dual form in Akkadian. e.g. "šepum" (foot) becomes "šepān" (two feet).
The ordinals are formed (with a few exceptions) by adding a case ending to the nominal form PaRuS (the P, R and S. must be substituted with the suitable consonants of the numeral). It is noted, however, that in the case of the numeral "one", the ordinal (masculine) and the cardinal number are the same. A metathesis occurs in the numeral "four". The following table contains the masculine and feminine forms of the status absolutus of some of the Akkadian cardinal numbers, as well as the corresponding ordinals.
Examples: erbē aššātum (four wives) (male numeral), meʾat ālānū (100 towns).
Syntax.
Nominal phrases.
Adjectives, relative clauses and appositions follow the noun.
While numerals precede the counted noun.
In the following table the nominal phrase "erbēt šarrū dannūtum ša ālam īpušū abūya" 'the four strong kings who built the city are my fathers' is analyzed:
Sentence syntax.
Akkadian sentence order was Subject+Object+Verb (SOV), which sets it apart from most other ancient Semitic languages such as Arabic and Biblical Hebrew, which typically have a verb–subject–object (VSO) word order. (Modern South Semitic languages in Ethiopia also have SOV order, but these developed within historical times from the classical verb–subject–object (VSO) language Ge'ez.) It has been hypothesized that this word order was a result of influence from the Sumerian language, which was also SOV. There is evidence that native speakers of both languages were in intimate language contact, forming a single society for at least 500 years, so it is entirely likely that a sprachbund could have formed. Further evidence of an original VSO or SVO ordering can be found in the fact that direct and indirect object pronouns are suffixed to the verb. Word order seems to have shifted to SVO/VSO late in the 1st millennium BC to the 1st millennium AD, possibly under the influence of Aramaic.
Vocabulary.
The Akkadian vocabulary is mostly of Semitic origin. Although classified as 'East Semitic', many elements of its basic vocabulary find no evident parallels in related Semitic languages. For example: "māru" 'son' (Semitic *bn), "qātu" 'hand' (Semitic *yd), "šēpu" 'foot' (Semitic *rgl), "qabû" 'say' (Semitic *qwl), "izuzzu" 'stand' (Semitic *qwm), "ana" 'to, for' (Semitic *li).
Due to extensive contact with Sumerian and Aramaic, the Akkadian vocabulary contains many loan words from these languages. Aramaic loan words, however, were limited to the 1st centuries of the 1st millennium BC and primarily in the north and middle parts of Mesopotamia, whereas Sumerian loan words were spread in the whole linguistic area. Beside the previous languages, some nouns were borrowed from Hurrian, Kassite, Ugaritic and other ancient languages.
Since Sumerian and Hurrian, two non-Semitic languages, differ from Akkadian in word structure, only nouns and some adjectives (not many verbs) were borrowed from these languages. However, some verbs were borrowed (along with many nouns) from Aramaic and Ugaritic, both of which are Semitic languages.
The following table contains examples of loan words in Akkadian:
Akkadian was also a source of borrowing to other languages, above all Sumerian. Some examples are: Sumerian "da-ri" ('lastingly', from Akkadian "dāru"), Sumerian "ra gaba" ('riders, messenger', from Akkadian "rākibu").
Example text.
The following text is the 7th section of the Hammurabi law code, written in the mid-18th century BC:
Translation: "If a man bought silver, gold, a slave (masculine), a slave (feminine), an ox, a sheep, a donkey or something other from the hand of another man or a slave of a man without witnesses or contract, or accepted (them) for safekeeping (without same), then this man is a thief; he is to be killed."
References.
</dl>

</doc>
<doc id="50521" url="http://en.wikipedia.org/wiki?curid=50521" title="Sumer">
Sumer

Sumer () was one of the ancient civilizations and historical regions in southern Mesopotamia, modern-day southern Iraq, during the Chalcolithic and Early Bronze Age. Although it was previously thought that the earliest forms of writing in the region do not go back much further than c. 3500 BC, modern historians have suggested that Sumer was first permanently settled between c. 5500 and 4000 BC by a non-Semitic people who spoke the Sumerian language (pointing to the names of cities, rivers, basic occupations, etc. as evidence). These conjectured, prehistoric people are now called "proto-Euphrateans" or "Ubaidians", and are theorized to have evolved from the Samarra culture of northern Mesopotamia (Assyria). The Ubaidians were the first civilizing force in Sumer, draining the marshes for agriculture, developing trade, and establishing industries, including weaving, leatherwork, metalwork, masonry, and pottery. 
However, some scholars such as Piotr Michalowski and Gerd Steiner, contest the idea of a Proto-Euphratean language or one substrate language. It has been suggested by them and others, that the Sumerian language was originally that of the hunter and fisher peoples, who lived in the marshland and the Eastern Arabia littoral region, and were part of the Arabian bifacial culture. Reliable historical records begin much later; there are none in Sumer of any kind that have been dated before Enmebaragesi (c. 26th century BC). Professor Juris Zarins believes the Sumerians were settled along the coast of Eastern Arabia, today's Persian Gulf region, before it flooded at the end of the Ice Age. Sumerian literature speaks of their homeland being Dilmun.
Sumerian civilization took form in the Uruk period (4th millennium BC), continuing into the Jemdat Nasr and Early Dynastic periods. During the 3rd millennium BC, a close cultural symbiosis developed between the Sumerians (who spoke a language isolate) and the Semitic Akkadian speakers, which included widespread bilingualism. The influence of Sumerian on Akkadian (and "vice versa") is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the 3rd millennium BC as a "Sprachbund". Sumer was conquered by the Semitic-speaking kings of the Akkadian Empire around 2270 BC (short chronology), but Sumerian continued as a sacred language. Native Sumerian rule re-emerged for about a century in the Third Dynasty of Ur (Sumerian Renaissance) aproximately 2100-2000 BC, but the Akkadian language also remained in use. The Sumerian city of Eridu, on the coast of the Persian Gulf, was the world's first city, where three separate cultures fused — that of peasant Ubaidian farmers, living in mud-brick huts and practicing irrigation; that of mobile nomadic Semitic pastoralists living in black tents and following herds of sheep and goats; and that of fisher folk, living in reed huts in the marshlands, who may have been the ancestors of the Sumerians.
The irrigated farming together with annual replenishment of soil fertility and the surplus of storable food in temple granaries created by this economy allowed the population of this region to rise to levels never before seen, unlike those found in earlier cultures of shifting cultivators. This much greater population density in turn created and required an extensive labour force and division of labour with many specialised arts and crafts. At the same time, historic overuse of the irrigated soils led to progressive salinisation, and a Malthusian crisis which led to depopulation of the Sumerian region over time, leading to its progressive eclipse by the Akkadians of middle Mesopotamia.
Sumer was also the site of early development of writing, progressing from a stage of proto-writing in the mid 4th millennium BC to writing proper in the 3rd millennium BC (see Jemdet Nasr period).
Origin of name.
The term "Sumerian" is the common name given to the ancient non-Semitic inhabitants of Mesopotamia, Sumer, by the Semitic Akkadians. The Sumerians referred to themselves as "ùĝ saĝ gíg-ga" (cuneiform: &#x12326; &#x12295; &#x1222A; &#x120B5;), phonetically uŋ saŋ giga, literally meaning "the black-headed people". The Akkadian word "Shumer" may represent the geographical name in dialect, but the phonological development leading to the Akkadian term "šumerû" is uncertain. Hebrew "Shinar", Egyptian "Sngr", and Hittite "Šanhar(a)", all referring to southern Mesopotamia, could be western variants of "Shumer".
City-states in Mesopotamia.
By the late 4th millennium BC, Sumer was divided into about a dozen independent city-states, which were divided by canals and boundary stones. Each was centered on a temple dedicated to the particular patron god or goddess of the city and ruled over by a priestly governor (ensi) or by a king (lugal) who was intimately tied to the city's religious rites.
The five "first" cities said to have exercised pre-dynastic kingship:   
Other principal cities:
<br>
Minor cities (from south to north):
Apart from Mari, which lies full 330 km northwest of Agade, but which is credited in the king list as having “exercised kingship” in the Early Dynastic II period, and Nagar, an outpost, these cities are all in the Euphrates-Tigris alluvial plain, south of Baghdad in what are now the Bābil, Diyala, Wāsit, Dhi Qar, Basra, Al-Muthannā and Al-Qādisiyyah governorates of Iraq.
History.
The Sumerian city-states rose to power during the prehistoric Ubaid and Uruk periods. Sumerian written history reaches back to the 27th century BC and before, but the historical record remains obscure until the Early Dynastic III period, c. the 23rd century BC, when a now deciphered syllabary writing system was developed, which has allowed archaeologists to read contemporary records and inscriptions. Classical Sumer ends with the rise of the Akkadian Empire in the 23rd century BC. Following the Gutian period, there is a brief Sumerian Renaissance in the 21st century BC, cut short in the 20th century BC by Semitic Amorite invasions. The Amorite "dynasty of Isin" persisted until c. 1700 BC, when Mesopotamia was united under Babylonian rule. The Sumerians were eventually absorbed into the Akkadian (Assyro-Babylonian) population.
Ubaid period.
The Ubaid period is marked by a distinctive style of fine quality painted pottery which spread throughout Mesopotamia and the Persian Gulf. During this time, the first settlement in southern Mesopotamia was established at Eridu (Cuneiform: NUN.KI), c. 5300 BC, by farmers who brought with them the Hadji Muhammed culture, which first pioneered irrigation agriculture. It appears this culture was derived from the Samarran culture from northern Mesopotamia. It is not known whether or not these were the actual Sumerians who are identified with the later Uruk culture. Eridu remained an important religious center when it was gradually surpassed in size by the nearby city of Uruk. The story of the passing of the "me" (gifts of civilisation) to Inanna, goddess of Uruk and of love and war, by Enki, god of wisdom and chief god of Eridu, may reflect this shift in hegemony.
Uruk period.
The archaeological transition from the Ubaid period to the Uruk period is marked by a gradual shift from painted pottery domestically produced on a slow wheel to a great variety of unpainted pottery mass-produced by specialists on fast wheels. The Uruk period is a continuation and an outgrowth of Ubaid with pottery being the main visible change.
By the time of the Uruk period (c. 4100–2900 BC calibrated), the volume of trade goods transported along the canals and rivers of southern Mesopotamia facilitated the rise of many large, stratified, temple-centered cities (with populations of over 10,000 people) where centralized administrations employed specialized workers. It is fairly certain that it was during the Uruk period that Sumerian cities began to make use of slave labor captured from the hill country, and there is ample evidence for captured slaves as workers in the earliest texts. Artifacts, and even colonies of this Uruk civilization have been found over a wide area—from the Taurus Mountains in Turkey, to the Mediterranean Sea in the west, and as far east as Central Iran.
The Uruk period civilization, exported by Sumerian traders and colonists (like that found at Tell Brak), had an effect on all surrounding peoples, who gradually evolved their own comparable, competing economies and cultures. The cities of Sumer could not maintain remote, long-distance colonies by military force.
Sumerian cities during the Uruk period were probably theocratic and were most likely headed by a priest-king ("ensi"), assisted by a council of elders, including both men and women. It is quite possible that the later Sumerian pantheon was modeled upon this political structure. There was little evidence of institutionalized violence or professional soldiers during the Uruk period, and towns were generally unwalled. During this period Uruk became the most urbanised city in the world, surpassing for the first time 50,000 inhabitants.
The ancient Sumerian king list includes the early dynasties of several prominent cities from this period. The first set of names on the list is of kings said to have reigned before a major flood occurred. These early names may be fictional, and include some legendary and mythological figures, such as Alulim and Dumizid.
The end of the Uruk period coincided with the Piora oscillation, a dry period from c. 3200 – 2900 BC that marked the end of a long wetter, warmer climate period from about 9,000 to 5,000 years ago, called the Holocene climatic optimum.
Early Dynastic Period.
The Dynastic period begins c. 2900 BC and includes such legendary figures as Enmerkar and Gilgamesh—who are supposed to have reigned shortly before the historic record opens c. 2700 BC, when the now deciphered syllabic writing started to develop from the early pictograms. The center of Sumerian culture remained in southern Mesopotamia, even though rulers soon began expanding into neighboring areas, and neighboring Semitic groups adopted much of Sumerian culture for their own.
The earliest Dynastic king on the Sumerian king list whose name is known from any other legendary source is Etana, 13th king of the first Dynasty of Kish. The earliest king authenticated through archaeological evidence is Enmebaragesi of Kish (c. 26th century BC), whose name is also mentioned in the Gilgamesh epic—leading to the suggestion that Gilgamesh himself might have been a historical king of Uruk. As the Epic of Gilgamesh shows, this period was associated with increased violence. Cities became walled, and increased in size as undefended villages in southern Mesopotamia disappeared. (Gilgamesh is credited with having built the walls of Uruk).
1st Dynasty of Lagash.
c. 2500–2270 BC
The dynasty of Lagash, though omitted from the king list, is well attested through several important monuments and many archaeological finds.
Although short-lived, one of the first empires known to history was that of Eannatum of Lagash, who annexed practically all of Sumer, including Kish, Uruk, Ur, and Larsa, and reduced to tribute the city-state of Umma, arch-rival of Lagash. In addition, his realm extended to parts of Elam and along the Persian Gulf. He seems to have used terror as a matter of policy—his Stele of the Vultures has been found, showing violent treatment of enemies. His empire collapsed shortly after his death.
Later, Lugal-Zage-Si, the priest-king of Umma, overthrew the primacy of the Lagash dynasty in the area, then conquered Uruk, making it his capital, and claimed an empire extending from the Persian Gulf to the Mediterranean. He was the last ethnically Sumerian king before the arrival of the Semitic king, Sargon of Akkad.
Akkadian Empire.
c. 2270–2083 BC (short chronology)
The Semitic Akkadian language is first attested in proper names of the kings of Kish c. 2800 BC, preserved in later king lists. There are texts written entirely in Old Akkadian dating from c. 2500 BC. Use of Old Akkadian was at its peak during the rule of Sargon the Great (c. 2270–2215 BC), but even then most administrative tablets continued to be written in Sumerian, the language used by the scribes. Gelb and Westenholz differentiate three stages of Old Akkadian: that of the pre-Sargonic era, that of the Akkadian empire, and that of the "Neo-Sumerian Renaissance" that followed it. Akkadian and Sumerian coexisted as vernacular languages for about one thousand years, but by around 1800 BC, Sumerian was becoming more of a literary language familiar mainly only to scholars and scribes. Thorkild Jacobsen has argued that there is little break in historical continuity between the pre- and post-Sargon periods, and that too much emphasis has been placed on the perception of a "Semitic vs. Sumerian" conflict. However, it is certain that Akkadian was also briefly imposed on neighboring parts of Elam that were previously conquered by Sargon.
Gutian period.
c. 2083–2050 BC (short chronology)
2nd Dynasty of Lagash.
c. 2093–2046 BC (short chronology)
Following the downfall of the Akkadian Empire at the hands of Gutians, another native Sumerian ruler, Gudea of Lagash, rose to local prominence and continued the practices of the Sargonid kings' claims to divinity. Like the previous Lagash dynasty, Gudea and his descendants also promoted artistic development and left a large number of archaeological artifacts.
Sumerian Renaissance.
c. 2047–1940 BC (short chronology)
Later, the 3rd dynasty of Ur under Ur-Nammu and Shulgi, whose power extended as far as northern Mesopotamia, was the last great "Sumerian renaissance", but already the region was becoming more Semitic than Sumerian, with the rise in power of the Akkadian speaking Semites, and the influx of waves of Semitic Martu (Amorites) who were to found several competing local powers including Isin, Larsa, and Babylon. The last of these eventually came to dominate the south of Mesopotamia as the Babylonian Empire, just as the Assyrian Empire did in the north. The Sumerian language continued as a sacerdotal language taught in schools in Babylonia and Assyria, much as Latin was used in the Medieval period, for as long as cuneiform was utilized
Decline.
This period is generally taken to coincide with a major shift in population from southern Mesopotamia toward the north. Ecologically, the agricultural productivity of the Sumerian lands was being compromised as a result of rising salinity. Soil salinity in this region had been long recognized as a major problem. Poorly drained irrigated soils, in an arid climate with high levels of evaporation, led to the buildup of dissolved salts in the soil, eventually reducing agricultural yields severely. During the Akkadian and Ur III phases, there was a shift from the cultivation of wheat to the more salt-tolerant barley, but this was insufficient, and during the period from 2100 BC to 1700 BC, it is estimated that the population in this area declined by nearly three fifths. This greatly upset the balance of power within the region, weakening the areas where Sumerian was spoken, and comparatively strengthening those where Akkadian was the major language. Henceforth Sumerian would remain only a literary and liturgical language, similar to the position occupied by Latin in medieval Europe.
Following an Elamite invasion and sack of Ur during the rule of Ibbi-Sin (c. 1940 BC), Sumer came under Amorites rule (taken to introduce the Middle Bronze Age). The independent Amorite states of the 20th to 18th centuries are summarized as the "Dynasty of Isin" in the Sumerian king list, ending with the rise of Babylonia under Hammurabi c. 1700 BC.
Population.
Uruk, one of Sumer's largest cities, has been estimated to have had a population at its height of 50-80,000; given the other cities in Sumer, and the large agricultural population, a rough estimate for Sumer's population might be 0.8million-1.5million. The world population at this time has been estimated at about 27m.
The Sumerians were a non-Semitic people, and spoke a language isolate; a number of linguists believed they could detect a substrate language beneath Sumerian, names of some of Sumer's major cities are not Sumerian, revealing influences of earlier inhabitants. However, the archaeological record shows clear uninterrupted cultural continuity from the time of the Early Ubaid period (5300 – 4700 BC C-14) settlements in southern Mesopotamia. The Sumerian people who settled here farmed the lands in this region that were made fertile by silt deposited by the Tigris and the Euphrates rivers.
It is speculated by some archaeologists that Sumerian speakers were farmers who moved down from the north, after perfecting irrigation agriculture there [note there is no consensus among scholars on the origins of the Sumerians]. The Ubaid pottery of southern Mesopotamia has been connected via Choga Mami Transitional ware to the pottery of the Samarra period culture (c. 5700 – 4900 BC C-14) in the north, who were the first to practice a primitive form of irrigation agriculture along the middle Tigris River and its tributaries. The connection is most clearly seen at Tell Awayli ("Oueilli", "Oueili") near Larsa, excavated by the French in the 1980s, where eight levels yielded pre-Ubaid pottery resembling Samarran ware. According to this view, farming peoples spread down into southern Mesopotamia because they had developed a temple-centered social organization for mobilizing labor and technology for water control, enabling them to survive and prosper in a difficult environment.
Others have suggested a continuity of Sumerians, from the indigenous hunter-fisherfolk traditions, associated with the Arabian bifacial assemblages found on the Arabian littoral. The Sumerians themselves claimed kinship with the people of Dilmun, associated with Bahrein in the Persian Gulf. Professor Juris Zarins believes the Sumerians may have been the people living in the Persian Gulf region before it flooded at the end of the last Ice Age.
Culture.
Social and family life.
In the early Sumerian period (i.e. Uruk), the primitive pictograms suggest that
There is considerable evidence that the Sumerians loved music, which seems to have been an important part of religious and civic life in Sumer. Lyres were popular in Sumer, among the best-known examples being the Lyres of Ur.
Inscriptions describing the reforms of king Urukagina of Lagash (c. 2300 BC) say that he abolished the former custom of polyandry in his country, prescribing that a woman who took multiple husbands be stoned with rocks upon which her crime had been written.
Though women were protected by late Sumerian law and were able to achieve a higher status in Sumer than in other contemporary civilizations, the culture was male-dominated. The Code of Ur-Nammu, the oldest such codification yet discovered, dating to the Ur-III "Sumerian Renaissance", reveals a glimpse at societal structure in late Sumerian law. Beneath the "lu-gal" ("great man" or king), all members of society belonged to one of two basic strata: The "lu" or free person, and the slave (male, "arad"; female "geme"). The son of a "lu" was called a "dumu-nita" until he married. A woman ("munus") went from being a daughter ("dumu-mi"), to a wife ("dam"), then if she outlived her husband, a widow ("numasu") and she could then remarry.
Language and writing.
The most important archaeological discoveries in Sumer are a large number of tablets written in cuneiform. Sumerian writing, while proven to be not the oldest example of writing on earth, is considered to be a great milestone in the development of man's ability to not only create historical records but also in creating pieces of literature both in the form of poetic epics and stories as well as prayers and laws. Although pictures — that is, hieroglyphs — were first used, symbols were later made to represent syllables. Triangular or wedge-shaped reeds were used to write on moist clay. A large body of hundreds of thousands of texts in the Sumerian language have survived, such as personal or business letters, receipts, lexical lists, laws, hymns, prayers, stories, daily records, and even libraries full of clay tablets. Monumental inscriptions and texts on different objects like statues or bricks are also very common. Many texts survive in multiple copies because they were repeatedly transcribed by scribes-in-training. Sumerian continued to be the language of religion and law in Mesopotamia long after Semitic speakers had become dominant.
The Sumerian language is generally regarded as a language isolate in linguistics because it belongs to no known language family; Akkadian, by contrast, belongs to the Semitic branch of the Afroasiatic languages. There have been many failed attempts to connect Sumerian to other language groups. It is an agglutinative language; in other words, morphemes ("units of meaning") are added together to create words, unlike analytic languages where morphemes are purely added together to create sentences.
Understanding Sumerian texts today can be problematic even for experts. Most difficult are the earliest texts, which in many cases do not give the full grammatical structure of the language.
During the 3rd millennium BC a cultural symbiosis developed between the Sumerians and the Akkadians, which included widespread bilingualism. The influences between Sumerian on Akkadian are evident in all areas including lexical borrowing on a massive scale--and syntactic, morphological, and phonological convergence. This mutual influence has prompted scholars to refer to Sumerian and Akkadian of the 3rd millennium BC as a "Sprachbund".
Akkadian gradually replaced Sumerian as a spoken language somewhere around the turn of the 3rd and the 2nd millennium BC, but Sumerian continued to be used as a sacred, ceremonial, literary, and scientific language in Babylonia and Assyria until the 1st century CE.
Religion.
At an early stage following the dawn of recorded history, Nippur in central Mesopotamia replaced Eridu in the south as the primary temple city, whose priests also conferred the status of political hegemony on the other city-states. Nippur retained this status throughout the first Sumerian period, until Sargon of Akkad is said to have transferred it to Babylon.
Deities.
Sumerians believed in an anthropomorphic polytheism, or the belief in many gods in human form. There was no common set of gods; each city-state had its own patrons, temples, and priest-kings, however they were not exclusive. The gods of one city were often acknowledged elsewhere. Sumerian speakers were among the earliest people to record their beliefs in writing, and were a major inspiration in later Mesopotamian mythology, religion, and astrology. 
The Sumerians worshiped:
These deities formed a core pantheon; there were additionally hundreds of minor ones. Sumerian gods could thus have associations with different cities, and their religious importance often waxed and waned with those cities' political power. The gods were said to have created human beings from clay for the purpose of serving them. The temples organized the mass labour projects needed for irrigation agriculture. Citizens had a labor duty to the temple, though they could avoid it by a payment of silver.
Cosmology.
Sumerians believed that the universe consisted of a flat disk enclosed by a dome. The Sumerian afterlife involved a descent into a gloomy netherworld to spend eternity in a wretched existence as a Gidim (ghost).
The universe was divided into four quarters. 
Their known world extended from "The Upper Sea" or Mediterranean coastline, to "The Lower Sea", the Persian Gulf and the land of Meluhha (probably the Indus Valley) and Magan (Oman), famed for its copper ores.
Temples and temple organisation.
Ziggurats (Sumerian temples) each had an individual name and consisted of a forecourt, with a central pond for purification. The temple itself had a central nave with aisles along either side. Flanking the aisles would be rooms for the priests. At one end would stand the podium and a mudbrick table for animal and vegetable sacrifices. Granaries and storehouses were usually located near the temples. After a time the Sumerians began to place the temples on top of multi-layered square constructions built as a series of rising terraces, giving rise to the Ziggurat style.
Funerary practices.
It was believed that when people died, they would be confined to a gloomy world of Ereshkigal, whose realm was guarded by gateways with various monsters designed to prevent people entering or leaving. The dead were buried outside the city walls in graveyards where a small mound covered the corpse, along with offerings to monsters and a small amount of food. Human sacrifice was found in the death pits at the Ur royal cemetery where Queen Puabi was accompanied in death by her servants.
It is also said that the Sumerians invented the first oboe-like instrument, and used them at royal funerals.
Agriculture and hunting.
The Sumerians adopted an agricultural lifestyle perhaps as early as c. 5000 BC – 4500 BC. The region demonstrated a number of core agricultural techniques, including organized irrigation, large-scale intensive cultivation of land, mono-cropping involving the use of plough agriculture, and the use of an agricultural specialized labour force under bureaucratic control. The necessity to manage temple accounts with this organization led to the development of writing (c. 3500 BC).
In the early Sumerian Uruk period, the primitive pictograms suggest that sheep, goats, cattle, and pigs were domesticated. They used oxen as their primary beasts of burden and donkeys or equids as their primary transport animal and "woollen clothing as well as rugs were made from the wool or hair of the animals. ... By the side of the house was an enclosed garden planted with trees and other plants; wheat and probably other cereals were sown in the fields, and the "shaduf" was already employed for the purpose of irrigation. Plants were also grown in pots or vases."
The Sumerians were one of the first known beer drinking societies. Cereals were plentiful and were the key ingredient in their early brew. They brewed multiple kinds of beer consisting of wheat, barley, and mixed grain beers. Beer brewing was very important to the Sumerians. It was referenced in the Epic of Gilgamesh when Enkidu was introduced to the food and beer of Gilgamesh's people: "Drink the beer, as is the custom of the land... He drank the beer-seven jugs! and became expansive and sang with joy!"
The Sumerians practiced similar irrigation techniques as those used in Egypt. American anthropologist Robert McCormick Adams says that irrigation development was associated with urbanization, and that 89% of the population lived in the cities.
They grew barley, chickpeas, lentils, wheat, dates, onions, garlic, lettuce, leeks and mustard. Sumerians caught many fish and hunted fowl and gazelle.
Sumerian agriculture depended heavily on irrigation. The irrigation was accomplished by the use of "shaduf", canals, channels, dykes, weirs, and reservoirs. The frequent violent floods of the Tigris, and less so, of the Euphrates, meant that canals required frequent repair and continual removal of silt, and survey markers and boundary stones needed to be continually replaced. The government required individuals to work on the canals in a corvee, although the rich were able to exempt themselves.
As is known from the "Sumerian Farmer's Almanac", after the flood season and after the Spring Equinox and the "Akitu" or New Year Festival, using the canals, farmers would flood their fields and then drain the water. Next they made oxen stomp the ground and kill weeds. They then dragged the fields with pickaxes. After drying, they plowed, harrowed, and raked the ground three times, and pulverized it with a mattock, before planting seed. Unfortunately the high evaporation rate resulted in a gradual increase in the salinity of the fields. By the Ur III period, farmers had switched from wheat to the more salt-tolerant barley as their principal crop.
Sumerians harvested during the spring in three-person teams consisting of a reaper, a binder, and a sheaf handler. The farmers would use threshing wagons, driven by oxen, to separate the cereal heads from the stalks and then use threshing sleds to disengage the grain. They then winnowed the grain/chaff mixture.
Architecture.
The Tigris-Euphrates plain lacked minerals and trees. Sumerian structures were made of plano-convex mudbrick, not fixed with mortar or cement. Mud-brick buildings eventually deteriorate, so they were periodically destroyed, leveled, and rebuilt on the same spot. This constant rebuilding gradually raised the level of cities, which thus came to be elevated above the surrounding plain. The resultant hills, known as tells, are found throughout the ancient Near East.
According to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that "Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones — or rather bricks — of a house were consecrated by certain objects that were deposited under them."
The most impressive and famous of Sumerian buildings are the ziggurats, large layered platforms which supported temples. Sumerian cylinder seals also depict houses built from reeds not unlike those built by the Marsh Arabs of Southern Iraq until as recently as 400 CE. The Sumerians also developed the arch, which enabled them to develop a strong type of dome. They built this by constructing and linking several arches. Sumerian temples and palaces made use of more advanced materials and techniques, such as buttresses, recesses, half columns, and clay nails.
Mathematics.
The Sumerians developed a complex system of metrology c. 4000 BC. This metrology advanced resulting in the creation of arithmetic, geometry, and algebra. From c. 2600 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period. The period c. 2700 – 2300 BC saw the first appearance of the abacus, and a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. The Sumerians were the first to use a place value numeral system. There is also anecdotal evidence the Sumerians may have used a type of slide rule in astronomical calculations. They were the first to find the area of a triangle and the volume of a cube.
Economy and trade.
Discoveries of obsidian from far-away locations in Anatolia and lapis lazuli from Badakhshan in northeastern Afghanistan, beads from Dilmun (modern Bahrain), and several seals inscribed with the Indus Valley script suggest a remarkably wide-ranging network of ancient trade centered around the Persian Gulf.
The Epic of Gilgamesh refers to trade with far lands for goods such as wood that were scarce in Mesopotamia. In particular, cedar from Lebanon was prized. The finding of resin in the tomb of Queen Puabi at Ur, indicates it was traded from as far away as Mozambique.
The Sumerians used slaves, although they were not a major part of the economy. Slave women worked as weavers, pressers, millers, and porters.
Sumerian potters decorated pots with cedar oil paints. The potters used a bow drill to produce the fire needed for baking the pottery. Sumerian masons and jewelers knew and made use of alabaster (calcite), ivory, iron, gold, silver, carnelian, and lapis lazuli.
Military.
The almost constant wars among the Sumerian city-states for 2000 years helped to develop the military technology and techniques of Sumer to a high level. The first war recorded in any detail was between Lagash and Umma in c. 2525 BC on a stele called the Stele of the Vultures. It shows the king of Lagash leading a Sumerian army consisting mostly of infantry. The infantrymen carried spears, wore copper helmets and carried leather or wicker shields. The spearmen are shown arranged in what resembles the phalanx formation, which requires training and discipline; this implies that the Sumerians may have made use of professional soldiers.
The Sumerian military used carts harnessed to onagers. These early chariots functioned less effectively in combat than did later designs, and some have suggested that these chariots served primarily as transports, though the crew carried battle-axes and lances. The Sumerian chariot comprised a four or two-wheeled device manned by a crew of two and harnessed to four onagers. The cart was composed of a woven basket and the wheels had a solid three-piece design.
Sumerian cities were surrounded by defensive walls. The Sumerians engaged in siege warfare between their cities, but the mudbrick walls were able to deter some foes.
Technology.
Examples of Sumerian technology include: the wheel, cuneiform, arithmetic and geometry, irrigation systems, Sumerian boats, lunisolar calendar, bronze, leather, saws, chisels, hammers, braces, bits, nails, pins, rings, hoes, axes, knives, lancepoints, arrowheads, swords, glue, daggers, waterskins, bags, harnesses, armor, quivers, war chariots, scabbards, boots, sandals, harpoons and beer.
The Sumerians had three main types of boats:
Legacy.
Evidence of wheeled vehicles appeared in the mid 4th millennium BC, near-simultaneously in Mesopotamia, the Northern Caucasus (Maykop culture) and Central Europe. The wheel initially took the form of the potter's wheel. The new concept quickly led to wheeled vehicles and mill wheels. The Sumerians' cuneiform writing system is the oldest (or second oldest after the Egyptian hieroglyphs) which has been deciphered (the status of even older inscriptions such as the Jiahu symbols and Tartaria tablets is controversial). The Sumerians were among the first astronomers, mapping the stars into sets of constellations, many of which survived in the zodiac and were also recognized by the ancient Greeks. They were also aware of the five planets that are easily visible to the naked eye.
They invented and developed arithmetic by using several different number systems including a mixed radix system with an alternating base 10 and base 6. This sexagesimal system became the standard number system in Sumer and Babylonia. They may have invented military formations and introduced the basic divisions between infantry, cavalry, and archers. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The first true city-states arose in Sumer, roughly contemporaneously with similar entities in what are now Syria and Lebanon. Several centuries after the invention of cuneiform, the use of writing expanded beyond debt/payment certificates and inventory lists to be applied for the first time, about 2600 BC, to messages and mail delivery, history, legend, mathematics, astronomical records, and other pursuits. Conjointly with the spread of writing, the first formal schools were established, usually under the auspices of a city-state's primary temple.
Finally, the Sumerians ushered in domestication with intensive agriculture and irrigation. Emmer wheat, barley, sheep (starting as mouflon), and cattle (starting as aurochs) were foremost among the species cultivated and raised for the first time on a grand scale.

</doc>
<doc id="50524" url="http://en.wikipedia.org/wiki?curid=50524" title="Melanocytic nevus">
Melanocytic nevus

A melanocytic nevus (also known as "nevocytic nevus") is a type of lesion that contains nevus cells (a type of melanocyte). Some sources equate the term mole with "melanocytic nevus". Other sources reserve the term "mole" for other purposes.
The majority of moles appear during the first two decades of a person's life, with about one in every 100 babies being born with moles. Acquired moles are a form of benign neoplasm, while congenital moles, or congenital nevi, are considered a minor malformation or hamartoma and may be at a higher risk for melanoma. A mole can be either subdermal (under the skin) or a pigmented growth on the skin, formed mostly of a type of cell known as a melanocyte. The high concentration of the body's pigmenting agent, melanin, is responsible for their dark color. Moles are a member of the family of skin lesions known as nevi.
Classification.
Some sources equate the term mole with "melanocytic nevus". Other sources reserve the term "mole" for other purposes such as the animal of the same name.
Melanocytic nevi represent a family of lesions. The most common variants are:
Signs and symptoms.
According to the American Academy of Dermatology, the most common types of moles are skin tags, raised moles and flat moles. Benign moles are usually circular or oval and not very large, though some can be larger than the size of a typical pencil eraser. Some moles produce dark, coarse hair. Common mole hair removal procedures include plucking, cosmetic waxing, electrolysis, threading and cauterization.
Aging.
Moles tend to appear during early childhood and during the first 30 years of life. They may change slowly, becoming raised, changing color or gradually fading. Most people have between 30 and 40 moles, but some have as many as 600.
The number of moles a person has was found to have a correlation with telomere length. The length of telomeres may be of significance in the ageing process. But the relation between telomeres and aging remains uncertain.
Cause.
The cause is not clearly understood, but is thought to be caused by a defect in embryologic development. This is in the first twelve weeks of pregnancy. The defect is thought to cause a proliferation of melanocytes. This means melanocytes, the cells in the body in charge of normal skin color, are being produced at an extremely fast rate. This is causing the melanocytes to form in clusters instead of spread out, causing abnormal skin pigmentation in some areas of the body.
Genetics.
Genes can have an influence on a person's moles.
Dysplastic nevi and atypical mole syndrome are hereditary conditions which causes a person to have a large quantity of moles (often 100 or more) with some larger than normal or atypical. This often leads to a higher risk of melanoma, a serious skin cancer. Dysplastic nevi are more likely than ordinary moles to become cancerous. Dysplastic nevi are common, and many people have a few of these abnormal moles. Having more than 50 ordinary moles increases the risk of developing melanoma.
In the overall population, a slight majority of melanomas do "not" form in an existing mole, but rather create a new growth on the skin. Somewhat surprisingly, this also applies to those with dysplastic nevi. They are at a higher risk of melanoma occurring not only where there is an existing mole, but also where there are none. Such persons need to be checked regularly for any changes in their moles and to note any new ones.
Sunlight.
Ultraviolet light from the sun causes premature aging of the skin and skin damage that can lead to melanoma. Some scientists hypothesize that overexposure to UV, including excessive sunlight, may play a role in the formation of acquired moles. However, more research is needed to determine the complex interaction between genetic makeup and overall exposure to ultraviolet light. Some strong indications that this is so (but falling short of proof), are:
Studies have found that sunburns and too much time in the sun can increase the risk factors for melanoma. This is "in addition to" those who have dysplastic nevi being at higher risk of this cancer (the uncertainty is in regard to acquiring "benign" moles). To prevent and reduce the risk of melanoma caused by UV radiation, the American Academy of Dermatology and the National Cancer Institute recommends staying out of the sun between 10 a.m. and 4 p.m. standard time (or whenever one's shadow is shorter than one's height). The National Cancer Institute also recommends wearing long sleeves and trousers, hats with a wide brim, sunscreens, and sunglasses that have UV-deflecting lenses.
Hormones.
Hormonal changes during pregnancy and diabetics (i.e. insulin) are often contributing to mole formation.
Diagnosis.
Clinical diagnosis can be made with the naked eye using the ABCD guideline or by using dermatoscopy. An online-screening test is also available to help screen out benign moles.
Differentiation from melanoma.
It often requires a dermatologist to fully evaluate moles. For instance, a small blue or bluish black spot, often called a blue nevus, is usually benign but often mistaken for melanoma. Conversely, a junctional nevus, which develops at the junction of the dermis and epidermis, is potentially cancerous.
A basic reference chart used for consumers to spot suspicious moles is found in the mnemonic A-B-C-D, used by institutions such as the American Academy of Dermatology and the National Cancer Institute. The letters stand for asymmetry, border, color, and diameter. Sometimes, the letter E (for elevation or evolving) is added. According to the American Academy of Dermatology, if a mole starts changing in size, color, shape or, especially, if the border of a mole develops ragged edges or becomes larger than a pencil eraser, it would be an appropriate time to consult with a physician. Other warning signs include a mole, even if smaller than a pencil eraser, that is different from the others and begins to crust over, bleed, itch, or become inflamed. The changes may indicate developing melanomas. The matter can become clinically complicated because mole removal depends on which types of cancer, if any, come into suspicion.
A recent and novel method of melanoma detection is the "ugly duckling sign" It is simple, easy to teach, and highly effective in detecting melanoma. Simply, correlation of common characteristics of a person's skin lesion is made. Lesions which greatly deviate from the common characteristics are labeled as an "ugly duckling", and further professional exam is required. The "little red riding hood sign", suggests that individuals with fair skin and light colored hair might have difficult-to-diagnose melanomas. Extra care and caution should be rendered when examining such individuals as they might have multiple melanomas and severely dysplastic nevi. A dermatoscope must be used to detect "ugly ducklings", as many melanomas in these individuals resemble non-melanomas or are considered to be "wolves in sheep clothing". These fair skinned individuals often have lightly pigmented or amelanotic melanomas which will not present easy-to-observe color changes and variation in colors. The borders of these amelanotic melanomas are often indistinct, making visual identification without a dermatoscope very difficult.
People with a personal or family history of skin cancer or of dysplastic nevus syndrome (multiple atypical moles) should see a dermatologist at least once a year to be sure they are not developing melanoma.
Complications.
Experts, such as the American Academy of Dermatology, say that vast majority of moles are benign. Nonetheless, the U.S. National Cancer Institute estimated that 62,480 new cases of melanoma and 8,420 related deaths would appear in the United States in the year 2008.
Data on the chances of transformation from melanocytic nevi to melanoma is controversial, but it appears that about 10% of malignant melanomas has a precursor lesion, of which about 10% is menanocytic nevi. Thus it appears that malignant melanoma quite seldomly (1% of cases) has a melanocytic nevi as a precursor.
Management.
First, a diagnosis must be made. If the lesion is a seborrheic keratosis, then shave excision, electrodesiccation or cryosurgery may be performed, usually leaving very little if any scarring. If the lesion is suspected to be a skin cancer, a skin biopsy must be done first, before considering removal. This is unless an excisional biopsy is warranted. If the lesion is a melanocytic nevus, one has to decide if it is medically indicated or not.
If a melanocytic nevus is suspected of being a melanoma, it needs to be sampled or removed and sent for microscopic evaluation by a pathologist by a method called skin biopsy. One can do a complete excisional skin biopsy or a punch skin biopsy, depending on the size and location of the original nevus. Other reasons for removal may be cosmetic, or because a raised mole interferes with daily life (e.g. shaving). Removal can be by excisional biopsy or by shaving. A shaved site leaves a red mark on the site which returns to the patient’s usual skin color in about two weeks. However, there might still be a risk of spread of the melanoma, so the methods of Melanoma diagnosis, including excisional biopsy, are still recommended even in these instances. Additionally, moles can be removed by laser, surgery or electrocautery.
In properly trained hands, some medical lasers are used to remove flat moles level with the surface of the skin, as well as some raised moles. While laser treatment is commonly offered and may require several appointments, other dermatologists think lasers are not the best method for removing moles because the laser only cauterizes or, in certain cases, removes very superficial levels of skin. Moles tend to go deeper into the skin than non-invasive lasers can penetrate. After a laser treatment a scab is formed, which falls off about seven days later, in contrast to surgery, where the wound has to be sutured. A second concern about the laser treatment is that if the lesion is a melanoma, and was misdiagnosed as a benign mole, the procedure might delay diagnosis. If the mole is incompletely removed by the laser, and the pigmented lesion regrows, it might form a recurrent nevus.
Electrocautery is available as an alternative to laser cautery. Electrocautery is a procedure that uses a light electrical current to burn moles, skin tags, and warts off the skin. Electric currents are set to a level such that they only reach the outermost layers of the skin, thus reducing the problem of scarring. Approximately 1-3 treatments may be needed to completely remove a mole. Typically, a local anesthetic is applied to the treated skin area before beginning the mole removal procedure.
For surgery, many dermatologic and plastic surgeons first use a freezing solution, usually liquid nitrogen, on a raised mole and then shave it away with a scalpel. If the surgeon opts for the shaving method, he or she usually also cauterizes the stump. Because a circle is difficult to close with stitches, the incision is usually elliptical or eye-shaped. However, freezing should not be done to a nevus suspected to be a melanoma, as the ice crystals can cause pathological changes called "freezing artifacts" which might interfere with the diagnosis of the melanoma.
Mole removal risks.
Mole removal risks mainly depend on the type of mole removal method the patient undergoes. First, mole removal may be followed by some discomfort that can be relieved with pain medication. Second, there is a risk that a scab will form or that redness will occur. However, such scabs and redness usually heal within one or two weeks. Third, as in other surgeries, there is also risk of infection or an anesthetic allergy or even nerve damage. Lastly, the mole removal may imply an uncomfortable scar depending on the mole size.
History.
In the 1950s and 60s (and, to lesser extent, currently) a facial mole was known as a "beauty mark" when it appeared in certain spots on a woman's face. Examples include Marilyn Monroe, model Cindy Crawford, singer Madonna, and the fictional Ms. Pac-Man. Madonna's facial mole—below her right nostril—has been surgically removed. Some folklore about moles includes the notion that picking at a mole can cause it to become cancerous or grow back larger.
Society and culture.
Throughout human history, individuals who have possessed facial moles have been subject to ridicule and attack based on superstition. Throughout most of history, facial moles were not considered objects of beauty on lovely faces. Rather most moles were considered hideous growths that appeared mostly on the noses, cheeks, and chins of witches, frogs and other low creatures.
Both folklore and modern popular culture use physical traits to denote a character's either good or evil tendencies. In contrast to the fine features and smooth skin of its heroes and heroines, characters who possess negative or evil characteristics have also been known to possess more rugged features and skin blemishes, including facial moles.
In Medieval Europe, among those accused of demonic possession, ecclesiastical edicts interpreted large warts and moles on the skin as physical signs of the entry point of the devil into the soul.
In the 16th century, a popular pseudoscience was invented that supposedly described how every facial mole had a corresponding birthmark somewhere else on the body. Once you knew where both moles were, the theory went, you had the inside track on what made the person tick. For instance, if a man had a mole on the bridge of his nose, he supposedly had another on his right thigh. Taken together, the moles meant the owners were persons of good moods and would eventually receive healthy inheritances.
Face mole reading.
In traditional Chinese culture, facial moles are respected and they are used in moleomancy, or face mole reading. The moles' meaning varies according to which of the nine "wealth spots" of the face they are located in. Depending on their position and color, a person's facial moles may render their face "lucky" or "unlucky."
Moles that can be easily seen may be considered warnings or reminders, while hidden moles may symbolize good luck and fortune. Furthermore, traditional Chinese culture holds that each facial mole indicates the presence of a corresponding mole on another part of the body. For instance, if a mole is present around the mouth, a corresponding mole should be found in the pubic region.

</doc>
<doc id="50525" url="http://en.wikipedia.org/wiki?curid=50525" title="Clarke County">
Clarke County

Clarke County may refer to:
Clarke County was also the official name of Clark County, Washington from 1849 until 1925, when the spelling was changed.

</doc>
<doc id="50526" url="http://en.wikipedia.org/wiki?curid=50526" title="Robert Walpole">
Robert Walpole

Robert Walpole, 1st Earl of Orford, KG, KB, PC (26 August 1676 – 18 March 1745), known before 1742 as Sir Robert Walpole, was a British statesman who is generally regarded as the first Prime Minister of Great Britain. Although the exact dates of his dominance are a matter of scholarly debate, 1721–1742 are often used. He dominated the Walpole–Townshend Ministry and the Ministry and holds the record as the longest serving Prime Minister in British history. Critics called his system the "Robinocracy." Speck says that Walpole's uninterrupted run of 20 years as Prime Minister "is rightly regarded as one of the major feats of British political history... Explanations are usually offered in terms of his expert handling of the political system after 1720, [and] his unique blending of the surviving powers of the crown with the increasing influence of the Commons.
He was a Whig from the gentry class, who was first elected to parliament in 1701, and held many senior positions. He was a country squire and looked to country gentlemen for his political base. Historian Frank O'Gorman says his leadership in Parliament reflected his "reasonable and persuasive oratory, his ability to move both the emotions as well as the minds of men, and, above all, his extraordinary self-confidence." Hoppit says Walpole's policies sought moderation: he worked for peace, lower taxes, growing exports, and allowed a little more tolerance for Protestant Dissenters. He avoided controversy and high-intensity disputes, as his middle way attracted moderates from both the Whig and Tory camps.
Dickinson sums up his historical role:
Early life.
Walpole was born in Houghton, Norfolk, in 1676. One of 19 children, he was the third son and fifth child of Robert Walpole, a member of the local gentry and a Whig politician who represented the borough of Castle Rising in the House of Commons, and his wife Mary Walpole, the daughter and heiress of Sir Geoffrey Burwell of Rougham, Suffolk. Horatio Walpole, 1st Baron Walpole was his younger brother.
Education.
As a child, Walpole attended a private school at Massingham, Norfolk. Walpole entered Eton College in 1690 where he was considered "an excellent scholar". He left Eton on 2 April 1696 and matriculated at King's College, Cambridge on the same day. On 25 May 1698 he left Cambridge after the death of his only remaining elder brother, Edward, so that he could help his father administer the family estate to which he had become the heir. Walpole had planned to become a clergyman but as he was now the eldest surviving son in the family, he abandoned the idea. In November 1700 his father died, and Walpole succeeded to the estate. A paper in his father's handwriting, dated 9 June 1700, shows the family estate in Norfolk and Suffolk to have been nine manors in Norfolk and one in Suffolk.
Early political career.
Walpole's political career began in January 1701 when he won a seat in the general election at Castle Rising. He left Castle Rising in 1702 so that he could represent the neighbouring borough of King's Lynn, a pocket borough that would re-elect him for the remainder of his political career.
Like his father, Robert Walpole was a member of the Whig Party. In 1705, Walpole was appointed by Queen Anne to be a member of the council for her husband, Prince George of Denmark, Lord High Admiral. After having been singled out in a struggle between the Whigs and the government, Walpole became the intermediary for reconciling the government to the Whig leaders. His abilities were recognised by Lord Godolphin (the Lord High Treasurer and leader of the Cabinet) and he was subsequently appointed to the position of Secretary at War in 1708; for a short period of time in 1710 he also simultaneously held the post of Treasurer of the Navy.
Despite his personal clout, however, Walpole could not stop Lord Godolphin and the Whigs from pressing for the prosecution of Henry Sacheverell, a minister who preached anti-Whig sermons. The trial was extremely unpopular with much of the country, causing the Sacheverell riots, and was followed by the downfall of the Duke of Marlborough and the Whig Party in the general election of 1710. The new ministry, under the leadership of the Tory Robert Harley, removed Walpole from his office of Secretary at War but he remained Treasurer of the Navy until 2 January 1711. Harley had first attempted to entice him and then threatened him to join the Tories, but Walpole rejected the offers, instead becoming one of the most outspoken members of the Whig Opposition. He effectively defended Lord Godolphin against Tory attacks in parliamentary debate, as well as in the press.
In 1712, Walpole was accused of venality and corruption in the matter of two forage contracts for Scotland. Although it was proven that he had retained none of the money, Walpole was pronounced "guilty of a high breach of trust and notorious corruption". He was impeached by the House of Commons and found guilty by the House of Lords; he was then imprisoned in the Tower of London for six months and expelled from Parliament. While in the Tower he was regarded as a political martyr, and visited by all the Whig leaders. After he was released, Walpole wrote and published anonymous pamphlets attacking the Harley ministry and assisted Sir Richard Steele in crafting political pamphlets. Walpole was re-elected for King's Lynn in 1713.
Stanhope–Sunderland Ministry.
Queen Anne died in 1714. Under the Act of Settlement 1701, which excluded Roman Catholics from the line of succession, Anne was succeeded by a distant German cousin, George I. George I distrusted the Tories, whom he believed opposed his right to succeed to the Throne. The year of George's accession, 1714, marked the ascendancy of the Whigs who would remain in power for the next fifty years. Robert Walpole became a Privy Councillor and rose to the position of Paymaster of the Forces in a Cabinet nominally led by Lord Halifax, but actually dominated by Lord Townshend (Walpole's brother-in-law) and James Stanhope. Walpole was also appointed chairman of a secret committee formed to investigate the actions of the previous Tory ministry in 1715. Lord Oxford was impeached, and Lord Bolingbroke suffered from an act of attainder.
Lord Halifax, the titular head of the administration, died in 1715. Walpole was appointed to the posts of First Lord of the Treasury and Chancellor of the Exchequer; in this position, he introduced the sinking fund, a device to reduce the national debt. The Cabinet of which he was a member was often divided over most important issues. Normally, Walpole and Lord Townshend were on one side, with Stanhope and Lord Sunderland on the other. Foreign policy was the primary issue of contention; George I was thought to be conducting foreign affairs with the interests of his German territories, rather than those of Great Britain, at heart. The Stanhope–Sunderland faction, however, had the King's support. In 1716 Townshend was removed from the important post of Northern Secretary and put in the lesser office of Lord Lieutenant of Ireland. Even this change did not appease Stanhope and Sunderland who secured the dismissal of Townshend from the Lord-Lieutenancy in April 1717. On the next day, Walpole resigned from the Cabinet to join the Opposition "because I could not connive at some things that were carrying on", and by joining the opposition he did not intend "to make the king uneasy or to embarrass his affairs." In the new Cabinet, Sunderland and Stanhope (who was created an Earl) were the effective heads.
Soon after Walpole's resignation, a bitter family quarrel between the King and the Prince of Wales split the Royal Family. Walpole and others who opposed the Government often congregated at Leicester House, the home of the Prince of Wales, to form political plans. Walpole also became an adviser and close friend of the Prince of Wales's wife, Caroline. In 1720 he improved his position by bringing about a reconciliation between the Prince of Wales and the King.
Walpole continued to be an influential figure in the House of Commons. He was especially active in opposing one of the Government's more significant proposals, the Peerage Bill, which would have limited the power of the monarch to create new peerages. Walpole brought about a temporary abandonment of the bill in 1719 and the outright rejection of the bill by the House of Commons. This defeat led Lord Stanhope and Lord Sunderland to reconcile with their opponents; Walpole returned as Paymaster of the Forces and Townshend was appointed Lord President of the Council. By accepting the position of Paymaster, however, Walpole lost the favour of the Prince of Wales (the future King George II) who still harboured disdain for his father's Government.
Rise to power.
Soon after Walpole returned to the Cabinet, Britain was swept by a wave of over-enthusiastic speculation which led to the South Sea Bubble. The Government had established a plan whereby the South Sea Company would assume the national debt of Great Britain in exchange for lucrative bonds. It was widely believed that the Company would eventually reap an enormous profit through international trade in cloth, agricultural goods, and slaves. Many in the country, including Walpole himself (who sold at the top of the market and made 1,000 percent profit), frenziedly invested in the company. By the latter part of 1720, however, the company had begun to collapse as the price of its shares plunged.. 
In 1721 a committee investigated the scandal, finding that there was corruption on the part of many in the Cabinet. Among those implicated were John Aislabie (the Chancellor of the Exchequer), James Craggs the Elder (the Postmaster General), James Craggs the Younger (the Southern Secretary), and even Lords Stanhope and Sunderland (the heads of the Ministry). Both Craggs the Elder and Craggs the Younger died in disgrace; the remainder were impeached for their corruption. Aislabie was found guilty and imprisoned, but the personal influence of Walpole saved both Stanhope and Sunderland. For his role in preventing these individuals and others from being punished, Walpole gained the nickname of "The Screen", or "Screenmaster-General".
The resignation of Sunderland and the death of Stanhope in 1721 left Walpole as the most important figure in the administration. In April 1721 he was appointed First Lord of the Treasury, Chancellor of the Exchequer and Leader of the House of Commons. Walpole's "de facto" tenure as "Prime Minister" is often dated to his appointment as First Lord of the Treasury in 1721. His brother-in-law Lord Townshend, served as Secretary of State for the Northern Department and controlled the nation's foreign affairs. The two also had to contend with the Secretary of State for the Southern Department, Lord Carteret. Townshend and Walpole were thus restored to power and "annihilated the opposing faction".
Premiership under George I.
Under the guidance of Walpole, Parliament attempted to deal with the financial crisis brought on by the South Sea Bubble. The estates of the directors of the South Sea Company were used to relieve the suffering of the victims, and the stock of the company was divided between the Bank of England and East India Company. The crisis had significantly damaged the credibility of the King and of the Whig Party, but Walpole defended both with skilful oratory in the House of Commons.
Walpole's first year as Prime Minister was also marked by the discovery of a plot formed by Francis Atterbury, the Bishop of Rochester. The exposure of the scheme crushed the hopes of the Jacobites whose previous attempts at rebellion (most notably the risings of 1715 and 1719) had also failed. The Tory Party was equally unfortunate even though Lord Bolingbroke, a Tory leader who fled to France to avoid punishment for his Jacobite sympathies, was permitted to return to Britain in 1723.
During the remainder of George I's reign, Walpole's ascendancy continued; the political power of the monarch was gradually diminishing and that of his ministers gradually increasing. In 1724 the primary political rival of Walpole and Townshend in the Cabinet, Lord Carteret, was dismissed from the post of Southern Secretary and once again appointed to the lesser office of Lord Lieutenant of Ireland. In Ireland, Lord Carteret used his power to secretly aid in the controversy over Wood's Halfpence and support "Drapier's Letters" behind the scenes and cause harm to Walpole's power. Walpole was able to recover from these events by removing the patent. However, Irish sentiment was situated against the English control.
Townshend, working with the king, helped keep Great Britain at peace, especially by negotiating a treaty with France and Prussia in 1725. Walpole was not consulted and stated that Townshend was "too precipitate" in his actions. Great Britain, free from Jacobite threats, from war, and from financial crises, grew prosperous, and Robert Walpole acquired the favour of George I. In 1725 he persuaded the king to revive the Knight of the Bath, and in 1726 a Knight of the Garter, earning him the nickname "Sir Bluestring". Moreover, his eldest son was granted a barony.
Premiership under George II.
Walpole's position was threatened in 1727 when George I died and was succeeded by George II. For a few days it seemed that Walpole would be dismissed but, on the advice of Queen Caroline, the King agreed to keep him in office. Although the King disliked Townshend, he retained him as well. Over the next years Walpole continued to share power with Townshend but the two clashed over British foreign affairs, especially over policy regarding Austria. Gradually Walpole became the clearly dominant partner in government. His colleague retired on 15 May 1730 and this date is sometimes given as the beginning of Walpole's unofficial tenure as Prime Minister. Townsend's departure enabled Walpole to conclude the Treaty of Vienna, creating the Anglo-Austrian alliance.
Opposition.
Walpole, a polarising figure, had many opponents, the most important of whom were in the Country Party, such as Lord Bolingbroke (who had been his political enemy since the days of Queen Anne) and William Pulteney (a capable Whig statesman who felt snubbed when Walpole failed to include him in the Cabinet). Bolingbroke and Pulteney ran a periodical called "The Craftsman" in which they incessantly denounced the Prime Minister's policies. Walpole was also satirised and parodied extensively; he was often compared to the criminal Jonathan Wild as, for example, John Gay did in his farcical "Beggar's Opera". Walpole's other enemies included Jonathan Swift, Alexander Pope, Henry Fielding, and Dr Samuel Johnson.
Support.
Despite such opposition, Walpole secured the support of the people and of the House of Commons with a policy of avoiding war which, in turn, allowed him to impose low taxes. He used his influence to prevent George II from entering a European conflict in 1733 when the War of the Polish Succession broke out. In the same year, however, his influence was seriously threatened by a taxation scheme he introduced. The revenue of the country had been severely diminished by smugglers so Walpole proposed that the tariff on wine and tobacco be replaced by an excise tax. To countervail the threat of smuggling, the tax was to be collected not at ports but at warehouses. This new proposal, however, was extremely unpopular and aroused the opposition of the nation's merchants. Walpole agreed to withdraw the bill before Parliament voted on it, but he dismissed the politicians who had dared to oppose it in the first place. Thus, Walpole lost a considerable element of his Whig Party to the Opposition.
After the general elections of 1734, Walpole's supporters still formed a majority in the House of Commons although they were less numerous than before. Though he maintained his parliamentary supremacy, his popularity began to wane. In 1736 an increase in the tax on gin inspired riots in London. The even more serious Porteous Riots broke out in Edinburgh after the King pardoned a captain of the guard (John Porteous) who had commanded his troops to shoot a group of protesters. Though these events diminished Walpole's popularity, they failed to shake his majority in Parliament. Walpole's domination over the House of Commons was highlighted by the ease with which he secured the rejection of Sir John Barnard's plan to reduce the interest on the national debt. Walpole was also able to persuade Parliament to pass the Licensing Act of 1737 under which London theatres were regulated. The Act revealed a disdain for Swift, Pope, Fielding, and other literary figures who had attacked his government in their works.
While the "country party" attacked Walpole relentlessly, he subsidised writers who spoke up in his behalf. William Arnall and others defended Walpole from the charge of evil political corruption by arguing that corruption is the universal human condition. Furthermore, they argued, political divisiveness was also universal and inevitable because of selfish passions that were integral to human nature. Arnall argued that government must be strong enough to control conflict, and in that regard Walpole was quite successful. This style of "court" political rhetoric continued through the 18th century.
Decline.
The year 1737 saw the death of Walpole's close friend Queen Caroline. Though her death did not end his personal influence with George II, who had grown loyal to the Prime Minister during the preceding years, Walpole's domination of government continued to decline. His opponents acquired a vocal leader in the Prince of Wales who was estranged from his father, the King. Several young politicians including William Pitt the Elder and George Grenville formed a faction known as the "Patriot Boys" and joined the Prince of Wales in opposition.
Walpole's failure to maintain a policy of avoiding military conflict eventually led to his fall from power. Under the Treaty of Seville (1729), Great Britain agreed not to trade with the Spanish colonies in North America. Spain claimed the right to board and search British vessels to ensure compliance with this provision. Disputes, however, broke out over trade with the West Indies. Walpole attempted to prevent war but was opposed by the King, the House of Commons, and by a faction in his own Cabinet. In 1739 Walpole abandoned all efforts to stop the conflict and commenced the War of Jenkins' Ear (so called because Robert Jenkins, a Welsh mariner, claimed that a Spaniard inspecting his vessel had severed his ear).
Walpole's influence continued to dramatically decline even after the war began. In the 1741 general election his supporters secured an increase in votes in constituencies that were decided by mass electorates but failed to win in many pocket boroughs (constituencies subject to the informal but strong influence of patrons). In general the government made gains in England and Wales but this was not enough to overturn the reverses of the 1734 election and further losses in Cornwall where many constituencies were obedient to the will of the Prince of Wales (who was also Duke of Cornwall). These constituencies returned members of parliament hostile to the Prime Minister. Similarly, the influence of the Duke of Argyll secured the election of members opposed to Walpole in some parts of Scotland. Walpole's new majority was difficult to determine because of the uncertain loyalties of many new members, but contemporaries and historians estimated it as low as fourteen to eighteen.
In the new Parliament, many Whigs thought the aging Prime Minister incapable of leading the military campaign. Moreover, his majority was not as strong as it used to be, his detractors—such as William Pulteney, earl of Bath, and Lord Perceval—being approximately as numerous as his supporters. Behind these political enemies were opposition Whigs, Tories and Jacobites. Walpole was alleged to have presided over an immense increase in corruption and to have enriched himself enormously whilst in office. Parliamentary committees were formed to investigate these charges. In 1742 when the House of Commons was prepared to determine the validity of an allegedly rigged by-election in Chippenham, Walpole and others agreed to treat the issue as a Motion of No Confidence. As Walpole was defeated on the vote, he agreed to resign from the Government. The news of the naval disaster against Spain in the Battle of Cartagena de Indias also prompted the end of his political career. King George II wept on his resignation and begged to see him frequently. As part of his resignation the King agreed to elevate him to the House of Lords as the Earl of Orford and this occurred on 6 February 1742. Five days later he formally relinquished the seals of office.
Although no longer First Lord of the Treasury, Walpole remained politically involved. His son, Horace, claimed that his father would "assist the Ministry in the Lords" and remarked that his enemies "cry out that he is still Minister behind the curtain." His former colleagues were still pleased to see him, perhaps in part because he retained the king's favor. After his resignation, his main political roles were to support the government by means of advice, to deal with patronage and to speak on the ministry's behalf in the Lords.
Later years.
Lord Orford was succeeded as Prime Minister by Lord Wilmington in an administration whose true head was Lord Carteret. A committee was created to inquire into Walpole's ministry but no substantial evidence of wrongdoing or corruption was discovered. Though no longer a member of the Cabinet, Lord Orford continued to maintain personal influence with George II and was often dubbed the "Minister behind the Curtain" for this advice and influence. In 1744 he managed to secure the dismissal of Carteret and the appointment of Henry Pelham whom he regarded as a political protégé. He advised Pelham to make use of his seat in the Commons to serve as a bridge between the King and Parliament, just as Walpole had done.
During this time, Walpole also made two interventions in the Lords. The first was in January 1744 in the debate on Hanoverian troops being kept in British pay. Walpole prevented them from losing the troops. In his second intervention, Walpole, with fear of a Jacobite-inspired invasion in February of 1744, made a speech on the situation. Frederick, Prince of Wales, usually hostile to Walpole, warmly received him at his court the next day, most likely because his father's throne, and the future of the whole Hanoverian dynasty, was at risk from the Stuart Pretender.
Along with his political interests in his last years, Walpole enjoyed the pleasures of the hunt. Back at his recently rebuilt country seat in Houghton, Norfolk, such pastimes were denied him due to "dismal weather." He also enjoyed the beauties of the countryside. His art collection gave him particular pleasure. He had spent much money in the 1720s and 1730s in building up a collection of Old Masters from all over Europe. Walpole also concerned himself with estate matters.
His health, never good, deteriorated rapidly toward the end of 1744; Lord Orford died in London in 1745, aged nearly sixty-nine years; he was buried in the parish church of his home estate at Houghton, Norfolk. His earldom passed to his eldest son Robert who was in turn succeeded by his only son George. Upon the death of the third Earl, the Earldom was inherited by the first Earl's younger son Horace Walpole (a writer and friend of poet Thomas Gray), who died without heirs in 1797.
Legacy.
Walpole exercised a tremendous influence on the politics of his day. The Tories became a minor, insignificant faction, and the Whigs became a dominant and largely unopposed party. His influence on the development of the uncodified constitution of Great Britain was less momentous even though he is regarded as Great Britain's first Prime Minister. He relied primarily on the favour of the King rather than on the support of the House of Commons. His power stemmed from his personal influence instead of the influence of his office. Most of his immediate successors were, comparatively speaking, extremely weak; it would take several decades more for the premiership to develop into the most powerful and most important office in the country.
Walpole's strategy of keeping Great Britain at peace contributed greatly to the country's prosperity. Walpole also managed to secure the position of the Hanoverian Dynasty, and effectively countervailed Jacobitism. The Jacobite threat ended, soon after Walpole's term ended, with the defeat of the rebellion of 1745. Later in the century, the Whig MP Edmund Burke "admitted him into the whig pantheon". Burke wrote:
He was an honorable man and a sound Whig. He was not, as the Jacobites and discontented Whigs of his time have represented him, and as ill-informed people still represent him, a prodigal and corrupt minister. They charged him in their libels and seditious conversations as having first reduced corruption to a system. Such was their cant. But he was far from governing by corruption. He governed by party attachments. The charge of systematic corruption is less applicable to him, perhaps, than to any minister who ever served the crown for so great a length of time. He gained over very few from the Opposition. Without being a genius of the first class, he was an intelligent, prudent, and safe minister. He loved peace; and he helped to communicate the same disposition to nations at least as warlike and restless as that in which he had the chief direction of affairs...With many virtues, public and private, he had his faults; but his faults were superficial. A careless, coarse, and over familiar style of discourse, without sufficient regard to persons or occasions, and an almost total want of political decorum, were the errours by which he was most hurt in the public opinion: and those through which his enemies obtained the greatest advantage over him. But justice must be done. The prudence, steadiness, and vigilance of that man, joined to the greatest possible lenity in his character and his politics, preserved the crown to this royal family; and with it, their laws and liberties to this country.
Lord Chesterfield expressed scepticism as to whether "an impartial Character of Sr Robert Walpole, will or can be transmitted to Posterity, for he governed this Kingdom so long that the various passions of Mankind mingled, and in a manner incorporated themselves, with every thing that was said or writt concerning him. Never was Man more flattered nor more abused, and his long power, was probably the chief cause of both". Chesterfield claimed he was "much acquainted with him both in his publick and his private life":
In private life he was good natured, Chearfull, social. Inelegant in his manners, loose in his morals. He had a coarse wit, which he was too free of for a Man in his Station, as it is always inconsistent with dignity. He was very able as a Minister, but without a certain Elevation of mind...He was both the ablest Parliament man, and the ablest manager of a Parliament, that I believe ever lived...Money, not Prerogative, was the chief Engine of his administration, and he employed it with a success that in a manner disgraced humanity...When he found any body proof, against pecuniary temptations, which alass! was but seldom, he had recourse to still a worse art. For he laughed at and ridiculed all notions of Publick virtue, and the love of one's Country, calling them the "Chimerical school boy flights of Classical learning"; declaring himself at the same time, "No Saint, no Spartan, no reformer". He would frequently ask young fellows at their first appearance in the world, while their honest hearts were yet untainted, "well are you to be an old Roman? a Patriot? you will soon come off of that, and grow wiser". And thus he was more dangerous to the morals, than to the libertys of his country, to which I am persuaded that he meaned no ill in his heart...His Name will not be recorded in History among the best men, or the best Ministers, but much much less ought it to be ranked among the worst.
10 Downing Street represents another part of Walpole's legacy. George II offered this home to Walpole as a personal gift in 1732, but Walpole accepted it only as the official residence of the First Lord of the Treasury, taking up his residence there on 22 September 1735. His immediate successors did not always reside in Number 10 (preferring their larger private residences) but the home has nevertheless become established as the official residence of the Prime Minister (in his or her capacity as First Lord of the Treasury).
Walpole has attracted attention from hetrodox economists as a pioneer of protectionist policies, in the form of tariffs and subsidies to woolen manufacturers. As a result the industry became Britain's primary export, enabling the country to import the raw materials and food that fueled the industrial revolution.
Walpole is immortalised in St Stephen's Hall, where he and other notable Parliamentarians look on at visitors to Parliament.
Walpole built as his country seat Houghton Hall in Norfolk.
Walpole also left behind a collection of art which he had assembled during his career. His grandson, the 3rd Earl of Orford, sold many of the works in this collection to the Russian Empress Catherine II in 1779. This collection — then regarded as one of the finest in Europe — now lies in the State Hermitage Museum in Saint Petersburg, Russia. In 2013 the Hermitage loaned the collection to Houghton for display following the original William Kent hanging plan, recently discovered at Houghton.
The nursery rhyme, "Who Killed Cock Robin?", may allude to the fall of Walpole, who carried the popular nickname "Cock Robin".
The town of Walpole, Massachusetts, USA, founded in 1724, takes its name from Sir Robert Walpole.
The town of Orford, New Hampshire, USA, incorporated in 1761, takes its name from Robert Walpole, Earl of Orford.
Walpole Street in Wolverhampton is named after Sir Robert Walpole.
Walpole Island, named for Sir Robert Walpole, comprises an island and an Indian reserve in southwestern Ontario, Canada, on the border between Ontario and Michigan. It lies at the mouth of the St. Clair River on Lake St. Clair, approximately thirty miles (50 km) northeast of Detroit, Michigan, and of Windsor, Ontario.
Marriage and children.
Catherine Shorter.
On 30 July 1700, Walpole married Catherine, daughter of John Shorter of Bybrook in Ashford, Kent. She was described as "a woman of exquisite beauty and accomplished manners". Her £20,000 dowry was, according to her brother-in-law Horatio Walpole, spent on the wedding, christenings and jewels. Together they had two daughters and three sons:
Walpole's first wife Catherine died on 20 August 1737 and was buried in Henry VII Chapel, Westminster Abbey.
Maria Skerritt.
Prior to the death of his first wife, Walpole took on a mistress, Maria, daughter of Thomas Skerrett (died 1734; an Irish merchant living in Dover Street, London). She was a fashionable socialite of wit and beauty, with an independent fortune of £30,000. Walpole had married her by March 1738. They had been living openly together in Richmond Park and Houghton Hall before 1728. Maria had borne him a daughter, also called Maria, who was no longer illegitimate after her parents' marriage and, as the daughter of an Earl, became Lady Maria Walpole. In 1746, this daughter married Colonel Charles Churchill of Chalfont (1720–1812), illegitimate son of General Charles Churchill and became the housekeeper of Windsor Castle. Their daughter Mary became the second wife of Charles Cadogan, 1st Earl Cadogan, and had issue. The second Lady Orford died of a miscarriage on 4 June 1739. Walpole considered her "indispensable to his happiness", and her loss plunged him into a "deplorable and comfortless condition", which ended in a severe illness.

</doc>
<doc id="50527" url="http://en.wikipedia.org/wiki?curid=50527" title="Enchilada">
Enchilada

An enchilada (, ]) is a corn tortilla rolled around a filling and covered with a chili pepper sauce. Enchiladas can be filled with a variety of ingredients, including meat, cheese, beans, potatoes, vegetables, seafood or combinations.
Etymology.
The Real Academia Española defines the word "enchilada", as used in Mexico, as a rolled maize tortilla stuffed with meat and covered with a tomato and chili sauce. "Enchilada" is the past participle of Spanish "enchilar", "to add chili pepper to", literally to "season (or decorate) with chili".
When used in an idiom, the "whole enchilada" means the whole thing.
History.
Enchiladas originated in Mexico, where the practice of rolling tortillas around other food dates back at least to Maya times. The people living in the lake region of the Valley of Mexico traditionally ate corn tortillas folded or rolled around small fish. Writing at the time of the Spanish conquistadors, Bernal Díaz del Castillo documented a feast enjoyed by Europeans hosted by Hernán Cortés in Coyoacán, which included foods served in corn tortillas. (Note that the native Nahuatl name for the flat corn bread used was "tlaxcalli"; the Spanish give it the name "tortilla".) The Nahuatl word for enchilada is "chīllapītzalli" which is formed of the Nahuatl word for "chili", "chīlli" and the Nahuatl word for "flute", "tlapītzalli" . In the 19th century, as Mexican cuisine was being memorialized, enchiladas were mentioned in the first Mexican cookbook, "El cocinero mexicano" ("The Mexican Chef"), published in 1831, and in Mariano Galvan Rivera's "Diccionario de Cocina", published in 1845. An early mention, in English, is a 1914 recipe found in "California Mexican-Spanish Cookbook", by Bertha Haffner Ginger.
Varieties.
In their original form as Mexican street food, enchiladas were simply corn tortillas dipped in chili sauce and eaten without fillings. There are now many varieties, which are distinguished primarily by their sauces, fillings and, in one instance, by their form. Various adjectives may be used to describe the recipe content or origin, e.g. "enchilada tapatia" would be a recipe from Jalisco.
Varieties include:
Fillings, toppings and garnishes.
Fillings include meat, such as chicken, beef or pork, seafood, cheese, potatoes, vegetables, and any combination of these. Enchiladas are commonly topped or garnished with cheese, sour cream, lettuce, olives, chopped onions, chili peppers, salsa, or fresh cilantro.

</doc>
<doc id="50528" url="http://en.wikipedia.org/wiki?curid=50528" title="Dutchess County, New York">
Dutchess County, New York

Dutchess County is a county located in the south east corner of the U.S. state of New York. As of the 2010 census, the population was 297,488. The county seat is Poughkeepsie. The county was created in 1683 and later organized in 1713.
Dutchess County is part of the New York-Newark-Jersey City, NY-NJ-PA Metropolitan Statistical Area. It is located in the Mid-Hudson Region of the Hudson Valley.
History.
Prior to Anglo-Dutch settlement, what is today Dutchess County was a leading center for the native Wappinger peoples. They had their council-fire at what is now present-day Fishkill Hook, and also held gatherings along the Danskammer. On November 1, 1683, the Province of New York established its first twelve counties, with Dutchess County being one of them. Its boundaries at that time included the present Putnam County, and a small portion of the present Columbia County (the towns of Clermont and Germantown). The county was named for Mary of Modena, Duchess of York, second wife of James, Duke of York (later James II, King of England). "Dutchess" is an archaic form of duchess.
The Province of New York and the Connecticut Colony negotiated an agreement on November 28, 1683, establishing their border as 20 mi east of the Hudson River, north to Massachusetts. The 61660 acre east of the Byram River making up the Connecticut Panhandle were granted to Connecticut, in recognition of the wishes of the residents. In exchange, Rye was granted to New York, along with a 1.81 mi wide strip of land running north from Ridgefield to Massachusetts alongside the New York counties of Westchester, Putnam then Dutchess, known as "The Oblong". The eastern half of the stub of land in northeast Dutchess County containing Rudd Pond and Taconic State Park is the northernmost extension of The Oblong.
Until 1713, Dutchess was administered by Ulster County. On October 23, 1713 Queen Anne gave permission for Dutchess County to elect its own officers from among their own population including a Supervisor, Tax Collector, Tax Assessor and Treasurer. In 2013, Dutchess County celebrated its 300th anniversary of democracy based upon a legislative resolution sponsored by County Legislator Michael Kelsey from Salt Point. In 1812, Putnam County was detached from Dutchess.
The patents.
In the twelve years 1685–1697 lawful patents had been granted securing for their purchasers every foot of Hudson River shoreline in the original county. Three additional patents, to 1706, laid claim to the remaining interior lands.
Early settlement.
From 1683 to 1715 most of the settlers in Dutchess County were Dutch. Many of these moved in from Albany and Ulster Counties. They settled along the Fishkill River and in the areas that are now Poughkeepsie and Rhinebeck.
From 1715 to 1730 most of the new settlers in Dutchess county were Germans. From 1730 until 1775 New Englanders were the main new settlers in Dutchess County.
20th century.
Franklin D. Roosevelt lived in his family home in Hyde Park, overlooking the Hudson River. His family's home is now the Home of Franklin D. Roosevelt National Historic Site, managed by the National Park Service.
Prior to the 1960s, Dutchess County was primarily agricultural. Since then the southwestern part (from Poughkeepsie south and From the Taconic State Parkway westward) of the county has developed into a largely residential area, suburban in character, with many of its residents commuting to jobs in New York City. The northern and eastern regions of the county remain rural with large farmlands but at the same time developed residences used during the summer and or on weekends by people living in the New York City urban area.
Geography.
According to the U.S. Census Bureau, the county has a total area of 825 sqmi, of which 796 sqmi is land and 30 sqmi (3.6%) is water.
Dutchess County is located in southeastern New York State, between the Hudson River on its west and the New York-Connecticut border on its east, about halfway between the cities of Albany and New York. It contains two cities: Beacon and Poughkeepsie.
The terrain of the county is mostly hilly, especially in the Hudson Highlands in the southwestern corner and the Taconic Mountains to the northeast. Some areas nearer the river are flatter.
The highest point in the county is the summit of Brace Mountain, in the Taconics, at 2,311 feet (704 m) above sea level. The lowest point is sea level, along the Hudson River.
Almost a half mile long border exists with Berkshire County, Massachusetts in the extreme northern end of the county.
Demographics.
As of the census of 2000, there were 280,150 people, 99,536 households, and 69,177 families residing in the county. The population density was 350 people per square mile (135/km²). There were 106,103 housing units at an average density of 132 per square mile (51/km²). The racial makeup of the county was 83.66% White (80.3% non-Hispanic whites), 9.32% Black or African American, 0.22% Native American, 2.52% Asian, 0.03% Pacific Islander, 2.37% from other races, and 1.89% from two or more races. 6.45% were Hispanic or Latino of any race. 22.0% were of Italian, 16.9% Irish, 11.3% German and 6.7% English ancestry according to Census 2000. 88.3% spoke English and 4.8% Spanish
Based on the Census Ancestry tallies, including people who listed more than one ancestry, Italians were the largest group in Dutchess County with 60,645. Irish came in a very close second at 59,991. In third place were the 44,915 Germans who barely exceeded the 44,078 people not in the 105 specifically delineated ancestry groups.
There were 99,536 households out of which 34.50% had children under the age of 18 living with them, 55.50% were married couples living together, 10.30% had a female householder with no husband present, and 30.50% were non-families. 24.60% of all households were made up of individuals and 9.00% had someone living alone who was 65 years of age or older. The average household size was 2.63 and the average family size was 3.16.
In the county the population was spread out with 25.10% under the age of 18, 9.40% from 18 to 24, 30.20% from 25 to 44, 23.20% from 45 to 64, and 12.00% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 100.10 males. For every 100 females age 18 and over, there were 98.20 males.
The median income for a household in the county was $53,086, and the median income for a family was $63,254. Males had a median income of $45,576 versus $30,706 for females. The per capita income for the county was $23,940. About 5.00% of families and 7.50% of the population were below the poverty line, including 8.50% of those under age 18 and 6.50% of those age 65 or over.
The per capita income and average home values have increased noticeably in recent years mainly due to affluent residents relocating from nearby and expensive Westchester County, NY. In recent years, there has been a large influx of people that have relocated from New York City, mainly from the Borough of The Bronx.
The decrease in population between 1810 and 1820 was due the separation of Putnam County from Dutchess in 1812.
Government.
The county is governed via a county executive and a county legislature. The county legislature consists of 25 members each elected from single member districts.
Law enforcement.
The Cities of Beacon and Poughkeepsie; Towns of Fishkill, Hyde Park, Pine Plains, Poughkeepsie, Rhinebeck, Red Hook, and East Fishkill and Villages of Millerton, Wappingers Falls, Millbrook, and Fishkill have their own Police departments. The remainder of the county is patrolled by the Dutchess County Sheriff's Office and New York State Police. The New York State Police Troop K headquarters is located in Millbrook.
Elections.
The current composition of the County Legislature is 15 Republicans, 8 Democrats, 1 Conservative and 1 Green. The current county executive is Republican Marcus Molinaro. The county executive is elected in a countywide vote. The majority of the county is located in New York's 19th congressional district, which is currently being represented by Republican Chris Gibson.
Dutchess County has historically leaned Republican due to its affluence and large suburban swaths; it has voted for Democratic presidential candidates only four times - 1964, 1996, 2008 and 2012. The largely suburban southern towns of Dutchess tend to be more conservative, while the small villages and rural areas of the northern tier have become somewhat more liberal. Almost all elected officials are Republican, but in the 2008 Presidential Election, Barack Obama carried Dutchess with 54% of the vote. The previous election saw George W. Bush win with 51%. This gives Dutchess a Cook PVI of EVEN.
Transportation.
Railroads.
Amtrak has stations in Rhinecliff, a small hamlet in the Town of Rhinebeck, and Poughkeepsie, with both stations being served by Empire Service trains as well as other trains that run along the line. The latter station is the terminus of the Hudson Line of the Metro-North Railroad. The Hudson Line also has station stops in New Hamburg (a hamlet of the town of Poughkeepsie) and Beacon.
The Harlem Line, on the eastern side of the county, has station stops in Pawling, Wingdale, Dover Plains, and two stops in Wassaic (one along the Tenmile River and the other the namesake terminus of that line).
Buses.
Public transportation in Dutchess County is handled by the Dutchess County Department of Mass Transit, branded publicly as the LOOP system. Outside of the urbanized area of the county, most service is limited. The City of Poughkeepsie operates its own limited system as well. Privately run lines connect Poughkeepsie to New Paltz and Beacon to Newburgh.
For intercity bus service, Adirondack Trailways and Short Line Bus also operate some service through Poughkeepsie, Rhinebeck, and the southern part of the county. The last time service ran outside that area was in the late-1990s when Peter Pan/Bonanza ran service to New York City in the eastern part of the county.
Air.
The Dutchess County Airport, located in the town of Wappinger, is a general aviation facility which once had commercial service. The closest commercial airport, Stewart International Airport, is located across the Hudson River in Newburgh.
Also located in the county is Sky Park Airport, a public use general aviation facility in Red Hook, New York.
Culture.
Dutchess County Chamber of Commerce holds an annual hot air balloon launch typically in the first week of July. The main launch sites are along the Hudson River. As many as 20 balloons participate in the event.
Sports.
The Hudson Valley Renegades are a minor league baseball team affiliated with the Tampa Bay Rays. The team is a member of the New York - Penn League, and play at Dutchess Stadium in Fishkill.
The Hudson Valley Bears are one of four founding members of the Eastern Professional Hockey League (EPHL). They play their home games at the Mid-Hudson Civic Center in Poughkeepsie.
The Hudson Valley Hawks is a team in the newly formed National Professional Basketball League. The team's home court is at Beacon High School, in Beacon.
Communities.
N.B.: Cities, Towns and Villages are official political designations.

</doc>
<doc id="50529" url="http://en.wikipedia.org/wiki?curid=50529" title="Columbia County">
Columbia County

Columbia County is the name of eight counties in the United States:

</doc>
<doc id="50530" url="http://en.wikipedia.org/wiki?curid=50530" title="Mole (animal)">
Mole (animal)

Moles are small mammals adapted to a subterranean lifestyle. They have cylindrical bodies, velvety fur, very small, inconspicuous ears and eyes, reduced hindlimbs and short, powerful forelimbs with large paws adapted for digging. The term "mole" is especially and most properly used for "true moles" of the Talpidae family in the order Soricomorpha found in most parts of North America, Asia, and Europe although may refer to other completely unrelated mammals of Australia and southern Africa which have also evolved the mole body plan; it is not commonly used for some talpids, such as desmans and shrew-moles, which do not quite fit the common definition of "mole".
Terminology.
By the era of Early Modern English, the mole was also known in English as "mouldywarp", a word having cognates in other Germanic languages such as German ("Maulwurf"), and Danish, Norwegian, Swedish and Icelandic ("muldvarp", "mullvad", "moldvarpa"), where the "muld/mull/mold" part of the word means soil and the "varp/vad/varpa" part means throw, hence "one who throws soil" or "dirt tosser".
Male moles are called "boars", females are called "sows". A group of moles is called a "labour".
Characteristics.
Breathing underground.
Moles have been found to tolerate higher levels of carbon dioxide than other mammals, because their blood cells have a special and unique haemoglobin protein. Moles are able to reuse the oxygen inhaled when above ground, and as a result, are able to survive in low-oxygen environments such as underground burrows.
Extra thumb.
Moles have polydactyl forepaws; each has an extra thumb (also known as a prepollex) next to the regular thumb. While the mole's other digits have multiple joints, the prepollex has a single, sickle-shaped bone which develops later and differently from the other fingers during embryogenesis from a transformed sesamoid bone in the wrist, independently evolved but similar to the giant panda thumb. This supernumerary digit is species-specific, as it is not present in shrews, the mole's closest relatives. Androgenic steroids are known to affect the growth and formation of bones, and a connection is possible between this species-specific trait and the "male" genitals apparatus in female moles of many mole species (gonads with testicular and ovary tissues).
Diet.
A mole's diet primarily consists of earthworms and other small invertebrates found in the soil, and a variety of nuts. The mole runs are in reality 'worm traps', the mole sensing when a worm falls into the tunnel and quickly running along to kill and eat it. Because their saliva contains a toxin that can paralyze earthworms, moles are able to store their still-living prey for later consumption. They construct special underground "larders" for just this purpose; researchers have discovered such larders with over a thousand earthworms in them. Before eating earthworms, moles pull them between their squeezed paws to force the collected earth and dirt out of the worm's gut.
The star-nosed mole can detect, catch and eat food faster than the human eye can follow. 
Breeding.
Breeding season for a mole depends on species but is generally February through May. Males search for females by letting out high-pitched squeals and tunneling through foreign areas.
The gestation period of the Eastern (US) mole (Scalopus aquaticus) is approximately 42 days. Three to five young are born, mainly in March and early April.
Townsend moles mate in February and March, and the 2–4 young are born in March and April after a gestation period of about 1 month. The Townsend mole is endangered in the United States and Canada.
Coast moles produce a litter of 2–5 pups between March and April.
Pups leave the nest 30–45 days after birth to find territories of their own.
Social structure.
Moles are solitary creatures, coming together only to mate. Territories may overlap, but moles avoid each other and males may fight fiercely if they meet.
Classification.
The family Talpidae contains all the true moles and some of their close relatives. Desmans, which are Talpidae but are not normally called "moles", are not shown below, but belong to the subfamily Talpinae (note the slightly different name). Those species called "shrew moles" represent an intermediate form between the moles and their shrew ancestors, and as such may not be fully described by the article.
On the other hand, there is no monophyletic relation between the mole and the hedgehog, both of which were previously placed in the now abandoned order Insectivora. As a result, Soricomorpha ("shrew-like animals" including moles), previously within Insectivora, has been elevated to the level of an order.
Other "moles".
While many groups of burrowing animals (pink fairy armadillos, tuco-tucos, mole rats, mole crickets and mole crabs) have developed close physical similarities with moles due to convergent evolution, two of these are so similar to true moles, they are commonly called and thought of as "moles" in common English, although they are completely unrelated to true moles or to each other. These are the golden moles of southern Africa and the marsupial moles of Australia. While difficult to distinguish from each other, they are most easily distinguished from true moles by shovel-like patches on their noses which they use in tandem with their abbreviated forepaws to swim through sandy soils.
The golden moles.
The golden moles belong to the same branch on the tree of life as the tenrecs, called Tenrecomorpha or Afrosoricida, which in turn stem from a main branch of placental mammals called the Afrosoricida. This means they share a closer common ancestor with such existing afrosoricids as elephants, manatees, and aardvarks than they do with other placental mammals such as true Talpidae moles.
Marsupial moles.
As marsupials, these moles are even more distantly related to true Talpidae moles than golden moles, both of which belong to the eutheria, or placental mammals. This means they are more closely related to such existing Australian marsupials as kangaroos or koalas, and even to a lesser extent to American marsupials, such as opossums than they are to placental mammals such as golden or Talpidae moles.
Class Mammalia
Interaction with humans.
Pelts.
Moles' pelts have a velvety texture not found in surface animals. Surface-dwelling animals tend to have longer fur with a natural tendency for the nap to lie in a particular direction, but to facilitate their burrowing lifestyle, mole pelts are short and very dense and have no particular direction to the nap. This makes it easy for moles to move backwards underground, as their fur is not "brushed the wrong way". The leather is extremely soft and supple. Queen Alexandra, the wife of Edward VII of the United Kingdom, ordered a mole-fur garment to start a fashion that would create a demand for mole fur, thereby turning what had been a serious pest problem in Scotland into a lucrative industry for the country. Hundreds of pelts are cut into rectangles and sewn together to make a coat. The natural color is taupe, but it is readily dyed any color.
Pest status.
Moles are considered to be agricultural pests in some countries, while in others, such as Germany, they are a protected species, but may be killed if a permit is received. Problems cited as caused by moles include contamination of silage with soil particles, making it unpalatable to livestock, the covering of pasture with fresh soil reducing its size and yield, damage to agricultural machinery by the exposure of stones, damage to young plants through disturbance of the soil, weed invasion of pasture through exposure of freshly tilled soil, and damage to drainage systems and watercourses. Other species such as weasels and voles may use mole tunnels to gain access to enclosed areas or plant roots.
Moles burrow lawns, raising molehills, and killing the lawn, for which they are sometimes considered pests. They can undermine plant roots, indirectly causing damage or death. Moles do not eat plant roots.
Moles are controlled with traps such as mole-catchers, smoke bombs, and poisons such as calcium carbide. Strychnine was also used for this purpose in the past. The most common method now is Phostoxin or Talunex tablets. They contain aluminium phosphide and are inserted in the mole tunnels, where they turn into phosphine gas (not be confused with phosgene gas). More recently, high-grade nitrogen gas has proven effective at killing moles, with the added advantage of having no polluting effect to the environment.
Other common defensive measures include cat litter and blood meal, to repel the mole, or flooding or smoking its burrow. Devices are also sold to trap the mole in its burrow, when one sees the "mole hill" moving and therefore knows where the animal is, and then stabbing it. Humane traps which capture the mole alive so it may be transported elsewhere are also options.
However, in many gardens, the damage caused by moles to lawns is mostly visual, and it is also possible to simply remove the earth of the molehills as they appear, leaving their permanent galleries for the moles to continue their existence underground.However, when the tunnels are near the surface, they collapse when the ground is soft after heavy rain and leave unsightly furrows in the lawn.
Meat.
Although the mole can be eaten, the taste is said to be deeply unpleasant.

</doc>
<doc id="50531" url="http://en.wikipedia.org/wiki?curid=50531" title="Monroe County">
Monroe County

Monroe County is the name of seventeen counties in the United States, all named after James Monroe, fifth President of the United States:

</doc>
<doc id="50533" url="http://en.wikipedia.org/wiki?curid=50533" title="Madison County">
Madison County

Madison County is the name of nineteen counties and one parish in the United States, most of which are named for James Madison, fourth President of the United States. It may refer to:

</doc>
<doc id="50534" url="http://en.wikipedia.org/wiki?curid=50534" title="Caroline of Ansbach">
Caroline of Ansbach

Wilhelmina Charlotte Caroline of Brandenburg-Ansbach (1 March 1683 – 20 November 1737), commonly known as Caroline of Ansbach, was queen of Great Britain as the wife of King George II.
Her father, Margrave John Frederick of Brandenburg-Ansbach, belonged to a branch of the House of Hohenzollern and was the ruler of a small German state, the Principality of Ansbach. Caroline was orphaned at a young age and moved to the enlightened court of her guardians, King Frederick I and Queen Sophia Charlotte of Prussia. At the Prussian court, her previously limited education was widened, and she adopted the liberal outlook possessed by Sophia Charlotte, who became her good friend and whose views influenced Caroline all her life.
As a young woman, Caroline was much sought-after as a bride. After rejecting the suit of the nominal King of Spain, Archduke Charles of Austria, she married George Augustus, the third-in-line to the British throne and heir apparent to the Electorate of Hanover. They had eight children, seven of whom grew to adulthood. 
Caroline moved permanently to Britain in 1714 when her husband became Prince of Wales. As Princess of Wales, she joined her husband in rallying political opposition to his father King George I. In 1717, her husband was expelled from court after a family row. Caroline came to be associated with Robert Walpole, an opposition politician who was a former government minister. Walpole rejoined the government in 1720, and Caroline's husband and King George I reconciled publicly, on Walpole's advice. Over the next few years, Walpole rose to become the leading minister.
Caroline succeeded as queen and electress consort in 1727, when her husband became King George II. Her eldest son, Frederick, became Prince of Wales. He was a focus for the opposition, like his father before him, and Caroline's relationship with him was strained. As princess and as queen, Caroline was known for her political influence, which she exercised through and for Walpole. Her tenure included four regencies during her husband's stays in Hanover, and she is credited with strengthening the Hanoverian dynasty's place in Britain during a period of political instability. Caroline was widely mourned following her death in 1737, not only by the public but also by the King, who refused to remarry.
Early life.
Caroline was born on 1 March 1683 at Ansbach, the daughter of John Frederick, Margrave of Brandenburg-Ansbach, and his second wife, Princess Eleonore Erdmuthe of Saxe-Eisenach. Her father was the ruler of one of the smallest German states; he died of smallpox at the age of 32, when Caroline was three years old. Caroline and her only full sibling, her younger brother Margrave William Frederick, left Ansbach with their mother, who returned to her native Eisenach.
In 1692, Caroline's widowed mother was pushed into an unhappy marriage with the Elector of Saxony, and she and her two children moved to the Saxon court at Dresden. Eleonore Erdmuthe was widowed again two years later, after her unfaithful husband contracted smallpox from his mistress. Eleonore remained in Saxony for another two years, until her death in 1696. The orphaned Caroline and William Frederick returned to Ansbach to stay with their elder half-brother, Margrave George Frederick II. George Frederick was a youth with little interest in parenting a girl, and so Caroline soon moved to Lützenburg outside Berlin, where she entered into the care of her new guardians, Frederick, Elector of Brandenburg, and his wife, Sophia Charlotte, who had been a friend of Eleonore Erdmuthe.
Education.
Frederick and Sophia Charlotte became king and queen of Prussia in 1701. The queen was the daughter of Dowager Electress Sophia of Hanover, and the sister of George, Elector of Hanover. She was renowned for her intelligence and strong character, and her uncensored and liberal court attracted a great many scholars, including philosopher Gottfried Leibniz. Caroline was exposed to a lively intellectual environment quite different from anything she had experienced previously. Before she began her education under Sophia Charlotte's care, Caroline had received little formal education; her handwriting remained poor throughout her life. With her lively mind, Caroline developed into a scholar of considerable ability. She and Sophia Charlotte developed a strong relationship in which Caroline was treated as a surrogate daughter; the queen once declared Berlin was "a desert" without Caroline whenever she left temporarily for Ansbach.
Marriage.
An intelligent and attractive woman, Caroline was much sought-after as a bride. Dowager Electress Sophia called her "the most agreeable Princess in Germany". She was considered for the hand of Archduke Charles of Austria, who was a candidate for the throne of Spain and later became Holy Roman Emperor. Charles made official overtures to her in 1703, and the match was encouraged by King Frederick of Prussia. After some consideration, Caroline refused in 1704, as she would not convert from Lutheranism to Catholicism. Early in the following year, Queen Sophia Charlotte died on a visit to her native Hanover. Caroline was devastated, writing to Leibniz, "The calamity has overwhelmed me with grief and sickness, and it is only the hope that I may soon follow her that consoles me."
In June 1705, Queen Sophia Charlotte's nephew, George Augustus, the electoral prince of Hanover, visited the Ansbach court, supposedly incognito, to inspect Caroline, as his father the Elector did not want his son to enter into a loveless arranged marriage as he himself had. The nephew of three childless uncles, George Augustus was under pressure to marry and father an heir to prevent endangering the Hanoverian succession. He had heard reports of Caroline's "incomparable beauty and mental attributes". He immediately took a liking to her "good character" and the British envoy reported that George Augustus "would not think of anybody else after her". For her part, Caroline was not fooled by the prince's disguise, and found her suitor attractive. He was the heir apparent of his father's Electorate of Hanover and third-in-line to the British throne of his distant cousin Queen Anne, after his grandmother Dowager Electress Sophia and his father the Elector.
On 22 August 1705, Caroline arrived in Hanover for her wedding to George Augustus; they were married that evening in the palace chapel at Herrenhausen. By May of the following year, Caroline was pregnant, and her first child Prince Frederick was born on 20 January 1707. A few months after the birth, in July, Caroline fell seriously ill with smallpox followed by pneumonia. Her baby was kept away from her, but George Augustus remained at her side devotedly, and caught and survived the infection himself. Over the next seven years, Caroline had three more children, Anne, Amelia and Caroline, all of whom were born in Hanover.
George Augustus and Caroline had a successful and loving marriage, though he continued to keep mistresses, as was customary for the time. Caroline was well aware of his infidelities, as they were well known and he told her about them. His two best-known mistresses were Henrietta Howard, later Countess of Suffolk, and, from 1735, Amalie von Wallmoden, Countess of Yarmouth. Howard was one of Caroline's Women of the Bedchamber and became Mistress of the Robes when her husband inherited a peerage in 1731; she retired in 1734. In contrast with her mother-in-law and husband, Caroline was known for her marital fidelity; she never made any embarrassing scenes nor did she take lovers. She preferred her husband's mistresses to be ladies-in-waiting, as that way she believed she could keep a closer eye on them.
The succession of her husband's family to the British throne was still insecure, as Queen Anne's half-brother James Stuart contested the Hanoverian claim, and Queen Anne and Caroline's grandmother-in-law Dowager Electress Sophia had fallen out. Anne refused permission for any of the Hanoverians to visit Britain in her lifetime. Caroline wrote to Leibniz, "I accept the comparison which you draw, though all too flattering, between me and Queen Elizabeth as a good omen. Like Elizabeth, the Electress's rights are denied her by a jealous sister [Queen Anne], and she will never be sure of the English crown until her accession to the throne." In June 1714, Dowager Electress Sophia died in Caroline's arms at the age of 84, and Caroline's father-in-law became heir presumptive to Queen Anne. Just weeks later, Anne died and the Elector of Hanover was proclaimed as her successor, becoming George I of Great Britain.
Princess of Wales.
George Augustus sailed to England in September 1714, and Caroline and two of her daughters followed in October. Her journey across the North Sea from The Hague to Margate was the only sea voyage she took in her life. Their young son, Prince Frederick, remained in Hanover for the rest of George I's reign to be brought up by private tutors.
On the accession of George I in 1714, Caroline's husband automatically became Duke of Cornwall and Duke of Rothesay. Shortly afterwards, he was invested as Prince of Wales, whereupon she became Princess of Wales. Caroline was the first woman to receive the title at the same time as her husband received his. She was the first Princess of Wales for over two hundred years, the last one being Catherine of Aragon. As George I had repudiated his wife Sophia Dorothea of Celle in 1694 prior to his becoming King of Great Britain, there was no queen consort, and Caroline was the highest-ranking woman in the kingdom. George Augustus and Caroline made a concerted effort to "anglicise" by acquiring knowledge of England's language, people, politics and customs. Two separate courts developed with strong contrasts; the old king's had German courtiers and government ministers, while the Wales's court attracted English nobles out of favour with the King, and was considerably more popular with the British people. Political opposition to the King gradually became centered around George Augustus and Caroline.
Two years after their arrival in England, Caroline suffered a stillbirth, which her friend the Countess of Bückeburg blamed on the incompetence of English doctors, but the following year she had another son, Prince George William. At the baptism in November 1717, her husband fell out with his father over the choice of godparents, leading to the couple's placement under house arrest at St. James's Palace prior to their banishment from court. Caroline was originally allowed to stay with their children, but refused as she believed her place was with her husband. She and her husband moved into Leicester House, while their children remained in the care of the King. Caroline fell sick with worry, and fainted during a secret visit to her children made without the King's approval. By January, the King had relented and allowed Caroline unrestricted access. In February, Prince George William fell ill, and the King allowed both George Augustus and Caroline to see him at Kensington Palace without any conditions. When the baby died, a post-mortem was conducted to prove that the cause of death was disease (a polyp on the heart) rather than the separation from his mother. Further tragedy occurred in 1718, when Caroline miscarried at Richmond Lodge, her country residence. Over the next few years, Caroline had three more children: William, Mary and Louise.
Leicester House became a frequent meeting place for the ministry's political opponents. Caroline struck up a friendship with politician Sir Robert Walpole, a former minister in the Whig government who led a disgruntled faction of the party. In April 1720, Walpole's wing of the Whig party reconciled with the governing wing, and Walpole and Caroline helped to effect a reconciliation between the King and her husband for the sake of public unity. Caroline wanted to regain her three eldest daughters, who remained in the care of the King, and thought the reconciliation would lead to their return, but negotiations came to nothing. George Augustus came to believe that Walpole had tricked him into the reconciliation as part of a scheme to gain power. The prince was isolated politically when Walpole's Whigs joined the government, and Leicester House played host to literary figures and wits, such as John Arbuthnot and Jonathan Swift, rather than politicians. Arbuthnot told Swift that Caroline had enjoyed his "Gulliver's Travels", particularly the tale of the crown prince who wore one high-heel and one low-heel in a country where the King and his party wore low heels, and the opposition wore high ones: a barely veiled reference to the political leanings of the Prince of Wales.
Caroline's intellect far outstripped her husband's, and she read avidly. She established an extensive library at St. James's Palace. As a young woman, she corresponded with Gottfried Leibniz, the intellectual colossus who was courtier and factotum to the House of Hanover. She later facilitated the Leibniz-Clarke correspondence, arguably the most important philosophy of physics discussion of the 18th century. She helped to popularise the practice of variolation (an early type of immunisation), which had been witnessed by Lady Mary Wortley Montagu and Charles Maitland in Constantinople. At the direction of Caroline, six condemned prisoners were offered the chance to undergo variolation instead of execution: they all survived, as did six orphan children given the same treatment as a further test. Convinced of its medical value, Caroline had her children Amelia, Caroline and Frederick inoculated against smallpox in the same manner. In praising her support for smallpox inoculation, Voltaire wrote of her, "I must say that despite all her titles and crowns, this princess was born to encourage the arts and the well-being of mankind; even on the throne she is a benevolent philosopher; and she has never lost an opportunity to learn or to manifest her generosity."
Queen and regent.
Caroline became queen consort on the death of her father-in-law in 1727, and she was crowned alongside her husband at Westminster Abbey on 11 October that year. She was the first queen consort to be crowned since Anne of Denmark in 1603. Though George II denounced Walpole as a "rogue and rascal" over the terms of the reconciliation with his father, Caroline advised her husband to retain Walpole as the leading minister. Walpole commanded a substantial majority in Parliament and George II had little choice but to accept him or risk ministerial instability. Walpole secured a civil list payment of £100,000 a year for Caroline, and she was given both Somerset House and Richmond Lodge. Courtier Lord Hervey called Walpole "the Queen's minister" in recognition of their close relationship. For the next ten years, Caroline had immense influence. She persuaded the King to adopt policies at the behest of Walpole, and persuaded Walpole against taking inflammatory actions. Caroline had absorbed the liberal opinions of her mentor, Queen Sophia Charlotte of Prussia, and supported clemency for the Jacobites (supporters of the rival Stuart claim to the throne), freedom of the press, and freedom of speech in Parliament.
Over the next few years, she and her husband fought a constant battle against their eldest son, Frederick, Prince of Wales, who had been left behind in Germany when they came to England. He joined the family in 1728, by which time he was an adult, had mistresses and debts, and was fond of gambling and practical jokes. He opposed his father's political beliefs, and complained of his lack of influence in government. The Regency Act 1728 made Caroline rather than Frederick regent when her husband was in Hanover for five months from May 1729. During her regency, a diplomatic incident with Portugal (where a British ship had been seized on the Tagus) was defused, and the negotiation of the Treaty of Seville between Britain and Spain was concluded. From May 1732, she was regent for four months while George II was again away in Hanover. An investigation into the penal system uncovered widespread abuses, including cruel treatment and conspiracy in the escape of wealthy convicts. Caroline pressed Walpole for reform, largely unsuccessfully. In March 1733, Walpole introduced an unpopular excise bill to parliament, which the Queen supported, but it gathered such strong opposition that it was eventually dropped.
Caroline's entire life in Britain was spent in the South-East of England in or around London. As queen, she continued to surround herself with artists, writers and intellectuals. She collected jewellery, especially cameos and intaglios, acquired important portraits and miniatures, and enjoyed the visual arts. She commissioned works such as terracotta busts of the kings and queens of England from Michael Rysbrack, and supervised a more naturalistic design of the royal gardens by William Kent and Charles Bridgeman. In 1728, she rediscovered sets of sketches by Leonardo da Vinci and Hans Holbein that had been hidden in a drawer since the reign of William III.
Caroline's eldest daughter Anne married William IV of Orange in 1734, and moved with her husband to the Netherlands. Caroline wrote to her daughter of her "indescribable" sadness at the parting. Anne soon felt homesick, and travelled back to England when her husband went on campaign. Eventually, her husband and father commanded her to return to Holland.
Final years.
In mid-1735, Frederick, Prince of Wales, was further dismayed when Caroline, rather than himself, again acted as regent while the King was absent in Hanover. The King and Queen arranged Frederick's marriage, in 1736, to Princess Augusta of Saxe-Gotha. Shortly after the wedding, George went to Hanover, and Caroline resumed her role as "Protector of the Realm". As regent, Caroline considered the reprieve of Captain John Porteous, who had been convicted of murder in Edinburgh. Before she could act, a mob stormed the jail where he was held and killed him. Caroline was appalled. The King's absences abroad were leading to unpopularity, and in late 1736 he made plans to return, but his ship was caught in poor weather, and it was rumoured that he had been lost at sea. Caroline was devastated, and disgusted by the insensitivity of her son, who hosted a grand dinner while the gale was blowing. During her regency, the Prince of Wales attempted to start a number of quarrels with his mother, whom he saw as a useful proxy to irritate the King. George eventually returned in January 1737.
Frederick applied to Parliament unsuccessfully for an increased financial allowance that had hitherto been denied him by the King, and public disagreement over the money drove a further wedge between parents and son. On the advice of Walpole, Frederick's allowance was raised in an attempt to mitigate further conflict, but by less than he had asked. In June 1737, Frederick informed his parents that Augusta was pregnant, and due to give birth in October. In fact, Augusta's due date was earlier and a peculiar episode followed in July in which the prince, on discovering that his wife had gone into labour, sneaked her out of Hampton Court Palace in the middle of the night, to ensure that the King and Queen could not be present at the birth. George and Caroline were horrified. Traditionally, royal births were witnessed by members of the family and senior courtiers to guard against supposititious children, and Augusta had been forced by her husband to ride in a rattling carriage for an hour and a half while heavily pregnant and in pain. With a party including two of her daughters and Lord Hervey, the Queen raced over to St. James's Palace, where Frederick had taken Augusta. Caroline was relieved to discover that Augusta had given birth to a "poor, ugly little she-mouse" rather than a "large, fat, healthy boy" as the pitiful nature of the baby made a supposititious child unlikely. The circumstances of the birth deepened the estrangement between mother and son. According to Lord Hervey, she once remarked after seeing Frederick, "Look, there he goes—that wretch!—that villain!—I wish the ground would open this moment and sink the monster to the lowest hole in hell!"
In the final years of her life, Caroline was troubled by gout in her feet, but more seriously she had suffered an umbilical hernia at the birth of her final child in 1724. On 9 November 1737, she felt an intense pain and, after struggling through a formal reception, took to her bed. Her womb had ruptured. Over the next few days she was bled, purged, and operated on, without anaesthetic, but there was no improvement in her condition. The King refused Frederick permission to see his mother, a decision with which she complied; she sent her son a message of forgiveness through Walpole. She asked her husband to remarry after her death, which he rejected saying he would take only mistresses; she replied "Ah, mon Dieu, cela n'empêche pas" ("My God, that doesn't prevent it"). On 17 November, her strangulated bowel burst. She died on 20 November 1737 at St. James's Palace.
She was buried in Westminster Abbey on 17 December. Frederick was not invited to the funeral. George Frideric Handel composed an anthem for the occasion, "The Ways of Zion Do Mourn / Funeral Anthem for Queen Caroline". The King arranged for a pair of matching coffins with removable sides, so that when he followed her to the grave (23 years later), they could lie together again.
Legacy.
Caroline was widely mourned. The Protestants lauded her moral example, and even the Jacobites acknowledged her compassion, and her intervention on the side of mercy for their compatriots. During her lifetime her refusal to convert when offered the hand of Archduke Charles was used to portray her as a strong adherent to Protestantism. For example, John Gay wrote of Caroline in "A Letter to A Lady" (1714):
She was widely seen by both the public and the court as having great influence over her husband. A satirical verse of the period went:
The memoirs of the eighteenth century, particularly those of John, Lord Hervey, fed perceptions that Caroline and Walpole governed her husband. Peter Quennell wrote that Hervey was the "chronicler of this remarkable coalition" and that she was Hervey's "heroine". Using such sources, biographers of the nineteenth and twentieth centuries credit her with aiding the establishment of the House of Hanover in Britain, in the face of Jacobite opposition. R. L. Arkell wrote "by her acumen and geniality, [Caroline] ensured the dynasty's rooting itself in England", and W. H. Wilkins said her "gracious and dignified personality, her lofty ideals and pure life did much to counteract the unpopularity of her husband and father-in-law, and redeem the early Georgian era from utter grossness." Although modern historians tend to believe that Hervey, Wilkins and Arkell have overestimated her importance, it is nevertheless probable that Caroline of Ansbach was one of the most influential consorts in British history.
Titles, styles, honours and arms.
Honours.
Caroline County in the British Colony of Virginia was named in her honour when it was formed in 1727.
Arms.
The royal coat of arms of the United Kingdom are impaled with those of her father, John Frederick, Margrave of Brandenburg-Ansbach. The arms of her father were quarterly of fifteen, 1st, per fess gules and argent, within a bordure counter-changed of the same (for Magdeburg); 2nd, argent, an eagle displayed sable, crowned or; 3rd, or, a griffin segreant gules, crowned; 4th and 5th, argent, a griffin segreant gules; 6th, or, a griffin segreant sable; 7th, argent, an eagle displayed sable (for Crossen); 8th, per pale argent and gules within a bordure counter-changed of the same (for Halberstadt); 9th, argent, an eagle displayed sable; 10th, or, a lion rampant sable, crowned, within a bordure goboné argent and gules (for Nuremberg); 11th, gules, two keys in saltire or (for Minden); 12th, quarterly argent and sable (for Hohenzollern); 13th, the field gules, the figure argent; 14th, per fess gules and argent; 15th, plain field of gules (for right of regalia); overall an inescutcheon, argent, an eagle displayed gules (for Brandenburg).
Issue.
Caroline's ten pregnancies resulted in eight live births. One of her children died in infancy, and seven lived to adulthood.

</doc>
<doc id="50535" url="http://en.wikipedia.org/wiki?curid=50535" title="Warren County">
Warren County

Warren County is the name of fourteen counties in the USA. They are named after General Joseph Warren, who was killed in the Battle of Bunker Hill in the American Revolutionary War:

</doc>
<doc id="50538" url="http://en.wikipedia.org/wiki?curid=50538" title="Eugene V. Debs">
Eugene V. Debs

Eugene Victor "Gene" Debs (November 5, 1855 – October 20, 1926) was an American union leader, one of the founding members of the Industrial Workers of the World (IWW or the Wobblies), and five times the candidate of the Socialist Party of America for President of the United States. Through his presidential candidacies, as well as his work with labor movements, Debs eventually became one of the best-known socialists living in the United States.
Early in his political career, Debs was a member of the Democratic Party. He was elected as a Democrat to the Indiana General Assembly in 1884. After working with several smaller unions, including the Brotherhood of Locomotive Firemen, Debs was instrumental in the founding of the American Railway Union (ARU), one of the nation's first industrial unions. After workers at the Pullman Palace Car Company organized a wildcat strike over pay cuts in the summer of 1894, Debs signed many into the ARU. He called a boycott of the ARU against handling trains with Pullman cars, in what became the nationwide Pullman Strike, affecting most lines west of Detroit, and more than 250,000 workers in 27 states. To keep the mail running, President Grover Cleveland used the United States Army to break the strike. As a leader of the ARU, Debs was convicted of federal charges for defying a court injunction against the strike and served six months in prison.
In prison, Debs read the works of Karl Kautsky, a chief expositor of Karl Marx, and learned about socialism. Upon his release, he launched his career as the nation's most prominent Socialist in the first decades of the 20th century. He ran as the Socialist Party's candidate for the presidency in 1900 (earning 0.63% of the popular vote), 1904 (2.98%), 1908 (2.83%), 1912 (5.99%), and 1920 (3.41%), the last time from a prison cell.
Debs was noted for his oratory, and his speech denouncing American participation in World War I led to his second arrest in 1918. He was convicted under the Espionage Act of 1917 and sentenced to a term of 10 years. President Warren G. Harding commuted his sentence in December 1921. Debs died in 1926, not long after being admitted to a sanatorium.
Biography.
Early life.
Eugene Debs was born on November 5, 1855, in Terre Haute, Indiana, to Jean Daniel and Marguerite Mari Bettrich Debs, who both immigrated to the United States from Colmar, Alsace, France. His father, who came from a prosperous family in France, owned a textile mill and meat market. Debs was named after the French authors Eugene Sue and Victor Hugo.
Debs attended public school, dropping out of high school at age 14. He took a job in the Vandalia railroad car shops, first working as a painter and a car cleaner. In December 1871 he left the railroad yards for work on the railways as a locomotive fireman for the same company.
In July 1875 he left to work at a wholesale grocery house, where he remained for the next four years, attending a local business school at night.
Debs had joined the Brotherhood of Locomotive Firemen (BLF) in February 1875 and became active in this fraternal benefit organization. In 1877 he served as a delegate of the Terre Haute lodge to the organization's national convention. Debs was elected associate editor of the BLF's monthly organ, "Firemen's Magazine", in 1878. Two years later, he was appointed Grand Secretary and Treasurer of the BLF and editor of the magazine in July 1880. He worked as a BLF functionary until January 1893 and as the magazine's editor until September 1894.
At the same time he became a prominent figure in the community. He served two terms as Terre Haute's city clerk from September 1879 to September 1883. In the fall of 1884 he was elected to the Indiana General Assembly as a Democrat, serving for one term.
Marriage and family.
Eugene Debs married Kate Metzel on June 9, 1885. Their home still stands in Terre Haute, preserved on the campus of Indiana State University.
Labor activism.
The railroad brotherhoods were comparatively conservative organizations, focused on providing fellowship and services rather than on collective bargaining. Debs gradually became convinced of the need for a more unified and confrontational approach as railroads were powerful companies in the economy. One influence was his involvement in the Burlington Railroad Strike of 1888, a defeat for labor that convinced Debs of the necessity of organizing along craft lines.
After stepping down as Brotherhood Grand Secretary in 1893, Debs organized one of the first industrial unions in the United States, the American Railway Union (ARU), for unskilled workers. The Union successfully struck the Great Northern Railway in April 1894, winning most of its demands.
Pullman Strike.
In 1894 Debs became involved in the Pullman Strike, which grew out of a compensation dispute started by the workers who constructed the train cars made by the Pullman Palace Car Company. The Pullman Company, citing falling revenue after the economic Panic of 1893, had cut the wages of its employees by 28%. The workers, many of whom were already members of the American Railway Union, appealed for support to the union at its convention in Chicago, Illinois. Debs tried to persuade the Union members who worked on the railways that the boycott was too risky, given the hostility of both the railways and the federal government, the weakness of the union, and the possibility that other unions would break the strike. The membership ignored his warnings and refused to handle Pullman cars or any other railroad cars attached to them, including cars containing U.S. Mail. After A.R.U. Board Director Martin J. Elliot extended the strike to St. Louis, doubling its size to 80,000 workers, Debs relented and decided to take part in the strike, which was now endorsed by almost all members of the ARU in the immediate area of Chicago. On July 9, 1894, a "New York Times" editorial called Debs "a lawbreaker at large, an enemy of the human race." Strikers fought by establishing boycotts of Pullman train cars, and with Debs' eventual leadership, the strike came to be known as "Debs' Rebellion".
The U.S. federal government intervened, obtaining an injunction against the strike on the theory that the strikers had obstructed the U.S. Mail, carried on Pullman cars, by refusing to show up for work. President Grover Cleveland sent the United States Army to enforce the injunction. The entrance of the Army was enough to break the strike; 13 strikers were killed, and thousands were blacklisted. An estimated $80 million worth of property was damaged, and Debs was found guilty of contempt of court for violating the injunction and sent to federal prison.
Debs was represented by Clarence Darrow, hitherto a corporate lawyer for the railroad company, who "switched sides" to represent Debs. Darrow, a leading American lawyer and civil libertarian, had resigned his corporate position in order to represent Debs, making a substantial financial sacrifice in order to do so. A Supreme Court case decision, "In re Debs", later upheld the right of the federal government to issue the injunction.
Socialist leader.
At the time of his arrest for mail obstruction, Debs was not yet a socialist. While serving his six-month term in the jail at Woodstock, Illinois, Debs and his ARU comrades received a steady stream of letters, books, and pamphlets in the mail from socialists around the country.
Debs recalled several years later:
"...I began to read and think and dissect the anatomy of the system in which workingmen, however organized, could be shattered and battered and splintered at a single stroke. The writings of Bellamy and Blatchford early appealed to me. "The Cooperative Commonwealth" of Gronlund also impressed me, but the writings of Kautsky were so clear and conclusive that I readily grasped, not merely his argument, but also caught the spirit of his socialist utterance – and I thank him and all who helped me out of darkness into light."
Additionally, Debs was visited in jail by Milwaukee socialist newspaper editor Victor L. Berger, who, in Debs' words, "came to Woodstock, as if a providential instrument, and delivered the first impassioned message of Socialism I had ever heard." In his 1926 obituary in "Time", it was said that Berger left him a copy of Das Kapital and "prisoner Debs read it slowly, eagerly, ravenously." Debs emerged from jail at the end of his sentence a changed man. He would spend the final three decades of his life proselytizing for the socialist cause.
After Debs' release from prison in 1895, he started his Socialist political career. Debs persuaded the American Railway Union membership to join with the Brotherhood of the Cooperative Commonwealth to found the Social Democracy of America. Debs, along with Elliott, were the first federal office candidates for the fledgling Socialist party, running (unsuccessfully) for US president and Congress in 1900. Along with his running mate Job Harriman, Debs received 87,945 votes – 0.6% of the popular vote – and no electoral votes.
Debs' wife Kate was opposed to Debs' socialism. The "tempestuous relationship with a wife who rejects the very values he holds most dear" was the basis of Irving Stone's biographical novel "Adversary in the House".
Split to found the Social Democratic Party.
One year later this group split and Debs went with the majority faction to found the Social Democratic Party of the United States, also called the Social Democratic Party. Debs was elected chairman of the Executive Board of the National Council, the board which governed the party. But the party did not have a sole figure that governed its actions, Debs' position as chairman and his notoriety gave him the status of party figurehead. He was the Socialist Party of America candidate for president in 1904, 1908, 1912, and 1920 (the final time from prison).
In his showing in the 1904 election, Debs received 402,810 votes, which was 2.98% of the popular vote. Debs received no electoral votes, and, with vice presidential candidate Benjamin Hanford, ultimately finished third overall. In the 1908 election, Debs again ran on the same ticket as Benjamin Hanford. While receiving a slightly higher number of votes in the popular vote, 420,852, he received 2.83% of the popular vote. Again Debs received no electoral votes. Debs received 5.99% of the popular vote (a total of 901,551 votes) in 1912, while his total of 913,693 votes in the 1920 campaign remains the all-time high for a Socialist Party candidate. Running alongside Emil Seidel, Debs again received no electoral votes.
Although he received some success as a third-party candidate, Debs was largely dismissive of the electoral process; he distrusted the political bargains that Victor Berger and other "Sewer Socialists" had made in winning local offices. He put much more value on organizing workers into unions, favoring unions that brought together all workers in a given industry over those organized by the craft skills workers practiced.
Founding the IWW.
After his work with the Brotherhood of Locomotive Firemen and the American Railway Union, Debs' next major work in organizing a labor union came during the founding of the Industrial Workers of the World (IWW). On June 27, 1905, in Chicago, Illinois, Debs and other influential union leaders including Big Bill Haywood, leader of the Western Federation of Miners, and Daniel De León, leader of the Socialist Labor Party, held what Haywood called the "Continental Congress of the working class". Haywood stated: "We are here to confederate the workers of this country into a working class movement that shall have for its purpose the emancipation of the working class...", and for Debs: "We are here to perform a task so great that it appeals to our best thought, our united energies, and will enlist our most loyal support; a task in the presence of which weak men might falter and despair, but from which it is impossible to shrink without betraying the working class."
Socialists split with the IWW.
Although the IWW was built on the basis of uniting workers of industry, a rift began between the union and the Socialist Party. It started when the electoral wing of the Socialist Party, led by Victor Berger and Morris Hillquit, became irritated with speeches by Haywood. In December 1911, Haywood told a Lower East Side audience at New York's Cooper Union that parliamentary Socialists were "step-at-a-time people whose every step is just a little shorter than the preceding step." It was better, Haywood said, to "elect the superintendent of some branch of industry, than to elect some congressman to the United States Congress." In response, Hillquit attacked the IWW as "purely anarchistic..."
The Cooper Union speech was the beginning of a split between Bill Haywood and the Socialist Party, leading to the split between the factions of the IWW, one faction loyal to the Socialist Party, and the other to Haywood. The rift presented a problem for Debs, who was influential in both the IWW and the Socialist Party. The final straw between Haywood and the Socialist Party came during the Lawrence Textile Strike when, disgusted with the decision of the elected officials in Lawrence, Massachusetts, to send police who subsequently used their clubs on children, Haywood publicly declared that "I will not vote again" until such a circumstance was rectified. Haywood was purged from the National Executive Committee by passage of an amendment that focused on the direct action and sabotage tactics advocated by the IWW. Debs was probably the only person who could have saved Haywood's seat.
In 1906, when Haywood had been on trial for his life in Idaho, Debs had described him as "the Lincoln of Labor" and called for Haywood to run against Theodore Roosevelt for President of the United States, but times had changed and Debs, facing a split in the Party, chose to echo Hillquit's words, accusing the IWW of representing anarchy. Debs thereafter stated that he had opposed the amendment, but that once it was adopted it should be obeyed. Debs remained friendly to Haywood and the IWW after the expulsion, despite their perceived differences over IWW tactics.
Prior to Haywood's dismissal, the Socialist Party membership had reached an all-time high of 135,000. One year later, four months after Haywood was recalled, the membership dropped to 80,000. The reformists in the Socialist Party attributed the decline to the departure of the "Haywood element", and predicted that the party would recover. It did not; in the election of 1912 many of the Socialists who had been elected to public office lost their seats.
Leadership style.
Debs was noted by many to be a charismatic speaker who sometimes called on the vocabulary of Christianity and much of the oratorical style of evangelism – even though he was generally disdainful of organized religion. It has been said, that Debs was what every socialist and radical should be; fierce in his convictions, but kind and compassionate in his personal relations. As Heywood Broun noted in his eulogy for Debs, quoting a fellow Socialist: "That old man with the burning eyes actually believes that there can be such a thing as the brotherhood of man. And that's not the funniest part of it. As long as he's around I believe it myself."
Although sometimes called "King Debs", Debs himself was not wholly comfortable with his standing as a leader. As he told an audience in Detroit in 1906:
I am not a Labor Leader; I do not want you to follow me or anyone else; if you are looking for a Moses to lead you out of this capitalist wilderness, you will stay right where you are. I would not lead you into the promised land if I could, because if I led you in, some one else would lead you out. You must use your heads as well as your hands, and get yourself out of your present condition.
Arrest and imprisonment.
Debs' speeches against the Wilson administration and the war earned the enmity of President Woodrow Wilson, who later called Debs a "traitor to his country." On June 16, 1918, Debs made a speech in Canton, Ohio, urging resistance to the military draft of World War I. He was arrested on June 30 and charged with ten counts of sedition.
His trial defense called no witnesses, asking that Debs be allowed to address the court in his defense. That unusual request was granted, and Debs spoke for two hours. He was found guilty on September 12. At his sentencing hearing on September 14, he again addressed the court, and his speech has become a classic. Heywood Broun, a liberal journalist and not a Debs partisan, said it was "one of the most beautiful and moving passages in the English language. He was for that one afternoon touched with inspiration. If anyone told me that tongues of fire danced upon his shoulders as he spoke, I would believe it."
Debs said in part:
Debs was sentenced on November 18, 1918, to ten years in prison. He was also disenfranchised for life. Debs presented what has been called his best-remembered statement at his sentencing hearing:
Debs appealed his conviction to the Supreme Court. In its ruling on "Debs v. United States", the court examined several statements Debs had made regarding World War I and socialism. While Debs had carefully worded his speeches in an attempt to comply with the Espionage Act, the Court found he had the intention and effect of obstructing the draft and military recruitment. Among other things, the Court cited Debs' praise for those imprisoned for obstructing the draft. Justice Oliver Wendell Holmes, Jr. stated in his opinion that little attention was needed since Debs' case was essentially the same as that of "Schenck v. United States", in which the Court had upheld a similar conviction.
Debs went to prison on April 13, 1919. In protest of his jailing, Charles Ruthenberg led a parade of unionists, socialists, anarchists and communists to march on May 1 (May Day) 1919, in Cleveland, Ohio. The event quickly broke into the violent May Day Riots of 1919.
Debs ran for president in the 1920 election while in prison in Atlanta, Georgia, at the Atlanta Federal Penitentiary. He received 919,799 write-in votes (3.4%), slightly less than he had won in 1912, when he received 6%, the highest number of votes for a Socialist Party presidential candidate in the U.S. During his time in prison, Debs wrote a series of columns deeply critical of the prison system. They appeared in sanitized form in the Bell Syndicate and were published in his only book, "Walls and Bars", with several added chapters. It was published posthumously.
In March 1919, President Wilson asked Attorney General A. Mitchell Palmer for his opinion on clemency, offering his own: "I doubt the wisdom and public effect of such an action." Palmer generally favored releasing people convicted under the wartime security acts, but when he consulted with Debs' prosecutors – even those with records as defenders of civil liberties – they assured him that Debs' conviction was correct and his sentence appropriate. The President and his Attorney General both believed that public opinion opposed clemency and that releasing Debs could strengthen Wilson's opponents in the debate over the ratification of the peace treaty. Palmer proposed clemency in August and October 1920 without success.
At one point, Wilson wrote: 
"While the flower of American youth was pouring out its blood to vindicate the cause of civilization, this man, Debs, stood behind the lines sniping, attacking, and denouncing them...This man was a traitor to his country and he will never be pardoned during my administration."
In January 1921, Palmer, citing Debs' deteriorating health, proposed to Wilson that Debs receive a presidential pardon freeing him on February 12, Lincoln's birthday. Wilson returned the paperwork after writing "Denied" across it.
On December 23, 1921, President Harding commuted Debs' sentence to time served, effective Christmas Day. He did not issue a pardon. A White House statement summarized the administration's view of Debs' case: 
"There is no question of his guilt...He was by no means as rabid and outspoken in his expressions as many others, and but for his prominence and the resulting far-reaching effect of his words, very probably might not have received the sentence he did. He is an old man, not strong physically. He is a man of much personal charm and impressive personality, which qualifications make him a dangerous man calculated to mislead the unthinking and affording excuse for those with criminal intent."
Last years.
When Debs was released from the Atlanta Penitentiary, the other prisoners sent him off with "a roar of cheers" and a crowd of 50,000 greeted his return to Terre Haute to the accompaniment of band music. En route home, Debs was warmly received at the White House by Harding, who greeted him by saying: "Well, I've heard so damned much about you, Mr. Debs, that I am now glad to meet you personally."
In 1924, Debs was nominated for the Nobel Peace Prize by the Finnish Socialist Karl H. Wiik on the grounds that "Debs started to work actively for peace during World War I, mainly because he considered the war to be in the interest of capitalism." In late 1926, Debs was admitted to Lindlahr Sanitarium in Elmhurst, Illinois. He died there of heart failure on October 20, 1926, at the age of 70. His body was cremated and buried in Highland Lawn Cemetery.
Legacy.
Eugene Debs helped motivate the American Left as a measure of political opposition to corporations and World War I. American socialists, communists, and anarchists honor his compassion for the labor movement and motivation to have the average working man build socialism without large state involvement. Several books have been written about his life as an inspirational American socialist.
On May 22, 1962, Debs' home was purchased by the Eugene V. Debs Foundation for $9,500, which worked to preserve it as a Debs memorial was begun. In 1965 it was designated as an official historic site of the state of Indiana, and in 1966 it was designated as a National Historic Landmark of the United States. The preservation of the museum is monitored by the National Park Service. In 1990, the U.S. Department of Labor named Debs a member of its Labor Hall of Fame.
While Debs did not leave a collection of papers to a university library, the pamphlet collection which he and his brother amassed is held by Indiana State University in Terre Haute. The scholar Bernard Brommel, author of a 1978 biography of Debs, has donated his biographical research materials to the Newberry Library in Chicago, where they are open to researchers. The original manuscript of Debs' book "Walls and Bars," with handwritten amendments, presumably by Debs, is held in the Thomas J. Morgan Papers in the Special Collections department of the University of Chicago Library. 
The town of Debs, Minnesota is named after Debs.
Former New York radio station WEVD (now ESPN radio) was named in his honor.
"Debs Place", a housing block in Co-op City in the Bronx, New York, was named in his honor.
The Eugene V. Debs Inter-Cooperative House at the University of Michigan in Ann Arbor, Michigan was named after Debs.
There are at least two beers named after Debs - "Debs' Red Ale" and "Eugene".

</doc>
<doc id="50542" url="http://en.wikipedia.org/wiki?curid=50542" title="James Rouse">
James Rouse

James Wilson Rouse (April 26, 1914 – April 9, 1996), founder of The Rouse Company, was a pioneering American real estate developer, urban planner, civic activist, and later, free enterprise-based philanthropist.
Youth, education, early career.
James "Jim" Rouse was born in Easton, Maryland, the son of Lydia Agnes (née Robinson) and Willard Goldsmith Rouse, a canned-foods broker. His father, a lawyer trained at Johns Hopkins University, once ran as the state's attorney for Harford County. When he lost, the Rouse family moved from Bel Air, Maryland to Easton. Rouse grew up in Easton (then population: 5,000) on a well-to-do street on the edge of town. He was taught at home by his mother until second grade, when he was transferred to a public school (which found him to be well prepared enough to skip a grade). In 1930 at the age of 16, in his senior year of high school, Rouse faced significant life changes: he lost his father to cancer of the bladder and mother to heart failure, and childhood home on Brooklett's Avenue to bank foreclosure. Rouse was funded by his brother Bill to attend the private preparatory Tome School in Port Deposit, Maryland for a year.
Facing money problems and unable to continue at the Tome School, the Rouse family sought a way for him to attend college by appealing to his oldest sister, who had married a Navy officer stationed in Hawaii. Rouse declared himself his sister's dependent and, with Navy connections now secured, was thereby able to attend the University of Hawaii at a greatly reduced cost. Despite the exotic lure of the islands, Rouse missed Maryland. Again, the Rouse family found a scholarship for Jim at the University of Virginia. He declared his major as political science and waited tables at a local boarding house. Not being able to cover the gap between his scholarship and his remaining expenses, he proposed leaving Charlottesville and moving to Baltimore to try to make it on his own.
In 1933, Rouse arrived in Baltimore in search of opportunity. He was fortunate to find a job parking cars at the St. Paul Garage. He later remarked that he got the job even though he could not drive. He convinced his foreman to teach him rather than fire him. He worked in the garage one year. In May 1935 Rouse wrote Millard Tydings who found him a position with the Federal Housing Administration as a clerk specializing in completing FHA loans to eastern Maryland banks. Although he had only two years of undergraduate college on his transcript, in the 1930s that was enough to qualify for law school. He borrowed money in March 1936 from Guy Hollyday who was a loan officer with the Title Guarantee and Trust Company seeking FHA loan guarantees and attended classes three nights a week at the University of Maryland law school. He was hired at age 22 by his mentor Hollyday.
Rouse graduated in 1937 and in 1939 left the FHA and became partner with Hunter Moss at a mortgage banking firm called the Moss-Rouse Company funded by a $20,000 loan from Moss's sister, which would eventually become the Rouse Company. The company would specialize in FHA backed loans, and hired Churchill G. Carey from Connecticut General, with his former company providing loan capitol to Moss-Rouse. Both Moss and Rouse served during WWII, with Moss joining the Marines and Rouse the Navy. Rouse was able to defer duty while his wife was pregnant, shipping out to Hawaii to work on John Henry Towers staff on 4 July 1942. Rouse returned from the war to put his gambling assets to work with Moss. In 1952, Moss would leave the firm after Rouse brought in his brother Bill as vice president. Rouse would make new connections that year crossing party lines as a board member of "Democrats for Eisenhower".
After World War II he co-founded the Citizens Housing and Planning Association and became involved in Baltimore, Maryland's efforts to rehabilitate its decayed housing stock for profit through The Baltimore Plan. The national publicity of this program led to his participation in Dwight D. Eisenhower's National Housing Task Force starting in 1953. He introduced (or at least helped popularize) the term "urban renewal" to describe the series of recommendations made by that task force.
Shopping malls.
In 1958, Rouse built Harundale Mall in Glen Burnie, Maryland, the first enclosed shopping center east of the Mississippi River and the first built by a real estate developer. His company used the term "mall" to describe the development, which was an alternative to the more typical strip malls usually built in the suburbs (the "mall" in "strip mall" came into usage later, after the enclosed mall had been popularized by Rouse's company). Although many now attribute the rise of the shopping mall to the decline of the American downtown core, Rouse's focus at the time was on the introduction of malls as a form of town center for the suburbs.
His company became an active developer and manager of shopping center and mall properties, even as he shifted focus to new projects which eventually included planned communities and festival marketplaces.
Harundale Mall has been replaced by Harundale Plaza. In 1999, the mall reopened and redeveloped as Harundale Plaza, a strip shopping center. Stores include A.J. Wright, a Super Fresh supermarket, Outback Steakhouse, Hollywood Video, Burlington Coat Factory, and a U.S. Post Office, along with several other typical strip-mall stores. The signature "rock" from Harundale Mall is now at Harundale Plaza.
Planned communities.
In the 1960s Rouse turned his focus to planned communities. After engaging in a planning exercise for the Pocatinco Hills estate of the Rockefellers, Rouse constructed his first planned residential development: the Village of Cross Keys in Baltimore. On June 16, 1961, Rouse bought 68 acre inside the city from the Baltimore Country Club for $25,000 an acre. Rouse excitedly proclaimed that this undertaking “will be the largest, and potentially most important development in the history of Baltimore.” Rouse hoped that he could bring to the residential field “some of the fresh thinking, good taste and high standards which we believe have marked our shopping center developments.”
Familiar with bad housing in Baltimore and Washington, D.C., Rouse now had an opportunity to demonstrate what housing within a city’s borders could be like. “There is a real need for residential development,” he said, “in which there is a strong sense of community; a need to feed into the city some of the atmosphere and pace of the small town and village; a need to create a community which can meet as many as possible of the needs of the people who live there; which can bring these people into natural contact with one another; which can produce out of these relationships a spirit and feeling of neighborliness and a rich sense of belonging to a community.” In a city that practiced strict racial segregation, Rouse intended Cross Keys to be open to all who could afford to live there. The development was a mixture of townhouses, garden apartments, a high-rise apartment house designed by Frank Gehry, stores grouped around a village square, and an office complex. By 1970, the Village of Cross Keys had become among the most desirable places to live in the Baltimore area.
While Cross Keys was still under construction, Rouse decided to build a whole new "city." The creation of Columbia, Maryland, between Baltimore and Washington, D.C., was the greatest adventure of Rouse’s life. Columbia was the ultimate opportunity: the chance to embody his ideals in a whole new city. For the undertaking that would become Columbia, Rouse turned to his partner in previous projects, the Connecticut General Life Insurance Company ("CG"). At a meeting at company headquarters in Hartford, Rouse made his pitch to CG’s top real estate and mortgage people and the company’s chairman of the board, Frazar B. Wilde. The questioning was mostly negative, until Wilde joined in. He expressed the view that CG couldn’t lose. If Rouse’s project did not succeed, the land could always be sold, and probably for a higher price than what it cost.
The land for the new city would be owned by a subsidiary called Howard Research and Development Corporation. CG would own half of that corporation and Rouse’s corporation the other half. Rouse would be responsible for the management of the acquired land and for preparing a master plan for development. CG also put up some of the money for Columbia’s infrastructure. The rest was supplied by Teachers Insurance and Annuity Association and the Chase Manhattan Bank.
By the end of the summer of 1963 close to 14000 acre of Howard County farmland had been acquired, and the time was at hand to begin planning what to do with it. Rouse wanted to hear from a wide assortment of experts and scholars. He brought together an assemblage which became known as "the Work Group." It consisted of top people in health, family life, education, recreation, government, transportation, and employment. Ultimately emerging was the idea that the new city should be a real multi-faceted city, not a bedroom suburb. It should be possible for its residents to find everything they needed right there—jobs, education, recreation, health care, and any other necessity.
Rouse was not reluctant to bring up his home town of Easton as a model for Columbia. Consensus formed around the idea that the basic subdivision within the new city should be the village, a unit of from 10,000-15,000 people. This number was thought to be the most likely to foster a local feeling of identification: for merchants to get to know their customers, ministers their memberships, and teachers their pupils and parents.
Within the city, there would be 12 villages. Each village would have a central gathering place where people of different income levels and types of housing would cross paths and mix. Each village would have a middle school and a high school, a teen center, a supermarket, a library, a hospital, an auditorium, offices, restaurants, some specialty shops, and a few larger recreational facilities. It also would have a multi-denominational house of worship known as an "interfaith center" based on the Gordon Cosby's Ecumenical Church of the Savior called the Kittamaqundi Community. The hope was that one building would be used by several religions. 
In addition to the villages there would be a core area that would function as the new city’s “downtown.” In it would be the central office of the city’s governing body, the Columbia Association. Here would be the main cluster of retail stores (arranged as a mall), a hotel and conference center, a hospital, movie theaters and a concert hall, a community college, and branches of the Maryland Institute College of Art and the Peabody Conservatory of Music.
The main entertainment area was to be known as Tivoli, after the entertainment area in Copenhagen. Early on, Rouse said that he hoped Tivoli would be a place “where, under the benign influence of having fun and relaxing in familiar ways, people would have opportunities, especially attractively and conveniently presented, for discovering new ways to enjoy their free time—new foods, new visual and tactile aesthetic experiences, even new social relations.” Rouse wanted the town center in Columbia to provide the most comprehensive range of recreational activities and services that had ever been contemplated in a new town.
In late 1973, the Columbia project took a downturn as Maryland land developers such as Joel Kline, and politicians such as Governor Marvin Mandel, and Vice President Spiro Agnew were indicted on various charges of corruption related to land speculation. Rouse was indicted on board of elections charges for donating over the limits toward Mandel's 1974 campaign, but had charges dropped because the charges were tried just outside the one year limit.
Festival marketplaces.
Starting in the mid-1970s and continuing into the 1980s Rouse shifted focus to what he ended up calling the "festival marketplace", of which the Faneuil Hall Marketplace was the first and most successful example. Completed in 1976, the Faneuil Hall Marketplace (comprising Quincy Market and other spaces adjacent to Boston's Faneuil Hall) was designed by architect Benjamin C. Thompson and was a financial success, an act of historic preservation, and an anchor for urban revitalization. However, at its inception, it was considered a highly risky venture, and many critics felt it was doomed to fail. Rouse's innovative business vision looked obvious in retrospect, but it was a bold, contrarian move with few supporters at the outset.
Other examples of Rouse Company "festival marketplace" developments include South Street Seaport in New York City, The Gallery at Market East, in Philadelphia, Harborplace in Baltimore, St. Louis Union Station in St. Louis, Downtown Portland's Pioneer Place, and the Riverwalk Marketplace of New Orleans. The early festival marketplaces like Faneuil Hall and Harborplace led "TIME" magazine to dub Rouse "the man who made cities fun again."
Retirement.
After 40 years at the Rouse Company, James Rouse retired from day-to-day management in 1979 becoming CEO. Soon afterwards, he and his wife founded the Enterprise Foundation, a not-for-profit foundation funded in part by a for-profit subsidiary, The Enterprise Development Company, and focused on seeding partnerships with community groups that would address the need for affordable housing and associated social services for poor neighborhoods. In 1984, Jim Rouse was soliciting business representing both Rouse Company as CEO and Enterprise Development as president. The Rouse Company board of directors asked Jim Rouse to leave as CEO of the Rouse Company and his position in Enterprise Development which ended his involvement with the company he founded.
Rouse was inducted into the Junior Achievement U.S. Business Hall of Fame in 1981. In 1988, Rouse was awarded the second Honor Award from the National Building Museum.
The Rouse Theatre in Wilde Lake High School is named after James. In May 2006, an approximately four-mile stretch of Maryland Route 175 between Interstate 95 and U.S. Route 29 in Columbia, Maryland, was named after Rouse and his wife, Patty. The Jim Rouse Visionary Center opened in 2006 in a formerly contaminated Whiskey Warehouse in Baltimore.
Patty Rouse died on March 5, 2012.
Rouse received the Presidential Medal of Freedom in 1995.
Awards.
In 1978, Rouse received the S. Roger Horchow Award for Greatest Public Service by a Private Citizen, an award given out annually by Jefferson Awards.
Personal life.
James Rouse first wife was Elizabeth Jamieson "Libby" (née Winstead) who he married on 3 May 1941. His daughter Robin is the mother of actor Edward Norton, His son Jim applied for conscientious objector status during the Vietnam conflict with James support. In May 1970, Rouse posted full page anti-war ads in the Washington Post and later the New York Times that upset the new Nixon administration. Rouse separated from Libby in 1973, and married Myrtle Patricia Traugott from Norfolk, Virginia in November 1974.
Rouse's nephew, Willard Rouse III, was another notable real estate developer.

</doc>
<doc id="50544" url="http://en.wikipedia.org/wiki?curid=50544" title="Chester (disambiguation)">
Chester (disambiguation)

Chester is a city and is also the county town of Cheshire, United Kingdom.
Chester may also refer to:

</doc>
<doc id="50548" url="http://en.wikipedia.org/wiki?curid=50548" title="Oakland, California">
Oakland, California

Oakland is a major West Coast port city in the U.S. state of California. The Port of Oakland is the busiest port for San Francisco Bay and all of Northern California. Oakland is the third largest city in the San Francisco Bay Area, the eighth-largest city in California, and the 45th-largest city in the U.S. with a population of 406,253 as of 2013[ [update]]. Incorporated in 1852, Oakland is the county seat of Alameda County. It serves as a major transportation hub and trade center for the entire region and is also the principal city of the Bay Area Region known as the East Bay. The city is situated directly across the bay, six miles (6 mi) east of San Francisco.
Oakland's territory covers what was once a mosaic of coastal terrace prairie, oak woodland, and north coastal scrub. Its land served as a rich resource when its hillside oak and redwood timber were logged to build San Francisco, and Oakland's fertile flatland soils helped it become a prolific agricultural region. In the late 1860s, Oakland was selected as the western terminal of the Transcontinental Railroad. It continued to grow in the 20th century with its busy port, shipyards, and a thriving automobile manufacturing industry. Following the 1906 San Francisco earthquake, many San Franciscans relocated to Oakland, enlarging the city's population, increasing its housing stock and improving its infrastructure.
A steady influx of immigrants during the 20th century, along with thousands of African-American war-industry workers who relocated from the Deep South during the 1940s, have made Oakland one of the most ethnically diverse major cities in the country. Oakland is known for its history of political activism, as well as its professional sports franchises and major corporations, which include health care, dot-com companies and manufacturers of household products. The city is a transportation hub for the greater Bay Area, and its shipping port is the fifth busiest in the United States.
Oakland has a Mediterranean climate with an average of 260 sunny days per year. Lake Merritt, a large estuary centrally located east of Downtown, was designated the United States' first official wildlife refuge. Jack London Square, named for the author and former resident, is a tourist destination on the Oakland waterfront.
While progress has been made in reducing the city's high property crime rate, violent crime has remained a persistent problem in Oakland, although this is primarily concentrated in certain neighborhoods. Oakland is continually listed among the top cities in the United States for sustainability practices, including a No. 1 ranking for usage of electricity from renewable resources. Significant portions of Oakland suffer from severe lead contamination.
In recent years, Oakland has gained national recognition as a travel destination. In 2012, Oakland was named the top North American city to visit, highlighting its growing number of sophisticated restaurants and bars, top music venues, and increasing nightlife appeal. Oakland also took the No. 16 spot in "America's Coolest Cities," ranked by metrics like entertainment options and recreational opportunities per capita, etc. In 2013, Oakland topped the No. 1 spot in "America's Most Exciting Cities," notably having the most movie theaters, theater companies, and museums per square mile. In "America's Most Hipster Cities," Oakland took the No. 5 spot, cited for luring San Francisco "hippies" into the city.
History.
Pre-incorporation.
The earliest known inhabitants were the Huchiun, who lived there for thousands of years. The Huchiun belonged to a linguistic grouping later called the Ohlone (a Miwok word meaning "western people"). In Oakland, they were concentrated around Lake Merritt and Temescal Creek, a stream that enters the San Francisco Bay at Emeryville.
In 1772, the area that later became Oakland was claimed, with the rest of California, by Spanish settlers for the King of Spain. In the early 19th century, the Spanish crown granted the East Bay area to Luis María Peralta for his Rancho San Antonio. The grant was confirmed by the successor Mexican republic upon its independence from Spain. Upon his death in 1842, Peralta divided his land among his four sons. Most of Oakland fell within the shares given to Antonio Maria and Vicente. The portion of the parcel that is now Oakland was called "encinal"—Spanish for "oak grove"—due to the large oak forest that covered the area, which eventually led to the city's name.
City beginnings.
In 1851, three men—Horace Carpentier, Edson Adams, and Andrew Moon—began developing what is now downtown Oakland. On May 4, 1852, the Town of Oakland incorporated. Two years later, on March 25, 1854, Oakland re-incorporated as the City of Oakland, with Horace Carpentier elected the first mayor, though a scandal ended his mayorship in less than a year. The city and its environs quickly grew with the railroads, becoming a major rail terminal in the late 1860s and 1870s. In 1868, the Central Pacific constructed the Oakland Long Wharf at Oakland Point, the site of today's Port of Oakland.
A number of horsecar and cable car lines were constructed in Oakland during the latter half of the 19th century. The first electric streetcar set out from Oakland to Berkeley in 1891, and other lines were converted and added over the course of the 1890s. The various streetcar companies operating in Oakland were acquired by Francis "Borax" Smith and consolidated into what eventually became known as the Key System, the predecessor of today's publicly owned AC Transit.
1900–1950s.
The original extent of Oakland, upon its incorporation, lay south of today's major intersection of San Pablo Avenue, Broadway, and Fourteenth Street. The city gradually annexed farmlands and settlements to the east and the north. Oakland's rise to industrial prominence, and its subsequent need for a seaport, led to the digging of a shipping and tidal channel in 1902, which created an island of nearby town Alameda. In 1906, its population doubled with refugees made homeless after the 1906 San Francisco earthquake and fire.
In 1916, General Motors opened a major automobile factory in East Oakland called Oakland Assembly, making Chevrolet cars and then GMC trucks until 1963, when it was moved to Fremont in southern Alameda County. Also in 1916, the Fageol Motor Company chose East Oakland for their first factory, manufacturing farming tractors from 1918 to 1923. By 1920, Oakland was the home of numerous manufacturing industries, including metals, canneries, bakeries, internal combustion engines, automobiles, and shipbuilding. By 1929, when Chrysler expanded with a new plant there, Oakland had become known as the "Detroit of the West."
Oakland expanded during the 1920s, flexing enough to meet the influx of factory workers. Approximately 13,000 homes were built between 1921 and 1924, more than between 1907 and 1920. Many of the large downtown office buildings, apartment buildings, and single-family houses still standing in Oakland were built during the 1920s; and they reflect the architectural styles of the time.
Russell Clifford Durant established Durant Field at 82nd Avenue and East 14th Street in 1916. The first transcontinental airmail flight finished its journey at Durant Field on August 9, 1920, flown by Army Capt. Eddie Rickenbacker and Navy Lt. Bert Acosta. Durant Field was often called Oakland Airport, though the current Oakland International Airport was soon established four miles (4 mi) southwest.
During World War II, the East Bay Area was home to many war-related industries. Oakland's Moore Dry Dock Company expanded its shipbuilding capabilities and built over 100 ships. Valued at $100 million in 1943, Oakland's canning industry was its second-most-valuable war contribution after shipbuilding. The largest canneries were in the Fruitvale District and included the Josiah Lusk Canning Company, the Oakland Preserving Company (which started the Del Monte brand), and the California Packing Company. The war attracted tens of thousands of laborers from around the country, though most were poor whites and blacks from Alabama, Arkansas, Georgia, Louisiana, Mississippi, Missouri, South Carolina, Tennessee and Texas—sharecroppers and tenant farmers who had been recruited by Henry J. Kaiser to work in his shipyards. These immigrants from the Jim Crow South brought their racial attitudes with them, and the racial harmony that Oakland blacks had been accustomed to prior to the war evaporated. Also migrating to the area during this time were many Mexican Americans from southwestern states such as New Mexico, Texas, and Colorado, many working for the Southern Pacific Railroad, at its major rail yard in West Oakland. Oakland experienced its own "zoot suit riots" in downtown Oakland in 1943 in the wake of the one in Los Angeles.
In 1946 National City Lines (NCL), a General Motors holding company, acquired 64% of Key System stock; during the next several years NCL engaged in the conspiratorial dissolution of Oakland's electric streetcar system, where the city's electric streetcar fleet was converted to diesel buses. The state Legislature created the Alameda and Contra Costa Transit District in 1955, which still exists today as AC Transit, the third-largest bus-only transit system in the nation.
Soon after the war, with the disappearance of Oakland's shipbuilding industry and the decline of its automobile industry, jobs became scarce. There was also an increase in racial tension. Oakland was the center of a general strike during the first week of December 1946, one of six cities across the country that experienced such a strike after World War II.
1960–1999.
In 1960, Kaiser Corporation erected its headquarters; it was the largest skyscraper in Oakland, as well as "the largest office tower west of Chicago" up to that time. In 1966, only 16 of the city's 661 police officers were black. Tensions between the black community and the largely white police force were high, and police malfeasance against blacks was common. The Black Panther Party was founded by students Huey Newton and Bobby Seale at Merritt College. During the 1970s, Oakland began to experience serious problems with gang-controlled dealing of heroin and cocaine when drug kingpin Felix Mitchell created the nation's first large-scale operation of this kind. Both violent crime and property crime increased during this period, and Oakland's murder rate rose to twice that of San Francisco or New York City.
As in many other American cities during the 1980s, crack cocaine became a serious problem in Oakland. Drug dealing in general, and the dealing of crack cocaine in particular, resulted in elevated rates of violent crime, causing Oakland to consistently be listed as one of America's most crime-ridden cities. During the late 1980s and early 1990s, Oakland's black population reached its peak at approximately 47% of the overall population.
On October 20, 1991, a massive firestorm swept down from the Berkeley Hills above the Caldecott Tunnel. Twenty-five people were killed, 150 people were injured, and nearly 4,000 homes destroyed. With the loss of life and an estimated economic loss of $1.5 billion, this was the worst urban firestorm in American history. During the mid-1990s, Oakland experienced an improved economy compared to previous decades, with large development and urban renewal projects, concentrated especially in the downtown area, at the Port of Oakland and at the Oakland International Airport.
The Loma Prieta earthquake, a rupture of the San Andreas fault that affected the entire San Francisco Bay Area, occurred on October 17, 1989. The quake's surface wave measured 6.9 on the Richter magnitude scale, and many structures in Oakland were badly damaged. The double-decker portion of the Interstate 880 freeway structure collapsed. A section of the eastern span of the San Francisco – Oakland Bay Bridge also collapsed and was closed to traffic for one month.
2000s.
After his 1999 inauguration, Oakland Mayor Jerry Brown continued his predecessor Elihu Harris' public policy of supporting downtown housing development in the area defined as the Central Business District in Oakland's 1998 General Plan. Brown's plan and other redevelopment projects were controversial due to potential rent increases and gentrification, which would displace lower-income residents from downtown Oakland into outlying neighborhoods and cities. Further hampering Oakland's economic recovery were the economic crises in 2001 and 2008. These downturns resulted in lowered sales, rentals and occupancy of the new housing and slower growth and economic recovery than expected.
Due to allegations of misconduct by the Oakland Police Department, the City of Oakland has paid claims for a total of $57 million during the 2001–2011 timeframe to plaintiffs claiming police abuse—the largest sum of any city in California. On October 10, 2011, protesters and civic activists began "Occupy Oakland" demonstrations at Frank Ogawa Plaza in Downtown Oakland.
Geography.
Oakland is on the east side of San Francisco Bay; in 1991 the City Hall tower was at (NAD83). (The building still exists, but like the rest of the Bay Area it has shifted northwest perhaps 0.6 meter in the last twenty years.)
The United States Census Bureau says the city's total area is 78.0 sqmi, including 55.8 sqmi of land and 22.2 sqmi (28.48 percent) of water.
Oakland's highest point is near Grizzly Peak Blvd, east of Berkeley, just over 1760 ft above sea level at about . Oakland has 19 mi of shoreline, but Radio Beach is the only beach in Oakland.
Oaklanders refer to their city's terrain as "the flatlands" and "the hills", which until recent waves of gentrification have also been a reference to Oakland's deep economic divide, with "the hills" being more affluent communities. About two-thirds of Oakland lies in the flat plain of the East Bay, with one-third rising into the foothills and hills of the East Bay range.
Cityscape.
Lake Merritt panorama
Neighborhoods.
Oakland has more than 50 distinct neighborhoods. The greater divisions in the city include downtown Oakland and its greater Central Business District, Lake Merritt, East Oakland, North Oakland, West Oakland, and the Oakland Hills.
East Oakland, which includes the East Oakland Hills, encompasses more than half of Oakland's land area, stretching from Lakeshore Avenue on the east shore of Lake Merritt southeast to the San Leandro border. North Oakland encompasses the neighborhoods between downtown and Berkeley and Emeryville. West Oakland is the area between downtown and the Bay, partially surrounded by the Oakland Point, and encompassing the Port of Oakland. In 2011, Oakland was ranked the 10th most walkable city in the United States.
Lake Merritt, an urban estuary near downtown, is a mix of fresh and salt water draining in and out from the Oakland Harbor at the San Francisco Bay and one of Oakland's most notable features. It was designated the United States' first official wildlife refuge in 1870. Originally a marsh-lined wildlife haven, Lake Merritt was dredged and bordered with parks from the 1890s to the 1910s. Despite this reduction in habitat, Oakland is home to a number of rare and endangered species, many of which are localized to serpentine soils and bedrock. Lake Merritt is surrounded by residential and business districts, including downtown and Grand Lake.
The city of Piedmont, incorporated in Oakland's central foothills after the 1906 earthquake, is a small independent city surrounded by the city of Oakland.
Climate and vegetation.
Based on data gathered by the National Oceanic and Atmospheric Administration, Oakland is ranked No. 1 in climate among U.S. cities. Oakland's climate is typified by the temperate and seasonal Mediterranean climate. Summers are usually dry and warm and winters are mild and damp. More specifically, it has features found in both nearby coastal cities such as San Francisco and inland cities such as San Jose, making it warmer than San Francisco and cooler than San Jose. Its position on San Francisco Bay across from the Bay Bridge means that the Northern part of the city can experience cooling maritime fog. It is far enough inland, though, that the fog often burns off by midday, allowing it to have typically sunny California days. The hills tend to have more fog than the flatlands, as the fog drifts down from Berkeley.
The U.S. Weather Bureau kept weather records in downtown Oakland from October 4, 1894, to July 31, 1958. During that time, the record high temperature was 104 °F on June 24, 1957, and the record low temperature was 24 °F on January 23, 1949. Dry, warm offshore "Diablo" winds (similar to the Santa Ana winds of Southern California) sometimes occur, especially in fall, and raise the fire danger. In 1991, such an episode allowed the catastrophic Oakland Hills fire to spread and consume many homes. The wettest year was 1940 with 38.65 in and the driest year was 1910 with 12.02 in. The most rainfall in one month was 15.35 in in January 1911. The most rainfall in 24 hours was 4.27 in on February 12, 1904. Rainfall near the bayfront is only 23 inches per year, but is higher in the Oakland Hills to the east (up to 30 inches).
The higher rainfall in the hills supports woods of oak, madrone, pine, fir and a few redwood groves in the wetter areas. Before being logged in the 19th century, some of the tallest redwood trees in California (even used for navigation by ships entering the Golden Gate) may have stood in the Oakland Hills. One old stump 30 feet across can still be seen near Redwood Regional Park. Sunny, drier slopes are grassy or covered in scattered oaks and chaparral brush. Australian eucalyptus trees have been extensively planted in many areas.
The National Weather Service today has two official weather stations in Oakland: Oakland International Airport and the Oakland Museum (established 1970).
Ruptures along the nearby San Andreas fault caused severe earth movement in the San Francisco Bay Area in 1906 and 1989. San Andreas quakes induces creep (movement occurring on earthquake faults) in the Hayward fault, which runs directly through Oakland, Berkeley, San Jose and other Bay Area cities. In 1991, an urban conflagration, the Oakland Hills Fire, destroyed nearly 4,000 homes and killed twenty five people in the Oakland hills range; it was the worst urban firestorm in American history.
Demographics.
Race and ethnicity.
The 2010 United States Census reported that Oakland had a population of 390,724. The population density was 5,009.2 people per square mile (1,934.0/km²). The racial makeup of Oakland was 134,925 (34.5%) White (non-Hispanic White 25.9%), 109,471 (28.0%) African American, 3,040 (0.8%) Native American, 65,811 (16.8%) Asian (8.7% Chinese, 2.2% Vietnamese, 1.6% Filipino, 0.7% Cambodian, 0.7% Laotian, 0.6% Korean, 0.5% Japanese, 0.5% Indian, 0.1% Mongolian), 2,222 (0.6%) Pacific Islander (0.3% Tongan), 53,378 (13.7%) from other races, and 21,877 (5.6%) from two or more races. Hispanic or Latino of any race were 99,068 persons (25.4%). 18.1% of the population were of Mexican descent, 1.9% Salvadoran, 1.3% Guatemalan, and 0.7% Puerto Rican.
Educational attainment and income.
Oakland has the fifth largest cluster of "elite zip codes" ranked by the number of households with the highest combination of income and education. 37.9% of residents over 25 years of age have bachelor's degree or higher. Oakland ranked among the top cities with residents with bachelor degrees and graduate degrees per square mile.
Oakland ranks in the top 20 of American cities in median household income, with a 2012 value of $51,863. In 2012, the median income for a household in the city was $51,863 and the median income for a family was $59,459. The mean income for a household was $77,888 and the mean income for a family was $90,948. Males had a median income of $50,140 versus $50,304 for females. The unemployment rate as of December 2013 was 9.7%.
In 2008 the median income for a household in the city was $48,596 and the median income for a family was $55,949. Males had a median income of $46,383 versus $44,690 for females. The per capita income for the city was $30,094. In 2007 approximately 15.3 percent of families and 17.0 percent of the general population were below the poverty line, including 27.9 percent of those under age 18 and 13.1 percent of those age 65 or over. 0.7% of the population is homeless. Home ownership is 41% and 14% of rental units are subsidized. The unemployment rate as of August 2009 is 15.2%.
As of the census of 2000, the median income for a household in the city was $40,055, and the median income for a family was $44,384. Males had a median income of $37,433 versus $35,088 for females. The per capita income for the city was $21,936. 19.4% of the population and 16.2% of families were below the poverty line. Out of the total population, 27.9% of those under the age of 18 and 13.1% of those 65 and older were living below the poverty line.
Households.
The census reported that 382,586 people (97.9% of the population) lived in households, 5,675 (1.5%) lived in non-institutionalized group quarters, and 2,463 (0.6%) were institutionalized.
There were 153,791 households, out of which 44,762 (29.1%) had children under the age of 18 living in them, 50,797 (33.0%) were opposite-sex married couples living together, 24,122 (15.7%) had a female householder with no husband present, 8,799 (5.7%) had a male householder with no wife present. There were 11,289 (7.3%) unmarried opposite-sex partnerships, and 3,442 (2.2%) same-sex married couples or partnerships. 52,103 households (33.9%) were made up of individuals and 13,778 (9.0%) had someone living alone who was 65 years of age or older. The average household size was 2.49. There were 83,718 families (54.4% of all households); the average family size was 3.27.
The population was spread out with 83,120 people (21.3%) under the age of 18, 36,272 people (9.3%) aged 18 to 24, 129,139 people (33.1%) aged 25 to 44, 98,634 people (25.2%) aged 45 to 64, and 43,559 people (11.1%) who were 65 years of age or older. The median age was 36.2 years. For every 100 females there were 94.2 males. For every 100 females age 18 and over, there were 91.8 males.
There were 169,710 housing units at an average density of 2,175.7 per square mile (840.0/km²), of which 63,142 (41.1%) were owner-occupied, and 90,649 (58.9%) were occupied by renters. The homeowner vacancy rate was 3.0%; the rental vacancy rate was 8.5%. 166,662 people (42.7% of the population) lived in owner-occupied housing units and 215,924 people (55.3%) lived in rental housing units.
Shifting of cultures.
Oakland is one of the most ethnically diverse major cities in the country. Oakland was ranked the 4th most diverse city in America, with an overall diversity score of 91.4. The city's formerly most populous ethnic group, whites, declined from 95.3% in 1940 to 32.5% by 1990. Since the 1960s, Oakland has been known as a center of Northern California's African-American community. However, between 2000 and 2010 Oakland lost nearly one-fourth of its black population. The city demographics have changed due to a combination of gentrification, along with many blacks relocating to Bay Area suburbs, or moving to the Southern United States. Blacks formed a strong plurality for many years, peaking in 1980 at about 47% of the population of Oakland.
Black residents maintained their status as Oakland's single largest ethnic group as of 2010, forming 27% of the population, followed by non-Hispanic whites at 25.9%, and Hispanics of any race at 25.4%.
Recent trends and cultural shifts have led to a decline among some of Oakland's longstanding black institutions, such as churches, businesses, and nightclubs, which has been a point of contention for some long-time black residents.
In recent years, immigrants and others have marched by the thousands down Oakland's International Boulevard in support of legal reforms benefiting illegal immigrants.
An analysis by the Urban Institute of U.S. Census 2000 numbers showed that Oakland had the third-highest concentration of gays and lesbians among the 50 largest U.S. cities, behind San Francisco and Seattle. Census data showed that, among incorporated places that have at least 500 female couples, Oakland had the nation's largest proportion. In 2000, Oakland counted 2,650 lesbian couples; one in every 41 Oakland couples listed themselves as a same-sex female partnership.
Economy.
Oakland is a major West Coast port, and the fifth busiest in the United States by cargo volume. The Port of Oakland handles 99% of all containerized goods moving through Northern California, representing $41 billion worth of international trade. There are nearly 200,000 jobs related to marine cargo transport in the Oakland area. These jobs range from minimum wage hourly positions to Transportation Storage and Distribution Managers who earn an annual average salary of $91,520. The Port of Oakland was an early innovator/pioneer in the technologies of Intermodal Containerized Shipping. The city is also home to several major corporations including Kaiser Permanente and Clorox, as well as the corporate headquarters for national brands such as Dreyer's ice cream, and retailer Cost Plus World Markets. Tech companies such as Ask.com and Pandora Radio are located in Oakland, and in recent years many start-up high tech and green energy companies have found a home in the downtown neighborhoods of Uptown, City Center, Jack London Square and Lake Merritt Financial District. In 2014, Oakland was the fifth ranked city for tech entrepreneurs by total venture capital investment.
As of 2013, the San Francisco-Oakland-Hayward has a GDP (Gross Domestic Product) of $360.4 billion, ranking eighth among metropolitan areas in the United States. In 2014, Oakland was amongst the best cities to start a career, the highest ranked city in California after San Francisco. Additionally, Oakland ranked fourth in cities with professional opportunities. Numerous companies in San Francisco continue to expand in or migrate over to Oakland.
Oakland experienced an increase of both its population and of land values in the early-to-mid first decade of the 21st century. The 10k Plan, which began during former mayor Elihu Harris' administration, and intensified during former mayor Jerry Brown's administration resulted in several thousand units of new multi-family housing and development.
Top employers.
As of 2014, the top employers in the city were:
Tourism.
In 2013, over 2.5 million people visited Oakland, injecting US$1.3 billion into the economy. Oakland has been experiencing an increase in hotel demand. Occupancy is 74%, while RevPAR (Revenue Per Available Room) increased by 14%, the highest increase of any big city in the western region of the United States. Both Oakland and San Francisco were forecasted to experience the highest increases in ADR (Average daily rate).
Oakland has gained increasing appeal as a travel destination, both nationally and internationally.
Arts and culture.
Oakland has a significant art scene and claims the highest concentration of artists per capita in the United States. In 2013, Oakland was designated as one of America's top twelve art communities, recognizing Downtown (including Uptown), Chinatown, Old Oakland, and Jack London Square as communities "that have most successfully combine art, artists and venues for creativity and expression with independent businesses, retail shops and restaurants, and a walkable lifestyle to make vibrant neighborhoods." Galleries exist in various parts of Oakland, with the newest additions centered mostly in the Uptown area. Oakland ranked 11th in cities for designers and artists.
The city offers a wide variety of cuisine in restaurants and markets, often featuring locally grown produce and international foods that reflect the city's ethnically diverse population.
Historically a focal point of the West Coast blues and jazz scenes, Oakland is also home to musicians representing such genres as rhythm and blues, gospel, funk, punk, heavy metal, Rap/Gangsta rap, and hip hop.
Nightlife.
Downtown Oakland has an assortment of bars and nightclubs. They include dive bars, dance clubs, modern lounges and jazz bars. The Paramount Theater features headlining musical tours and productions, while Fox Oakland Theatre draws various musical genres including jam bands, rock, punk, blues, jazz, and reggae. The Paramount and Fox theaters often book simultaneous events, creating busy nights uptown. In 2012, Oakland was dubbed a "New Sin City", following its 2010 decision to relax its cabaret laws, which gave a boost to its nightclub and bar scene.
Recent years have seen the growth of the Oakland Art Murmur event, occurring in the Uptown neighborhood the first Friday evening of every month. The event attracts around 20,000 people along twenty city blocks, featuring live performances, food trucks, and over 30 galleries and venues.
"There is no there there".
 The HERETHERE sculpture straddling the Oakland Berkeley border
Gertrude Stein wrote about Oakland in her 1937 book "Everybody's Autobiography": "There is no there there," Stein wrote on learning that the neighborhood where she lived as a child had been torn down to make way for an industrial park. The quote is sometimes misconstrued to refer to Oakland as a whole.
Modern-day Oakland has turned the quote on its head, with a statue downtown simply titled "There." Additionally, in 2005 a sculpture called HERETHERE was installed by the City of Berkeley on the Berkeley-Oakland border at Martin Luther King Jr. Way. The sculpture consists of eight-foot-tall letters spelling "HERE" and "THERE" in front of the BART tracks as they descend from their elevated section in Oakland to the subway through Berkeley.
Professional sports.
Oakland has teams in three professional sports: Baseball, basketball, and football. The Oakland Athletics MLB club won three consecutive World Series championships in 1972, 1973, and 1974, and appeared in another three consecutive World Series from 1988 to 1990, winning their fourth championship in 1989. The Golden State Warriors won the 1974–1975 NBA championship. The Oakland Raiders of the NFL won Super Bowl XI in 1977 and Super Bowl XV in 1981, while also appearing in Super Bowl II in 1968 and Super Bowl XXXVII in 2003. The Raiders left Oakland for Los Angeles in 1982, where they won a third Super Bowl championship, and returned to Oakland in 1995. The Warriors announced in April 2014 that they will leave Oakland once their new arena is built across the Bay in San Francisco, while the Raiders are in discussion with city officials about building a new football-only stadium.
Oakland's former sports teams include:
Parks and recreation.
Parks.
Oakland has many parks and recreation centers which total 5937 acres. In its 2013 ParkScore ranking, The Trust for Public Land, a national land conservation organization, reported that Oakland had the 18th best park system among the 50 most populous U.S. cities. In 2013, Oakland ranked 4th among American cities as an urban destination for nature lovers.
Some of the city's most notable parks include:
Additionally, the following seven East Bay Regional Parks are located entirely or partially in the city of Oakland:
Places of worship.
Some of the most prominent places of worship in Oakland include: First Congregational Church of Oakland, Evangelistic Outreach Center, Green Pastures, the Presbyterian, First Presbyterian Church of Oakland; Greek Orthodox Ascension Cathedral; the Roman Catholic Cathedral of Christ the Light; the United Methodist Chinese Community Church; the Unitarian First Unitarian Church; the Mormon Oakland California Temple; the Muslim, 31st Street Islamic Center, Light-House Mosque; the Reform Jewish Temple Sinai; the Conservative Jewish, Temple Beth Abraham; Allen Temple Baptist Church; and the Orthodox Jewish, Beth Jacob Congregation, American Baptist; Faith Baptist Church of Oakland, St. Paul Lutheran, six Kingdom Halls of Jehovah's Witnesses and St. Vartan Armenian Apostolic Church.
Law and government.
Oakland has a mayor-council government. The mayor is elected for a four-year term. The Oakland City Council has eight council members representing seven districts in Oakland with one member elected at-large; council members serve staggered four-year terms. The mayor appoints a city administrator, subject to the confirmation by the City Council, who is the chief administrative officer of the city. Other city officers include: city attorney (elected), city auditor (elected), and city clerk (appointed by city administrator). Oakland's mayor is limited to two terms. There are no term limits for the city council. Council member Larry Reid, also serving as vice-mayor, was elected to a fifth term in November 2012.
Oakland City Hall was evacuated after the 1989 Loma Prieta earthquake until $80M seismic retrofit and hazard abatement work was complete in 1995.
Jean Quan was elected mayor in November 2010, beating Don Perata and Rebecca Kaplan in the city's first ranked choice balloting.
Oakland is also part of Alameda County, for which the Government of Alameda County is defined and authorized under the California Constitution, California law, and the Charter of the County of Alameda. The County government provides countywide services such as elections and voter registration, law enforcement, jails, vital records, property records, tax collection, public health, and social services. The County government is primarily composed of the elected five-member Board of Supervisors, other elected offices including the Sheriff/Coroner, the District Attorney, Assessor, Auditor-Controller/County Clerk/Recorder, and Treasurer/Tax Collector, and numerous county departments and entities under the supervision of the County Administrator.
In the California State Legislature, Oakland is in the 9 Senate District, represented by Democrat Loni Hancock, and is split between the 15th and 18th Assembly districts, represented by Tony Thurmond and Rob Bonta, respectively. In the United States House of Representatives, Oakland is in California's 13th congressional district, represented by Democrat Barbara Lee.
Politics.
Oakland was politically conservative from the 1860s to the 1950s, led by the Republican-oriented "Oakland Tribune" newspaper. In the 1950s and '60s, the majority stance shifted to favor liberal policies and the Democratic Party. Oakland has by far the highest percentage of registered Democrats of any of the incorporated cities in Alameda County. As of 2009, Oakland has 204,646 registered voters, and 140,858 (68.8%) are registered Democrats, 12,248 (5.9%) are registered Republicans, and 41,109 (20.1%) decline to state a political affiliation. Oakland is widely regarded as being one of the most liberal major cities in the nation. The Cook Partisan Voting Index of Congressional District 13, which includes Oakland and Berkeley, is D+37, making it the fourth most Democratic congressional district in the US.
Crime.
Oakland's crime rate began to escalate during the late 1960s, and by the end of the 1970s Oakland's per capita murder rate had risen to twice that of its neighbor city, San Francisco, or that of New York City. The rise in crime may have been an effect of the different method that was used to deal with rebellious youth. Prior to 1960, there were successful government funded social programs, where workers would work in neighborhoods searching for rebellious teens to enter them in youth centers that would be able to teach them proper values and improve their behavior. However, by the late 1960s,the police and Federal Bureau of Investigation(FBI) used military tactics to manage unwanted behavior, that then led to an increase in crime and imprisonment. During the first decade of the 21st century Oakland has consistently been listed as one of the most dangerous large cities in the United States. Until 2010 the homicide rate dropped four times in a row, and violent crime in general had dropped 27%. Violent crime in general, and homicides in particular, increased during 2011. In 2012 Oakland reported 131 homicides, the highest since 2006 (when there were 148 recorded). In 2013, there was a 33% decrease in homicides, allowing Oakland to record its lowest homicide count since 2004. Aggravated assaults were down 10% and rapes declined by 27%, reaching its lowest level in eight years. In the first quarter of 2014, homicides, aggravated assaults, and burglaries were down 15% relative to 2013. Additionally, shootings were down 31% and robberies were down 36%.
Oakland's police force has dropped to 612 officers, down from more than 800 in 2009, and far below the 925 recommended by the city's strategic plan, but Oakland recently started to rebuild its force by hiring more often and recently graduating 34 officers. Although the police department's resources have been diminishing, according to former Police Chief Howard Jordan the Oakland Police Department is committed to improved public safety by increasing police presence during peak crime hours, improving intelligence gathering, and moving more aggressively to arrest violent crime suspects.
Among Oakland's 35 police patrol beats, violent crime remains a serious problem in specific East and West Oakland neighborhoods. In 2008, homicides were disproportionately concentrated: 72% occurred in three City Council districts, District 3 in West Oakland and Districts 6 and 7 in East Oakland, even though these districts represent only 44% of Oakland's residents.
In 2012, Oakland implemented a gang violence reduction plan used previously in other cities, Operation Ceasefire, based in part on the research and strategies of author David M. Kennedy.
Education.
Primary and secondary education.
Most public schools in Oakland are operated by the Oakland Unified School District (OUSD), which covers the entire city of Oakland; due to financial troubles and administrative failures, it has been in receivership by the state of California since 2002. The Oakland Unified School District (2006–2007) includes 59 elementary schools, 23 middle schools, 19 high schools, with 9 alternative education schools and programs, 4 adult education schools and early childhood education centers at most of the elementary schools There are 46,000 K–12 students, 32,000 adult students, and 6,000 plus employees. In the 2005 results of the STAR testing, over 50 percent of students taking the test performed "below basic," while only 20 percent performed at least "proficient" on the English section of the test. Some individual schools have much better performance than the city-wide average, for instance, in 2005 over half the students at Hillcrest Elementary School in the Montclair upper hills neighborhood performed at the "advanced" level in the English portion of the test, and students at Lincoln Elementary School in the Chinatown neighborhood performed at the "advanced" level in the math portion.
Oakland's three largest public high schools are Oakland High School, Oakland Technical High School, and Skyline High School.
Oakland Tech has various academies, including its much renowned Engineering Academy, which sent more girls to MIT in 2007 than any other public school west of the Mississippi. There are also numerous small public high schools within Castlemont Community of Small Schools, Fremont Federation of High Schools, and McClymonds Educational Complex, all of which were once single, larger public high schools that were reorganized due to poor performance (Castlemont High School, Fremont High School, and McClymonds High School, respectively).
25 public charter schools with 5,887 students operate outside the domain of OUSD. One, North Oakland Community Charter School (NOCCS), an elementary and middle school, is one of the few public progressive schools in the country. Lionel Wilson College Prep Academy and Oakland Unity High School have been certified by the California Charter Schools Association. Other charter schools include the Oakland Military Institute, Oakland School for the Arts, Bay Area Technology School, and Oakland Charter Academy.
There are several private high schools including the secular The College Preparatory School and Head-Royce School, and the Catholic Bishop O'Dowd High School, Holy Names High School and St. Elizabeth High School. Catholic schools in Oakland are operated by the Roman Catholic Diocese of Oakland also include eight K–8 schools (plus one in Piedmont on the Oakland city border). Northern Light School is a private nonprofit elementary and middle school.
Bentley School is an Independent Co-educational K–12, college preparatory school, located on two campuses in Oakland and Lafayette, California.
Colleges and universities.
Accredited colleges and universities include:
In 2001, the SFSU Oakland Multimedia Center was opened, allowing San Francisco State University to conduct classes near downtown Oakland. The Oakland Higher Education Consortium and the City of Oakland's Community and Economic Development Agency (CEDA) opened the Oakland Higher Education Center downtown in 2002 to provide "access to multiple higher education service providers within a shared urban facility." Member schools include primary user California State University, East Bay as well as Lincoln University, New College of California, Saint Mary's College of California, SFSU Multimedia Studies Program, UC Berkeley Extension, University of Phoenix and Peralta Community College District.
Media.
Oakland is served by major television stations broadcasting primarily out of San Francisco and San Jose. The region's Fox affiliate, KTVU, is based in (and licensed to) Oakland at Jack London Square along with co-owned independent station KICU-TV (licensed to San Jose). In addition, the city is served by various radio stations as well; AM stations KKSF, KMKY, KNEW and KQKE are licensed to Oakland.
Oakland is served by the Oakland Tribune, which published its first newspaper on February 21, 1874. The Tribune Tower, which features a large clock, is an Oakland landmark. At key times throughout the day (8:00 am, noon and 5:00 pm), the clock tower carillon plays a variety of classic melodies, which change daily. In 2007, the Oakland Tribune moved its offices from the tower to an East Oakland location, before folding in 2011.
The "East Bay Express", a locally owned free weekly paper, is based in Jack London Square and distributed throughout the East Bay.
 is a thriving (mostly) English-language LocalWiki.
Infrastructure.
Transportation.
Air and rail.
Oakland residents have access to the three major airports of the San Francisco Bay Area: Oakland International Airport, San Francisco International Airport, and San Jose International Airport. Oakland International Airport, located within the city limits of Oakland, is 4 mi south of downtown Oakland and serves domestic and international destinations. AC Transit provides 24-hour service to the airport, and the Coliseum–Oakland International Airport line automated guideway transit provides frequent service between the airport and BART's Oakland Coliseum Station.
The city has regional and long distance passenger train service provided by Amtrak, with stations located near Jack London Square and the Oakland Coliseum. Amtrak's "California Zephyr" has its western terminus at the nearby Emeryville station.
Historically, the city was served by several train companies, which terminated in different terminals. Santa Fe trains terminated at the 40th and San Pablo station. Southern Pacific trains ended at the 16th Street Station. Western Pacific trains ended at the 3rd and Washington station. However, a common feature was that the different railroads continued one more stop to a station at Oakland Pier. From this latter point passengers would ride ferries to San Francisco.
Mass transit and bicycling.
The most recent census data compiled in 2007 before gasoline price spikes in 2008, show 24.3 percent of Oaklanders used public transportation, walked or used "other means" to commute to work, not including telecommuting, with 17 percent of Oakland households being "car free" and or statistically categorized as having "no vehicles available."
Bus transit service in Oakland and the inner East Bay is provided by the Alameda and Contra Costa Transit District, AC Transit. The district originated in 1958 after the conspiratorial dissolution of the Key System of streetcars. Many AC Transit lines follow old routes of the Key System.
Intercity bus companies that serve Oakland include Greyhound, BoltBus, Megabus, USAsia, and Hoang Transportation.
The metropolitan area is served by Bay Area Rapid Transit (BART) from eight stations in Oakland. The system has headquarters in Oakland, with major transfer hubs at MacArthur and 19th Street stations. BART's headquarters was located in a building above the Lake Merritt BART station until 2006, when it relocated to the Kaiser Center due to seismic safety concerns.
The Alameda / Oakland Ferry operates ferry service from Jack London Square to Alameda, San Francisco, and Angel Island.
Oakland licenses taxi cabs, and has zoned cab stands in its downtown, including a bicycle pedi-cab service.
The Oakland City Council adopted a Bicycle Master Plan in 1999 as a part of the Land Use and Transportation (LUTE) element of Oakland's 1998 General Plan. In addition, the Oakland City Council reaffirmed the bike plan in 2005 and 2007. Several miles of bike lanes were created as a result of the plan, with more awaiting funding. Facilities for parking thousands of bicycles have been installed downtown and in other commercial districts throughout Oakland. According to the U.S. Census Bureau's 2011 American Community Survey, Oakland moved into 7th place in the nation by percentage of people that choose to commute by bike in 2011.
Bridges, freeways, and tunnels.
Oakland is served by several major highways: Eastbound Bay Bridge traffic entering Oakland then splits into three freeways at the MacArthur Maze freeway interchange: Interstate 580 (MacArthur Freeway) heads southeast toward Hayward and eventually to the California Central Valley; Interstate 880 (Nimitz Freeway) runs south to San Jose; and the Eastshore Freeway (Interstate 80/I-580) runs north, providing connections to Sacramento and San Rafael, respectively.
Interstate 980 (Williams Freeway) begins its eastbound journey at I-880 in Downtown Oakland before turning into State Route 24 (Grove Shafter Freeway) at I-580. State Route 13 begins as the Warren Freeway at I-580, and runs through a scenic valley in the Montclair District before entering Berkeley. A stub of a planned freeway was constructed at the High Street exit from the Nimitz Freeway, but that freeway extension plan was abandoned.
In 1989, the Loma Prieta earthquake caused the Cypress Street Viaduct double-deck segment of the Nimitz Freeway to collapse, killing 42 people. The old freeway segment had passed through the middle of West Oakland, forming a barrier between West Oakland neighborhoods. Following the earthquake, this section was rerouted around the perimeter of West Oakland and rebuilt in 1999. The east span of the San Francisco – Oakland Bay Bridge also suffered damage from the quake when a 50-foot (15-m) section of the upper deck collapsed onto the lower deck; the damaged section was repaired within a month of the earthquake. As a result of Loma Prieta, a significant seismic retrofit was performed on the western span of the Bay Bridge. The eastern span is currently being replaced, with a projected completion date of 2014.
Two underwater tunnels, the Webster and Posey Tubes, connect the main island of Alameda to downtown Oakland, coming above ground in Chinatown. In addition, the Park Street, Fruitvale, and High Street bridges connect Alameda to East Oakland over the Oakland Estuary.
In the hills, the Leimert Bridge crosses Dimond Canyon, connecting the Oakmore neighborhood to Park Boulevard. The Caldecott Tunnel carries Highway 24 through the Berkeley Hills, connecting central Contra Costa County to Oakland. The Caldecott has four bores.
Freight rail.
Freight service, which consists primarily of moving shipping containers to and from the Port of Oakland, is provided today by Union Pacific Railroad (UP), and to a lesser extent by BNSF Railway (which now shares the tracks of the UP between Richmond and Oakland).
Historically, Oakland was served by several railroads. Besides the transcontinental line of the Southern Pacific, there was also the Santa Fe (whose Oakland terminal was actually in Emeryville), the Western Pacific Railroad (who built a pier adjacent to the SP's), and the Sacramento Northern Railroad (eventually absorbed by the Western Pacific, which in turn was absorbed by UP in 1983).
Shipping.
As one of the three major ports on the West Coast of the United States, the Port of Oakland is the largest seaport on San Francisco Bay and the fifth busiest container port in the United States. It was one of the earliest seaports to switch to containerization and to intermodal container transfer, thereby displacing the Port of San Francisco, which never modernized its waterfront. One of the earlier limitations to growth was the inability to transfer containers to rail lines, all cranes historically operating between ocean vessels and trucks. In the 1980s the Port of Oakland began the evaluation of development of an intermodal container transfer capability, i.e. facilities that now allow trans-loading of containers from vessels to either trucks or rail modes.
Utilities.
Water and sewage treatment are provided by East Bay Municipal Utility District (EBMUD). Pacific Gas and Electric Company provides natural gas and electricity service. Municipal garbage collection is franchised to Waste Management, Inc. Telecommunications and subscriber television services are provided by multiple private corporations and other service providers in accordance with the competitive objectives of the Telecommunications Act of 1996.
Oakland tops the list of the 50 largest US cities using electricity from renewable sources.
Healthcare.
Originating in Oakland, Kaiser Permanente, is an HMO started in 1942, during World War II, by industrialist Henry J. Kaiser to provide medical care for Kaiser Shipyards workers. It is the largest managed care organization in the United States and the largest non-governmental health care provider in the world. It is headquartered at 1950 Franklin Street in Downtown Oakland and maintains a large medical center in the Piedmont Avenue neighborhood.
Alta Bates Summit Medical Center, an East Bay hospital system, maintains its Summit Campus in the neighborhood known as "Pill Hill" north of downtown. Until 2000, it was the Summit Medical Center before merging with Berkeley-based Alta Bates. All campuses now operate under the Sutter Health network.
Alameda County Medical Center is operated by the county and provides medical services to county residents, including the medically indigent who do not have health insurance. The main campus, Highland Hospital in East Oakland, is the trauma center for the northern area of the East Bay.
Children's Hospital Oakland is the primary medical center specializing in pediatrics in the East Bay. It is a designated Level I pediatric trauma center, and the only independent children's hospital in Northern California.
Sister cities.
Oakland has 13 sister cities:

</doc>
<doc id="50549" url="http://en.wikipedia.org/wiki?curid=50549" title="Diocese">
Diocese

A diocese, from the Greek term "διοίκησις", meaning "administration", is the district under the supervision of a bishop. It is also known as a bishopric. A diocese is divided into parishes (in the Church of England into benefices and parishes). This structure of church governance is known as episcopal polity. Not to be confused with Disease.
The word diocesan means relating or pertaining to a diocese. It can also be used as a noun meaning the bishop who has the principal supervision of a diocese.
In the Latter Day Saint movement the term "bishopric" is used not of the ward or congregation of which a bishop has charge but instead of the bishop himself together with his two counsellors. On this see Bishop (Latter Day Saints).
An archdiocese (or archiepiscopal see or archbishopric) is more significant than a diocese. An archdiocese is presided over by an archbishop whose see may have or have had importance due to size or historical significance. The archbishop may have metropolitan authority over any other suffragan bishops and their dioceses within his ecclesiastical province.
Bishops (Greek: επίσκοπος, transliterated epískopos, which literally means overseers) claim apostolic succession; a direct historical lineage dating back to the original Twelve Apostles. A diocese also may be referred to as a bishopric or "episcopal see", though strictly the term "episcopal see" refers to the domain of ecclesiastical authority officially held by the bishop, and the term "bishopric" to the post of being bishop.
Especially in the Middle Ages, some bishops (e.g. prince-bishops) held political as well as religious authority within their dioceses, which in practice were thus also independent or semi-independent states.
Catholic Church.
s of 2015[ [update]], in the Catholic Church there are 2,851 regular dioceses: 1 papal see, 641 archdioceses (including 9 patriarchates, 4 major archdioceses, 551 metropolitan archdioceses, 77 single archdioceses) and 2,209 dioceses in the world.
In the Eastern rites in communion with the Pope, the equivalent unit is called an "eparchy".
Eastern Orthodox Church.
The Eastern Orthodox Church calls dioceses metropoleis in the Greek tradition or eparchies in the Slavic tradition.
Church of England and Anglican Communion.
After the Reformation, the Church of England retained the existing diocesan structure which remains throughout the Anglican Communion. The one change is that the areas administered under the Archbishop of Canterbury and Archbishop of York are properly referred to as provinces, not archdioceses. This usage is relatively common in the Anglican Communion.
Lutheranism.
Germany and Nordic countries.
Certain Lutheran denominations such as the Church of Sweden do have individual dioceses similar to Roman Catholics. These dioceses and archdioceses are under the government of a bishop (see Archbishop of Uppsala). Other Lutheran bodies and synods that have dioceses and bishops include the Church of Denmark, the Evangelical Lutheran Church of Finland, the Evangelical Church in Germany (partially), and the Church of Norway.
Lutheranism in USA.
Some American Lutheran synods such as the Evangelical Lutheran Church in America do have a bishop acting as the head of the synod, but the synod does not have dioceses and archdioceses as the churches listed above. Rather, it is divided into a middle judicatory.
The Lutheran Church-International, based in Springfield, Illinois, presently uses a traditional diocesan structure, with four dioceses in North America. Its current president is Archbishop Robert W. Hotes.
Church of God in Christ.
The Church of God in Christ ("COGIC") has many dioceses all across the United States. In the COGIC, each U.S. state is divided up into at least three dioceses that are all led by a bishop, but there are many U.S. states that have as many as seven dioceses. In the COGIC, the dioceses are called "Jurisdictions."
Churches that have bishops, but not dioceses.
Methodism.
In the United Methodist Church (the United States and some other countries), a bishop is given oversight over a geographical area called an Episcopal Area. Each episcopal area contains one or more annual conferences, which is how the churches and clergy under the bishop's supervision are organized. Thus, the use of the term "diocese" referring to geography is the most equivalent in the United Methodist Church, whereas each annual conference is part of one episcopal area (though that area may contain more than one conference). The African Methodist Episcopal Church has a similar structure to the United Methodist Church, also using the Episcopal Area. Note that the bishops govern the church as a single bench.
In the British Methodist Church and Irish Methodist Church,the closest equivalent to a diocese is the 'circuit'. Each local church belongs to a circuit, and the circuit is overseen by a superintendent minister who has pastoral charge of all the circuit churches (though in practice he or she delegates such charge to other presbyters who each care for a section of the circuit and chair the local church meetings as deputies of the superintendent). This echoes the practice of the early church where the bishop was supported by a bench of presbyters. Circuits are grouped together to form Districts. All of these, combined with the local membership of the Church, are referred to as the 'Connexion'. This 18th-century term, endorsed by John Wesley, describes how people serving in different geographical centres are 'connected' to each other. Personal oversight of the Methodist Church is exercised by the President of the Conference, a presbyter elected to serve for a year by the Methodist Conference; such oversight is shared with the Vice-President, who is always a deacon or layperson. Each District is headed by a 'Chair', a presbyter who oversees the district. Although the district is similar in size to a diocese, and Chairs meet regularly with their partner bishops, the Methodist superintendent is closer to the bishop in function than is the Chair. The purpose of the district is to resource the circuits; it has no function otherwise.
The Church of Jesus Christ of Latter-day Saints.
The Church of Jesus Christ of Latter-day Saints (LDS Church) has bishops, but not dioceses. LDS bishops are leaders of a ward. They serve a term, or calling, and are sustained by the members of the ward. The bishop and two councilors are called the bishopric. A ward is similar in size to a parish or congregation. Branches, which are smaller than a ward are led by a branch president.
Churches that have neither bishops nor dioceses.
Baptists.
Most Baptists hold that no church or ecclesiastical organization has inherent authority over a Baptist church. Churches can properly relate to each other under this polity only through voluntary cooperation, never by any sort of coercion. Furthermore, this Baptist polity calls for freedom from governmental control. Most Baptists believe in "Two offices of the church"—pastor-elder and deacon—based on certain scriptures (; ).
Exceptions to this local form of local governance include a few churches that submit to the leadership of a body of elders, as well as the Episcopal Baptists that have an Episcopal system.
History.
In the later organization of the Roman Empire, the increasingly subdivided provinces were administratively associated in a larger unit, the diocese (Latin "dioecesis", from the Greek term "διοίκησις", meaning "administration").
With the adoption of Christianity as the Empire's official religion in the 4th century, the clergy assumed official positions of authority alongside the civil governors. A formal church hierarchy was set up, parallel to the civil administration, whose areas of responsibility often coincided.
With the collapse of the Western Empire in the 5th century, the bishops in Western Europe assumed a large part of the role of the former Roman governors. A similar, though less pronounced, development occurred in the East, where the Roman administrative apparatus was largely retained by the Byzantine Empire. In modern times, many dioceses, though later subdivided, have preserved the boundaries of a long-vanished Roman administrative division. For Gaul, Bruce Eagles has observed that "it has long been an academic commonplace in France that the medieval dioceses, and their constituent "pagi", were the direct territorial successors of the Roman "civitates".
Modern usage of 'diocese' tends to refer to the sphere of a bishop's jurisdiction. This became commonplace during the self-conscious "classicizing" structural evolution of the Carolingian empire in the 9th century, but this usage had itself been evolving from the much earlier "parochia" ("parish"), dating from the increasingly formalised Christian authority structure in the 4th century (see "EB" 1911).

</doc>
<doc id="50550" url="http://en.wikipedia.org/wiki?curid=50550" title="Oakland, Maryland">
Oakland, Maryland

Oakland is a town in the west-central part of Garrett County, Maryland. The town has a population of 1,925 according to the 2010 United States Census. The town is also the county seat of Garrett County and is located within the Pittsburgh DMA.
Oakland is situated only miles from the source of the Potomac River, which flows directly into the Chesapeake Bay. It is also near the Wisp Resort at Deep Creek Lake, a major ski resort for many Marylanders and other visitors. The Autumn Glory Festival, a fall festival that attracts a great number of tourists, takes place in Oakland every October.
History.
Oakland was formally incorporated as a town in 1862.
The town is home to a historic B&O train station, which was listed on the National Register of Historic Places in 1973, and restored in the 2000s. Trains still run on the train tracks behind the station, but it is mainly used for special organizations or gatherings as of the present day. A gift shop is located within the station. In front of the station, there are a plethora of festivities that go on, mainly seasonal activities such as housing the town Christmas tree, decorating the plaza for a holiday, and sometimes parties.
Main Street of Oakland consists mainly of historic two to four story edifices that house the main shopping in the area, such as a theatre, museum, book store, a local pharmacy, antique shops, clothing stores and banks. Many of the homes and businesses in the downtown area are examples of Victorian architecture. Much of the central section of Oakland is part of the Oakland Historic District, listed on the National Register of Historic Places in 1984. Also listed on the National Register are the Garrett County Courthouse and Hoye Site.
One of the most prominent and historic churches in Oakland is St. Matthew's Episcopal Church, where U.S. Presidents Ulysses S. Grant, James Garfield, Grover Cleveland, and Benjamin Harrison have all attended services. Because of this, it is now called the "Church of Presidents." Another prominent and historic church is St. Peter the Apostle Church, a Catholic church located on Fourth Street. A large neoclassical courthouse is also very prominent and dominates the town center.
In the late 19th century and early 20th century, a large hotel named the Oakland Hotel was located near the downtown train station. It was constructed in 1878 by the B&O Railroad. The hotel was a major tourist attraction for that time period until it was torn down in the early 20th Century.
Geography.
Oakland is in the south-central to western portion of Garrett County, located at (39.410480, −79.404380). It is set in a small valley.
According to the United States Census Bureau, the town has a total area of 2.60 sqmi, of which 2.59 sqmi is land and 0.01 sqmi is water. It is only 6.61 miles from Second Street to Deep Creek Lake.
Oakland, due to its high elevation and valley location is among the coldest and snowiest locales in the state of Maryland, and has a warm-summer humid continental climate (Köppen "Dfb"), with 106 in of snowfall in an average season. The monthly mean temperature ranges from 25.1 °F in January to 68.4 °F in July, with temperatures not reaching above freezing on an average 34 days and falling to 0 °F or below on an average of 5.8 nights; from 1981 to 2010, only 13 years reached 90 °F. The average first and last dates for freezing temperatures are September 28 and May 15, respectively; for measurable (≥0.1 in) snowfall, they are November 13 and April 7. The state record low of -40 °F was recorded here on January 13, 1912; the record high is 101 °F on August 7, 1918, which, together with the preceding day, are the only two instances of 100 °F+ readings on record in Oakland. The most snow in 24 hours was 40.0 in on February 16, 1908.
Demographics.
2010 census.
As of the census of 2010, there were 1,925 persons, 875 households, and 470 families residing in the town. The population density was 743.2 PD/sqmi. There were 1,009 housing units at an average density of 389.6 /sqmi. The racial makeup of the town was 98.0% White, 0.2% African American, 0.3% Native American, 0.6% Asian, 0.1% from other races, and 0.9% from two or more races. Hispanic or Latino of any race were 0.7% of the population.
There were 875 households of which 23.1% had children under the age of 18 living with them, 40.1% were married couples living together, 10.5% had a female householder with no husband present, 3.1% had a male householder with no wife present, and 46.3% were non-families. 40.5% of all households were made up of individuals and 16.6% had someone living alone who was 65 years of age or older. The average household size was 2.03 and the average family size was 2.73.
The median age in the town was 46.9 years. 17.5% of residents were under the age of 18; 7.3% were between the ages of 18 and 24; 22.6% were from 25 to 44; 30.2% were from 45 to 64; and 22.5% were 65 years of age or older. The gender makeup of the town was 47.7% male and 52.3% female.
2000 census.
As of the census of 2000, there were 1,930 persons, 787 households, and 447 families residing in the town. The population density was 915.7 PD/sqmi. There were 918 housing units at an average density of 435.5 /sqmi. The racial makeup of the town was 98.13% White, 0.73% African American, 0.16% Native American, 0.57% Asian, 0.16% from other races, and 0.26% from two or more races. Hispanic or Latino of any race were 0.78% of the population.
There were 787 households out of which 25.3% had children under the age of 18 living with them, 43.6% were married couples living together, 9.8% had a female householder with no husband present, and 43.1% were non-families. 38.8% of all households were made up of individuals and 18.7% had someone living alone who was 65 years of age or older. The average household size was 2.09 and the average family size was 2.75.
In the town the population was spread out with 18.5% under the age of 18, 9.4% from 18 to 24, 24.0% from 25 to 44, 23.2% from 45 to 64, and 24.9% who were 65 years of age or older. The median age was 43 years. For every 100 females there were 85.8 males. For every 100 females age 18 and over, there were 84.2 males.
The median income for a household in the town was $26,728, and the median income for a family was $38,750. Males had a median income of $29,625 versus $21,542 for females. The per capita income for the town was $16,872. About 13.3% of families and 19.0% of the population were below the poverty line, including 21.9% of those under age 18 and 21.0% of those age 65 or over.

</doc>
<doc id="50552" url="http://en.wikipedia.org/wiki?curid=50552" title="James V of Scotland">
James V of Scotland

James V (10 April 1512 – 14 December 1542) was King of Scots from 9 September 1513 until his death, which followed the Scottish defeat at the Battle of Solway Moss. His only surviving legitimate child, Mary, succeeded him to the throne when she was just six days old.
Early life.
James was son of King James IV of Scotland and his queen Margaret Tudor, a daughter of Henry VII of England, and was the only legitimate child of James IV to survive infancy. He was born on 10 April 1512, at Linlithgow Palace, Linlithgowshire and christened the next day, receiving the titles Duke of Rothesay and Prince and Great Steward of Scotland. He became king at just seventeen months old when his father was killed at the Battle of Flodden Field on 9 September 1513.
James was crowned in the Chapel Royal at Stirling Castle on 21 September 1513. During his childhood, the country was ruled by regents, first by his mother, until she remarried the following year, and then by John Stewart, 2nd Duke of Albany, who was next in line to the throne after James and his younger brother, the posthumously-born Alexander Stewart, Duke of Ross. Other regents included Robert Maxwell, 5th Lord Maxwell, a member of the Council of Regency who was also bestowed as Regent of Arran, the largest island in the Firth of Clyde. In February 1517, James came from Stirling to Holyroodhouse, Edinburgh, but during an outbreak of plague in the city he was moved to the care of Antoine d'Arces at nearby rural Craigmillar Castle. At Stirling, the 10-year-old James had a guard of 20 footmen dressed in his colours, red and yellow. When he went to the park below the Castle, "by secret and in right fair and soft wedder (weather)," six horsemen would scour the countryside two miles roundabout for intruders. Poets wrote his own nursery rhymes, advising him on royal behaviour. William Stewart in his "Princelie Majestie" counselled against ice-skating:
To princes als it is ane vyce,
To ryd or run over rakleslie,
Or aventure to go on yce,
Accordis nocht to thy majestie.
In the autumn of 1524 James dismissed his Regents and was proclaimed an adult ruler by his mother. Several new court servants were appointed including a trumpeter, Henry Rudeman. The English diplomat, Thomas Magnus gave an impression of the new Scottish court at Holyroodhouse on All Saints' Day 1524; "trumpets and shamulles did sounde and blewe up mooste pleasauntely." Magnus saw the young king singing, with his horses, and playing with a spear at Leith, and was given the impression that he preferred English manners over French fashions. In 1525, Archibald Douglas, 6th Earl of Angus, the young king's stepfather, took custody of James and held him as a virtual prisoner for three years, exercising power on his behalf. There were several attempts made to free the young King - one was made by Walter Scott of Branxholme and Buccleuch, who ambushed the King's forces on 25 July 1526 at the battle of Melrose, and was routed off the field. Another attempt later that year, on 4 Sept at the battle of Linlithgow Bridge, failed again to relieve the King from the clutches of Angus. When James and his mother came to Edinburgh on 20 November 1526, she stayed in the chambers at Holyroodhouse which Albany had used, and James used the rooms above. In February 1527, Henry Fitzroy,
Duke of Richmond, gave James twenty hunting hounds and a huntsman. Magnus thought the Scottish servant sent to Sheriff Hutton Castle for the dogs was intended to note the form and fashion of the Duke's household, for emulation in Scotland. James finally escaped from Angus's care in 1528 and assumed the reins of government himself.
Reign and Religion.
His first action as king was to remove Angus from the scene. The Douglas family were forced into exile and James besieged their castle at Tantallon. He then subdued the Border rebels and the chiefs of the Western Isles. As well as taking advice from his nobility and using the services of the Duke of Albany in France and at Rome, James had a team of professional lawyers and diplomats, including Adam Otterburn and Thomas Erskine of Haltoun. Even his pursemaster and yeoman of the wardrobe, John Tennent of Listonschiels was sent on an errand to England, though he got a frosty reception.
James increased his income by tightening control over royal estates and from the profits of justice, customs and feudal rights. He also gave his illegitimate sons lucrative benefices, diverting substantial church wealth into his coffers. James spent a large amount of his wealth on building work at Stirling Castle, Falkland Palace, Linlithgow Palace and Holyrood and built up a collection of tapestries from those inherited from his father. James sailed to France for his first marriage and built up the royal fleet. In 1540 he sailed to Kirkwall in Orkney, then Lewis, in his ship the "Salamander", first making a will in Leith, knowing this to be, "uncertane aventuris." The purpose of this voyage was to show the royal presence and hold regional courts, called "justice ayres."
Domestic and international policy was affected by the Reformation, especially after Henry VIII broke from the Catholic Church. James V did not tolerate heresy and during his reign, a number of outspoken Protestants were persecuted. The most famous of these was Patrick Hamilton, who was burned at the stake as a heretic at St Andrews in 1528. Later in the reign, the English ambassador Ralph Sadler tried to encourage James to close the monasteries and take their revenue, so that he would not have to keep sheep like a mean subject. James replied that he had no sheep, he could depend on his god-father the King of France, and it was against reason to close the abbeys which, "stand these many years, and God's service maintained and kept in the same, and I might have anything I require of them." (Sadler knew that James did farm sheep on his estates.)
James recovered money from the church by getting Pope Clement VII to allow him to tax monastic incomes. He sent £50 to Johann Cochlaeus, a German opponent of Martin Luther, after receiving one of his books in 1534. On 19 January 1537 Pope Paul III sent James a blessed sword and hat symbolising his prayers that James would be strengthened against heresies from across the border. These gifts were delivered by the Pope's messenger while James was at Compiègne in France on 25 February 1537.
According to 16th-century writers, his treasurer James Kirkcaldy of Grange tried to persuade him against the persecution of Protestants and to meet Henry VIII at York. Although Henry VIII sent his tapestries to York in September 1541 ahead of a meeting, James did not come. The lack of commitment to this meeting was regarded by English observers as a sign that Scotland was firmly allied to France and Catholicism, particularly by the influence of Cardinal Beaton, Keeper of the Privy Seal, and a cause for war.
Marriages.
As early as August 1517, a clause of the Treaty of Rouen provided that if the Auld Alliance between France and Scotland was maintained, James should have a French royal bride. Yet the daughters of Francis I of France were promised elsewhere or sickly. Perhaps to remind Francis of his obligations, James's envoys began negotiations for his marriage elsewhere from the summer of 1529, both to Catherine de'Medici, the Duchess of Urbino, and Mary of Austria, Queen of Hungary, the sister of Holy Roman Emperor Charles V. But plans changed. In February 1533, two French ambassadors, Guillaume du Bellay, sieur de Langes, and Etienne de Laigue, sieur de Beauvais, who had just been in Scotland, told the Venetian ambassador in London that James was thinking of marrying Christina of Denmark.
Francis I insisted that his daughter Madeleine's health was too poor for marriage. Eventually, on 6 March 1536, a contract was made for James V to marry Mary of Bourbon, daughter of the Duke of Vendôme. She would have a dowry as if she were a French Princess. James decided to visit France in person. He sailed from Kirkcaldy on 1 September 1536, with the Earl of Argyll, the Earl of Rothes, Lord Fleming, David Beaton, the Prior of Pittenweem, the Laird of Drumlanrig and 500 others, using the "Mary Willoughby" as his flagship. First he visited Mary of Bourbon at St. Quentin in Picardy, but then went south to meet King Francis I. During his stay in France, in October 1536, James went boar-hunting at Loches with Francis, his son the Dauphin, the King of Navarre and Ippolito II d'Este.
James renewed the Auld Alliance and fulfilled the 1517 Treaty of Rouen on 1 January 1537 by marrying Madeleine of Valois, the king's daughter, in Notre Dame de Paris. The wedding was a great event: Francis I made a contract with six painters for the splendid decorations, and there were days of jousting at the Château du Louvre. At his entry to Paris, James wore a coat described as "sad cramasy velvet slashed all over with gold cut out on plain cloth of gold fringed with gold and all cut out, knit with horns and lined with red taffeta." James V so liked red clothing that, during the wedding festivities, he upset the city dignitaries who had sole right to wear that colour in processions. They noted he could not speak a word of French.
James and Madeleine returned from France on 19 May 1537, arriving at Leith, the king's Scottish fleet accompanied with ten great French ships. As the couple sailed northwards, some Englishmen had come aboard off Bridlington and Scarborough. While the fleet was off Bamburgh on 15 May, three English fishing boats supplied fish, and the King's butcher landed in Northumbria to buy meat. The English border authorities were dismayed by this activity.
Madeleine did not enjoy good health. In fact, she was consumptive and died soon after arrival in Scotland in July 1537. Spies told Thomas Clifford, the Captain of Berwick, that James omitted "all manner of pastime and pleasure," but continually oversaw the maintenance of his guns, going twice a week secretly to Dunbar Castle with six companions. James then proceeded to marry Mary of Guise, daughter of Claude, Duke of Guise, and widow of Louis II d'Orléans, Duke of Longueville, by proxy on 12 June 1538. Mary already had two sons from her first marriage, and the union produced two sons. However, both died in April 1541, just eight days after baby Robert was baptised. Their daughter and James's only surviving legitimate child, Mary, was born in 1542 at Linlithgow Palace.
Outside interests.
According to legend, James was nicknamed "King of the Commons" as he would sometimes travel around Scotland disguised as a common man, describing himself as the "Gudeman of Ballengeich" ('Gudeman' means 'landlord' or 'farmer', and 'Ballengeich' was the nickname of a road next to Stirling Castle – meaning 'windy pass' in Gaelic). James was also a keen lute player. In 1562 Sir Thomas Wood reported that James had "a singular good ear and could sing that he had never seen before" (sight-read), but his voice was "rawky" and "harske." At court, James maintained a band of Italian musicians who adopted the name Drummond. These were joined for the winter of 1529/30 by a musician and diplomat sent by the Duke of Milan, Thomas de Averencia de Brescia, probably a lutenist. The historian Andrea Thomas makes a useful distinction between the loud music provided at ceremonies and processionals and instruments employed for more private occasions or worship; the "music fyne" described by Helena Mennie Shire. This quieter music included a consort of viols played by four Frenchmen led by Jacques Columbell. It seems certain that David Peebles wrote music for James V and probable that the Scottish composer Robert Carver was in royal employ, though evidence is lacking.
As a patron of poets and authors James supported William Stewart and John Bellenden the son of his nurse, who translated the Latin "History of Scotland" compiled in 1527 by Hector Boece into verse and prose. Sir David Lindsay of the Mount, the Lord Lyon, head of the Lyon Court and diplomat, was a prolific poet. He produced an interlude at Linlithgow Palace thought to be a version of his play "The Thrie Estaitis" in 1540. James also attracted the attention of international authors. The French poet Pierre de Ronsard, who had been a page of Madeleine of Valois, offered unqualified praise;
"Son port estoit royal, son regard vigoureux
De vertus, et de l'honneur, et guerre amoureux
La douceur et la force illustroient son visage
Si que Venus et Mars en avoient fait partage"
His royal bearing, and vigorous pursuit
of virtue, of honour, and love's war,
this sweetness and strength illuminate his face,
as if he were the child of Venus and Mars.
James was poet himself including "The Gaberlunzieman" and "The Jolly Beggar"
When he married Mary of Guise, Giovanni Ferrerio, an Italian scholar who had been at Kinloss Abbey in Scotland, dedicated to the couple a new edition of his work, "On the true significance of comets against the vanity of astrologers." Like Henry VIII, James employed many foreign artisans and craftsmen in order to enhance the prestige of his renaissance court. Robert Lindsay of Pitscottie listed their professions;he plenished the country with all kind of craftsmen out of other countries, as French-men, Spaniards, Dutch men, and Englishmen, which were all cunning craftsmen, every man for his own hand. Some were gunners, wrights, carvers, painters, masons, smiths, harness-makers (armourers), tapesters, broudsters, taylors, cunning chirugeons, apothecaries, with all other kind of craftsmen to apparel his palaces. One technological initiative was a special mill for polishing armour at Holyroodhouse next to his mint. The mill had a pole drive 32 feet long powered by horses. Mary of Guise's mother Antoinette of Bourbon sent him an armourer. The armourer made steel plates for his jousting saddles in October 1538, and delivered a skirt of plate armour in February 1540. In the same year, for his wife's coronation, the treasurer's accounts record that James personally devised fireworks made by his master gunners. When James took steps to suppress the circulation of slanderous ballads and rhymes against Henry VIII, Henry sent Fulke ap Powell, Lancaster Herald, to give thanks and to make arrangements for the present of a lion for James's menagerie of exotic pets.
War with England.
The death of James's mother in 1541 removed any incentive for peace with England, and war broke out. Initially the Scots won a victory at the Battle of Haddon Rig in August 1542. The Imperial ambassador in London, Eustace Chapuys, wrote on 2 October that the Scottish ambassadors ruled out a conciliatory meeting between James and Henry VIII in England until the pregnant Mary of Guise delivered her child. Henry would not accept this condition and mobilised his army against Scotland.
James was with his army at Lauder on 31 October 1542. Although he hoped to invade England, his nobles were reluctant.
He returned to Edinburgh on the way writing a letter in French to his wife from Falahill mentioning he had three days of illness. Next month his army suffered a serious defeat at the Battle of Solway Moss. He took ill shortly after this, on 6 December; by some accounts this was a nervous collapse caused by the defeat, although some historians consider that it may just have been an ordinary fever. John Knox later described his final movements in Fife. Whatever the cause of his illness, he was on his deathbed at Falkland Palace when his only surviving legitimate child, a girl, was born. Sir George Douglas of Pittendreich brought the news of the king's death to Berwick. He said James died at midnight on Thursday 15 December; the king was talking but delirious and spoke no "wise words." According to George Douglas in his delirium James lamented the capture of his banner and Oliver Sinclair at Solway Moss more than his other losses. An English chronicler suggested another cause of the king's grief was his discomfort on hearing of the murder of the English Somerset Herald, Thomas Trahern, at Dunbar. James was buried at Holyrood Abbey in Edinburgh.
 This was either a reference to the Stewart dynasty's accession to the throne through Marjorie Bruce, daughter of Robert the Bruce or to the medieval origin myth of the Scots nation, recorded in the Scotichronicon in which the Scots people are descended from the Princess Scota.
Aftermath.
James was succeeded by his infant daughter Mary. He was buried at Holyrood Abbey alongside his first wife Madeleine and his two sons in January 1543. David Lindsay supervised the construction of his tomb. One of his French artists, Andrew Mansioun, carved a lion and an inscription in Roman letters measuring eighteen feet. The tomb was destroyed in the sixteenth-century, according to William Drummond of Hawthornden as early as 1544, by the English during the burning of Edinburgh. Scotland was ruled by Regent Arran and was soon drawn into the war of the Rough Wooing.
Issue.
"By" Madeleine of Valois
"By" Mary of Guise
Additionally, James V had nine known illegitimate children, at least three of whom were fathered before the age of 20. The young King was said to have been encouraged in his amorous affairs by the Angus regime to keep him distracted from politics. In addition to these aristocratic liaisons, David Lindsay described the king's other affairs in his poem, "The Answer to the Kingis Flyting"; 'ye be now strang lyke ane elephand, And in till Venus werkis maist vailyeand.'
Many of the sons of his aristocratic mistresses entered ecclesiastical careers. Pope Clement VII sent a dispensation to James V dated 30 August 1534, allowing four of the children to take holy orders when they came of age. The document stated that James elder was in his fifth year, James younger and John in their third year, and Robert in his first year.
Titles, styles, honours, and arms.
James's full style prior to acceding the throne was "Prince James Stewart, Duke of Rothesay, Earl of Carrick, Lord Renfrew, Prince and Great Steward of Scotland"
Fictional portrayals.
James V has been depicted in historical novels, poems and short stories. They include:
External links.
 Media related to at Wikimedia Commons
 incorporates text from a publication now in the public domain: 

</doc>
<doc id="50553" url="http://en.wikipedia.org/wiki?curid=50553" title="Pseudotsuga">
Pseudotsuga

Pseudotsuga is a genus of evergreen coniferous trees in the family Pinaceae. Common names include Douglas fir, Douglas-fir, Douglas tree, and Oregon pine. "Pseudotsuga menziesii" is widespread in western North America and is an important source of timber. The number of species has long been debated, but two in western North America and two to four in eastern Asia are commonly acknowledged. Nineteenth-century botanists had problems in classifying Douglas-firs, due to the species' similarity to various other conifers better known at the time; they have at times been classified in "Pinus", "Picea", "Abies", "Tsuga", and even "Sequoia". Because of their distinctive cones, Douglas-firs were finally placed in the new genus "Pseudotsuga" (meaning "false hemlock") by the French botanist Carrière in 1867. The genus name has also been hyphenated as "Pseudo-tsuga".
Name.
The common name Douglas-fir honors David Douglas, the Scottish botanist who first introduced "Pseudotsuga menziesii" into cultivation at Scone Palace in 1827. Douglas is known for introducing many North American native conifers to Europe. The hyphen in the name indicates that Douglas-firs are not true firs, not being members of the genus "Abies". Most dictionaries don't recognize this distinction, however, including Merriam-Webster and Webster's New World College Dictionary.
Description.
Douglas-firs are medium-size to extremely large evergreen trees, 20 - tall (although only Coast Douglas-firs reach such great height). The leaves are flat, soft, linear, 2 - long, generally resembling those of the firs, occurring singly rather than in fascicles; they completely encircle the branches, which can be useful in recognizing the species. The female cones are pendulous, with persistent scales (unlike true firs), and are distinctive in having a long tridentine (three-pointed) bract that protrudes prominently above each scale (it resembles the back half of a mouse, with two feet and a tail).
The Coast Douglas-fir has attained heights of 393 feet (120 m). That was the estimated height of the tallest conifer ever well-documented, the Mineral Tree (Mineral, Washington), measured in 1924 by Dr. Richard E. McArdle, former chief of the U.S. Forest Service. The volume of that tree was 515 m3. The tallest living individual is the Brummitt (Doerner) Fir in Coos County, Oregon, 99.4 m tall. Only Coast Redwood and "Eucalyptus regnans" reach greater heights based on current knowledge of living trees (379 and 331 feet, respectively).
At Quinault, Washington, is found a collection of the largest Douglas-firs in one area. Quinault Rain Forest hosts the most of the top ten known largest Douglas-firs.
s of 2009[ [update]], the largest known Douglas-firs in the world are, by volume:
Species and varieties.
By far the best-known is the very widespread and abundant North American species "Pseudotsuga menziesii", a taxonomically complex species divided into two major varieties (treated as distinct species or subspecies by some botanists): coast Douglas-fir or "green Douglas-fir", on the Pacific coast; and Rocky Mountain Douglas-fir or "interior Douglas-fir", in the interior west of the continent. According to some botanists, Rocky Mountain Douglas-fir extends south into Mexico to include all Mexican Douglas-fir populations, whereas others have proposed multiple separate species in Mexico and multiple varieties in the United States. Morphological and genetic evidence suggest that Mexican Douglas-fir should probably be considered a distinct variety within "P. menziesii".
All of the other species are of restricted range and little-known outside of their respective native environments, where they are often rare and of scattered occurrence in mixed forests; all those have unfavorable conservation status. The taxonomy of the Asian Douglas-firs continues to be disputed, but the most recent taxonomic treatment accepts four species: three Chinese and one Japanese. The three Chinese species have been variously considered varieties of "P. sinensis" or broken down into additional species and varieties. In the current treatment, the Chinese species "P. sinensis" is further subdivided into two varieties: var. "sinensis" and var. "wilsoniana".
Uses.
Douglas-fir wood is used for structural applications that are required to withstand high loads. It is used extensively in the construction industry. Other examples include its use for homebuilt aircraft such as the RJ.03 IBIS canard. Very often, these aircraft were designed to utilize Sitka spruce, which is becoming increasingly difficult to source in aviation quality grades. Oregon pine is also used in boat building when it is available in long, fairly knot-free lengths. Most timber now comes from plantation forests in North America which are managed to produce faster growing timber with fewer knots. This timber is generally lighter but weaker. Traditionally, Oregon pine was used in mast building due to its ability to resist bending loads without fracturing. This was based on using older native forest wood with a high number of growth rings per inch. This sort of wood is seldom available new but can be sourced from merchants dealing in recycled timber. Native Oregon pine is considerably heavier than Sitka spruce, which is about the same weight as Western red cedar, but with far better bending characteristics than cedar. Large-sized Oregon pine, as used in beams, is inclined to split as it dries, like oak, but this does not reduce its strength.
Douglas-fir is one of the most commonly marketed Christmas tree species in the United States, where they are sold alongside firs like Noble fir and Grand fir. Douglas-fir Christmas trees are usually trimmed to a near perfect cone instead of left to grow naturally like Noble and Grand firs.
Pests and diseases.
Douglas-firs are used as food plants by the larvae of some Lepidoptera species, including Autumnal Moth, Bordered White, The Engrailed, Pine Beauty and Turnip Moth. The gelechiids "Chionodes abella" and "Chionodes periculella" and the tortrix moth "Cydia illutana" have been specifically recorded on "P. menziesii".
Culture.
A California Native American myth explains that each three-ended bract is the tail and two tiny legs of a mouse that hid inside the scales of the tree's cones during forest fires, and the tree was kind enough to be its enduring sanctuary.
A Douglas-fir species, "Pseudotsuga menziesii", is the state tree of Oregon.
In the American television series "Twin Peaks", primary character Special Agent Dale Cooper displays a fascination with the firs upon arriving in the titular town.
The Douglas fir is featured at the center of the flag of the Cascadian independence movement.

</doc>
<doc id="50555" url="http://en.wikipedia.org/wiki?curid=50555" title="Domoic acid">
Domoic acid

Domoic acid (DA) is a kainic acid analog neurotoxin that causes amnesic shellfish poisoning (ASP). It is produced by algae and accumulates in shellfish, sardines, and anchovies. When sea lions, otters, cetaceans, humans etc., then eat contaminated animals poisoning may result. Exposure to the biotoxin affects the brain, causing seizures, and possibly death.
History.
There has been little use of domoic acid throughout history except for in Japan, where it has been used as an anthelmintic for centuries.
Domoic acid was first isolated in 1959 from a species of red algae, chondria armata, in Japan; commonly referred to as “doumoi” or “hanyanagi”. Poisonings in history have been rare, or undocumented; however, it is thought that the increase in human activates is resulting in an increasing frequency of toxic algal blooms along coastlines in recent years. Consequently, poisonings have been affecting sea animal, birds, and humans.
It has been speculated that the story in Exodus, from the Bible, of Moses turning the Nile in to blood, may have been attributed to a red algal bloom that produced domoic acid. 
In 1961 seabirds attacked the Santa Cruz and Capitola areas in California, and though it was never confirmed, they were thought to be under the influence of domoic acid.
In 1987, Prince Edward Island, Canada, there was a shellfish poisoning resulting in 3 deaths. The culprit was domoic acid contaminated blue mussels (Mytulis edulis).
An incident in which domoic acid may have been involved took place on June 22, 2006, when a California brown pelican flew through the windshield of a car on the Pacific Coast Highway.
Chemistry.
General.
Domoic acid is a structural analog of kainic acid and proline. Ohfune and Tomita, who wanted to investigate its absolute stereochemistry, were the first and only to synthesized domoic acid in 1982. 
Biosynthesis.
In 1999, using 13C- and 14C-labelled precursors, the biosynthesis of domoic acid in the diatom Pseudo-nitzschia was examined. After addition of [1,2-13C2]-acetate, NMR spectroscopy showed enrichment of every carbon in domoic acid. This enrichment was consistent with 2 biosynthetic pathways. The labeling pattern determined that domoic acid can be biosynthesized by an isoprenoid intermediate in combination with a tricarboxylic acid (TCA) cycle intermediate.
Synthesis.
Using intermediates 5 and 6, a Diels-Alder reaction produced a bicyclic compound (7). 7 then underwent ozonolysis to open the six-membered ring leading to selenide (8). 8 was then deselenated to form 9 (E-9 and Z-9), lastly leading to the formation of (-) domoic acid. 
Mechanism of Action.
The effects of domoic acid have been attributed to several mechanisms, but the one of concern is through glutamate receptors. Domoic acid is an excitatory amino acid analogue of glutamate; a neurotransmitter in the brain that activates glutamate receptors. Domoic acid has a very strong affinity for these receptors, which results in excitotoxicity initiated by an integrative action on ionotropic glutamate receptors at both sides of the synapse blocking the channel from rapid desensitization. In addition there is a synergistic effect with endogenous glutamate and N-Methyl-D-aspartate receptor agonists that contribute to the excitotoxicity. 
In the brain, domoic acid especially damages the hippocampus and amygdaloid nucleus. It damages the neurons by activating AMPA and kainate receptors, causing an influx of calcium. Although calcium flowing into cells is normal, the uncontrolled increase of calcium causes the cells to degenerate. Because the hippocampus may be severely damaged, short-term memory loss occurs. It may also cause kidney damage – even at levels considered safe for human consumption, a new study in mice has revealed. The kidney is affected at a hundred times lower than the concentration allowed under FDA regulations.
Toxicology.
Domoic acid producing algal blooms are associated with the phenomenon of amnesic shellfish poisoning (ASP). Domoic acid can bioaccumulate in marine organisms such as shellfish, anchovies, and sardines that feed on the phytoplankton known to produce this toxin. It can accumulate in high concentrations in the tissues of these plankton feeders when the toxic phytoplankton are high in concentration in the surrounding waters.
Domoic acid is a neurotoxin that inhibits neurochemical processes, causing short-term memory loss, brain damage, and, in severe cases, death in humans. In marine mammals, domoic acid typically causes seizures and tremors.
Studies have shown that there are no symptomatic effects in humans at levels of 0.5mg/kg of body weight. In the 1987 domoic acid poisoning on Prince Edward Island concentrations ranging from 0.31-1.28mg/g of muscle tissue were noted in people that became ill (three of which died).Dangerous levels of domoic acid have been calculated based on cases such as the one on Prince Edward island, however the exact LD50 for humans is unknown; for mice the LD50 is 3.6mg/kg. 
New research has found that domoic acid is heat resistant and very stable toxin, which can damage kidneys at concentrations that are 100 times lower than what causes neurological effects.
Diagnosis and Prevention.
In order to be diagnosed and treated if poisoned, domoic acid must first be detected. Methods such as ELISA (enzyme-linked immunosorbent assay), or probe development with PCR (polymerase chain reaction) may be used.
There is no known antidote available for domoic acid therefore if poisoning occurs, it is advised to go quickly to a hospital. 
It is important to note that cooking or freezing affected fish or shellfish tissue that are contaminated with domoic acid does not lessen the toxicity. 
As a public health concern, the concentration of domoic acid in shellfish and shellfish parts at point of sale should not exceed the current permissible limit of 20mg/g tissue. In addition during processing shellfish, it is important to pay attention to environmental condition factors.
Pop Culture.
On August 18, 1961, in Capitola and Santa Cruz, California there was an invasion of what people described as chaotic seabirds. These birds were believed to be under the influence of domoic acid, and it inspired a scene in Alfred Hitchcock’s feature film "The Birds" (1963).
In addition domoic acid was used to poison a witness in the TV series "Elementary", episode "The Red Team".

</doc>
<doc id="50556" url="http://en.wikipedia.org/wiki?curid=50556" title="James IV of Scotland">
James IV of Scotland

James IV (17 March 1473 – 9 September 1513) was the King of Scots from 11 June 1488 to his death. He is generally regarded as the most successful of the Stewart monarchs of Scotland, but his reign ended with the disastrous defeat at the Battle of Flodden Field, where he became the last monarch from not only Scotland, but also from all of Great Britain, to be killed in battle.
Early life.
James was the son of James III and Margaret of Denmark, probably born in Stirling Castle. As heir apparent to the Scottish crown, he became Duke of Rothesay. In 1474, his father arranged his betrothal to Princess Cecily of England. His father was not a popular king and faced two major rebellions during his reign. The marriage negotiations and dowry payments led to the invasion of Scotland and capture of Berwick in 1482 by his uncle Alexander, Duke of Albany and Richard, Duke of Gloucester while James remained at Stirling. James III's army rebelled against him and the English army reached Edinburgh.
During the second rebellion, the rebels set up the 15-year-old James as their nominal leader. His father was killed fighting rebels at the Battle of Sauchieburn on 11 June 1488, and James took the throne and was crowned at Scone on 24 June. When he realised the indirect role which he had played in the death of his father, he decided to do penance for his sin. From that date on, he wore a heavy iron chain cilice around his waist, next to the skin, each Lent as penance, adding every year extra ounces.
Reign.
Politics.
James IV quickly proved an effective ruler and wise king. He defeated another rebellion in 1489, took direct interest in the administration of justice and finally brought the Lord of the Isles under control in 1493. For a time, he supported Perkin Warbeck, pretender to the English throne, and carried out a brief invasion of England on his behalf in September 1496. Then, in August 1497, James laid siege to Norham Castle, using his Grandfather's bombard Mons Meg.
James recognized nonetheless that peace between Scotland and England was in the interest of both countries, and established good diplomatic relations with England, which was at that time emerging from a period of civil war. First he ratified the Treaty of Ayton in February 1498. Then, in 1502 James signed the Treaty of Perpetual Peace with Henry VII. He also maintained his relations with France. With rumours that James would renew the auld alliance, in April 1508 Thomas Wolsey was sent to discuss Henry VII's concerns over this. Wolsey found "there was never a man worse welcome into Scotland than I, ... they keep their matters so secret here that the wives in the market know every cause of my coming."
James saw the importance in building a fleet that could provide Scotland with a strong maritime presence. James founded two new dockyards for the purpose and acquired a total of 38 ships for the Royal Scots Navy, including the "Margaret", and the carrack "Michael" or "Great Michael". The latter, built at great expense at Newhaven and launched in 1511, was 240 ft in length, weighed 1,000 tons and was, at that time, the largest ship in Europe.
Culture.
James IV was a true Renaissance prince with an interest in practical and scientific matters. He granted the Incorporation of Surgeons and Barbers of Edinburgh (later the Royal College of Surgeons of Edinburgh) a royal charter in 1506, turned Edinburgh Castle into one of Scotland's foremost gun foundries, and welcomed the establishment of Scotland's first printing press in 1507. He built a part of Falkland Palace, Great Halls at Stirling and Edinburgh castles, and furnished his palaces with tapestries. James was a patron of the arts, including many literary figures, most notably the Scots makars whose diverse and socially observant works convey a vibrant and memorable picture of cultural life and intellectual concerns in the period. Figures associated with his court include William Dunbar, Walter Kennedy and Gavin Douglas, who made the first complete translation of Virgil's Aeneid in northern Europe. His reign also saw the passing of the makar Robert Henryson.
James was well educated and a fluent polyglot. In July 1498 the Spanish envoy Pedro de Ayala reported to Ferdinand and Isabella that;The King is 25 years and some months old. He is of noble stature, neither tall nor short, and as handsome in complexion and shape as a man can be. His address is very agreeable. He speaks the following foreign languages ; Latin, very well ; French, German, Flemish, Italian, and Spanish ; Spanish as well as the Marquis, but he pronounces it more distinctly. He likes, very much, to receive Spanish letters. His own Scots language is as different from English as Aragonese from Castilian. The King speaks, besides, the language of the savages who live in some parts of Scotland and on the islands. It is as different from Scots as Biscayan is from Castilian. His knowledge of languages is wonderful. He is well read in the Bible and in some other devout books. He is a good historian. He has read many Latin and French histories, and profited by them, as he has a very good memory. He never cuts his hair or his beard. It becomes him very well.
James IV was the last King of Scots known to have spoken Scottish Gaelic. James is one of the rulers reported to have conducted a language deprivation experiment, sending two children to be raised by a mute woman alone on the island of Inchkeith, to determine if language was learned or innate. At Stirling Castle, James maintained an alchemical workshop with a furnace of the quintessence. The project consumed quantities of quick-silver, golden litharge, and tin. It was said that one of his alchemists Father Damian attempted to fly from Stirling Castle.
Policy in the Highlands and Isles.
In May 1493 John MacDonald, Lord of the Isles, was forfeited by the Parliament of Scotland. King James himself sailed to Dunstaffnage Castle, where the western chiefs made their submission to him. John surrendered and was brought back as a pensioner to the royal court, then lived at Paisley Abbey. The Highlands and Islands now fell to direct royal control. John's grandson Domhnall Dubh (Donald Owre), one of the possible claimants to the Lordship was peaceable, but the other, his nephew Alexander MacDonald of Lochalsh invaded Ross and was later killed on the island of Oronsay in 1497.
In October 1496 the Royal Council ordered that the Clan Chiefs in the region would be held responsible by the king for crimes of the islanders. This act for the governance of the region was unworkable, and after the Act of Revocation of 1498 undermined the Chief's titles to their lands, resistance to Edinburgh rule was strengthened. James waited at Kilkerran Castle at Campbeltown Loch to re-grant the Chief's charters in the summer of 1498. Few of the Chiefs turned up. At first, Archibald Campbell, 2nd Earl of Argyll, was set to fill the power vacuum and enforce royal authority, but he met with limited success in a struggle with his brother-in-law, Torquil MacLeod of Lewis. Torquil was ordered to hand over Donald Dubh, heir to the lordship of the Isles, to James IV at Inverness in 1501. James waited, but Torquil never came.
After this defiance, Alexander Gordon, 3rd Earl of Huntly, was granted Torquil's lands. He raised an army in Lochaber and also cleared the tenants of that area, replacing them with his supporters. After the parliament of 1504, a royal fleet sailed north from Ayr to attack the Castle of Cairn-na-Burgh, west of Mull, where, it is thought, Maclean of Duart had Donald Dubh in his keeping. As progress at the siege was slow, James sent Hans the royal gunner in Robert Barton's ship and then the Earl of Arran with provisions and more artillery. Cairn-na-Burgh was captured by June 1504 but Donald Dubh remained at liberty. In September 1507, Torquil MacLeod was besieged at Stornoway Castle on Lewis. Donald Dubh was captured and kept in prison for the rest of life, and Torquil MacLeod died in exile in 1511. The Earl of Huntly was richly rewarded for his troubles, a price that James was prepared to pay.
War and death.
When war broke out between England and France as a result of the Italian Wars, James found himself in a difficult position as an ally by treaty both to France and England. Henry VIII of England invaded France, and James reacted by declaring war on England. Pope Leo X sent a letter to James threatening him with ecclesiastical censure for breaking peace treaties on 28 June 1513, and subsequently James was excommunicated by Cardinal Christopher Bainbridge. James summoned sailors and sent the Scottish navy, including the "Great Michael" to join the ships of Louis XII of France, so joining in the war of the League of Cambrai. Hoping to take advantage of Henry's absence at the siege of Thérouanne, he led an invading army southward into Northumbria, only to be killed, with many of his nobles and common soldiers, at the disastrous Battle of Flodden on 9 September 1513.
Both English and Scottish accounts of Flodden emphasise the King's determination to fight. In his otherwise flattering portrayal of James, Pedro de Ayala remarks on James's ability as a military commander, portraying him as brusque and fearless on the battlefield;He is courageous, even more so than a king should be. I am a good witness of it. I have seen him often undertake most dangerous things in the last wars. On such occasions he does not take the least care of himself. He is not a good captain, because he begins to fight before he has given his orders. He said to me that his subjects serve him with their persons and goods, in just and unjust quarrels, exactly as he likes, and that therefore he does not think it right to begin any warlike undertaking without being himself the first in danger. His deeds are as good as his words.
A body, thought to be that of James, was recovered from the battlefield and taken to London for burial. James had been excommunicated, and although Henry VIII had obtained a breve from the Pope on 29 November 1513 to have the King buried in consecrated ground at St. Pauls, the embalmed body lay unburied for many years at Sheen Priory in Surrey. The body was lost after the Reformation, which led to the demolition of the priory. John Stow claimed to have seen it, and said the king's head (with red hair) was removed by a glazier and eventually buried at St Michael Wood Street. The church was later demolished and the site redeveloped many times; it is now occupied by a pub. James's bloodstained coat was sent to Henry VIII (then on campaign in France) by his queen, Catherine of Aragon.
Erasmus provided an epitaph for the King in his "Adagia." Later, in 1533, he wrote to James V of Scotland pointing out this essay on duty under the adage "Spartam nactus es", (You who were born to Sparta shall serve her), on the subject of the Flodden campaign and the death of James and his son, Alexander.
Legends of the King's resting place.
However rumours persisted for many years that James had survived and had gone into exile, or his body was buried in Scotland, with no evidence to support them. Two castles in the Scottish Borders are claimed to be the real resting place of James. These stories follow the legend that, prior to the Scots charge at Flodden, James had ripped off his royal surcoat to show his nobles that he was prepared to fight as an ordinary man at arms. What was reputed to be James' IV body recovered by the English did not have the iron chain round its waist. (So some historians claimed he removed his chain while dallying in Lady Heron's bedroom.) However, Border legend claimed that during the battle of Flodden four Home horsemen or supernatural riders swept across the field snatching up the King's body as such a prize could not be allowed to fall into English hands after such a humiliating defeat, or that the King left the field alive and was killed soon after. In the 18th century when the medieval well of Hume Castle was being cleared the skeleton of a man with a chain round his waist was discovered in a side cave. Unfortunately this skeleton has since disappeared. Another version of this tale has the skeleton discovered at Hume a few years after the battle and re-interred at Holyrood Abbey. Exactly the same story was told for Roxburgh Castle, the skeleton there discovered in the 17th century. Yet another tradition is the discovery of the royal body at Berry Moss, near Kelso. Fuelling these legends, Robert Lindsay of Pitscottie writing in the 1570s claimed that a convicted criminal offered to show Regent Albany the King's grave ten years after the battle, but Albany refused.
Marriage.
His early betrothal to Cecily of England came to nothing, but interest in an English marriage remained.
In a ceremony at the altar of Glasgow Cathedral on 10 December 1502, James confirmed the Treaty of Perpetual Peace with Henry VII of England. By this treaty James married Henry's daughter Margaret Tudor. After a wedding by proxy in London, the marriage was confirmed in person on 8 August 1503 at Holyrood Abbey, Edinburgh. Their wedding was commemorated by the gift of a Book of Hours.
The union produced four children plus two stillbirths:
Illegitimate children.
James also had eight illegitimate children with four different mistresses:
Fictional portrayals.
James IV has been depicted in historical novels and short stories. They include:
References.
Primary Sources

</doc>
<doc id="50557" url="http://en.wikipedia.org/wiki?curid=50557" title="Phytoplankton">
Phytoplankton

Phytoplankton are the autotrophic components of the plankton community and a key factor of oceans, seas and freshwater basins ecosystems. The name comes from the Greek words φυτόν ("phyton"), meaning "plant", and πλαγκτός ("planktos"), meaning "wanderer" or "drifter". Most phytoplankton are too small to be individually seen with the unaided eye. However, when present in high enough numbers, some varieties may be noticeable as a green discoloration of the water; this is due to the presence of chlorophyll within their cells, and in some species also due to the presence of accessory pigments such as phycobiliproteins, xanthophylls, etc.).
Ecology.
Phytoplankton are photosynthesizing microscopic organisms that inhabit the upper sunlit layer of almost all oceans and bodies of fresh water. They are agents for "primary production," the creation of organic compounds from carbon dioxide dissolved in the water, a process that sustains the aquatic food web. Phytoplankton obtain energy through the process of photosynthesis and must therefore live in the well-lit surface layer (termed the euphotic zone) of an ocean, sea, lake, or other body of water. Phytoplankton account for half of all photosynthetic activity on Earth. Their cumulative energy fixation in carbon compounds (primary production) is the basis for the vast majority of oceanic and also many freshwater food webs (chemosynthesis is a notable exception). The effects of anthropogenic warming on the global population of phytoplankton is an area of active research. Changes in the vertical stratification of the water column, the rate of temperature-dependent biological reactions, and the atmospheric supply of nutrients are expected to have important effects on future phytoplankton productivity. Additionally, changes in the mortality of phytoplankton due to rates of zooplankton grazing may be significant. As a side note, one of the more remarkable food chains in the ocean – remarkable because of the small number of links – is that of phytoplankton-feeding krill (a crustacean similar to a tiny shrimp) feeding baleen whales.
Phytoplankton are also crucially dependent on minerals. These are primarily macronutrients such as nitrate, phosphate or silicic acid, whose availability is governed by the balance between the so-called biological pump and upwelling of deep, nutrient-rich waters. However, across large regions of the World Ocean such as the Southern Ocean, phytoplankton are also limited by the lack of the micronutrient iron. This has led to some scientists advocating iron fertilization as a means to counteract the accumulation of human-produced carbon dioxide (CO2) in the atmosphere. Large-scale experiments have added iron (usually as salts such as iron sulphate) to the oceans to promote phytoplankton growth and draw atmospheric CO2 into the ocean. However, controversy about manipulating the ecosystem and the efficiency of iron fertilization has slowed such experiments.
Phytoplankton depend on other substances to survive as well. In particular, Vitamin B is crucial. Areas in the ocean have been identified as having a major lack of Vitamin B, and correspondingly, phytoplankton.
While almost all phytoplankton species are obligate photoautotrophs, there are some that are mixotrophic and other, non-pigmented species that are actually heterotrophic (the latter are often viewed as zooplankton). Of these, the best known are dinoflagellate genera such as "Noctiluca" and "Dinophysis", that obtain organic carbon by ingesting other organisms or detrital material.
The term phytoplankton encompasses all photoautotrophic microorganisms in aquatic food webs. Phytoplankton serve as the base of the aquatic food web, providing an essential ecological function for all aquatic life. However, unlike terrestrial communities, where most autotrophs are plants, phytoplankton are a diverse group, incorporating protistan eukaryotes and both eubacterial and archaebacterial prokaryotes. There are about 5,000 known species of marine phytoplankton. There is uncertainty in how such diversity has evolved in an environment where competition for only a few resources would suggest limited potential for niche differentiation.
In terms of numbers, the most important groups of phytoplankton include the diatoms, cyanobacteria and dinoflagellates, although many other groups of algae are represented. One group, the coccolithophorids, is responsible (in part) for the release of significant amounts of dimethyl sulfide (DMS) into the atmosphere. DMS is oxidized to form sulfate which, in areas where ambient aerosol particle concentrations are low, can contribute to the population of cloud condensation nuclei, mostly leading to increased cloud cover and cloud albedo according to the so-called CLAW Hypothesis.
 In oligotrophic oceanic regions such as the Sargasso Sea or the South Pacific Gyre, phytoplankton is dominated by the small sized cells, called picoplankton, mostly composed of cyanobacteria ("Prochlorococcus", "Synechococcus") and picoeucaryotes such as "Micromonas".
Growth strategy.
In the early twentieth century, Alfred C. Redfield found the similarity of the phytoplankton’s elemental composition to the major dissolved nutrients in the deep ocean. Redfield proposed that the ratio of nitrogen to phosphorus (16:1) in the ocean was controlled by the phytoplankton’s requirements which subsequently release nitrogen and phosphorus as they are remineralized. This so-called “Redfield ratio” in describing stoichiometry of phytoplankton and seawater has become a fundamental principle to understand the marine ecology, biogeochemistry and phytoplankton evolution. However, Redfield ratio is not a universally value and it may diverge due to the changes in exogenous nutrient delivery and microbial metabolisms in the ocean, such as nitrogen fixation, denitrification and anammox.
The dynamic stoichiometry shown in unicellular algae reflects their capability to stockpile nutrients in internal pool, shift between enzymes with various nutrient requirements and alter osmolyte composition. Different cellular components have their own unique stoichiometry characteristics, for instance, resource (light or nutrients) acquisition machinery such as proteins and chlorophyll contain high concentration of nitrogen but low in phosphorus. Meanwhile, growth machinery such as ribosomal RNA contains high nitrogen and phosphorus concentration.
Based on allocation of resources, phytoplankton is classified into three different growth strategies, namely survivalist, bloomer and generalist. Survivalist phytoplankton has high ratio of N:P (>30) and contains numerous resource-acquisition machinery to sustain growth under scarce resources. Bloomer phytoplankton has low N:P ratio (<10), contains high proportion of growth machinery and adapted to exponential growth. Generalist phytoplankton has similar N:P to Redfield ratio and contain relatively equal resource-acquisition and growth machinery.
Environmental controversy.
A 2010 study published in "Nature" reported that marine phytoplankton have declined substantially in the world's oceans over the past century. Phytoplankton concentrations in surface waters were estimated to have decreased by about 40% since 1950 alone, at a rate of around 1% per year, possibly in response to ocean warming. The study generated debate among scientists and led to several communications and criticisms, also published in "Nature". These studies and the need to understand the phytoplankon in the ocean led to the creation of the Secchi Disk Citizen Science study in 2013. The Secchi Disk study is a global study of the phytoplankton conducted by seafarers (sailors, anglers, divers) involving a Secchi Disk and a free mobile phone app called Secchi.
Signs that such a large decline in phytoplankton has not occurred include not observing a comparable percentage decline in fish species which feed on phytoplankton. The role of cadmium has been reviewed. Another global ocean primary productivity study found a net increase in phytoplankton, as judged from measured chlorophyll, when comparing observations in 1998–2002 to those conducted during a prior mission in 1979–1986. The airborne fraction of CO2 from human emissions, the percentage neither sequestered by photosynthetic life on land and sea nor absorbed in the oceans abiotically, has been almost constant over the past century, and that suggests a moderate upper limit on how much a component of the carbon cycle as large as phytoplankton may have declined, if such declined in recent decades. In the example of the northeast Atlantic, a case where chlorophyll measurements extend particularly far back, the location of the Continuous Plankton Recorder (CPR) survey, there was net increase over a 1948 to 2002 period examined. During 1998–2005, global ocean net primary productivity rose during 1998 followed by primarily decline during the rest of that period, although still slightly higher at its end than at its start. Using six different climate model simulations, a large multi-university study of ocean ecosystems predicts, by 2050 A.D., "a global increase in primary production of 0.7% at the low end to 8.1% at the high end," although with "very large regional differences" including "a contraction of the highly productive marginal sea ice biome by 42% in the Northern Hemisphere and 17% in the Southern Hemisphere."
Aquaculture.
Phytoplankton are a key food item in both aquaculture and mariculture. Both utilize phytoplankton as food for the animals being farmed. In mariculture, the phytoplankton is naturally occurring and is introduced into enclosures with the normal circulation of seawater. In aquaculture, phytoplankton must be obtained and introduced directly. The plankton can either be collected from a body of water or cultured, though the former method is seldom used. Phytoplankton is used as a foodstock for the production of rotifers, which are in turn used to feed other organisms. Phytoplankton is also used to feed many varieties of aquacultured molluscs, including pearl oysters and giant clams.
The production of phytoplankton under artificial conditions is itself a form of aquaculture. Phytoplankton is cultured for a variety of purposes, including foodstock for other aquacultured organisms, a nutritional supplement for captive invertebrates in aquaria. Culture sizes range from small-scale laboratory cultures of less than 1L to several tens of thousands of liters for commercial aquaculture. Regardless of the size of the culture, certain conditions must be provided for efficient growth of plankton. The majority of cultured plankton is marine, and seawater of a specific gravity of 1.010 to 1.026 may be used as a culture medium. This water must be sterilized, usually by either high temperatures in an autoclave or by exposure to ultraviolet radiation, to prevent biological contamination of the culture. Various fertilizers are added to the culture medium to facilitate the growth of plankton. A culture must be aerated or agitated in some way to keep plankton suspended, as well as to provide dissolved carbon dioxide for photosynthesis. In addition to constant aeration, most cultures are manually mixed or stirred on a regular basis. Light must be provided for the growth of phytoplankton. The colour temperature of illumination should be approximately 6,500 K, but values from 4,000 K to upwards of 20,000 K have been used successfully. The duration of light exposure should be approximately 16 hours daily; this is the most efficient artificial day length.

</doc>
<doc id="50558" url="http://en.wikipedia.org/wiki?curid=50558" title="Zooplankton">
Zooplankton

Zooplankton are heterotrophic (sometimes detritivorous) plankton. Plankton are organisms drifting in oceans, seas, and bodies of fresh water. The word "zooplankton" is derived from the Greek "zoon" (ζῴον), meaning "animal", and "planktos" (πλαγκτός), meaning "wanderer" or "drifter". Individual zooplankton are usually microscopic, but some (such as jellyfish) are larger and visible with the naked eye.
Ecology.
Zooplankton is a categorization spanning a range of organism sizes including small protozoans and large metazoans. It includes holoplanktonic organisms whose complete life cycle lies within the plankton, as well as meroplanktonic organisms that spend part of their lives in the plankton before graduating to either the nekton or a sessile, benthic existence. Although zooplankton are primarily transported by ambient water currents, many have locomotion, used to avoid predators (as in diel vertical migration) or to increase prey encounter rate.
Ecologically important protozoan zooplankton groups include the foraminiferans, radiolarians and dinoflagellates (the latter are often mixotrophic). Important metazoan zooplankton include cnidarians such as jellyfish and the Portuguese Man o' War; crustaceans such as copepods and krill; chaetognaths (arrow worms); molluscs such as pteropods; and chordates such as salps and juvenile fish. This wide phylogenetic range includes a similarly wide range in feeding behavior: filter feeding, predation and symbiosis with autotrophic phytoplankton as seen in corals. Zooplankton feed on bacterioplankton, phytoplankton, other zooplankton (sometimes cannibalistically), detritus (or marine snow) and even nektonic organisms. As a result, zooplankton are primarily found in surface waters where food resources (phytoplankton or other zooplankton) are abundant.
Just as any species can be limited within a geographical region, so is zooplankton. However, species of zooplankton are not dispersed uniformly or randomly within a region of the ocean. Instead ‘patches’ of zooplankton species (this also applies to phytoplankton) exist throughout the ocean. Though few physical barriers exist above the mesopelagic, specific species of zooplankton are strictly restricted by salinity and temperature gradients; while other species can withstand wide temperature and salinity gradients. Zooplankton patchiness can also be influenced by biological factors, as well as other physical factors. Biological factors include breeding, predation, concentration of phytoplankton, and vertical migration. The physical factor that influences zooplankton distribution the most is mixing of the water column (upwelling and downwelling along the coast and in the open ocean) that affects nutrient availability and, in turn, phytoplankton production.
Through their consumption and processing of phytoplankton and other food sources, zooplankton play a role in aquatic food webs, as a resource for consumers on higher trophic levels (including fish), and as a conduit for packaging the organic material in the biological pump. Since they are typically small, zooplankton can respond rapidly to increases in phytoplankton abundance, for instance, during the spring bloom.
Zooplankton can also act as a disease reservoir. Crustacean zooplankton have been found to house the bacterium "Vibrio cholerae", which causes cholera, by allowing the cholera vibrios to attach to their chitinous exoskeletons. This symbiotic relationship enhances the bacterium's ability to survive in an aquatic environment, as the exoskeleton provides the bacterium with carbon and nitrogen.

</doc>
<doc id="50559" url="http://en.wikipedia.org/wiki?curid=50559" title="Albany County, New York">
Albany County, New York

Albany County ( ) is a county located in the U.S. state of New York; its northern border is formed by the Mohawk River, at its confluence with the Hudson River, which is on the east. As of the 2010 census, the population was 304,204. The county seat is Albany, the state capital. As originally established by the English government in the colonial era, Albany County had an indefinite amount of land, but has only 530 sqmi as of March 3, 1888. The county is named for the Duke of York and of Albany, who became James II of England (James VII of Scotland).
Albany County is included in the Albany-Schenectady-Troy, NY Metropolitan Statistical Area.
History.
Colonial.
After England took control of New Netherlands from the Dutch, Albany County was created on November 1, 1683 by New York Governor Thomas Dongan, and later confirmed on October 1, 1691. The act creating the county vaguely defined its territory "to containe the Towns of Albany, the Collony Rensselaerwyck, Schonecteda, and all the villages, neighborhoods, and Christian Plantaçons on the east side of Hudson River from Roelef's Creek, and on the west side from Sawyer's Creek (Saugerties) to the Sarraghtoga." The confirmation declared in 1691 was similar but omitted the Town of Albany, substituted "Mannor of Ranselaerswyck" for "Collony Rensselaerwyck", and stated "to the uttermost end of Sarraghtoga" instead of just "to Sarraghtoga". Livingston Manor was annexed to Albany County from Dutchess County in 1717. 
Albany's boundaries were defined more closely as various state statutes would add land to the county, or more commonly subtract land for the formation of new counties. In 1772 with the creation of Tryon and Charlotte counties, Albany gained definitive boundaries and included what are now Albany, Columbia, Rensselaer, Saratoga, and Schenectady counties; large parts of Greene and Washington counties; and the disputed southwest corner of Vermont.
The city of Albany was the first municipality within this large county, founded as the village (dorp in Dutch) of Beverwyck by the Director-General of New Amsterdam, Pieter Stuyvesant, who also established the first court in Albany. Albany was established as a city in 1686 by Governor Dongan through the Dongan Charter after the English took over the colony. Schenectady to the west was given a patent with some municipal rights in 1684 and became a borough in 1765. 
The Manor of Rensselaerswyck was created as a district within the county in 1772, and subsequently divided into two districts, one on each side of the Hudson River in 1779. The west district included all of what is now Albany County with the exception of what lands were in the city of Albany at that time. Though the Manor of Rensselaerswyck was the only district (along with the city of Albany) in what is today Albany County, it was not the only district in what was Albany County at the time. Pittstown in 1761, and Duanesburgh in 1764, were created as townships. But when districts were created in 1772, those townships were incorporated into new districts, Pittstown in Schaghticoke and Duanesburgh into the United Districts of Duanesburgh and Schoharie. Schenectady was made from a borough to a district in 1772 as well. Other districts established in 1772 were Hoosick, Coxsackie, Cambridge, Saratoga, Halfmoon, Kinderhook, Kings, Claverack, Great Imboght, and the Manor of Livingston.
In a census of 1697, there were 1,452 individuals living in Albany County; two years later it would be counted as 2,016 at the beginning of King William's War. By the end of the war in 1698, the population had dropped to 1,482, but rebounded quickly and was at 2,273 by 1703. By 1723, it had increased to 6,501 and in 1731 to 8,573, which was slightly less than the population within the city of New York in the same year. In 1737, the inhabitants of Albany County would outnumber those of New York County by 17 people. In 1774, Albany County, with 42,706 people, was the largest county in colonial New York. According to the first Federal Census in 1790, Albany County reached 75,921 inhabitants and was still the largest county in the state.
Formation of towns.
On March 7, 1788, the state of New York divided the entire state into towns eliminating districts as administrative units by passing New York Laws of 1788, Chapters 63 and 64.
Timeline of boundary changes.
Albany County was one of the original twelve counties created by the Province of New York on November 1, 1683. At that time it included all of the present Bennington County, Vermont, all of New York state north of the counties of Dutchess and Ulster, and theoretically stretched west to the Pacific Ocean.
On May 27, 1717, Albany County was adjusted to gain an indefinite amount of land from Dutchess County and other non-county lands.
On October 7, 1763, King George III, as part of his Proclamation of 1763, created the new province of Quebec, implicitly setting the northern limit of New York at the parallel of 45 degrees north latitude from the Atlantic-St. Lawrence watershed westward to the St. Lawrence River, implicitly setting the northern limit of Albany County, but it was never mapped.
On July 20, 1764, King George III established the boundary between New Hampshire and New York along the west bank of the Connecticut River, north of Massachusetts and south of the parallel of 45 degrees north latitude. Albany County implicitly gained present-day Vermont. Although disputes occasionally broke out later, this line became the boundary between New Hampshire and Vermont, and has remained unchanged to the present. When New York refused to recognize land titles through the New Hampshire Grants (towns created earlier by New Hampshire in present Vermont), dissatisfied colonists organized in opposition, which led to the creation of independent Vermont in 1777.
On July 3, 1766, Cumberland County was partitioned from Albany County to cover all territory to the northern and eastern limits of the colony, including Windsor County, most of Windham County, and parts of Bennington and Rutland counties in present-day Vermont.
On June 26, 1767, Albany County regained all of Cumberland County.
On March 19, 1768, Albany County was re-partitioned, and Cumberland County restored.
On March 16, 1770, Albany County was again partitioned. Gloucester County was created to include all of Orange, Caledonia and Essex counties, most of Washington County, and parts of Orleans, Lamoille, Addison and Chittenden counties in present-day Vermont.
On March 12, 1772, Albany County was partitioned again, this time into the counties of Albany, Tryon (now Montgomery), and Charlotte (now Washington). This established a definite area for Albany County of 5,470 sqmi.
On March 24, 1772, Albany County was partitioned again, with an additional 50 sqmi handed over to Cumberland County.
On March 9, 1774, Albany County was partitioned again, this time passing 1090 sqmi to Ulster County.
On April 1, 1775, Albany was again partitioned, this time giving up 60 sqmi to Charlotte County, who then exchanged this land with a like parcel in Cumberland County.
On January 15, 1777, Albany County was again partitioned, this time on account of the independence of Vermont from New York, reducing Albany County by an additional 300 sqmi.
On June 26, 1781, Bennington County, Vermont attempted to annex a portion of Albany County that today includes portions of Washington and Rensselaer counties to form what they called "The West Union". The fledgling United States – under the Articles of Confederation – arbitrated this annexation, and condemned it, resulting in Vermont ceasing the annexation on 1782-02-23.
On April 4, 1786, Columbia County was created from 650 sqmi of Albany County land.
On March 7, 1788, New York, refusing to recognize the independence of Vermont, and the attendant elimination of Cumberland County, attempted to adjust the line that separated Cumberland from Albany County in present-day Vermont, but to no effect.
On February 7, 1791, Albany County was partitioned again, this time to form Rensselaer and Saratoga counties. Rensselaer received 660 sqmi, while Saratoga received 850 sqmi. Also the town of Cambridge was transferred to Washington County. A total of 1680 sqmi changed hands.
On June 1, 1795, Albany County was once again partitioned, this time losing 460 sqmi to Schoharie County.
On April 5, 1798, another partition took place, with 90 sqmi passing to Ulster County.
On March 25, 1800, once again Albany County was partitioned, with 360 sqmi being used to create Greene County.
On April 3, 1801, all New York counties were redefined, with Albany County gaining 10 sqmi.
On March 3, 1808, Albany County turned Havre Island over to Saratoga County, with no resultant loss in land.
On March 7, 1809, Schenectady County was created from 230 sqmi of Albany County land.
The result was the production of Albany County as it exists today.
Geography.
According to the U.S. Census Bureau, the county has a total area of 533 sqmi, of which 523 sqmi is land and 10 sqmi (2.0%) is water.
Albany County is in the east central part of New York, extending southward and westward from the point where the Mohawk River joins the Hudson River. Its eastern boundary is the Hudson; a portion of its northern boundary is the Mohawk.
The terrain of the county ranges from flat near the Hudson and Mohawk Rivers to high and hilly to the southwest, where the Helderberg Escarpment marks the beginnings of the Catskill Mountains. The highest point is one of several summits near Henry Hill at approximately 2,160 feet (658 m) above sea level; the lowest point is sea level along the Hudson.
Climate.
The Capital District has a humid continental climate, with cold, snowy winters, and hot, wet summers. Albany receives around 36.2 in of rain per year, with 135 days of at least 0.01 in of precipitation. Snowfall is significant, totaling about 63 in annually, but with less accumulation than the lake-effect areas to the north and west, being far enough from Lake Ontario. Albany County is however, close enough to the coast to receive heavy snow from Nor'easters, and the region gets the bulk of its yearly snowfall from these types of storms. The county also occasionally receives Alberta clippers. Winters are often very cold with fluctuating conditions, temperatures often drop to below 0 °F (−18 °C) at night. Summers in the Albany can contain stretches of excessive heat and humidity, with temperatures above 90 °F and dew points near 70. Severe thunderstorms are common but tornadoes are rare. Albany receives on average per year 69 sunny days, 111 partly cloudy days, and 185 cloudy days; and an average, over the course of a year, of less than four hours of sunshine per day, with just over an average of 2.5 hours per day over the course of the winter. The chance during daylight hours of sunshine is 53%, with the highest percentage of sunny daylight hours being in July with 64%, and the lowest month is November with 37%.
Adjacent counties.
Albany County is bordered by six counties. Listed clockwise, they are:
Cityscape.
Architecture.
Albany County has a myriad of different architectural styles spanning centuries of development. Within the city of Albany alone there is Dutch Colonial (the Quackenbush House), French Renaissance (the New York State Capitol), Federal style (the original Albany Academy in Academy Park), Romanesque Revival (Albany City Hall), Art deco (the Alfred E. Smith Building), and Modern (Empire State Plaza). The cities of Albany, Cohoes, and Watervliet and the village of Green Island are more urban in architecture; while the towns of Colonie, Guilderland, New Scotland, and Bethlehem more suburban and the remaining Hilltowns (Berne, Knox, Westerlo, and Rensselaerville) very rural.
Parks.
Albany County is home to the Emma Treadwell Thacher Nature Center, which opened in July 2001 and is located near the shore of Thompson's Lake between the two state parks that are in Albany County- Thompson's Lake State Park and John Boyd Thacher State Park. There are also state-owned nature preserves with interactive educational programs such as the Five Rivers Environmental Education Center and the Albany Pine Bush. The cities, towns, and villages of Albany County have many municipal parks, playgrounds, and protected green areas. Washington Park in the city of Albany and The Crossing in the town of Colonie are two of the largest. There are many small hiking and biking trails and longer distance bike-hike trails such the Mohawk-Hudson Bike-Hike Trail which goes from the city of Albany north to Cohoes and then west along the Mohawk River to Schenectady County.
Festivals.
One of the largest events in Albany County is the Tulip Fest held in the city of Albany every spring at Washington Park. The tradition stems from when Mayor Erastus Corning 2nd had a city ordinance passed declaring the tulip as Albany's official flower on July 1, 1948. The African-American tradition of Pinksterfest, whose origins are traced back even further to Dutch festivities, was later incorporated into the Tulip Fest. The Albany LatinFest has been held since 1996 and drew 10,000 to Washington Park in 2008. PolishFest is a three-day celebration of Polish culture in the Capital District, held in the town of Colonie for the past eight years.
Amusement.
Albany County has two shopping malls that are classified as super-regional malls (malls with over 800,000 sq ft), Crossgates Mall in Guilderland and Colonie Center in Colonie with over one million square feet of rentable space in each. A regional mall, the Latham Circle Mall, is in Latham (town of Colonie). South of the Latham Circle Mall, in the neighboring hamlet of Newtonville also within the town of Colonie, is Hoffman's Playland. Hoffman's is a children's amusement park open during the summer. During the winter there are over 18 mi of official trails for snowshoeing at the Albany Pine Bush Preserve, in the city of Albany and towns of Colonie and Guilderland.
Culture and contemporary life.
Albany is often derided as "Smallbany" (also spelled Smalbany) for its perceived lack of culture and as a backwater in tourism circles, even though it consistently ranks high on lists of top cities/metro areas for culture, such as being 23rd in the book "Cities Ranked & Rated". To locals the Smallbany title references the perceived notion that all residents are interconnected and know each other in some way. Albany is home to some of the oldest museums, historical sites, and places of worship in the state of New York and the United States; some of the finest collections of historical artifacts and art can be found in Albany County.
Museums.
Albany County has many historical sites and museums covering a wide range of topics and time periods. The Albany Institute of History and Art founded in 1791 is one of the oldest museums in the United States, and the New York State Museum is the oldest and largest state museum in the country. Many of the museums are historical sites themselves, such as Cherry Hill, the Ten Broeck Mansion, and the Schuyler Mansion in the city of Albany and the Pruyn House in Colonie. The Quackenbush House is the second oldest house in the city of Albany and is part of the Albany Heritage Area Visitors Center, which includes a planetarium. The Albany Pine Bush Discovery Center in the city of Albany includes hands-on activities to learn about the unique Pine Bush Barrens of the city of Albany and towns of Guilderland and Colonie. Covering the history of pharmacy is the Throop Drug Store Museum at the Albany College of Pharmacy. The USS Slater, DE-766 is a World War II Destroyer Escort, the last floating Destroyer Escort, owned by the Destroyer Escort Historical Museum is moored from Spring to Fall at the foot of Quay Street in the Hudson River. The ship is open for tours each week and contains an excellent and well-maintained collection of World War II US Naval artifacts. www.ussslater.org
There are several art museums in Albany County; including the Albany Center Gallery, in downtown Albany, which exhibits works by local artists within a 100 mi radius of that city; the University Art Museum, at the University at Albany, SUNY; and the Opalka Gallery, at the Sage College of Albany. The Empire State Plaza in Albany has one of the most important state collections of modern art in the U.S.
Performing arts.
Albany County itself owns the largest venue for performing arts in the county, the Times Union Center, which was originally built as the Knickerbocker Arena; it opened on January 30, 1990 with a performance by Frank Sinatra. In 1996, the The Grateful Dead released a concert album from their March 1990 performances titled "Dozin' at the Knick".
Sports.
Many athletes and coaches in major sports have begun their careers in Albany County. Phil Jackson, former NBA head coach of the Chicago Bulls and Los Angeles Lakers won his first championship ring as a coach when he guided the Albany Patroons to the 1984 CBA championship. Three years later, the Patroons completed a 50–6 regular season, including winning all 28 of their home games; at that time, Sacramento Kings head coach George Karl was the Patroons' head coach. Future NBA stars Mario Elie and Vincent Askew were part of that season's squad. Mike Tyson received his early training in the Capital District and his first professional fight was in Albany in 1985 and Tyson's first televised fight was in Troy in 1986. He fought professionally four times in Albany and twice each in Troy and Glens Falls between 1985 and 1986.
Since 1988, the Siena College men's basketball team (the Siena Saints) have appeared in six NCAA Tournaments (1989, 1999, 2002, 2008, 2009, and 2010).
Religious life.
Albany County was originally settled primarily by Protestants from northern Europe: the Netherlands, British Isles, and Germany. In the 19th century it was a destination for many Catholic immigrants, first from Ireland - fleeing the Great Famine, and later from southern Germany, central and southern Europe. Late 19th and early 20th century immigrants included Jews from eastern Europe. In addition to other Jewish congregations, the county has one of the few Karaite Jewish communities outside Israel. This community is active and has its own synagogue.
Demographics.
As of the census of 2010, there were 304,204 people, 124,682 households, and 74,521 families residing in the county. The population density was 563 people per square mile (217/km²). There were 134,072 housing units at an average density of 248 per square mile (96/km²). The racial makeup of the county was 78.2% White, 12.7% Black or African American, 0.2% Native American, 4.8% Asian, 0.0% Pacific Islander, 1.6% from other races, and 2.5% from two or more races. 4.9% of the population were Hispanic or Latino of any race. 19.2% were of Irish, 16.0% Italian, 11.0% German, 6.1% English and 5.1% Polish ancestry according to Census 2000. 90.4% spoke English, 2.7% Spanish and 1.0% Italian as their first language.
There were 124,682 households out of which 28.9% had children under the age of 18 living with them, 43.2% were married couples living together, 12.2% had a female householder with no husband present, and 41.1% were non-families. 33.0% of all households were made up of individuals and 11.3% had someone living alone who was 65 years of age or older. The average household size was 2.32 and the average family size was 2.99.
In the county the population was spread out with 22.6% under the age of 18, 11.3% from 18 to 24, 28.8% from 25 to 44, 22.8% from 45 to 64, and 14.5% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 91.7 males. For every 100 females age 18 and over, there were 87.8 males.
The median income for a household in the county was $42,935, and the median income for a family was $56,724. Males had a median income of $39,838 versus $30,127 for females. The per capita income for the county was $23,345. About 7.2% of families and 13.1% of the population were below the poverty line, including 14.9% of those under age 18 and 6.3% of those age 65 or over.
Government and politics.
Albany County was governed by an elected board of supervisors until 1967. 
On January 1, 1968 a 39-member County Legislature was established. On January 1, 1976, Albany County government was changed by a new county charter establishing provisions for a county executive elected at-large, in addition to the 39-seat county legislature.
Each of the 39 members are elected from single-member districts. As of 2015, the county council has 29 Democrats, 9 Republicans and 1 Conservative, who caucuses with the Republicans.
The County Executive is Daniel P. McCoy (D) and the Chair of the Legislature is Shawn M. Morse (D). Other county elected officials include County Sheriff Craig Apple, County District Attorney David Soares, and County Comptroller Michael F. Conners. The county executive is elected in a countywide vote.
Transportation.
Albany County has long been at the forefront of transportation technology from the days of turnpikes and plank roads to the Erie Canal, from the first passenger railroad in the state to the oldest municipal airport in the United States. Today, Interstates, Amtrak, and the Albany International Airport continue to make the Albany County a major crossroads of the Northeastern United States.
The Capital District Transportation Committee (CDTC) is the Metropolitan Planning Organization (MPO) for the Albany-Schenectady-Troy Metropolitan Statistical Area (MSA). Every metropolitan area in the United States with a population of over 50,000 must have a MPO in order to get any federal transportation funding. The US Department of Transportation (USDOT) uses an MPO to make decisions on what projects are most important to a metro area for immediate versus long term funding. The USDOT will not approve federal funds for transportation projects unless they are on an MPO's list.
Interstate and other major highways.
Albany County is situated at a major crossroads of the Northeastern United States, first formed by the Mohawk and Hudson rivers. Even before the Interstate Highway System and the U.S. Highway system, Albany County was the hub of many turnpikes and plank roads that connected the region, as well as the Erie Canal reaching the Great Lakes. 
Today, Interstate 87 and Interstate 90 meet in Albany County. The Thomas E. Dewey New York State Thruway is a toll-road that from Exit 24 in the city of Albany is I-87 and travels south to connect the county with downstate New York. West from Exit 24, the Thruway is I-90 and connects the county with Schenectady, Utica, Syracuse, Rochester, and Buffalo. 
North of Exit 24, I-87 is the Adirondack Northway and connects the city and county of Albany with their suburbs in Saratoga County and provides long-distance travel to Montreal. East of Exit 24, I-90 travels along the northern boundary of the city of Albany and exits the county on the Patroon Island Bridge into Rensselaer County to access Albany's eastern suburbs. Interstate 787 connects the Thruway (I-87) to Downtown Albany, Menands, Watervliet, and Cohoes. U.S. Route 9 enters the county on the Dunn Memorial Bridge and travels through the city of Albany north, connecting it with the suburbs in the Colonie and Saratoga County. U.S. Route 20 also enters the county on the Dunn Memorial Bridge and travels west through Albany (city) and the Town of Guilderland. New York State Route 5 and New York State Route 7 are two important highways that bisect the county and are developed as important shopping strips.
Mass transit.
Albany County is served by the Capital District Transportation Authority, a four-county bus service that also serves Rensselaer, Schenectady, and Saratoga counties. Greyhound Lines, Trailways, and Peter Pan/Bonanza buses all serve a downtown terminal. A Chinatown bus service leaves from Central Avenue and provides service to Chinatown in Manhattan.
Airports.
Albany International Airport is the only commercial airport in the county. Destinations for flights out of Albany include Atlanta, Georgia; Las Vegas; Chicago; Charlotte, North Carolina; and Orlando, Florida, among many others.
Rail.
Since 1968 when Union Station in the city of Albany was abandoned for a new station across the Hudson in the city of Rensselaer, Albany County has been without a train station. Amtrak has several routes serving the Albany-Rensselaer Station. The Adirondack (north to Montreal, Quebec and south to the city of New York), Empire Service (west to Buffalo and Niagara Falls, south to New York), Ethan Allen Express (northeast to Rutland, Vermont and south to New York), Maple Leaf (west to Toronto and south to New York), and the Lake Shore Limited (at Albany-Rensselaer separate routes from Boston and New York merge to one train west to Chicago, on way east one train splits to two, one east to Boston and another south to New York).
Communities.
Albany County is composed of three cities and 10 towns.

</doc>
<doc id="50560" url="http://en.wikipedia.org/wiki?curid=50560" title="Lemures">
Lemures

In Roman mythology, the lemures were shades or spirits of the restless or malignant dead, and are probably cognate with an extended sense of larvae (from Latin "larva", "mask") as disturbing or frightening. "Lemures" is the more common literary term but even this is rare: it is used by the Augustan poets Horace and Ovid, the latter in his "Fasti", the six-book calendar poem on Roman holidays and religious customs. 
Description.
"Lemures" may represent the wandering and vengeful spirits of those not afforded proper burial, funeral rites or affectionate cult by the living: they are not attested by tomb or votive inscriptions. Ovid interprets them as vagrant, unsatiated and potentially vengeful "di manes" or "di parentes", ancestral gods or spirits of the underworld. To him, the rites of their cult suggest an incomprehensibly archaic, quasi-magical and probably very ancient rural tradition. Four centuries later, St. Augustine describes both the "lemures" and the "larvae" as evil and restless "manes" that torment and terrify the living: "lares", on the other hand, are good "manes".
"Lemures" were formless and liminal, associated with darkness and its dread. In Republican and Imperial Rome, May 9, 11, and 13 were dedicated to their placation in the household practices of "Lemuralia" or "Lemuria". The head of household ("paterfamilias") would rise at midnight and cast black beans behind him with averted gaze; the "Lemures" were presumed to feast on them. Black was the appropriate colour for offerings to chthonic deities. William Warde Fowler interprets the gift of beans as an offer of life, and points out that they were a ritual pollution for priests of Jupiter. The "lemures" themselves were both fearsome and fearful: any malevolent shades dissatisfied with the offering of the "paterfamilias" could be startled into flight by the loud banging of bronze pots.
In scientific Latin.
The Lemures inspired Linnaeus' Modern Latin backformation Lemur (denoting a type of primates). According to Linnaeus' own explanation, the name was selected because of the nocturnal activity and slow movements of the slender loris. Being familiar with the works of Virgil and Ovid and seeing an analogy that fit with his naming scheme, Linnaeus adapted the term "lemur" for these nocturnal primates. However, it has been commonly and falsely assumed that Linnaeus was referring to the ghost-like appearance, reflective eyes, and ghostly cries of lemurs. In Goethe's "Faust", a chorus of Lemurs who serve Mephistopheles dig Faustus' grave.

</doc>
<doc id="50561" url="http://en.wikipedia.org/wiki?curid=50561" title="Dominance">
Dominance

Dominance may refer to:

</doc>
<doc id="50562" url="http://en.wikipedia.org/wiki?curid=50562" title="Smyrna">
Smyrna

Smyrna (Ancient Greek: Σμύρνη or Σμύρνα) was an ancient city located at a central and strategic point on the Aegean coast of Anatolia. Due to its advantageous port conditions, its ease of defence and its good inland connections, Smyrna rose to prominence. The ancient city is located at two sites within modern İzmir, Turkey, as well as being its namesake. The first site, probably founded indigenously, rose to prominence during the Archaic Period as one of the principal ancient Greek settlements in western Anatolia. The second, whose foundation is associated with Alexander the Great, reached metropolitan proportions during the period of the Roman Empire. Most of the present-day remains date from the Roman era, the majority from after a 2nd-century AD earthquake.
In practical terms, a distinction is often made between Old Smyrna, the initial settlement founded around the 11th century BC, first as an Aeolian settlement, and later taken over and developed during the Archaic Period by the Ionians, and Smyrna proper, the new city moved into from the older one as of the 4th century BC and whose foundation was inspired by Alexander the Great.
Old Smyrna was located on a small peninsula connected to the mainland by a narrow isthmus at the northeastern corner of the inner Gulf of İzmir, at the edge of a fertile plain and at the foot of Mount Yamanlar which had seen the earlier Anatolian settlement commanding the gulf. Today, the archeological site, named Bayraklı Höyüğü, is approximately 700 m inland, in the Tepekule neighbourhood of Bayraklı at .
"New" Smyrna developed simultaneously on the slopes of the Mount Pagos (Kadifekale today) and alongside the coastal strait immediately below where a small bay existed until the 18th century.
The core of the late Hellenistic and early Roman Smyrna forms today the large area of İzmir Agora Open Air Museum at this site. Research is being pursued at the sites of both the old and the new cities in a continuous manner and in a regionalized structure, since 1997 for Old Smyrna and since 2002 for the Classical Period city, in collaboration between İzmir Archaeology Museum and the Metropolitan Municipality of İzmir.
History.
Etymology.
"For full explanations on etymology of the city's name, see İzmir"
There are several explanations brought forth as regards its name. One of these involve a Greek myth derived from an eponymous Amazon named "Σμύρνα" (Smyrna), which was also the name of a quarter of Ephesus, and can also be recognized under the form Myrina, a city of Aeolis.
In inscriptions and coins it is often written "Ζμύρνα" ("Zmyrna"), "Ζμυρναῖος" ("Zmyrneos"), "of Smyrna".
The name Smyrna may also have been taken from the ancient Greek word for myrrh, "smyrna", which was the chief export of the city in ancient times.
Third millennium to 687 BC.
The region was settled at least as of the beginning of the third millennium BC, or perhaps earlier, as the recent finds in Yeşilova Höyük suggests. It could have been a city of the autochthonous Leleges before the Greek colonists started to settle along the coast of Asia Minor as of the beginning of the first millennium BC. Throughout antiquity Smyrna was a leading city-state of Ionia, with influence over the Aegean shores and islands. Smyrna was also among the cities that claimed Homer as a resident.
The early Aeolian Greek settlers of Lesbos and Cyme, expanding eastwards, occupied the valley of Smyrna. It was one of the confederacy of Aeolian city-states, marking the Aeolian frontier with the Ionian colonies.
Strangers or refugees from the Ionian city of Colophon settled in the city and finally (traditionally in 688 BC) by an uprising Smyrna passed into their hands and became the thirteenth of the Ionian city-states. Revised mythologies made it a colony of Ephesus. In 688 BC, the Ionian boxer Onomastus of Smyrna won the prize at Olympia, but the "coup" was probably then a recent event. The Colophonian conquest is mentioned by Mimnermus (before 600 BC), who counts himself equally of Colophon and of Smyrna. The Aeolic form of the name was retained even in the Attic dialect, and the epithet "Aeolian Smyrna" remained current long after the conquest.
Smyrna's position at the mouth of the small river Hermus at the head of a deep arm of the sea ("Smyrnaeus Sinus") that reached far inland and admitted Greek trading ships into the heart of Lydia, placed it on an essential trade route between Anatolia and the Aegean and raised Smyrna during the 7th century BC to power and splendor. One of the great trade routes which cross Anatolia descends the Hermus valley past Sardis, and then, diverging from the valley, passes south of Mount Sipylus and crosses a low pass into the little valley where Smyrna lies between the mountains and the sea. Miletus and later Ephesus were situated at the sea end of the other great trade route across Anatolia and competed for a time successfully with Smyrna; but after both cities' harbors silted up, Smyrna remained without a rival.
The Meles River, which flowed by Smyrna, is famous in literature and was worshiped in the valley. A common and consistent tradition connects Homer with the valley of Smyrna and the banks of the Meles; his figure was one of the stock types on coins of Smyrna, one class of which numismatists call "Homerian"; the epithet "Melesigenes" was applied to him; the cave where he was wont to compose his poems was shown near the source of the river; his temple, the "Homereum", stood on its banks. The steady equable flow of the Meles, alike in summer and winter, and its short course, beginning and ending near the city, are celebrated by Aristides and Himerius. The description applies admirably to the stream which rises from abundant springs east of the city and flows into the southeast extremity of the gulf.
The archaic city ("Old Smyrna") contained a temple of Athena from the 7th century BC.
Lydian period.
When the Mermnad kings raised the Lydian power and aggressiveness, Smyrna was one of the first points of attack. Gyges (ca. 687—652 BC) was, however, defeated on the banks of the Hermus, the situation of the battlefield showing that the power of Smyrna extended far to the east. A strong fortress was built probably by the Smyrnaean Ionians to command the valley of Nymphi, the ruins of which are still imposing, on a hill in the pass between Smyrna and Nymphi.
According to Theognis (c. 500 BC), it was pride that destroyed Smyrna. Mimnermus laments the degeneracy of the citizens of his day, who could no longer stem the Lydian advance. Finally, Alyattes II (609—560 BC) conquered the city and sacked it, and though Smyrna did not cease to exist, the Greek life and political unity were destroyed, and the "polis" was reorganized on the village system. Smyrna is mentioned in a fragment of Pindar and in an inscription of 388 BC, but its greatness was past.
Hellenistic period.
Alexander the Great conceived the idea of restoring the Greek city in a scheme that was, according to Strabo, actually carried out under Antigonus (316—301 BC) and Lysimachus (301 BC—281 BC), who enlarged and fortified the city. The ruined acropolis of the ancient city, the "crown of Smyrna", had been on a steep peak about 1250 ft high, which overhangs the northeast extremity of the gulf. Modern İzmir was constructed atop the later Hellenistic city, partly on the slopes of a rounded hill the Greeks called "Pagos" near the southeast end of the gulf, and partly on the low ground between the hill and the sea. The beauty of the Hellenistic city, clustering on the low ground and rising tier over tier on the hillside, was frequently praised by the ancients and is celebrated on its coins.
Smyrna is shut in on the west by a hill now called Deirmen Tepe, with the ruins of a temple on the summit. The walls of Lysimachus crossed the summit of this hill, and the acropolis occupied the top of Pagus. Between the two the road from Ephesus entered the city by the Ephesian gate, near which was a gymnasium. Closer to the acropolis the outline of the stadium is still visible, and the theatre was situated on the north slopes of Pagus. Smyrna possessed two harbours. The outer harbour was simply the open roadstead of the gulf, and the inner was a small basin with a narrow entrance partially filled up by Tamerlane in 1402 AD.
The streets were broad, well paved and laid out at right angles; many were named after temples: the main street, called the Golden, ran across the city from west to east, beginning probably from the temple of Zeus Akraios on the west slope of Pagus, and running round the lower slopes of Pagus (like a necklace on the statue, to use the favorite terms of Aristides the orator) towards Tepecik outside the city on the east, where probably stood the temple of Cybele, worshipped under the name of Meter Sipylene, the patroness of the city. (name deriving from the nearby Mount Sipylus, which bounds the valley of the city's backlands). The plain towards the sea was too low to be properly drained, and hence in rainy weather the streets of the lower town were deep with mud and water.
At the end of the Hellenistic period, in 197 BC, the city suddenly cut its ties with King Eumenes of Pergamum and instead appealed to Rome for help. Because Rome and Smyrna had had no ties until then, Smyrna created a cult of Rome to establish a bond, and the cult eventually became widespread through the whole Roman Empire. As of 195 BC, the city of Rome started to be deified, in the cult to the goddess Roma. In this sense, the Smyrneans can be considered as the creators of the goddess Roma.
In 133 BC, when the last Attalid king Attalus III died without an heir, his will conferred his entire kingdom, including Smyrna, to the Romans. They organized it into the Roman province of Asia, making Pergamum the capital. Smyrna, however, as a major seaport, became a leading city in the newly constituted province.
Roman and Byzantine period.
As one of the principal cities of Roman Asia, Smyrna vied with Ephesus and Pergamum for the title "First City of Asia."
A Christian church and a bishopric existed here from a very early time, probably originating in the considerable Jewish colony. It was one of the seven churches addressed in the Book of Revelation. Saint Ignatius of Antioch visited Smyrna and later wrote letters to its bishop, Polycarp. A mob of Jews and pagans abetted the martyrdom of Polycarp in AD 153. Saint Irenaeus, who heard Polycarp as a boy, was probably a native of Smyrna. Another famous resident of the same period was Aelius Aristides.
Polycrates reports a succession of bishops including Polycarp of Smryna, as well as others in nearby cities such as Melito of Sardis. Related to that time the German historian W. Bauer wrote:
Asian Jewish Christianity received in turn the knowledge that henceforth the "church" would be open without hesitation to the Jewish influence mediated by Christians, coming not only from the apocalyptic traditions, but also from the synagogue with its practices concerning worship, which led to the appropriation of the Jewish passover observance. Even the observance of the sabbath by Christians appears to have found some favor in Asia...we find that in post-apostolic times, in the period of the formation of ecclesiastical structure, the Jewish Christians in these regions come into prominence.
In the late 2nd century, Irenaeus also noted:
Polycarp also was not only instructed by apostles, and conversed with many who had seen Christ, but was also, by apostles in Asia, appointed bishop of the Church in Smyrna…always taught the things which he had learned from the apostles, and which the Church has handed down, and which alone are true. To these things all the Asiatic Churches testify, as do also those men who have succeeded Polycarp. 
Tertullian wrote c. 208 AD.
Anyhow the heresies are at best novelties, and have no continuity with the teaching of Christ. Perhaps some heretics may claim Apostolic antiquity: we reply: Let them publish the origins of their churches and unroll the catalogue of their bishops till now from the Apostles or from some bishop appointed by the Apostles, as the Smyrnaeans count from Polycarp and John, and the Romans from Clement and Peter; let heretics invent something to match this.
Hence, apparently the church in Smyrna was one of only two that Tertullian felt could have had some type of apostolic succession. During the mid-3rd century, however, changes occurred in Asia Minor, and most there became affiliated with the Greco-Roman churches.
When Constantinople became the seat of government, the trade between Anatolia and the West diminished in importance, and Smyrna declined. The Seljuk commander Tzachas seized Smyrna in 1084 and used it as a base for naval raids, but the city was recovered by the general John Doukas. The city was several times ravaged by the Turks, and had become quite ruinous when the Nicaean emperor John III Doukas Vatatzes rebuilt it about 1222.
Ottoman period.
Ibn Batuta found it still in great part a ruin when the homonymous chieftain of the Beylik of Aydın had conquered it about 1330 and made his son Umur governor. It became the port of the emirate. Soon afterwards the Knights of Saint John established themselves in the town but failed to conquer the citadel. In 1402, Tamerlane stormed the town and massacred almost all the inhabitants. The Mongol conquest was only temporary, but Smyrna was recovered by the Turks under the Aydın dynasty after which it became Ottoman, when the Ottomans took over the lands of Aydın.
Greek influence was so strong in the area that the Turks called it "Smyrna of the infidels" (Gavur İzmir). While Turkish sources track the emergence of the term to the 14th century when two separate parts of the city were controlled by two different powers, the upper İzmir being Muslim and the lower part of the city Christian.
During the late 19th and early 20th century, the city was an important financial and cultural center of the Greek world. Out of the 391 factories 322 belonged to local Greeks, while 3 out of the 9 banks were backed by Greek capital. Education was also dominated by the local Greek communities with 67 male and 4 female schools in total. The Ottomans continued to control the area, with the exception of the 1919–1922 period, when the city was assigned to Greece by the Treaty of Sèvres.
The most important Greek educational institution of the region was the Evangelical School that operated from 1733 to 1922.
Post World War I.
Smyrna was occupied by Greece from May 15, 1919, after the end of the First World War. A military administration was formed by Greece in and around Smyrna. The Greek premier Venizelos had plans to annex Smyrna and he seemed to be realizing his objective in the Treaty of Sèvres signed on August 10, 1920. (However, this treaty was not ratified by the parties; it was replaced by the Treaty of Peace of Lausanne.)
The occupation of Smyrna came to an end when the Turkish army of Kemal Atatürk entered the city on September 9, 1922, at the end of the Greco-Turkish War (1919–1922). Four days after the Turks regained control of the city, The Great Fire of Smyrna resulted in the massacre of the Greek and Armenian populations. The death toll is estimated to range from 10,000 to 100,000.
Agora.
The remains of the ancient agora of Smyrna constitute today the space of "İzmir Agora Museum" in İzmir's Namazgah quarter, although its area is commonly referred to as "Agora" by the city's inhabitants.
Situated on the northern slopes of the Pagos hills, it was the commercial, judicial and political nucleus of the ancient city, its center for artistic activities and for teaching.
"İzmir Agora Open Air Museum" consists of five parts, including the agora area, the base of the northern basilica gate, the stoa and the ancient shopping centre.
The agora of Smyrna was built during the Hellenistic era. After a destructive earthquake in 178 AD, Smyrna was rebuilt in the Roman period (2nd century AD) under the emperor Marcus Aurelius, according to an urban plan drawn by Hippodamus of Miletus. The bust of the emperor's wife Faustina on the second arch of the western stoa confirms this fact.
It was constructed on a sloping terrain in three floors, close to the city center. The terrain is 165 m wide and 200 m long. It is bordered on all sides by porticos. Because a Byzantine and later an Ottoman cemetery were located over the ruins of the agora, it was preserved from modern constructions. This agora is now the largest and the best preserved among Ionian agoras. The agora is now surrounded by modern buildings that still cover its eastern and southern parts.
The agora was used until the Byzantine period.
On entering the courtyard, to the left is the western stoa, in the back the basilica and on the right side the Ottoman cemetery. The courtyard was surrounded by porticoes on three sides. The basilica and the western portico were built over an infrastructure of basements with round arches to protect them against future earthquakes. The eastern end and the southern porticoes consisted of a two-floor compounded structure. Beneath the basilica was a covered market place. The design of the basement has a strong resemblance with the crypto-porticus constructions of the western provinces.
The monumental entrance at the eastern side was one of the most magnificent and arched structures of the Hellenistic era.
A two-storied stoa, 17.5 m wide, was constructed at the eastern and western side of the agora. Each stoa was divided in three galleries by two rows of columns. Each stoa had an upper story. The stoas were protected from sun and rain by a roof. These impressive structures measured 75 m by 18 m. The southern part of the western stoa has many water channels and large water reservoirs, pointing to the presence of water in the agora.
Excavations.
Although Smyrna was explored by Charles Texier in the 19th century and the German consul in İzmir had purchased the land around the ancient theater in 1917 to start excavations, the first scientific digs can be said to have started in 1927. Most of the discoveries were made by archaeological exploration carried as an extension during the period between 1931 and 1942 by the German archaeologist Rudolf Naumann and Selâhattin Kantar, the director of İzmir and Ephesus museums. They uncovered a three-floor, rectangular compound with stairs in the front, built on columns and arches around a large courtyard in the middle of the building.
New excavations in the agora began in 1996. They have continued since 2002 under the sponsorship of the Metropolitan Municipality of İzmir. A primary school adjacent to the agora that had burned in 1980 was not reconstructed. Instead, its space was incorporated into the historical site. The area of the agora was increased to 16590 m2. This permitted the evacuation of a previously unexplored zone. The archaeologists and the local authorities, means permitting, are also keenly eyeing a neighbouring multi-storey car park, which is known to cover an important part of the ancient settlement. During the present renovations the old restorations in concrete are gradually being replaced by marble.
The new excavation has uncovered the agora's northern gate. It has been concluded that embossed figures of the goddess Hestia found in these digs were a continuation of the Zeus altar uncovered during the first digs. Statues of the gods Hermes, Dionysos, Eros and Heracles have also been found, as well as many statues, heads, embossments, figurines and monuments of people and animals, made of marble, stone, bone, glass, metal and terracotta. Inscriptions found here list the people who provided aid to Smyrna after the earthquake of 178 AD.
Economy.
In the early 20th-century, there were mills spinning thread. As of 1920, there were two factories in Smyrna dying yarn, which were owned by British companies. These companies employed over 60,000 people. During this time, there was also a French owned cotton spinning mill. The city also produced soap made of refuse olive oil. An ironworks, also owned by the British, produced tools and equipment. Those tools were used to extract tannin from valonia oak. As of 1920, the ironwork was exporting 5,000 tons of product a year. The city also produced wooden boxes, which were used for fig and raisin storage. The wood for the boxes was imported from Austria and Romania.
Toponyms.
Several American cities have been named after Smyrna, including Smyrna, Georgia; Smyrna, Tennessee; Smyrna, Delaware; Smyrna, Michigan; and New Smyrna Beach, Florida.
External links.
</dl>

</doc>
<doc id="50563" url="http://en.wikipedia.org/wiki?curid=50563" title="Sucrose">
Sucrose

Sucrose, commonly named table sugar or sugar, is cane and beet sugar. Saccharose is an obsolete name for sugars in general, especially sucrose. The molecule is a disaccharide combination of the monosaccharides glucose and fructose with the formula C12H22O11.
A white, odorless, crystalline powder with a sweet taste, it is best known for its role in food. About 175 million metric tons of sucrose sugar were produced worldwide in 2013.
The word "sucrose" was coined in 1857 by the English chemist William Miller from the French "sucre" ("sugar") and the generic chemical suffix for sugars "-ose". The abbreviated term "Suc" is often used for "sucrose" in scientific literature.
Physical and chemical properties.
Sucrose is a molecule with nine stereocenters and many sites that are reactive or can be reactive. The molecule exists as a single isomer.
Structural O-α--glucopyranosyl-(1→2)-β--fructofuranoside.
In sucrose, the components glucose and fructose are linked via an ether bond between C1 on the glucosyl subunit and C2 on the fructosyl unit. The bond is called a glycosidic linkage. Glucose exists predominantly as two isomeric "pyranoses" (α and β), but only one of these forms links to the fructose. Fructose itself exists as a mixture of "furanoses", each of which having α and β isomers, but only one particular isomer links to the glucosyl unit. What is notable about sucrose is that, unlike most disaccharides, the glycosidic bond is formed between the reducing ends of both glucose and fructose, and not between the reducing end of one and the nonreducing end of the other. This linkage inhibits further bonding to other saccharide units. Since it contains no anomeric hydroxyl groups, it is classified as a non-reducing sugar.
Sucrose crystallizes in the monoclinic space group P21 with room-temperature lattice parameters "a" = 1.08631 nm, "b" = 0.87044 nm, "c" = 0.77624 nm, β = 102.938°.
The purity of sucrose is measured by polarimetry, through the rotation of plane-polarized light by a solution of sugar. The specific rotation at 20 °C using yellow "sodium-D" light (589 nm) is +66.47°. Commercial samples of sugar are assayed using this parameter. Sucrose does not deteriorate at ambient conditions.
Thermal and oxidative degradation.
Sucrose does not melt at high temperatures. Instead, it decomposes—at 186 °C—to form caramel. Like other carbohydrates, it combusts to carbon dioxide and water. Mixing sucrose with the oxidizer potassium nitrate produces the fuel known as rocket candy that is used to propel amateur rocket motors.
This reaction is somewhat simplified though. Some of the carbon does get fully oxidized to carbon dioxide, and other reactions, such as the water-gas shift reaction also take place. A more accurate theoretical equation is:
Sucrose burns with chloric acid, formed by the reaction of hydrochloric acid and potassium chlorate:
Sucrose can be dehydrated with sulfuric acid to form a black, carbon-rich solid, as indicated in the following idealized equation:
The formula for sucrose's decomposition can be represented as
in which carbon dioxide and water along with hydrogen gas is formed.
Hydrolysis.
Hydrolysis breaks the glycosidic bond converting sucrose into glucose and fructose. Hydrolysis is, however, so slow that solutions of sucrose can sit for years with negligible change. If the enzyme sucrase is added, however, the reaction will proceed rapidly. Hydrolysis can also be accelerated with acids, such as cream of tartar or lemon juice, both weak acids. Likewise, gastric acidity converts sucrose to glucose and fructose during digestion.
Synthesis and biosynthesis of sucrose.
The biosynthesis of sucrose proceeds via the precursors UDP-glucose and fructose 6-phosphate, catalyzed by the enzyme sucrose-6-phosphate synthase. The energy for the reaction is gained by the cleavage of Uridine diphosphate (UDP).
Sucrose is formed by plants and cyanobacteria but not by other organisms. Sucrose is found naturally in many food plants along with the monosaccharide fructose. In many fruits, such as pineapple and apricot, sucrose is the main sugar. In others, such as grapes and pears, fructose is the main sugar.
Chemical synthesis.
Although sucrose is invariably isolated from natural sources, its chemical synthesis was first achieved in 1953 by Raymond Lemieux.
Production.
History of sucrose.
The production of table sugar has a long history. Some scholars claim Indians discovered how to crystallize sugar during the Gupta dynasty, around AD 350.
Other scholars point to the ancient manuscripts of China, dated to the 8th century BC, where one of the earliest historical mentions of sugar cane is included along with the fact that their knowledge of sugar cane was derived from India. Further, it appears that by about 500 BC, residents of present-day India began making sugar syrup and cooling it in large flat bowls to make raw table sugar crystals that were easier to store and transport. In the local Indian language, these crystals were called "khanda" (खण्ड), which is the source of the word "candy".
The army of Alexander the Great was halted on the banks of river Indus by the refusal of his troops to go further east. They saw people in the Indian subcontinent growing sugarcane and making "granulated, salt-like sweet powder", locally called "sākhar" (साखर), pronounced as "sakcharon" (ζακχαρον) in Greek (Modern Greek, "zachari" ζάχαρη). On their return journey, the Greek soldiers carried back some of the "honey-bearing reeds". Sugarcane remained a limited crop for over a millennium. Sugar was a rare commodity and traders of sugar became wealthy. Venice, at the height of its financial power, was the chief sugar-distributing center of Europe. Arabs started producing it in Sicily and Spain. Only after the Crusades did it begin to rival honey as a sweetener in Europe. The Spanish began cultivating sugarcane in the West Indies in 1506 (and in Cuba in 1523). The Portuguese first cultivated sugarcane in Brazil in 1532.
Sugar remained a luxury in much of the world until the 18th century. Only the wealthy could afford it. In the 18th century, the demand for table sugar boomed in Europe and by the 19th century it had become regarded as a human necessity. The use of sugar grew from use in tea, to cakes, confectionery and chocolates. Suppliers marketed sugar in novel forms, such as solid cones, which required consumers to use a sugar nip, a pliers-like tool, in order to break off pieces.
The demand for cheaper table sugar drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and table sugar manufacturing could thrive. Growing sugar cane crop in hot humid climates, and producing table sugar in high temperature sugar mills was harsh, inhumane work. The demand for cheap and docile labor for this work, in part, first drove slave trade from Africa (in particular West Africa), followed by indentured labor trade from South Asia (in particular India). Millions of slaves, followed by millions of indentured laborers were brought into the Caribbean, Indian Ocean, Pacific Islands, East Africa, Natal, north and eastern parts of South America, and southeast Asia. The modern ethnic mix of many nations, settled in the last two centuries, has been influenced by table sugar.
Beginning in the late 18th century, the production of sugar became increasingly mechanized. The steam engine first powered a sugar mill in Jamaica in 1768, and, soon after, steam replaced direct firing as the source of process heat. During the same century, Europeans began experimenting with sugar production from other crops. Andreas Marggraf identified sucrose in beet root and his student Franz Achard built a sugar beet processing factory in Silesia (Poland). However, the beet-sugar industry really took off during the Napoleonic Wars, when France and the continent were cut off from Caribbean sugar. In 2010, about 20 percent of the world's sugar was produced from beets.
Today, a large beet refinery producing around 1,500 tonnes of sugar a day needs a permanent workforce of about 150 for 24-hour production.
Current trends.
Table sugar (sucrose) comes from plant sources. Two important sugar crops predominate: sugarcane ("Saccharum spp.") and sugar beets ("Beta vulgaris"), in which sugar can account for 12% to 20% of the plant's dry weight. Minor commercial sugar crops include the date palm ("Phoenix dactylifera"), sorghum ("Sorghum vulgare"), and the sugar maple ("Acer saccharum"). Sucrose is obtained by extraction of these crops with hot water, concentration of the extract gives syrups, from which solid sucrose can be crystallized. In 2013, worldwide production of table sugar amounted to 175 million tonnes.
Most cane sugar comes from countries with warm climates, because sugarcane does not tolerate frost. Sugar beets, on the other hand, grow only in cooler temperate regions and do not tolerate extreme heat. About 80 percent of sucrose is derived from sugarcane, the rest almost all from sugar beets.
In 2010, Brazil, India, European Union, China, Thailand, and United States were the major sugar-producing countries in the world. Brazil produced about 40 million tonnes of table sugar in 2013, while India produced 25 million, EU-27 countries 16 million, China 14 million, Thailand about 10 million, and United States over 7 million. The country rankings for table sugar production change with each year's sugarcane crop harvest and as new sugar production plants are commissioned worldwide.
Viewed by region, Asia predominates in cane sugar production, with large contributions from India, China, Thailand, and other countries combining to account for 40% of global production in 2006. South America comes in second place with 32% of global production; Africa and Central America each produce 8% and Australia 5%. The United States, the Caribbean, and Europe make up the remainder, with roughly 3% each.
Beet sugar comes from regions with cooler climates: northwest and eastern Europe, northern Japan, plus some areas in the United States (including California). In the northern hemisphere, the beet-growing season ends with the start of harvesting around September. Harvesting and processing continues until March in some cases. The availability of processing plant capacity and the weather both influence the duration of harvesting and processing – the industry can store harvested beets until processed, but a frost-damaged beet becomes effectively unprocessable.
Brazil is the world's largest sugar exporter at 29 million tonnes in the year 2013. The European Union (EU) has become the world's second-largest sugar exporter. The Common Agricultural Policy of the EU sets maximum quotas for members' production to match supply and demand, and a price. Europe exports excess production quota (approximately 5 million tonnes in 2003). Part of this, "quota" sugar, gets subsidised from industry levies, the remainder (approximately half) sells as "C quota" sugar at market prices without subsidy. These subsidies and a high import tariff make it difficult for other countries to export to the EU states, or to compete with the Europeans on world markets.
The United States sets high sugar prices to support its producers, with the effect that many former purchasers of sugar have switched to corn syrup (beverage manufacturers) or moved out of the country (candy manufacturers).
India consumes the most sugar at 26 million tonnes of table sugar in 2013. EU-27 is in second place at 18 million and China is third at above 16 million.
Low prices of sugar are expected to stimulate global consumption and trade, with exports forecast 4 percent higher at 59 million tons.
The low prices of glucose syrups produced from wheat and corn (maize) threaten the traditional sugar market. Used in combination with artificial sweeteners, they can allow drink manufacturers to produce very low-cost goods.
High-fructose corn syrup.
In the USA there are tariffs on the importation of sugar, and subsidies for the production of maize (corn). High-fructose corn syrup (HFCS) is derived from corn, and significantly cheaper there than sucrose as a sweetener. This has led to sucrose being partially displaced in U.S. industrial food production by HFCS and other non-sucrose natural sweeteners.
There are claims that consumption of fructose, and HFCS in particular, are responsible for the obesity epidemic in the US. The role of HFCS and sugars generally in the obesity epidemic are disputed by the industry. See also below, under Obesity.
Types.
Cane.
Since the 6th century BC, cane sugar producers have crushed the harvested vegetable material from sugarcane in order to collect and filter the juice. They then treat the liquid (often with lime (calcium oxide)) to remove impurities and then neutralize it. Boiling the juice then allows the sediment to settle to the bottom for dredging out, while the scum rises to the surface for skimming off. In cooling, the liquid crystallizes, usually in the process of stirring, to produce sugar crystals. Centrifuges usually remove the uncrystallized syrup. The producers can then either sell the sugar product for use as is, or process it further to produce lighter grades. The later processing may take place in another factory in another country.
Sugarcane is a major component of Brazilian agriculture; the country is a top producer of sugarcane products, such as crystallized sugar and ethanol (ethanol fuel). The sucrose found in sugarcane produces ethanol when fermented and distilled. Brazil has implemented ethanol as an alternative fuel on a national scale.
Beet.
Beet sugar producers slice the washed beets, then extract the sugar with hot water in a "diffuser". An alkaline solution ("milk of lime" and carbon dioxide from the lime kiln) then serves to precipitate impurities (see carbonatation). After filtration, evaporation concentrates the juice to a content of about 70% solids, and controlled crystallisation extracts the sugar. A centrifuge removes the sugar crystals from the liquid, which gets recycled in the crystalliser stages. When economic constraints prevent the removal of more sugar, the manufacturer discards the remaining liquid, now known as molasses, or sells it on to producers of animal feed.
Sieving the resultant white sugar produces different grades for selling.
Cane versus beet.
It is difficult to distinguish between fully refined sugar produced from beet and cane. One way is by isotope analysis of carbon. Cane uses C4 carbon fixation, and beet uses C3 carbon fixation, resulting in a different ratio of 13C and 12C isotopes in the sucrose. Tests are used to detect fraudulent abuse of European Union subsidies or to aid in the detection of adulterated fruit juice.
Sugar cane tolerates hot climates better, but the production of sugar cane needs approximately four times as much water as the production of sugar beet, therefore some countries that traditionally produced cane sugar (such as Egypt) have built new beet sugar factories since about 2008. Some sugar factories process both sugar cane and sugar beets and extend their processing period in that way.
The production of sugar leaves residues that differ substantially depending on the raw materials used and on the place of production. While cane molasses is often used in food preparation, humans find molasses from sugar beets unpalatable, and it consequently ends up mostly as industrial fermentation feedstock (for example in alcohol distilleries), or as animal feed. Once dried, either type of molasses can serve as fuel for burning.
Pure beet sugar is difficult to find, so labelled, in the marketplace. Although some brands label their product clearly as "pure cane sugar", beet sugar is almost always labeled simply as sugar or pure sugar. Interviews with the 5 major beet sugar-producing companies revealed that many store brands or "private label" sugar products are pure beet sugar. The lot code can be used to identify the company and the plant from which the sugar came, enabling beet sugar to be identified if the codes are known.
Culinary sugars.
Mill white.
Mill white, also called plantation white, crystal sugar or superior sugar is produced from raw sugar. It is exposed to sulfur dioxide during the production to reduce the concentration of color compounds and helps prevent further color development during the crystallization process. Although common to sugarcane-growing areas, this product does not store or ship well. After a few weeks, its impurities tend to promote discoloration and clumping; therefore this type of sugar is generally limited to local consumption.
Blanco directo.
Blanco directo, a white sugar common in India and other south Asian countries, is produced by precipitating many impurities out of cane juice using phosphoric acid and calcium hydroxide, similar to the carbonatation technique used in beet sugar refining. Blanco directo is more pure than mill white sugar, but less pure than white refined.
White refined.
White refined is the most common form of sugar in North America and Europe. Refined sugar is made by dissolving and purifying raw sugar using phosphoric acid similar to the method used for blanco directo, a carbonatation process involving calcium hydroxide and carbon dioxide, or by various filtration strategies. It is then further purified by filtration through a bed of activated carbon or bone char. Beet sugar refineries produce refined white sugar directly without an intermediate raw stage.
White refined sugar is typically sold as granulated sugar, which has been dried to prevent clumping and comes in various crystal sizes for home and industrial use:
Brown sugar comes either from the late stages of cane sugar refining, when sugar forms fine crystals with significant molasses content, or from coating white refined sugar with a cane molasses syrup (blackstrap molasses). Brown sugar's color and taste becomes stronger with increasing molasses content, as do its moisture-retaining properties. Brown sugars also tend to harden if exposed to the atmosphere, although proper handling can reverse this.
Measurement.
Dissolved sugar content.
Scientists and the sugar industry use degrees Brix (symbol °Bx), introduced by Adolf Brix, as units of measurement of the mass ratio of dissolved substance to water in a liquid. A 25 °Bx sucrose solution has 25 grams of sucrose per 100 grams of liquid; or, to put it another way, 25 grams of sucrose sugar and 75 grams of water exist in the 100 grams of solution.
The Brix degrees are measured using an infrared sensor. This measurement does not equate to Brix degrees from a density or refractive index measurement, because it will specifically measure dissolved sugar concentration instead of all dissolved solids. When using a refractometer, one should report the result as "refractometric dried substance" (RDS). One might speak of a liquid as having 20 °Bx RDS. This refers to a measure of percent by weight of "total" dried solids and, although not technically the same as Brix degrees determined through an infrared method, renders an accurate measurement of sucrose content, since sucrose in fact forms the majority of dried solids. The advent of in-line infrared Brix measurement sensors has made measuring the amount of dissolved sugar in products economical using a direct measurement.
Consumption.
Refined sugar was a luxury before the 18th century. It became widely popular in the 18th century, then graduated to becoming a necessary food in the 19th century. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. Eventually, table sugar became sufficiently cheap and common enough to influence standard cuisine and flavored drinks.
Sucrose forms a major element in confectionery and desserts. Cooks use it for sweetening — its fructose component, which has almost double the sweetness of glucose, makes sucrose distinctively sweet in comparison to other carbohydrate foods. It can also act as a food preservative when used in sufficient concentrations. Sucrose is important to the structure of many foods, including biscuits and cookies, cakes and pies, candy, and ice cream and sorbets. It is a common ingredient in many processed and so-called "junk foods."
Nutritional information.
Refined sugar is 99.9% sucrose, therefore providing only carbohydrate value as a major dietary nutrient and 390 kilocalories per 100 g serving (USDA data, right table). There are no micronutrients having significant content (right table).
Metabolism of sucrose.
In humans and other mammals, sucrose is broken down into its constituent monosaccharides, glucose and fructose, by sucrase or isomaltase glycoside hydrolases, which are located in the membrane of the microvilli lining the duodenum. The resulting glucose and fructose molecules are then rapidly absorbed into the bloodstream. In bacteria and some animals, sucrose is digested by the enzyme invertase.
Sucrose is an easily assimilated macronutrient that provides a quick source of energy, provoking a rapid rise in blood glucose upon ingestion. Sucrose, as a pure carbohydrate, has an energy content of 3.94 kilocalories per gram (or 17 kilojoules per gram).
Overconsumption of sucrose has been linked with adverse health effects.
Dental caries or tooth decay may be caused by oral bacteria converting sugars, including sucrose, from food into acids that attack tooth enamel.
When large amounts of refined food that contain high percentages of sucrose are consumed, beneficial nutrients can be displaced from the diet, which can contribute to an increased risk for chronic disease. The rapidity with which sucrose raises blood glucose can cause problems for people suffering from defective glucose metabolism, such as persons with hypoglycemia or diabetes mellitus.
Sucrose can contribute to the development of metabolic syndrome. In an experiment with rats that were fed a diet one-third of which was sucrose, the sucrose first elevated blood levels of triglycerides, which induced visceral fat and ultimately resulted in insulin resistance. Another study found that rats fed sucrose-rich diets developed high triglycerides, hyperglycemia, and insulin resistance. A 2004 study recommended that the consumption of sucrose-containing drinks should be limited due to the growing number of people with obesity and insulin resistance.
Human health.
Human beings have long sought sugars, but aside from wild honey, have not had access to the large quantities that characterize the modern diet. Studies have indicated potential links between consumption of free sugars including sucrose (particularly prevalent in processed foods) and health hazards, including obesity and tooth decay. It is also considered as a source of endogenous glycation processes since it metabolises into glucose and fructose in the body.
Tooth decay.
Tooth decay (dental caries) has become a prominent health hazard associated with the consumption of sugars, especially sucrose. Oral bacteria such as "Streptococcus mutans" live in dental plaque and metabolize "any" sugars (not just sucrose, but also glucose, lactose, fructose, and cooked starches) into lactic acid. The resultant lactic acid lowers the pH of the tooth's surface, stripping it of minerals in the process known as tooth decay.
All 6-carbon sugars and disaccharides based on 6-carbon sugars can be converted by dental plaque bacteria into acid that demineralizes teeth, but sucrose may be uniquely useful to "Streptococcus sanguinis" (formerly "Streptococcus sanguis") and "Streptococcus mutans". Sucrose is the only dietary sugar that can be converted to sticky glucans (dextran-like polysaccharides) by extracellular enzymes. These glucans allow the bacteria to adhere to the tooth surface and to build up thick layers of plaque. The anaerobic conditions deep in the plaque encourage the formation of acids, which leads to carious lesions. Thus, sucrose could enable "S. mutans", "S. sanguinis" and many other species of bacteria to adhere strongly and resist removal, e.g. by flow of saliva (although they are easily removed by brushing). The glucans and levans (fructose polysaccharides) produced by the plaque bacteria also act as a reserve food supply for the bacteria.
Such a special role of sucrose in the formation of tooth decay is much more significant in light of the almost universal use of sucrose as the most desirable sweetening agent. Widespread replacement of sucrose by high-fructose corn syrup (HFCS) has not diminished the danger from sucrose. If smaller amounts of sucrose are present in the diet, they will still be sufficient for the development of thick, anaerobic plaque and plaque bacteria will metabolise other sugars in the diet, such as the glucose and fructose in HFCS.
Glycemic index.
Sucrose is a disaccharide made up of 50% glucose and 50% fructose and has a glycemic index of 65. Sucrose is digested rapidly, but has a relatively low glycemic index due to its content of fructose, which has a minimal effect on blood glucose.
As with other sugars, sucrose is digested into its components via the enzyme sucrase to glucose (blood sugar) and fructose. The glucose component is transported into the blood (90%) and excess glucose is converted to temporary storage in the liver – named glycogen. The fructose is either bonded to cellulose and transported out the GI tract or processed by the liver into citrates, aldehydes, and, for the most part, lipid droplets (fat).
As the glycemic index measures the speed at which glucose is released into the bloodstream a refined sugar containing glucose is considered high-glycemic. As with other sugars, over-consumption may cause an increase in blood sugar levels from a normal 90 mg/dL to up over 150 mg/dL. (5 mmol/l to over 8.3 mmol/l).
Diabetes mellitus.
Diabetes mellitus, a disease that causes the body to metabolize sugar poorly, occurs when either:
When glucose builds up in the bloodstream, it can cause two problems:
Authorities advise diabetics to avoid sugar-rich foods to prevent adverse reactions.
Obesity.
The National Health and Nutrition Examination Survey I and their follow-on studies as part of a series indicate that the population in the United States has increased its proportion of energy consumption from carbohydrates and decreased its proportion from total fat while obesity has increased. This implies, along with the United Nations report cited below, that obesity may correlate better with sugar consumption than with fat consumption, and that reducing fat consumption while increasing sugar consumption may increase the level of obesity. The following table summarizes this study (based on the proportion of energy intake from different food sources for US Adults 20–74 years old, as carried out by the U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, National Center for Health Statistics, Hyattsville, MD):
Added sugar is not always evident in food products. While expected in desserts, candies, and soft drinks, it is also added to a wide range of non-sweet items such as bread, crackers, potato chips, peanut butter, soup, salad dressing, ketchup, mayonnaise, and many other common sauces. Forms of added sugar include technically accurate, but misleading, terms such as cane juice, evaporated cane juice, corn syrup and corn syrup solids, malt syrup, rice syrup, dextrose, maltose, maltodextrin, molasses, treacle, and xylose.
A 2002 study conducted by the U.S. National Academy of Sciences concluded that, due to discrepancies in data from different studies, it could not set a tolerable upper intake level, since "there is no clear and consistent association between increased intakes of added sugars and BMI." However, it explains that this may be due to the underreporting of the consumption of added sugars. (BMI, or "body mass index," is a measure of weight and height used to estimate body fat.)
Gout.
The occurrence of gout is connected with an excess production of uric acid. A diet rich in sucrose may lead to gout as it raises the level of insulin, which prevents excretion of uric acid from the body. As the concentration of uric acid in the body increases, so does the concentration of uric acid in the joint liquid and beyond a critical concentration, the uric acid begins to precipitate into crystals. Researchers have implicated sugary drinks high in fructose in a surge in cases of gout.
UN dietary recommendation.
In 2003, four United Nations agencies (including the World Health Organization and the Food and Agriculture Organization) commissioned a report compiled by a panel of 30 international experts. The panel stated that the total of free sugars (all monosaccharides and disaccharides added to foods by manufacturers, cooks or consumers, plus sugars naturally present in honey, syrups, and fruit juices) should not account for more than 10% of the energy intake of a healthy diet, while carbohydrates in total should represent between 55% and 75% of the energy intake.
Religious concerns.
The sugar refining industry often uses bone char (calcinated animal bones) for decolorizing. About 25% of sugar produced in the U.S. is processed using bone char as a filter, the remainder being processed with activated carbon. As bone char does not seem to remain in finished sugar, Jewish religious leaders consider sugar filtered through it to be pareve and therefore kosher.
Trade and economics.
One of the most widely-traded commodities in the world throughout history, sugar accounts for around 2% of the global dry cargo market. International sugar prices show great volatility, ranging from around 3 to over 60 cents per pound in the past[ [update]] 50 years. About 100 of the world's 180 countries produce sugar from beet or cane, a few more refine raw sugar to produce white sugar, and all countries consume sugar. Consumption of sugar ranges from around 3 kilograms per person per annum in Ethiopia to around 40 kg/person/yr in Belgium. Consumption per capita rises with income per capita until it reaches a plateau of around 35 kg per person per year in middle income countries.
Many countries subsidize sugar production heavily. The European Union, the United States, Japan, and many developing countries subsidize domestic production and maintain high tariffs on imports. Sugar prices in these countries have often exceeded prices on the international market by up to three times; today[ [update]], with world market sugar futures prices currently[ [update]] strong, such prices typically exceed world prices by two times.
World raw sugar price from 1960 to 2006
Within international trade bodies, especially in the World Trade Organization, the "G20" countries led by Brazil have long argued that, because these sugar markets in essence exclude cane sugar imports, the G20 sugar producers receive lower prices than they would under free trade. While both the European Union and United States maintain trade agreements whereby certain developing and less developed countries (LDCs) can sell certain quantities of sugar into their markets, free of the usual import tariffs, countries outside these preferred trade régimes have complained that these arrangements violate the "most favoured nation" principle of international trade. This has led to numerous tariffs and levies in the past.
In 2004, the WTO sided with a group of cane sugar exporting nations (led by Brazil and Australia) and ruled the EU sugar-régime and the accompanying ACP-EU Sugar Protocol (whereby a group of African, Caribbean, and Pacific countries receive preferential access to the European sugar market) illegal. In response to this and to other rulings of the WTO, and owing to internal pressures on the EU sugar-régime, the European Commission proposed on 22 June 2005 a radical reform of the EU sugar-régime, cutting prices by 39% and eliminating all EU sugar exports.
The African, Caribbean, Pacific and least developed country sugar exporters reacted with dismay to the EU sugar proposals. On 25 November 2005, the Council of the EU agreed to cut EU sugar prices by 36% as from 2009. In 2007, it seemed
that the U.S. Sugar Program could become the next target for reform. However, some commentators expected heavy lobbying from the U.S. sugar industry, which donated $2.7 million to US House and US Senate incumbents in the 2006 US election, more than any other group of US food-growers.
Especially prominent lobbyists include The Fanjul Brothers, so-called "sugar barons" who made the single largest[ [update]] individual contributions of soft money to both the Democratic and Republican parties in the political system of the United States of America.
Small quantities of sugar, especially specialty grades of sugar, reach the market as 'fair trade' commodities; the fair trade system produces and sells these products with the understanding that a larger-than-usual fraction of the revenue will support small farmers in the developing world. However, whilst the Fairtrade Foundation offers a premium of $60.00 per tonne to small farmers for sugar branded as "Fairtrade",
government schemes such the U.S. Sugar Program and the ACP Sugar Protocol offer premiums of around $400.00 per tonne above world market prices. However, the EU announced on 14 September 2007 that it had offered "to eliminate all duties and quotas on the import of sugar into the EU".
The US Sugar Association has launched a campaign to promote sugar over artificial substitutes. The Association now[ [update]] aggressively challenges many common beliefs regarding negative side-effects of sugar consumption. The campaign aired a high-profile television commercial during the 2007 Primetime Emmy Awards on FOX Television. The Sugar Association uses the trademark tagline "Sugar: sweet by nature."

</doc>
<doc id="50564" url="http://en.wikipedia.org/wiki?curid=50564" title="Gray code">
Gray code

The reflected binary code, also known as Gray code after Frank Gray, is a binary numeral system where two successive values differ in only one bit (binary digit).
The reflected binary code was originally designed to prevent spurious output from electromechanical switches. Today, Gray codes are widely used to facilitate error correction in digital communications such as digital terrestrial television and some cable TV systems.
Name.
 Bell Labs researcher Frank Gray introduced the term "reflected binary code" in his 1947 patent application, remarking that the code had "as yet no recognized name". He derived the name from the fact that it "may be built up from the conventional binary code by a sort of reflection process".
The code was later named after Gray by others who used it. Two different 1953 patent applications use "Gray code" as an alternative name for the "reflected binary code"; one of those also lists "minimum error code" and "cyclic permutation code" among the names. A 1954 patent application refers to "the Bell Telephone Gray code".
Motivation.
Many devices indicate position by closing and opening switches. If that device uses natural binary codes, these two positions would be right next to each other:
 011
 100
The problem with natural binary codes is that, with physical, mechanical switches, it is very unlikely that switches will change states exactly in synchrony. In the transition between the two states shown above, all three switches change state. In the brief period while all are changing, the switches will read some spurious position. Even without keybounce, the transition might look like 011 — 001 — 101 — 100. When the switches appear to be in position 001, the observer cannot tell if that is the "real" position 001, or a transitional state between two other positions. If the output feeds into a sequential system, possibly via combinational logic, then the sequential system may store a false value.
The reflected binary code solves this problem by changing only one switch at a time, so there is never any ambiguity of position,
 Dec Gray Binary
 0 000 000
 1 001 001
 2 011 010
 3 010 011
 4 110 100
 5 111 101
 6 101 110
 7 100 111
Notice that state 7 can roll over to state 0 with only one switch change. This is called the "cyclic" property of a Gray code. In the standard Gray coding the least significant bit follows a repetitive pattern of 2 on, 2 off ( … 11001100 … ); the next digit a pattern of 4 on, 4 off; and so forth.
More formally, a Gray code is a code assigning to each of a contiguous set of integers, or to each member of a circular list, a word of symbols such that each two adjacent code words differ by one symbol. These codes are also known as "single-distance codes", reflecting the Hamming distance of 1 between adjacent codes. There can be more than one Gray code for a given word length, but the term was first applied to a particular binary code for the non-negative integers, the "binary-reflected Gray code", or BRGC, the three-bit version of which is shown above.
History and practical application.
Reflected binary codes were applied to mathematical puzzles before they became known to engineers. The French engineer Émile Baudot used Gray codes in telegraphy in 1878. He received the French Legion of Honor medal for his work. The Gray code is sometimes attributed, incorrectly, to Elisha Gray (in "Principles of Pulse Code Modulation", K. W. Cattermole, for example).
Frank Gray, who became famous for inventing the signaling method that came to be used for compatible color television, invented a method to convert analog signals to reflected binary code groups using vacuum tube-based apparatus. The method and apparatus were patented in 1953 and the name of Gray stuck to the codes. The "PCM tube" apparatus that Gray patented was made by Raymond W. Sears of Bell Labs, working with Gray and William M. Goodall, who credited Gray for the idea of the reflected binary code.
The use of his eponymous codes that Gray was most interested in was to minimize the effect of error in the conversion of analog signals to digital; his codes are still used today for this purpose, and others.
Position encoders.
Gray codes are used in position encoders (linear encoders and rotary encoders), in preference to straightforward binary encoding. This avoids the possibility that, when several bits change in the binary representation of an angle, a misread will result from some of the bits changing before others. Originally, the code pattern was electrically conductive, supported (in a rotary encoder) by an insulating disk. Each track had its own stationary metal spring contact; one more contact made the connection to the pattern. That common contact was connected by the pattern to whichever of the track contacts were resting on the conductive pattern. However, sliding contacts wear out and need maintenance, which favors optical encoders.
Regardless of the care in aligning the contacts, and accuracy of the pattern, a natural-binary code would have errors at specific disk positions, because it is impossible to make all bits change at exactly the same time as the disk rotates. The same is true of an optical encoder; transitions between opaque and transparent cannot be made to happen simultaneously for certain exact positions. Rotary encoders benefit from the cyclic nature of Gray codes, because consecutive positions of the sequence differ by only one bit. This means that, for a transition from state A to state B, timing mismatches can only affect when the A → B transition occurs, rather than inserting one or more (up to "N" − 1 for an "N"-bit codeword) false intermediate states, as would occur if a standard binary code were used.
Towers of Hanoi.
The binary-reflected Gray code can also be used to serve as a solution guide for the Towers of Hanoi problem, as well as the classical Chinese rings puzzle, a sequential mechanical puzzle mechanism. It also forms a Hamiltonian cycle on a hypercube, where each bit is seen as one dimension.
Genetic algorithms.
Due to the Hamming distance properties of Gray codes, they are sometimes used in genetic algorithms. They are very useful in this field, since mutations in the code allow for mostly incremental changes, but occasionally a single bit-change can cause a big leap and lead to new properties.
Karnaugh maps.
Gray codes are also used in labelling the axes of Karnaugh maps.
Error correction.
In modern digital communications, Gray codes play an important role in error correction. For example, in a digital modulation scheme such as QAM where data is typically transmitted in symbols of 4 bits or more, the signal's constellation diagram is arranged so that the bit patterns conveyed by adjacent constellation points differ by only one bit. By combining this with forward error correction capable of correcting single-bit errors, it is possible for a receiver to correct any transmission errors that cause a constellation point to deviate into the area of an adjacent point. This makes the transmission system less susceptible to noise.
Communication between clock domains.
Digital logic designers use Gray codes extensively for passing multi-bit count information between synchronous logic that operates at different clock frequencies. The logic is considered operating in different "clock domains". It is fundamental to the design of large chips that operate with many different clocking frequencies.
Gray code counters and arithmetic.
A typical use of Gray code counters is building a FIFO (first-in, first-out) data buffer that has read and write ports that exist in different clock domains. The input and output counters inside such a dual-port FIFO are often stored using Gray code to prevent invalid transient states from being captured when the count crosses clock domains.
The updated read and write pointers need to be passed between clock domains when they change, to be able to track FIFO empty and full status in each domain. Each bit of the pointers is sampled non-deterministically for this clock domain transfer. So for each bit, either the old value or the new value is propagated. Therefore, if more than one bit in the multi-bit pointer is changing at the sampling point, a "wrong" binary value (neither new nor old) can be propagated. By guaranteeing only one bit can be changing, Gray codes guarantee that the only possible sampled values are the new or old multi-bit value. Typically Gray codes of power-of-two length are used.
Sometimes digital buses in electronic systems are used to convey quantities that can only increase or decrease by one at a time, for example the output of an event counter which is being passed between clock domains or to a digital-to-analog converter. The advantage of Gray codes in these applications is that differences in the propagation delays of the many wires that represent the bits of the code cannot cause the received value to go through states that are out of the Gray code sequence. This is similar to the advantage of Gray codes in the construction of mechanical encoders, however the source of the Gray code is an electronic counter in this case. The counter itself must count in Gray code, or if the counter runs in binary then the output value from the counter must be reclocked after it has been converted to Gray code, because when a value is converted from binary to Gray code, it is possible that differences in the arrival times of the binary data bits into the binary-to-Gray conversion circuit will mean that the code could go briefly through states that are wildly out of sequence. Adding a clocked register after the circuit that converts the count value to Gray code may introduce a clock cycle of latency, so counting directly in Gray code may be advantageous. A Gray code counter was patented in 1962 US3020481, and there have been many others since. In recent times a Gray code counter can be implemented as a state machine in Verilog. In order to produce the next count value, it is necessary to have some combinational logic that will increment the current count value that is stored in Gray code. Probably the most obvious way to increment a Gray code number is to convert it into ordinary binary code, add one to it with a standard binary adder, and then convert the result back to Gray code. This approach was discussed in a paper in 1996 and then subsequently patented by someone else in 1998 US5754614. Other methods of counting in Gray code are discussed in a report by R. W. Doran, including taking the output from the first latches of the master-slave flip flops in a binary ripple counter.
Perhaps the most common electronic counter with the "only one bit changes at a time" property is the Johnson counter.
Constructing an "n"-bit Gray code.
The binary-reflected Gray code list for "n" bits can be generated recursively from the list for "n" − 1 bits by reflecting the list (i.e. listing the entries in reverse order), concatenating the original list with the reversed list, prefixing the entries in the original list with a binary 0, and then prefixing the entries in the reflected list with a binary 1. For example, generating the "n" = 3 list from the "n" = 2 list:
The one-bit Gray code is "G"1 = (0, 1). This can be thought of as built recursively as above from a zero-bit Gray code "G"0 = { Λ } consisting of a single entry of zero length. This iterative process of generating "G""n"+1 from "G""n" makes the following properties of the standard reflecting code clear:
These characteristics suggest a simple and fast method of translating a binary value into the corresponding Gray code. Each bit is inverted if the next higher bit of the input value is set to one. This can be performed in parallel by a bit-shift and exclusive-or operation if they are available: the "n"th Gray code is obtained by computing formula_1
A similar method can be used to perform the reverse translation, but the computation of each bit depends on the computed value of the next higher bit so it cannot be performed in parallel. Assuming formula_2 is the formula_3th gray-coded bit (formula_4 being the most significant bit), and formula_5 is the formula_3th binary-coded bit (formula_7 being the most-significant bit), the reverse translation can be given recursively: formula_8, and formula_9. Alternatively, decoding a Gray code into a binary number can be described as a prefix sum of the bits in the Gray code, where each individual summation operation in the prefix sum is performed modulo two.
To construct the binary-reflected Gray code iteratively, at step 0 start with the formula_10, and at step formula_11 find the bit position of the least significant 1 in the binary representation of formula_3 and flip the bit at that position in the previous code formula_13 to get the next code formula_14. The bit positions start 0, 1, 0, 2, 0, 1, 0, 3, ... (sequence in OEIS). See find first set for efficient algorithms to compute these values.
Converting to and from Gray code.
The following functions in C convert between binary numbers and their associated Gray codes. While it may seem that gray-to-binary conversion requires each bit to be handled one at a time, faster algorithms exist.
Special types of Gray codes.
In practice, a "Gray code" almost always refers to a binary-reflected Gray code (BRGC).
However, mathematicians have discovered other kinds of Gray codes.
Like BRGCs, each consists of a lists of words, where each word differs from the next in only one digit (each word has a Hamming distance of 1 from the next word).
"n"-ary Gray code.
There are many specialized types of Gray codes other than the binary-reflected Gray code. One such type of Gray code is the "n"-ary Gray code, also known as a non-Boolean Gray code. As the name implies, this type of Gray code uses non-Boolean values in its encodings.
For example, a 3-ary (ternary) Gray code would use the values {0, 1, 2}. The ("n", "k")-"Gray code" is the "n"-ary Gray code with "k" digits.
The sequence of elements in the (3, 2)-Gray code is: {00, 01, 02, 12, 10, 11, 21, 22, 20}. The ("n", "k")-Gray code may be constructed recursively, as the BRGC, or may be constructed iteratively. An algorithm to iteratively generate the ("N", "k")-Gray code is presented (in C):
// inputs: base, digits, value
// output: gray
// Convert a value to a graycode with the given base and digits.
// Iterating through a sequence of values would result in a sequence
// of Gray codes in which only one digit changes at a time.
void to_gray(unsigned base, unsigned digits, unsigned value, unsigned gray[digits])
 unsigned baseN[digits]; // Stores the ordinary base-N number, one digit per entry
 unsigned i; // The loop variable
 // Put the normal baseN number into the baseN array. For base 10, 109 
 // would be stored as [9,0,1]
 for (i = 0; i < digits; i++) {
 baseN[i] = value % base;
 value = value / base;
 
 // Convert the normal baseN number into the graycode equivalent. Note that
 // the loop starts at the most significant digit and goes down.
 unsigned shift = 0;
 while (i--) {
 // The gray digit gets shifted down by the sum of the higher
 // digits.
 gray[i] = (baseN[i] + shift) % base;
 shift = shift + base - gray[i]; // Subtract from base so shift is positive
// EXAMPLES
// input: value = 1899, base = 10, digits = 4
// output: baseN[] = [9,9,8,1], gray[] = [0,1,7,1]
// input: value = 1900, base = 10, digits = 4
// output: baseN[] = [0,0,9,1], gray[] = [0,1,8,1]
There are other graycode algorithms for ("n","k")-Gray codes. The ("n","k")-Gray code produced by the above algorithm is always cyclical; some algorithms, such as that by Guan, lack this property when k is odd. On the other hand, while only one digit at a time changes with this method, it can change by wrapping (looping from "n" − 1 to 0). In Guan's algorithm, the count alternately rises and falls, so that the numeric difference between two graycode digits is always one.
Gray codes are not uniquely defined, because a permutation of the columns of such a code is a Gray code too. The above procedure produces a code in which the lower the significance of a digit, the more often it changes, making it similar to normal counting methods.
Balanced Gray code.
Although the binary reflected Gray code is useful in many scenarios, it is not optimal in certain cases because of a lack of "uniformity". In balanced Gray codes, the number of changes in different coordinate positions are as close as possible. To make this more precise, let "G" be an "R"-ary complete Gray cycle having transition sequence formula_15; the "transition counts (spectrum)" of "G" are the collection of integers defined by
A Gray code is "uniform" or "uniformly balanced" if its transition counts are all equal, in which case we have formula_17
for all "k". Clearly, when formula_18, such codes exist only if "n" is a power of 2. Otherwise, if "n" does not divide formula_19 evenly, it is possible to construct "well-balanced" codes where every transition count is either formula_20 or formula_21. Gray codes can also be "exponentially balanced" if all of their transition counts are adjacent powers of two, and such codes exist for every power of two.
For example, a balanced 4-bit Gray code has 16 transitions, which can be evenly distributed among all four positions (four transitions per position), making it uniformly balanced:
 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 
 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 
 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 
 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1
whereas a balanced 5-bit Gray code has a total of 32 transitions, which cannot be evenly distributed among the positions. In this example, four positions have six transitions each, and one has eight:
 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 
 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 
 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 
 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 
 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1
We will now show a construction for well-balanced binary Gray codes which allows us to generate an "n"-digit balanced Gray code for every "n". The main principle is to inductively construct an ("n" + 2)-digit Gray code formula_22 given an "n"-digit Gray code "G" in such a way that the balanced property is preserved. To do this, we consider partitions of formula_23 into an even number "L" of non-empty blocks of the form
where formula_25, and formula_26 (mod formula_27). This partition induces an formula_28-digit Gray code given by
If we define the "transition multiplicities" formula_33 to be the number of times the digit in position "i" changes between consecutive blocks in a partition, then for the ("n" + 2)-digit Gray code induced by this partition the transition spectrum formula_34 is
The delicate part of this construction is to find an adequate partitioning of a balanced "n"-digit Gray code such that the code induced by it remains balanced. Uniform codes can be found when formula_36 and formula_37, and this construction can be extended to the "R"-ary case as well.
Monotonic Gray codes.
Monotonic codes are useful in the theory of interconnection networks, especially for
minimizing dilation for linear arrays of processors.
If we define the "weight" of a binary string to be the number of 1s in
the string, then although we clearly cannot have a Gray code with strictly
increasing weight, we may want to approximate this by having the code run
through two adjacent weights before reaching the next one.
We can formalize the concept of monotone Gray codes as follows: consider the
partition of the hypercube formula_38 into "levels" of vertices
that have equal weight, i.e.
for formula_40. These levels satisfy formula_41. Let formula_42 be the subgraph of formula_43 induced by formula_44, and let formula_45 be the edges in formula_42. A monotonic Gray code is then a Hamiltonian path in formula_43 such that whenever formula_48 comes before formula_49 in the path, then formula_50.
An elegant construction of monotonic "n"-digit Gray codes for any "n" is based on the idea of recursively building subpaths formula_51 of length formula_52 having edges in formula_53. We define formula_54, formula_55 whenever formula_56 or formula_57, and
otherwise. Here, formula_59 is a suitably defined permutation and formula_60 refers
to the path "P" with its coordinates permuted by formula_61. These paths give rise to
two monotonic "n"-digit Gray codes formula_62 and formula_63 given by
The choice of formula_59 which ensures that these codes are indeed Gray codes turns out to be formula_66. The first few values of formula_51 are shown in the table below.
These monotonic Gray codes can be efficiently implemented in such a way that each subsequent element can be generated in "O"("n") time. The algorithm is most easily described using coroutines.
Monotonic codes have an interesting connection to the Lovász conjecture,
which states that every connected vertex-transitive graph contains a Hamiltonian
path. The "middle-level" subgraph formula_68 is vertex-transitive (that is, its automorphism group is transitive, so that each vertex has the same "local environment"" and cannot be differentiated from the others, since we can relabel the coordinates as well as the binary digits to obtain an automorphism) and the problem of finding a Hamiltonian path in this subgraph is called the "middle-levels problem", which can provide insights into the more general
conjecture. The question has been answered affirmatively for formula_69, and the preceding construction for monotonic codes ensures a Hamiltonian path of length at least 0.839"N" where "N" is the number of vertices in the middle-level
subgraph.
Beckett–Gray code.
Another type of Gray code, the Beckett–Gray code, is named for Irish playwright Samuel Beckett, who was interested in symmetry. His play "Quad" features four actors and is divided into sixteen time periods. Each period ends with one of the four actors entering or leaving the stage. The play begins with an empty stage, and Beckett wanted each subset of actors to appear on stage exactly once. Clearly the set of actors currently on stage can be represented by a 4-bit binary Gray code. Beckett, however, placed an additional restriction on the script: he wished the actors to enter and exit so that the actor who had been on stage the longest would always be the one to exit. The actors could then be represented by a first in, first out queue, so that (of the actors onstage) the actor being dequeued is always the one who was enqueued first. Beckett was unable to find a Beckett–Gray code for his play, and indeed, an exhaustive listing of all possible sequences reveals that no such code exists for "n" = 4. It is known today that such codes do exist for "n" = 2, 5, 6, 7, and 8, and do not exist for "n" = 3 or 4. An example of an 8-bit Beckett–Gray code can be found in Knuth's "Art of Computer Programming". According to Sawada and Wong, the search space for "n" = 6 can be explored in 15 hours, and more than 9,500 solutions for the case "n" = 7 have been found.
Snake-in-the-box codes.
Snake-in-the-box codes, or "snakes", are the sequences of nodes of induced paths in an "n"-dimensional hypercube graph, and coil-in-the-box codes, or "coils", are the sequences of nodes of induced cycles in a hypercube. Viewed as Gray codes, these sequences have the property of being able to detect any single-bit coding error. Codes of this type were first described by W. H. Kautz in the late 1950s; since then, there has been much research on finding the code with the largest possible number of codewords for a given hypercube dimension.
Single-track Gray code.
Yet another kind of Gray code is the single-track Gray code (STGC) developed by N. B. Spedding (NZ Patent 264738 - October 28, 1994)
and refined by Hiltgen, Paterson and Brandestini in "Single-track Gray codes" (1996). The STGC is a cyclical list of "P" unique binary encodings of length n such that two consecutive words differ in exactly one position, and when the list is examined as a "P" × "n" matrix, each column is a cyclic shift of the first column.
The name comes from their use with rotary encoders, where a number of tracks are being sensed by contacts, resulting for each in an output of 0 or 1. To reduce noise due to different contacts not switching at exactly the same moment in time, one preferably sets up the tracks so that the data output by the contacts are in Gray code. To get high angular accuracy, one needs lots of contacts; in order to achieve at least 1 degree accuracy, one needs at least 360 distinct positions per revolution, which requires a minimum of 9 bits of data, and thus the same number of contacts.
If all contacts are placed at the same angular position, then 9 tracks are needed to get a standard BRGC with at least 1 degree accuracy. However, if the manufacturer moves a contact to a different angular position (but at the same distance from the center shaft), then the corresponding "ring pattern" needs to be rotated the same angle to give the same output. If the most significant bit (the inner ring in Figure 1) is rotated enough, it exactly matches the next ring out. Since both rings are then identical, the inner ring can be cut out, and the sensor for that ring moved to the remaining, identical ring (but offset at that angle from the other sensor on that ring). Those two sensors on a single ring make a quadrature encoder.
That reduces the number of tracks for a "1 degree resolution" angular encoder to 8 tracks. Reducing the number of tracks still further can't be done with BRGC.
For many years, Torsten Sillke and other mathematicians believed that it was impossible to encode position on a single track such that consecutive positions differed at only a single sensor, except for the 2-sensor, 1-track quadrature encoder.
So for applications where 8 tracks were too bulky, people used single-track incremental encoders (quadrature encoders) or 2-track "quadrature encoder + reference notch" encoders.
N. B. Spedding, however, registered a patent in 1994 with several examples showing that it was possible. Although it is not possible to distinguish 2"n" positions with "n" sensors on a single track, it "is" possible to distinguish close to that many. For example, when "n" is itself a power of 2, "n" sensors can distinguish 2"n" − 2"n" positions. Hiltgen and Paterson published a paper in 2001 exhibiting a single-track gray code with exactly 360 angular positions, constructed using 9 sensors. Since this number is larger than 28 = 256, more than 8 sensors are required by any code, although a BRGC could distinguish 512 positions with 9 sensors.
An STGC for "P" = 30 and "n" = 5 is reproduced here:
 10000
 10100
 11100
 11110
 11010
 11000
 01000
 01010
 01110
 01111
 01101
 01100
 00100
 00101
 00111
 10111
 10110
 00110
 00010
 10010
 10011
 11011
 01011
 00011
 00001
 01001
 11001
 11101
 10101
 10001
Each column is a cyclic shift of the first column, and from any row to the next row only one bit changes.
The single-track nature (like a code chain) is useful in the fabrication of these wheels (compared to BRGC), as only one track is needed, thus reducing their cost and size.
The Gray code nature is useful (compared to chain codes, also called De Bruijn sequences), as only one sensor will change at any one time, so the uncertainty during a transition between two discrete states will only be plus or minus one unit of angular measurement the device is capable of resolving.
Gray isometry.
The bijective mapping { 0 ↔ 00, 1 ↔ 01, 2 ↔ 11, 3 ↔ 10 } establishes an isometry between the metric space over the finite field formula_70 with the metric given by the Hamming distance and the metric space over the finite ring formula_71 (the usual modulo arithmetic) with the metric given by the Lee distance. The mapping is suitably extended to an isometry of the Hamming spaces formula_72 and formula_73. Its importance lies in establishing a correspondence between various "good" but not necessarily linear codes as Gray-map images in formula_70 of ring-linear codes from formula_71.

</doc>
<doc id="50565" url="http://en.wikipedia.org/wiki?curid=50565" title="Executive">
Executive

Executive may refer to:

</doc>
<doc id="50567" url="http://en.wikipedia.org/wiki?curid=50567" title="Gold code">
Gold code

A Gold code, also known as Gold sequence, is a type of binary sequence, used in telecommunication (CDMA) and satellite navigation (GPS). Gold codes are named after Robert Gold. Gold codes have bounded small cross-correlations within a set, which is useful when multiple devices are broadcasting in the same frequency range. A set of Gold code sequences consists of 2"n" − 1 sequences each one with a period of 2"n" − 1.
A set of Gold codes can be generated with the following steps. Pick two maximum length sequences of the same length 2"n" − 1 such that their absolute cross-correlation is less than or equal to 2("n"+2)/2, where "n" is the size of the LFSR used to generate the maximum length sequence (Gold '67). The set of the 2"n" − 1 exclusive-ors of the two sequences in their various phases (i.e. translated into all relative positions) is a set of Gold codes. The highest absolute cross-correlation in this set of codes is 2("n"+2)/2 + 1 for even "n" and 2("n"+1)/2 + 1 for odd "n".
The exclusive or of two different Gold codes from the same set is another Gold code in some phase.
Within a set of Gold codes about half of the codes are balanced – the number of ones and zeros differs by only one.
References.
</dl>

</doc>
<doc id="50568" url="http://en.wikipedia.org/wiki?curid=50568" title="Personal rapid transit">
Personal rapid transit

Personal rapid transit (PRT), also called podcar, is a public transport mode featuring small automated vehicles operating on a network of specially built guide ways. PRT is a type of automated guideway transit (AGT), a class of system which also includes larger vehicles all the way to small subway systems.
PRT vehicles are sized for individual or small group travel, typically carrying no more than 3 to 6 passengers per vehicle. Guide ways are arranged in a network topology, with all stations located on sidings, and with frequent merge/diverge points. This allows for nonstop, point-to-point travel, bypassing all intermediate stations. The point-to-point service has been compared to a taxi or a horizontal lift (elevator).
As of July 2013, four PRT systems are operational: The world's oldest and most extensive PRT system is in Morgantown, West Virginia. It has been in continuous operation since 1975. Colloquially known merely as 'the PRT,' West Virginia University's system moves student and visitors alike to a number of popular destinations throughout the city. Since 2010 a 10-vehicle 2getthere system at Masdar City, UAE, and since 2011 a 21-vehicle Ultra PRT system at London Heathrow Airport. A 40-vehicle Vectus system with in-line stations officially opened in Suncheon, South Korea in April 2014 after a year of testing. Expansion of the Masdar system was cancelled just after the pilot scheme opened. Numerous other PRT systems have been proposed but not implemented, including many substantially larger than those now operating.
Overview.
Most mass transit systems move people in groups over scheduled routes. This has inherent inefficiencies. For passengers, time is wasted by waiting for the next arrival, indirect routes to their destination, stopping for passengers with other destinations, and often confusing or inconsistent schedules. Slowing and accelerating large weights can undermine public transport's benefit to the environment while slowing other traffic. Personal rapid transit systems attempt to eliminate these wastes by moving small groups nonstop in automated vehicles on fixed tracks. Passengers can ideally board a pod immediately upon arriving at a station, and can — with a sufficiently extensive network of tracks — take relatively direct routes to their destination without stops.
Perhaps most importantly, PRT systems offer many traits similar to cars. For example, they offer privacy and the ability to choose one's own schedule. PRT may in fact allow for quicker transportation than cars during rush hour, since automated vehicles avoid unnecessary slowing. A PRT system can also transport freight.
The low weight of PRT's small vehicles allows smaller guideways and support structures than mass transit systems like light rail. The smaller structures translate into lower construction cost, smaller easements, and less visually obtrusive infrastructure.
As it stands, a city-wide deployment with many lines and closely spaced stations, as envisioned by proponents, has yet to be constructed. Past projects have failed because of financing, cost overruns, regulatory conflicts, political issues, misapplied technology, and flaws in design, engineering or review.
However, the theory remains active. For example, from 2002–2005, the EDICT project, sponsored by the European Union, conducted a study on the feasibility of PRT in four European cities. The study involved 12 research organizations, and concluded that PRT:
The report also concluded that, despite these advantages, public authorities will not commit to building PRT because of the risks associated with being the first public implementation.
The PRT acronym was introduced formally in 1978 by J. Edward Anderson. The Advanced Transit Association (ATRA), a group which advocates the use of technological solutions to transit problems, compiled a definition in 1988 that can be seen here.
List of operational ATN systems.
Currently, five advanced transit networks (ATN) systems are operational, and several more are in the planning stage.
List of automated transit networks (ATN) suppliers.
The following list summarizes several well-known automated transit networks (ATN) suppliers as of 2014.
History.
Origins.
Modern PRT concepts began around 1953 when Donn Fichter, a city transportation planner, began research on PRT and alternative transportation methods. In 1964, Fichter published a book, which proposed an automated public transit system for areas of medium to low population density. One of the key points made in the book was Fichter's belief that people would not leave their cars in favor of public transit unless the system offered flexibility and end-to-end transit times that were much better than existing systems – flexibility and performance he felt only a PRT system could provide. Several other urban and transit planners also wrote on the topic and some early experimentation followed, but PRT remained relatively unknown.
Around the same time, Edward Haltom was studying monorail systems. Haltom noticed that the time to start and stop a conventional large monorail train, like those of the Wuppertal Schwebebahn, meant that a single line could only support between 20 and 40 vehicles an hour. In order to get reasonable passenger movements on such a system, the trains had to be large enough to carry hundreds of passengers (see headway for a general discussion). This, in turn, demanded large guideways that could support the weight of these large vehicles, driving up capital costs to the point where he considered them unattractive.
Haltom turned his attention to developing a system that could operate with shorter timings, thereby allowing the individual cars to be smaller while preserving the same overall route capacity. Smaller cars would mean less weight at any given point, which meant smaller and less expensive guideways. To eliminate the backup at stations, the system used "offline" stations that allowed the mainline traffic to bypass the stopped vehicles. He designed the Monocab system using six-passenger cars suspended on wheels from an overhead guideway. Like most suspended systems, it suffered from the problem of difficult switching arrangements. Since the car rode on a rail, switching from one path to another required the rail to be moved; a slow process that limited the possible headways.
UMTA is formed.
By the late 1950s the problems with urban sprawl were becoming evident in the US. When cities improved roads and the transit times were lowered, suburbs developed at ever increasing distances from the city cores, and people moved out of the downtown areas. Lacking pollution control systems, the rapid rise in car ownership and the longer trips to and from work was causing significant air quality problems. Additionally, movement to the suburbs led to a flight of capital from the downtown areas, one cause of the rapid urban decay seen in the US.
Mass transit systems were one way to combat these problems. Yet during this period, the US federal government was feeding the problems by funding the development of the Interstate Highway System, while at the same time funding for mass transit was being rapidly scaled back. Public transit ridership in most cities plummeted.
In 1962, President John F. Kennedy charged the United States Congress with the task of addressing these problems. These plans came to fruition in 1964, when President Lyndon B. Johnson signed the Urban Mass Transportation Act of 1964 into law, thereby forming the Urban Mass Transportation Administration. The UMTA was set up to fund mass transit developments in the same fashion that the earlier Federal Aid Highway Act of 1956 had helped create in the Interstate Highways. That is, the UMTA would help cover the capital costs of building out new infrastructure.
PRT research starts.
However, planners who were aware of the PRT concept were worried that building more systems based on existing technologies would not help the problem, as Fitcher had earlier noted. Proponents suggested that systems would have to offer the flexibility of a car:
The reason for the sad state of public transit is a very basic one - the transit systems just do not offer a service which will attract people away from their automobiles. Consequently, their patronage comes very largely from those who cannot drive, either because they are too young, too old, or because they are too poor to own and operate an automobile. Look at it from the standpoint of a commuter who lives in a suburb and is trying to get to work in the central business district (CBD). If he is going to go by transit, a typical scenario might be the following: he must first walk to the closest bus stop, let us say a five or ten minute walk, and then he may have to wait up to another ten minutes, possibly in inclement weather, for the bus to arrive. When it arrives, he may have to stand unless he is lucky enough to find a seat. The bus will be caught up in street congestion and move slowly, and it will make many stops completely unrelated to his trip objective. The bus may then let him off at a terminal to a suburban train. Again he must wait, and, after boarding the train, again experience a number of stops on the way to the CBD, and possibly again he may have to stand in the aisle. He will get off at the station most convenient to his destination and possibly have to transfer again onto a distribution system. It is no wonder that in those cities where ample inexpensive parking is available, most of those who can drive do drive.
In 1966, the United States Department of Housing and Urban Development was asked to "undertake a project to study … new systems of urban transportation that will carry people and goods … speedily, safely, without polluting the air, and in a manner that will contribute to sound city planning". The resulting report was published in 1968, and proposed the development of PRT, as well as other systems such as dial-a-bus and high-speed interurban links
In the late 1960s, the Aerospace Corporation, an independent non-profit corporation set up by the US Congress, spent substantial time and money on PRT, and performed much of the early theoretical and systems analysis. However, this corporation is not allowed to sell to non-federal government customers. In 1969, members of the study team published the first widely publicized description of PRT in "Scientific American".
In 1978 the team also published a book. These publications sparked off a sort of "transit race" in the same sort of fashion as the space race, with countries around the world rushing to join what appeared to be a future market of immense size.
The oil crisis of 1973 made vehicle fuels more expensive, which naturally interested people in alternative transportation.
System developments.
In 1967, aerospace giant Matra started the Aramis project in Paris. After spending about 500 million francs, the project was canceled when it failed its qualification trials in November 1987. The designers tried to make Aramis work like a "virtual train," but control software issues caused cars to bump unacceptably. The project ultimately failed.
Between 1970 and 1978, Japan operated a project called "Computer-controlled Vehicle System" (CVS). In a full-scale test facility, 84 vehicles operated at speeds up to 60 km/h on a 4.8 km guideway; one-second headways were achieved during tests. Another version of CVS was in public operation for six months from 1975–1976. This system had 12 single-mode vehicles and four dual-mode vehicles on a 1.6 km track with five stations. This version carried over 800,000 passengers. CVS was cancelled when Japan's Ministry of Land, Infrastructure and Transport declared it unsafe under existing rail safety regulations, specifically in respect of braking and headway distances.
On March 23, 1973, U.S. Urban Mass Transportation Administration (UMTA) administrator Frank Herringer testified before Congress: "A DOT program leading to the development of a short, one-half to one-second headway, high-capacity PRT (HCPRT) system will be initiated in fiscal year 1974." However, this HCPRT program was diverted into a modest technology program. According to PRT supporter J. Edward Anderson, this was "because of heavy lobbying from interests fearful of becoming irrelevant if a genuine PRT program became visible". From that time forward people interested in HCPRT were unable to obtain UMTA research funding.
In 1975, the Morgantown Personal Rapid Transit project was completed. It has five off-line stations that enable non-stop, individually programmed trips along an 8.7-mile track serviced by a fleet of 71 cars. This is a crucial characteristic of PRT. However, it is not considered a PRT system because its vehicles are too heavy and carry too many people. When it carries many people, it does not operate in a point-to-point fashion, instead running like an automated people mover from one end of the line to the other. Morgantown PRT is still in continuous operation at West Virginia University in Morgantown, West Virginia with about 15,000 riders per day (as of 2003[ [update]]). It successfully demonstrates automated control, but was not sold to other sites because the steam-heated track has proven too expensive for a system that requires an operation and maintenance budget of $5 million annually.
From 1969 to 1980, Mannesmann Demag and MBB cooperated to build the "Cabinentaxi" urban transportation system in Germany. Together the firms formed the Cabintaxi Joint Venture. They created an extensive PRT technology that was considered fully developed by the German Government and its safety authorities. The system was to have been installed in Hamburg, but budget cuts stopped the proposed project before the start of construction. With no other potential projects on the horizon, the joint venture disbanded, and the fully developed PRT technology was never installed. Cabintaxi Corporation, a US-based company obtained the technology in 1985, and remains active in the private-sector market for transportation systems.
Later developments.
In the 1990s, Raytheon invested heavily in a system called PRT 2000, based on technology developed by J. Edward Anderson at the University of Minnesota. Raytheon failed to install a contracted system in Rosemont, Illinois, near Chicago, when estimated costs escalated to US$50 million per mile, allegedly due to design changes that increased the weight and cost of the system relative to Anderson's original design. In 2000, rights to the technology reverted to the University of Minnesota, and were subsequently purchased by Taxi2000.
In 2002, 2getthere operated 25 4-passenger "CyberCabs" at Holland's 2002 Floriade horticultural exhibition. These transported passengers along a track spiraling up to the summit of Big Spotters Hill. The track was approximately 600 m long (one-way) and featured only two stations. The six-month operations were intended to research the public acceptance of PRT-like systems. The CyberCab as designed for the exhibition was very open. It was comparable to a Neighborhood electric vehicle, except it steered itself using magnetic guidance points embedded in the pavement.
Ford Research proposed a dual-mode system called PRISM.
It would use public guideways with privately purchased but certified dual-mode vehicles. The vehicles would weigh less than 600 kg. Most energy use occurs on highways, so small, elevated guideways would inductively power highway use and recharge batteries for off-guideway use. Central computers could do routing.
In January 2003, the prototype ULTra ("Urban Light Transport") system in Cardiff, Wales, was certified to carry passengers by the UK Railway Inspectorate on a 1 km test track. ULTra was selected in October 2005 by BAA plc for London's Heathrow Airport. Since May 2011 a three station system has been open to the public, transporting passengers from a remote parking lot to terminal 5. In May 2013 Heathrow Airport Limited included in its draft five year (2014-2019) master plan a scheme to use the PRT system to connect terminal 2 and terminal 3 to their respective business car parks. The proposal was not included in the final plan due to spending priority given to other capital projects and has been deferred.
In June 2006, a Korean/Swedish consortium, Vectus Ltd, started constructing a 400 m test track in Uppsala, Sweden.
This test system was presented at the 2007 PodCar City conference in Uppsala, Sweden. A 40-vehicle, 2 station, 4.46 km system called "SkyCube" was opened in Suncheon, South Korea in April 2014.
The Vectus project was based on The Fornebu/Oslo PRT Project. At the time, the urban development area around Telenor's new headquarter (at the Fornebu area near Oslo) was subject to intense debates as to various more or less innovative public transport systems. The idea of a PRT came up as a possible local solution as well as a business opportunity. In 2000, The Fornebu/Oslo PRT Project started as a part of an internal educational exercise within ICT strategy innovation within Telenor ASA, a major ICT corporation. As the poster shows, the student project was later transformed into a fast working concept, technology and business development project with various industry partners and a project group of around 10. The Korean steel company POSCO joined in, and developed the project further in Uppsala, Sweden, in part through new partners, but with all essential elements from the Fornebu/Oslo PRT Project, as further industrial or governmental support found in the Oslo area vanished. The poster describes the consortium and main results from the Oslo PRT project period. Key persons in this concept development phase were - as to technology and operational features development - Ingmar Andreasson, Göteborg, Sweden, Jan Orsten, indep. traffic planner, Oslo, Alan Forster, Force Ltd, GB, and Andrew Howard, HWG Ltd, GB. Beyond the general conceptual description, the ICT systems were developed by Noventus AB and others at later stages.
In 2007, the Polish PRT system MISTER was prototyped, and permission was given to install it in two Polish cities. MISTER is a typical overhead PRT system engineered for economical aerial reuse of streets' right of ways, that still gives ground-level access to wheelchairs and freight.
System design.
Among the handful of prototype systems (and the larger number that exist on paper) there is a substantial diversity of design approaches, some of which are controversial.
Vehicle design.
Vehicle weight influences the size and cost of a system's guideways, which are in turn a major part of the capital cost of the system. Larger vehicles are more expensive to produce, require larger and more expensive guideways, and use more energy to start and stop. If vehicles are too large, point-to-point routing also becomes more expensive. Against this, smaller vehicles have more surface area per passenger (thus have higher total air resistance which dominates the energy cost of keeping vehicles moving at speed) and larger motors are generally more efficient than smaller ones.
The number of riders who will share a vehicle is a key unknown. In the U.S., the average car carries 1.16 persons, and most industrialized countries commonly average below two people; not having to share a vehicle with strangers is a key advantage of private transport. Based on these figures, some have suggested that two passengers per vehicle (such as with UniModal), or even a single passenger per vehicle is optimum. Other designs use a car for a model, and choose larger vehicles, making it possible to accommodate families with small children, riders with bicycles, disabled passengers with wheelchairs, or a pallet or two of freight.
Propulsion.
All current designs (except for the human-powered Shweeb) are powered by electricity. In order to reduce vehicle weight, power is generally transmitted via lineside conductors rather than using on-board batteries. According to the designer of Skyweb/Taxi2000, J. Edward Anderson, the lightest system is a linear induction motor (LIM) on the car, with a stationary conductive rail for both propulsion and braking. LIMs are used in a small number of rapid transit applications, but most designs use rotary motors. Most such systems retain a small on-board battery to reach the next stop after a power-failure.
ULTra uses on-board batteries, recharged at stops. This increases the safety, and reduces the complexity, cost and maintenance of the guideway. As a result, a street-level ULTRa guideway resembles a sidewalk with curbs and is very inexpensive to construct. ULTRa resembles a small automated electric car, and uses similar components.
Switching.
Most designers avoid track switching, instead advocating vehicle-mounted switches or conventional steering. Those designers say that vehicle-switching permits faster switching, so vehicles can be closer together. It also simplifies the guideway, makes junctions less visually obtrusive and reduces the impact of malfunctions, because a failed switch on one vehicle is less likely to affect other vehicles. Other designers point out that track-switching simplifies the vehicles, reducing the number of small moving parts in each car. Track-switching replaces in-vehicle mechanisms with larger track-moving components, that can be designed for durability with little regard for weight or size.
Track switching greatly increases headway distance. A vehicle must wait for the previous vehicle to clear the track, for the track to switch and for the switch to be verified. If the track switching is faulty, vehicles must be able to stop before reaching the switch, and all vehicles approaching the failed junction would be affected.
Mechanical vehicle switching minimizes inter-vehicle spacing or headway distance, but it also increases the minimum distances between consecutive junctions. A mechanically switching vehicle, maneuvering between two adjacent junctions with different switch settings cannot proceed from one junction to the next. The vehicle must adopt a new switch position, and then wait for the in-vehicle switch's locking mechanism to be verified. If the vehicle switching is faulty, that vehicle must be able to stop before reaching the next switch, and all vehicles approaching the failed vehicle would be affected.
Conventional steering allows a simpler 'track' consisting only of a road surface with some form of reference for the vehicle's steering sensors. Switching would be accomplished by the vehicle following the appropriate reference line- maintaining a set distance from the left roadway edge would cause the vehicle to diverge left at a junction, for example.
Infrastructure design.
Guideways.
There is some debate over the best type of guideway. Proposals include beams similar to monorails, bridge-like trusses supporting internal tracks, and cables embedded in a roadway. Most designs put the vehicle on top of the track, which reduces visual intrusion and cost as well as easing ground-level installation. An overhead track is necessarily higher, but may also be narrower. Most designs use the guideway to distribute power and data communications, including to the vehicles. The Morgantown PRT failed its cost targets because of its steam-heated track, so most proposals plan to resist snow and ice in ways that should be less expensive. Masdar's system has been limited because it attempted to dedicate ground-level to PRT guideways. This led to unrealistically expensive buildings and roads.
Stations.
Proposals usually have stations close together, and located on side tracks so that through traffic can bypass vehicles picking up or dropping off passengers. Each station might have multiple berths, with perhaps one-third of the vehicles in a system being stored at stations waiting for passengers. Stations are envisioned to be minimalistic, without facilities such as rest rooms. For elevated stations, an elevator may be required for accessibility.
At least one system, MISTER provides wheelchair and freight access by using a cogway in the track, so that the vehicle itself can go from a street-level stop to an overhead track.
Some designs have included substantial extra expense for the track needed to decelerate to and accelerate from stations. In at least one system, Aramis, this nearly doubled the width and cost of the required right-of-way and caused the nonstop passenger delivery concept to be abandoned. Other designs have schemes to reduce this cost, for example merging vertically to reduce the footprint.
When user demand is low, surplus vehicles could be configured to stop at empty stations at strategically placed points around the network. This enables an empty vehicle to quickly be despatched to wherever it is required, with minimal waiting time for the passenger.
Operational characteristics.
Headway distance.
Spacing of vehicles on the guideway influences the maximum passenger capacity of a track, so designers prefer smaller headway distances. Computerized control theoretically permits closer spacing than the two-second headways recommended for cars at speed, since multiple vehicles can be braked simultaneously. There are also prototypes for automatic guidance of private cars based on similar principles.
Very short headways are controversial. The UK Railway Inspectorate has evaluated the ULTra design and is willing to accept one-second headways, pending successful completion of initial operational tests at more than 2 seconds. In other jurisdictions, existing rail regulations apply to PRT systems (see CVS, above); these typically calculate headways in terms of absolute stopping distances, which would restrict capacity and make PRT systems unfeasible. No regulatory agency has yet endorsed headways below one second, although proponents believe that regulators may be willing to reduce headways as operational experience increases.
Capacity.
PRT is usually proposed as an alternative to rail systems, so comparisons tend to be with rail. PRT vehicles seat fewer passengers than trains and buses, and must offset this by combining higher average speeds, diverse routes, and shorter headways. Proponents assert that equivalent or higher overall capacity can be achieved by these means.
Single line capacity.
With two-second headways and four-person vehicles, a single PRT line can achieve theoretical maximum capacity of 7,200 passengers per hour. However, most estimates assume that vehicles will not generally be filled to capacity, due to the point-to-point nature of PRT. At a more typical average vehicle occupancy of 1.5 persons per vehicle, the maximum capacity is 2,700 passengers per hour. Some researchers have suggested that rush hour capacity can be improved if operating policies support ridesharing.
Capacity is inversely proportional to headway. Therefore, moving from two-second headways to one-second headways would double PRT capacity. Half-second headways would quadruple capacity. Theoretical minimum PRT headways would be based on the mechanical time to engage brakes, and these are much less than a half second. Although no regulatory agency has as yet (June 2006) approved headways shorter than two seconds, researchers suggest that "high capacity PRT" (HCPRT) designs could operate safely at half-second headways. Using the above figures, capacities above 10,000 passengers per hour seem in reach.
In simulations of rush hour or high-traffic events, about one-third of vehicles on the guideway need to travel empty to resupply stations with vehicles in order to minimize response time. This is analogous to trains and buses travelling nearly empty on the return trip to pick up more rush hour passengers.
Grade separated light rail systems can move 15,000 passengers per hour on a fixed route, but these are usually fully grade separated systems. Street level systems typically move up to 7,500 passengers per hour. Heavy rail subways can move 50,000 passengers per hour. As with PRT, these estimates depend on having enough trains. Neither light nor heavy rail scales well for off-peak operation.
Networked PRT capacity.
The above discussion compares line or corridor capacity and may therefore not be relevant for a networked PRT system, where several parallel lines (or parallel components of a grid) carry traffic. In addition, Muller estimated that while PRT may need more than one guideway to match the capacity of a conventional system, the capital cost of the multiple guideways may still be less than that of the single guideway conventional system. Thus comparisons of line capacity should also consider the cost per line.
PRT systems should require much less horizontal space than existing metro systems, with individual cars being typically around 50% as wide for side-by-side seating configurations, and less than 33% as wide for single-file configurations. This is an important factor in densely populated, high-traffic areas.
Travel speed.
For a given peak speed, nonstop journeys are about three times as fast as those with intermediate stops. This is not just because of the time for starting and stopping. Scheduled vehicles are also slowed by boardings and exits for multiple destinations.
Therefore, a given PRT seat transports about three times as many passenger miles per day as a seat performing scheduled stops. So PRT should also reduce the number of needed seats threefold for a given number of passenger miles.
While a few PRT designs have operating speeds of 100 km/h (60 mph), and one as high as 241 km/h (150 mph), most are in the region of 40–70 km/h (25–45 mph). Rail systems generally have higher maximum speeds, typically 90–130 km/h (55–80 mph) and sometimes well in excess of 160 km/h (100 mph), but average travel speed is reduced about threefold by scheduled stops and passenger transfers.
Ridership attraction.
If PRT designs deliver the claimed benefit of being substantially faster than cars in areas with heavy traffic, simulations suggest that PRT could attract many more car drivers than other public transit systems. Standard mass transit simulations accurately predict that 2% of trips (including cars) will switch to trains. Similar methods predict that 11% to 57% of trips would switch to PRT, depending on its costs and delays.
Control algorithms.
The typical control algorithm places vehicles in imaginary moving "slots" that go around the loops of track. Real vehicles are allocated a slot by track-side controllers. Traffic jams are prevented by placing north/south vehicles in even slots, and east/west vehicles in odd slots. At intersections, the traffic in these systems can interpenetrate without slowing.
On-board computers maintain their position by using a negative feedback loop to stay near the center of the commanded slot. Early PRT vehicles measured their position by adding up the distance using odometers, with periodic check points to compensate for cumulative errors. Next-generation GPS and radio location could measure positions as well.
Another system, "pointer-following control," assigns a path and speed to a vehicle, after verifying that the path does not violate the safety margins of other vehicles. This permits system speeds and safety margins to be adjusted to design or operating conditions, and may use slightly less energy.
The maker of the ULTra PRT system reports that testing of its control system shows lateral (side-to-side) accuracy of 1 cm, and docking accuracy better than 2 cm.
Safety.
Computer control eliminates errors from human drivers, so PRT designs in a controlled environment should be much safer than private motoring on roads. Most designs enclose the running gear in the guideway to prevent derailments. Grade-separated guideways would prevent conflict with pedestrians or manually controlled vehicles. Other public transit safety engineering approaches, such as redundancy and self-diagnosis of critical systems, are also included in designs.
The Morgantown system, more correctly described as an Automated Guideway Transit system (AGT), has completed 110 million passenger-miles without serious injury. According to the U.S. Department of Transportation, AGT systems as a group have higher injury rates than any other form of rail-based transit (subway, metro, light rail, or commuter rail) though still much better than ordinary buses or cars. More recent research by the British company ULTra PRT reported that AGT systems have a better safety than more conventional, non-automated modes.
As with many current transit systems, personal passenger safety concerns are likely to be addressed through CCTV monitoring, and communication with a central command center from which engineering or other assistance may be dispatched.
Energy efficiency.
The energy efficiency advantages claimed by PRT proponents include two basic operational characteristics of PRT: an increased average load factor; and the elimination of intermediate starting and stopping.
Average load factor, in transit systems, is the ratio of the total number of riders to the total theoretical capacity. A transit vehicle running at full capacity has a 100% load factor, while an empty vehicle has 0% load factor. If a transit vehicle spends half the time running at 100% and half the time running at 0%, the "average" load factor is 50%. Higher average load factor corresponds to lower energy consumption per passenger, so designers attempt to maximize this metric.
Scheduled mass transit (i.e. buses or trains,) trades off service frequency and load factor. Buses and trains must run on a predefined schedule, even during off-peak times when demand is low and vehicles are nearly empty. So to increase load factor, transportation planners try to predict times of low demand, and run reduced schedules or smaller vehicles at these times. This increases passengers' wait times. In many cities, trains and buses do not run at all at night or on weekends.
PRT vehicles, in contrast, would only move in response to demand, which places a theoretical lower bound on their average load factor. This allows 24-hour service without many of the costs of scheduled mass transit. 
ULTra PRT estimates its system will consume 839 BTU per passenger mile (0.55 MJ per passenger km). By comparison, cars consume 3,496 BTU, and personal trucks consume 4,329 BTU per passenger mile.
Due to PRT's efficiency, some proponents say solar becomes a viable power source. PRT elevated structures provide a ready platform for solar collectors, therefore some proposed designs include solar power as a characteristic of their networks.
For bus and rail transit, the energy per passenger-mile depends on the ridership and the frequency of service. Therefore, the energy per passenger-mile can vary significantly from peak to non-peak times. In the US, buses consume an average of 4,318 BTU/passenger-mile, transit rail 2,750 BTU/passenger-mile, and commuter rail 2,569 BTU/passenger-mile.
Cost characteristics.
The initial capital costs of PRT are large, but compare favorably with those of other transportation modes. Its system design tries to pay down those costs as quickly as possible, while maximizing the useful lifetime of the project. Proponents' cost estimates in passenger mile range from the cost of a bicycle (US $0.01–0.05/passenger-mile, Unimodal) to the cost of a small motorcycle ($0.20/passenger mile, TAXI 2000), and are strongly disputed by opponents. It's agreed that PRT systems require no individual license, parking or insurance fees, and buy energy in bulk from inexpensive providers.
Most of the initial investment is in guideways. Estimates of guideway cost range from US$0.8 million (for MicroRail) to $22 million per mile, with most estimates falling in the $10m to $15m range. These costs may not include the purchase of rights of way or system infrastructure, such as storage and maintenance yards and control centers, and reflect unidirectional travel along one guideway, the standard form of service in current PRT proposals. Bidirectional service is normally provided by moving vehicles around the block. To reach capacities of competing systems, a system requires thousands of vehicles. Some PRT proposals incorporate these costs in their per-mile estimates.
PRT designs generally assume dual-use rights of way, for example by mounting the transit system on narrow poles on an existing street. If dedicated rights of way were required for an application, costs could be considerably higher. If tunneled, small vehicle size can reduce tunnel volume compared with that required for an automated people mover (APM). Dual mode systems would use existing roads, as well as special-purpose PRT guideways. In some designs the guideway is just a cable buried in the street (a technology proven in industrial automation). Similar technology could equally be applied to cars.
A design with many modular components, mass production, driverless operation and redundant systems should in theory result in low operating costs and high reliability. Predictions of low operating cost generally depend on low operations and maintenance costs. Whether these assumptions are valid will not be known until full scale operations are commenced since reliability cannot be proven by prototype systems.
Transportation systems allocate the cost of their roads by measuring wear. PRT routes are disaggregated, and vehicles only move to carry passengers, so PRT measures wear and energy based on passengers or weight carried, rather than vehicle schedules. This brings large theoretical savings compared to trains, but appears more expensive than buses and streetcars, whose roads are subsidized by sunk, preallocated fuel taxes.
So, some planners dispute the cost-estimates of PRT when compared to light rail systems, whose costs vary widely with non-grade-separated streetcars being relatively low cost and systems involving elevated track or tunnels costing up to US$200 million per mile.
Opposition and controversy.
Opponents to PRT schemes have expressed a number of concerns:
Technical feasibility debate.
The Ohio, Kentucky, Indiana (OKI) Central Loop Report compared the Taxi 2000 PRT concept proposed by the Skyloop Committee to other transportation modes (bus, light rail and vintage trolley). In the Taxi 2000 PRT system, the Loop Study Advisory Committee identified "significant environmental, technical and potential fire and life safety concerns…" and the PRT system was "…still an unproven technology with significant questions about cost and feasibility of implementation." Skyloop contested this conclusion, arguing that Parsons Brinckerhoff changed several aspects of the system design without consulting with Taxi 2000, then rejected this modified design. Despite the report's concerns regarding the implementation obstacles of PRT, the report did conclude that compared to the other alternatives, PRT offered the most acceptable point-to-point travel times, the most reliable service levels, the highest level of frequency of service and geography coverage, and was most able to maintain schedule. The report further concluded that, compared to the other alternatives, PRT would have over 3 times the ridership of the next closest alternative, including new transit riders over 9 times higher than the next closest alternative.
Vukan R. Vuchic, Professor of Transportation Engineering at the University of Pennsylvania and a proponent of traditional forms of transit, has stated his belief that the combination of small vehicles and expensive guideway makes it highly impractical in both cities (not enough capacity) and suburbs (guideway too expensive). According to Vuchic: "...the PRT concept combines two mutually incompatible elements of these two systems: very small vehicles with complicated guideways and stations. Thus, in central cities, where heavy travel volumes could justify investment in guideways, vehicles would be far too small to meet the demand. In suburbs, where small vehicles would be ideal, the extensive infrastructure would be economically unfeasible and environmentally unacceptable."
PRT supporters claim that Vuchic's conclusions are based on flawed assumptions. PRT proponent J.E. Anderson wrote, in a rebuttal to Vuchic: "I have studied and debated with colleagues and antagonists every objection to PRT, including those presented in papers by Professor Vuchic, and find none of substance. Among those willing to be briefed in detail and to have all of their questions and concerns answered, I find great enthusiasm to see the system built."
The manufacturers of ULTra acknowledge that current forms of their system would provide insufficient capacity in high density areas such as central London, and that the investment costs for the tracks and stations are comparable to building new roads, making the current version of ULTra more suitable for suburbs and other moderate capacity applications, or as a supplementary system in larger cities.
Regulatory concerns.
Possible regulatory concerns include emergency safety, headways, and accessibility for the disabled.
Many jurisdictions regulate PRT systems as if they were trains. At least one successful prototype, CVS, failed deployment because it could not obtain permits from regulators.
Also, several PRT systems have been proposed for California, but the California Public Utilities Commission (CPUC) states that its rail regulations apply to PRT, and these require railway-sized headways.
The degree to which CPUC would hold PRT to "light rail" and "rail fixed guideway" safety standards is not clear because it can grant particular exemptions and revise regulations.
Other forms of automated transit have been approved for use in California, notably the Airtrain system at SFO. CPUC decided to not require compliance with General Order 143-B (for light rail) since Airtrain has no on-board operators. They did require compliance with General Order 164-D which mandates a safety and security plan, as well as periodic on-site visits by an oversight committee.
If safety or access considerations require the addition of walkways, ladders, platforms or other emergency/disabled access to or egress from PRT guideways, the size of the guideway may be increased. This may impact the feasibility of a PRT system, though the degree of impact would depend on both the PRT design and the municipality.
Concerns about PRT research.
Wayne D. Cottrell of the University of Utah conducted a critical review of PRT academic literature since the 1960s. He concluded that there are several issues that would benefit from more research, including: urban integration, risks of PRT investment, bad publicity, technical problems, and competing interests from other transport modes. He suggests that these issues, "while not unsolvable, are formidable," and that the literature might be improved by better introspection and criticism of PRT. He also suggests that more government funding is essential for such research to proceed, especially in the US.
New urbanist opinion.
Several proponents of new urbanism, an urban design movement that advocates for walkable cities, have expressed opinions on PRT.
Peter Calthorpe and Sir Peter Hall have supported the concept, but James Howard Kunstler disagrees: "If we're going to replace the car why do it with something that's not only like the car, but not really as good as the car? It just seems crazy." He also referred to PRT proponents as "a particular kind of crank".
Group rapid transit.
Group rapid transit (GRT) is similar to personal rapid transit but with higher-occupancy vehicles and grouping of passengers with potentially different origin-destination pairs. In this respect GRT can be seen as a sort of horizontal elevator. Such systems may have fewer direct-to-destination trips than single-destination PRT but still have fewer average stops than conventional transit, acting more as an automated share taxi system than a private cab system. Such a system may have advantages over low-capacity PRT in some applications, such as where higher passenger density is required or advantageous. It is also conceivable for a GRT system to have a range of vehicle sizes to accommodate different passenger load requirements, for example at different times of day or on routes with less or more average traffic. Such a system may constitute an "optimal" surface transportation routing solution in terms of balancing trip time and convenience with resource efficiency.
GRT has principally been proposed as a corridor service, where it can potentially provide a travel time improvement over conventional rail or bus and can also interface with PRT systems. However, GRT's necessary grouping of passengers makes it much less attractive in applications with lower passenger density or where few origin-destination pairs are shared among passengers.
Automated transit networks (ATN) is an umbrella term for GRT and PRT. While they have long been considered separate systems, Vectus is developing GRT vehicles formed by combining multiple PRT vehicles. The larger vehicles are designed to accommodate standees and operate on the same guideway as the PRT vehicles. The door spacing of the larger vehicles matches the door spacing of PRT vehicles stopped in a station, allowing the GRT vehicles to share the same station infrastructure too. The concept is intended to allow GRT to serve high-demand station pairs during peak periods, while PRT serves all stations at all times in a network which includes the high-demand station pairs as well as other stations.
The same passenger grouping and destination scheduling approach is used in some advanced elevators, in the form of a destination control system.

</doc>
<doc id="50571" url="http://en.wikipedia.org/wiki?curid=50571" title="Transportation engineering">
Transportation engineering

Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods (transport). It is a sub-discipline of civil engineering and of industrial engineering. Transportation engineering is a major component of the civil engineering and mechanical engineering disciplines, according to specialisation of academic courses and main competences of the involved territory. The importance of transportation engineering within the civil and industrial engineering profession can be judged by the number of divisions in ASCE (American Society of Civil Engineers) that are directly related to transportation. There are six such divisions (Aerospace; Air Transportation; Highway; Pipeline; Waterway, Port, Coastal and Ocean; and Urban Transportation) representing one-third of the total 18 technical divisions within the ASCE (1987).
The planning aspects of transportation engineering relate to urban planning, and involve technical forecasting decisions and political factors. Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (how many trips for what purpose), trip distribution (destination choice, where is the traveler going), mode choice (what mode is being taken), and route assignment (which streets or routes are being used). More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important to civil engineers, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, as practiced by civil engineers, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track). 
Before any planning occurs the Engineer must take what is known as an inventory of the area or if it is appropriate, the previous system in place. This inventory or database must include information on (1)population, (2)land use, (3)economic activity, (4)transportation facilities and services, (5)travel patterns and volumes, (6)laws and ordinances, (7)regional financial resources, (8)community values and expectations. These inventories help the engineer create business models to complete accurate forecasts of the future conditions of the systemReview.
Operations and management involve traffic engineering, so that vehicles move smoothly on the road or track. Older techniques include signs, signals, markings, and tolling. Newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. Human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.
Highway engineering.
Engineers in this specialization:
Railroad engineering.
Railway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or even monorails). Typical tasks would include determining horizontal and vertical alignment design, station location and design, and construction cost estimating. Railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control.
Railway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands. In the United States, railway engineers work with elected officials in Washington, D.C. on rail transportation issues to make sure that the rail system meets the country's transportation needs.
Port and harbor engineering.
Port and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities. This is not to be confused with marine engineering.
Airport engineering.
Airport engineers design and construct airports. Airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. These engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.

</doc>
<doc id="50578" url="http://en.wikipedia.org/wiki?curid=50578" title="Queueing theory">
Queueing theory

Queueing theory is the mathematical study of waiting lines, or queues. In queueing theory a model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing
and the design of factories, shops, offices and hospitals.
Single queueing nodes.
Single queueing nodes are usually described using Kendall's notation in the form "A"/"S"/"C" where "A" describes the time between arrivals to the queue, "S" the size of jobs and "C" the number of servers at the node. Many theorems in queue theory can be proved by reducing queues to mathematical systems known as Markov chains, first described by Andrey Markov in his 1906 paper.
Agner Krarup Erlang, a Danish engineer who worked for the Copenhagen Telephone Exchange, published the first paper on what would now be called queueing theory in 1909. He modeled the number of telephone calls arriving at an exchange by a Poisson process and solved the M/D/1 queue in 1917 and M/D/k queueing model in 1920. In Kendall's notation:
The M/M/1 queue is a simple model where a single server serves jobs that arrive according to a Poisson process and have exponentially distributed service requirements. In an M/G/1 queue the G stands for general and indicates an arbitrary probability distribution. The M/G/1 model was solved by Felix Pollaczek in 1930, a solution later recast in probabilistic terms by Aleksandr Khinchin and now known as the Pollaczek–Khinchine formula.
After World War II queueing theory became an area of research interest to mathematicians. In 1953 David Kendall solved the GI/M/k queue and introduced the modern notation for queues, now known as Kendall's notation. In 1957 Pollaczek studied the GI/G/1 using an integral equation. John Kingman gave a formula for the mean waiting time in a G/G/1 queue: Kingman's formula.
The matrix geometric method and matrix analytic methods have allowed queues with phase-type distributed inter-arrival and service time distributions to be considered.
Problems such as performance metrics for the M/G/k queue remain an open problem.
Service disciplines.
Various scheduling policies can be used at queuing nodes:
Queueing networks.
Networks of queues are systems in which a number of queues are connected by customer routing. When a customer is serviced at one node it can join another node and queue for service, or leave the network. For a network of "m" the state of the system can be described by an "m"–dimensional vector ("x"1,"x"2...,"x""m") where "x""i" represents the number of customers at each node. The first significant results in this area were Jackson networks, for which an efficient product-form stationary distribution exists and the mean value analysis which allows average metrics such as throughput and sojourn times to be computed.
If the total number of customers in the network remains constant the network is called a closed network and has also been shown to have a product–form stationary distribution in the Gordon–Newell theorem. This result was extended to the BCMP network where a network with very general service time, regimes and customer routing is shown to also exhibit a product-form stationary distribution. The normalizing constant can be calculated with the Buzen's algorithm, proposed in 1973.
Networks of customers have also been investigated, Kelly networks where customers of different classes experience different priority levels at different service nodes.
Another type of network are G-networks first proposed by Erol Gelenbe in 1993: these networks do not assume exponential time distributions like the classic Jackson Network.
Routing algorithms.
In discrete time networks where there is a constraint on which service nodes can be active at any time, the max-weight scheduling algorithm chooses a service policy to give optimal throughput in the case that each job visits only a single service node. In the more general case where jobs can visit more than one node, backpressure routing gives optimal throughput.
A network scheduler must choose a queuing algorithm, which affects the characteristics of the larger network.
Mean field limits.
Mean field models consider the limiting behaviour of the empirical measure (proportion of queues in different states) as the number of queues ("m" above) goes to infinity. The impact of other queues on any given queue in the network is approximated by a differential equation. The deterministic model converges to the same stationary distribution as the original model.
Fluid limits.
Fluid models are continuous deterministic analogs of queueing networks obtained by taking the limit when the process is scaled in time and space, allowing heterogeneous objects. This scaled trajectory converges to a deterministic equation which allows us stability of the system to be proven. It is known that a queueing network can be stable, but have an unstable fluid limit.
Heavy traffic/diffusion approximations.
In a system with high occupancy rates (utilisation near 1) a heavy traffic approximation can be used to approximate the queueing length process by a reflected Brownian motion, Ornstein–Uhlenbeck process or more general diffusion process. The number of dimensions of the RBM is equal to the number of queueing nodes and the diffusion is restricted to the non-negative orthant.

</doc>
<doc id="50579" url="http://en.wikipedia.org/wiki?curid=50579" title="Gaussian quadrature">
Gaussian quadrature

In numerical analysis, a quadrature rule is an approximation of the definite integral of a function, usually stated as a weighted sum of function values at specified points within the domain of integration.
(See numerical integration for more on quadrature rules.) An "n"-point Gaussian quadrature rule, named after Carl Friedrich Gauss, is a quadrature rule constructed to yield an exact result for polynomials of degree 2"n" − 1 or less by a suitable choice of the points xi and weights wi for . The domain of integration for such a rule is conventionally taken as [−1, 1], so the rule is stated as
Gaussian quadrature as above will only produce accurate results if the function "f"("x") is well approximated by a polynomial function within the range [−1, 1]. The method is not, for example, suitable for functions with singularities. However, if the integrated function can be written as formula_2, where "g"("x") is approximately polynomial and "ω"("x") is known, then alternative weights formula_3 and points formula_4 that depend on the weighting function "ω"("x") may give better results, where
Common weighting functions include formula_6 (Chebyshev–Gauss) and formula_7 (Gauss–Hermite).
It can be shown (see Press, et al., or Stoer and Bulirsch) that the evaluation points xi are just the roots of a polynomial belonging to a class of orthogonal polynomials.
Gauss–Legendre quadrature.
For the simplest integration problem stated above, i.e. with formula_8, the associated polynomials are Legendre polynomials, "P""n"("x"), and the method is usually known as Gauss–Legendre quadrature. With the n-th polynomial normalized to give "P""n"(1) = 1, the i-th Gauss node, xi, is the i-th root of Pn; its weight is given by 
Some low-order rules for solving the integration problem are listed below.
Change of interval.
An integral over ["a", "b"] must be changed into an integral over [−1, 1] before applying the Gaussian quadrature rule. This change of interval can be done in the following way:
Applying the Gaussian quadrature rule then results in the following approximation:
Other forms.
The integration problem can be expressed in a slightly more general way by introducing a positive weight function ω into the integrand, and allowing an interval other than [−1, 1]. That is, the problem is to calculate
for some choices of "a", "b", and ω. For "a" = −1, "b" = 1, and ω("x") = 1, the problem is the same as that considered above.
Other choices lead to other integration rules. Some of these are tabulated below. Equation numbers are given for Abramowitz and Stegun (A & S).
Fundamental theorem.
Let pn be a nontrivial polynomial of degree "n" such that
If we pick the "n" nodes xi to be the zeros of pn, then there exist "n" weights wi which make the Gauss-quadrature computed integral exact for all polynomials "h"("x") of degree 2"n" − 1 or less. Furthermore, all these nodes xi will lie in the open interval ("a", "b") .
The polynomial pn is said to be an orthogonal polynomial of degree "n" associated to the weight function "ω"("x"). It is unique up to a constant normalization factor. The idea underlying the proof is that, because of its sufficiently low degree, "h"("x") can be divided by formula_14 to produce a quotient "q"("x") of degree strictly lower than "n", and a remainder "r"("x") of still lower degree, so that both will be orthogonal to formula_14, by the defining property of formula_14. Thus
Because of the choice of nodes "x""i", the corresponding relation
holds also. The exactness of the computed integral for formula_19 then follows from corresponding exactness for polynomials of degree only "n" or less (as is formula_20).
General formula for the weights.
The weights can be expressed as
where formula_22 is the coefficient of formula_23 in formula_24. To prove this, note that using Lagrange interpolation one can express "r"("x") in terms of formula_25 as
because "r"("x") has degree less than n and is thus fixed by the values it attains at n different points. Multiplying both sides by "ω"("x") and integrating from a to b yields
The weights wi are thus given by 
This integral expression for formula_29 can be expressed in terms of the orthogonal polynomials formula_30 and formula_31 as follows.
We can write
where formula_33 is the coefficient of formula_34 in formula_30. Taking the limit of x to formula_36 yields using L'Hôpital's rule
We can thus write the integral expression for the weights as
In the integrand, writing
yields
provided formula_41, because 
is a polynomial of degree k-1 which is then orthogonal to formula_30. So, if "q"("x") is a polynomial of at most nth degree we have
We can evaluate the integral on the right hand side for formula_45 as follows. Because formula_46 is a polynomial of degree n-1, we have
where "s"("x") is a polynomial of degree formula_48. Since "s"("x") is orthogonal to formula_49 we have
We can then write
The term in the brackets is a polynomial of degree formula_48, which is therefore orthogonal to formula_49. The integral can thus be written as
According to Eq. (2), the weights are obtained by dividing this by formula_55 and that yields the expression in Eq. (1).
Proof that the weights are positive.
Consider the following polynomial of degree 2n-2
where as above the xj are the roots of the polynomial formula_30. Since the degree of f(x) is less than 2n-1, the Gaussian quadrature formula involving the weights and nodes obtained from formula_30 applies. Since formula_59 for j not equal to i, we have
Since both formula_61 and f(x) are non-negative functions, it follows that formula_62.
Computation of Gaussian quadrature rules.
For computing the nodes xi and weights wi of Gaussian quadrature rules, the fundamental tool is the three-term recurrence relation satisfied by the set of orthogonal polynomials associated to the corresponding weight function. For n points, these nodes and weights can be computed in "O"("n"2) operations by an algorithm derived by Gautschi (1968).
Gautschi's theorem.
Gautschi's theorem (Gautschi, 1968) states that orthogonal polynomials formula_63 with formula_64 for formula_65 for a scalar product formula_66 to be specified later, degreeformula_67 and leading coefficient one (i.e. monic orthogonal polynomials) satisfy the recurrence relation
for formula_69 where n is the maximal degree which can be taken to be infinity, and where formula_70. First of all, it is obvious that the polynomials defined by the recurrence relation starting with formula_71 have leading coefficient one and correct degree. Given the starting point by formula_72, the orthogonality of formula_63 can be shown by induction. For formula_74 one has
Now if formula_76 are orthogonal, then also formula_77, because in
all scalar products vanish except for the first one and the one where formula_79 meets the same orthogonal polynomial. Therefore,
However, if the scalar product satisfies formula_81 (which is the case for Gaussian quadrature), the recurrence relation reduces to a three-term recurrence relation: For formula_82 is a polynomial of degree less or equal to "r" − 1. On the other hand, formula_63 is orthogonal to every polynomial of degree less or equal to "r" − 1. Therefore, one has formula_84 and formula_85 for "s" < "r" − 1. The recurrence relation then simplifies to 
or
(with the convention formula_88) where
(the last because of formula_90, since formula_91 differs from formula_63 by a degree less than r).
The Golub-Welsch algorithm.
The three-term recurrence relation can be written in the matrix form formula_93 where formula_94, formula_95 is the formula_96th standard basis vector, i.e. formula_97, and J is the so-called Jacobi matrix:
The zeros formula_99 of the polynomials up to degree n which are used as nodes for the Gaussian quadrature can be found by computing the eigenvalues of this tridiagonal matrix. This procedure is known as "Golub–Welsch algorithm".
For computing the weights and nodes, it is preferable to consider the symmetric tridiagonal matrix formula_100 with elements 
J and formula_100 are similar matrices and therefore have the same eigenvalues (the nodes). The weights can be computed from the corresponding eigenvectors: If formula_103 is a normalized eigenvector (i.e., an eigenvector with euclidean norm equal to one) associated to the eigenvalue xj, the corresponding weight can be computed from the first component of this eigenvector, namely:
where formula_105 is the integral of the weight function
See, for instance, for further details.
Error estimates.
The error of a Gaussian quadrature rule can be stated as follows . For an integrand which has 2"n" continuous derivatives,
for some ξ in ("a", "b"), where pn is the monic (i.e. the leading coefficient is 1) orthogonal polynomial of degree n and where
In the important special case of , we have the error estimate 
Stoer and Bulirsch remark that this error estimate is inconvenient in practice, since it may be difficult to estimate the order 2"n" derivative, and furthermore the actual error may be much less than a bound established by the derivative. Another approach is to use two Gaussian quadrature rules of different orders, and to estimate the error as the difference between the two results. For this purpose, Gauss–Kronrod quadrature rules can be useful.
Important consequence of the above equation is that Gaussian quadrature of order n is accurate for all polynomials up to degree 2"n" − 1.
Gauss–Kronrod rules.
If the interval ["a", "b"] is subdivided, the Gauss evaluation points of the new subintervals never coincide with the previous evaluation points (except at zero for odd numbers), and thus the integrand must be evaluated at every point. "Gauss–Kronrod rules" are extensions of Gauss quadrature rules generated by adding "n" + 1 points to an n-point rule in such a way that the resulting rule is of order 3"n" + 1. This allows for computing higher-order estimates while re-using the function values of a lower-order estimate. The difference between a Gauss quadrature rule and its Kronrod extension are often used as an estimate of the approximation error.
Gauss–Lobatto rules.
Also known as Lobatto quadrature , named after Dutch mathematician Rehuel Lobatto. It is similar to Gaussian quadrature with the following differences:
Lobatto quadrature of function "f"("x") on interval [−1, 1]:
Abscissas: xi is the formula_111st zero of formula_112.
Weights:
Remainder:
Some of the weights are:

</doc>
<doc id="50580" url="http://en.wikipedia.org/wiki?curid=50580" title="Orange County, Texas">
Orange County, Texas

Orange County is a county in the U.S. state of Texas. As of the 2010 census, its population was 81,837. The county seat is Orange.
Orange County is included in the Beaumont-Port Arthur, TX Metropolitan Statistical Area. It is located in the very southeastern corner of Texas, with a boundary with Louisiana, within the Golden Triangle of Texas.
History.
Orange County was formed in 1852 from portions of Jefferson County. It was named after the orange fruit, the common citrus fruit grown by the early settlers of this County near the mouth of the Sabine River.
Due to periodic spells of quite cold winter weather (frosts) in Orange County, it is no longer the home of orange trees and citrus orchards. The production of those fruits in Texas long ago was moved a long way southwest into the Rio Grande Valley, where the weather is almost always warm all winter long. Citrus trees produce their fruit in the wintertime, which makes them especially vulnerable to frost and icy weather.
A similar thing has happened in Florida, where orchards of citrus trees no longer exist in either Citrus County or Orange County because of bad winter freezes in some years. In both Florida and Texas, the citrus agriculture has been moved farther south in search of milder winters, and away from the periodic frosts.
During World War II, Orange County was the home of a large amount of shipbuilding for the navies the United States and allied countries. The major shipbuilder, the Consolidated Steel Corporation was located in the town of Orange, and among the warships that it built were the (1942), the first warship built there, the (1943), and the (1945–46), the last warship built there. During the war, the Consolidate Steel Corporation employed as many as 20,000 people at its shipyard in Orange, Texas.
Geography.
According to the U.S. Census Bureau, the county has a total area of 380 sqmi, of which 334 sqmi is land and 46 sqmi (12%) is water.
Orange County is bordered on its east by the Sabine River, on its southeast by Sabine Lake, and on the northwest by the Neches River.
The geography of Orange County varies relatively little, with an elevation that reaches 33 feet (10 meters) above sea level at very few points within the county. Orange County is very flat, and its soil is quite sandy, as could be expected in a county along the Gulf of Mexico. (Sandy soil is also common in southern Louisiana, Mississippi, and Alabama, and in western and southern Florida.) There are saltwater marshes in much of the southeastern part of Orange County that borders the Sabine River. There are piney woods (sometimes capitalized) in the northern part of the county.
Demographics.
As of the census of 2000, there were 84,966 people, 31,642 households, and 23,794 families residing in the county. The population density was 238 people per square mile (92/km²). There were 34,781 housing units at an average density of 98 per square mile (38/km²). The racial makeup of the county was 87.98% White, 8.38% Black or African American, 0.56% Native American, 0.78% Asian, 0.03% Pacific Islander, 1.12% from other races, and 1.15% from two or more races. 3.62% of the population were Hispanic or Latino of any race.
There were 31,642 households out of which 35.30% had children under the age of 18 living with them, 58.80% were married couples living together, 12.10% had a female householder with no husband present, and 24.80% were non-families. 21.70% of all households were made up of individuals and 9.30% had someone living alone who was 65 years of age or older. The average household size was 2.65 and the average family size was 3.08.
In the county, the population was spread out with 27.30% under the age of 18, 8.70% from 18 to 24, 28.10% from 25 to 44, 23.20% from 45 to 64, and 12.70% who were 65 years of age or older. The median age was 36 years. For every 100 females there were 96.40 males. For every 100 females age 18 and over, there were 92.60 males.
The median income for a household in the county was $37,586, and the median income for a family was $44,152. Males had a median income of $40,185 versus $21,859 for females. The per capita income for the county was $17,554. About 11.40% of families and 13.80% of the population were below the poverty line, including 18.50% of those under age 18 and 12.40% of those age 65 or over.
Government.
The Orange County Courthouse serves as the court for the region.
Orange County lies in Texas House District 21, represented beginning in 2015 by the Republican Dade Phelan of Beaumont.
Economy.
Primary economic activities in Orange County are the petroleum refining industry, paper milling, rice farming, and shrimping.
Orange County was formerly a center for the building of warships, and there is still a large U.S. Navy ghost fleet (reserve fleet) in Jefferson County - from which currently, many old warships are being cleaned of water pollution sources and then scrapped for their metals. Thus, there is still employment for residents of Orange County in shipbreaking.
Transportation.
Airports.
Orange County Airport operates general aviation flights.
Nearby Southeast Texas Regional Airport (Port Arthur) operates commercial flights.
Education.
The county is served by 5 school districts:
West Orange-Cove Consolidated Independent School District.
The district's Superintendent of Schools is James Colbert Jr.
Bridge City ISD.
The Superintendent is Jamey Harrison. It includes:
Little Cypress-Mauriceville Consolidated ISD.
The Superintendent is Dr. Pauline Hargrove. It includes:

</doc>
<doc id="50582" url="http://en.wikipedia.org/wiki?curid=50582" title="Zircon">
Zircon

Zircon ( or ; including hyacinth or yellow zircon) is a mineral belonging to the group of nesosilicates. Its chemical name is zirconium silicate and its corresponding chemical formula is ZrSiO4. A common empirical formula showing some of the range of substitution in zircon is (Zr1–y, REEy)(SiO4)1–x(OH)4x–y. Zircon forms in silicate melts with large proportions of high field strength incompatible elements. For example, hafnium is almost always present in quantities ranging from 1 to 4%. The crystal structure of zircon is tetragonal crystal system. The natural color of zircon varies between colorless, yellow-golden, red, brown, blue, and green. Colorless specimens that show gem quality are a popular substitute for diamond and are also known as "Matura diamond".
The name derives from the Persian "zargun" (زرگون), meaning golden-colored. This word is corrupted into "jargoon", a term applied to light-colored zircons. The English word "zircon" is derived from "Zirkon," which is the German adaptation of this word. Red zircon is called "hyacinth", from the flower "hyacinthus", whose name is of Ancient Greek origin.
Properties.
Zircon is ubiquitous in the crust of Earth. It occurs as a common accessory mineral in igneous rocks (as primary crystallization products), in metamorphic rocks and as detrital grains in sedimentary rocks. Large zircon crystals are rare. Their average size in granite rocks is about 0.1–0.3 mm, but they can also grow to sizes of several centimeters, especially in mafic pegmatites and carbonatites.
Because of their uranium and thorium content, some zircons undergo metamictization. Connected to internal radiation damage, these processes partially disrupt the crystal structure and partly explain the highly variable properties of zircon. As zircon becomes more and more modified by internal radiation damage, the density decreases, the crystal structure is compromised, and the color changes.
Zircon occurs in many colors, including reddish brown, yellow, green, blue, gray and colorless. The color of zircons can sometimes be changed by heat treatment. Common brown zircons can be transformed into colorless and blue zircons by heating to 800 to 1000 °C. In geological settings, the development of pink, red, and purple zircon occurs after hundreds of millions of years, if the crystal has sufficient trace elements to produce color centers. Color in this red or pink series is annealed in geological conditions above the temperature about 350 °C.
Applications.
Zircon is mainly consumed as an opacifier, and has been known to be used in the decorative ceramics industry. It is also the principal precursor not only to metallic zirconium, although this application is small, but also to all compounds of zirconium including zirconium dioxide (ZrO2), one of the most refractory materials known.
Occurrence.
Zircon is a common accessory to trace mineral constituent of most granite and felsic igneous rocks. Due to its hardness, durability and chemical inertness, zircon persists in sedimentary deposits and is a common constituent of most sands. Zircon is rare within mafic rocks and very rare within ultramafic rocks aside from a group of ultrapotassic intrusive rocks such as kimberlites, carbonatites, and lamprophyre, where zircon can occasionally be found as a trace mineral owing to the unusual magma genesis of these rocks.
Zircon forms economic concentrations within heavy mineral sands ore deposits, within certain pegmatites, and within some rare alkaline volcanic rocks, for example the Toongi Trachyte, Dubbo, New South Wales Australia in association with the zirconium-hafnium minerals eudialyte and armstrongite.
Australia leads the world in zircon mining, producing 37% of the world total and accounting for 40% of world EDR (economic demonstrated resources) for the mineral.
Radiometric dating.
Zircon has played an important role during the evolution of radiometric dating. Zircons contain trace amounts of uranium and thorium (from 10 ppm up to 1 wt%) and can be dated using several modern analytical techniques. Because zircons can survive geologic processes like erosion, transport, even high-grade metamorphism, they contain a rich and varied record of geological processes. Currently, zircons are typically dated by uranium-lead (U-Pb), fission-track, and U+Th/He techniques.
Zircons from Jack Hills in the Narryer Gneiss Terrane, Yilgarn Craton, Western Australia, have yielded U-Pb ages up to 4.404 billion years, interpreted to be the age of crystallization, making them the oldest minerals so far dated on Earth. In addition, the oxygen isotopic compositions of some of these zircons have been interpreted to indicate that more than 4.4 billion years ago there was already water on the surface of the Earth. This interpretation is supported by additional trace element data, but is also the subject of debate.
Similar minerals.
Hafnon (HfSiO4), xenotime (YPO4), béhierite, schiavinatoite ((Ta,Nb)BO4), thorite (ThSiO4), and coffinite (USiO4) all share the same crystal structure (VIIIX IVY O4) as zircon.
Further reading.
</dl>

</doc>
<doc id="50585" url="http://en.wikipedia.org/wiki?curid=50585" title="Philadelphia">
Philadelphia

Philadelphia () is the largest city in the Commonwealth of Pennsylvania and its only consolidated city-county, the fifth-most-populous city in the United States, and the core of the sixth-largest metropolitan area in the country. Located in the Northeastern United States at the confluence of the Delaware and Schuylkill rivers, Philadelphia is the economic and cultural center of the Delaware Valley. The population of the city was counted at 1,526,006 in 2010 and estimated at 1,560,297 in 2014, according to the United States Census Bureau. The four Pennsylvania counties nearest Philadelphia had an estimated total population of 2,510,793 in 2013; while by 2014 census estimates, the Philadelphia metropolitan area, also known as the Delaware Valley, is home to 6.1 million residents, and the larger Philadelphia-Reading-Camden, PA-NJ-DE-MD Combined Statistical Area contains approximately 7.2 million residents.
In 1682, William Penn founded the city to serve as capital of the Pennsylvania Colony. During the American Revolution, Philadelphia played an instrumental role as a meeting place for the Founding Fathers of the United States, who signed the Declaration of Independence in 1776 and the Constitution in 1787. Philadelphia was one of the nation's capitals during the Revolutionary War, and the city served as the temporary U.S. capital while Washington, D.C., was under construction. During the 19th century, Philadelphia became a major industrial center and railroad hub that grew from an influx of European immigrants. It became a prime destination for African Americans during the Great Migration and surpassed two million occupants by 1950. Following numerous civil rights protests and riots, the city experienced decades of heavy crime and neared bankruptcy by the 1980s. Revitalization began in the 1990s, with gentrification turning around many neighborhoods and reversing its decades-long trend of population loss.
The city is the center of economic activity in Pennsylvania and is home to seven Fortune 1000 companies. The Philadelphia skyline is growing, with several nationally prominent skyscrapers. It is also known for its arts, culture, and history, which attracted over 39 million domestic tourists in 2013. The city has more outdoor sculptures and murals than any other American city, and Philadelphia's Fairmount Park is the largest landscaped urban park in the world. The 67 National Historic Landmarks in the city helped account for the $10 billion generated by tourism. It is also the home of many US firsts, including the first library (1731), first hospital (1751) and medical school (1765), first Capitol (1777), first stock exchange (1790), first zoo (1874), and first business school (1881).
History.
Before Europeans arrived, the Philadelphia area was home to the Lenape (Delaware) Indians in the village of Shackamaxon.
Europeans came to the Delaware Valley in the early 17th century, with the first settlements founded by the Dutch, who in 1623 built Fort Nassau on the Delaware River opposite the Schuylkill River in what is now Brooklawn, New Jersey. The Dutch considered the entire Delaware River valley to be part of their New Netherland colony. In 1638, Swedish settlers led by renegade Dutch established the colony of New Sweden at Fort Christina (present day Wilmington, Delaware) and quickly spread out in the valley. In 1644, New Sweden supported the Susquehannocks in their military defeat of the English colony of Maryland. In 1648, the Dutch built Fort Beversreede on the west bank of the Delaware, south of the Schuylkill near the present-day Eastwick section of Philadelphia, to reassert their dominion over the area. The Swedes responded by building Fort Nya Korsholm, named New Korsholm after a town that is now in Finland. In 1655, a Dutch military campaign led by New Netherland Director-General Peter Stuyvesant took control of the Swedish colony, ending its claim to independence, although the Swedish and Finnish settlers continued to have their own militia, religion, and court, and to enjoy substantial autonomy under the Dutch. The English conquered the New Netherland colony in 1664, but the situation did not really change until 1682, when the area was included in William Penn's charter for Pennsylvania.
In 1681, in partial repayment of a debt, Charles II of England granted William Penn a charter for what would become the Pennsylvania colony. Despite the royal charter, Penn bought the land from the local Lenape to be on good terms with the Native Americans and ensure peace for his colony. According to legend Penn made a treaty of friendship with Lenape chief Tammany under an elm tree at Shackamaxon, in what is now the city's Fishtown section. Penn named the city Philadelphia, which is Greek for brotherly love (from "philos", "love" or "friendship", and "adelphos", "brother"). As a Quaker, Penn had experienced religious persecution and wanted his colony to be a place where anyone could worship freely. This tolerance, far more than afforded by most other colonies, led to better relations with the local Native tribes and fostered Philadelphia's rapid growth into America's most important city.
Penn planned a city on the Delaware River to serve as a port and place for government. Hoping that Philadelphia would become more like an English rural town instead of a city, Penn laid out roads on a grid plan to keep houses and businesses spread far apart, with areas for gardens and orchards. The city's inhabitants did not follow Penn's plans, as they crowded by the Delaware River, the port, and subdivided and resold their lots. Before Penn left Philadelphia for the last time, he issued the Charter of 1701 establishing it as a city. It became an important trading center, poor at first, but with tolerable living conditions by the 1750s. Benjamin Franklin, a leading citizen, helped improve city services and founded new ones, such as fire protection, a library, and one of the American colonies' first hospitals.
A number of important philosophical societies were formed, which were centers of the city's intellectual life: the Philadelphia Society for Promoting Agriculture (1785), the Pennsylvania Society for the Encouragement of Manufactures and the Useful Arts (1787), the Academy of Natural Sciences (1812), and the Franklin Institute (1824). These worked to develop and finance new industries and attract skilled and knowledgeable immigrants from Europe.
Philadelphia's importance and central location in the colonies made it a natural center for America's revolutionaries. By the 1750s, Philadelphia had surpassed Boston to become the largest city and busiest port in British America, and second in the British Empire, behind London. The city hosted the First Continental Congress before the war; the Second Continental Congress, which signed the United States Declaration of Independence, during the war; and the Constitutional Convention (1787) after the war. Several battles were fought in and near Philadelphia as well.
Philadelphia served as the temporary capital of the United States, 1790–1800, while the Federal City was under construction in the District of Columbia. In 1793, the largest yellow fever epidemics in U.S. history killed at least 4,000 and up to 5,000 people in Philadelphia, roughly 10% of the city's population.
The state government left Philadelphia in 1799, and the federal government was moved to Washington, DC in 1800 with completion of the White House and Capitol. The city remained the young nation's largest with a population of nearly 50,000 at the turn of the 19th century; it was a financial and cultural center. Before 1800, its free black community founded the African Methodist Episcopal Church (AME), the first independent black denomination in the country, and the first black Episcopal Church. The free black community also established many schools for its children, with the help of Quakers. New York City soon surpassed Philadelphia in population, but with the construction of roads, canals, and railroads, Philadelphia became the first major industrial city in the United States.
Throughout the 19th century, Philadelphia had a variety of industries and businesses, the largest being textiles. Major corporations in the 19th and early 20th centuries included the Baldwin Locomotive Works, William Cramp and Sons Ship and Engine Building Company, and the Pennsylvania Railroad. Industry, along with the U.S. Centennial, was celebrated in 1876 with the Centennial Exposition, the first official World's Fair in the United States. Immigrants, mostly Irish and German, settled in Philadelphia and the surrounding districts. The rise in population of the surrounding districts helped lead to the Act of Consolidation of 1854, which extended the city limits of Philadelphia from the 2 square miles of present-day Center City to the roughly 130 square miles of Philadelphia County. 
These immigrants were largely responsible for the first general strike in North America in 1835, in which workers in the city won the ten-hour workday. The city was a destination for thousands of Irish immigrants fleeing the Great Famine in the 1840s; housing for them was developed south of South Street, and was later occupied by succeeding immigrants. They established a network of Catholic churches and schools, and dominated the Catholic clergy for decades. Anti-Irish, anti-Catholic Nativist riots had erupted in Philadelphia in 1844. In the latter half of the century, immigrants from Russia, Eastern Europe and Italy; and African Americans from the southern U.S. settled in the city. Between 1880 and 1930, the African-American population of Philadelphia increased from 31,699 to 219,559. Twentieth-century black newcomers were part of the Great Migration out of the rural South to northern and midwestern industrial cities.
In the American Civil War, Philadelphia was represented by the Washington Grays (Philadelphia).
By the 20th century, Philadelphia had become known as "corrupt and contented", with a complacent population and an entrenched Republican political machine. The first major reform came in 1917 when outrage over the election-year murder of a police officer led to the shrinking of the Philadelphia City Council from two houses to just one. In July 1919, Philadelphia was one of more than 36 industrial cities nationally to suffer a race riot of ethnic whites against blacks during Red Summer, in post-World War I unrest, as recent immigrants competed with blacks for jobs. In the 1920s, the public flouting of Prohibition laws, mob violence, and police involvement in illegal activities led to the appointment of Brigadier General Smedley Butler of the U.S. Marine Corps as director of public safety, but political pressure prevented any long-term success in fighting crime and corruption.
In 1940, non-Hispanic whites constituted 86.8% of the city's population. The population peaked at more than two million residents in 1950, then began to decline with the restructuring of industry, which led to the loss of many middle-class union jobs. In addition, suburbanization had been drawing off many of the wealthier residents to outlying railroad commuting towns and newer housing. Revitalization and gentrification of neighborhoods began in the late 1970s and continues into the 21st century, with much of the development in the Center City and University City areas of the city. After many of the old manufacturers and businesses left Philadelphia or shut down, the city started attracting service businesses and began to more aggressively market itself as a tourist destination. Glass-and-granite skyscrapers were built in Center City. Historic areas such as Independence National Historical Park located in Old City and Society Hill were renovated during the reformist mayoral era of the 1950s through the 1980s. They are now among the most desirable living areas of Center City. This has slowed the city's 40-year population decline after it lost nearly one-quarter of its population. The city has attracted more recent immigrants: Hispanics from Central and South America and Asian refugees from Laos, Vietnam and Cambodia. Educated Asians from India have tended to settle in suburbs with other middle- and upper-class people.
Geography.
Topography.
Philadelphia is at 39° 57′ north latitude and 75° 10′ west longitude, and the 40th parallel north passes through the northern parts of the city. The city encompasses 142.6 sqmi, of which 135.1 sqmi is land and 7.6 sqmi, or 5.29%, is water. Bodies of water include the Delaware and Schuylkill rivers, and Cobbs, Wissahickon, and Pennypack creeks.
The lowest point is 10 ft above sea level, while the highest point is in Chestnut Hill, about 445 ft above sea level (near the intersection of Germantown Avenue and Bethlehem Pike).
Philadelphia sits on the Fall Line that separates the Atlantic Coastal Plain from the Piedmont. The rapids on the Schuylkill River at East Falls were inundated by the completion of the Fairmount Dam.
The city is the seat of its own county. The adjacent counties are Montgomery to the north; Bucks to the northeast; Burlington County, New Jersey, to the east; Camden County, New Jersey, to the southeast; Gloucester County, New Jersey, to the south; and Delaware County to the west.
Cityscape.
Center City Philadelphia in 1913.
Panoramic view of the growing Center City Philadelphia skyline, viewed from Camden, New Jersey, across the Delaware River. Comcast Center and the spired One Liberty Place are recognizable as the two tallest skyscrapers in this image.
City planning.
Philadelphia's central city was created in the 17th century following the plan by William Penn's surveyor Thomas Holme. Center City is structured with long straight streets running east-west and north-south forming a grid pattern. The original city plan was designed to allow for easy travel and to keep residences separated by open space that would help prevent the spread of fire. The Delaware River and Schuylkill Rivers served as early boundaries between which the city's early street plan was kept within. In addition, Penn planned the creation of five public parks in the city which were renamed in 1824 (in parenthesis): Centre Square, North East Publick Square (Franklin Square), Northwest Square (Logan Square), Southwest Square (Rittenhouse Square), and Southeast Square (Washington Square).
Philadelphia's neighborhoods are divided into large sections—North, Northeast, Northwest, West, South and Southwest Philadelphia—all of which surround Center City, which corresponds closely with the city's limits before consolidation in 1854. Each of these large areas contains numerous neighborhoods, some of whose boundaries derive from the boroughs, townships, and other communities that made up Philadelphia County before their absorption into the city.
The City Planning Commission, tasked with guiding growth and development of the city, has divided the city into 18 planning districts as part of the "Philadelphia2035" physical development plan. Much of the city's 1980 zoning code was overhauled from 2007-2012 as part of a joint effort between Major John F. Street and Michael Nutter. The zoning changes were intended to rectify incorrect zoning mapping that would streamline future community preferences and development, which the city forecasts an additional 100,000 residents and 40,000 jobs to be added to Philadelphia in 2035.
The Philadelphia Housing Authority is the largest landlord in Pennsylvania. Established in 1937, it is the nation's fourth-largest housing authority, housing about 84,000 people and employing 1,250. In 2013, its budget was $371 million. The Philadelphia Parking Authority works to ensure adequate parking for city residents, businesses and visitors.
Architecture.
Philadelphia's architectural history dates back to Colonial times and includes a wide range of styles. The earliest structures were of logs construction, but brick structures were common by 1700. During the 18th century, the cityscape was dominated by Georgian architecture, including Independence Hall and Christ Church.
In the first decades of the 19th century, Federal architecture and Greek Revival architecture were dominated by Philadelphia architects such as Benjamin Latrobe, William Strickland, John Haviland, John Notman, Thomas U. Walter, and Samuel Sloan. Frank Furness is considered Philadelphia's greatest architect of the second half of the 19th century, but his contemporaries included John McArthur, Jr., Addison Hutton, Wilson Eyre, the Wilson Brothers, and Horace Trumbauer. In 1871, construction began on the Second Empire-style Philadelphia City Hall. The Philadelphia Historical Commission was created in 1955 to preserve the cultural and architectural history of the city. The commission maintains the Philadelphia Register of Historic Places, adding historic buildings, structures, sites, objects and districts as it sees fit.
In 1932, Philadelphia became home to the first International Style skyscraper in the United States, The PSFS Building, designed by George Howe and William Lescaze. It is the United States' first modern skyscraper and considered the most important one built in the first part of the 20th century.
The 548 ft City Hall remained the tallest building in the city until 1987 when One Liberty Place was constructed. Numerous glass and granite skyscrapers were built in Philadelphia's Center City from the late 1980s onwards. In 2007, the Comcast Center surpassed One Liberty Place to become the city's tallest building. The Comcast Innovation and Technology Center is under construction in Center City and is planned to reach a height of 1,121 feet (342 meters); upon completion, the tower is expected to be the tallest skyscraper in the United States outside of New York City and Chicago.
For much of Philadelphia's history, the typical home has been the row house. The row house was introduced to the United States via Philadelphia in the early 19th century and, for a time, row houses built elsewhere in the United States were known as "Philadelphia rows". A variety of row houses are found throughout the city, from Victorian-style homes in North Philadelphia to twin row houses in West Philadelphia. While newer homes are scattered throughout the city, much of the housing is from the early 20th century or older. The great age of the homes has created numerous problems, including blight and vacant lots in many parts of the city, while other neighborhoods such as Society Hill, which has the largest concentration of 18th-century architecture in the United States, have been rehabilitated and gentrified.
Climate.
Under the Köppen climate classification, Philadelphia falls in the northern periphery of the humid subtropical climate zone (Köppen "Cfa"). Summers are typically hot and muggy, fall and spring are generally mild, and winter is cold. Snowfall is highly variable, with some winters bringing only light snow and others bringing several major snowstorms, with the normal seasonal snowfall standing at 22.4 in; snow in November or April is rare, and a sustained snow cover is rare. Precipitation is generally spread throughout the year, with eight to twelve wet days per month, at an average annual rate of 41.5 in, but historically ranging from 29.31 in in 1922 to 64.33 in in 2011. The most rain recorded in one day occurred on July 28, 2013, when 8.02 in fell at Philadelphia International Airport.
The January daily average is 33.0 °F, though, in a normal winter, the temperature frequently rises to 50 °F during thaws and dips to 10 °F for 2 or 3 nights. July averages 78.1 °F, although heat waves accompanied by high humidity and heat indices are frequent; highs reach or exceed 90 °F on 27 days of the year. The average window for freezing temperatures is November 6 thru April 2, allowing a growing season of 217 days. Early fall and late winter are generally dry; February's average of 2.64 in makes it the area's driest month. The dewpoint in the summer averages between 59.1 °F to 64.5 °F.
Seasonal snowfall accumulation has ranged from trace amounts in 1972–73 to 78.7 in in the winter of 2009–10. The city's heaviest single-storm snowfall, at 30.7 in, occurred in January 1996.
The highest recorded temperature was 106 °F on August 7, 1918, but 100 °F+ temperatures are uncommon. The lowest officially recorded temperature was -11 °F on February 9, 1934, but with the last such occurrence being January 19, 1994, temperatures at or below the 0 °F mark are rare. The record low maximum is 5 °F on February 10, 1899 and December 30, 1880, while the record high minimum is 83 °F on July 23, 2011 and July 24, 2010.
In the American Lung Association 2015 State of the Air report, Philadelphia County received an ozone grade of F and a 24-hour particle pollution rating of C. The county passed the annual particle pollution rating.
Demographics.
According to the 2014 United States Census estimates, there were 1,560,297 people residing in the City of Philadelphia, representing a 2.2% increase since 2010. From the 1960s up until 2006, the city's population declined year after year. It eventually reached a low of 1,488,710 residents in 2006 before beginning to rise again. Since 2006, Philadelphia added 71,587 residents in eight years. A study done by the city projected that the population would increase to about 1,630,000 residents by 2035, an increase of about 100,000 from 2010.
The racial makeup of the city in 2013 was 45.5% White (36.3% Non-Hispanic), 44.2% Black or African American, 0.8% Native American and Alaska Native, 6.9% Asian, 0.1% Native Hawaiian and Other Pacific Islander, 2.4% Two or More Races, and 13.3% were Hispanic or Latino.
In comparison, the 2010 Census Redistricting Data indicated that the racial makeup of the city was 661,839 (43.4%) African American, 626,221 (41.0%) White, 6,996 (0.5%) Native American, 96,405 (6.3%) Asian (2.0% Chinese, 1.2% Indian, 0.9% Vietnamese, 0.6% Cambodian, 0.4% Korean, 0.3% Filipino, 0.2% Pakistani, 0.1% Indonesian), 744 (0.0%) Pacific Islander, 90,731 (5.9%) from other races, and 43,070 (2.8%) from two or more races. Hispanic or Latino of any race were 187,611 persons (12.3%); 8.0% of Philadelphia is Puerto Rican, 1.0% Dominican, 1.0% Mexican, 0.3% Cuban, and 0.3% Colombian. The racial breakdown of Philadelphia's Hispanic/Latino population was 63,636 (33.9%) White, 17,552 (9.4%) African American, 3,498 (1.9%) Native American, 884 (0.47%) Asian, 287 (0.15%) Pacific Islander, 86,626 (46.2%) from other races, and 15,128 (8.1%) from two or more races. The five largest European ancestries reported in the 2010 United States Census Census included Irish (12.5%), Italian (8.4%), German (8.1%), Polish (3.6%), and English (3.0%).
The average population density was 11,457 people per square mile (4,405.4/km²). The Census reported that 1,468,623 people (96.2% of the population) lived in households, 38,007 (2.5%) lived in non-institutionalized group quarters, and 19,376 (1.3%) were institutionalized. In 2013, the city reported having 668,247 total housing units, down slightly from 670,171 housing units in 2010. As of 2013, 87 percent of housing units were occupied, while 13 percent were vacant, a slight change from 2010 where 89.5 percent of units were occupied, or 599,736 and 10.5 percent were vacant, or 70,435. Of the city's residents, 32 percent reported having no vehicles available while 23 percent had two or more vehicles available, as of 2013.
In 2010, 24.9 percent of households reported having children under the age of 18 living with them, 28.3 percent were married couples living together and 22.5 percent had a female householder with no husband present, 6.0 percent had a male householder with no wife present, and 43.2 percent were non-families. The city reported 34.1 percent of all households were made up of individuals while 10.5 percent had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 3.20. In 2013, the percentage of women who gave birth in the previous 12 months who were unmarried was 56 percent. Of Philadelphia's adults, 31 percent were married or lived as a couple, 55 percent were not married, 11 percent were divorced or separated, and 3 percent were widowed.
According to the Census Bureau, the median household income in 2013 was $36,836, down 7.9 percent from 2008 when the median household income was $40,008 (in 2013 dollars). For comparison, the median household income among metropolitan areas was $60,482, down 8.2 percent in the same period, and the national median household income was $55,250, down 7.0 percent from 2008. The city's wealth disparity is evident when neighborhoods are compared. Residents in Society Hill had a median household income of $93,720 while residents in one of North Philadelphia's districts reported the lowest median household income, $14,185.
During the last decade, Philadelphia experienced a large shift in its age profile. In 2000, the city's population pyramid had a largely stationary shape. In 2013, the city took on an expansive pyramid shape, with an increase in the three millennial age groups, 20 to 24, 25 to 29, and 30 to 34. The city's 25- to 29-years-old age group was the city's largest age cohort. According to the 2010 Census, 343,837 (22.5%) were under the age of 18; 203,697 (13.3%) from 18 to 25; 434,385 (28.5%) from 25 to 44; 358,778 (23.5%) from 45 to 64; and 185,309 (12.1%) who were 65 years of age or older. The median age was 33.5 years. For every 100 females there were 89.4 males. For every 100 females age 18 and over, there were 85.7 males. The city had 22,018 births in 2013, down from a peak 23,689 births in 2008. Philadelphia's death rate was at its lowest in at least a half-century, 13,691 deaths in 2013. Another factor attributing to the population increase is Philadelphia's immigration rate. In 2013, 12.7 percent of residents were foreign-born, just shy of the national average, 13.1 percent.
Irish, Italians, Polish, Germans, English, and Greeks are the largest ethnic European groups in the city. Philadelphia has the second-largest Irish and Italian populations in the United States, after New York City. South Philadelphia remains one of the largest Italian neighborhoods in the country and is home to the Italian Market. The Pennsport neighborhood and Gray's Ferry section of South Philadelphia, home to many Mummer clubs, are well known as Irish neighborhoods. The Kensington section, Port Richmond, and Fishtown have historically been heavily Irish and Polish. Port Richmond is well known in particular as the center of the Polish immigrant and Polish-American community in Philadelphia, and it remains a common destination for Polish immigrants. Northeast Philadelphia, although known for its Irish and Irish-American population, is also home to a large Jewish and Russian population. Mount Airy in Northwest Philadelphia also contains a large Jewish community, while nearby Chestnut Hill is historically known as an Anglo-Saxon Protestant stronghold.
There has also been an increase of yuppie, bohemian, and hipster types particularly around Center City, the neighborhood of Northern Liberties, and in the neighborhoods around the city's universities, such as near Temple in North Philadelphia and particularly near Drexel and University of Pennsylvania in West Philadelphia.
The African American population in Philadelphia is the third-largest in the country, after New York City and Chicago. Historically, West Philadelphia and North Philadelphia were largely black neighborhoods, but many are leaving these areas in favor of the Northeast and Southwest sections of Philadelphia. There is a higher proportion of Muslims in the African American population than most cities in America. West Philadelphia also has significant Caribbean and African populations.
The Puerto Rican population in Philadelphia is the second-largest after New York City, and the second-fastest growing after Orlando. There are large Puerto Rican and Dominican populations in North Philadelphia and the Northeast, as well as a significant Mexican population in South Philadelphia.
Philadelphia has significant Asian populations mainly hailing from countries like India, China, Vietnam, and South Korea. Chinatown and the Northeast have the largest Asian presences, with a large Korean community in Olney, Philadelphia. South Philadelphia is also home to large Cambodian, Vietnamese, and Chinese communities. It has the fifth largest Muslim population among American cities.
Languages.
As of 2010, 79.12% (1,112,441) of Philadelphia residents age 5 and older spoke English at home as a primary language, while 9.72% (136,688) spoke Spanish, 1.64% (23,075) Chinese, 0.89% (12,499) Vietnamese, 0.77% (10,885) Russian, 0.66% (9,240) French, 0.61% (8,639) other Asian languages, 0.58% (8,217) African languages, 0.56% (7,933) Cambodian (Mon-Khmer), and Italian was spoken as a main language by 0.55% (7,773) of the population over the age of five. In total, 20.88% (293,544) of Philadelphia's population age 5 and older spoke a mother language other than English.
Economy.
Philadelphia is the center of economic activity in Pennsylvania with the headquarters of seven Fortune 1000 companies located within city limits. According to the Bureau of Economic Analysis, the Philadelphia area had a total gross metropolitan product of $347 billion in 2010, the seventh-largest metropolitan economy in the United States. Philadelphia was rated by the GaWC5 as a 'Beta+ City' in its categorization of world cities.
Philadelphia's economic sectors include information technology, manufacturing, oil refining, food processing, health care and biotechnology, tourism and financial services. Financial activities account for the largest sector of the metro economy, and it is one of the largest health education and research centers in the United States.
The city is home to the Philadelphia Stock Exchange some of the area's largest companies including cable television and internet provider Comcast, insurance companies Colonial Penn, CIGNA, Independence Blue Cross, energy company Sunoco, food services company Aramark and Crown, chemical makers Rohm and Haas and FMC, pharmaceutical company GlaxoSmithKline, Boeing Rotorcraft Systems, and automotive parts retailer Pep Boys.
Philadelphia's an annualized unemployment rate was 7.8 percent in 2014, down from 10.0 percent the previous year. This is higher than the national average of 6.2 percent. Similarly, the rate of new jobs added to the city's economy lagged behind the national job growth. In 2014, about 8,800 jobs were added to the city's economy. Sectors with the largest number of jobs added were in education and health services, leisure and hospitality, and professional and business services. Declines were seen in the city's manufacturing and government sectors.
While about 31.9 percent of the city's population is not in the labor force, the city's largest employers are the federal and city governments, respectively. Philadelphia's largest private employer is the University of Pennsylvania followed by the Children's Hospital of Philadelphia. A study commissioned by the city's government projected 40,000 jobs to be added to the city by 2035, raising the city's 2010 number of jobs from 675,000 total to an estimated 715,000 jobs.
Philadelphia's history attracts many tourists, with the Liberty Bell receiving over 3.6 million visitors in 2014. The Greater Philadelphia region was visited by 39 million people in 2013 generating $10 billion in economic impact.
Culture.
Philadelphia is home to many national historical sites that relate to the founding of the United States. Independence National Historical Park is the center of these historical landmarks being one of the country's 22 UNESCO World Heritage Sites. Independence Hall, where the Declaration of Independence was signed, and the Liberty Bell are the city's most famous attractions. Other historic sites include homes for Edgar Allan Poe, Betsy Ross, and Thaddeus Kosciuszko, early government buildings like the First and Second Banks of the United States, Fort Mifflin, and the Gloria Dei (Old Swedes') Church. Philadelphia alone has 67 National Historic Landmarks, the third most of any city in the country. 
Philadelphia's major science museums include the Franklin Institute, which contains the Benjamin Franklin National Memorial; the Academy of Natural Sciences; the Mütter Museum; and the University of Pennsylvania Museum of Archaeology and Anthropology. History museums include the National Constitution Center, the Atwater Kent Museum of Philadelphia History, the National Museum of American Jewish History, the African American Museum in Philadelphia, the Historical Society of Pennsylvania, the Grand Lodge of Free and Accepted Masons in the state of Pennsylvania and The Masonic Library and Museum of Pennsylvania and Eastern State Penitentiary. Philadelphia is home to the United States' first zoo and hospital, as well as Fairmount Park, one of America's oldest and largest urban parks.
The city is home to important archival repositories, including the Library Company of Philadelphia, established in 1731, and the Athenaeum of Philadelphia, founded in 1814. The Presbyterian Historical Society, the country's oldest continuous denominational historical society, is also located there.
Accent.
The Philadelphia dialect, which is spread throughout the Delaware Valley and South Jersey, is part of Mid-Atlantic American English, and as such it is identical in many ways to the Baltimore dialect. Unlike the Baltimore dialect, however, the Philadelphia accent also shares many similarities with the New York accent. Thanks to over a century of linguistics data collected by researchers at the University of Pennsylvania, the Philadelphia dialect is one of the best-studied forms of American English.
Arts.
The city contains many art museums, such as the Pennsylvania Academy of the Fine Arts and the Rodin Museum, which holds the largest collection of work by Auguste Rodin outside of France. The city's major art museum, the Philadelphia Museum of Art, is one of the largest art museums in the United States. Its long flight of steps to the main entrance became famous after the film "Rocky" (1976).
The city is home to the Philadelphia Sketch Club, one of the country's oldest artists' clubs, and The Plastic Club, started by women excluded from the Sketch Club. It has a profusion of art galleries, many of which participate in the First Friday event. The first Friday of every month, galleries in Old City are open late. Annual events include film festivals and parades, the most famous being the New Year's Day Mummers Parade.
Areas such as South Street and Old City have a vibrant night life. The Avenue of the Arts in Center City contains many restaurants and theaters, such as the Kimmel Center for the Performing Arts, which is home to the Philadelphia Orchestra, generally considered one of the top five orchestras in the United States, and the Academy of Music, the nation's oldest continually operating opera house, home to the Opera Company of Philadelphia and the Pennsylvania Ballet. The Wilma Theatre and Philadelphia Theatre Company have new buildings constructed in the last decade on the avenue. They produce a variety of new works. Several blocks to the east are the Walnut Street Theatre, America's oldest theatre and the largest subscription theater in the world; as well as the Lantern Theatre at St. Stephens Church, one of a number of smaller venues.
Philadelphia has more public art than any other American city. In 1872, the Association for Public Art (formerly the Fairmount Park Art Association) was created, the first private association in the United States dedicated to integrating public art and urban planning. In 1959, lobbying by the Artists Equity Association helped create the Percent for Art ordinance, the first for a U.S. city. The program, which has funded more than 200 pieces of public art, is administered by the Philadelphia Office of Arts and Culture, the city's art agency.
Philadelphia has more murals than any other U.S. city, thanks in part to the 1984 creation of the Department of Recreation's Mural Arts Program, which seeks to beautify neighborhoods and provide an outlet for graffiti artists. The program has funded more than 2,800 murals by professional, staff and volunteer artists and educated more than 20,000 youth in underserved neighborhoods throughout Philadelphia.
Philadelphia artists have had a prominent national role in popular music. In the 1970s, Philadelphia soul influenced the music of that and later eras. On July 13, 1985, Philadelphia hosted the American end of the Live Aid concert at John F. Kennedy Stadium. The city reprised this role for the Live 8 concert, bringing some 700,000 people to the Ben Franklin Parkway on July 2, 2005. Philadelphia is home to the world-renowned Philadelphia Boys Choir & Chorale, which has performed its music all over the world. Dr. Robert G. Hamilton, founder of the choir, is a notable native Philadelphian. The Philly Pops is another famous Philadelphia music group. The city has played a major role in the development and support of American rock music and rap music. Hip-hop/Rap artists such as The Roots, DJ Jazzy Jeff & The Fresh Prince, The Goats, Freeway, Schoolly D, Eve, and Lisa "Left Eye" Lopes hail from the city.
Cuisine.
The city is known for its hoagies, scrapple, soft pretzels, water ice, Irish potato candy, Tastykake, and is home to the cheesesteak, developed by German and Italian immigrants. Philadelphia boasts a number of cheesesteak establishments, however two locations in South Philadelphia are perhaps the most famous among tourists: Pat's King of Steaks and its across the street rival Geno's Steaks.
Its high-end restaurants include Morimoto, "Iron Chef" Masaharu Morimoto's first restaurant, Vetri, famous on the East Coast for its take on Northern Italian cuisine, and Lacroix, a staple restaurant situated in Rittenhouse Square. Italian specialties have been supplemented by many new Vietnamese and other Asian restaurants, both budget and high-end.
Philadelphia is also home to a landmark eatery founded in 1892, the Reading Terminal Market. The enclosed public market hosts over a hundred merchants offering Pennsylvania Dutch specialties, artisan cheese and meat, locally grown groceries, and specialty and ethnic foods.
Sports.
Philadelphia's professional sports teams date at least to the 1860 founding of baseball's Athletics. The city is one of 12 U.S. cities to have all four major sports: the Philadelphia Eagles of the National Football League, the Philadelphia Flyers of the National Hockey League, the Philadelphia Phillies in the National League of Major League Baseball, and the Philadelphia 76ers in the National Basketball Association.
The Philadelphia metro area is also home of the Philadelphia Union of Major League Soccer. The Union play their home games at PPL Park, a soccer-specific stadium in Chester, Pennsylvania. Philadelphia began play in MLS in 2010, after beating several other cities in competition for the rights to an MLS expansion franchise.
The city's professional teams went without a championship from 1983, when the 76ers won the NBA Championship, until 2008, when the Phillies won the World Series. In 2004, ESPN ranked Philadelphia second on its list of The Fifteen Most Tortured Sports Cities. The failure was sometimes attributed in jest to the "Curse of Billy Penn."
Major-sport professional sports teams that originated in Philadelphia but ultimately moved to other cities include the Golden State Warriors basketball team and the Oakland Athletics baseball team.
Philadelphia is also the home city of the Philadelphia Spinners, a professional ultimate team that is part of the American Ultimate Disc League. They are one of the original eight teams of the league that began in April 2012 and play their home games at the University of Pennsylvania.
Philadelphia is home to professional, semi-professional and elite amateur teams in cricket, rugby league (Philadelphia Fight), rugby union and other sports. Major sporting events in the city include the Penn Relays, Stotesbury Cup, Philadelphia Marathon, Broad Street Run, Philadelphia International Championship bicycle race, and the Dad Vail Regatta. The Collegiate Rugby Championship is played every June at PPL Park; the CRC is broadcast live on NBC and regularly draws attendances of 18,000.
Philadelphia is home to the Philadelphia Big 5, a group of five Division I college basketball programs. The Big 5 are Saint Joseph's University, University of Pennsylvania, La Salle University, Temple University, and Villanova University. The sixth NCAA Division I school in Philadelphia is Drexel University. At least one of the teams is competitive nearly every year and at least one team has made the NCAA tournament for the past four decades.
Parks.
The total parkland amounts to about 10334 acre. Philadelphia's largest park, Fairmount Park, encompasses 9200 acre of this parkland and includes 63 neighborhood and regional parks. The largest tract of Fairmount Park is on the west side of the city along the Schuylkill River and Wissahickon Creek and includes the Philadelphia Zoo.
The total expenditures of the park in 2005 were $164 million. Fairmount Park is the world's largest landscaped urban park.
Law and government.
From a governmental perspective, Philadelphia County is a legal nullity, as all county functions were assumed by the city in 1952, which has been coterminous with the county since 1854.
Philadelphia's 1952 Home Rule Charter was written by the City Charter Commission, which was created by the Pennsylvania General Assembly in an Act of April 21, 1949, and a city ordinance of June 15, 1949. The existing City Council received a proposed draft on February 14, 1951, and the electors approved it in an election held April 17, 1951. The first elections under the new Home Rule Charter were held in November 1951, and the newly elected officials took office in January 1952.
The city uses the strong-mayor version of the mayor-council form of government, which is headed by one mayor, in whom executive authority is vested. Elected at-large, the mayor is limited to two consecutive four-year terms under the city's home rule charter, but can run for the position again after an intervening term. The Mayor is Michael Nutter, who replaced John Street, who served two terms from 1999 to January 2008. Nutter, as all Philadelphia mayors have been since 1952, is a member of the Democratic Party, which tends to dominate local politics so thoroughly that the Democratic Mayoral primary is often more widely covered than the general election. The legislative branch, the Philadelphia City Council, consists of ten council members representing individual districts and seven members elected at large. Democrats currently hold 14 seats, with Republicans representing two allotted at-large seats for the minority party, as well as the Northeast-based Tenth District. The current council president is Darrell Clarke.
Courts.
The Philadelphia County Court of Common Pleas (First Judicial District) is the trial court of general jurisdiction for Philadelphia, hearing felony-level criminal cases and civil suits above the minimum jurisdictional limit of $7000 (excepting small claims cases valued between $7000 and $12000 and landlord-tenant issues heard in the Municipal Court) under its original jurisdiction; it also has appellate jurisdiction over rulings from the Municipal and Traffic Courts and over decisions of certain Pennsylvania state agencies (e.g. the Pennsylvania Liquor Control Board). It has 90 legally trained judges elected by the voters. It is funded and operated largely by city resources and employees. The current District Attorney is Seth Williams, a Democrat. The last Republican to hold the office is Ron Castille, who left in 1991 and is currently the Chief Justice of the Pennsylvania Supreme Court.
The Philadelphia Municipal Court handles matters of limited jurisdiction as well as landlord-tenant disputes, appeals from traffic court, preliminary hearings for felony-level offenses, and misdemeanor criminal trials. It has 25 legally trained judges elected by the voters.
Philadelphia Traffic Court is a court of special jurisdiction that hears violations of traffic laws. It has seven judges elected by the voters. As with magisterial district judges, the judges need not be lawyers, but must complete the certifying course and pass the qualifying examination administered by the Minor Judiciary Education Board.
Pennsylvania's three appellate courts also have sittings in Philadelphia. The Supreme Court of Pennsylvania, the court of last resort in the state, regularly hears arguments in Philadelphia City Hall. Also, the Superior Court of Pennsylvania and the Commonwealth Court of Pennsylvania sit in Philadelphia several times a year. Judges for these courts are elected at large. Each court has a prothonotary's office in Philadelphia as well.
Additionally, Philadelphia is home to the federal United States District Court for the Eastern District of Pennsylvania and the Court of Appeals for the Third Circuit, both of which are housed in the James A. Byrne United States Courthouse.
Politics.
As of December 31, 2009, there were 1,057,038 registered voters in Philadelphia. Registered voters constitute 68.3% of the total population.
From the American Civil War until the mid-20th century, Philadelphia was a bastion of the Republican Party, which arose from the staunch pro-Northern views of Philadelphia residents during and after the war (Philadelphia was chosen as the host city for the first Republican National Convention in 1856). After the Great Depression, Democratic registrations increased, but the city was not carried by Democrat Franklin D. Roosevelt in his landslide victory of 1932 (in which Pennsylvania was one of the few states won by Republican Herbert Hoover). Four years later, however, voter turnout surged and the city finally flipped to the Democrats. Roosevelt carried Philadelphia with over 60% of the vote in 1936. The city has remained loyally Democratic in every presidential election since.
The city is now one of the most Democratic in the country; in 2008, Democrat Barack Obama drew 83% of the city's vote. Obama's win was even greater in 2012, capturing 85% of the vote.
Philadelphia once comprised six congressional districts. However, as a result of the city's declining population, it now has only four: the 1st district, represented by Bob Brady; the 2nd, represented by Chaka Fattah; the 8th, represented by Mike Fitzpatrick; and the 13th, represented by Brendan Boyle. All but Fitzpatrick are Democrats. Although they are usually swamped by Democrats in city, state and national elections, Republicans still have some support in the area, primarily in the northeast. A Republican represented a significant portion of Philadelphia in the House as late as 1983, and Sam Katz ran competitive mayoral races as the Republican nominee in both 1999 and 2003.
Pennsylvania's longest-serving Senator, Arlen Specter, was from Philadelphia; he served as a Republican from 1981 and as a Democrat from 2009, losing that party's primary in 2010 and leaving office in January 2011. He was also the city's District Attorney from 1966 to 1974.
Crime.
Like many American cities, Philadelphia saw a gradual yet pronounced rise in crime in the years following World War II. There were 525 murders in 1990, a rate of 31.5 per 100,000. There were an average of about 600 murders a year for most of the 1990s. The murder count dropped in 2002 to 288, then rose four years later to 406 in 2006 and 392 in 2007. A few years later, Philadelphia began to see a rapid drop in homicides and violent crime. In 2013, there were 246 murders, which is a decrease of over 25% from the previous year, and a decrease of over 44% since 2007. And in 2014, there were 248 homicides, up by one since 2013.
In 2006, Philadelphia's homicide rate of 27.7 per 100,000 people was the highest of the country's 10 most populous cities. In 2012, Philadelphia had the fourth-highest homicide rate among the country's most populous cities. And in 2014, the rate dropped to 16.0 homicides per 100,000 residents placing Philadelphia as the sixth-highest city in the country.
In 2004, there were 7,513.5 crimes per 200,000 people in Philadelphia. Among its neighboring Mid-Atlantic cities in the same population group, Baltimore and Washington, D.C. were ranked second- and third-most dangerous cities in the United States, respectively. Camden, New Jersey, a city across the Delaware River from Philadelphia, was ranked as the most dangerous city in the United States.
The number of shootings in the city has declined significantly in the last 10 years. Shooting incidents peaked in 2006 when 1,857 shootings were recorded. That number has dropped 44 percent to 1,047 shootings in 2014. Similarly, major crimes in the city has decreased gradually in the last ten years since its peak in 2006 when 85,498 major crimes were reported. In the past three years, the number of reported major crimes fell 11 percent to a total of 68,815. Violent crimes, which include homicide, rape, aggravated assault, and robbery, decreased 14 percent in the past three years with a reported 15,771 occurrences in 2014. Based on the rate of violent crimes per 1,000 residents in American cities with 25,000 people or more, Philadelphia was ranked as the 54th most dangerous city in 2015.
Education.
Primary and secondary education.
Education in Philadelphia is provided by many private and public institutions. The School District of Philadelphia runs the city's public schools. The Philadelphia School District is the eighth largest school district in the United States with 142,266 students in 218 public schools and 86 charter schools as of 2014.
The city's K-12 enrollment in district run schools has dropped in the last five years from 156,211 students in 2010 to 130,104 students in 2015. During the same time period, the enrollment in charter schools has increased from 33,995 students in 2010 to 62,358 students in 2015. This consistent drop in enrollment has led the city to close 24 of its public schools in 2013. During the 2014 school year, the city spent an average of $12,570 per pupil, below the average among comparable urban school districts.
Graduation rates among district-run schools, meanwhile, have steadily increased in the last ten years. In 2005, Philadelphia had a district graduation rate of 52%. This number has increased to 65% in 2014, still below the national and state averages. Scores on the state's standardized test, the Pennsylvania System of School Assessment (PSSA) have trended upward from 2005 to 2011 but have decreased since. In 2005, the district-run schools scored an average of 37.4% on math and 35.5% on reading. The city's schools reached its peak scores in 2011 with 59.0% on math and 52.3% on reading. In 2014, the scores dropped significantly to 45.2% on math and 42.0% on reading.
Of the city's public high schools, including charter schools, only four performed above the national average on the SAT (1497) in 2014: Masterman, Central, Girard, and MaST Community Charter School. All other district-run schools were below average.
Higher Education.
Philadelphia has the third-largest student concentration on the East Coast, with over 120,000 college and university students enrolled within the city and nearly 300,000 in the metropolitan area. There are over 80 colleges, universities, trade, and specialty schools in the Philadelphia region. One of the founding members of the Association of American Universities is in city, the University of Pennsylvania, an Ivy League institution with claims to being the oldest university in the country.
The city's largest private school by number of students is Temple University, followed by Drexel University. Along with the University of Pennsylvania, Temple University and Drexel University make up the city's major research universities. The city is also home to five schools of medicine: Drexel University College of Medicine, Perelman School of Medicine at the University of Pennsylvania, Philadelphia College of Osteopathic Medicine, Temple University School of Medicine, and the Thomas Jefferson University. Hospitals, universities, and higher education research institutions in Philadelphia's four congressional districts received more than $252 million in National Institutes of Health grants in 2015.
Other institutions of higher learning within the city's borders include:
The Philadelphia suburbs are home to a number of other colleges and universities, including Villanova University, Bryn Mawr College, Haverford College, Swarthmore College, Ursinus College, Cabrini College, and Eastern University.
Media.
Newspapers.
Philadelphia's two major daily newspapers are "The Philadelphia Inquirer", which is the eighteenth largest newspaper and third-oldest surviving daily newspaper in the country, and the "Philadelphia Daily News". Both newspapers were purchased from The McClatchy Company (after buying out Knight Ridder) in 2006 by Philadelphia Media Holdings and operated by the group until the organization declared bankruptcy in 2010. After two years of financial struggle, the two newspapers were sold to Interstate General Media in 2012. The two newspapers have a combined circulation of about 500,000 readers.
The city also has a number of other, smaller newspapers and magazine in circulation such as the "Philadelphia Tribune", which serves the African-American community, the "Philadelphia", a monthly regional magazine; "Philadelphia Weekly", an weekly-printed alternative newspaper; "Philadelphia City Paper" another weekly-printed newspaper; "Philadelphia Gay News", which services the LGBT community; "The Jewish Exponent" a weekly-printed newspaper servicing the Jewish community; "Philadelphia Metro", free daily newspaper; and "Al Día", a weekly newspaper servicing the Latino community.
In addition, there are several student-run newspapers including "The Daily Pennsylvanian", "The Temple News", and "The Triangle".
Radio and Television.
The first experimental radio license was issued in Philadelphia in August 1912 to St. Joseph's College. The first commercial broadcasting radio stations appeared in 1922: first WIP, then owned by Gimbel's department store, on March 17, followed the same year by WFIL, WOO, WCAU and WDAS. The highest-rated stations in Philadelphia include soft rock WBEB, KYW Newsradio, and urban adult contemporary WDAS-FM. Philadelphia is served by three major non-commercial public radio stations, WHYY-FM (NPR), WRTI (jazz, classical), and WXPN-FM (adult alternative music), as well as several smaller stations.
Rock stations WMMR and WYSP had historically been intense rivals. However, in 2011, WYSP switched to sports talk as WIP-FM, which broadcasts all Philadelphia Eagles games. WMMR's "The Preston and Steve Show" has been the area's top-rated morning show since Howard Stern left broadcast radio for satellite-based Sirius Radio.
Four urban stations (WUSL ("Power 99"), WPHI ("Hot 107.9"), WDAS and WRNB ("Old School 100.3") are popular choices on the FM dial. WBEB is the city's Adult Contemporary station, while WRDW ("Wired 96.5") is the major Rhythmic Top 40 station.
In the 1930s, the experimental station W3XE, owned by Philco, became the first television station in Philadelphia; it became NBC's first affiliate in 1939, and later became KYW-TV (CBS). WCAU-TV, WPVI-TV, WHYY-TV, WPHL-TV, and WTXF-TV had all been founded by the 1970s. In 1952, WFIL (now WPVI) premiered the television show "Bandstand", which later became the nationally broadcast "American Bandstand" hosted by Dick Clark. Today, as in many large metropolitan areas, each of the commercial networks has an affiliate, and call letters have been replaced by corporate IDs: CBS3, 6ABC, NBC10, Fox29, Telefutura28, Telemundo62, Univision65, plus My PHL 17 and CW Philly 57. The region is served also by public broadcasting stations WYBE-TV (Philadelphia), WHYY-TV (Wilmington, Delaware and Philadelphia), WLVT-TV (Lehigh Valley), and NJTV (New Jersey). In September 2007, Philadelphia approved a Public-access television cable TV channel.
Until September 2014, Philadelphia was the only media market in the United States with owned-and-operated stations of all five English-language major broadcast networks (NBC – WCAU, CBS – KYW-TV, ABC – WPVI-TV, Fox – WTXF-TV and The CW – WPSG); three of the major Spanish-language networks (Univision, UniMas and Telemundo) also have O&Os serving the market (respectively, WUVP-DT, WFPA-CA and WWSI).
The city is also the nation's fourth-largest consumer in media market, as ranked by the Nielsen Media Research, with over 2.9 million TV homes.
Infrastructure.
Transportation.
Philadelphia is served by the Southeastern Pennsylvania Transportation Authority (SEPTA), which operates buses, trains, rapid transit, trolleys, and trackless trolleys throughout Philadelphia, the four Pennsylvania suburban counties of Bucks, Chester, Delaware, and Montgomery, in addition to service to Mercer County, New Jersey and New Castle County, Delaware. The city's subway, opened in 1907, is the third-oldest in America.
In 1981, large sections of the SEPTA Regional Rail service to the far suburbs of Philadelphia were discontinued due to lack of funding. Several projects have been proposed to extend rail service back to these areas, but lack of funding has again been the chief obstacle to implementation. These projects include the proposed Schuylkill Valley Metro to Wyomissing, PA, and extension of the Media/Elwyn line back to Wawa, PA. SEPTA's Airport Regional Rail Line Regional Rail offers direct service to the Philadelphia International Airport.
Philadelphia's 30th Street Station is a major railroad station on Amtrak's Northeast Corridor, which offers access to Amtrak, SEPTA, and New Jersey Transit lines.
The PATCO Speedline provides rapid transit service to Camden, Collingswood, Westmont, Haddonfield, Woodcrest (Cherry Hill), Ashland (Voorhees), and Lindenwold, New Jersey, from stations on Locust Street between 16th and 15th, 13th and 12th, and 10th and 9th Streets, and on Market Street at 8th Street.
Airports.
Two airports serve Philadelphia: the Philadelphia International Airport (PHL), straddling the southern boundary of the city, and the Northeast Philadelphia Airport (PNE), a general aviation reliever airport in Northeast Philadelphia. Philadelphia International Airport provides scheduled domestic and international air service, while Northeast Philadelphia Airport serves general and corporate aviation. In 2013, Philadelphia International Airport was the 15th busiest airport in the world measured by traffic movements (i.e. takeoffs and landings). It is also a second largest hub and primary international hub for US Airways.
Roads.
William Penn initially planned a Philadelphia that had numbered streets traversing north and south and "tree" named streets traversing east and west, with the two main streets Broad Street and High Street converging at Centre Square. The plans have since expanded to include major highways that span other major sections of Philadelphia.
Interstate 95 runs through the city along the Delaware River as a main north-south artery known as the Delaware Expressway. The city is also served by the Schuylkill Expressway, a portion of Interstate 76 that runs along the Schuylkill River. It meets the Pennsylvania Turnpike at King of Prussia, Pennsylvania, providing access to Harrisburg, Pennsylvania and points west. Interstate 676, the Vine Street Expressway, was completed in 1991 after years of planning. A link between I-95 and I-76, it runs below street level through Center City, connecting to the Ben Franklin Bridge at its eastern end.
Roosevelt Boulevard and the Roosevelt Expressway (U.S. 1) connect Northeast Philadelphia with Center City. Woodhaven Road (Route 63), built in 1966, and Cottman Avenue (Route 73) serve the neighborhoods of Northeast Philadelphia, running between Interstate 95 and the Roosevelt Boulevard (U.S. 1). The Fort Washington Expressway (Route 309) extends north from the city's northern border, serving Montgomery County and Bucks County. U.S. 30, extending east-west from West Philadelphia to Lancaster, is known as Lancaster Avenue throughout most of the city and through the adjacent Main Line suburbs.
Interstate 476, commonly nicknamed the "Blue Route" through Delaware County, bypasses the city to the west, serving the city's western suburbs, as well as providing a link to Allentown and points north. Similarly, Interstate 276, the Pennsylvania Turnpike's Delaware River Extension, acts as a bypass and commuter route to the north of the city as well as a link to the New Jersey Turnpike to New York.
However, other planned freeways have been canceled, such as an Interstate 695 running southwest from downtown; two freeways connecting Interstate 95 to Interstate 76 that would have replaced Girard Avenue and South Street; and a freeway upgrade of Roosevelt Boulevard.
The Delaware River Port Authority operates four bridges in the Philadelphia area across the Delaware River to New Jersey: the Walt Whitman Bridge (I-76), the Benjamin Franklin Bridge (I-676 and US 30), the Betsy Ross Bridge (Route 90), and the Commodore Barry Bridge (US 322). The Tacony-Palmyra Bridge connects PA Route 73 in the Tacony section of Northeast Philadelphia with New Jersey's Route 73 in Palmyra, Camden County, and is maintained by the Burlington County Bridge Commission.
Bus service.
Philadelphia is also a major hub for Greyhound Lines, which operates 24-hour service to points east of the Mississippi River. Most of Greyhound's services in Philadelphia operate to/from the Philadelphia Greyhound Terminal, located at 1001 Filbert Street in Center City Philadelphia. In 2006, the Philadelphia Greyhound Terminal was the second busiest Greyhound terminal in the United States, after the Port Authority Bus Terminal in New York. Besides Greyhound, six other bus operators provide service to the Center City Greyhound terminal: Bieber Tourways, Capitol Trailways, Martz Trailways, Peter Pan Bus Lines, Susquehanna Trailways, and the bus division for New Jersey Transit. Other services include Megabus and Bolt Bus.
Rail.
Since the early days of rail transport in the United States, Philadelphia has served as hub for several major rail companies, particularly the Pennsylvania Railroad and the Reading Railroad. The Pennsylvania Railroad first operated Broad Street Station, then 30th Street Station and Suburban Station, and the Reading Railroad operated out of Reading Terminal, now part of the Pennsylvania Convention Center. The two companies also operated competing commuter rail systems in the area, known collectively as the Regional Rail system. The two systems today, for the most part still intact but now connected, operate as a single system under the control of the SEPTA, the regional transit authority. Additionally, the PATCO Speedline subway system and New Jersey Transit's Atlantic City Line operate successor services to southern New Jersey.
Philadelphia, once home to more than 4,000 trolleys on 65 lines, is one of the few North American cities to maintain streetcar lines. Today, SEPTA operates five "subway-surface" trolleys that run on street-level tracks in West Philadelphia and subway tunnels in Center City. SEPTA also recently reintroduced trolley service to the Girard Avenue Line, Route 15.
Today, Philadelphia is a regional hub of the federally owned Amtrak system, with 30th Street Station being a primary stop on the Washington-Boston Northeast Corridor and the Keystone Corridor to Harrisburg and Pittsburgh. 30th Street also serves as a major station for services via the Pennsylvania Railroad's former Pennsylvania Main Line to Chicago. 30th Street is Amtrak's third-busiest station in numbers of passengers as of fiscal year 2013.
Walkability.
A 2015 study by Walk Score ranked Philadelphia the fourth most walkable major city in the United States.
Utilities.
Historically, Philadelphia sourced its water by the Fairmount Water Works, the nation's first major urban water supply system. In 1909, Water Works was decommissioned as the city transitioned to modern sand filtration methods. Today, the Philadelphia Water Department (PWD) provides drinking water, wastewater collection, and stormwater services for Philadelphia, as well as surrounding counties. PWD draws about 57 percent of its drinking water from the Delaware River and the balance from the Schuylkill River. The public wastewater system consists of three water pollution control plants, 21 pumping stations, and about 3,657 miles of sewers. A 2007 investigation by the Environmental Protection Agency found elevated levels of Iodine-131 in the city's potable water. In 2012, the EPA's readings discovered that the city had the highest readings of I-131 in the nation. The city campaigned against against an Associated Press report that the high levels of I-131 were the results of local gas drilling in the Upper Delaware River.
PECO Energy Company, founded as the Philadelphia Electric Company in 1881, provides electricity to over 1.6 million customers in the southeastern Pennsylvania. The company has over 500 power substations and 29,000 miles of distribution of transmission lines in its service making it the largest combination utility in the state.
Philadelphia Gas Works, overseen by the Pennsylvania Public Utility Commission, is the nation's largest municipally owned natural gas utility. It serves over 500,000 homes and business in the Philadelphia area. Founded in 1836, the company came under city ownership in 1987 and has been providing the majority of gas distributed within city limits. In 2014, the Philadelphia City Council explored a $1.86 billion sale of PGW to settle city deficit but was ultimately declined by the prospective buyer.
Southeastern Pennsylvania was assigned the 215 area code in 1947 when the North American Numbering Plan of the "Bell System" went into effect. The geographic area covered by the code was split nearly in half in 1994 when area code 610 was created, with the city and its northern suburbs retaining 215. Overlay area code 267 was added to the 215 service area in 1997, and 484 was added to the 610 area in 1999. A plan in 2001 to introduce a third overlay code to both service areas (area code 445 to 215, area code 835 to 610) was delayed and later rescinded.
An effort was approved on 2005 to provide low-cost, citywide Wi-Fi service to the city. Wireless Philadelphia would have been the first municipal internet utility offering in a large US city, but the plan was abandoned in 2008 as EarthLink pushed back the completion date several times. Mayor Nutter's administration closed the project in 2009 after an attempt to revitalize it failed.
Sister cities.
Philadelphia has seven official sister cities, as designated by the Citizen Diplomacy International - Philadelphia:
Philadelphia also has three partnership cities or regions:
Philadelphia has dedicated landmarks to its sister cities. Dedicated in June 1976, the Sister Cities Plaza, a site of 0.5 acre located at 18th and Benjamin Franklin Parkway, honors Philadelphia's relationships with Tel Aviv and Florence which were its first sister cities. Another landmark, the Toruń Triangle, honoring the sister city relationship with Toruń, Poland, was constructed in 1976, west of the United Way building at 18th Street and the Benjamin Franklin Parkway. In addition, the Triangle contains the Copernicus monument. Renovations were made to Sister Cities Park in mid-2011 and on May 10, 2012, SCP was reopened and currently features an interactive fountain honoring Philadelphia's ten sister and friendship cities, a café and visitor's center, children's play area, outdoor garden, and boat pond, as well as pavilion built to environmentally friendly standards.
The Chinatown Gate, erected in 1984 and crafted by artisans of Tianjin, stands astride the intersection of 10th and Arch Streets as an elaborate and colorful symbol of the sister city relationship. The CDI of Philadelphia has participated in the U.S. Department of State's "Partners for Peace" project with Mosul, Iraq, as well as accepting visiting delegations from dozens of other countries.
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="50588" url="http://en.wikipedia.org/wiki?curid=50588" title="Expo 67">
Expo 67

The 1967 International and Universal Exposition or Expo 67, as it was commonly known, was the general exhibition, Category One World's Fair held in Montreal, Quebec, Canada, from April 27 to October 29, 1967. It is considered to be the most successful World's Fair of the 20th century , with the most attendees to that date and 62 nations participating. It also set the single-day attendance record for a world's fair, with 569,500 visitors on its third day.
Expo 67 was Canada's main celebration during its centennial year. The fair was originally intended to be held in Moscow, to help the Soviet Union celebrate the Russian Revolution's 50th anniversary; however, for various reasons, the Soviets decided to cancel, and Canada was awarded it in late 1962.
The project was not originally overwhelmingly supported in Canada. It took the determination of Montreal's mayor, Jean Drapeau, and a new team of managers to guide it past political, physical and temporal hurdles. Defying a computer analysis that said it could not be done, the fair opened on time.
After Expo 67 ended in October 1967, the site and most of the pavilions continued on as an exhibition called Man and His World, open during the summer months from 1968 until 1984. By that time, most of the buildings — which had not been designed to last beyond the original exhibition — had deteriorated and were dismantled. Today, the islands that hosted the world exhibition are mainly used as parkland and for recreational use, with only a few remaining structures from Expo 67 to show that the event was held there.
History.
Background.
The idea of hosting the 1967 World Exhibition dates back to 1956, but it was in 1958 that Conservative Senator Mark Drouin suggested during his speech at the Brussels Exhibition that Canada should host a World Exhibition to celebrate its centennial. Initially, the exposition was offered to Toronto but politicians there rejected the idea. However, Montreal's mayor, Sarto Fournier, backed the proposal, allowing Canada to make a bid to the Bureau International des Expositions (BIE). At the BIE's May 5, 1960 meeting in Paris, Moscow was awarded the fair after five rounds of voting that eliminated Austria's and then Canada's bids. In April 1962, the Soviets scrapped plans to host the fair due to financial constraints and security concerns. Montreal's new mayor, Jean Drapeau, lobbied the Canadian government to try again for the fair, which they did. On November 13, 1962, the BIE changed the location of the World Exhibition to Canada, and Expo 67 went on to become the fourth-best attended BIE-sanctioned world expositions, after Shanghai, Osaka, and Paris.
Several sites were proposed as the main Expo grounds. One location that was considered was Mount Royal Park, to the north of the downtown core. But it was Drapeau's idea to create new islands in the St. Lawrence river, along with enlarging the existing Saint Helen's Island. The choice overcame opposition from Montreal's surrounding municipalities, and also prevented land speculation.
Key people.
Expo did not get off to a smooth start; in 1963, many top organizing committee officials resigned. The main reason for the resignations was Mayor Drapeau's choice of the site on new islands to be created around the existing Ste. Helen's island and also that a computer program predicted that the event could not possibly be constructed in time. Another more likely reason for the mass resignations was that on April 22, 1963, the federal Liberal government of Prime Minister Lester Pearson took power. This meant that former Prime Minister John Diefenbaker's Progressive Conservative government appointees to the board of directors of the Canadian Corporation for the 1967 World Exhibition were likely forced to resign.
Canadian diplomat Pierre Dupuy was named Commissioner General, after Diefenbaker appointee Paul Bienvenu resigned from the post in 1963. One of the main responsibilities of the Commissioner General was to attract other nations to build pavilions at Expo. Dupuy would spend most of 1964 and 1965 soliciting 125 countries, spending more time abroad than in Canada. Dupuy's 'right-hand' man was Robert Fletcher Shaw, the deputy commissioner general and vice-president of the corporation. He also replaced a Diefenbaker appointee, C.F. Carsley, Deputy Commissioner General. Shaw was a professional engineer and builder, and is widely credited for the total building of the Exhibition. Dupuy hired Andrew Kniewasser as the general manager. The management group became known as "Les Durs" - the tough guys - and they were in charge of creating, building and managing Expo. "Les Durs" consisted of: Jean-Claude Delorme, Legal Counsel and Secretary of the Corporation; Dale Rediker, Director of Finances; Colonel Edward Churchill, Director of Installations; Philippe de Gaspé Beaubien, Director of Operations, dubbed "The Mayor of Expo"; Pierre de Bellefeuille, Director of Exhibitors; and Yves Jasmin, Director of Information, Advertising and Public Relations. To this group the chief architect Édouard Fiset was added. All ten were honored by the Canadian government as recipients of the Order of Canada, Companions for Dupuy and Shaw, Officers for the others.
Jasmin wrote a book, in French, "La petite histoire d'Expo 67", about his 45-month experience at Expo and created the Expo 67 Foundation (available on the web site under that name) to commemorate the event for future generations.
As historian Pierre Berton put it, the cooperation between Canada's French and English speaking communities "was the secret of Expo's success — 'the Québécois flair, the English-Canadian pragmatism.'" However, Berton also points out that this is an over-simplification of national stereotypes. Arguably Expo did, for a short period anyway, bridge the 'Two Solitudes.'
Montebello conference produces theme.
In May 1963, a group of prominent Canadian thinkers — including Alan Jarvis, director of the National Gallery of Canada; novelists Hugh MacLennan and Gabrielle Roy; J. Tuzo Wilson, geophysicist; and Claude Robillard, town planner — met for three days at the Seigneury Club in Montebello, Quebec. The theme, "Man and His World", was based on the 1939 book entitled "Terre des Hommes" (translated as "Wind, Sand and Stars") by Antoine de Saint-Exupéry. In Roy's introduction to the Expo 67 corporation's book, entitled "Terre des Hommes/Man and His World", she elucidates the theme:
 In "Terre des Hommes", his haunting book, so filled with dreams and hopes for the future, Antoine de Saint-Exupéry writes of how deeply moved he was when, flying for the first time by night alone over Argentina, he happened to notice a few flickering lights scattered below him across an almost empty plain. They "twinkled here and there, alone like stars."
... In truth, being made aware of our own solitude can give us insight into the solitude of others. It can even cause us to gravitate towards one another as if to lessen our distress. Without this inevitable solitude, would there be any fusion at all, any tenderness between human beings.
Moved as he was by a heightened awareness of the solitude of all creation and by the human need for solidarity, Saint-Exupéry found a phrase to express his anguish and his hope that was as simple as it was rich in meaning; and because that phrase was chosen many years later to be the governing idea of Expo 67, a group of people from all walks of life was invited by the Corporation to reflect upon it and to see how it could be given tangible form.
 — Gabrielle Roy
The organizers also created seventeen theme elements for Man and his World:
Construction begins.
Construction started on August 13, 1963, with an elaborate ceremony hosted by Mayor Drapeau on barges anchored in the St. Lawrence River. Ceremonially, construction began when Prime Minister Lester B. Pearson pulled a lever that signalled a front-end loader to dump the first batch of fill to enlarge "Saint Helen's Island", and Quebec premier Jean Lesage spread the fill with a bulldozer. The 25 million tons of fill needed to construct the islands was coming from the Montreal Metro's excavations, a public works project that was already under construction before Expo was awarded to Montreal. Expo's initial construction period mainly centred on enlarging Saint Helen's Island, creating the artificial island of Notre Dame Island and lengthening and enlarging the Mackay Pier which became the Cité du Havre. While construction continued, the land rising out of Montreal harbour was not owned by the Expo Corporation yet. After the final mounds of earth completed the islands, the grounds that would hold the fair were officially transferred from the City of Montreal to the corporation on June 20, 1964. This gave Colonel Churchill only 1042 days to have everything built and functioning for opening day. To get Expo built in time, Churchill used the then new project management tool known as the critical path method (CPM). On April 28, 1967, opening day, everything was ready, with one exception: Habitat 67, which was then displayed as a work in progress.
Building and enlarging the islands, along with the new Concorde Bridge built to connect them with the site-specific mass transit system known as the Montreal Expo Express, plus a boat pier, cost more than the Saint Lawrence Seaway project did only five years earlier: this was even before any buildings or infrastructure were constructed. With the initial phase of construction completed, it is easy to see why the budget for the exhibition was going to be larger than anyone expected. In the fall of 1963, Expo's general manager, Andrew Kniewasser, presented the master plan and the preliminary budget of $167 million for construction: it would balloon to over $439 million by 1967. The plan and budget narrowly passed a vote in Pearson's federal cabinet, passing by one vote, and then it was officially submitted on December 23, 1963.
Logo.
The logo was designed by Montreal artist Julien Hébert. The basic unit of the logo is an ancient symbol of man. Two of the symbols (pictograms of "man") are linked as to represent friendship. The icon was repeated in a circular arrangement to represent "friendship around the world". The logotype is lower-case Optima font. It did not enjoy unanimous support from federal politicians, as some of them tried to kill it with a motion in the Canadian House of Commons.
Theme songs.
The official Expo 67 theme song was composed by Stéphane Venne and was titled: "Hey Friend, Say Friend/Un Jour, Un Jour".<ref name="Un Jour/ Hey Friend">
</ref> Complaints were made about the suitability of the song, as its lyrics mentioned neither Montreal nor Expo 67. The song was selected from an international competition with over 2,200 entries from 35 countries.
However, the song that most Canadians associate with Expo was written by Bobby Gimby, a veteran commercial jingle writer who composed the popular Centennial tune "Ca-na-da". Gimby earned the name the "Pied Piper of Canada".
The theme song "Something to Sing About", used for the Canadian pavilion, was initially written for a 1963 television special. The Ontario pavilion also had its own theme song: "A Place to Stand, A Place to Grow", which has evolved to become an unofficial theme song for the province.
Expo opens.
Official opening ceremonies were held on Thursday afternoon, April 27, 1967. The ceremonies were an invitation-only event, held at Place des Nations. Canada's Governor General, Roland Michener, proclaimed the exhibition open after the Expo flame was ignited by Prime Minister Pearson. On hand were over 7,000 media and invited guests including 53 heads of state. Over 1,000 reporters covered the event, broadcast in NTSC Colour, live via satellite, to a worldwide audience of over 700 million viewers and listeners.
Expo 67 officially opened to the public on the morning of Friday, April 28, 1967, with a space age style countdown. A capacity crowd at Place d'Accueil participated in the atomic clock-controlled countdown that ended when the exhibition opened precisely at 9:30 a.m. EST. An estimated crowd of between 310,000 and 335,000 visitors showed up for opening day, as opposed to the expected crowd of 200,000. The first person through the Expo gates at "Place d'Accueil" was Al Carter, a 41-year-old jazz drummer from Chicago, who was recognized for his accomplishment by Expo 67's director of operations Philippe de Gaspé Beaubien. Beaubien presented Carter with a gold watch for his feat.
On opening day, there was considerable comment on the uniform of the hostesses from the UK Pavilion. The dresses had been designed to the then new miniskirt style, popularized a year earlier by Mary Quant.
In conjunction with the opening of Expo 67, the Canadian Post Office Department issued a 5¢ stamp commemorating the fair, designed by Harvey Thomas Prosser.
Entertainment, Ed Sullivan Show, and VIPs.
The World Festival of Art and Entertainment at Expo 67 featured art galleries, opera, ballet and theatre companies, orchestras, jazz groups, famous Canadian pop musicians and other cultural attractions. Many pavilions had music and performance stages, where visitors could find free concerts and shows. Most of the featured entertainment took place in the following venues: Place des Arts; Expo Theatre; Place des Nations; La Ronde and Automotive Stadium.
The La Ronde amusement park was always intended to be a lasting legacy of the fair. Most of its rides and booths were permanent. When the Expo fairgrounds closed nightly, at around 10:00 p.m., visitors could still visit La Ronde, which closed at 2:30 a.m.
In addition, "The Ed Sullivan Show" was broadcast live on May 7 and May 21 from Expo 67. Stars on the shows included America's The Supremes, Britain's Petula Clark and Australia's The Seekers.
Another attraction was the Canadian Armed Forces Tattoo 1967 at the Autostad in Montreal.
The fair was visited by many of the most notable people of the day, including Canada's monarch, Queen Elizabeth II, Lyndon Johnson, Princess Grace, Jacqueline Kennedy, Robert F. Kennedy, Ethiopia's emperor Haile Selassie, Charles de Gaulle, Bing Crosby, Harry Belafonte, Maurice Chevalier, Maharishi Mahesh Yogi and Marlene Dietrich. Musicians like Thelonious Monk, Grateful Dead, Tiny Tim, The Tokens and Jefferson Airplane entertained the crowds.
Problems.
Despite its successes, there were problems: Front de libération du Québec militants had initially threatened to disrupt the exhibition, but were inactive during this period. Vietnam war protesters picketed during the opening day, April 28. American President Lyndon B. Johnson's visit became a focus of war protesters. The Cuba pavilion attracted threats that it would be destroyed by anti-Castro forces that never materialized. In June, the Arab-Israeli conflict in the Middle East flared up again in the Six Day War, which resulted in Kuwait pulling out of the fair in protest to the way Western nations dealt with the war. The president of France, Charles De Gaulle, caused an international incident on July 24 when he addressed thousands at Montreal City Hall by yelling out the now famous words "Vive Montréal... Vive le Québec ...Vive le Québec Libre!" 
In September, the most serious problem turned out to be a 30-day transit strike. By the end of July, estimates predicted that Expo would exceed 60 million visitors, but the strike cut deeply into attendance and revenue figures, just as the fair was cruising to its conclusion. Another major problem, beyond the control of Expo's management, was guest accommodation and lodging. Logexpo was created to direct visitors to accommodations in the Montreal area, which usually meant that visitors would stay at the homes of people they were unfamiliar with, rather than traditional hotels or motels. The Montreal populace opened their homes to thousands of guests. Unfortunately for some visitors, they were sometimes sent to less than respectable establishments where operators took full advantage of the tourist trade. Management of Logexpo was refused to Expo and was managed by a Quebec provincial authority. Still, Expo would get most of the blame for directing visitors to these establishments. But overall, a visit to Expo from outside Montreal was still seen as a bargain.
Expo ends.
Expo 67 closed on Sunday afternoon, October 29, 1967. On the final day 221,554 visitors added to the more than 50 million (50,306,648) that attended Expo 67 at a time when Canada's population was only 20 million, setting a per-capita record for World Exhibition attendance that still stands.
Starting at 2:00 p.m., Expo Commissioner General Pierre Dupuy officiated over the medal ceremony, in which participating nations and organizations received gold and silver medallions, as well as the ceremony in which national flags were lowered in the reverse order to which they had been raised, with Canada's flag lowered first and Nigeria's lowered last. After Prime Minister Pearson doused the Expo flame, Governor General Michener closed Expo at Place des Nations with the mournful spontaneous farewell: "It is with great regret that I declare that the Universal and International Exhibition of 1967 has come to an official end." All rides and the minirail were shut down by 3:50 p.m., and the Expo grounds closed at 4:00 p.m., with the last Expo Express train leaving for "Place d'Accueil" at that time. A fireworks display, that went on for an hour, was Expo's concluding event.
Expo performed better financially than expected. Expo was intended to have a deficit, shared between the federal, provincial and municipal levels of government. Significantly better-than-expected attendance revenue reduced the debt to well below the original estimates. The final financial statistics, in 1967 Canadian dollars, were: revenues of $221,239,872, costs of $431,904,683, and a deficit of $210,664,811.
Pavilions.
Expo 67 featured 90 pavilions representing Man and His World themes, nations, corporations, and industries including the U.S. pavilion, a geodesic dome designed by Buckminster Fuller. Expo 67 also featured the Habitat 67 modular housing complex designed by architect Moshe Safdie, which was later purchased by private individuals and is still occupied.
The most popular pavilion was the Soviet Union's exhibit. It attracted about 13 million visitors. Rounding out the top five pavilions, in terms of attendance were: the Canadian Pavilion (11 million visitors), the United States (9 million), France (8.5 million), and Czechoslovakia (8 million).
The participating countries were
Countries conspicuously absent were Spain, South Africa, the People's Republic of China, and many South American countries.
Legacy.
After 1967, the exposition struggled for several summer seasons as a standing collection of international pavilions known as "Man and His World". However, as attendance declined, the physical condition of the site deteriorated, and less and less of it was open to the public. After the 1971 season, the entire Notre Dame Island site closed and three years later completely rebuilt around the new rowing and canoe sprint (then flatwater canoeing) basin for Montreal's 1976 Summer Olympics. Space for the basin, the boathouses, the changing rooms and other buildings was obtained by demolishing many of the former pavilions and cutting in half the area taken by the artificial lake and the canals. In 1976, a fire destroyed the acrylic outer skin of Buckminster Fuller's dome, and the previous year the Ontario pavilion was gutted by a major fire. With the site falling into disrepair, it began to resemble ruins of a futuristic city. Minor thematic exhibitions were held at the Atlantic pavilion and Quebec pavilion, until the Montreal Casino was built. In 1980 the Notre Dame Island site partially reopened, primarily for the Floralies, albeit in a much smaller section. After the 1981 season, the Saint Helen's Island site permanently closed, shutting out the majority of attractions. Man and His World was able to still continue in a limited fashion at the small re-opened section on Notre Dame Island. Not to last however, the few remaining original exhibits closed for good in 1984.
After the "Man and his World" summer exhibitions were discontinued, with most pavilions and remnants demolished between 1985 to 1986, the former site for Expo 67 on Saint Helen's Island and Notre Dame Island was incorporated into a municipal park run by the city of Montreal. In 2000, the park was renamed from Parc des Îles to Parc Jean-Drapeau, after Mayor Jean Drapeau, who brought the exhibition to Montreal. In 2006, the corporation that runs the park also changed its name from the "Société du parc des Îles" to the "Société du parc Jean-Drapeau". Two prominent buildings remain in use on the former Expo grounds: the American pavilion's metal-lattice skeleton from its Buckminster Fuller dome, now enclosing an environmental sciences museum called the Montreal Biosphère; and Habitat 67, now a condominium residence. Also, the French and Quebec pavilions now form the Montreal Casino. La Toundra Hall is part of the surviving structural remains of the Canadian pavilion. It is now a restaurant and special events hall. Another part of the pavilion now serves as the administration building of Parc Jean-Drapeau. Katimavik's distinctive inverted pyramid and much of the rest of the Canadian pavilion were dismantled during the 1970s. Place des Nations, where the opening and closing ceremonies were held, the Jamaican and Tunisian pavilions and the partial remains of the Korean pavilion (roof only) also survive. Other remaining structures include sculptures, lampposts and landscaping. The Montreal Metro subway still has at least one "Man and His World" logo on a station's wall. La Ronde survives, and since 2001 it was leased to the New York amusement park company Six Flags.
The Alcan Aquarium built for the Expo remained in operation for a number of decades until its closure in 1991. The Expo 67 parking lot was converted into Victoria STOLport, an experimental short-take off airport in the 1970s.
Another attraction on today's Notre Dame Island site is the Circuit Gilles Villeneuve race track that is used for the Canadian Grand Prix. The Olympic basin is used today by many local rowing clubs. A beach was built on the shores of the remaining artificial lake. There are many acres of parkland and cycle paths on both Saint Helen's Island and the western tip of Notre Dame Island. In previous years the site has been used for a number of events such as a BIE sponsored international botanical festival, "Les floralies". The young trees and shrubs planted for Expo 67 are now mature. The plants introduced during the botanical events have flourished also.
In a political and cultural context, Expo 67 was seen as a landmark moment in Canadian history. In 1968, as a salute to the cultural impact the exhibition had on the city, Montreal's new Major League baseball team, the Expos (now the Washington Nationals), was named after the event. 1967 was also the year that invited Expo guest Charles De Gaulle, on July 24, addressed thousands at Montreal City Hall by yelling out the now famous words: "Vive Montréal... Vive le Québec... Vive le Québec Libre!" (See Vive le Québec libre speech). De Gaulle was rebutted in Ottawa by Prime Minister Lester B. Pearson: "Canadians do not need to be liberated, Canada will remain united and will reject any effort to destroy her unity". In the years that followed, the tensions between the English and French communities would continue. As an early 21st-century homage to the fair, satirists Bowser and Blue wrote a full-length musical set at Expo 67 called "The Paris of America" which ran for six sold-out weeks at Centaur Theatre in Montreal in April and May 2003.
Expo 67 was one of the most successful World Exhibitions and is still regarded fondly by Canadians. 1967 is often referred to as "the last good year" before economic decline, Quebec Sovereignism (seen as negative from a federalist viewpoint), and political apathy became common. In this way, it has much in common with the 1964-65 New York World's Fair. In 2007, a new group, Expo 17, was looking to bring a smaller-scale — BIE sanctioned — exposition to Montreal for Expo 67's 50th anniversary and Canada's Sesquicentennial in 2017. Expo 17 hoped a new world's fair would regenerate the spirit of Canada's landmark centennial project.

</doc>
<doc id="50589" url="http://en.wikipedia.org/wiki?curid=50589" title="Spanking">
Spanking

Spanking is a form of corporal punishment consisting of striking the buttocks of another person to cause temporary pain. It generally involves one person striking the buttocks of another person with an open hand. When an open hand is used, spanking is referred to in some countries as slapping or smacking. More severe forms of spanking, such as switching, paddling, belting, caning, whipping, and birching, involve the use of an implement instead of a hand. Corporal punishment is most commonly used to discipline a child or teenager. It generally involves an adult – typically a parent, guardian, or teacher – striking the child's buttocks as punishment for unacceptable behavior. Historically, boys have tended to be more frequently spanked than girls. Some countries have outlawed the spanking of children in every setting, but many allow it at least when administered by a parent or guardian. For the legal status of corporal punishment in different countries, see corporal punishment in the home and school corporal punishment.
In some cultures, the spanking of a wife by her husband is considered an acceptable form of domestic discipline, though the practice is far less common than it used to be. In other contexts, the spanking of an adult can be considered a playful gesture during a social ritual or as a form of entertainment.
In the home.
Law and public opinion.
In many cultures, parents have historically been regarded as having the duty of disciplining their children, and the right to spank them when appropriate; however, attitudes in many countries changed in the 1950s and 60s following the publication by pediatrician Dr. Spock of "Baby and Child Care" in 1946, which advised parents to treat children as individuals, whereas the previous conventional wisdom had been that child rearing should focus on building discipline, and that, e.g., babies should not be "spoiled" by picking them up when they cried. The change in attitude was followed by legislation. Sweden was the first to abolish corporal punishment of children in the family in 1979. As of January 2015, a total of forty‑six countries, including 28 in Europe, had outlawed corporal punishment of children in all contexts, including in the home. In many other places the practice is considered controversial.
Numerous human rights organizations have decried any use of corporal punishment on children, asserting that corporal punishment is a violation of children's human rights. 
In many countries in Africa and the Middle East, and in most parts of Eastern Asia (including China, Taiwan, Japan, and Korea), corporal punishment of one's own children is lawful. In Singapore and Hong Kong, punishing one's own child with corporal punishment is legal but not particularly encouraged. Culturally, many people in the region believe a certain amount of corporal punishment for their own children is appropriate and necessary, and thus such practice is accepted by society as a whole.
Lay opinions are divided on whether spanking is helpful or harmful to a child's behavior. Public attitudes towards the acceptability and effectiveness of spanking vary a great deal by nation and region. For example in the United States and United Kingdom, social acceptance of spanking children maintains a majority position, from approximately 61% to 80%. In Sweden, before the 1979 ban, more than half of the population considered corporal punishment a necessary part of child rearing. By 1996 the rate was 11%, and less than 34% considered it acceptable in a national survey.
On the other hand, many professional and child welfare organizations oppose it. The American Academy of Pediatrics has disavowed the practice of spanking, citing ineffectiveness, the chance of injury, and the likelihood that physical punishment will escalate into physical abuse.
Research.
In one 2006 study, children whose parents spanked them commonly reported feelings of fear, anger, and sadness as a result.  Young children aged between five and seven in a UK study said of being spanked by their parents, "it feels like someone banged you with a hammer" and "it hurts and it’s painful inside – it’s like breaking your bones". 
Dr. Elizabeth T. Gershoff, a "leading researcher" on spanking according to the American Psychological Association as well as CNN, found in 2013 that spanking children did not achieve parents' aims of either short‑term or long‑term compliance, based on numerous prior studies. She calls spanking a form of "violence" that should be stopped.  According to Dr. Gershoff, the belief that spanking increased immediate compliance was "overly influenced by one study". Dr. Joan Durrant and Ron Ensom with the University of Manitoba and Children's Hospital of Eastern Ontario, respectively, reached a similar conclusion in a systematic review of two decades of spanking research, finding that spanking increased children's aggression over time and was not effective in promoting desired behaviors.
A longitudinal study by Tulane University in 2010 found a 50% greater risk of aggressive behavior two years later in young children who were spanked more than twice in the month before the study began. The study controlled for a wide variety of confounding variables, including initial levels of aggression in the children. According to the study's leader, Catherine Taylor, this suggests that "it's not just that children who are more aggressive are more likely to be spanked."
A 2008 study at the University of North Carolina at Chapel Hill showed that mothers who spanked their children were also more likely to abuse them by "beating, burning, kicking, hitting with an object somewhere other than the buttocks, or shaking a child less than 2 years old", according to the researchers. In Dr. Gershoff's words, "The link between spanking and physical abuse is the most disturbing of these unintended effects, but it should not be a surprising one; both parental acts involve hitting, and purposefully hurting, children. ... [M]ost documented cases of physical abuse begin with parents physically punishing their children for a perceived misdeed". The study authors believed that media, educational, and legislative efforts to reduce spanking may reduce the incidence of physical child abuse.
A small minority of scientists have claimed that "non‑abusive" spanking is not harmful.  Dr. Durrant, on the other hand, argues that "over 100" studies have shown that spanking can be harmful to children. She asserts that spanking inhibits children's cognitive development and predicts various mental illnesses in adulthood. Dr. Durrant maintains that no study has demonstrated any long‑term benefit to spanking. Dr. Alan Kazdin, psychology professor at Yale University and former president of the American Psychological Association, echoed Dr. Durrant's assertion in 2012, saying, "There is no need for corporal punishment based on the research. We are not giving up an effective technique. We are saying this is a horrible thing that does not work".
The authors of a 2009 study found reduced gray matter in areas of the brain related to self‑control, depression, and addiction in young adults who had been regularly spanked as children. Murray Straus, regarded as the "foremost researcher" on child corporal punishment according to "Science Daily", has also reported damage to cognitive development and subsequent lower academic performance in children who were spanked.
Straus also published a study in 2013 which found that children across numerous cultures who were spanked committed more crimes as adults than children who were not spanked. He noted, "So many parents and child psychologists believe that if spanking is done by loving and helpful parents, it has no harmful effect...This study and only one other study I know of that empirically investigated this belief found that it is not true. Spanking seems to be associated with an increased probability of subsequent child behavior problems regardless of culture and, regardless of whether it [is] done by loving and helpful parents".
A few researchers have been critical of the methodology used in many of the studies on spanking, as well as their authors' conclusions. But even these scientists contend that spanking beyond a specific set of criteria (children age 2–6, no objects, in private, less than once per week) is still harmful. A 2013 meta‑analysis by Dr. Chris Ferguson employed an alternative statistical analysis that still showed negative outcomes in children subjected to spanking and corporal punishment, but found the overall relationship to be "trivial" or nearly so. However, Ferguson acknowledged this still indicates harmful outcomes and noted some limitations of his analysis, stating "On the other hand, there was no evidence from the current meta-analysis to indicate that spanking or CP held any particular advantages. There appears, from the current data, to be no reason to believe that spanking/CP holds any benefits related to the current outcomes, in comparison to other forms of discipline."
There is an ongoing debate on whether or not the sexual deviation "spanking fetishism" is caused by spankings received or witnessed in childhood (or puberty age). A study by Murray Straus found a positive correlation with childhood spanking and adult interest in masochistic sexual practices, but also found that up to 40% of adults with such interests had no history of childhood spanking. This suggests that while spanking may contribute, there are other significant variables involved.
In schools.
Corporal punishment, usually delivered with an implement (such as a paddle or cane) rather than with the open hand, used to be a common form of school discipline in many countries, but it is now banned in most of the western world. These bans have been controversial, and in many cultures opinion remains sharply divided as to the efficacy or suitability of spanking as a punishment for misbehaviour by school students.
Formal caning, notably for teenage boys, remains a common form of discipline in schools in several Asian and African countries, especially those with a British heritage; in these cultures it is referred to as "caning" and not "spanking".
In the United States, the Supreme Court in 1977 held that the paddling of school students was not "per se" unlawful. However, 31 states have now banned paddling in public schools. It is still common in some schools in the South.
Adult spanking.
In some cultures, the spanking of women, by the male head of the family or by the husband (sometimes called domestic discipline) has been – and sometimes continues to be – a common and approved custom. In those cultures and in those times it was the belief that the husband, as head of the family, had a right and even the duty to discipline his wife and children when he saw fit, and manuals were available to instruct the husband how to discipline his household. In most western countries, this practice has come to be regarded as socially unacceptable wife-beating, domestic violence or abuse. Routine corporal punishment of women by their husbands, however, does still exist in some parts of the developing world, and still occurs in isolated cases in western countries.
Today, spanking of an adult tends to be confined to erotic spanking between people engaging in other intimate activities, such as foreplay or sexual roleplay.
In popular culture.
Adult spanking, or the threat of being spanked, has appeared in numerous films and TV series. In most cases, it is a man spanking or threatening to spank a woman. Some examples include:
Terminology.
In North America, the word "spanking" has often been used as a synonym for an official paddling in school, and sometimes even as a euphemism for the formal corporal punishment of adults in an institution.
In British English, most dictionaries define "spanking" as being given only with the open hand.
In American English, dictionaries define spanking as being administered with either the open hand or an implement such as a paddle. Thus, the standard form of corporal punishment in US schools (use of a paddle) is often referred to as a spanking, whereas its pre-1997 English equivalent (strokes of the cane) would never have been so described.
The word "licks" is also a common term in West Indian countries, especially Trinidad & Tobago. It usually refers to any sort of spanking or beating. Licks can involve "switches" or small tree branches, pieces of cocoyea, or any object nearby. These can also include belts, spoons, brooms, and even rolling pins.
In Britain, Ireland, Australia and New Zealand, the word "smacking" is generally used in preference to "spanking" when describing striking with an open hand, rather than with an implement. Whereas a spanking is invariably administered to the bottom, "smacking" is less specific and may refer to slapping the child's hands, arms or legs as well as its bottom.
Ritual spanking traditions.
There are some rituals or traditions which involve spanking. For example, on the first day of the lunar Chinese new year holidays, a week-long 'Spring Festival', the most important festival for Chinese people all over the world, thousands of Chinese visit the Taoist "Dong Lung Gong" temple in Tungkang to go through the century-old ritual to get rid of bad luck, men by receiving spankings and women by being whipped, with the number of strokes to be administered (always lightly) by the temple staff being decided in either case by the god Wang Ye and by burning incense and tossing two pieces of wood, after which all go home happily, believing their luck will improve.
On Easter Monday, there is a Slavic tradition of hitting girls and young ladies with woven willow switches (Czech: "pomlázka"; Slovak: "korbáč") and dousing them with water.
In Slovenia, there is a jocular tradition that anyone who succeeds in climbing to the top of Mount Triglav receives a spanking or birching.
According to Ovid's Fasti (ii.305), during the ancient Roman festival of the Lupercalia naked men ran through the streets of the city, carrying straps with which they swatted the outstretched palms of the hands of women lining the racecourse who wished to become pregnant.
In North America, there is a tradition of "birthday spankings" where the birthday girl or boy receives the same number of hits as her/his age (plus "one to grow on") during the birthday party. Birthday spankings are administered over the clothes and usually by close friends or family members, and are generally playful swats not meant to cause real pain.
References.
Notes

</doc>
<doc id="50591" url="http://en.wikipedia.org/wiki?curid=50591" title="United States Postal Service">
United States Postal Service

The United States Postal Service (originally called the U.S. Post Office Department, when it was completely managed by the U.S. government before 1971) also known as the Post Office, U.S. Mail, or Postal Service, often abbreviated as USPS, is an independent agency of the United States federal government responsible for providing postal service in the United States. It is one of the few government agencies explicitly authorized by the United States Constitution. The USPS traces its roots to 1775 during the Second Continental Congress, where Benjamin Franklin was appointed the first postmaster general. The cabinet-level Post Office Department was created in 1792 from Franklin's operation and transformed into its current form in 1971 under the Postal Reorganization Act.
The USPS employed 617,254 workers (as of February 2015) and operated 211,264 vehicles in 2014. The USPS is the operator of the largest civilian vehicle fleet in the world. The USPS is legally obligated to serve all Americans, regardless of geography, at uniform price and quality. The USPS has exclusive access to letter boxes marked "U.S. Mail" and personal letterboxes in the United States, but still competes against private package delivery services, such as UPS and has part use with FedEx Express.
The USPS has not directly received taxpayer-dollars since the early 1980s with the exception of subsidies for costs associated with the disabled and overseas voters. Since the 2006 all-time peak mail volume, after which Congress passed the Postal Accountability and Enhancement Act, (which mandated $5.5 billion per year to be paid into an account to fully prefund both employee retirement health and pension benefits, a requirement exceeding that of other government and private organizations ), revenue dropped sharply due to recession-influenced declining mail volume, prompting the postal service to look to other sources of revenue while cutting costs to reduce its budget deficit. The USPS lost US$5.5 billion in fiscal 2014, and its revenue was US$67.8 billion.
History.
Foundations.
In the early years of the North American colonies, many attempts were made to initiate a postal service. These early attempts were of small scale and usually involved a colony, Massachusetts Bay Colony for example, setting up a location in Boston where one could post a letter back home to England. Other attempts focused on a dedicated postal service between two of the larger colonies, such as Massachusetts and Virginia, but the available services remained limited in scope and disjointed for many years. For example, informal independently-run postal routes operated in Boston as early as 1639, with a Boston to New York City service starting in 1672.
A central postal organization came to the colonies in 1691, when Thomas Neale received a 21-year grant from the British Crown for a North American Postal Service. On February 17, 1691, a grant of "letters patent" from the joint sovereigns, William and Mary, empowered him:
"to erect, settle, and establish within the chief parts of their majesties' colonies and plantations in America, an office or offices for receiving and dispatching letters and pacquets, and to receive, send, and deliver the same under such rates and sums of money as the planters shall agree to give, and to hold and enjoy the same for the term of twenty-one years."
The patent included the exclusive right to establish and collect a formal postal tax on official documents of all kinds. The tax was repealed a year later. Neale appointed Andrew Hamilton, Governor of New Jersey, as his deputy postmaster. The first postal service in America commenced in February 1692. Rates of postage were fixed and authorized, and measures were taken to establish a post office in each town in Virginia. Massachusetts and the other colonies soon passed postal laws, and a very imperfect post office system was established. Neale's patent expired in 1710, when Parliament extended the English postal system to the colonies. The chief office was established in New York City, where letters were conveyed by regular packets across the Atlantic.
Before the Revolution, there was only a trickle of business or governmental correspondence between the colonies. Most of the mail went back and forth to counting houses and government offices in London. The Revolution made Philadelphia, the seat of the Continental Congress, the information hub of the new nation. News, new laws, political intelligence, and military orders circulated with a new urgency, and a postal system was necessary. Journalists took the lead, securing post office legislation that allowed them to reach their subscribers at very low cost, and to exchange news from newspapers between the thirteen states. Overthrowing the London-oriented imperial postal service in 1774-1775, printers enlisted merchants and the new political leadership, and created new postal system. The "United States Post Office" (USPO) was created on July 26, 1775, by decree of the Second Continental Congress. Benjamin Franklin headed it briefly.
Before the Revolution, individuals like Benjamin Franklin and William Goddard were the colonial postmasters who managed the mails then and were the general architects of a postal system that started out as an alternative to the Crown Post.
The official post office was created in 1792 as the Post Office Department (USPOD). It was based on the Constitutional authority empowering Congress "To establish post offices and post roads". The 1792 law provided for a greatly expanded postal network, and served editors by charging newspapers an extremely low rate. The law guaranteed the sanctity of personal correspondence, and provided the entire country with low-cost access to information on public affairs, while establishing a right to personal privacy.
Rufus Easton was appointed by Thomas Jefferson first postmaster of St. Louis under the recommendation of Postmaster General Gideon Granger. Rufus Easton was the first postmaster and built the first post office west of the Mississippi. At the same time Easton was appointed by Thomas Jefferson, judge of Louisiana Territory, the largest territory in North America. Bruce Adamson wrote that: "Next to Benjamin Franklin, Rufus Easton was one of the most colorful people in United States Postal History." It was Easton who educated Abraham Lincoln's Attorney General, Edward Bates. In 1815 Edward Bates moved into the Easton home and lived there for years at Third and Elm. Today this is the site of the Jefferson Memorial Park. In 1806 Postmaster General Gideon Granger wrote a three-page letter to Easton, begging him not to partake in a duel with vice-president Aaron Burr. Two years earlier it was Burr who had shot and killed Alexander Hamilton. Many years later in 1852, Easton's son, Major-General Langdon Cheves Easton, was commissioned by William T. Sherman, at Fort Union to delivery a letter to Independence, Missouri. Sherman wrote: “In the Spring of 1852, General Sherman mentioned that the quartermaster, Major L.C. Easton, at Fort Union, New Mexico, had occasion to send some message east by a certain date, and contracted with Aubrey to carry it to the nearest post office (then Independence, Missouri), making his compensation conditional on the time consumed. He was supplied with a good horse, and an order on the outgoing trains for exchange. Though the whole route was infested with hostile Indians, and not a house on it, Aubrey started alone with his rifle. He was fortunate in meeting several outward-bound trains, and thereby made frequent changes of horses, some four or five, and reached Independence in six days, having hardly rested or slept the whole way." 
To cover long distances, the Post Office used a hub-and-spoke system, with Washington as the hub and chief sorting center. By 1869, with 27,000 local post offices to deal with, it had changed to sorting mail en route in specialized railroad mail cars, called Railway Post Offices, or RPOs. The system of postal money orders began in 1864. Free mail delivery began in the larger cities in 1863.
19th century.
The postal system played a crucial role in national expansion. It facilitated expansion into the West by creating an inexpensive, fast, convenient communication system. Letters from early settlers provided information and boosterism to encourage increased migration to the West, helped scattered families stay in touch and provide assistance, assisted entrepreneurs in finding business opportunities, and made possible regular commercial relationships between merchants in the west and wholesalers and factories back east. The postal service likewise assisted the Army in expanding control over the vast western territories. The widespread circulation of important newspapers by mail, such as the "New York Weekly Tribune," facilitated coordination among politicians in different states. The postal service helped integrate established areas with the frontier, creating a spirit of nationalism and providing a necessary infrastructure.
The Post Office in the 19th century was a major source of federal patronage. Local postmasterships were rewards for local politicians—often the editors of party newspapers. About 3/4 of all federal civilian employees worked for the Post Office. In 1816 it employed 3341 men, and in 1841, 14,290. The volume of mail expanded much faster than the population, as it carried annually 100 letters and 200 newspapers per 1000 white population in 1790, and 2900 letters and 2700 newspapers per thousand in 1840.
The Post Office Department was enlarged during the tenure of President Andrew Jackson. As the Post Office expanded, difficulties were experienced due to a lack of employees and transportation. The Post Office's employees at that time were still subject to the so-called "spoils" system, where faithful political supporters of the executive branch were appointed to positions in the post office and other government corporations as a reward for their patronage. These appointees rarely had prior experience in postal service and mail delivery. This system of political patronage was replaced in 1883, after passage of the Pendleton Civil Service Reform Act.
Ten years before waterways were declared post roads in 1823, the Post Office used steamboats to carry mail between post towns where no roads existed. Once it became clear that the postal system in the United States needed to expand across the entire country, the use of the railroad to transport the mail was instituted in 1832, on one line in Pennsylvania. All railroads in the United States were designated as post routes, after passage of the Act of July 7, 1838. Mail service by railroad increased rapidly thereafter.
An Act of Congress provided for the issuance of stamps on March 3, 1847, and the Postmaster General immediately let a contract to the New York City engraving firm of Rawdon, Wright, Hatch, and Edson. The first stamp issue of the U.S. was offered for sale on July 1, 1847, in New York City, with Boston receiving stamps the following day and other cities thereafter. The 5-cent stamp paid for a letter weighing less than 1 oz and traveling less than 300 miles, the 10-cent stamp for deliveries to locations greater than 300 miles, or twice the weight deliverable for the 5-cent stamp.
In 1847, the U.S. Mail Steamship Company acquired the contract which allowed it to carry the U.S. mails from New York, with stops in New Orleans and Havana, to the Isthmus of Panama for delivery in California. The same year, the Pacific Mail Steamship Company had acquired the right to transport mail under contract from the United States Government from the Isthmus of Panama to California. In 1855, William Henry Aspinwall completed the Panama Railway, providing rail service across the Isthmus and cutting to three weeks the transport time for the mails, passengers and goods to California. This remained an important route until the completion of the transcontinental railroad in 1869. Railroad companies greatly expanded mail transport service after 1862, and the Railway Mail Service was inaugurated in 1869.
Rail cars designed to sort and distribute mail while rolling were soon introduced. RMS employees sorted mail "on-the-fly" during the journey, and became some of the most skilled workers in the postal service. An RMS sorter had to be able to separate the mail quickly into compartments based on its final destination, before the first destination arrived, and work at the rate of 600 pieces of mail an hour. They were tested regularly for speed and accuracy.
Parcel Post service began with the introduction of International Parcel Post between the USA and foreign countries in 1887. That same year, the U.S. Post Office (predecessor of the USPS) and the Postmaster General of Canada established parcel-post service between the two nations. A bilateral parcel-post treaty between the independent (at the time) Kingdom of Hawaii and the USA was signed on 19 December 1888 and put into effect early in 1889. Parcel-post service between the USA and other countries grew with the signing of successive postal conventions and treaties. While the Post Office agreed to deliver parcels sent into the country under the UPU treaty, it did not institute a domestic parcel-post service for another twenty-five years.
20th century.
The advent of Rural Free Delivery (RFD) in the U.S. in 1896, and the inauguration of a domestic parcel post service by Postmaster General Frank H. Hitchcock in 1913, greatly increased the volume of mail shipped nationwide, and motivated the development of more efficient postal transportation systems. Many rural customers took advantage of inexpensive Parcel Post rates to order goods and products from businesses located hundreds of miles away in distant cities for delivery by mail. From the 1910s to the 1960s, many college students and others used parcel post to mail home dirty laundry, as doing so was less expensive than washing the clothes themselves.
After four-year-old Charlotte May Pierstorff was mailed from her parents to her grandparents in Idaho in 1914, mailing of people was prohibited. In 1917, the Post Office imposed a maximum daily mailable limit of two hundred pounds per customer per day after a business entrepreneur, W.H. Coltharp, used inexpensive parcel-post rates to ship more than eighty thousand masonry bricks some four hundred seven miles via horse-drawn wagon and train for the construction of a bank building in Vernal, Utah.
The advent of parcel post also led to the growth of Mail order businesses that substantially increased rural access to modern goods over what was typically stocked in local general stores.
In 1912, carrier service was announced for establishment in towns of second and third class with $100,000 appropriated by Congress. From January 1, 1911, until July 1, 1967, the United States Post Office Department operated the United States Postal Savings System. An Act of Congress of June 25, 1910, established the Postal Savings System in designated Post Offices, effective January 1, 1911. The legislation aimed to get money out of hiding, attract the savings of immigrants accustomed to the postal savings system in their native countries, provide safe depositories for people who had lost confidence in banks, and furnish more convenient depositories for working people. The law establishing the system directed the Post Office Department to redeposit most of the money in the system in local banks, where it earned 2.5 percent interest. 
The system paid 2-percent interest per year on deposits. The half percent difference in interest was intended to pay for the operation of the system. Certificates were issued to depositors as proof of their deposit. Depositors in the system were initially limited to hold a balance of $500, but this was raised to $1,000 in 1916 and to $2,500 in 1918. The initial minimum deposit was $1. In order to save smaller amounts for deposit, customers could purchase a 10-cent postal savings card and 10-cent postal savings stamps to fill it. The card could be used to open or add to an account when its value, together with any attached stamps, amounted to one or more dollars, or it could be redeemed for cash. At its peak in 1947, the system held almost $3.4 billion in deposits, with more than four million depositors using 8,141 postal units.
On August 12, 1918, the Post Office Department took over airmail service from the United States Army Air Service (USAAS). Assistant Postmaster General, Otto Praeger, appointed Benjamin B. Lipsner to head the civilian-operated Air Mail Service. One of Lipsner's first acts was to hire four pilots, each with at least 1,000 hours flying experience, paying them an average of $4,000 per year ($ today). The Post Office Department used mostly World War I military surplus de Havilland DH-4 aircraft.
During 1918, the Post Office hired an additional 36 pilots. In its first year of operation, the Post Office completed 1,208 airmail flights with 90 forced landings. Of those, 53 were due to weather and 37 to engine failure. By 1920, the Air Mail service had delivered 49 million letters. Domestic air mail became obsolete in 1975, and international air mail in 1995, when the USPS began transporting First-Class mail by air on a routine basis.
The Post Office was one of the first government departments to regulate obscene materials on a national basis. When the U.S. Congress passed the Comstock laws of 1873, it became illegal to send through the U.S. mail any material considered obscene or indecent, or which promoted abortion issues, birth control, or alcohol consumption.
On March 18, 1970, postal workers in New York City — upset over low wages and poor working conditions, and emboldened by the Civil Rights movement — organized a strike against the United States government. The strike initially involved postal workers in only New York City, but it eventually gained support of over 210,000 United States Post Office Department workers across the nation. While the strike ended without any concessions from the Federal government, it did ultimately allow for postal worker unions and the government to negotiate a contract which gave the unions most of what they wanted, as well as the signing of the Postal Reorganization Act by President Richard Nixon on August 12, 1970. The Act replaced the cabinet-level Post Office Department with the independent United States Postal Service, and took effect on July 1, 1971.
Current operations.
The United States Postal Service employs some 617,000 workers, making it the third-largest civilian employer in the United States behind the federal government and Wal-Mart. In a 2006 U.S. Supreme Court decision, the Court noted: "Each day, according to the Government's submissions here, the United States Postal Service delivers some 660 million pieces of mail to as many as 142 million delivery points." As of 2014, the USPS operates 31,000 post offices and locations in the U.S., and delivers 155 billion pieces of mail annually.
The USPS operates the largest civilian vehicle fleet in the world, with an estimated 211,264 vehicles, the majority of which are the easily identified Chevrolet/Grumman LLV (Long-Life Vehicle), and the newer Ford/Utilimaster FFV (Flex-Fuel Vehicle), originally also referred to as the "CRV" (Carrier Route Vehicle). It is by geography and volume the globe's largest postal system, delivering 40% of the world's mail. For every penny increase in the national average price of gasoline, the USPS spends an extra $8 million per year to fuel its fleet.
The number of gallons of fuel used in 2009 was 444 million, at a cost of US$ billion. The fleet is notable in that many of its vehicles are right-hand drive, an arrangement intended to give drivers the easiest access to roadside mailboxes. Some Rural Letter Carriers use personal vehicles. Standard postal-owned vehicles do not have license plates. These vehicles are identified by a seven digit number displayed on the front and rear.
The Department of Defense and the USPS jointly operate a postal system to deliver mail for the military; this is known as the Army Post Office (for Army and Air Force postal facilities) and the Fleet Post Office (for Navy, Marine Corps and Coast Guard postal facilities).
In February 2013, the Postal Service announced that on Saturdays it would only deliver packages, mail-order medicines, Priority Mail, and Express Mail, effective August 10, 2013. However, this change was reversed by federal law in the Consolidated and Further Continuing Appropriations Act, 2013. They now deliver packages on Sunday for Amazon.com only.
Five-year plans.
In October 2008, the Postal Service released "Vision 2013", a five-year plan required by law starting in 1993. One planned improvement is the introduction of the Intelligent Mail Barcode, which will allow pieces of mail to be tracked through the delivery system, as competitors like UPS and FedEx currently do.
Initiatives.
In 2011, numerous media outlets reported that the USPS was going out of business. The USPS's strategy came under fire as new technologies emerged and the USPS was not finding ways to generate new sources of revenue.
Budget.
In 2014, the Postal Service collected $67.8 billion in revenue.
Revenue decline and planned cuts.
In 2012, the USPS had its third straight year of operational losses, which amounted to $4.8 billion.
Declining mail volume.
First Class mail volume peaked in 2001 and has declined 29% from 1998 to 2008, due to the increasing use of email and the World Wide Web for correspondence and business transactions.
FedEx and United Parcel Service (UPS) directly compete with USPS Express Mail and package delivery services, making nationwide deliveries of urgent letters and packages.
Lower volume means lower revenues to support the fixed commitment to deliver to every address once a day, six days a week. According to an official report on November 15, 2012, the U.S. Postal Service lost $15.9 billion its 2012 fiscal year.
Internal streamlining and delivery slowdown.
In response, the USPS has increased productivity each year from 2000 to 2007, through increased automation, route re-optimization, and facility consolidation. Despite these efforts, the organization saw an $8.5 billion budget shortfall in 2010, and was losing money at a rate of about $3 billion per quarter in 2011.
On December 5, 2011 the USPS announced it would close more than half of its mail processing centers, eliminate 28,000 jobs and reduce overnight delivery of First-Class Mail. This will close down 252 of its 461 processing centers. (At peak mail volume in 2006, the USPS operated 673 facilities.) As of May 2012, the plan was to start the first round of consolidation in summer 2012, pause from September to December, and begin a second round in February 2014; 80% of first class mail would still be delivered overnight through the end of 2013. New delivery standards were issued in January 2015, and the majority of single-piece (not presorted) first-class mail is now being delivered in two days instead of one. Large commercial mailers can still have first-class mail delivered overnight if delivered directly to a processing center in the early morning, though as of 2014 this represented only 11% of first-class mail. Unsorted first-class mail will continued to be delivered anywhere in the contiguous United States within three days.
Post office closures.
In July 2011, the USPS announced a plan to close about 3,700 small post offices. Various representatives in Congress protested, and the Senate passed a bill that would have kept open all post offices further than 10 miles from the next office. In May 2012, the service announced it had modified its plan. Instead, rural post offices would remain open with reduced retail hours (some as little as two hours per day) unless there was a community preference for a different option. In a survey of rural customers, 20% preferred the "Village Post Office" replacement (where a nearby private retail store would provide basic mail services with expanded hours), 15% preferred merger with another Post Office, and 11% preferred expanded rural delivery services. Approximately 40% of postal revenue already comes from online purchases or private retail partners including Walmart, Staples, Office Depot, Walgreens, Sam's Club, Costco, and grocery stores. The American Postal Workers Union has argued that these counters should be manned by postal employees who earn far more and have "a generous package of health and retirement benefits".
Elimination of Saturday delivery averted.
On January 28, 2009, Postmaster General John E. Potter testified before the Senate that, if the Postal Service could not readjust its payment toward the contractually funding earned employee retiree health benefits, as mandated by the Postal Accountability & Enhancement Act of 2006, the USPS would be forced to consider cutting delivery to five days per week during June, July, and August.
H.R. 22, addressing this issue, passed the House of Representatives and Senate and was signed into law on September 30, 2009. However, Postmaster General Potter continued to advance plans to eliminate Saturday mail delivery.
On June 10, 2009, the National Rural Letter Carriers' Association (NRLCA) was contacted for its input on the USPS's current study of the impact of five-day delivery along with developing an implementation plan for a five-day service plan. A team of Postal Service headquarters executives and staff has been given a time frame of sixty days to complete the study. The current concept examines the impact of five-day delivery with no business or collections on Saturday, with Post Offices with current Saturday hours remaining open.
On Thursday, April 15, 2010, the House Committee on Oversight and Government Reform held a hearing to examine the status of the Postal Service and recent reports on short and long term strategies for the financial viability and stability of the USPS entitled "Continuing to Deliver: An Examination of the Postal Service's Current Financial Crisis and its Future Viability." At which, PMG Potter testified that by the year 2020, the USPS cumulative losses could exceed $238 billion, and that mail volume could drop 15 percent from 2009.
In February 2013, the USPS announced that in order to save about $2 billion per year, Saturday delivery service would be discontinued except for packages, mail-order medicines, Priority Mail, Express Mail, and mail delivered to Post Office boxes, beginning August 10, 2013. However the Consolidated and Further Continuing Appropriations Act, 2013, passed in March, reversed the cuts to Saturday delivery.
Retirement funding and payment defaults.
The Postal Accountability and Enhancement Act of 2006 (PAEA) obligates the USPS to fund the present value of earned retirement obligations (essentially past promises which have not yet come due) within a ten-year time span. (In contrast, private businesses in the United States have no legal obligation to pay for retirement costs at promise-time rather than retirement-time, but about one quarter do.)
The Office of Personnel Management (OPM) is the main bureaucratic organization responsible for the human resources aspect of many federal agencies and their employees. The PAEA created the Postal Service Retiree Health Benefit Fund (PSRHB) after Congress removed the Postal Service contribution to the Civil Service Retirement System (CSRS). Most other employees that contribute to the CSRS have 7% deducted from their wages.
On September 30, 2014, the USPS failed to make $5.7 billion payment on this debt, the fourth such defaulted payment.
Rate increases.
Congress has limited rate increases for First-Class Mail to the cost of inflation, unless approved by the Postal Regulatory Commission. A 3¢ surcharge above inflation increased the 1 oz rate to 49¢ in January, 2014, but this was approved by the Commission for two years only.
Reform packages, delivery changes, and alcohol delivery.
Comprehensive reform packages considered in the 113th Congress include S.1486 and H.R.2748. These include the efficiency measure, supported by Postmaster General Patrick Donahoe of ending door-to-door delivery of mail for some or most of the 35 million addresses that currently receive it, replacing that with either curbside boxes or nearby "cluster boxes". This would save $4.5 billion per year out of the $30 billion delivery budget; door-to-door city delivery costs annually on average $353 per stop, curbside $224, and cluster box $160 (and for rural delivery, $278, $176, and $126, respectively).
S.1486, also with the support of Postmaster Donahoe, would also allow the USPS to ship alcohol in compliance with state law, from manufacturers to recipients with ID to show they are over 21. This is projected to raise approximately $50 million per year. (Shipping alcoholic beverages is currently illegal under #redirect (f).)
In 2014, the Postal Service was requesting reforms to worker's compensation, moving from a pension to defined contribution retirement savings plan, and paying senior retiree health care costs out of Medicare funds, as is done for private-sector workers.
Governance and organization.
The Board of Governors of the United States Postal Service sets policy, procedure, and postal rates for services rendered, and has a similar role to a corporate board of directors. Of the eleven members of the Board, nine are appointed by the President and confirmed by the United States Senate (see #redirect ). The nine appointed members then select the United States Postmaster General, who serves as the board's tenth member, and who oversees the day-to-day activities of the service as Chief Executive Officer (see #redirect ). The ten-member board then nominates a Deputy Postmaster General, who acts as Chief Operating Officer, to the eleventh and last remaining open seat.
The independent Postal Regulatory Commission (formerly the Postal Rate Commission) is also controlled by appointees of the President confirmed by the Senate. It oversees postal rates and related concerns, having the authority to approve or reject USPS proposals.
The USPS is often mistaken for a government-owned corporation (e.g., Amtrak) because it operates much like a business, but as noted above, it is legally defined as an "independent establishment of the executive branch of the Government of the United States", (#redirect ) as it is controlled by Presidential appointees and the Postmaster General. As a quasi-governmental agency, it has many special privileges, including sovereign immunity, eminent domain powers, powers to negotiate postal treaties with foreign nations, and an exclusive legal right to deliver first-class and third-class mail. Indeed, in 2004, the U.S. Supreme Court ruled in a unanimous decision that the USPS was not a government-owned corporation, and therefore could not be sued under the Sherman Antitrust Act.
The U.S. Supreme Court has also upheld the USPS's statutory monopoly on access to letter boxes against a First Amendment freedom of speech challenge; it thus remains illegal in the U.S. for "anyone", other than the employees and agents of the USPS, to deliver mailpieces to letter boxes marked "U.S. Mail."
The Postal Service also has a Mailers' Technical Advisory Committee and local Postal Customer Councils, which are advisory and primarily involve business customers.
Universal service obligation and monopoly status.
Article I, section 8, Clause 7 of the United States Constitution grants Congress the power to establish post offices and post roads, which has been interpreted as a de facto Congressional monopoly over the delivery of first class residential mail - which has been defined as non-urgent residential letters (not packages). Accordingly, no other system for delivering first class residential mail – public or private – has been tolerated, absent Congress's consent.
The mission of the Postal Service is to provide the American public with trusted universal postal service at affordable prices. While not explicitly defined, the Postal Service's universal service obligation (USO) is broadly outlined in statute and includes multiple dimensions: geographic scope, range of products, access to services and facilities, delivery frequency, affordable and uniform pricing, service quality, and security of the mail. While other carriers may claim to voluntarily provide delivery on a broad basis, the Postal Service is the only carrier with a "legal obligation" to provide all the various aspects of universal service at affordable rates.
Proponents of universal service principles claim that since any obligation must be matched by the financial capability to meet that obligation, the postal monopoly was put in place as a funding mechanism for the USO, and it has been in place for over a hundred years. It consists of two parts: the Private Express Statutes (PES) and the mailbox access rule. The PES refers to the Postal Service's monopoly on the delivery of letters, and the mailbox rule refers to the Postal Service's exclusive access to customer mailboxes.
Proponents of universal service principles further claim that eliminating or reducing the PES or mailbox rule would have an impact on the ability of the Postal Service to provide affordable universal service. If, for example, the PES and the mailbox rule were to be eliminated, and the USO maintained, then either billions of dollars in tax revenues or some other source of funding would have to be found.
Some proponents of universal service principles suggest that private communications that are protected by the veil of government promote the exchange of free ideas and communications. This separates private communications from the ability of a private for-profit or non-profit organization to corrupt. Security for the individual is in this way protected by the United States Post Office, maintaining confidentiality and anonymity, as well as government employees being much less likely to be instructed by superiors to engage in nefarious spying. It is seen by some as a dangerous step to extract the universal service principle from the post office, as the untainted nature of private communications is preserved as assurance of the protection of individual freedom of privacy.
Critics of universal service principles include several professional economists advocating for the privatization of the mail delivery system, or at least a relaxation of the universal service model that currently exists. Rick Geddes argued in 2000:
However, as the recent notice of a termination of mail service to residents of the Frank Church—River of No Return Wilderness indicates, mail service has been contracted to private firms such as Arnold Aviation for many decades. KTVB-TV reported:
"We cannot go out every week and pick up our mail...it's impossible", said Heinz Sippel. "Everyone gets their mail. Why can't we?" said Sue Anderson. Getting mail delivered, once a week, by airplane is not a luxury, it's a necessity for those who live in Idaho's vast wilderness – those along the Salmon and Selway rivers. It's a service that's been provided to them for more than half a century – mostly by Ray Arnold of Arnold Aviation.
The decision was reversed; U.S. Postmaster General John Potter indicated that acceptable service to back country customers could not be achieved in any other fashion than continuing an air mail contract with Arnold Aviation to deliver the mail.
The Postal Act of 2006 required the Postal Regulatory Commission (PRC) to submit a report to the President and Congress on universal postal service and the postal monopoly in December 2008. The report must include any recommended changes. The Postal Service report supports the requirement that the PRC is to consult with and solicit written comments from the Postal Service. In addition, the Government Accountability Office is required to evaluate broader business model issues by 2011.
On October 15, 2008, the Postal Service submitted a report to the PRC on its position related to the Universal Service Obligation (USO). It said no changes to the USO and restriction on mailbox access were necessary at this time, but increased regulatory flexibility was required to ensure affordable universal service in the future. In 2013, the Postal Service announced that starting August 2013, Saturday delivery would be discontinued.
Obligations of the USO include uniform prices, quality of service, access to services, and six-day delivery to every part of the country. To assure financial support for these obligations, the postal monopoly provides the Postal Service the exclusive right to deliver letters and restricts mailbox access solely for mail. The report argued that eliminating or reducing either aspect of the monopoly "would have a devastating impact on the ability...to provide the affordable universal service that the country values so highly." Relaxing access to the mailbox would also pose security concerns, increase delivery costs, and hurt customer service, according to the Post Office. The report notes:
Most of these alternatives are not actually free in some communities. For example, in the Chicago metropolitan area and many other major metros one must get a background check from police and pay a daily fee for the right to solicit or post commercial messages on private property.
Regarding the monopoly on delivery of letters, the report notes that the monopoly is not complete, as there is an exception for letters where either "the amount paid for private carriage of the letter equals at least six times the current rate for the first ounce of a single-piece First-Class Mail letter (also known as the “base rate” or “base tariff”)" or "the letter weighs at least 12.5 ounces."
The Postal Service said that the USO should continue to be broadly defined and there should be no changes to the postal monopoly. Any changes would have far-reaching effects on customers and the trillion dollar mailing industry. "A more rigidly defined USO would ... ultimately harm the American public and businesses," according to the report, which cautions that any potential change must be studied carefully and the effects fully understood.
Competitors.
FedEx and United Parcel Service (UPS) directly compete with USPS Express Mail and package delivery services, making nationwide deliveries of urgent letters and packages. Due to the postal monopoly, they are not allowed to deliver non-urgent letters and may not directly ship to U.S. Mail boxes at residential and commercial destinations. However both companies have transit agreements with the USPS in which an item can be dropped off with either FedEx or UPS who will then provide shipment up to the destination post office serving the intended recipient where it will be transferred for delivery to the U.S. Mail destination, including Post Office Box destinations. These services also deliver packages which are larger and heavier than USPS will accept. DHL Express was the third major competitor until February 2009, when it ceased domestic delivery operations in the United States.
A variety of other move cargo around the country, but either have limited geographic scope for delivery points, or specialize in items too large to be mailed. Many of the thousands of courier companies focus on same-day delivery, for example, by bicycle messenger.
Although USPS and FedEx are direct competitors, USPS contracts with FedEx for air transport of Priority and Express Priority Mail.
Alternative transmission methods.
The Post Office Department owned and operated the first public telegraph lines in the United States, starting in 1844 from Washington to Baltimore, and eventually extending to New York, Boston, Buffalo, and Philadelphia. In 1847 the telegraph system was privatized, except for a period during World War I, when it was used to accelerate the delivery of letters arriving at night.
Between 1942 and 1945 "V-Mail" (for "Victory Mail") service was available for military mail. Letters were converted into microfilm and reprinted near the destination, to save room on transport vehicles for military cargo.
From 1982 to 1985 Electronic Computer Originated Mail, known as E-COM was accepted for bulk mailings. Text was transmitted electronically to one of 25 post offices nationwide. The Postal Service would print the mail and put it in special envelopes bearing a blue E-COM logo. Delivery was assured within 2 days.
To improve accuracy and be more efficient, the came up with the program to complement the zip code system. This uses bar codes for mail tracking to give consumers and postal staff more accurate mail tracking. Other initiatives include , which can be paid for online and printed at home by consumers. These labels are then attached to the parcel or letter and can be dropped into any mailbox as the postage is paid.
Law enforcement agencies.
Postal Inspection Service.
The United States Postal Inspection Service (USPIS) is one of the oldest law enforcement agencies in the U.S. Founded by Benjamin Franklin, its mission is to protect the Postal Service, its employees, and its customers from crime and protect the nation's mail system from criminal misuse.
Postal Inspectors enforce over 200 federal laws providing for the protection of mail in investigations of crimes that may adversely affect or fraudulently use the U.S. Mail, the postal system or postal employees.
The USPIS has the power to enforce the USPS monopoly by conducting search and seizure raids on entities they suspect of sending non-urgent mail through overnight delivery competitors. According to the American Enterprise Institute, a private conservative think tank, the USPIS raided Equifax offices in 1993 to ascertain if the mail they were sending through Federal Express was truly "extremely urgent." It was found that the mail was not, and Equifax was fined $30,000.
Lastly, the PIS oversees the activities of the Postal Police Force who patrol in and around selected high-risk postal facilities in major metropolitan areas in the United States and its territories.
Office of Inspector General.
The United States Postal Service Office of Inspector General (OIG) was authorized by law in 1996. Prior to the 1996 legislation, the Postal Inspection Service performed the duties of the OIG. The Inspector General, who is independent of postal management, is appointed by and reports directly to the nine presidentially appointed, Senate–confirmed members of the Board of Governors of the United States Postal Service.
The primary purpose of the OIG is to prevent, detect and report fraud, waste and program abuse, and promote efficiency in the operations of the Postal Service. The OIG has "oversight" responsibility for all activities of the Postal Inspection Service.
How delivery services work.
Elements of addressing and preparing domestic mail.
All mailable articles (e.g., letters, flats, machinable parcels, irregular parcels, etc.) shipped within the United States must comply with an array of standards published in the USPS Domestic Mail Manual (DMM). Before addressing the mailpiece, one must first comply with the various mailability standards relating to attributes of the actual mailpiece such as: minimum/maximum dimensions and weight, acceptable mailing containers, proper mailpiece sealing/closure, utilization of various markings, and restrictions relating to various hazardous (e.g., explosives, flammables, etc.) and restricted (e.g., cigarettes, smokeless tobacco, etc.) materials, as well as others articulated in § 601 of the DMM.
The USPS specifies the following key elements when preparing the face of a mailpiece:
Domestic First-Class Mail costs 49¢ for envelopes (34¢ for post cards) and upwards, depending on the weight and dimensions of the letter and the class.
Mail going to naval vessels is known as the Fleet Post Office (FPO) and to Army or Air Force installations use the city abbreviation APO (Army Post Office or Air Force Post Office).
Undeliverable mail that cannot be readily returned, including mail without return addresses, is treated as dead mail at a Mail Recovery Center in Atlanta, Georgia or Saint Paul, Minnesota.
The USPS maintains a list of proper abbreviations.
The format of a return address is similar. Though some style manuals recommend using a comma between the city and state name when typesetting addresses in other contexts, for optimal automatic character recognition, the Post Office does not recommend this when addressing mail. The official recommendation is to use all upper case block letters with proper formats and abbreviations, and leave out all punctuation except for the hyphen in the ZIP+4 code. If the address is unusually formatted or illegible enough, it will require hand-processing, delaying that particular item. The USPS publishes the entirety of their postal addressing standards.
Customers can look up ZIP codes and verify addresses using USPS Web Tools at from the official website, or on their Facebook page, as well as on a third-party site.
Paying postage.
The actual postage can be paid via:
All unused U.S. postage stamps issued since 1861 are still valid as postage at their indicated value. Stamps with no value shown or denominated by a letter are also still valid, although the value depends upon the particular stamp. For some stamps issued without a printed value, the current value is the original value. But some stamps beginning in 1988 or earlier, including "Forever Stamps" that were issued beginning in April 2007, and all 1st class mail 1st ounce stamps beginning 2011-01-21, the value is the current value of a 1st class mail 1st ounce stamps. (The USPS calls these "Forever Stamps". The generic name is non-denominated postage.)
Forever stamps are sold at the First-Class Mail postage rate at the time of purchase, but will always be valid for First-Class Mail (1 oz and under), no matter how rates rise in the future. Britain has had a similar stamp since 1989. The cost of mailing a 1 oz First-Class letter increased to 49 cents on January 26, 2014.
Postage meters.
A postage meter is a mechanical device used to create and apply physical evidence of postage (or franking) to mailed matter. Postage meters are regulated by a country's postal authority; for example, in the United States, the United States Postal Service specifies the rules for the creation, support, and use of postage meters. A postage meter imprints an amount of postage, functioning as a postage stamp, a cancellation and a dated postmark all in one. The meter stamp serves as proof of payment and eliminates the need for adhesive stamps.
PC postage.
In addition to using standard stamps, postage can now be printed in the form of an electronic stamp, or e-stamp, from a personal computer using a system called Information Based Indicia. This online PC Postage method relies upon application software on the customer's computer contacting a postal security device at the office of the postal service.
Other electronic postage payment methods.
Electronic Verification System (eVS) is the Postal Service's integrated mail management technology that centralizes payment processing and electronic postage reports. Part of an evolving suite of USPS electronic payment services called PostalOne!, eVS allows mailers shipping large volumes of parcels through the Postal Service a way to circumvent use of hard-copy manifests, postage statements and drop-shipment verification forms. Instead, mailers can pay postage automatically through a centralized account and track payments online.
Beginning in August 2007, the Postal Service began requiring mailers shipping Parcel Select packages using a permit imprint to use eVS for manifesting their packages.
Stamp copyright and reproduction.
All U.S. postage stamps issued under the former United States Post Office Department and other postage items that were released before 1978 are not subject to copyright, but stamp designs since 1978 are copyrighted. Following the creation of the United States Postal Service, the United States Copyright Office in section 206.02(b) of the Compendium of U.S. Copyright Office Practices holds that "Works prepared by officers or employees of the U.S. Postal Service... are not considered works of the U.S. Government"; Thus, the USPS holds copyright to such materials released since 1978 under Title 17 of the United States Code. Written permission is required for use of copyrighted postage stamp images, although under USPS rules, permission is "generally" not required for "educational use", "news reporting" or "philatelic advertising use," but users must cite USPS as the source of the image and include language such as "© United States Postal Service. All rights reserved."
Service level choices.
General domestic services.
Basic Pricing for First-Class Mail, as of February 2014[ [update]].
The price of a First-Class Mail item can vary greatly depending on the type of mailing, the weight of the item, the size and shape of the mailing, and whether it is subject to a nonmachinable surcharge.
Domestic postage includes Monday through Saturday delivery (excepting federal holidays) to any address, Post Office Box, or general delivery Post Office in the United States, or any U.S. military mail destination.
The Post Office will not deliver packages heavier than 70 lb or if the length (the package's longest dimension) plus the girth (the measurement around the package at its largest point in the two shorter dimensions) is greater than 108 in combined (130 in for Parcel Post). Other carriers handle packages that exceed these limits.
Deliveries outside the contiguous United States may take longer than those listed below.
As of April 2011, domestic postage levels for low-volume mailers include:
Bulk mail.
Discounts are available for large volumes of mail. Depending on the postage level, certain conditions might be required or optional for an additional discount:
In addition to bulk discounts on Express, Priority, and First-Class Mail, the following postage levels are available for bulk mailers:
Extra services.
Depending on the type of mail, additional services are available for an additional fee:
International services.
In May 2007, the USPS restructured international service names to correspond with domestic shipping options. Formerly, USPS International services were categorized as Airmail (Letter Post), Economy (Surface) Parcel Post, Airmail Parcel Post, Global Priority, Global Express, and Global Express Guaranteed Mail. The former Airmail (Letter Post) is now First-Class Mail International, and includes small packages weighing up to four pounds (1.8 kg). Economy Parcel Post was discontinued for international service, while Airmail Parcel Post was replaced by Priority Mail International. Priority Mail International Flat-Rate packaging in various sizes was introduced, with the same conditions of service previously used for Global Priority. Global Express is now Express Mail International, while Global Express Guaranteed is unchanged. The international mailing classes with a tracking ability are Express, Express Guaranteed, and Priority (except that tracking is not available for Priority Mail International Flat Rate Envelopes or Priority Mail International Small Flat Rate Boxes).
One of the major changes in the new naming and services definitions is that USPS-supplied mailing boxes for Priority and Express mail are now allowed for international use. These services are offered to ship letters and packages to almost every country and territory on the globe. The USPS provides much of this service by contracting with a private parcel service, FedEx.
On May 14, 2007, the USPS canceled all outgoing international surface mail (sometimes known as "sea mail") from the United States, citing increased costs and reduced demand due to competition from airmail services such as FedEx and UPS. The decision has been criticized by the Peace Corps and military personnel overseas, as well as independent booksellers and other small businesses who rely on international deliveries.
The USPS provides an M-bag service for international shipment of printed matter; previously surface M-bags existed, but with the 2007 elimination of surface mail, only airmail M-bags remain. The term "M-bag" is not expanded in USPS publications; M-bags are simply defined as "direct sacks of printed matter ... sent to a single foreign addressee at a single address"; however, the term is sometimes referred to informally as "media bag", as the bag can also contain "discs, tapes, and cassettes", in addition to books, for which the usual umbrella term is "media"; some also refer to them as "mail bags".
Military mail is billed at domestic rates when being sent from the United States to a military outpost, and is free when sent by deployed military personnel. The overseas logistics are handled by the Military Postal Service Agency in the Department of Defense. Outside of forward areas and active operations, military mail First-Class takes 7–10 days, Priority 10–15 days, and Parcel Post about 24 days.
Three independent countries with a Compact of Free Association with the U.S. (Palau, the Marshall Islands, and the Federated States of Micronesia) have a special relationship with the United States Postal Service:
Sorting and delivery process.
Processing of standard sized envelopes and cards is highly automated, including reading of handwritten addresses. Mail from individual customers and public postboxes is collected by mail carriers into plastic tubs, which are taken to one of approximately 251 Processing and Distribution Centers (P&DC) across the United States. Each P&DC sort mails for a given region (typically with a radius of around 200 mi) and connects with the national network for interregional mail.
At the P&DC, mail is emptied into hampers which are then automatically dumped into a Dual Pass Rough Cull System (DPRCS). As mail travels through the DPRCS, large items, such as packages and mail bundles, are removed from the stream. As the remaining mail enters the first machine for processing standard mail, the Advanced Facer-Canceler System (AFCS), pieces that passed through the DPRCS but do not conform to physical dimensions for processing in the AFCS (e.g., large envelopes or overstuffed standard envelopes) are automatically diverted from the stream. Mail removed from the DPRCS and AFCS is manually processed or sent to parcel sorting machines.
In contrast to the previous system, which merely canceled and postmarked the upper right corner of the envelope, thereby missing any stamps which were inappropriately placed, the Advanced Facer-Canceler System locates indicia (stamp or metered postage mark), regardless of the orientation of the mail as it enters the machine, and cancels it by applying a postmark. Detection of indicia enables the AFCS to determine the orientation of each mailpiece and sort it accordingly, rotating pieces as necessary so all mail is sorted right-side up and faced in the same direction in each output bin.
Mail is output by the machine into three categories: mail already affixed with a bar code and addressed (such as business reply envelopes and cards); mail with machine printed (typed) addresses; and mail with handwritten addresses. Additionally, machines with a recent Optical Character Recognition (OCR) upgrade have the capability to read the address information, including handwritten, and sort the mail based on local or outgoing ZIP codes.
Mail with typed addresses goes to a Multiline Optical Character Reader (MLOCR) which reads the ZIP Code and address information and prints the appropriate bar code onto the envelope. Mail (actually the scanned image of the mail) with handwritten addresses (and machine-printed ones that are not easily recognized) goes to the Remote Bar Coding System. It also corrects spelling errors and, where there is an error, omission, or conflict in the written address, identifies the most likely correct address.
When it has decided on a correct address, it prints the appropriate bar code onto the envelopes, similarly to the MLOCR system. RBCS also has facilities in place, called Remote Encoding Centers, that have humans look at images of mail pieces and enter the address data. The address data is associated with the image via an ID Tag, a fluorescent barcode printed by mail processing equipment on the back of mail pieces.
Processed mail is imaged by the Mail Isolation Control and Tracking (MICT) system to allow easier tracking of hazardous substances. Images are taken at more than 200 mail processing centers, and are destroyed after being retained for 30 days.
If a customer has filed a change of address card and his or her mail is detected in the mailstream with the old address, the mailpiece is sent to a machine that automatically connects to a Computerized Forwarding System database to determine the new address. If this address is found, the machine will paste a label over the former address with the current address. The mail is returned to the mailstream to forward to the new location.
Mail with addresses that cannot be resolved by the automated system are separated for human intervention. If a local postal worker can read the address, he or she manually sorts it out according to the ZIP code on the article. If the address cannot be read, mail is either returned to the sender (First-Class Mail with a valid return address) or is sent to the Mail Recovery Center in Atlanta, Georgia (formerly known as Dead Letter Offices, originated by Benjamin Franklin in the 1770s) where it receives more intense scrutiny, including being opened to determine if any of the contents are a clue. If no valid address can be determined, the items are held for 90 days in case of inquiry by the customer; and if they are not claimed then they are either destroyed or auctioned off at the monthly Postal Service Unclaimed Parcel auction to raise money for the service.
Once the mail is bar coded, it is automatically sorted by a Delivery Bar Code System that reads the bar code and determines the destination of the mailpiece to postal stations.
Regional mail is trucked to the appropriate local post office or kept in the building for carrier routes served directly from the P&DC. Out-of-region mail is trucked to the airport and then flown, usually as baggage on commercial airlines, to the airport nearest the destination station. At the destination P&DC, mail is once again read by a DBCS which sorts the items into their local destinations, including grouping them by individual mail carrier.
At the carrier route level, 95% of letters arrive pre-sorted; the remaining mail must be sorted by hand. The Post Office is working to increase the percentage of automatically sorted mail, including a pilot program to sort "flats".
FedEx provides the air transport service for Priority and Express Mail. Priority Mail and Express Mail are transported from Priority Mail processing centers to the airport where they are handed off to FedEx. FedEx then flies them to the destination airport where they are handed off back to the postal service for final transport to the local post office and delivery.
Types of postal facilities.
Although its customer service centers are called post offices in regular speech, the USPS recognizes several types of postal facilities, including the following:
While common usage refers to all types of postal facilities as "substations", the USPS Glossary of Postal Terms does not define or even list that word. Post Offices often share facilities with other governmental organizations located within a city's central business district. In those locations, often Courthouses and Federal Buildings, the building is owned by the General Services Administration while the U.S. Postal Services operates as a tenant. The USPS retail system has approximately 36,000 post offices, stations, and branches. Temporary stations are also set up for applying pictorial cancellations.
Automated Postal Centers.
In 2004 the USPS began deploying Automated Postal Centers (APC). APCs are unattended kiosks that are capable of weighing, franking, and storing packages for later pickup as well as selling domestic and international postage stamps. Since its introduction, APCs do not take cash payments - they only accept credit or debit cards. Similarly, traditional vending machines are available at many post offices to purchase stamps, though these are being phased out in many areas. Due to increasing use of Internet services, as of June 2009, no retail post office windows are open 24 hours; overnight services are limited to those provided by an Automated Postal Center.
Evolutionary Network Development (END) program.
In February 2006, the USPS announced that they plan to replace the nine existing facility-types with five processing facility-types:
Over a period of years, these facilities are expected to replace Processing & Distribution Centers, Customer Service Facilities, Bulk Mail Centers, Logistic and Distribution Centers, annexes, the Hub and Spoke Program, Air Mail Centers, and International Service Centers.
The changes are a result of the declining volumes of single-piece First-Class Mail, population shifts, the increase in drop shipments by advertising mailers at destinating postal facilities, advancements in equipment and technology, redundancies in the existing network, and the need for operational flexibility.
Airline and rail division.
The United States Postal Service does not directly own or operate any aircraft or trains, although both were formerly operated. The mail and packages are flown on airlines with which the Postal Service has a contractual agreement. The contracts change periodically. Depending on the contract, aircraft may be painted with the USPS paint scheme. Contract airlines have included: UPS, Emery Worldwide, Ryan International Airlines, FedEx Express, American Airlines, United Airlines, and Express One International. The Postal Service also contracts with Amtrak to carry some mail between certain cities such as Chicago and Minneapolis – Saint Paul.
The last air delivery route in the continental U.S., to residents in the Frank Church—River of No Return Wilderness, was scheduled to be ended in June 2009. The weekly bush plane route, contracted out to an air taxi company, had in its final year an annual cost of $46,000, or $2400/year per residence, over ten times the average cost of delivering mail to a residence in the United States. This decision has been reversed by the U.S. Postmaster General.
Parcel forwarding and private interchange.
Private US parcel forwarding or US mail forwarding companies focusing on personal shopper, relocation, Ex-pat and mail box services often interface with the United States Postal Service for transporting of mail and packages for their customers.
Delivery timing.
Delivery days.
From 1810, mail was delivered seven days a week. In 1828, local religious leaders noticed a decline in Sunday-morning church attendance because of local post offices' doubling as gathering places. These leaders appealed to the government to intervene and close post offices on Sundays. The government, however, declined, and mail was delivered 7 days a week until 1912.
Today, U.S. Mail (with the exception of Express Mail) is not delivered on Sunday, except in a few towns in which the local religion has had an effect on the policy, such as Loma Linda, California, which has a significant Seventh-day Adventist population and where U.S. Mail is delivered Sunday through Friday, with the exception of observed federal holidays.
Saturday delivery was temporarily suspended in April 1957, because of lack of funds, but quickly restored.
Budget problems prompted consideration of dropping Saturday delivery starting around 2009. This culminated in a 2013 announcement that regular mail services would be cut to five days a week, which was reversed by Congress before it could take effect. (See the section Revenue decline and planned cuts.)
Direct delivery vs. customer pickup.
Originally, mail was not delivered to homes and businesses, but to post offices. In 1863, "city delivery" began in urban areas with enough customers to make this economical. This required streets to be named, houses to be numbered, with sidewalks and lighting provided, and these street addresses to be added to envelopes. The number of routes served expanded over time. In 1891, the first experiments with Rural Free Delivery began in less densely populated areas. There is currently an effort to reduce direct delivery in favor of mailbox clusters.
To compensate for high mail volume and slow long-distance transportation which saw mail arrive at post offices throughout the day, deliveries were made multiple times a day. This ranged from twice for residential areas to up to seven times for the central business district of Brooklyn, New York. In the late 19th century, mail boxes were encouraged, saving carriers the time it took to deliver directly to the addressee in person; in the 1910s and 1920s, they were phased in as a requirement for service. In the 1940s, multiple daily deliveries began to be reduced, especially on Saturdays. By 1990, the last twice-daily deliveries in New York City were eliminated.
Today, mail is delivered once a day on-site to most private homes and businesses. The USPS still distinguishes between city delivery (where carriers generally walk and deliver to mailboxes hung on exterior walls or porches, or to commercial reception areas) and rural delivery (where carriers generally drive). With "curbside delivery", mailboxes are at the ends of driveways, on the nearest convenient road. "Central point delivery" is used in some locations, where several nearby residences share a "cluster" of individual mailboxes in a single housing.
Some customers choose to use post office boxes for an additional fee, for privacy or convenience. This provides a locked box at the post office to which mail is addressed and delivered (usually earlier in the day than home delivery). Customers in less densely populated areas where there is no city delivery and who do not qualify for rural delivery may receive mail only through post office boxes. High-volume business customers can also arrange for special pick-up.
Another option is the old-style general delivery, for people who have neither post office boxes nor street addresses. Mail is held at the post office until they present identification and pick it up.
Some customers receive free post office boxes if the USPS declines to provide door-to-door delivery to their location or a nearby box. People with medical problems can request door-to-door delivery. Homeless people are also eligible for post office boxes at the discretion of the local postmaster, or can use general delivery.
Special delivery.
From 1885 to 1997, a service called special delivery was available, which caused a separate delivery to the final location earlier in the day than the usual daily rounds.
Same-day trials.
In December 2012, the USPS began a limited one-year trial of same-day deliveries directly from retailers or distribution hubs to residential addresses in the same local area, a service it dubbed "Metro Post". The trial was initially limited to San Francisco and the only retailer to participate in the first few weeks was 1-800-FLOWERS.
In March 2013, the USPS faced new same-day competition for e-commerce deliveries from Google Shopping Express.
In November 2013, the Postal Service began regular package delivery on Sundays for Amazon customers in New York and Los Angeles, which it expanded to 15 cities in May 2014.
Other competition in this area includes online grocers such as AmazonFresh, Webvan, and delivery services operated by grocery stores like Peapod and Safeway.
Forwarding and holds.
Residential customers can fill out a form to forward mail to a new address, and can also send pre-printed forms to any of their frequent correspondents. They can also put their mail on "hold", for example, while on vacation. The Post Office will store mail during the hold, instead of letting it overflow in the mailbox. These services are not available to large buildings and customers of a commercial mail receiving agency, where mail is subsorted by non-Post Office employees into individual mailboxes.
Financial services.
Postal money orders provide a safe alternative to sending cash through the mail, and are available in any amount up to $1000. Like a bank cheque, money orders are cashable only by the recipient. Unlike a personal bank check, they are prepaid and therefore cannot be returned because of insufficient funds. Money orders are a declining business for the USPS, as companies like PayPal, PaidByCash and others are offering electronic replacements.
From 1911 to 1967, the Postal Service also operated the United States Postal Savings System, not unlike a savings and loan association with the amount of the deposit limited.
A January 2014 report by the Inspector General of the USPS suggested that the agency could earn $8.9 billion per year in revenue by providing financial services, especially in areas where there are no local banks but there is a local post office, and to customers who currently do not have bank accounts.
Employment in the USPS.
The Postal Service is the nation's second-largest civilian employer. s of 2011[ [update]], it employed 574,000 personnel, divided into offices, processing centers, and actual post offices. The United States Postal Service would rank 29th on the 2010 Fortune 500 list, if considered a private company.
Labor unions representing USPS employees include: The American Postal Workers Union (APWU), which represents postal clerks and maintenance, motor vehicle, mail equipment shops, material distribution centers, and operating services and facilities services employees, postal nurses, and IT and accounting; the National Association of Letter Carriers (NALC), which represents city letter carriers; the National Rural Letter Carriers' Association (NRLCA), which represents rural letter carriers; and the National Postal Mail Handlers Union (NPMHU).
USPS employees are divided into three major crafts according to the work they engage in:
Other non-managerial positions in the USPS include:
Though the USPS employs many individuals, as more Americans send information via email, fewer postal workers are needed to work dwindling amounts of mail. Post offices and mail facilities are constantly downsizing, replacing craft positions with new machines and consolidating mail routes through the MIARAP (Modified Interim Alternate Route Adjustment Process) agreement. A major round of job cuts, early retirements, and a construction freeze were announced on March 20, 2009.
Workplace violence.
In the early 1990s, widely publicized workplace shootings by disgruntled employees at USPS facilities led to a Human Resource effort to provide care for stressed workers and resources for coworker conflicts. Due to media coverage, postal employees gained a reputation among the general public as more likely to be mentally ill. The USPS Commission on a Safe and Secure Workplace found that "Postal workers are only a third as likely as those in the national workforce to be victims of homicide at work." In the documentary Murder by Proxy: How America Went Postal, it was argued that this number failed to factor out workers killed by external subjects rather than by fellow employees.
These series of events in turn has influenced American culture, as seen in the slang term "going postal" (see Patrick Sherrill for information on his August 20, 1986, rampage) and the computer game "Postal". Also, in the opening sequence of "", a yell of "Disgruntled postal workers" is heard, followed by the arrival of postal workers with machine guns. In an episode of "Seinfeld", the character Newman, who is a mailman, explained in a dramatic monologue that postal workers "go crazy and kill everyone" because the mail never stops. In "The Simpsons" episode "Sunday, Cruddy Sunday," Nelson Muntz asks Postmaster Bill if he has "ever gone on a killing spree"; Bill replies, "The day of the gun-toting, disgruntled postman shooting up the place went out with the Macarena".
The series of massacres led the US Postal Service to issue a rule prohibiting the possession of any type of firearms (except for those issued to Postal Inspectors) in all designated USPS facilities.
See also.
Unions of the U.S. Postal Service:
History:
International associations:
Workplace violence

</doc>
<doc id="50592" url="http://en.wikipedia.org/wiki?curid=50592" title="Hyperlexia">
Hyperlexia

Hyperlexia was initially identified by Silberberg and Silberberg (1967), who defined it as the precocious ability to read words without prior training in learning to read typically before the age of 5. They indicated that children with hyperlexia have a significantly higher word decoding ability than their reading comprehension levels.
Hyperlexic children are characterized by having average or above average IQs and word-reading ability well above what would be expected given their age. First named and scientifically described in 1967, it can be viewed as a superability in which word recognition ability goes far above expected levels of skill. Some hyperlexics, however, have trouble understanding speech. Some experts believe that most or perhaps all children with hyperlexia lie on the autism spectrum. However, one expert, Darold Treffert, proposes that hyperlexia has subtypes, only some of which overlap with autism. Between 5 and 10 percent of children with autism have been estimated to be hyperlexic.
Hyperlexic children are often fascinated by letters or numbers. They are extremely good at decoding language and thus often become very early readers. Some hyperlexic children learn to spell long words (such as "elephant") before they are two years old and learn to read whole sentences before they turn three.
An fMRI study of a single child showed that hyperlexia may be the neurological opposite of dyslexia.
Development.
Despite hyperlexic children's precocious reading ability, they may struggle to communicate. Often, hyperlexic children will have a precocious ability to read but will learn to speak only by rote and heavy repetition, and may also have difficulty learning the rules of language from examples or from trial and error, which may result in social problems. Their language may develop using echolalia, often repeating words and sentences. Often, the child has a large vocabulary and can identify many objects and pictures, but cannot put their language skills to good use. Spontaneous language is lacking and their pragmatic speech is delayed. Hyperlexic children often struggle with Who? What? Where? Why? and How? questions. Between the ages of 4 and 5 years old, many children make great strides in communicating.
The social skills of a child with hyperlexia often lag tremendously. Hyperlexic children often have far less interest in playing with other children than do their peers.
Types of hyperlexia.
In one paper, Darold Treffert proposes three types of hyperlexia. Specifically:
A different paper by Rebecca Williamson Brown, OD proposes only two types of hyperlexia. These are:
Acquisition.
Although it is generally associated with autism, a 69-year-old woman appears to have been made hyperlexic because of a "cerebral infarction in the left anterior cingulate cortex and corpus callosum".

</doc>
<doc id="50593" url="http://en.wikipedia.org/wiki?curid=50593" title="BNF">
BNF

BNF may stand for:
In computer science:
In science:
In politics:
Other uses:

</doc>
<doc id="50595" url="http://en.wikipedia.org/wiki?curid=50595" title="Flash memory">
Flash memory

Flash memory is an electronic non-volatile computer storage medium that can be electrically erased and reprogrammed.
Introduced by Toshiba in 1984, flash memory was developed from EEPROM (electrically erasable programmable read-only memory). There are two main types of flash memory, which are named after the NAND and NOR logic gates. The individual flash memory cells exhibit internal characteristics similar to those of the corresponding gates.
Whereas EPROMs had to be completely erased before being rewritten, NAND type flash memory may be written and read in blocks (or pages) which are generally much smaller than the entire device. NOR type flash allows a single machine word (byte) to be written—&#X200B;to an erased location—&#X200B;or read independently.
The NAND type is primarily used in memory cards, USB flash drives, solid-state drives (those produced in 2009 or later), and similar products, for general storage and transfer of data. NAND or NOR flash memory is also often used to store configuration data in numerous digital products, a task previously made possible by EEPROM or battery-powered static RAM. One significant disadvantage of flash memory is the finite number of read/write cycles in a specific block.
Example applications of both types of flash memory include personal computers, PDAs, digital audio players, digital cameras, mobile phones, synthesizers, video games, scientific instrumentation, industrial robotics, medical electronics, and so on. In addition to being non-volatile, flash memory offers fast read access times, as fast as dynamic RAM, although not as fast as static RAM or ROM. Its mechanical shock resistance helps explain its popularity over hard disks in portable devices, as does its high durability, being able to withstand high pressure, temperature, immersion in water, etc.
Although flash memory is technically a type of EEPROM, the term "EEPROM" is generally used to refer specifically to non-flash EEPROM which is erasable in small blocks, typically bytes. Because erase cycles are slow, the large block sizes used in flash memory erasing give it a significant speed advantage over non-flash EEPROM when writing large amounts of data. s of 2013[ [update]], flash memory costs much less than byte-programmable EEPROM and has become the dominant memory type wherever a system requires a significant amount of non-volatile, solid-state storage.
History.
Flash memory (both NOR and NAND types) was invented by Dr. Fujio Masuoka while working for Toshiba circa 1980. According to Toshiba, the name "flash" was suggested by Masuoka's colleague, Shōji Ariizumi, because the erasure process of the memory contents reminded him of the flash of a camera. Masuoka and colleagues presented the invention at the "IEEE 1984 International Electron Devices Meeting" (IEDM) held in San Francisco.
Intel Corporation saw the massive potential of the invention and introduced the first commercial NOR type flash chip in 1988. NOR-based flash has long erase and write times, but provides full address and data buses, allowing random access to any memory location. This makes it a suitable replacement for older read-only memory (ROM) chips, which are used to store program code that rarely needs to be updated, such as a computer's BIOS or the firmware of set-top boxes. Its endurance may be from as little as 100 erase cycles for an on-chip flash memory, to a more typical 10,000 or 100,000 erase cycles, up to 1,000,000 erase cycles. NOR-based flash was the basis of early flash-based removable media; CompactFlash was originally based on it, though later cards moved to less expensive NAND flash.
NAND flash has reduced erase and write times, and requires less chip area per cell, thus allowing greater storage density and lower cost per bit than NOR flash; it also has up to ten times the endurance of NOR flash. However, the I/O interface of NAND flash does not provide a random-access external address bus. Rather, data must be read on a block-wise basis, with typical block sizes of hundreds to thousands of bits. This makes NAND flash unsuitable as a drop-in replacement for program ROM, since most microprocessors and microcontrollers required byte-level random access. In this regard, NAND flash is similar to other secondary data storage devices, such as hard disks and optical media, and is thus very suitable for use in mass-storage devices, such as memory cards. The first NAND-based removable media format was SmartMedia in 1995, and many others have followed, including:
A new generation of memory card formats, including RS-MMC, miniSD and microSD, and Intelligent Stick, feature extremely small form factors. For example, the microSD card has an area of just over 1.5 cm2, with a thickness of less than 1 mm. microSD capacities range from 64 MB to 200 GB, as of March 2015.
Principles of operation.
Flash memory stores information in an array of memory cells made from floating-gate transistors. In traditional single-level cell (SLC) devices, each cell stores only one bit of information. Some newer flash memory, known as multi-level cell (MLC) devices, including triple-level cell (TLC) devices, can store more than one bit per cell by choosing between multiple levels of electrical charge to apply to the floating gates of its cells.
The floating gate may be conductive (typically polysilicon in most kinds of flash memory) or non-conductive (as in SONOS flash memory).
Floating-gate transistor.
In flash memory, each memory cell resembles a standard MOSFET, except the transistor has two gates instead of one. On top is the control gate (CG), as in other MOS transistors, but below this there is a floating gate (FG) insulated all around by an oxide layer. The FG is interposed between the CG and the MOSFET channel. Because the FG is electrically isolated by its insulating layer, electrons placed on it are trapped until they are removed by another application of electric field (e.g. Applied voltage or UV as in EPROM). Counter-intuitively, placing electrons on the FG sets the transistor to the logical "0" state. Once the FG is charged, the electrons in it screen (partially cancel) the electric field from the CG, thus, increasing the threshold voltage (VT1) of the cell with no charged FG to a higher threshold voltage (VT2). This means that a higher voltage must be applied to the CG to make the channel conductive. In order to read a value from the transistor, an intermediate voltage between the threshold voltages (VT1 & VT2) is applied to the CG. If the channel conducts at this intermediate voltage, the FG must not be charged (if it were, we would not get conduction because the intermediate voltage is less than VT2), and hence, a logical "1" is stored in the gate. If the channel does not conduct at the intermediate voltage, it indicates that the FG is charged, and hence, a logical "0" is stored in the gate. The presence of a logical "0" or "1" is sensed by determining whether there is current flowing across the transistor when the intermediate voltage is asserted on the CG. In a multi-level cell device, which stores more than one bit per cell, the amount of current flow is sensed (rather than simply its presence or absence), in order to determine more precisely the level of charge on the FG.
Internal charge pumps.
Despite the need for high programming and erasing voltages, virtually all flash chips today require only a single supply voltage, and produce the high voltages using on-chip charge pumps.
Over half the energy used by a 1.8 V NAND flash chip is lost in the charge pump itself. Since boost converters are inherently more efficient than charge pumps, researchers developing low-power SSDs have proposed returning to the dual Vcc/Vpp supply voltages used on all the early flash chips, driving the high Vpp voltage for all flash chips in a SSD with a single shared external boost converter.
In spacecraft and other high-radiation environments, the on-chip charge pump is the first part of the flash chip to fail, although flash memories will continue to work at much higher radiation levels when in read-only mode.
NOR flash.
In NOR flash, each cell has one end connected directly to ground, and the other end connected directly to a bit line.
This arrangement is called "NOR flash" because it acts like a NOR gate: when one of the word lines (connected to the cell's CG) is brought high, the corresponding storage transistor acts to pull the output bit line low. NOR flash continues to be the technology of choice for embedded applications requiring a discrete non-volatile memory device. The low read latencies characteristic of NOR devices allow for both direct code execution and data storage in a single memory product.
Programming.
A single-level NOR flash cell in its default state is logically equivalent to a binary "1" value, because current will flow through the channel under application of an appropriate voltage to the control gate, so that the bitline voltage is pulled down. A NOR flash cell can be programmed, or set to a binary "0" value, by the following procedure:
Erasing.
To erase a NOR flash cell (resetting it to the "1" state), a large voltage "of the opposite polarity" is applied between the CG and source terminal, pulling the electrons off the FG through quantum tunneling. Modern NOR flash memory chips are divided into erase segments (often called blocks or sectors). The erase operation can be performed only on a block-wise basis; all the cells in an erase segment must be erased together. Programming of NOR cells, however, generally can be performed one byte or word at a time.
NAND flash.
NAND flash also uses floating-gate transistors, but they are connected in a way that resembles a NAND gate: several transistors are connected in series, and the bit line is pulled low only if all word lines are pulled high (above the transistors' VT). These groups are then connected via some additional transistors to a NOR-style bit line array in the same way that single transistors are linked in NOR flash.
Compared to NOR flash, replacing single transistors with serial-linked groups adds an extra level of addressing. Whereas NOR flash might address memory by page then word, NAND flash might address it by page, word and bit. Bit-level addressing suits bit-serial applications (such as hard disk emulation), which access only one bit at a time. Execute-in-place applications, on the other hand, require every bit in a word to be accessed simultaneously. This requires word-level addressing. In any case, both bit and word addressing modes are possible with either NOR or NAND flash.
To read, first the desired group is selected (in the same way that a single transistor is selected from a NOR array). Next, most of the word lines are pulled up above the VT of a programmed bit, while one of them is pulled up to just over the VT of an erased bit. The series group will conduct (and pull the bit line low) if the selected bit has not been programmed.
Despite the additional transistors, the reduction in ground wires and bit lines allows a denser layout and greater storage capacity per chip. (The ground wires and bit lines are actually much wider than the lines in the diagrams.) In addition, NAND flash is typically permitted to contain a certain number of faults (NOR flash, as is used for a BIOS ROM, is expected to be fault-free). Manufacturers try to maximize the amount of usable storage by shrinking the size of the transistors. 
Writing and erasing.
NAND flash uses tunnel injection for writing and tunnel release for erasing. NAND flash memory forms the core of the removable USB storage devices known as USB flash drives, as well as most memory card formats and solid-state drives available today.
Vertical NAND.
Vertical NAND (V-NAND) memory stacks memory cells vertically and uses a charge trap flash architecture. The vertical layers allow larger areal bit densities without requiring smaller individual cells.
Structure.
V-NAND uses a charge trap flash geometry (pioneered in 2002 by AMD) that stores charge on an embedded silicon nitride film. Such a film is more robust against point defects and can be made thicker to hold larger numbers of electrons. V-NAND wraps a planar charge trap cell into a cylindrical form.
An individual memory cell is made up of one planar polysilicon layer containing a hole filled by multiple concentric vertical cylinders. The hole's polysilicon surface acts as the gate electrode. The outermost silicon dioxide cylinder acts as the gate dielectric, enclosing a silicon nitride cylinder that stores charge, in turn enclosing a silicon dioxide cylinder as the tunnel dielectric that surrounds a central rod of conducting polysilicon which acts as the conducting channel.
Memory cells in different vertical layers do not interfere with each other, as the charges cannot move vertically through the silicon nitride storage medium, and the electric fields associated with the gates are closely confined within each layer. The vertical collection is electrically identical to the serial-linked groups in which conventional NAND flash memory is configured.
Construction.
Growth of a group of V-NAND cells begins with an alternating stack of conducting (doped) polysilicon layers and insulating silicon dioxide layers.
The next step is to form a cylindrical hole through these layers. In practice, a 128 Gibit V-NAND chip with 24 layers of memory cells requires about 2.9 billion such holes. Next the hole's inner surface receives multiple coatings, first silicon dioxide, then silicon nitride, then a second layer of silicon dioxide. Finally, the hole is filled with conducting (doped) polysilicon.
Performance.
As of 2013, V-NAND flash architecture allows read and write operations twice as fast as conventional NAND and can last up to 10 times as long, while consuming 50 percent less power. They offer comparable physical bit density using 10-nm lithography, but may be able to increase bit density by up to two orders of magnitude.
Limitations.
Block erasure.
One limitation of flash memory is that, although it can be read or programmed a byte or a word at a time in a random access fashion, it can only be erased a "block" at a time. This generally sets all bits in the block to 1. Starting with a freshly erased block, any location within that block can be programmed. However, once a bit has been set to 0, only by erasing the entire block can it be changed back to 1. In other words, flash memory (specifically NOR flash) offers random-access read and programming operations, but does not offer arbitrary random-access rewrite or erase operations. A location can, however, be rewritten as long as the new value's 0 bits are a superset of the over-written values. For example, a nibble value may be erased to 1111, then written as 1110. Successive writes to that nibble can change it to 1010, then 0010, and finally 0000. Essentially, erasure sets all bits to 1, and programming can only clear bits to 0. File systems designed for flash devices can make use of this capability, for example to represent sector metadata.
Although data structures in flash memory cannot be updated in completely general ways, this allows members to be "removed" by marking them as invalid. This technique may need to be modified for multi-level cell devices, where one memory cell holds more than one bit.
Common flash devices such as USB flash drives and memory cards provide only a block-level interface, or flash translation layer (FTL), which writes to a different cell each time to wear-level the device. This prevents incremental writing within a block; however, it does not help the device from being prematurely worn out by poorly designed systems (for example, the MS-FAT file system, having been designed for DOS and disk media).
Memory wear.
Another limitation is that flash memory has a finite number of program-erase cycles (typically written as P/E cycles). Most commercially available flash products are guaranteed to withstand around 100,000 P/E cycles before the wear begins to deteriorate the integrity of the storage. Micron Technology and Sun Microsystems announced an SLC NAND flash memory chip rated for 1,000,000 P/E cycles on 17 December 2008.
The guaranteed cycle count may apply only to block zero (as is the case with TSOP NAND devices), or to all blocks (as in NOR). This effect is partially offset in some chip firmware or file system drivers by counting the writes and dynamically remapping blocks in order to spread write operations between sectors; this technique is called wear leveling. Another approach is to perform write verification and remapping to spare sectors in case of write failure, a technique called bad block management (BBM). For portable consumer devices, these wearout management techniques typically extend the life of the flash memory beyond the life of the device itself, and some data loss may be acceptable in these applications. For high reliability data storage, however, it is not advisable to use flash memory that would have to go through a large number of programming cycles. This limitation is meaningless for 'read-only' applications such as thin clients and routers, which are programmed only once or at most a few times during their lifetimes.
In December 2012, Taiwanese engineers from Macronix revealed their intention to announce at the 2012 IEEE International Electron Devices Meeting that it has figured out how to improve NAND flash storage read/write cycles from 10,000 to 100 million cycles using a “self-healing” process that uses a flash chip with “onboard heaters that could anneal small groups of memory cells.” The built-in thermal annealing replaces the usual erase cycle with a local high temperature process that not only erases the stored charge, but also repairs the electron-induced stress in the chip, giving write cycles of at least 100 million. The result is a chip that can be erased and rewritten over and over, even when it should theoretically break down. As promising as Macronix’s breakthrough could be for the mobile industry, however, there are no plans for a commercial product to be released any time in the near future.
Read disturb.
The method used to read NAND flash memory can cause nearby cells in the same memory block to change over time (become programmed). This is known as read disturb. The threshold number of reads is generally in the hundreds of thousands of reads between intervening erase operations. If reading continually from one cell, that cell will not fail but rather one of the surrounding cells on a subsequent read. To avoid the read disturb problem the flash controller will typically count the total number of reads to a block since the last erase. When the count exceeds a target limit, the affected block is copied over to a new block, erased, then released to the block pool. The original block is as good as new after the erase. If the flash controller does not intervene in time, however, a read disturb error will occur with possible data loss if the errors are too numerous to correct with an error-correcting code.
X-ray effects.
Most flash ICs come in ball grid array (BGA) packages, and even the ones that do not are often mounted on a PCB next to other BGA packages.
After PCB Assembly, boards with BGA packages are often X-rayed to see if the balls are making proper connections to the proper pad, or if the BGA needs rework.
These X-rays can erase programmed bits in a flash chip (convert programmed "0" bits into erased "1" bits).
Erased bits ("1" bits) are not affected by X rays.
Low-level access.
The low-level interface to flash memory chips differs from those of other memory types such as DRAM, ROM, and EEPROM, which support bit-alterability (both zero to one and one to zero) and random access via externally accessible address buses.
NOR memory has an external address bus for reading and programming. For NOR memory, reading and programming are random-access, and unlocking and erasing are block-wise. For NAND memory, reading and programming are page-wise, and unlocking and erasing are block-wise.
NOR memories.
Reading from NOR flash is similar to reading from random-access memory, provided the address and data bus are mapped correctly. Because of this, most microprocessors can use NOR flash memory as execute in place (XIP) memory, meaning that programs stored in NOR flash can be executed directly from the NOR flash without needing to be copied into RAM first. NOR flash may be programmed in a random-access manner similar to reading. Programming changes bits from a logical one to a zero. Bits that are already zero are left unchanged. Erasure must happen a block at a time, and resets all the bits in the erased block back to one. Typical block sizes are 64, 128, or 256 KiB.
Bad block management is a relatively new feature in NOR chips. In older NOR devices not supporting bad block management, the software or device driver controlling the memory chip must correct for blocks that wear out, or the device will cease to work reliably.
The specific commands used to lock, unlock, program, or erase NOR memories differ for each manufacturer. To avoid needing unique driver software for every device made, special Common Flash Memory Interface (CFI) commands allow the device to identify itself and its critical operating parameters.
Besides its use as random-access ROM, NOR flash can also be used as a storage device, by taking advantage of random-access programming. Some devices offer read-while-write functionality so that code continues to execute even while a program or erase operation is occurring in the background. For sequential data writes, NOR flash chips typically have slow write speeds, compared with NAND flash.
Typical NOR flash does not need an error correcting code.
NAND memories.
NAND flash architecture was introduced by Toshiba in 1989. These memories are accessed much like block devices, such as hard disks. Each block consists of a number of pages. The pages are typically 512 or 2,048 or 4,096 bytes in size. Associated with each page are a few bytes (typically 1/32 of the data size) that can be used for storage of an error correcting code (ECC) checksum.
Typical block sizes include:
While reading and programming is performed on a page basis, erasure can only be performed on a block basis.
NAND devices also require bad block management by the device driver software, or by a separate controller chip. SD cards, for example, include controller circuitry to perform bad block management and wear leveling. When a logical block is accessed by high-level software, it is mapped to a physical block by the device driver or controller. A number of blocks on the flash chip may be set aside for storing mapping tables to deal with bad blocks, or the system may simply check each block at power-up to create a bad block map in RAM. The overall memory capacity gradually shrinks as more blocks are marked as bad.
NAND relies on ECC to compensate for bits that may spontaneously fail during normal device operation. A typical ECC will correct a one-bit error in each 2048 bits (256 bytes) using 22 bits of ECC code, or a one-bit error in each 4096 bits (512 bytes) using 24 bits of ECC code. If the ECC cannot correct the error during read, it may still detect the error. When doing erase or program operations, the device can detect blocks that fail to program or erase and mark them bad. The data is then written to a different, good block, and the bad block map is updated.
Hamming codes are the most commonly used ECC for SLC NAND flash.
Reed-Solomon codes and Bose-Chaudhuri-Hocquenghem codes are commonly used ECC for MLC NAND flash.
Some MLC NAND flash chips internally generate the appropriate BCH error correction codes.
Most NAND devices are shipped from the factory with some bad blocks. These are typically marked according to a specified bad block marking strategy. By allowing some bad blocks, the manufacturers achieve far higher yields than would be possible if all blocks had to be verified good. This significantly reduces NAND flash costs and only slightly decreases the storage capacity of the parts.
When executing software from NAND memories, virtual memory strategies are often used: memory contents must first be paged or copied into memory-mapped RAM and executed there (leading to the common combination of NAND + RAM). A memory management unit (MMU) in the system is helpful, but this can also be accomplished with overlays. For this reason, some systems will use a combination of NOR and NAND memories, where a smaller NOR memory is used as software ROM and a larger NAND memory is partitioned with a file system for use as a non-volatile data storage area.
NAND sacrifices the random-access and execute-in-place advantages of NOR. NAND is best suited to systems requiring high capacity data storage. It offers higher densities, larger capacities, and lower cost. It has faster erases, sequential writes, and sequential reads.
Standardization.
A group called the Open NAND Flash Interface Working Group (ONFI) has developed a standardized low-level interface for NAND flash chips. This allows interoperability between conforming NAND devices from different vendors. The ONFI specification version 1.0 was released on 28 December 2006. It specifies:
The ONFI group is supported by major NAND flash manufacturers, including Hynix, Intel, Micron Technology, and Numonyx, as well as by major manufacturers of devices incorporating NAND flash chips.
One major flash device manufacturer, Toshiba, has chosen to use an interface of their own design known as Toggle Mode (and now Toggle V2.0). This interface isn't directly, pin for pin compatible with the ONFI specification. The result is a product designed for one vendor's devices, can't use other vendor's devices.
A group of vendors, including Intel, Dell, and Microsoft, formed a Non-Volatile Memory Host Controller Interface (NVMHCI) Working Group. The goal of the group is to provide standard software and hardware programming interfaces for nonvolatile memory subsystems, including the "flash cache" device connected to the PCI Express bus.
Distinction between NOR and NAND flash.
NOR and NAND flash differ in two important ways:
These two are linked by the design choices made in the development of NAND flash. A goal of NAND flash development was to reduce the chip area required to implement a given capacity of flash memory, and thereby to reduce cost per bit and increase maximum chip capacity so that flash memory could compete with magnetic storage devices like hard disks.
NOR and NAND flash get their names from the structure of the interconnections between memory cells. In NOR flash, cells are connected in parallel to the bit lines, allowing cells to be read and programmed individually. The parallel connection of cells resembles the parallel connection of transistors in a CMOS NOR gate. In NAND flash, cells are connected in series, resembling a NAND gate. The series connections consume less space than parallel ones, reducing the cost of NAND flash. It does not, by itself, prevent NAND cells from being read and programmed individually.
Each NOR flash cell is larger than a NAND flash cell — 10 F2 vs 4 F2 — even when using exactly the same semiconductor device fabrication and so each transistor, contact, etc. is exactly the same size—because NOR flash cells require a separate metal contact for each cell.
When NOR flash was developed, it was envisioned as a more economical and conveniently rewritable ROM than contemporary EPROM and EEPROM memories. Thus random-access reading circuitry was necessary. However, it was expected that NOR flash ROM would be read much more often than written, so the write circuitry included was fairly slow and could only erase in a block-wise fashion. On the other hand, applications that use flash as a replacement for disk drives do not require word-level write address, which would only add to the complexity and cost unnecessarily.
Because of the series connection and removal of wordline contacts, a large grid of NAND flash memory cells will occupy perhaps only 60% of the area of equivalent NOR cells (assuming the same CMOS process resolution, for example, 130 nm, 90 nm, or 65 nm). NAND flash's designers realized that the area of a NAND chip, and thus the cost, could be further reduced by removing the external address and data bus circuitry. Instead, external devices could communicate with NAND flash via sequential-accessed command and data registers, which would internally retrieve and output the necessary data. This design choice made random-access of NAND flash memory impossible, but the goal of NAND flash was to replace hard disks, not to replace ROMs.
Write endurance.
The write endurance of SLC floating-gate NOR flash is typically equal to or greater than that of NAND flash, while MLC NOR and NAND flash have similar endurance capabilities. Example Endurance cycle ratings listed in datasheets for NAND and NOR flash are provided. 
However, by applying certain algorithms and design paradigms such as wear leveling and memory over-provisioning, the endurance of a storage system can be tuned to serve specific requirements.
Computation of NAND flash memory endurance is a challenging subject that depends on SLC/MLC/TLC memory type as well as use pattern. In order to compute the longevity of the NAND flash, one must use the size of the memory chip, the type of memory (e.g. SLC/MLC/TLC), and use pattern. The life of the flash can last from a few days to several hundred years.
Flash file systems.
Because of the particular characteristics of flash memory, it is best used with either a controller to perform wear leveling and error correction or specifically designed flash file systems, which spread writes over the media and deal with the long erase times of NOR flash blocks. The basic concept behind flash file systems is the following: when the flash store is to be updated, the file system will write a new copy of the changed data to a fresh block, remap the file pointers, then erase the old block later when it has time.
In practice, flash file systems are only used for memory technology devices (MTDs), which are embedded flash memories that do not have a controller. Removable flash memory cards and USB flash drives have built-in controllers to perform wear leveling and error correction so use of a specific flash file system does not add any benefit.
Capacity.
Multiple chips are often arrayed to achieve higher capacities for use in consumer electronic devices such as multimedia players or GPSs. The capacity of flash chips generally follows Moore's Law because they are manufactured with many of the same integrated circuits techniques and equipment.
Consumer flash storage devices typically are advertised with usable sizes expressed as a small integer power of two (2, 4, 8, etc.) and a designation of megabytes (MB) or gigabytes (GB); e.g., 512 MB, 8 GB. This includes SSDs marketed as hard drive replacements, in accordance with traditional hard drives, which use decimal prefixes. Thus, an SSD marked as "64 GB" is at least 64 × 1,0003 bytes (64 GB). Most users will have slightly less capacity than this available for their files, due to the space taken by file system metadata.
The flash memory chips inside them are sized in strict binary multiples, but the actual total capacity of the chips is not usable at the drive interface.
It is considerably larger than the advertised capacity in order to allow for distribution of writes (wear leveling), for sparing, for error correction codes, and for other metadata needed by the device's internal firmware.
In 2005, Toshiba and SanDisk developed a NAND flash chip capable of storing 1 GB of data using multi-level cell (MLC) technology, capable of storing two bits of data per cell. In September 2005, Samsung Electronics announced that it had developed the world’s first 2 GB chip.
In March 2006, Samsung announced flash hard drives with a capacity of 4 GB, essentially the same order of magnitude as smaller laptop hard drives, and in September 2006, Samsung announced an 8 GB chip produced using a 40 nm manufacturing process.
In January 2008, SanDisk announced availability of their 16 GB MicroSDHC and 32 GB SDHC Plus cards.
More recent flash drives (as of 2012) have much greater capacities, holding 64, 128, and 256 GB. 
A joint development at Intel and Micron will allow the production of 32 layer 3.5 terabyte (TB) NAND flash sticks and 10 TB standard-sized SSDs. The device includes 5 packages of 16 x 48 GB TLC dies, using a floating gate cell design.
Flash chips continue to be manufactured with capacities under or around 1 MB, e.g., for BIOS-ROMs and embedded applications.
Transfer rates.
NAND flash memory cards are much faster at reading than writing so it is the maximum read speed that is commonly advertised.
Performance also depends on the quality of memory controllers. Even when the only change to manufacturing is die-shrink, the absence of an appropriate controller can result in degraded speeds.
Applications.
Serial flash.
Serial flash is a small, low-power flash memory that uses a serial interface, typically Serial Peripheral Interface Bus (SPI), for sequential data access. When incorporated into an embedded system, serial flash requires fewer wires on the PCB than parallel flash memories, since it transmits and receives data one bit at a time. This may permit a reduction in board space, power consumption, and total system cost.
There are several reasons why a serial device, with fewer external pins than a parallel device, can significantly reduce overall cost:
There are two major SPI flash types. The first type is characterized by small pages and one or more internal SRAM page buffers allowing a complete page to be read to the buffer, partially modified, and then written back (for example, the Atmel AT45 "DataFlash" or the Micron Technology Page Erase NOR Flash). The second type has larger sectors. The smallest sectors typically found in an SPI flash are 4 kB, but they can be as large as 64 kB. Since the SPI flash lacks an internal SRAM buffer, the complete page must be read out and modified before being written back, making it slow to manage. "SPI flash" is cheaper than "DataFlash" and is therefore a good choice when the application is code shadowing.
The two types are not easily exchangeable, since they do not have the same pinout, and the command sets are incompatible.
Firmware storage.
With the increasing speed of modern CPUs, parallel flash devices are often much slower than the memory bus of the computer they are connected to. Conversely, modern SRAM offers access times below 10 ns, while DDR2 SDRAM offers access times below 20 ns. Because of this, it is often desirable to shadow code stored in flash into RAM; that is, the code is copied from flash into RAM before execution, so that the CPU may access it at full speed. Device firmware may be stored in a serial flash device, and then copied into SDRAM or SRAM when the device is powered-up. Using an external serial flash device rather than on-chip flash removes the need for significant process compromise (a process that is good for high-speed logic is generally not good for flash and vice versa). Once it is decided to read the firmware in as one big block it is common to add compression to allow a smaller flash chip to be used. Typical applications for serial flash include storing firmware for hard drives, Ethernet controllers, DSL modems, wireless network devices, etc.
Flash memory as a replacement for hard drives.
One more recent application for flash memory is as a replacement for hard disks. Flash memory does not have the mechanical limitations and latencies of hard drives, so a solid-state drive (SSD) is attractive when considering speed, noise, power consumption, and reliability. Flash drives are gaining traction as mobile device secondary storage devices; they are also used as substitutes for hard drives in high-performance desktop computers and some servers with RAID and SAN architectures.
There remain some aspects of flash-based SSDs that make them unattractive. The cost per gigabyte of flash memory remains significantly higher than that of hard disks. Also flash memory has a finite number of P/E cycles, but this seems to be currently under control since warranties on flash-based SSDs are approaching those of current hard drives.
For relational databases or other systems that require ACID transactions, even a modest amount of Flash storage can offer vast speedups over arrays of disk drives.
In June 2006, Samsung Electronics released the first flash-memory based PCs, the Q1-SSD and Q30-SSD, both of which used 32 GB SSDs, and were at least initially available only in South Korea.
A solid-state drive was offered as an option with the first Macbook Air introduced in 2008, and from 2010 onwards, all Macbook Air laptops shipped with an SSD. Starting in late 2011, as part of Intel's Ultrabook initiative, an increasing number of ultra thin laptops are being shipped with SSDs standard.
There are also hybrid techniques such as hybrid drive and ReadyBoost that attempt to combine the advantages of both technologies, using flash as a high-speed non-volatile cache for files on the disk that are often referenced, but rarely modified, such as application and operating system executable files.
Flash memory as RAM.
As of 2012, there are attempts to use flash memory as the main computer memory, DRAM.
Archival or long-term storage.
It is unclear how long flash memory will persist under archival conditions—i.e., benign temperature and humidity with infrequent access with or without prophylactic rewrite. Anecdotal evidence suggests that the technology is reasonably robust on the scale of years.
Industry.
One source states that, in 2008, the flash memory industry includes about US$9.1 billion in production and sales. Other sources put the flash memory market at a size of more than US$20 billion in 2006, accounting for more than eight percent of the overall semiconductor market and more than 34 percent of the total semiconductor memory market.
In 2012, the market was estimated at $26.8 billion.
Flash scalability.
Due to its relatively simple structure and high demand for higher capacity, NAND flash memory is the most aggressively scaled technology among electronic devices. The heavy competition among the top few manufacturers only adds to the aggressiveness in shrinking the design rule or process technology node. While the expected shrink timeline is a factor of two every three years per original version of Moore's law, this has recently been accelerated in the case of NAND flash to a factor of two every two years. 
As the feature size of flash memory cells reaches the minimum limit, further flash density increases will be driven by greater levels of MLC, possibly 3-D stacking of transistors, and improvements to the manufacturing process. The decrease in endurance and increase in uncorrectable bit error rates that accompany feature size shrinking can be compensated by improved error correction mechanisms. Even with these advances, it may be impossible to economically scale flash to smaller and smaller dimensions as the number of electron holding capacity reduces. Many promising new technologies (such as FeRAM, MRAM, PMC, PCM, ReRAM, and others) are under investigation and development as possible more scalable replacements for flash.

</doc>
<doc id="50597" url="http://en.wikipedia.org/wiki?curid=50597" title="EEPROM">
EEPROM

EEPROM (also written E2PROM and pronounced "e-e-prom", "double-e prom", "e-squared", or simply "e-prom") stands for Electrically Erasable Programmable Read-Only Memory and is a type of non-volatile memory used in computers and other electronic devices to store small amounts of data that must be saved when power is removed, e.g., calibration tables or device configuration.
Unlike bytes in most other kinds of non-volatile memory, individual bytes in a traditional EEPROM can be independently read, erased, and re-written.
When larger amounts of static data are to be stored (such as in USB flash drives) a specific type of EEPROM such as flash memory is more economical than traditional EEPROM devices. EEPROMs are organized as arrays of floating-gate transistors.
An EPROM usually must be removed from the device for erasing and programming, whereas EEPROMs can be programmed and erased in-circuit, by applying special programming signals. Originally, EEPROMs were limited to single byte operations which made them slower, but modern EEPROMs allow multi-byte page operations. It also has a limited life - that is, the number of times it could be reprogrammed was limited to tens or hundreds of thousands of times. That limitation has been extended to a million write operations in modern EEPROMs. In an EEPROM that is frequently reprogrammed while the computer is in use, the life of the EEPROM can be an important design consideration. It is for this reason that EEPROMs were used for configuration information, rather than random access memory.
History.
Eli Harari at Hughes Aircraft invented the EEPROM in 1977 utilising Fowler-Nordheim tunneling through a thin floating gate. Hughes went on to produce the first EEPROM devices.
In 1978, George Perlegos at Intel developed the Intel 2816, which was built on earlier EPROM technology, but used a thin gate oxide layer enabling the chip to erase its own bits without a UV source. Perlegos and others later left Intel to form Seeq Technology, which used on-device charge pumps to supply the high voltages necessary for programming EEPROMs.
Functions of EEPROM.
There are different types of electrical interfaces to EEPROM devices. Main categories of these interface types are:
How the device is operated depends on the electrical interface.
Serial bus devices.
Most common serial interface types are SPI, I²C, Microwire, UNI/O, and 1-Wire. These interfaces require between one and four control signals for operation, resulting in a memory device in an eight-pin (or less) package.
The serial EEPROM (or SEEPROM) typically operates in three phases: OP-Code Phase, Address Phase and Data Phase. The OP-Code is usually the first 8-bits input to the serial input pin of the EEPROM device (or with most I²C devices, is implicit); followed by 8 to 24 bits of addressing depending on the depth of the device, then data to be read or written.
Each EEPROM device typically has not its own set of OP-Code instructions to map to different functions. Some of the common operations on SPI EEPROM devices are:
Other operations supported by some EEPROM devices are:
Parallel bus devices.
Parallel EEPROM devices typically have an 8-bit data bus and an address bus wide enough to cover the complete memory. Most devices have chip select and write protect pins. Some microcontrollers also have integrated parallel EEPROM.
Operation of a parallel EEPROM is simple and fast when compared to serial EEPROM, but these devices are larger due to the higher pin count (28 pins or more) and have been decreasing in popularity in favor of serial EEPROM or Flash.
Other devices.
EEPROM memory is used to enable features in other types of products that are not strictly memory products. Products such as real-time clocks, digital potentiometers, digital temperature sensors, among others, may have small amounts of EEPROM to store calibration information or other data that needs to be available in the event of power loss.
It was also used on video game cartridges to save game progress and configurations, before the usage of external and internal flash memories.
Failure modes.
There are two limitations of stored information; endurance, and data retention.
During rewrites, the gate oxide in the floating-gate transistors gradually accumulates trapped electrons. The electric field of the trapped electrons adds to the electrons in the floating gate, lowering the window between threshold voltages for zeros vs ones. After sufficient number of rewrite cycles, the difference becomes too small to be recognizable, the cell is stuck in programmed state, and endurance failure occurs. The manufacturers usually specify the maximum number of rewrites being 1 million or more.
During storage, the electrons injected into the floating gate may drift through the insulator, especially at increased temperature, and cause charge loss, reverting the cell into erased state. The manufacturers usually guarantee data retention of 10 years or more.
Related types.
Flash memory is a later form of EEPROM. In the industry, there is a convention to reserve the term EEPROM to byte-wise erasable memories compared to block-wise erasable flash memories. EEPROM occupies more die area than flash memory for the same capacity, because each cell usually needs a read, a write, and an erase transistor, while flash memory erase circuits are shared by large blocks of cells (often 512×8).
Newer non-volatile memory technologies such as FeRAM and MRAM are slowly replacing EEPROMs in some applications, but are expected to remain a small fraction of the EEPROM market for the foreseeable future.
Comparison with EPROM and EEPROM/Flash.
The difference between EPROM and EEPROM lies in the way that the memory programs and erases. EEPROM can be programmed and erased electrically using field electron emission (more commonly known in the industry as "Fowler–Nordheim tunneling").
EPROMs can't be erased electrically, and are programmed via hot carrier injection onto the floating gate. Erase is via an ultraviolet light source, although in practice many EPROMs are encapsulated in plastic that is opaque to UV light, making them "one-time programmable".
Most NOR Flash memory is a hybrid style—programming is through hot carrier injection and erase is through Fowler–Nordheim tunneling.

</doc>
<doc id="50601" url="http://en.wikipedia.org/wiki?curid=50601" title="Cystic fibrosis">
Cystic fibrosis

Cystic fibrosis (CF), also known as mucoviscidosis, is a genetic disorder that affects mostly the lungs but also the pancreas, liver, kidneys and intestine. Long-term issues include difficulty breathing and coughing up sputum as a result of frequent lung infections. Other symptoms include sinus infections, poor growth, fatty stool, clubbing of the finger and toes, and infertility in males among others. Different people may have different degrees of symptoms.
CF is an autosomal recessive disorder. It is caused by the presence of mutations in both copies of the gene for the protein cystic fibrosis transmembrane conductance regulator (CFTR). Those with a single working copy are carriers and otherwise mostly normal. CFTR is involved in production of sweat, digestive fluids, and mucus. When not functional usually thin secretions become thick. The condition is diagnosed by a sweat test and genetic testing. Screening of infants at birth takes place in some areas of the world.
There is no cure for cystic fibrosis. Lung infections are treated with antibiotics which may be given intravenously, inhaled, or by mouth. Sometimes the antibiotic azithromycin is used long term. Inhaled hypertonic saline and salbutamol may also be useful. Lung transplantation may be an option if lung function continues to worsen. Pancreatic enzyme replacement and fat soluble vitamin supplementation are important, especially in the young. While not well supported by evidence, many people use airway clearance techniques such as chest physiotherapy. The average life expectancy is between 37 and 50 years in the developed world. Lung problems are responsible for death in 80% of people.
CF is most common among people of Northern European ancestry and affects about one out of every three thousand newborns. About one in twenty five are carriers. It is least common in Africans and Asians. It was first recognized as a specific disease by Dorothy Andersen in 1938, with descriptions that fit the condition occurring at least as far back as 1595. The name "cystic fibrosis" refers to the characteristic fibrosis and cysts that form within the pancreas.
Signs and symptoms.
The main signs and symptoms of cystic fibrosis are salty-tasting skin, poor growth, and poor weight gain despite normal food intake, accumulation of thick, sticky mucus, frequent chest infections, and coughing or shortness of breath. Males can be infertile due to congenital absence of the vas deferens. Symptoms often appear in infancy and childhood, such as bowel obstruction due to meconium ileus in newborn babies. As the children grow, they must exercise to release mucus in the alveoli. Ciliated epithelial cells in the person have a mutated protein that leads to abnormally viscous mucus production. The poor growth in children typically presents as an inability to gain weight or height at the same rate as their peers and is occasionally not diagnosed until investigation is initiated for poor growth. The causes of growth failure are multifactorial and include chronic lung infection, poor absorption of nutrients through the gastrointestinal tract, and increased metabolic demand due to chronic illness.
In rare cases, cystic fibrosis can manifest itself as a coagulation disorder. Vitamin K is normally absorbed from breast milk, formula, and later, solid foods. This absorption is impaired in some cystic fibrosis patients. Young children are especially sensitive to vitamin K malabsorptive disorders because only a very small amount of vitamin K crosses the placenta, leaving the child with very low reserves and limited ability to absorb vitamin K from dietary sources after birth. Because factors II, VII, IX, and X (clotting factors) are vitamin K–dependent, low levels of vitamin K can result in coagulation problems. Consequently, when a child presents with unexplained bruising, a coagulation evaluation may be warranted to determine whether there is an underlying disease.
Lungs and sinuses.
Lung disease results from clogging of the airways due to mucus build-up, decreased mucociliary clearance, and resulting inflammation. Inflammation and infection cause injury and structural changes to the lungs, leading to a variety of symptoms. In the early stages, incessant coughing, copious phlegm production, and decreased ability to exercise are common. Many of these symptoms occur when bacteria that normally inhabit the thick mucus grow out of control and cause pneumonia.
In later stages, changes in the architecture of the lung, such as pathology in the major airways (bronchiectasis), further exacerbate difficulties in breathing. Other symptoms include coughing up blood (hemoptysis), high blood pressure in the lung (pulmonary hypertension), heart failure, difficulties getting enough oxygen to the body (hypoxia), and respiratory failure requiring support with breathing masks, such as bilevel positive airway pressure machines or ventilators. "Staphylococcus aureus", "Haemophilus influenzae", and "Pseudomonas aeruginosa" are the three most common organisms causing lung infections in CF patients. In addition to typical bacterial infections, people with CF more commonly develop other types of lung disease. Among these is allergic bronchopulmonary aspergillosis, in which the body's response to the common fungus "Aspergillus fumigatus" causes worsening of breathing problems. Another is infection with "Mycobacterium avium" complex (MAC), a group of bacteria related to tuberculosis, which can cause a lot of lung damage and does not respond to common antibiotics.
Mucus in the paranasal sinuses is equally thick and may also cause blockage of the sinus passages, leading to infection. This may cause facial pain, fever, nasal drainage, and headaches. Individuals with CF may develop overgrowth of the nasal tissue (nasal polyps) due to inflammation from chronic sinus infections. Recurrent sinonasal polyps can occur in as many as 10% to 25% of CF patients. These polyps can block the nasal passages and increase breathing difficulties.
Cardiorespiratory complications are the most common cause of death (~80%) in patients at most CF centers in the United States.
Gastrointestinal.
Prior to prenatal and newborn screening, cystic fibrosis was often diagnosed when a newborn infant failed to pass feces (meconium). Meconium may completely block the intestines and cause serious illness. This condition, called meconium ileus, occurs in 5–10% of newborns with CF. In addition, protrusion of internal rectal membranes (rectal prolapse) is more common, occurring in as many as 10% of children with CF, and it is caused by increased fecal volume, malnutrition, and increased intra–abdominal pressure due to coughing.
The thick mucus seen in the lungs has a counterpart in thickened secretions from the pancreas, an organ responsible for providing digestive juices that help break down food. These secretions block the exocrine movement of the digestive enzymes into the duodenum and result in irreversible damage to the pancreas, often with painful inflammation (pancreatitis). The pancreatic ducts are totally plugged in more advanced cases, usually seen in older children or adolescents. This causes atrophy of the exocrine glands and progressive fibrosis.
The lack of digestive enzymes leads to difficulty absorbing nutrients with their subsequent excretion in the feces, a disorder known as malabsorption. Malabsorption leads to malnutrition and poor growth and development because of calorie loss. Resultant hypoproteinemia may be severe enough to cause generalized edema. Individuals with CF also have difficulties absorbing the fat-soluble vitamins A, D, E, and K.
In addition to the pancreas problems, people with cystic fibrosis experience more heartburn, intestinal blockage by intussusception, and constipation. Older individuals with CF may develop distal intestinal obstruction syndrome when thickened feces cause intestinal blockage.
Exocrine pancreatic insufficiency occurs in the majority (85% to 90%) of patients with CF. It is mainly associated with "severe" CFTR mutations, where both alleles are completely nonfunctional (e.g. ΔF508/ΔF508). It occurs in 10% to 15% of patients with one "severe" and one "mild" CFTR mutation where there still is a little CFTR activity, or where there are two "mild" CFTR mutations. In these milder cases, there is still sufficient pancreatic exocrine function so that enzyme supplementation is not required. There are usually no other GI complications in pancreas-sufficient phenotypes, and in general, such individuals usually have excellent growth and development. Despite this, idiopathic chronic pancreatitis can occur in a subset of pancreas-sufficient individuals with CF, and is associated with recurrent abdominal pain and life-threatening complications.
Thickened secretions also may cause liver problems in patients with CF. Bile secreted by the liver to aid in digestion may block the bile ducts, leading to liver damage. Over time, this can lead to scarring and nodularity (cirrhosis). The liver fails to rid the blood of toxins and does not make important proteins, such as those responsible for blood clotting. Liver disease is the third most common cause of death associated with CF.
Endocrine.
The pancreas contains the islets of Langerhans, which are responsible for making insulin, a hormone that helps regulate blood glucose. Damage of the pancreas can lead to loss of the islet cells, leading to a type of diabetes that is unique to those with the disease. This cystic fibrosis-related diabetes (CFRD) shares characteristics that can be found in type 1 and type 2 diabetics, and is one of the principal nonpulmonary complications of CF. Vitamin D is involved in calcium and phosphate regulation. Poor uptake of vitamin D from the diet because of malabsorption can lead to the bone disease osteoporosis in which weakened bones are more susceptible to fractures. In addition, people with CF often develop clubbing of their fingers and toes due to the effects of chronic illness and low oxygen in their tissues.
Infertility.
Infertility affects both men and women. At least 97% of men with cystic fibrosis are infertile, but not sterile and can have children with assisted reproductive techniques. The main cause of infertility in men with cystic fibrosis is congenital absence of the vas deferens (which normally connects the testes to the ejaculatory ducts of the penis), but potentially also by other mechanisms such as causing no sperm, teratospermia, and few sperm with poor motility. Many men found to have congenital absence of the vas deferens during evaluation for infertility have a mild, previously undiagnosed form of CF. Approximately 20% of women with CF have fertility difficulties due to thickened cervical mucus or malnutrition. In severe cases, malnutrition disrupts ovulation and causes amenorrhea.
Cause.
CF is caused by a mutation in the gene cystic fibrosis transmembrane conductance regulator (CFTR). The most common mutation, ΔF508, is a deletion (Δ signifying deletion) of three nucleotides that results in a loss of the amino acid phenylalanine (F) at the 508th position on the protein. This mutation accounts for two-thirds (66–70%) of CF cases worldwide and 90% of cases in the United States; however, there are over 1500 other mutations that can produce CF. Although most people have two working copies (alleles) of the CFTR gene, only one is needed to prevent cystic fibrosis. CF develops when neither allele can produce a functional CFTR protein. Thus, CF is considered an autosomal recessive disease.
The CFTR gene, found at the q31.2 locus of chromosome 7, is 230,000 base pairs long, and creates a protein that is 1,480 amino acids long. More specifically the location is between base pair 117,120,016 to 117,308,718 on the long arm of chromosome 7, region 3, band 1, sub-band 2, represented as 7q31.2. Structurally, CFTR is a type of gene known as an ABC gene. The product of this gene (the CFTR) is a chloride ion channel important in creating sweat, digestive juices and mucus. This protein possesses two ATP-hydrolyzing domains, which allows the protein to use energy in the form of ATP. It also contains two domains comprising 6 alpha helices apiece, which allow the protein to cross the cell membrane. A regulatory binding site on the protein allows activation by phosphorylation, mainly by cAMP-dependent protein kinase. The carboxyl terminal of the protein is anchored to the cytoskeleton by a PDZ domain interaction.
In addition, there is increasing evidence that genetic modifiers besides CFTR modulate the frequency and severity of the disease. One example is mannan-binding lectin, which is involved in innate immunity by facilitating phagocytosis of microorganisms. Polymorphisms in one or both mannan-binding lectin alleles that result in lower circulating levels of the protein are associated with a threefold higher risk of end-stage lung disease, as well as an increased burden of chronic bacterial infections.
Pathophysiology.
There are several mutations in the "CFTR" gene, and different mutations cause different defects in the CFTR protein, sometimes causing a milder or more severe disease. These protein defects are also targets for drugs which can sometimes restore their function. ΔF508-CFTR, which occurs in >90% of patients in the U.S., creates a protein that does not fold normally and is degraded by the cell. Other mutations result in proteins that are too short (truncated) because production is ended prematurely. Other mutations produce proteins that: do not use energy normally, do not allow chloride, iodide, and thiocyanate to cross the membrane appropriately, degrade at a faster rate than normal. Mutations may also lead to fewer copies of the CFTR protein being produced.
The protein created by this gene is anchored to the outer membrane of cells in the sweat glands, lungs, pancreas, and all other remaining exocrine glands in the body. 
The protein spans this membrane and acts as a channel connecting the inner part of the cell (cytoplasm) to the surrounding fluid. This channel is primarily responsible for controlling the movement of halogens from inside to outside of the cell; however, in the sweat ducts it facilitates the movement of chloride from the sweat duct into the cytoplasm. When the CFTR protein does not resorb ions in sweat ducts, chloride and thiocyanate released from sweat glands are trapped inside the ducts and pumped to the skin. Additionally hypothiocyanite, OSCN, cannot be produced by the immune defense system. Because chloride is negatively charged, this creates a difference in the electrical potential inside and outside the cell causing cations to cross into the cell. Sodium is the most common cation in the extracellular space. The excess chloride within sweat ducts prevents sodium resorption by epithelial sodium channels and the combination of sodium and chloride creates the salt, which is lost in high amounts in the sweat of individuals with CF. This lost salt forms the basis for the sweat test.
Most of the damage in CF is due to blockage of the narrow passages of affected organs with thickened secretions. These blockages lead to remodeling and infection in the lung, damage by accumulated digestive enzymes in the pancreas, blockage of the intestines by thick faeces, etc. There are several theories on how the defects in the protein and cellular function cause the clinical effects. The most current theory suggests that defective ion transport leads to dehydration in the airway epithelia, thickening mucus. In airway epithelial cells, the cilia exist in between the cell's apical surface and mucus in a layer known as Airway Surface Liquid (ASL). The flow of ions from the cell and into this layer is determined by ion channels like CFTR. CFTR not only allows Chloride ions to be drawn from the cell and into the ASL, but it also regulates another channel called ENac. ENac allows sodium ions to leave the ASL and enter the respiratory epithelium. CFTR normally inhibits this channel, but if the CFTR is defective, then sodium will flow freely from the ASL and into the cell. As water follows sodium, the depth of ASL will be depleted and the cilia will be left in the mucous layer. As cilia cannot effectively move in a thick viscous environment, there is deficient mucociliary clearance and a build up of mucous, clogging small airways. The accumulation of more viscous, nutrient-rich mucus in the lungs allows bacteria to hide from the body's immune system, causing repeated respiratory infections. The presence of the same CFTR proteins in pancreatic duct and skin cells are what cause symptoms in these systems.
Chronic infections.
The lungs of individuals with cystic fibrosis are colonized and infected by bacteria from an early age. These bacteria, which often spread among individuals with CF, thrive in the altered mucus, which collects in the small airways of the lungs. This mucus leads to the formation of bacterial microenvironments known as biofilms that are difficult for immune cells and antibiotics to penetrate. Viscous secretions and persistent respiratory infections repeatedly damage the lung by gradually remodeling the airways, which makes infection even more difficult to eradicate.
Over time, both the types of bacteria and their individual characteristics change in individuals with CF. In the initial stage, common bacteria such as "Staphylococcus aureus" and "Haemophilus influenzae" colonize and infect the lungs. Eventually, "Pseudomonas aeruginosa" (and sometimes "Burkholderia cepacia") dominates. By 18 years of age, 80% of patients with classic CF harbor "P. aeruginosa", and 3.5% harbor "B. cepacia". Once within the lungs, these bacteria adapt to the environment and develop resistance to commonly used antibiotics. "Pseudomonas" can develop special characteristics that allow the formation of large colonies, known as "mucoid" "Pseudomonas", which are rarely seen in people that do not have CF.
One way infection spreads is by passing between different individuals with CF. In the past, people with CF often participated in summer "CF Camps" and other recreational gatherings. Hospitals grouped patients with CF into common areas and routine equipment (such as nebulizers) was not sterilized between individual patients. This led to transmission of more dangerous strains of bacteria among groups of patients. As a result, individuals with CF are now routinely isolated from one another in the healthcare setting, and healthcare providers are encouraged to wear gowns and gloves when examining patients with CF to limit the spread of virulent bacterial strains.
CF patients may also have their airways chronically colonized by filamentous fungi (such as "Aspergillus fumigatus", "Scedosporium apiospermum", "Aspergillus terreus") and/or yeasts (such as "Candida albicans"); other filamentous fungi less commonly isolated include "Aspergillus flavus" and "Aspergillus nidulans" (occur transiently in CF respiratory secretions) and "Exophiala dermatitidis" and "Scedosporium prolificans" (chronic airway-colonizers); some filamentous fungi like "Penicillium emersonii" and "Acrophialophora fusispora" are encountered in patients almost exclusively in the context of CF. Defective mucociliary clearance characterizing CF is associated with local immunological disorders. In addition, the prolonged therapy with antibiotics and the use of corticosteroid treatments may also facilitate fungal growth. Although the clinical relevance of the fungal airway colonization is still a matter of debate, filamentous fungi may contribute to the local inflammatory response and therefore to the progressive deterioration of the lung function, as often happens with allergic broncho-pulmonary aspergillosis (ABPA) – the most common fungal disease in the context of CF, involving a Th2-driven immune response to Aspergillus.
Diagnosis and monitoring.
Cystic fibrosis may be diagnosed by many different methods including newborn screening, sweat testing, and genetic testing. As of 2006 in the United States, 10 percent of cases are diagnosed shortly after birth as part of newborn screening programs. The newborn screen initially measures for raised blood concentration of immunoreactive trypsinogen. Infants with an abnormal newborn screen need a sweat test to confirm the CF diagnosis. In many cases, a parent makes the diagnosis because the infant tastes salty. Trypsinogen levels can be increased in individuals who have a single mutated copy of the "CFTR" gene (carriers) or, in rare instances, in individuals with two normal copies of the "CFTR" gene. Due to these false positives, CF screening in newborns can be controversial. Most states and countries do not screen for CF routinely at birth. Therefore, most individuals are diagnosed after symptoms (e.g. sinopulmonary disease and GI manifestations) prompt an evaluation for cystic fibrosis. The most commonly used form of testing is the sweat test. Sweat-testing involves application of a medication that stimulates sweating (pilocarpine). To deliver the medication through the skin, iontophoresis is used to, whereby one electrode is placed onto the applied medication and an electric current is passed to a separate electrode on the skin. The resultant sweat is then collected on filter paper or in a capillary tube and analyzed for abnormal amounts of sodium and chloride. People with CF have increased amounts of sodium and chloride in their sweat. In contrast, people with CF have less thiocyanate and hypothiocyanite in their saliva and mucus (Banfi et al.). CF can also be diagnosed by identification of mutations in the CFTR gene.
People with CF may be listed in a disease registry that allows researchers and doctors to track health results and identify candidates for clinical trials.
Prenatal.
Couples who are pregnant or planning a pregnancy can have themselves tested for the CFTR gene mutations to determine the risk that their child will be born with cystic fibrosis. Testing is typically performed first on one or both parents and, if the risk of CF is high, testing on the fetus is performed. The American College of Obstetricians and Gynecologists (ACOG) recommends testing for couples who have a personal or close family history of CF, and they recommend that carrier testing be offered to all Caucasian couples and be made available to couples of other ethnic backgrounds.
Because development of CF in the fetus requires each parent to pass on a mutated copy of the CFTR gene and because CF testing is expensive, testing is often performed initially on one parent. If testing shows that parent is a CFTR gene mutation carrier, the other parent is tested to calculate the risk that their children will have CF. CF can result from more than a thousand different mutations, and as of 2006 it is not possible to test for each one. Testing analyzes the blood for the most common mutations such as ΔF508—most commercially available tests look for 32 or fewer different mutations. If a family has a known uncommon mutation, specific screening for that mutation can be performed. Because not all known mutations are found on current tests, a negative screen does not guarantee that a child will not have CF.
During pregnancy, testing can be performed on the placenta (chorionic villus sampling) or the fluid around the fetus (amniocentesis). However, chorionic villus sampling has a risk of fetal death of 1 in 100 and amniocentesis of 1 in 200; a recent study has indicated this may be much lower, approximately 1 in 1,600.
Economically, for carrier couples of cystic fibrosis, when comparing preimplantation genetic diagnosis (PGD) with natural conception (NC) followed by prenatal testing and abortion of affected pregnancies, PGD provides net economic benefits up to a maternal age of approximately 40 years, after which NC, prenatal testing and abortion has higher economic benefit.
Management.
While there are no cures for cystic fibrosis, there are several treatment methods. The management of cystic fibrosis has improved significantly over the past 70 years. While infants born with cystic fibrosis 70 years ago would have been unlikely to live beyond their first year, infants today are likely to live well into adulthood. Recent advances in the treatment of cystic fibrosis have meant that an individual with cystic fibrosis can live a fuller life less encumbered by their condition. The cornerstones of management are proactive treatment of airway infection, and encouragement of good nutrition and an active lifestyle. Pulmonary rehabilitation as a management of cystic fibrosis continues throughout a person's life, and is aimed at maximizing organ function, and therefore quality of life. At best, current treatments delay the decline in organ function. Because of the wide variation in disease symptoms, treatment typically occurs at specialist multidisciplinary centers, and is tailored to the individual. Targets for therapy are the lungs, gastrointestinal tract (including pancreatic enzyme supplements), the reproductive organs (including assisted reproductive technology (ART)) and psychological support.
The most consistent aspect of therapy in cystic fibrosis is limiting and treating the lung damage caused by thick mucus and infection, with the goal of maintaining quality of life. Intravenous, inhaled, and oral antibiotics are used to treat chronic and acute infections. Mechanical devices and inhalation medications are used to alter and clear the thickened mucus. These therapies, while effective, can be extremely time-consuming.
Antibiotics.
Many people with CF are on one or more antibiotics at all times, even when healthy, to prophylactically suppress infection. Antibiotics are absolutely necessary whenever pneumonia is suspected or there has been a noticeable decline in lung function, and are usually chosen based on the results of a sputum analysis and the person's past response. This prolonged therapy often necessitates hospitalization and insertion of a more permanent IV such as a peripherally inserted central catheter (PICC line) or Port-a-Cath. Inhaled therapy with antibiotics such as tobramycin, colistin, and aztreonam is often given for months at a time to improve lung function by impeding the growth of colonized bacteria. Inhaled antibiotic therapy helps lung function by fighting infection, but also has significant drawbacks like development of antibiotic resistance, tinnitus and changes in the voice. Oral antibiotics such as ciprofloxacin or azithromycin are given to help prevent infection or to control ongoing infection. The aminoglycoside antibiotics (e.g. tobramycin) used can cause hearing loss, damage to the balance system in the inner ear or kidney problems with long-term use. To prevent these side-effects, the amount of antibiotics in the blood is routinely measured and adjusted accordingly.
Other treatments for lung disease.
Several mechanical techniques are used to dislodge sputum and encourage its expectoration. In the hospital setting, chest physiotherapy (CPT) is utilized; a respiratory therapist percusses an individual's chest with his or her hands several times a day, to loosen up secretions. Devices that recreate this percussive therapy include the ThAIRapy Vest and the intrapulmonary percussive ventilator (IPV). Newer methods such as Biphasic Cuirass Ventilation, and associated clearance mode available in such devices, integrate a cough assistance phase, as well as a vibration phase for dislodging secretions. These are portable and adapted for home use.
Aerosolized medications that help loosen secretions include dornase alfa and hypertonic saline. Dornase is a recombinant human deoxyribonuclease, which breaks down DNA in the sputum, thus decreasing its viscosity. Denufosol is an investigational drug that opens an alternative chloride channel, helping to liquefy mucus. It is unclear if inhaled corticosteroids are useful.
As lung disease worsens, mechanical breathing support may become necessary. Individuals with CF may need to wear special masks at night that help push air into their lungs. These machines, known as bilevel positive airway pressure (BiPAP) ventilators, help prevent low blood oxygen levels during sleep. BiPAP may also be used during physical therapy to improve sputum clearance. During severe illness, a tube may be placed in the throat (a procedure known as a tracheostomy) to enable breathing supported by a ventilator.
For children, preliminary studies show massage therapy may help people and their families quality of life. It is unclear what effect pneumococcal vaccination has as it has not been studied as of 2014.
Transplantation.
Lung transplantation often becomes necessary for individuals with cystic fibrosis as lung function and exercise tolerance decline. Although single lung transplantation is possible in other diseases, individuals with CF must have both lungs replaced because the remaining lung might contain bacteria that could infect the transplanted lung. A pancreatic or liver transplant may be performed at the same time in order to alleviate liver disease and/or diabetes. Lung transplantation is considered when lung function declines to the point where assistance from mechanical devices is required or someone's survival is threatened.
Other aspects.
Newborns with intestinal obstruction typically require surgery, whereas adults with distal intestinal obstruction syndrome typically do not. Treatment of pancreatic insufficiency by replacement of missing digestive enzymes allows the duodenum to properly absorb nutrients and vitamins that would otherwise be lost in the feces. However, the best dosage and form of pancreatic enzyme replacement is unclear, as are the risks and long-term effectiveness of this treatment.
So far, no large-scale research involving the incidence of atherosclerosis and coronary heart disease in adults with cystic fibrosis has been conducted. This is likely due to the fact that the vast majority of people with cystic fibrosis do not live long enough to develop clinically significant atherosclerosis or coronary heart disease.
Diabetes is the most common non-pulmonary complication of CF. It mixes features of type 1 and type 2 diabetes, and is recognized as a distinct entity, cystic fibrosis-related diabetes (CFRD). While oral anti-diabetic drugs are sometimes used, the only recommended treatment is the use of insulin injections or an insulin pump, and, unlike in type 1 and 2 diabetes, dietary restrictions are not recommended.
Development of osteoporosis can be prevented by increased intake of vitamin D and calcium, and can be treated by bisphosphonates, although adverse effects can be an issue. Poor growth may be avoided by insertion of a feeding tube for increasing calories through supplemental feeds or by administration of injected growth hormone.
Sinus infections are treated by prolonged courses of antibiotics. The development of nasal polyps or other chronic changes within the nasal passages may severely limit airflow through the nose, and over time reduce the person's sense of smell. Sinus surgery is often used to alleviate nasal obstruction and to limit further infections. Nasal steroids such as fluticasone are used to decrease nasal inflammation.
Female infertility may be overcome by assisted reproduction technology, particularly embryo transfer techniques. Male infertility caused by absence of the vas deferens may be overcome with testicular sperm extraction (TESE), collecting sperm cells directly from the testicles. If the collected sample contains too few sperm cells to likely have a spontaneous fertilization, intracytoplasmic sperm injection can be performed. Third party reproduction is also a possibility for women with CF. It is unclear if taking antioxidants affects outcomes.
Prognosis.
The prognosis for cystic fibrosis has improved due to earlier diagnosis through screening, better treatment and access to health care. In 1959, the median age of survival of children with cystic fibrosis in the United States was six months. In 2010, survival is estimated to be 37 years for women and 40 for men. In Canada, median survival increased from 24 years in 1982 to 47.7 in 2007.
Of those with cystic fibrosis who are more than 18 years old as of 2009, 92% had graduated from high school, 67% had at least some college education, 15% were disabled and 9% were unemployed, 56% were single and 39% were married or living with a partner. In Russia the overall median age of patients is 25, which is caused by the absence or high cost of medication and the fact that lung transplantation is not performed.
Quality of life.
Chronic illnesses can be very difficult to manage. Cystic fibrosis (CF) is a chronic illness that affects the "digestive and respiratory tracts resulting in generalized malnutrition and chronic respiratory infections". The thick secretions clog the airways in the lungs, which often cause inflammation and severe lung infections. If it is compromised, it affects the quality of life of someone with CF and their ability to complete such tasks as everyday chores. 
It is important for CF patients to understand the detrimental relationship that chronic illnesses place on the quality of life. According to Schmitz and Goldbeck (2006), the fact that cystic fibrosis significantly increases emotional stress on both the individual and the family, "and the necessary time-consuming daily treatment routine may have further negative effects on quality of life (QOL)". However, Havermans and colleagues (2006) have shown that young outpatients with CF who have participated in the CFQ-R (Cystic Fibrosis Questionnaire-Revised) "rated some QOL domains higher than did their parents". Consequently, outpatients with CF have a more positive outlook for themselves. 
Furthermore, there are many ways to improve the QOL in CF patients. Exercise is promoted to increase lung function. Integrating an exercise regimen into the CF patient’s daily routine can significantly improve the quality of life. There is no definitive cure for cystic fibrosis. However, there are diverse medications used, such as mucolytics, bronchodilators, steroids, and antibiotics, that have the purpose of loosening mucus, expanding airways, decreasing inflammation, and fighting lung infections.
Epidemiology.
Cystic fibrosis is the most common life-limiting autosomal recessive disease among people of European heritage. In the United States, approximately 30,000 individuals have CF; most are diagnosed by six months of age. In Canada, there are approximately 4,000 people with CF. Approximately 1 in 25 people of European descent, and one in 30 of Caucasian Americans, is a carrier of a cystic fibrosis mutation. Although CF is less common in these groups, approximately 1 in 46 Hispanics, 1 in 65 Africans and 1 in 90 Asians carry at least one abnormal CFTR gene. Ireland has the world's highest prevalence of cystic fibrosis, at 1:1353.
Although technically a rare disease, cystic fibrosis is ranked as one of the most widespread life-shortening genetic diseases. It is most common among nations in the Western world. An exception is Finland, where only one in 80 people carry a CF mutation. The World Health Organization states that "In the European Union, 1 in 2000–3000 newborns is found to be affected by CF". In the United States, 1 in 3,500 children are born with CF. In 1997, about 1 in 3,300 caucasian children in the United States was born with cystic fibrosis. In contrast, only 1 in 15,000 African American children suffered from cystic fibrosis, and in Asian Americans the rate was even lower at 1 in 32,000.
Cystic fibrosis is diagnosed in males and females equally. For reasons that remain unclear, data has shown that males tend to have a longer life expectancy than females, however recent studies suggest this gender gap may no longer exist perhaps due to improvements in health care facilities, while a recent study from Ireland identified a link between the female hormone estrogen and worse outcomes in CF.
The distribution of CF alleles varies among populations. The frequency of ΔF508 carriers has been estimated at 1:200 in northern Sweden, 1:143 in Lithuanians, and 1:38 in Denmark. No ΔF508 carriers were found among 171 Finns and 151 Saami people. ΔF508 does occur in Finland, but it is a minority allele there. Cystic fibrosis is known to occur in only 20 families (pedigrees) in Finland.
Evolution.
The ΔF508 mutation is estimated to be up to 52,000 years old. Numerous hypotheses have been advanced as to why such a lethal mutation has persisted and spread in the human population. Other common autosomal recessive diseases such as sickle-cell anemia have been found to protect carriers from other diseases, a concept known as heterozygote advantage. Resistance to the following have all been proposed as possible sources of heterozygote advantage:
History.
It is supposed that CF appeared about 3,000 BC because of migration of peoples, gene mutations, and new conditions in nourishment. Although the entire clinical spectrum of CF was not recognized until the 1930s, certain aspects of CF were identified much earlier. Indeed, literature from Germany and Switzerland in the 18th century warned "Wehe dem Kind, das beim Kuß auf die Stirn salzig schmekt, er ist verhext und muss bald sterbe" or "Woe to the child who tastes salty from a kiss on the brow, for he is cursed and soon must die," recognizing the association between the salt loss in CF and illness.
In the 19th century, Carl von Rokitansky described a case of fetal death with meconium peritonitis, a complication of meconium ileus associated with cystic fibrosis. Meconium ileus was first described in 1905 by Karl Landsteiner. In 1936, Guido Fanconi published a paper describing a connection between celiac disease, cystic fibrosis of the pancreas, and bronchiectasis.
In 1938 Dorothy Hansine Andersen published an article, "Cystic Fibrosis of the Pancreas and Its Relation to Celiac Disease: a Clinical and Pathological Study," in the "American Journal of Diseases of Children". She was the first to describe the characteristic cystic fibrosis of the pancreas and to correlate it with the lung and intestinal disease prominent in CF. She also first hypothesized that CF was a recessive disease and first used pancreatic enzyme replacement to treat affected children. In 1952 Paul di Sant' Agnese discovered abnormalities in sweat electrolytes; a sweat test was developed and improved over the next decade.
The first linkage between CF and another marker (Paroxonase) was found in 1985 by Hans Eiberg, indicating that only one locus exists for CF. In 1988 the first mutation for CF, ΔF508 was discovered by Francis Collins, Lap-Chee Tsui and John R. Riordan on the seventh chromosome. Subsequent research has found over 1,000 different mutations that cause CF.
Because mutations in the CFTR gene are typically small, classical genetics techniques had been unable to accurately pinpoint the mutated gene. Using protein markers, gene-linkage studies were able to map the mutation to chromosome 7. Chromosome-walking and -jumping techniques were then used to identify and sequence the gene. In 1989 Lap-Chee Tsui led a team of researchers at the Hospital for Sick Children in Toronto that discovered the gene responsible for CF. Cystic fibrosis represents a classic example of how a human genetic disorder was elucidated strictly by the process of forward genetics.
Research.
Gene therapy.
Gene therapy has been explored as a potential cure for cystic fibrosis. Data from trials on topical cystic fibrosis transmembrane conductance regulator gene therapy do not support its use as outcomes studied in these trials were not of clinical relevance.
Ideally, gene therapy places a normal copy of the CFTR gene into affected cells. Transferring the normal CFTR gene into the affected epithelium cells would result in the production of functional CFTR in all target cells, without adverse reactions or an inflammation response. Studies have shown that to prevent the lung manifestations of cystic fibrosis, only 5–10% the normal amount of CFTR gene expression is needed. Multiple approaches have been tested for gene transfer, such as liposomes and viral vectors in animal models and clinical trials. However, both methods were found to be relatively inefficient treatment options. The main reason is that very few cells take up the vector and express the gene, so the treatment has little effect. Additionally, problems have been noted in cDNA recombination, such that the gene introduced by the treatment is rendered unusable. There has been a functional repair in culture of CFTR by CRISPR/Cas9 in intestinal stem cell organoids of cystic fibrosis patients.
Small molecules.
A number of small molecules that aim at compensating various mutations of the CFTR gene are under development. One approach is to develop drugs that get the ribosome to overcome the stop codon and synthesize a full-length CFTR protein. About 10% of CF result from a premature stop codon in the DNA, leading to early termination of protein synthesis and truncated proteins. These drugs target nonsense mutations such as G542X, which consists of the amino acid glycine in position 542 being replaced by a stop codon. Aminoglycoside antibiotics interfere with protein synthesis and error-correction. In some cases, they can cause the cell to overcome a premature stop codon by inserting a random amino acid, thereby allowing expression of a full-length protein.
The aminoglycoside gentamicin has been used to treat lung cells from CF patients in the laboratory to induce the cells to grow full-length proteins. Another drug targeting nonsense mutations is ataluren, which is undergoing Phase III clinical trials as of 2011[ [update]].
Ivacaftor (Kalydeco), approved for use by the FDA in the United States in January 2012, targets the mutation G551D (glycine in position 551 is substituted with aspartic acid). Lumacaftor aims at F508del (phenylalanine in position 508 is missing).
Other.
It is unclear as of 2014 if ursodeoxycholic acid is useful for those with cystic fibrosis-related liver disease.

</doc>
<doc id="50603" url="http://en.wikipedia.org/wiki?curid=50603" title="Multiple sclerosis">
Multiple sclerosis

Multiple sclerosis (MS), also known as disseminated sclerosis or encephalomyelitis disseminata, is an inflammatory disease in which the insulating covers of nerve cells in the brain and spinal cord are damaged. This damage disrupts the ability of parts of the nervous system to communicate, resulting in a wide range of signs and symptoms, including physical, mental, and sometimes psychiatric problems. MS takes several forms, with new symptoms either occurring in isolated attacks (relapsing forms) or building up over time (progressive forms). Between attacks, symptoms may disappear completely; however, permanent neurological problems often occur, especially as the disease advances.
While the cause is not clear, the underlying mechanism is thought to be either destruction by the immune system or failure of the myelin-producing cells. Proposed causes for this include genetics and environmental factors such as infections. MS is usually diagnosed based on the presenting signs and symptoms and the results of supporting medical tests.
There is no known cure for multiple sclerosis. Treatments attempt to improve function after an attack and prevent new attacks. Medications used to treat MS while modestly effective can have adverse effects and be poorly tolerated. Many people pursue alternative treatments, despite a lack of evidence. The long-term outcome is difficult to predict, with good outcomes more often seen in women, those who develop the disease early in life, those with a relapsing course, and those who initially experienced few attacks. Life expectancy is on average 5 to 10 years lower than that of an unaffected population.
Multiple sclerosis is the most common autoimmune disorder affecting the central nervous system. As of 2008, between 2 and 2.5 million people are affected globally with rates varying widely in different regions of the world and among different populations. In 2013, 20,000 people died from MS, up from 12,000 in 1990. The disease usually begins between the ages of 20 and 50 and is twice as common in women as in men. The name "multiple sclerosis" refers to scars (sclerae—better known as plaques or lesions) in particular in the white matter of the brain and spinal cord. MS was first described in 1868 by Jean-Martin Charcot. A number of new treatments and diagnostic methods are under development.
Signs and symptoms.
A person with MS can have almost any neurological symptom or sign; with autonomic, visual, motor, and sensory problems being the most common. The specific symptoms are determined by the locations of the lesions within the nervous system, and may include loss of sensitivity or changes in sensation such as tingling, pins and needles or numbness, muscle weakness, very pronounced reflexes, muscle spasms, or difficulty in moving; difficulties with coordination and balance (ataxia); problems with speech or swallowing, visual problems (nystagmus, optic neuritis or double vision), feeling tired, acute or chronic pain, and bladder and bowel difficulties, among others. Difficulties thinking and emotional problems such as depression or unstable mood are also common. Uhthoff's phenomenon, a worsening of symptoms due to exposure to higher than usual temperatures, and Lhermitte's sign, an electrical sensation that runs down the back when bending the neck, are particularly characteristic of MS. The main measure of disability and severity is the expanded disability status scale (EDSS), with other measures such as the multiple sclerosis functional composite being increasingly used in research.
The condition begins in 85% of cases as a clinically isolated syndrome over a number of days with 45% having motor or sensory problems, 20% having optic neuritis, and 10% having symptoms related to brainstem dysfunction, while the remaining 25% have more than one of the previous difficulties. The course of symptoms occurs in two main patterns initially: either as episodes of sudden worsening that last a few days to months (called relapses, exacerbations, bouts, attacks, or flare-ups) followed by improvement (85% of cases) or as a gradual worsening over time without periods of recovery (10-15% of cases). A combination of these two patterns may also occur or people may start in a relapsing and remitting course that then becomes progressive later on. Relapses are usually not predictable, occurring without warning. Exacerbations rarely occur more frequently than twice per year. Some relapses, however, are preceded by common triggers and they occur more frequently during spring and summer. Similarly, viral infections such as the common cold, influenza, or gastroenteritis increase their risk. Stress may also trigger an attack. Women with MS who become pregnant experience fewer relapses; however, during the first months after delivery the risk increases. Overall, pregnancy does not seem to influence long-term disability. Many events have not been found to affect relapse rates including vaccination, breast feeding, physical trauma, and Uhthoff's phenomenon.
Causes.
The cause of MS is unknown; however, it is believed to occur as a result of some combination of genetic and environmental factors such as infectious agents. Theories try to combine the data into likely explanations, but none has proved definitive. While there are a number of environmental risk factors and although some are partly modifiable, further research is needed to determine whether their elimination can prevent MS.
Geography.
MS is more common in people who live farther from the equator, although exceptions exist. These exceptions include ethnic groups that are at low risk far from the equator such as the Samis, Amerindians, Canadian Hutterites, New Zealand Māori, and Canada's Inuit, as well as groups that have a relatively high risk close to the equator such as Sardinians, inland Sicilians, Palestinians and Parsis. The cause of this geographical pattern is not clear. While the north-south gradient of incidence is decreasing, as of 2010 it is still present.
MS is more common in regions with northern European populations and the geographic variation may simply reflect the global distribution of these high-risk populations. Decreased sunlight exposure resulting in decreased vitamin D production has also been put forward as an explanation. A relationship between season of birth and MS lends support to this idea, with fewer people born in the northern hemisphere in November as compared to May being affected later in life. Environmental factors may play a role during childhood, with several studies finding that people who move to a different region of the world before the age of 15 acquire the new region's risk to MS. If migration takes place after age 15, however, the person retains the risk of his home country. There is some evidence that the effect of moving may still apply to people older than 15.
Genetics.
MS is not considered a hereditary disease; however, a number of genetic variations have been shown to increase the risk. The probability is higher in relatives of an affected person, with a greater risk among those more closely related. In identical twins both are affected about 30% of the time, while around 5% for non-identical twins and 2.5% of siblings are affected with a lower percentage of half-siblings. If both parents are affected the risk in their children is 10 times that of the general population. MS is also more common in some ethnic groups than others.
Specific genes that have been linked with MS include differences in the human leukocyte antigen (HLA) system—a group of genes on chromosome 6 that serves as the major histocompatibility complex (MHC). That changes in the HLA region are related to susceptibility has been known for over thirty years, and additionally this same region has been implicated in the development of other autoimmune diseases such as diabetes type I and systemic lupus erythematosus. The most consistent finding is the association between multiple sclerosis and alleles of the MHC defined as DR15 and DQ6. Other loci have shown a protective effect, such as HLA-C554 and HLA-DRB1*11. Overall, it has been estimated that HLA changes account for between 20 and 60% of the genetic predisposition. Modern genetic methods (genome-wide association studies) have discovered at least twelve other genes outside the HLA locus that modestly increase the probability of MS.
Infectious agents.
Many microbes have been proposed as triggers of MS, but none have been confirmed. Moving at an early age from one location in the world to another alters a person's subsequent risk of MS. An explanation for this could be that some kind of infection, produced by a widespread microbe rather than a rare one, is related to the disease. Proposed mechanisms include the hygiene hypothesis and the prevalence hypothesis. The hygiene hypothesis proposes that exposure to certain infectious agents early in life is protective, the disease being a response to a late encounter with such agents. The prevalence hypothesis proposes that the disease is due to an infectious agent more common in regions where MS is common and where in most individuals it causes an ongoing infection without symptoms. Only in a few cases and after many years does it cause demyelination. The hygiene hypothesis has received more support than the prevalence hypothesis.
Evidence for a virus as a cause include: the presence of oligoclonal bands in the brain and cerebrospinal fluid of most people with MS, the association of several viruses with human demyelination encephalomyelitis, and the occurrence of demyelination in animals caused by some viral infection. Human herpes viruses are a candidate group of viruses. Individuals having never been infected by the Epstein-Barr virus are at a reduced risk of getting MS, whereas those infected as young adults are at a greater risk than those having had it at a younger age. Although some consider that this goes against the hygiene hypothesis, since the non-infected have probably experienced a more hygienic upbringing, others believe that there is no contradiction, since it is a first encounter with the causative virus relatively late in life that is the trigger for the disease. Other diseases that may be related include measles, mumps and rubella.
Other.
Smoking has been shown to be an independent risk factor for MS. Stress may be a risk factor although the evidence to support this is weak. Association with occupational exposures and toxins—mainly solvents—has been evaluated, but no clear conclusions have been reached. Vaccinations were studied as causal factors; however, most studies show no association. Several other possible risk factors, such as diet and hormone intake, have been looked at; however, evidence on their relation with the disease is "sparse and unpersuasive". Gout occurs less than would be expected and lower levels of uric acid have been found in people with MS. This has led to the theory that uric acid is protective, although its exact importance remains unknown.
Pathophysiology.
The three main characteristics of MS are the formation of lesions in the central nervous system (also called plaques), inflammation, and the destruction of myelin sheaths of neurons. These features interact in a complex and not yet fully understood manner to produce the breakdown of nerve tissue and in turn the signs and symptoms of the disease. Additionally, MS is believed to be an immune-mediated disorder that develops from an interaction of the individual's genetics and as yet unidentified environmental causes. Damage is believed to be caused, at least in part, by attack on the nervous system by a person's own immune system.
Lesions.
The name "multiple sclerosis" refers to the scars (sclerae – better known as plaques or lesions) that form in the nervous system. These lesions most commonly affect the white matter in the optic nerve, brain stem, basal ganglia, and spinal cord, or white matter tracts close to the lateral ventricles. The function of white matter cells is to carry signals between grey matter areas, where the processing is done, and the rest of the body. The peripheral nervous system is rarely involved.
To be specific, MS involves the loss of oligodendrocytes, the cells responsible for creating and maintaining a fatty layer—known as the myelin sheath—which helps the neurons carry electrical signals (action potentials). This results in a thinning or complete loss of myelin and, as the disease advances, the breakdown of the axons of neurons. When the myelin is lost, a neuron can no longer effectively conduct electrical signals. A repair process, called remyelination, takes place in early phases of the disease, but the oligodendrocytes are unable to completely rebuild the cell's myelin sheath. Repeated attacks lead to successively less effective remyelinations, until a scar-like plaque is built up around the damaged axons. These scars are the origin of the symptoms and during an attack magnetic resonance imaging (MRI) often shows more than ten new plaques. This could indicate that there are a number of lesions below which the brain is capable of repairing itself without producing noticeable consequences. Another process involved in the creation of lesions is an abnormal increase in the number of astrocytes due to the destruction of nearby neurons. A number of lesion patterns have been described.
Inflammation.
Apart from demyelination, the other sign of the disease is inflammation. Fitting with an immunological explanation, the inflammatory process is caused by T cells, a kind of lymphocyte that plays an important role in the body's defenses. T cells gain entry into the brain via disruptions in the blood–brain barrier. The T cells recognize myelin as foreign and attack it, explaining why these cells are also called "autoreactive lymphocytes".
The attack of myelin starts inflammatory processes, which triggers other immune cells and the release of soluble factors like cytokines and antibodies. Further breakdown of the blood–brain barrier, in turn cause a number of other damaging effects such as swelling, activation of macrophages, and more activation of cytokines and other destructive proteins. Inflammation can potentially reduce transmission of information between neurons in at least three ways. The soluble factors released might stop neurotransmission by intact neurons. These factors could lead to or enhance the loss of myelin, or they may cause the axon to break down completely.
Blood–brain barrier.
The blood–brain barrier is a part of the capillary system that prevents the entry of T cells into the central nervous system. It may become permeable to these types of cells secondary to an infection by a virus or bacteria. After it repairs itself, typically once the infection has cleared, T cells may remain trapped inside the brain. Gadolinium cannot cross a normal BBB and, therefore, Gadolinium-enhanced MRI is used to show BBB breakdowns.
Diagnosis.
Multiple sclerosis is typically diagnosed based on the presenting signs and symptoms, in combination with supporting medical imaging and laboratory testing. It can be difficult to confirm, especially early on, since the signs and symptoms may be similar to those of other medical problems. The McDonald criteria, which focus on clinical, laboratory, and radiologic evidence of lesions at different times and in different areas, is the most commonly used method of diagnosis with the Schumacher and Poser criteria being of mostly historical significance. While the above criteria allow for a non-invasive diagnosis, some state that the only definitive proof is an autopsy or biopsy where lesions typical of MS are detected.
Clinical data alone may be sufficient for a diagnosis of MS if an individual has had separate episodes of neurologic symptoms characteristic of the disease. In those who seek medical attention after only one attack, other testing is needed for the diagnosis. The most commonly used diagnostic tools are neuroimaging, analysis of cerebrospinal fluid and evoked potentials. Magnetic resonance imaging of the brain and spine may show areas of demyelination (lesions or plaques). Gadolinium can be administered intravenously as a contrast agent to highlight active plaques and, by elimination, demonstrate the existence of historical lesions not associated with symptoms at the moment of the evaluation. Testing of cerebrospinal fluid obtained from a lumbar puncture can provide evidence of chronic inflammation in the central nervous system. The cerebrospinal fluid is tested for oligoclonal bands of IgG on electrophoresis, which are inflammation markers found in 75–85% of people with MS. The nervous system in MS may respond less actively to stimulation of the optic nerve and sensory nerves due to demyelination of such pathways. These brain responses can be examined using visual- and sensory-evoked potentials.
Clinical course.
Several phenotypes (commonly named types), or patterns of progression, have been described. Phenotypes use the past course of the disease in an attempt to predict the future course. They are important not only for prognosis but also for treatment decisions. In 1996, the United States National Multiple Sclerosis Society described four clinical courses. This set of courses was later reviewed by an international panel in 2013, adding clinically isolated syndrome (CIS) and radiologically isolated syndrome (RIS) as phenotypes, but leaving the main structure untouched.
The relapsing-remitting subtype is characterized by unpredictable relapses followed by periods of months to years of relative quiet (remission) with no new signs of disease activity. Deficits that occur during attacks may either resolve or leave problems, the latter in about 40% of attacks and being more common the longer a person has had the disease. This describes the initial course of 80% of individuals with MS. When deficits always resolve between attacks, this is sometimes referred to as "benign MS", although people will still build up some degree of disability in the long term. On the other hand, the term "malignant multiple sclerosis" is used to describe people with MS having reached significant level of disability in a short period. The relapsing-remitting subtype usually begins with a clinically isolated syndrome (CIS). In CIS, a person has an attack suggestive of demyelination, but does not fulfill the criteria for multiple sclerosis. 30 to 70% of persons experiencing CIS later develop MS.
Secondary progressive MS occurs in around 65% of those with initial relapsing-remitting MS, who eventually have progressive neurologic decline between acute attacks without any definite periods of remission. Occasional relapses and minor remissions may appear. The most common length of time between disease onset and conversion from relapsing-remitting to secondary progressive MS is 19 years.
The primary progressive subtype occurs in approximately 10–20% of individuals, with no remission after the initial symptoms. It is characterized by progression of disability from onset, with no, or only occasional and minor, remissions and improvements. The usual age of onset for the primary progressive subtype is later than of the relapsing-remitting subtype. It is similar to the age that secondary progressive usually begins in relapsing-remitting MS, around 40 years of age.
Progressive relapsing MS describes those individuals who, from onset, have a steady neurologic decline but also have clear superimposed attacks. This is the least common of all subtypes.
Unusual types of MS have been described; these include Devic's disease, Balo concentric sclerosis, Schilder's diffuse sclerosis, and Marburg multiple sclerosis. There is debate on whether they are MS variants or different diseases. Multiple sclerosis behaves differently in children, taking more time to reach the progressive stage. Nevertheless, they still reach it at a lower average age than adults usually do.
Management.
Although there is no known cure for multiple sclerosis, several therapies have proven helpful. The primary aims of therapy are returning function after an attack, preventing new attacks, and preventing disability. As with any medical treatment, medications used in the management of MS have several adverse effects. Alternative treatments are pursued by some people, despite the shortage of supporting evidence.
Acute attacks.
During symptomatic attacks, administration of high doses of intravenous corticosteroids, such as methylprednisolone, is the usual therapy, with oral corticosteroids seeming to have a similar efficacy and safety profile. Although, in general, effective in the short term for relieving symptoms, corticosteroid treatments do not appear to have a significant impact on long-term recovery. The consequences of severe attacks that do not respond to corticosteroids might be treatable by plasmapheresis.
Disease-modifying treatments.
Relapsing remitting multiple sclerosis.
As of 2014, nine disease-modifying treatments have been approved by regulatory agencies for relapsing-remitting multiple sclerosis (RRMS) including: interferon beta-1a, interferon beta-1b, glatiramer acetate, mitoxantrone, natalizumab, fingolimod, teriflunomide, dimethyl fumarate and alemtuzumab. Their cost effectiveness as of 2012 is unclear.
In RRMS they are modestly effective at decreasing the number of attacks. The interferons and glatiramer acetate are first-line treatments and are roughly equivalent, reducing relapses by approximately 30%. Early-initiated long-term therapy is safe and improves outcomes. Natalizumab reduces the relapse rate more than first-line agents; however, due to issues of adverse effects is a second-line agent reserved for those who do not respond to other treatments or with severe disease. Mitoxantrone, whose use is limited by severe adverse effects, is a third-line option for those who do not respond to other medications. Treatment of clinically isolated syndrome (CIS) with interferons decreases the chance of progressing to clinical MS. Efficacy of interferons and glatiramer acetate in children has been estimated to be roughly equivalent to that of adults. The role of some newer agents such as fingolimod, teriflunomide, and dimethyl fumarate, as of 2011, is not yet entirely clear.
Progressive multiple sclerosis.
No treatment has been shown to change the course of primary progressive MS and as of 2011 only one medication, mitoxantrone, has been approved for secondary progressive MS. In this population tentative evidence supports mitoxantrone moderately slowing the progression of the disease and decreasing rates of relapses over two years.
Adverse effects.
The disease-modifying treatments have several adverse effects. One of the most common is irritation at the injection site for glatiramer acetate and the interferons (up to 90% with subcutaneous injections and 33% with intramuscular injections). Over time, a visible dent at the injection site, due to the local destruction of fat tissue, known as lipoatrophy, may develop. Interferons may produce flu-like symptoms; some people taking glatiramer experience a post-injection reaction with flushing, chest tightness, heart palpitations, breathlessness, and anxiety, which usually lasts less than thirty minutes. More dangerous but much less common are liver damage from interferons, systolic dysfunction (12%), infertility, and acute myeloid leukemia (0.8%) from mitoxantrone, and progressive multifocal leukoencephalopathy occurring with natalizumab (occurring in 1 in 600 people treated).
Fingolimod may give rise to hypertension and slowed heart rate, macular edema, elevated liver enzymes or a reduction in lymphocyte levels. Tentative evidence supports the short-term safety of teriflunomide, with common side effects including: headaches, fatigue, nausea, hair loss, and limb pain. There have also been reports of liver failure and PML with its use and it is dangerous for fetal development. Most common side effects of dimethyl fumarate are flushing and gastrointestinal problems. While dimethyl fumarate may lead to a reduction in the white blood cell count there were no reported cases of opportunistic infections during trials.
Associated symptoms.
Both medications and neurorehabilitation have been shown to improve some symptoms, though neither changes the course of the disease. Some symptoms have a good response to medication, such as an unstable bladder and spasticity, while others are little changed. For neurologic problems, a multidisciplinary approach is important for improving quality of life; however, it is difficult to specify a 'core team' as many health services may be needed at different points in time. Multidisciplinary rehabilitation programs increase activity and participation of people with MS but do not influence impairment level. There is limited evidence for the overall efficacy of individual therapeutic disciplines, though there is good evidence that specific approaches, such as exercise, and psychology therapies, in particular cognitive behavioral approaches are effective.
Alternative treatments.
Over 50% of people with MS may use complementary and alternative medicine, although percentages vary depending on how alternative medicine is defined. The evidence for the effectiveness for such treatments in most cases is weak or absent. Treatments of unproven benefit used by people with MS include: dietary supplementation and regimens, vitamin D, relaxation techniques such as yoga, herbal medicine (including medical cannabis), hyperbaric oxygen therapy, self-infection with hookworms, reflexology and acupuncture. Regarding the characteristics of users, they are more frequently women, have had MS for a longer time, tend to be more disabled and have lower levels of satisfaction with conventional healthcare.
Prognosis.
The expected future course of the disease depends on the subtype of the disease; the individual's sex, age, and initial symptoms; and the degree of disability the person has. Female sex, relapsing-remitting subtype, optic neuritis or sensory symptoms at onset, few attacks in the initial years and especially early age at onset, are associated with a better course.
The average life expectancy is 30 years from the start of the disease, which is 5 to 10 years less than that of unaffected people. Almost 40% of people with MS reach the seventh decade of life. Nevertheless, two-thirds of the deaths are directly related to the consequences of the disease. Suicide is more common, while infections and other complications are especially dangerous for the more disabled. Although most people lose the ability to walk before death, 90% are capable of independent walking at 10 years from onset, and 75% at 15 years.
Epidemiology.
MS is the most common autoimmune disorder of the central nervous system. As of 2010, the number of people with MS was 2–2.5 million (approximately 30 per 100,000) globally, with rates varying widely in different regions. It is estimated to have resulted in 18,000 deaths that year. In Africa rates are less than 0.5 per 100,000, while they are 2.8 per 100,000 in South East Asia, 8.3 per 100,000 in the Americas, and 80 per 100,000 in Europe. Rates surpass 200 per 100,000 in certain populations of Northern European descent. The number of new cases that develop per year is about 2.5 per 100,000.
Rates of MS appear to be increasing; this, however, may be explained simply by better diagnosis. Studies on populational and geographical patterns have been common and have led to a number of theories about the cause.
MS usually appears in adults in their late twenties or early thirties but it can rarely start in childhood and after 50 years of age. The primary progressive subtype is more common in people in their fifties. Similar to many autoimmune disorders, the disease is more common in women, and the trend may be increasing. As of 2008, globally it is about two times more common in women than in men. In children, it is even more common in females than males, while in people over fifty, it affects males and females almost equally.
History.
Medical discovery.
The French neurologist Jean-Martin Charcot (1825–1893) was the first person to recognize multiple sclerosis as a distinct disease in 1868. Summarizing previous reports and adding his own clinical and pathological observations, Charcot called the disease "sclerose en plaques". The three signs of MS now known as Charcot's triad 1 are nystagmus, intention tremor, and telegraphic speech (scanning speech), though these are not unique to MS. Charcot also observed cognition changes, describing his patients as having a "marked enfeeblement of the memory" and "conceptions that formed slowly".
Before Charcot, Robert Carswell (1793–1857), a British professor of pathology, and Jean Cruveilhier (1791–1873), a French professor of pathologic anatomy, had described and illustrated many of the disease's clinical details, but did not identify it as a separate disease. Specifically, Carswell described the injuries he found as "a remarkable lesion of the spinal cord accompanied with atrophy". Under the microscope, Swiss pathologist Georg Eduard Rindfleisch (1836–1908) noted in 1863 that the inflammation-associated lesions were distributed around blood vessels. During the 20th century theories about the cause and pathogenesis were developed and effective treatments began to appear in the 1990s.
Historical cases.
There are several historical accounts of people who probably had MS and lived before or shortly after the disease was described by Charcot.
A young woman called Halldora who lived in Iceland around 1200 suddenly lost her vision and mobility but, after praying to the saints, recovered them seven days after. Saint Lidwina of Schiedam (1380–1433), a Dutch nun, may be one of the first clearly identifiable people with MS. From the age of 16 until her death at 53, she had intermittent pain, weakness of the legs, and vision loss—symptoms typical of MS. Both cases have led to the proposal of a "Viking gene" hypothesis for the dissemination of the disease.
Augustus Frederick d'Este (1794–1848), son of Prince Augustus Frederick, Duke of Sussex and Lady Augusta Murray and the grandson of George III of the United Kingdom, almost certainly had MS. D'Este left a detailed diary describing his 22 years living with the disease. His diary began in 1822 and ended in 1846, although it remained unknown until 1948. His symptoms began at age 28 with a sudden transient visual loss (amaurosis fugax) after the funeral of a friend. During his disease, he developed weakness of the legs, clumsiness of the hands, numbness, dizziness, bladder disturbances, and erectile dysfunction. In 1844, he began to use a wheelchair. Despite his illness, he kept an optimistic view of life. Another early account of MS was kept by the British diarist W. N. P. Barbellion, nom-de-plume of Bruce Frederick Cummings (1889–1919), who maintained a detailed log of his diagnosis and struggle. His diary was published in 1919 as "The Journal of a Disappointed Man".
Research.
Medications.
There is ongoing research looking for more effective, convenient, and tolerable treatments for relapsing-remitting MS; creation of therapies for the progressive subtypes; neuroprotection strategies; and effective symptomatic treatments.
During the 2000s and 2010s, there has been approval of several oral drugs that are expected to gain in popularity and frequency of use. Several more oral drugs are under investigation, one being laquinimod, which was announced in August 2012 and is in a third phase III trial after mixed results in the previous ones. Similarly, studies aimed to improve the efficacy and ease of use of already existing therapies are occurring. This includes the use of new preparations such as the PEGylated version of interferon-β-1a, which it is hoped may be given at less frequent doses with similar effects. Request for approval of "peginterferon beta-1a" is expected during 2013.
Monoclonal antibodies have also raised high levels of interest. Alemtuzumab, daclizumab, and CD20 monoclonal antibodies such as rituximab, ocrelizumab and ofatumumab have all shown some benefit and are under study as potential treatments. Their use has also been accompanied by the appearance of potentially dangerous adverse effects, the most important of which being opportunistic infections. Related to these investigations is the development of a test for JC virus antibodies, which might help to determine who is at greater risk of developing progressive multifocal leukoencephalopathy when taking natalizumab. While monoclonal antibodies will probably have some role in the treatment of the disease in the future, it is believed that it will be small due to the risks associated with them.
Another research strategy is to evaluate the combined effectiveness of two or more drugs. The main rationale for using a number of medications in MS is that the involved treatments target different mechanisms and, therefore, their use is not necessarily exclusive. Synergies, in which one drug improves the effect of another are also possible, but there can also be drawbacks such as the blocking of the action of the other or worsened side-effects. There have been several trials of combined therapy, yet none have shown positive enough results to be considered as a useful treatment for MS.
Research on neuroprotection and regenerative treatments, such as stem cell therapy, while of high importance, are in the early stages. Likewise, there are not any effective treatments for the progressive variants of the disease. Many of the newest drugs as well as those under development are probably going to be evaluated as therapies for PPMS or SPMS.
Disease biomarkers.
While diagnostic criteria are not expected to change in the near future, work to develop biomarkers that help with diagnosis and prediction of disease progression is ongoing. New diagnostic methods that are being investigated include work with anti-myelin antibodies, and studies with serum and cerebrospinal fluid, but none of them has yielded reliably positive results.
At the current time, there are no laboratory investigations that can predict prognosis. Several promising approaches have been proposed including: interleukin-6, nitric oxide and nitric oxide synthase, osteopontin, and fetuin-A. Since disease progression is the result of degeneration of neurons, the roles of proteins showing loss of nerve tissue such as neurofilaments, tau, and N-acetylaspartate are under investigation. Other effects include looking for biomarkers that distinguish between those who will and will not respond to medications.
Improvement in neuroimaging techniques such as positron emission tomography (PET) or magnetic resonance imaging (MRI) carry a promise for better diagnosis and prognosis predictions, although the effect of such improvements in daily medical practice may take several decades. Regarding MRI, there are several techniques that have already shown some usefulness in research settings and could be introduced into clinical practice, such as double-inversion recovery sequences, magnetization transfer, diffusion tensor, and functional magnetic resonance imaging. These techniques are more specific for the disease than existing ones, but still lack some standardization of acquisition protocols and the creation of normative values. There are other techniques under development that include contrast agents capable of measuring levels of peripheral macrophages, inflammation, or neuronal dysfunction, and techniques that measure iron deposition that could serve to determine the role of this feature in MS, or that of cerebral perfusion. Similarly, new PET radiotracers might serve as markers of altered processes such as brain inflammation, cortical pathology, apoptosis, or remylienation. Antibiodies against the Kir4.1 potassium channel may be related to MS.
Chronic cerebrospinal venous insufficiency.
In 2008, vascular surgeon Paolo Zamboni suggested that MS involves narrowing of the veins draining the brain, which he referred to as chronic cerebrospinal venous insufficiency (CCSVI). He found CCSVI in all patients with MS in his study, performed a surgical procedure, later called in the media the "liberation procedure" to correct it, and claimed that 73% of participants improved. This theory received significant attention in the media and among those with MS, especially in Canada. Concerns have been raised with Zamboni's research as it was neither blinded nor controlled, and its assumptions about the underlying cause of the disease is not backed by known data. Also, further studies have either not found a similar relationship or found one that is much less strong one, raising serious objections to the hypothesis. The "liberation procedure" has been criticized for resulting in serious complications and deaths with unproven benefits. It is, thus, as of 2013 not recommended for the treatment of MS. Additional research investigating the CCSVI hypothesis are under way.

</doc>
<doc id="50604" url="http://en.wikipedia.org/wiki?curid=50604" title="Interacting boson model">
Interacting boson model

The interacting boson model (IBM) is a model in nuclear physics in which
nucleons (protons or neutrons) pair up, essentially
acting as a single particle with boson properties, with
integral spin of 0, 2 or 4.
The IBM-I treats both types of nucleons the same and considers only pairs of nucleons coupled to
total angular momentum 0 and 2, called respectively, s and d bosons. 
The IBM-II treats protons and neutrons separately.
History.
This model was invented by Akito Arima and Francesco Iachello.

</doc>
<doc id="50605" url="http://en.wikipedia.org/wiki?curid=50605" title="Cerebral palsy">
Cerebral palsy

Cerebral palsy (CP) is a group of permanent movement disorders that appear in early childhood. Signs and symptoms vary between people. Often problems include: poor coordination, stiff muscles, weak muscles, trouble swallowing or speaking, and tremors among others. There may also be problems with sensation, vision, and hearing. Often babies with CP do not roll over, sit, crawl, or walk as early as other children their age. Difficulty with the ability to think or reason and seizures each occurs in about one third of cases. While the symptoms may get more noticeable over the first few years of life, the underlying problems do not worsen over time.
Cerebral palsy is caused by abnormal development or damage to the parts of the brain that control movement, balance, and posture. Most often the problems occur during pregnancy; however, they may also occur during childbirth, or shortly after birth. Often the cause is unknown. Risk factors include premature birth, being a twin, certain infections during pregnancy such as toxoplasmosis or rubella, exposure to methylmercury during pregnancy, and head trauma during the first few years of life, among others. About 2% of cases are believed to be due to an inherited genetic cause. A number of sub-types are classified based on the specific problems present. For example those with stiff muscles have spastic cerebral palsy, those with poor coordination have ataxic cerebral palsy, and those with writhing movements have athetoid cerebral palsy. Diagnosis is based on the child's development over time. Blood tests and medical imaging may be used to rule out other possible causes.
CP is partly preventable through immunization of the mother and efforts to prevent head injuries in children such as through improved safety. There is no cure for CP; however, supportive treatments, medications, and surgery may help many individuals. This may include physical therapy and speech therapy. Medications such as diazepam, baclofen, and botulinum toxin may help relax stiff muscles. Surgery may include lengthening muscles and cutting overly active nerves. Often external braces and other assistive technology are helpful. Some children have near normal adult lives with appropriate treatment. While alternative medicines are frequently used there is no evidence to support their use.
CP is the most common movement disorder in children. It occurs in about 2.1 per 1,000 live births. Cerebral palsy has been documented throughout history with the first known descriptions occurring in the work of Hippocrates in the 5th century BCE. Extensive study of the condition began in the 19th century by William John Little. A number of potential treatments are being examined, including stem cell therapy. However, more research is required to determine if it is effective and safe.
Signs and symptoms.
Cerebral palsy is defined as "a group of permanent disorders of the development of movement and posture, causing activity limitation, that are attributed to non-progressive disturbances that occurred in the developing fetal or infant brain." While the central feature of CP is a disorder with movement, difficulties with thinking, learning, feeling, communication and behavior often occur along with cerebral palsy. Of those with CP, 28% have epilepsy, 58% have difficulties with communication, at least 42% have problems with their vision, and 23–56% have learning disabilities.
Cerebral palsy is characterized by abnormal muscle tone, reflexes, or motor development and coordination. There can be joint and bone deformities and contractures (permanently fixed, tight muscles and joints). The classical symptoms are spasticities, spasms, other involuntary movements (e.g., facial gestures), unsteady gait, problems with balance, and/or soft tissue findings consisting largely of decreased muscle mass. Scissor walking (where the knees come in and cross) and toe walking (which can contribute to a gait reminiscent of a marionette) are common among people with CP who are able to walk, but taken on the whole, CP symptomatology is very diverse. The effects of cerebral palsy fall on a continuum of motor dysfunction, which may range from slight clumsiness at the mild end of the spectrum to impairments so severe that they render coordinated movement virtually impossible at the other end of the spectrum.
Babies born with severe CP often have an irregular posture; their bodies may be either very floppy or very stiff. Birth defects, such as spinal curvature, a small jawbone, or a small head sometimes occur along with CP. Symptoms may appear or change as a child gets older. Some babies born with CP do not show obvious signs right away. Classically, CP becomes evident when the baby reaches the developmental stage at 61⁄2 to 9 months and is starting to mobilise, where preferential use of limbs, asymmetry, or gross motor developmental delay is seen.
Resulting conditions can include seizures, epilepsy, apraxia, dysarthria or other communication disorders, eating problems, sensory impairments, intellectual disability, learning disabilities, urinary incontinence, fecal incontinence, and/or behavioural disorders.
Language.
Speech and language disorders are common in people with cerebral palsy. The incidence of dysarthria is estimated to range from 31% to 88%. Speech problems are associated with poor respiratory control, laryngeal and velopharyngeal dysfunction, and oral articulation disorders that are due to restricted movement in the oral-facial muscles. There are three major types of dysarthria in cerebral palsy: spastic, dyskinetic (athetosis), and ataxic.
Overall language delay is associated with problems of intellectual disability, hearing impairment, and learned helplessness. Children with cerebral palsy are at risk of learned helplessness and becoming passive communicators, initiating little communication. Early intervention with this clientele, and their parents, often targets situations in which children communicate with others so that they learn that they can control people and objects in their environment through this communication, including making choices, decisions, and mistakes.
Skeleton.
In order for bones to attain their normal shape and size, they require the stresses from normal musculature. Osseous findings will therefore mirror the specific muscular deficits in a given person with CP. The shafts of the bones are often thin (gracile) and become thinner during growth. When compared to these thin shafts (diaphyses), the centres (metaphyses) often appear quite enlarged (ballooning). With lack of use, articular cartilage may atrophy, leading to narrowed joint spaces. Depending on the degree of spasticity, a person with CP may exhibit a variety of angular joint deformities. Because vertebral bodies need vertical gravitational loading forces to develop properly, spasticity and an abnormal gait can hinder proper and/or full bone and skeletal development. People with CP tend to be shorter in height than the average person because their bones are not allowed to grow to their full potential. Sometimes bones grow to different lengths, so the person may have one leg longer than the other.
Pain and sleep.
Pain is common and may result from the inherent deficits associated with the condition, along with the numerous procedures children typically face. Pain is associated with tight or shortened muscles, abnormal posture, stiff joints, unsuitable orthosis, etc. There is also a high likelihood of chronic sleep disorders secondary to both physical and environmental factors.
Eating.
Those with CP may have difficulty preparing food, holding utensils, or chewing and swallowing due to sensory and motor impairments. An infant with CP may not be able to suck, swallow or chew. Children with CP may have too little or too much sensitivity around and in the mouth. Fine finger dexterity, like that needed for picking up a utensil, is more frequently impaired than gross manual dexterity, like that needed for spooning food onto a plate. Grip strength impairments are less common.
Causes.
Cerebral palsy is due to damage occurring to the developing brain. This damage can occur during pregnancy, delivery, the first month of life, or less commonly in early childhood. Structural problems in the brain are seen in 80% of cases, most commonly within the white matter.
More than three quarters of cases are believed to result from issues that occur during pregnancy.
While in certain cases there is no identifiable cause, typical causes include problems in intrauterine development (e.g. exposure to radiation, infection), hypoxia of the brain, and birth trauma during labor and delivery, and complications around birth or during childhood.
Prematurity.
Between 40% and 50% of all children who develop cerebral palsy were born prematurely. Most of these cases (75-90%) are believed to be due to issues that occur around the time of birth, often just after birth. Multiple-birth infants are also more likely than single-birth infants to have CP. They are also more likely to be born with a low birth weight.
In those who are born with a weight between 1 kg and 1.5 kg CP occurs in 6%. Among those born before 28 weeks of gestation it occurs in 11%. Genetic factors are believed to play an important role in prematurity and cerebral palsy generally. While in those who are born between 34 and 37 weeks the risk is 0.4% (three times normal).
Term infants.
In babies that are born at term risk factors include: problems with the placenta, birth defects, low birthweight, breathing meconium into the lungs, a delivery requiring either the use of instruments or an emergency Caesarean section, birth asphyxia, seizures just after birth, respiratory distress syndrome, low blood sugar, and infections in the baby.
It is unclear how much of a role birth asphyxia plays as a cause. It is believed, however, that only a small number of cases are caused by lack of oxygen during birth. It is unclear if the size of the placenta plays a role.
Genetics.
About 2% of all CP cases are inherited, with glutamate decarboxylase-1 being one of the possible enzymes involved. Most inherited cases are autosomal recessive, meaning both parents must be carriers for the disorder in order to have a child with the disease.
Early childhood.
After birth, other causes include toxins, severe jaundice, lead poisoning, physical brain injury, shaken baby syndrome, incidents involving hypoxia to the brain (such as near drowning), and encephalitis or meningitis. The three most common causes of asphyxia in the young child are: choking on foreign objects such as toys and pieces of food, poisoning, and near drowning.
Other.
Infections in the mother, even infections that are not easily detected, may triple the risk of the child developing the disorder. Infections of the fetal membranes known as chorioamnionitis increases the risk.
Intrauterine and neonatal insults (many of which are infectious in nature) increase the risk.
It has been hypothesised that many cases of cerebral palsy are caused by the death in very early pregnancy of an identical twin.
Diagnosis.
The diagnosis of cerebral palsy has historically rested on the person's history and physical examination. A general movements assessment, which involves measuring movements that occur spontaneously among those less than four months of age, appears to be most accurate. It is a type of developmental delay.
Once a person is diagnosed with cerebral palsy, further diagnostic tests are optional. Neuroimaging with CT or MRI is warranted when the cause of a person's cerebral palsy has not been established – an MRI is preferred over CT due to diagnostic yield and safety. When abnormal, the neuroimaging study can suggest the timing of the initial damage. The CT or MRI is also capable of revealing treatable conditions, such as hydrocephalus, porencephaly, arteriovenous malformation, subdural hematomas and hygromas, and a vermian tumour (which a few studies suggest are present 5–22% of the time). Furthermore, an abnormal neuroimaging study indicates a high likelihood of associated conditions, such as epilepsy and intellectual disability.
The age at which CP is diagnosed is important, but there is disagreement over what is the best age to make the diagnosis. The earlier CP is diagnosed correctly, the better the opportunities are to provide the child with physical and educational help, but there also might be a greater chance that CP will be confused with another problem, especially if the child is 18 months of age or younger. Infants may have temporary problems with muscle tone or control that can be confused with CP, which is permanent. A metabolism disorder or tumors in the nervous system may also appear to be CP, and metabolic disorders in particular can produce brain problems that look like CP on an MRI. Disorders that deteriorate the white matter in the brain, and certain kinds of problems that cause spasms and weakness in the legs, may also be mistaken for CP when they first appear early in life. However, these disorders get worse over time, and CP does not (although it may change in character); in infancy it may not be possible to tell the difference between them. Fragile X syndrome (a cause of autism) and general intellectual disability must also be ruled out. Cerebral palsy specialist John McLaughlin recommends waiting until the child is 36 months of age before making a diagnosis.
Classification.
CP is classified by the types of motor impairment of the limbs or organs, and by restrictions to the activities an affected person may perform. There are three main CP classifications by motor impairment: spastic, ataxic, and athetoid/dyskinetic. Additionally there is a mixed type that shows a combination of features of the other types. These classifications also reflect the areas of the brain that are damaged.
Spastic.
Spastic cerebral palsy, or cerebral palsy where spasticity (muscle tightness) is the exclusive or almost-exclusive impairment present, is by far the most common type of overall cerebral palsy, occurring in upwards of 70% of all cases. People with this type of CP are hypertonic and have what is essentially a neuromuscular mobility impairment (rather than hypotonia or paralysis) stemming from an upper motor neuron lesion in the brain as well as the corticospinal tract or the motor cortex. This damage impairs the ability of some nerve receptors in the spine to receive "gamma"-Aminobutyric acid properly, leading to hypertonia in the muscles signaled by those damaged nerves.
As compared to other types of CP, and especially as compared to hypotonic or paralytic mobility disabilities, spastic CP is typically more easily manageable by the person affected, and medical treatment can be pursued on a multitude of orthopedic and neurological fronts throughout life. In any form of spastic CP, clonus of the affected limb(s) may sometimes result, as well as muscle spasms resulting from the pain and/or stress of the tightness experienced. The spasticity can and usually does also lead to very early onset of muscle-stress symptoms like arthritis and tendinitis, especially in ambulatory individuals in their mid-20s and early-30s. Occupational therapy and physical therapy regimens of assisted stretching, strengthening, functional tasks, and/or targeted physical activity and exercise are usually the chief ways to keep spastic CP well-managed, although if the spasticity is too much for the person to handle, other remedies may be considered, such as various antispasmodic medications, botox, baclofen, or even a neurosurgery known as a selective dorsal rhizotomy (which eliminates the spasticity by eliminating the nerves causing it).
Ataxic.
Ataxia-type symptoms can be caused by damage to the cerebellum. Ataxia is a less common type of cerebral palsy, occurring between 5% and 10% of all cases. Some of these individuals have hypotonia and tremors. Motor skills such as writing, typing, or using scissors might be affected, as well as balance, especially while walking. It is common for individuals to have difficulty with visual and/or auditory processing. They usually have an awkward gait and as well with some dysarthria.
Athetoid.
Athetoid cerebral palsy or dyskinetic cerebral palsy is mixed muscle tone – both hypertonia and hypotonia mixed with involuntary motions. People with dyskinetic CP have trouble holding themselves in an upright, steady position for sitting or walking, and often show involuntary motions. For some people with dyskinetic CP, it takes a lot of work and concentration to get their hand to a certain spot (like scratching their nose or reaching for a cup). Because of their mixed tone and trouble keeping a position, they may not be able to hold onto objects, especially small ones requiring fine motor control (such as a toothbrush or pencil). About 10% of individuals with CP are classified as dyskinetic CP but some have mixed forms with spasticity and dyskinesia. The damage occurs to the extrapyramidal motor system and/or pyramidal tract and to the basal ganglia. In newborn infants, high bilirubin levels in the blood, if left untreated, can lead to brain damage in the basal ganglia (kernicterus), which can lead to dyskinetic cerebral palsy.
Mixed.
Mixed cerebral palsy is symptoms of athetoid, ataxic and spastic CP appearing simultaneously, each to varying degrees, and both with and without certain symptoms of each. Mixed CP is the most difficult to treat as it is extremely heterogeneous and sometimes unpredictable in its symptoms and development over the lifespan.
Prevention.
In those at risk of an early delivery magnesium sulphate appears to decrease the risk of cerebral palsy. It is unclear if it helps those who are born at term.
Cooling high-risk full-term babies shortly after birth may reduce disability.
Management.
Over time, the approach to CP management has shifted away from narrow attempts to fix individual physical problems – such as spasticity in a particular limb – to making such treatments part of a larger goal of maximizing the person's independence and community engagement.:886 However, the evidence base for the effectiveness of intervention programs reflecting this philososphy has not yet caught up: effective interventions for body structures and functions have a strong evidence base, but evidence is lacking for effective interventions targeted toward participation, environment, or personal factors. There is also no good evidence to show that an intervention that is effective at the body-specific level will result in an improvement at the activity level, or vice versa. Although such cross-over benefit might happen, not enough high-quality studies have been done to demonstrate it.
Treatment of cerebral palsy is a lifelong process focused on the management of associated conditions. It tries to allow healthy development on all levels. The brain, up to about the age of 8, is not set and has the ability to reroute many signal paths that may have been affected by the initial trauma; the earlier it has help in doing this the more successful it will be.
The treatments with the best evidence are medications (anticonvulsants, botulinum toxin, bisphosphonates, diazepam), therapy (bimanual training, casting, constraint-induced movement therapy, context-focused therapy, fitness training, goal-directed training, hip surveillance, home programmes, occupational therapy after botulinum toxin, pressure care) and surgery (selective dorsal rhizotomy).
Various forms of therapy are available to people living with cerebral palsy as well as caregivers and parents. Treatment may include one or more of the following: physical therapy; occupational therapy; speech therapy; water therapy; drugs to control seizures, alleviate pain, or relax muscle spasms (e.g. benzodiazepines,); surgery to correct anatomical abnormalities or release tight muscles; braces and other orthotic devices; rolling walkers; and communication aids such as computers with attached voice synthesisers.
However, there is only some benefit from therapy. Treatment is usually symptomatic and focuses on helping the person to develop as many motor skills as possible or to learn how to compensate for the lack of them. Non-speaking people with CP are often successful availing themselves of augmentative and alternative communication.
Therapy.
Physiotherapy programs are designed to encourage the patient to build a strength base for improved gait and volitional movement, together with stretching programs to limit contractures. Many experts believe that lifelong physiotherapy is crucial to maintain muscle tone, bone structure, and prevent dislocation of the joints.
Speech therapy helps control the muscles of the mouth and jaw, and helps improve communication. Just as CP can affect the way a person moves their arms and legs, it can also affect the way they move their mouth, face and head. This can make it hard for the person to breathe; talk clearly; and bite, chew and swallow food. Speech therapy often starts before a child begins school and continues throughout the school years.
Conductive education (CE) was developed in Hungary from 1945 based on the work of András Pető. It is a unified system of rehabilitation for people with neurological disorders including cerebral palsy, Parkinson's disease and multiple sclerosis, amongst other conditions. It is theorised to improve mobility, self-esteem, stamina and independence as well as daily living skills and social skills. The conductor is the professional who delivers CE in partnership with parents and children. Skills learned during CE should be applied to everyday life and can help to develop age-appropriate cognitive, social and emotional skills. It is available at specialised centres.
Biofeedback is a therapy in which people learn how to control their affected muscles. Biofeedback therapy has been found to significantly improve gait in children with cerebral palsy.
Massage therapy is designed to help relax tense muscles, strengthen muscles, and keep joints flexible. More research is needed to determine the health benefits of these therapies for people with CP.
Occupational therapy helps adults and children maximise their function, adapt to their limitations and live as independently as possible. A family-centred philosophy is used with children who have CP. Occupational therapists work closely with families in order to address their concerns and priorities for their child.
CP commonly causes hemiplegia. Those with hemiplegia have limited use of the limbs on one side of the body, and have normal use of the limbs on the other side. Hemiplegics often adapt by ignoring the limited limbs, and performing nearly all activities with the unaffected limbs, which can lead to increased problems with muscle tone, motor control and range of motion. An emerging technique called constraint-induced movement therapy (CIMT) is designed to address this. In CIMT, the unaffected limbs are constrained, forcing the individual to learn to use the affected limbs. There is limited, preliminary evidence that CIMT is effective, but more study is needed before it can be recommended with confidence.
Medication.
Botulinum toxin injections are given into muscles that are spastic or sometimes dystonic, the aim being to reduce the muscle hypertonus that can be painful. A reduction in muscle tone can also facilitate bracing and the use of orthotics. Most often lower extremity muscles are injected. Botulinum toxin is focal treatment, meaning that a limited number of muscles can be injected at the same time. The effect of the toxin is reversible and a reinjection is needed every 4–6 months.
Surgery.
Surgery usually involves one or a combination of:
Other surgical procedures are available to try to help with other problems. Those who have serious difficulties with eating may undergo a procedure called a gastrostomy: a hole is cut through the belly skin and into the stomach to allow for a feeding tube. There is no good evidence about the effectiveness or safety of gastrostomy.
Orthotics.
Orthotic devices such as ankle-foot orthoses (AFOs) are often prescribed to achieve the following objectives: correct and/or prevent deformity, provide a base of support, facilitate training in skills, and improve the efficiency of gait.
The available evidence suggests that orthoses can have positive effects on all temporal and spatial parameters of gait, i.e. velocity, cadence, step length, stride length, single and double support. AFOs have also been found to reduce energy expenditure.
Assistive technology.
There are now assitive technologies designed to help when dealing with cerebral palsy, most often to aid with meal times. There are manual feeding aids, for example one designed using viscous fluid damping to smooth out essential tremors associated with cerebral palsy. There are also electronic feeding aids on the market suitable for anyone who can chew and swallow but unable to feed themselves. There are manual drinking aids available, designed with non-return valves and holders or non slip surfaces, to enable users who lack the ability to drink from a cup or glass, due to tremors, weakness or limited head mobility, to gain a route to hydration and nutrition.
Other.
Hyperbaric oxygen therapy (HBOT), in which pressurised oxygen is inhaled inside a hyperbaric chamber, has been studied under the theory that improving oxygen availability to damaged brain cells can reactivate some of them to function normally. However, HBOT results in no significant difference from that of pressurised room air, and some children undergoing HBOT may experience adverse events such as seizures and the need for ear pressure equalisation tubes.
Patterning is a controversial form of alternative therapy for people with CP. The method is promoted by The Institutes for the Achievement of Human Potential (IAHP), a Philadelphia nonprofit organisation, but has been criticised by the American Academy of Pediatrics.
Prognosis.
CP is not a progressive disorder (meaning the brain damage does not worsen), but the symptoms can become more severe over time. A person with the disorder may improve somewhat during childhood if he or she receives extensive care, but once bones and musculature become more established, orthopedic surgery may be required. The full intellectual potential of a child born with CP will often not be known until the child starts school. People with CP are more likely to have learning disabilities, although these may be unrelated to IQ, and are more likely to show varying degrees of intellectual disability. Intellectual level among people with CP varies from genius to intellectually impaired, as it does in the general population, and experts have stated that it is important not to underestimate the capabilities of a person with CP and to give them every opportunity to learn.
The life expectancy of those with CP is less than that of the general population but has improved with the utilization of modern medicine.
The ability to live independently with CP varies widely, depending partly on the severity of each person's impairment and partly on the capability of each person to self-manage the logistics of life. Some individuals with CP require personal assistant services for all activities of daily living. Others only need assistance with certain activities, and still others do not require any physical assistance. But regardless of the severity of a person's physical impairment, a person's ability to live independently often depends primarily on the person's capacity to manage the physical realities of his or her life autonomously. In some cases, people with CP recruit, hire, and manage a staff of personal care assistants (PCAs). PCAs facilitate the independence of their employers by assisting them with their daily personal needs in a way that allows them to maintain control over their lives.
People with CP can usually expect to have a normal life expectancy; survival has been shown to be associated with the ability to ambulate, roll, and self-feed. As the condition does not affect reproductive function, people with CP can have children and parent successfully.
Self-care.
Self-care is any activity people do to care for themselves. For many children with CP, parents are heavily involved in self-care activities. Self-care activities, such as bathing, dressing, grooming, can be difficult for children with CP as self-care depends primarily on use of the upper limbs. For those living with CP, impaired upper limb function affects almost 50% of children and is considered the main factor contributing to decreased activity and participation. As the hands are used for many self-care tasks, sensory and motor impairments of the hands make daily self-care more difficult. Motor impairments cause more problems than sensory impairments. The most common impairment is that of finger dexterity, which is the ability to manipulate small objects with the fingers.
Productivity.
The effects of sensory, motor and cognitive impairments affect self-care occupations in children with CP and productivity occupations. Productivity can include, but is not limited to, school, work, household chores and contributing to the community.
Play is included as a productive occupation as it is often the primary activity for children. If play becomes difficult due to a disability, like CP, this can cause problems for the child. These difficulties can affect a child's self-esteem. In addition, the sensory and motor problems experienced by children with CP affect how the child interacts with their surroundings, including the environment and other people. Not only do physical limitations affect a child's ability to play, the limitations perceived by the child's caregivers and playmates also impact the child's play activities. Some children with disabilities spend more time playing by themselves. When a disability prevents a child from playing, there may be social, emotional and psychological problems which can lead to increased dependence on others, less motivation and poor social skills.
In school, students are asked to complete many tasks and activities, many of which involve handwriting. Many children with CP have the capacity to learn and write in the school environment. However, students with CP may find it difficult to keep up with the handwriting demands of school and their writing may be difficult to read. In addition, writing may take longer and require greater effort on the student's part. Factors linked to handwriting include postural stability, sensory and perceptual abilities of the hand, and writing tool pressure.
Speech impairments may be seen in children with CP depending on the severity of brain damage. Communication in a school setting is important because communicating with peers and teachers is very much a part of the "school experience" and enhances social interaction. Problems with language or motor dysfunction can lead to underestimating a student's intelligence. In summary, children with CP may experience difficulties in school, such as difficulty with handwriting, carrying out school activities, communicating verbally and interacting socially.
Leisure.
Leisure occupations are any activities that are done for enjoyment. Enjoyable activities depend on one's personality and environment. Leisure activities can have several positive effects on physical health, mental health, life satisfaction and psychological growth for people with physical disabilities like CP. Common benefits identified are stress reduction, development of coping skills, companionship, enjoyment, relaxation and a positive effect on life satisfaction. In addition, for children with CP, leisure appears to enhance adjustment to living with a disability.
Leisure can be divided into structured (formal) and unstructured (informal) activities. Studies show that children with disabilities, like CP, participate mainly in informal activities that are carried out in the family environment and are organised by adults. Typically, children with disabilities carry out leisure activities by themselves or with their parents rather than with friends. Therefore, children may experience limited diversity of activities and social engagements, as well as a more passive lifestyle than their peers. Although leisure is important for children with CP, they may have difficulties carrying out leisure activities due to social and physical barriers.
Participation and barriers.
Participation is involvement in life situations and everyday activities. Participation includes the domains of self-care, productivity and leisure. In fact, communication, mobility, education, home life, leisure and social relationships require participation and are indicators of the extent to which a child functions in his or her environment. Barriers can exist on three levels: micro, meso and macro. First, the barriers at the micro level involve the person. Barriers at the micro level include the child's physical limitations (motor, sensory and cognitive impairments) or their subjective feelings regarding their ability to participate. For example, the child may not participate in group activities due to lack of confidence. Second, barriers at the meso level include the family and community. These may include negative attitudes of people toward disability or lack of support within the family or in the community. One of the main reasons for this limited support appears to be the result of a lack of awareness and knowledge regarding the child's ability to engage in activities despite his or her disability. Third, barriers at the macro level incorporate the systems and policies that are not in place or hinder children with CP. These may be environmental barriers to participation such as architectural barriers, lack of relevant assistive technology and transportation difficulties due to limited wheelchair access or public transit that can accommodate children with CP. For example, a building without an elevator will prevent the child from accessing higher floors.
Epidemiology.
Cerebral palsy occurs in about 2.1 per 1000 live births. In those born at term rates are lower at 1 per 1000 live births. Rates appear to be similar in both the developing and developed world. The rate is higher in males than in females; in Europe it is 1.3 times more common in males. Variances in reported rates of incidence or prevalence across different geographical areas in industrialised countries are thought to be caused primarily by discrepancies in the criteria used for inclusion and exclusion. When such discrepancies are taken into account in comparing two or more registers of patients with cerebral palsy (for example, the extent to which children with mild cerebral palsy are included), the prevalence rates converge toward the average rate of 2:1000.
Overall, advances in care of pregnant mothers and their babies has not resulted in a noticeable decrease in CP. This is generally attributed to medical advances in areas related to the care of premature babies (which results in a greater survival rate). Only the introduction of quality medical care to locations with less-than-adequate medical care has shown any decreases. The incidence of CP increases with premature or very low-weight babies regardless of the quality of care.
Prevalence of cerebral palsy is best calculated around the school entry age of about 6 years, the prevalence in the U.S. is estimated to be 2.4 out of 1000 children.
History.
Cerebral palsy has affected humans since antiquity. A decorated grave marker dating from around the 15th to 14th century BCE shows a figure with one small leg and using a crutch, possibly due to cerebral palsy. The oldest likely physical evidence of the condition comes from the mummy of Siptah, an Egyptian Pharaoh who ruled from about 1196 to 1190 BCE and died at about 20 years of age. The presence of cerebral palsy has been suspected due to his deformed foot and hands.
The medical literature of the ancient Greeks discusses paralysis and weakness of the arms and legs; the modern word "palsy" comes from the Ancient Greek words "παράλυση" or "πάρεση", meaning paralysis or paresis respectively. The works of the school of Hippocrates (460–c. 370 BCE), and the manuscript "On the Sacred Disease" in particular, describe a group of problems that matches up very well with the modern understanding of cerebral palsy. The Roman Emperor Claudius (10 BCE–54 CE) is suspected of having CP, as historical records describe him as having several physical problems in line with the condition. Medical historians have begun to suspect and find depictions of CP in much later art. Several paintings from the 16th century and later show individuals with problems consistent with it, such as Jusepe de Ribera's 1642 painting "The Clubfoot".
The modern understanding of CP as resulting from problems within the brain began in the early decades of the 1800s with a number of publications on brain abnormalities by Johann Christian Reil, Claude François Lallemand and Philippe Pinel. Later physicians used this research to connect problems in the brain with specific symptoms. The English surgeon William John Little (1810–1894) was the first person to study CP extensively. In his doctoral thesis he stated that CP was a result of a problem around the time of birth. He later identified a difficult delivery, a preterm birth and perinatal asphyxia in particular as risk factors. The spastic diplegia form of CP came to be known as Little's Disease. In the 1880s British neurologist William Gowers built on Little's work by linking paralysis in newborns to difficult births. He named the problem "birth palsy" and classified birth palsies into two types: peripheral and cerebral.
Working in Pennsylvania in the 1880s, Canadian-born physician William Osler (1849–1919) reviewed dozens of CP cases to further classify the disorders by the site of the problems on the body and by the underlying cause. Osler made further observations tying problems around the time of delivery with CP, and concluded that problems causing bleeding inside the brain were likely the root cause. Osler also suspected polioencephalitis as an infectious cause. Through the 1890s, scientists commonly confused CP with polio.
Before moving to psychiatry, Austrian neurologist Sigmund Freud (1856–1939) made further refinements to the classification of the disorder. He produced the system still being used today. Freud's system divides the causes of the disorder into problems present at birth, problems that develop during birth, and problems after birth. Freud also made a rough correlation between the location of the problem inside the brain and the location of the affected limbs on the body, and documented the many kinds of movement disorders.
In the early 20th century, the attention of the medical community generally turned away from CP until orthopedic surgeon Winthrop Phelps became the first physician to treat the disorder. He viewed CP from a musculoskeletal perspective instead of a neurological one. Phelps developed surgical techniques for operating on the muscles to address issues such as spasticity and muscle rigidity. Hungarian physical rehabilitation practitioner András Pető developed a system to teach children with CP how to walk and perform other basic movements. Pető's system became the foundation for conductive education, widely used for children with CP today. Through the remaining decades, physical therapy for CP has evolved, and has become a core component of the CP management program.
In 1997, Robert Palisano "et al." introduced the Gross Motor Function Classification System (GMFCS) as an improvement over the previous rough assessment of limitation as either mild, moderate or severe. The GMFCS grades limitation based on observed proficiency in specific basic mobility skills such as sitting, standing and walking, and takes into account the level of dependency on aids such as wheelchairs or walkers. The GMFCS was further revised and expanded in 2007.
Society and culture.
Economic impact.
Access Economics has released a report on the economic impact of cerebral palsy in Australia. Launched by the Hon. Bill Shorten, MP, the report found that, in 2007, the financial cost of cerebral palsy (CP) in Australia was $1.47 billion or 0.14% of GDP. Of this:
The value of lost well-being (disability and premature death) was a further $2.4 billion.
In per capita terms, this amounts to a financial cost of $43,431 per person with CP per annum. Including the value of lost well-being, the cost is over $115,000 per person per annum.
Individuals with CP bear 37% of the financial costs, and their families and friends bear a further 6%. Federal government bears around one third (33%) of the financial costs (mainly through taxation revenues forgone and welfare payments). State governments bear under 1% of the costs, while employers bear 5% and the rest of society bears the remaining 19%. If the burden of disease (lost well-being) is included, individuals bear 76% of the costs.
The average lifetime cost for people with CP in the US is $921,000 per individual, including lost income.
In the United States many states allow Medicaid beneficiaries to use their Medicaid funds to hire their own PCAs, instead of forcing them to use institutional or managed care.
In India the government sponsored program called 'NIRAMAYA' for the medical care of children with neurological and muscular deformities has proved to be an ameliorating economic measure for persons with such disabilities. It has been seen that persons with mental or physically debilitating congenital disabilities can lead better lives if they have financial independence. In cases of financial penury such persons face extreme living conditions.
Use of the term.
The term "palsy" in modern language refers to disorder of movement, but the word root "palsy" technically means "paralysis", even though it is not used as such within the meaning of cerebral palsy. The use of "palsy" in the term cerebral palsy makes it important to note that paralytic disorders are in fact "not" cerebral palsy – meaning that the condition of quadriplegia, which comes from spinal cord injury or traumatic brain injury, should not be confused with spastic quadriplegia, which doesn't, nor should tardive dyskinesia be confused with dyskinetic cerebral palsy or the condition of (paralytic) "diplegia" with spastic diplegia. In fact, as of the early 21st century some clinicians have become so distressed at common incorrect use of these terms that they have resorted to new naming schemes rather than trying to reclaim the classic ones; one such example of this evolution is the increasing use of the term "bilateral spasticity" to refer to "spastic diplegia". Such clinicians even argue quite often that the "new" term is technically more clinically accurate than the established term.
Many people would rather be referred to as a person with a disability instead of handicapped. "Cerebral Palsy: A Guide for Care" at the University of Delaware offers the following guidelines:
 Impairment is the correct term to use to define a deviation from normal, such as not being able to make a muscle move or not being able to control an unwanted movement. Disability is the term used to define a restriction in the ability to perform a normal activity of daily living which someone of the same age is able to perform. For example, a three-year-old child who is not able to walk has a disability because a normal three-year-old can walk independently. A handicapped child or adult is one who, because of the disability, is unable to achieve the normal role in society commensurate with his age and socio-cultural milieu. As an example, a sixteen-year-old who is unable to prepare his own meal or care for his own toilet or hygiene needs is handicapped. On the other hand, a sixteen-year-old who can walk only with the assistance of crutches but who attends a regular school and is fully independent in activities of daily living is disabled but not handicapped. All disabled people are impaired, and all handicapped people are disabled, but a person can be impaired and not necessarily be disabled, and a person can be disabled without being handicapped.
The term "spastic" denotes the attribute of spasticity in types of spastic CP. In 1952 a UK charity called The Spastics Society was formed. The term "spastics" was used by the charity as a term for people with CP. The word "spastic" has since been used extensively as a general insult to disabled people, which some see as extremely offensive. They are also frequently used to insult able-bodied people when they seem overly uncoordinated, anxious, or unskilled in sports. The charity changed its name to Scope in 1994. In the United States the word spaz has the same usage as an insult, but is not generally associated with CP.
It is not taken as derogatory in the Indian context. Rather "spasticity" and "cerebral palsy" are used interchangeably. The term is widely used to connote cerebral palsy and is accepted for usage in medical fraternity as well as in social life. Many organisations known as "Spastic Societies" viz. Spastic Society of Gurgaon are working in different areas in India as charitable bodies for people with cerebral palsy, in care-taking, rehabilitation and medical support of children with neurological muscular development disabilities.
Media.
Maverick documentary filmmaker Kazuo Hara criticises the mores and customs of Japanese society in an unsentimental portrait of adults with cerebral palsy in his 1972 film "Goodbye CP" (Sayonara CP). Focusing on how the CP victims are generally ignored or disregarded in Japan, Hara challenges his society's taboos about physical handicaps. Using a deliberately harsh style, with grainy black-and-white photography and out-of-sync sound, Hara brings a stark realism to his subject.
"Spandan" (2012), a film by Vegitha Reddy and Aman Tripathi, delves into the dilemma of parents whose child suffers from cerebral palsy. While films made with children with special needs as central characters have been attempted before, the predicament of parents dealing with the stigma associated with condition and beyond is dealt in "Spandan". In one of the songs of "Spandan" "Chal chaal chaal tu bala" more than 50 CP kids have acted. The famous classical singer Devaki Pandit has given her voice to the song penned by Prof. Jayant Dhupkar and composed by National Film Awards winner Isaac Thomas Kottukapally.
"My Left Foot" (1989), is a drama film directed by Jim Sheridan and starring Daniel Day-Lewis. It tells the true story of Christy Brown, an Irishman born with cerebral palsy, who could control only his left foot. Christy Brown grew up in a poor, working-class family, and became a writer and artist. It won the Academy Award for Best Actor (Daniel Day-Lewis) and Best Actress in a Supporting Role (Brenda Fricker). It was also nominated for Best Director, Best Picture and Best Writing, Screenplay Based on Material from Another Medium. It also won the New York Film Critics Circle Award for Best Film for 1989.
Research.
Stem cell therapy is being studied as a treatment. A potential treatment for some forms of cerebral palsy may be deep brain stimulation.
References.
<div class="reflist columns references-column-count references-column-count-Institute for Middle East Understanding]</ref>
" style="-moz-column-count: Institute for Middle East Understanding]</ref>

</doc>
<doc id="50608" url="http://en.wikipedia.org/wiki?curid=50608" title="Parkinsonism">
Parkinsonism

Parkinsonism is a clinical syndrome characterized by tremor, bradykinesia, rigidity, and postural instability. Parkinsonism shares symptoms found in Parkinson's disease, from which it is named; but parkinsonism is a symptom complex, and differs from Parkinson disease which is a progressive neurodegenerative illness. The underlying causes of parkinsonism are numerous, and diagnosis can be complex. The neurodegenerative condition Parkinson's disease (PD) is the most common cause of parkinsonism. However, a wide range of other etiologies may lead to a similar set of symptoms, including some toxins, a few metabolic diseases, and a handful of neurological conditions other than Parkinson's.
About 7% of people with parkinsonism have developed their symptoms following treatment with particular medications. Side effect of medications, mainly neuroleptic antipsychotics especially the phenothiazines (such as perphenazine and chlorpromazine), thioxanthenes (such as flupenthixol and zuclopenthixol) and butyrophenones (such as haloperidol (Haldol)), piperazines (such as ziprasidone), and, rarely, antidepressants. The incidence of drug-induced parkinsonism increases with age. Drug induced parkinsonism tends to remain at its presenting level, i.e. does not progress like the parkinson disease.
Differential diagnoses.
Before Parkinson's disease is diagnosed the differential diagnoses include:

</doc>
<doc id="50609" url="http://en.wikipedia.org/wiki?curid=50609" title="Nuclear shell model">
Nuclear shell model

In nuclear physics and nuclear chemistry, the nuclear shell model is a model of the atomic nucleus which uses the Pauli exclusion principle to describe the structure of the nucleus in terms of energy levels. The first shell model was proposed by Dmitry Ivanenko (together with E. Gapon) in 1932. The model was developed in 1949 following independent work by several physicists, most notably Eugene Paul Wigner, Maria Goeppert-Mayer and J. Hans D. Jensen, who shared the 1963 Nobel Prize in Physics for their contributions.
The shell model is partly analogous to the atomic shell model which describes the arrangement of electrons in an atom, in that a filled shell results in greater stability. When adding nucleons (protons or neutrons) to a nucleus, there are certain points where the binding energy of the next nucleon is significantly less than the last one. This observation, that there are certain magic numbers of nucleons: 2, 8, 20, 28, 50, 82, 126 which are more tightly bound than the next higher number, is the origin of the shell model.
The shells for protons and for neutrons are independent of each other. Therefore, one can have "magic nuclei" where one nucleon type or the other is at a magic number, and "doubly magic nuclei", where both are. Due to some variations in orbital filling, the upper magic numbers are 126 and, speculatively, 184 for neutrons but only 114 for protons, playing a role in the search for the so-called island of stability. Some semimagic numbers have been found, notably Z=40 giving nuclear shell filling for the various elements; 16 may also be a magic number.
In order to get these numbers, the nuclear shell model starts from an average potential with a shape something between the square well and the harmonic oscillator. To this potential a spin orbit term is added. Even so, the total perturbation does not coincide with experiment, and an empirical spin orbit coupling, named the Nilsson Term, must be added with at least two or three different values of its coupling constant, depending on the nuclei being studied.
Nevertheless, the magic numbers of nucleons, as well as other properties, can be arrived at by approximating the model with a plus a spin-orbit interaction. A more realistic but also complicated potential is known as Woods Saxon potential.
Igal Talmi developed a method to obtain the information from experimental data and use it to calculate and predict energies which have not been measured. This method has been successfully used by many nuclear physicists and has led to deeper understanding of nuclear structure. The theory which gives a good description of these properties was developed. This description turned out to furnish the shell model basis of the elegant and successful Interacting boson model.
Deformed harmonic oscillator approximated model.
Consider a . This would give, for example, in the first two levels ("l" is angular momentum)
We can imagine ourselves building a nucleus by adding protons and neutrons. These will always fill the lowest available level. Thus the first two protons fill level zero, the next six protons fill level one, and so on. As with electrons in the periodic table, protons in the outermost shell will be relatively loosely bound to the nucleus if there are only few protons in that shell, because they are farthest from the center of the nucleus. Therefore nuclei which have a full outer proton shell will have a higher binding energy than other nuclei with a similar total number of protons. All this is true for neutrons as well.
This means that the magic numbers are expected to be those in which all occupied shells are full. We see that for the first two numbers we get 2 (level 0 full) and 8 (levels 0 and 1 full), in accord with experiment. However the full set of magic numbers does not turn out correctly. These can be computed as follows:
In particular, the first six shells are:
where for every "l" there are 2"l"+1 different values of "ml" and 2 values of "ms", giving a total of 4"l"+2 states for every specific level.
These numbers are twice the values of triangular numbers from the Pascal Triangle: 1,3,6,10,15,21...
Including a spin-orbit interaction.
We next include a spin-orbit interaction. First we have to describe the system by the quantum numbers "j", "mj" and parity instead of "l", "ml" and "ms", as in the Hydrogen-like atom. Since every even level includes only even values of "l", it includes only states of even (positive) parity; Similarly every odd level includes only states of odd (negative) parity. Thus we can ignore parity in counting states. The first six shells, described by the new quantum numbers, are
where for every "j" there are different states from different values of "mj".
Due to the spin-orbit interaction the energies of states of the same level but with different "j" will no longer be identical. This is because in the original quantum numbers, when formula_4 is parallel to formula_5, the interaction energy is positive; and in this case "j" = "l" + "s" = "l" + 1⁄2. When formula_4 is anti-parallel to formula_5 (i.e. aligned oppositely), the interaction energy is negative, and in this case . Furthermore, the strength of the interaction is roughly proportional to "l".
For example, consider the states at level 4:
Deforming the potential.
The harmonic oscillator potential formula_8 grows infinitely as the distance from the center "r" goes to infinity. A more realistic potential, such as Woods Saxon potential, would approach a constant at this limit. One main consequence is that the average radius of nucleons' orbits would be larger in a realistic potential; This leads to a reduced term formula_9 in the Laplace operator of the Hamiltonian. Another main difference is that orbits with high average radii, such as those with high "n" or high "l", will have a lower energy than in a harmonic oscillator potential. Both effects lead to a reduction in the energy levels of high "l" orbits.
Predicted magic numbers.
Together with the spin-orbit interaction, and for appropriate magnitudes of both effects, one is led to the following qualitative picture: At all levels, the highest "j" states have their energies shifted downwards, especially for high "n" (where the highest "j" is high). This is both due to the negative spin-orbit interaction energy and to the reduction in energy resulting from deforming the potential to a more realistic one. The second-to-highest "j" states, on the contrary, have their energy shifted up by the first effect and down by the second effect, leading to a small overall shift. The shifts in the energy of the highest "j" states can thus bring the energy of states of one level to be closer to the energy of states of a lower level. The "shells" of the shell model are then no longer identical to the levels denoted by "n", and the magic numbers are changed.
We may then suppose that the highest "j" states for "n" = 3 have an intermediate energy between the average energies of "n" = 2 and "n" = 3, and suppose that the highest "j" states for larger "n" (at least up to "n" = 7) have an energy closer to the average energy of . Then we get the following shells (see the figure)
and so on.
The magic numbers are then
and so on. This gives all the observed magic numbers, and also predicts a new one (the so-called "island of stability") at the value of 184 (for protons, the magic number 126 has not been observed yet, and more complicated theoretical considerations predict the magic number to be 114 instead).
Another way to predict magic (and semi-magic) numbers is by laying out the idealized filling order (with spin-orbit splitting but energy levels not overlapping). For consistency s is split into j = 1⁄2 and j = -1⁄2 components with 2 and 0 members respectively. Taking leftmost and rightmost total counts within sequences marked bounded by / here gives the magic and semi-magic numbers.
The rightmost predicted magic numbers of each pair within the quartets bisected by / are double tetrahedral numbers from the Pascal Triangle: 2,8,20,40,70,112,168,240 are 2x 1,4,10,20,35,56,84,120..., and the leftmost members of the pairs differ from the rightmost by double triangular numbers: 2-2=0, 8-6=2, 20-14=6, 40-28=12, 70-50=20, 112-82=30, 168-126=42, 240-184=56, where 0,2,6,12,20,30,42,56... are 2x 0,1,3,6,10,15,21,28...
Other properties of nuclei.
This model also predicts or explains with some success other properties of nuclei, in particular spin and parity of nuclei ground states, and to some extent their excited states as well. Take 178O as an example — its nucleus has eight protons filling the three first proton 'shells', eight neutrons filling the three first neutron 'shells', and one extra neutron. All protons in a complete proton shell have total angular momentum zero, since their angular momenta cancel each other; The same is true for neutrons. All protons in the same level ("n") have the same parity (either +1 or −1), and since the parity of a pair of particles is the product of their parities, an even number of protons from the same level ("n") will have +1 parity. Thus the total angular momentum of the eight protons and the first eight neutrons is zero, and their total parity is +1. This means that the spin (i.e. angular momentum) of the nucleus, as well as its parity, are fully determined by that of the ninth neutron. This one is in the first (i.e. lowest energy) state of the 4th shell, which is a d-shell ("l" = 2), and since formula_10, this gives the nucleus an overall parity of +1. This 4th d-shell has a "j" = 5⁄2, thus the nucleus of 178O is expected to have positive parity and total angular momentum 5⁄2, which indeed it has.
The rules for the ordering of the nucleus shells are similar to Hund's Rules of the atomic shells, however, unlike its use in atomic physics the completion of a shell is not signified by reaching the next "n", as such the shell model cannot accurately predict the order of excited nuclei states, though it is very successful in predicting the ground states. The order of the first few terms are listed as follows: 1s, 1p3⁄2, 1p1⁄2, 1d5⁄2, 2s, 1d3⁄2... For further clarification on the notation refer to the article on the Russell-Saunders term symbol.
For nuclei farther from the magic numbers one must add the assumption that due to the relation between the strong nuclear force and angular momentum, protons or neutrons with the same "n" tend to form pairs of opposite angular momenta. Therefore a nucleus with an even number of protons and an even number of neutrons has 0 spin and positive parity. A nucleus with an even number of protons and an odd number of neutrons (or vice versa) has the parity of the last neutron (or proton), and the spin equal to the total angular momentum of this neutron (or proton). By "last" we mean the properties coming from the highest energy level.
In the case of a nucleus with an odd number of protons and an odd number of neutrons, one must consider the total angular momentum and parity of both the last neutron and the last proton. The nucleus parity will be a product of theirs, while the nucleus spin will be one of the possible results of the sum of their angular momenta (with other possible results being excited states of the nucleus).
The ordering of angular momentum levels within each shell is according to the principles described above - due to spin-orbit interaction, with high angular momentum states having their energies shifted downwards due to the deformation of the potential (i.e. moving from a harmonic oscillator potential to a more realistic one). For nucleon pairs, however, it is often energetically favorable to be at high angular momentum, even if its energy level for a single nucleon would be higher. This is due to the relation between angular momentum and the strong nuclear force.
Nuclear magnetic moment is partly predicted by this simple version of the shell model. The magnetic moment is calculated through "j", "l" and "s" of the "last" nucleon, but nuclei are not in states of well defined "l" and "s". Furthermore, for odd-odd nuclei, one has to consider the two "last" nucleons, as in deuterium. Therefore one gets several possible answers for the nuclear magnetic moment, one for each possible combined "l" and "s" state, and the real state of the nucleus is a superposition of them. Thus the real (measured) nuclear magnetic moment is somewhere in between the possible answers.
The electric dipole of a nucleus is always zero, because its ground state has a definite parity, so its matter density (formula_11, where formula_12 is the wavefunction) is always invariant under parity. This is usually the situations with the atomic electric dipole as well.
Higher electric and magnetic multipole moments cannot be predicted by this simple version of the shell model, for the reasons similar to those in the case of the deuterium.
Alpha particle model.
A model derived from the nuclear shell model is the alpha particle model developed by Henry Margenau, Edward Teller, J. K. Pering, T.H. Skyrme.
Recent Lecture.
Internet streaming broadcasting both on WM and QT at (at 64 kbit/s, 256 kbit/s, 1 Mbit/s ) and DVD ISO (NTSC and PAL) delivery are now available at RIKEN Nishina Center.

</doc>
<doc id="50614" url="http://en.wikipedia.org/wiki?curid=50614" title="Pope Leo XIII">
Pope Leo XIII

Pope Leo XIII (2 March 1810 – 20 July 1903), born Vincenzo Gioacchino Raffaele Luigi Pecci to an Italian comital family, reigned from 20 February 1878 to his death in 1903. He was the oldest pope (reigning until the age of 93), and had the third longest pontificate, behind that of Pius IX (his immediate predecessor) and John Paul II. He is the most recent pontiff to date to take the pontifical name of "Leo" upon being elected to the pontificate.
He is well known for his intellectualism, the development of social teachings with his famous papal encyclical "Rerum novarum" and his attempts to define the position of the Catholic Church with regard to modern thinking. He influenced Roman Catholic Mariology and promoted both the rosary and the scapular. He issued a record eleven encyclicals on the rosary earning the moniker the "Rosary Pope", approved two new Marian scapulars and was the first pope to fully embrace the concept of Mary as mediatrix. He was the first pope to never have held any control over the Papal States, after they were dissolved by 1870.
Leo XIII died on July 20, 1903 at the age of 93 and was briefly buried in the grottos of Saint Peter's Basilica before his remains were later transferred to the Basilica of Saint John Lateran.
Early life.
Born in Carpineto Romano, near Rome, he was the sixth of the seven sons of Count Ludovico Pecci and his wife Anna Prosperi Buzzi. His brothers included Giuseppe and Giovanni Battista Pecci. Until 1818 he lived at home with his family, "in which religion counted as the highest grace on earth, as through her, salvation can be earned for all eternity". Together with his brother Giuseppe, he studied in the Jesuit College in Viterbo, where he stayed until 1824. He enjoyed the Latin language and was known to write his own Latin poems at the age of eleven.
In 1824 he and his older brother Giuseppe were called to Rome where their mother was dying. Count Pecci wanted his children near him after the loss of his wife, and so they stayed with him in Rome, attending the Jesuit Collegium Romanum. In 1828, Giuseppe entered the Jesuit order, while Vincenzo decided in favour of secular clergy.
He studied at the "Academia dei Nobili", mainly diplomacy and law. In 1834 he gave a student presentation, attended by several cardinals, on "papal judgements". For his presentation he received awards for academic excellence, and gained the attention of Vatican officials. Cardinal Secretary of State Luigi Lambruschini introduced him to Vatican congregations. During a cholera epidemic in Rome he ably assisted Cardinal Sala in his duties as overseer of all the city hospitals. Pope Gregory XVI appointed Pecci on 14 February 1837, as personal prelate even before he was ordained priest on 31 December 1837, by the Vicar of Rome, Cardinal Carlo Odescalchi. He celebrated his first mass together with his priest brother Giuseppe. He received his doctorate in theology in 1836 and doctorates of civil and Canon Law in Rome also.
Provincial administrator.
Shortly thereafter, Gregory XVI appointed Pecci as legate (provincial administrator) to Benevento. The smallest of papal provinces, Benevento included about 20,000 people.
The main problems facing Pecci were a decaying local economy, insecurity because of widespread bandits, and pervasive Mafia structures, who often were allied with aristocratic families. Pecci arrested the most powerful aristocrat in Benevento, and his troops captured others, who were either killed or imprisoned by him. With the public order restored, he turned to the economy and a reform of the tax system to stimulate trade with neighboring provinces.
Monsignor Pecci was first destined for Spoleto, a province with 100,000, but on 17 July 1841, he was sent to Perugia with 200,000 inhabitants.
His immediate concern was to prepare the province for a papal visitation in the same year. Pope Gregory XVI visited hospitals and educational institutions for several days, asking for advice and listing questions. The fight against corruption continued in Perugia, where Pecci himself investigated several incidents. When it was claimed that a bakery was selling bread below the prescribed pound weight, he personally went there, had all bread weighed, and confiscated it if below legal weight. The confiscated bread was distributed to the poor.
Nuncio to Belgium.
In 1843, Pecci, only thirty-three years old, was appointed Apostolic Nuncio to Belgium, a position which guaranteed the Cardinal's hat after completion of the tour.
On 27 April 1843, Pope Gregory XVI appointed Pecci Archbishop of Damiette and asked his Cardinal Secretary of State Lambruschini to consecrate him. Pecci developed excellent relations with the royal family and used the location to visit neighbouring Germany, where he was particularly interested in the resumed construction of the Cologne Cathedral.
Upon his initiative, a Belgian College in Rome was opened in 1844, where 102 years later, in 1946, Pope John Paul II would begin his Roman studies. He spent several weeks in England with Bishop Nicholas Wiseman, carefully reviewing the condition of the Catholic Church in that country.
In Belgium, the school question was then sharply debated between the Catholic majority and the Liberal minority. Pecci encouraged the struggle for Catholic schools, yet he was able to win the good will of the Court, not only of the pious Queen Louise, but also of King Leopold I, strongly Liberal in his views. The new nuncio succeeded in uniting the Catholics.
Archbishop of Perugia.
Papal assistant.
Pecci was named papal assistant in 1843. He first achieved note as a popular and successful Archbishop of Perugia from 1846 to 1877. After Pope Pius IX granted unlimited freedom for the press in the Papal States in 1847, Pecci, who had been highly popular in the first years of his episcopate, became the object of attacks in the media and at his residence. In 1848, revolutionary movements developed throughout Western Europe, including France, Germany and Italy. Austrian, French and Spanish troops reversed the revolutionary gains, but at a price for Pecci and the Catholic Church, who could not regain their former popularity.
Provincial council.
Pecci called a provincial council to reform the religious life in his dioceses. He invested in enlarging the seminary for future priests and in hiring new and prominent professors, preferably Thomists. He called on his brother Giuseppe Pecci, a noted Thomist scholar, to resign his professorship in Rome and teach in Perugia instead. His own residence was next to the seminary, which facilitated his daily contacts with the students.
Charitable activities.
Pecci developed several activities in support of Catholic charities. He founded homes for homeless boys and girls and for elderly women. Throughout his dioceses he opened branches of a "Bank, Monte de Pieta," which focused on low-income people and provided low interest loans. He created soup kitchens, which were run by the Capuchins. In the consistory of 19 December 1853, he was elevated to the College of Cardinals, as Cardinal-Priest of "S. Crisogono." In light of continuing earthquakes and floods, he donated all resources for festivities to the victims. Much of the public attention turned on the conflict between the Papal States and Italian nationalism, aiming at these states' annihilation so as to achieve the Unification of Italy.
Defence of the papacy.
Pecci defended the papacy and its claims. When Italian authorities expropriated convents and monasteries of Catholic orders, turning them into administration or military buildings, Cardinal Pecci protested but acted moderately. When the Italian state took over Catholic schools, Pecci, fearing for his theological seminary, simply added all secular topics from other schools and opened the seminary to non-theologians. The new government, in addition to the expropriations, levied taxes on the Church and issued legislation according to which all Episcopal or papal utterances were to be approved by the government before their publication.
Organizing the First Vatican Council.
Pope Pius IX announced an ecumenical council, which became known as the First Vatican Council, to take place in the Vatican on 8 December 1869. Pecci was likely to be well informed, since his brother Giuseppe had been named by the Pope to help prepare this event.
In his last years in Perugia, Pecci several times addressed the role of the Church in modern society. Pecci defined the Church as "the mother of material civilization", because the Church upholds human dignity of working people, opposes the excesses of industrialization, and has developed large scale charities for the needy.
In August 1877, on the death of Cardinal Filippo de Angelis, Pope Pius IX appointed him Camerlengo, so that he was obliged to reside in Rome.
Papal conclave.
Pope Pius IX died on 7 February 1878, and during his closing years the Liberal press had often insinuated that the Italian Government should take a hand in the conclave and occupy the Vatican. However the Russo-Turkish War and the sudden death of Victor Emmanuel II (9 January 1878) distracted the attention of the government.
In the conclave, the questions that the cardinals faced varied and issues discussed included church-state relations in Europe specifically with Italy, divisions in the church, and the status of the First Vatican Council. It was also debated that the conclave be moved somewhere else but it was Pecci that debated otherwise, and the conclave assembled in Rome on 18 February 1878.
Cardinal Pecci was elected on the third ballot of the conclave and he chose the name of Leo XIII. He was announced to the people and later crowned on 3 March 1878.
Papacy.
As soon as he was elected to the papacy, Leo XIII worked to encourage understanding between the Church and the modern world. When he firmly re-asserted the scholastic doctrine that science and religion co-exist, he required the study of Thomas Aquinas and opened the Vatican Secret Archives to qualified researchers, among whom was the noted historian of the Papacy Ludwig von Pastor. He also re-founded the Vatican Observatory
"so that everyone might see clearly that the Church and her Pastors are not opposed to true and solid science, whether human or divine, but that they embrace it, encourage it, and promote it with the fullest possible devotion."
Leo XIII was the first Pope of whom a sound recording was made. The recording can be found on a compact disc of Alessandro Moreschi's singing; a recording of his praying of the Ave Maria is available on the Web. He was also the first Pope to be filmed on the motion picture camera. He was filmed by its inventor, W. K. Dickson, and blessed the camera while being filmed.
Leo XIII brought normality back to the Church after the tumultuous years of Pius IX. Leo's intellectual and diplomatic skills helped regain much of the prestige lost with the fall of the Papal States. He tried to reconcile the Church with the working class, particularly by dealing with the social changes that were sweeping Europe. The new economic order had resulted in the growth of an impoverished working class who had increasing anti-clerical and socialist sympathies. Leo helped reverse this trend.
While Leo XIII was no radical in either theology or politics, his papacy did move the Catholic Church back to the mainstream of European life. Considered a great diplomat, he managed to improve relations with Russia, Prussia, Germany, France, Britain and other countries. Pope Leo XIII and Prince Nikola of Montenegro concluded a Concordat in Rome on 18 August 1886. The signatories were Cardinal Ludovico Jacobini (1832–87) for the Holy See, and Secretary Jovan Sundecic (1825–1900) for Montenegro.
Pope Leo XIII was able to reach several agreements in 1896 that resulted in better conditions for the faithful and additional appointments of bishops. During the Fifth cholera pandemic in 1891 he ordered the construction of a hospice inside the Vatican. That building would be torn down in 1996 to make way for construction of the Domus Sanctae Marthae.
Leo was a Vin Mariani drinker. He awarded a Vatican gold medal to the wine, and also appeared on a poster endorsing it.
His favorite poets were Virgil and Dante.
Foreign relations.
Russia.
Pope Leo XIII began his pontificate with a friendly letter to Tzar Alexander II, in which he reminded the Russian monarch of the millions of Catholics living in his empire who would like to be good Russian subjects, provided their dignity were respected.
After the assassination of Alexander II, the Pope sent a high ranking representative to the coronation of his successor. Alexander III was grateful and asked for all religious forces to unify. He asked the Pope to ensure that his bishops abstain from political agitation. Relations improved further, when Pope Leo XIII, due to Italian considerations, distanced the Vatican from the Rome-Vienna-Berlin alliance and helped to facilitate a rapprochement between Paris and St. Petersburg.
Germany.
Under Otto von Bismarck, the anti-Catholic "Kulturkampf" in Prussia led to massive reprisals against the Church. Under Leo, compromises were informally reached and the anti-Catholic attacks subsided.
The Centre Party in Germany represented Catholic interests and was a positive force for social change. It was encouraged by Leo's support for social welfare legislation and the rights of working people. Leo's forward-looking approach encouraged Catholic Action in other European countries where the social teachings of the Church were incorporated into the agenda of Catholic parties, particularly the Christian democratic parties, which became an acceptable alternative to socialist parties. Leo's social teachings were reiterated throughout the 20th century by his successors.
In his "Memoirs" Kaiser Wilhelm II discussed the "friendly, trustful relationship that existed between me and Pope Leo XIII." During Wilhelm's third visit to Leo: "It was of interest to me that the Pope said on this occasion that Germany must be the sword of the Catholic Church. I remarked that the old Roman Empire of the German nation no longer existed, and that conditions had changed. But he adhered to his words."
France.
Leo XIII was the first pope to come out strongly in favour of the French Republic, upsetting many French monarchists.
Italy.
However, in light of a hostile anti-Catholic climate in Italy, he continued the policies of Pius IX towards Italy, without major modifications. In his relations with the Italian state, Leo XIII continued the Papacy's self-imposed incarceration in the Vatican stance, and continued to insist that Italian Catholics should not vote in Italian elections or hold elected office. In his first consistory in 1879 he elevated his older brother Giuseppe to the cardinalate. He had to defend the freedom of the Church against what Catholics considered Italian persecutions and attacks in the area of education, expropriation and violation of Catholic Churches, legal measures against the Church and brutal attacks, culminating in anticlerical groups attempting to throw the body of the deceased Pope Pius IX into the Tiber river on 13 July 1881. The Pope even considered moving his residence to Trieste or Salzburg, two cities in Austria, an idea which the Austrian monarch Franz Josef I gently rejected.
United Kingdom.
Among the activities of Leo XIII that were important for the English-speaking world, he restored the Scottish hierarchy in 1878. In the following year, on 12 May 1879, raised to the rank of cardinal the convert clergyman John Henry Newman, who was to be beatified by Pope Benedict XVI in 2010. In British India, too, Leo established a Catholic hierarchy in 1886, and regulated some long-standing conflicts with the Portuguese authorities. A Papal Rescript (20 April 1888) condemned the Irish Plan of Campaign and all clerical involvement in it as well as boycotting, followed in June by the Papal encyclical "Saepe Nos" that was addressed to all the Irish bishops. Of outstanding significance, not least for the English-speaking world, was Leo's encyclical "Apostolicae Curae" on the invalidity of the Anglican orders, published in 1896.
United States.
The United States at many moments in time attracted the attention and admiration of Pope Leo. He confirmed the decrees of the Third Plenary Council of Baltimore (1884), and raised James Gibbons, archbishop of that city, to the cardinalate in 1886.
American newspapers criticized Pope Leo because they claimed that he was attempting to gain control of American public schools. One cartoonist drew Leo as a fox unable to reach grapes that were labeled for American schools; the caption read "Sour grapes!"
Brazil.
Pope Leo XIII is also remembered for the "First Plenary Council of Latin America" held at Rome in 1899, and his encyclical of 1888 to the bishops of Brazil on the abolition of slavery. In 1897, he published the Apostolic Letter "Trans Oceanum", which dealt with the privileges and ecclesiastical structure of the Catholic Church in Latin America.
Chile.
His role in South America will also be remembered, especially the pontifical benediction extended over Chilean troops on the eve of the Battle of Chorrillos during the War of the Pacific in January 1881. The Chilean soldiers thus blessed then looted the cities of Chorrillos and Barranco, including the churches, and their Chaplains headed the robbery at the Biblioteca Nacional del Perú, where the soldiers ransacked various items along with much capital, and Chilean Priests coveted rare and ancient editions of the Bible that were stored there. Despite this, one year later Chilean President Domingo Santa Marìa issued the "Laicist Laws", which separated the Church from the State, considered a slap in the face for the papacy.
Evangelization.
Pope Leo XIII sanctioned the missions to eastern Africa. In 1879 Catholic missionaries associated with the White Father Congregation (Society of the Missionaries of Africa) came to Uganda and others went to Tanganyika (present day Tanzania) and Rwanda.
Theology.
The pontificate of Leo XIII was theologically influenced by the First Vatican Council (1869–1870), which had ended only eight years earlier. Leo XIII issued some 46 apostolic letters and encyclicals dealing with central issues in the areas of marriage and family and state and society. He also wrote two prayers for the intercession of Michael the Archangel after having a vision of Michael and the end times.
Thomism.
As pope, he used all his authority for a revival of Thomism, the theology of Thomas Aquinas. On 4 August 1879, Leo XIII promulgated the encyclical Aeterni Patris (“Eternal Father”) which, more than any other single document, provided a charter for the revival of Thomism—the medieval theological system based on the thought of Aquinas—as the official philosophical and theological system of the Catholic Church. It was to be normative not only in the training of priests at church seminaries but also in the education of the laity at universities.
Following this encyclical Pope Leo XIII created the Pontifical Academy of St. Thomas Aquinas on October 15, 1879 and ordered the publication of the critical edition, the so-called "leonine edition", of the complete works of the "doctor angelicus". The superintendence of the leonine edition was entrusted to Tommaso Maria Zigliara, professor and rector of the "Collegium Divi Thomae de Urbe " the future Pontifical University of Saint Thomas Aquinas, "Angelicum". Leo XIII also founded the "Angelicum's" Faculty of Philosophy in 1882 and its Faculty of Canon Law in 1896.
Consecrations.
Pope Leo XIII performed a number of consecrations, at times entering new theological territory. After he received many letters from Sister Mary of the Divine Heart, the countess of Droste zu Vischering and Mother Superior in the Convent of the Congregation of the Good Shepherd Sisters in Porto, Portugal, asking him to consecrate the entire world to the Sacred Heart of Jesus, he commissioned a group of theologians to examine the petition on the basis of revelation and sacred tradition. The outcome of this investigation was positive, and so in the encyclical letter "Annum sacrum" (on May 25, 1899) he decreed that the consecration of the entire human race to the Sacred Heart of Jesus should take place on June 11, 1899.
The encyclical letter also encouraged the entire Roman Catholic episcopate to promote the First Friday Devotions, established June as the Month of the Sacred Heart, and included the Prayer of Consecration to the Sacred Heart. His consecration of the entire world to the Sacred Heart of Jesus presented theological challenges in consecrating non-Christians. Since about 1850, various congregations and States had consecrated themselves to the Sacred Heart, and, in 1875, this consecration was made throughout the Catholic world.
Scriptures.
In his 1893 encyclical "Providentissimus Deus," he described the importance of scriptures for theological study. It was an important encyclical for Catholic theology and its relation to the Bible, as Pope Pius XII pointed out fifty years later in his encyclical "Divino Afflante Spiritu".
Ecumenical efforts.
Pope Leo XIII fostered ecumenical relations, particularly with the East. He opposed efforts to Latinize the Eastern Rite Churches, stating that they constitute a most valuable ancient tradition and symbol of the divine unity of the Catholic Church.
Theological research.
Leo XIII is credited with great efforts in the areas of scientific and historical analysis. He opened the Vatican Archives and personally fostered a twenty-volume comprehensive scientific study of the Papacy by Ludwig von Pastor, an Austrian historian.
Mariology.
His predecessor, Pope Pius IX, became known as the Pope of the Immaculate Conception because of the dogmatization in 1854. Leo XIII, in light of his unprecedented promulgation of the rosary in eleven encyclicals, was called the Rosary Pope. In eleven encyclicals on the rosary he promulgates Marian devotion. In his encyclical on the fiftieth anniversary of the Dogma of the Immaculate Conception, he stresses her role in the redemption of humanity, mentioning Mary as Mediatrix and Co-Redemptrix.
Social teachings.
Church and state.
Leo XIII worked to encourage understanding between the Church and the modern world, though he preferred a cautious view on freedom of thought, stating that it "is quite unlawful to demand, defend, or to grant unconditional freedom of thought, or speech, of writing or worship, as if these were so many rights given by nature to man". Leo's social teachings are based on the Catholic premise that God is the Creator of the world and its Ruler. Eternal law commands the natural order to be maintained, and forbids that it be disturbed; men's destiny is far above human things and beyond the earth.
"Rerum novarum".
His encyclicals changed the Church's relations with temporal authorities, and, in the 1891 encyclical "Rerum novarum," for the first time addressed social inequality and social justice issues with Papal authority, focusing on the rights and duties of capital and labour. He was greatly influenced by Wilhelm Emmanuel von Ketteler, a German bishop who openly propagated siding with the suffering working classes in his book "Die Arbeiterfrage und das Christentum". Since Leo XIII, Papal teachings have expanded on the rights and obligations of workers and the limitations of private property: Pope Pius XI Quadragesimo anno, the Social teachings of Pope Pius XII on a huge range of social issues, John XXIII Mater et magistra in 1961, Pope Paul VI, the encyclical "Populorum progressio" on world development issues, and Pope John Paul II, "Centesimus annus", commemorating the 100th anniversary of "Rerum novarum". Leo XIII had argued that both capitalism and communism are flawed. "Rerum novarum" introduced the idea of subsidiarity, the principle that political and social decisions should be taken at a local level, if possible, rather than by a central authority, into Catholic social thought. A list of all of Leo's encyclicals can be found in the List of Encyclicals of Pope Leo XIII.
Canonizations and beatifications.
Leo XIII canonized the following saints during his pontificate:
Leo XIII also beatified several of his predecessors: Urban II (1881), Victor III (1887) and Innocent V (9 March 1898). He also canonized Adrian III on 2 June 1891.
He also beatified Giancarlo Melchiori on 22 January 1882, Giovanni Giovenale Ancina on 9 February 1890, Inés of Benigánim on 26 February 1888, Pompilio Pirrotti on 26 January 1890, Leopoldo Croci on 12 May 1893, Antonio Baldinucci on 16 April 1893, Rodolfo Acquaviva and 4 Companions on 30 April 1893, Diego José López-Caamaño on 22 April 1894, Anthony Maria Zaccaria (whom he later canonized) on 3 January 1890, John Baptist de la Salle (whom he later canonized) on 19 February 1888, Maria Maddalena Martinengo on 3 June 1900, Dénis Berthelot of the Nativity and Redento Rodríguez of the Cross on 10 June 1900, Antonio Grassi on 30 September 1900, Gerard Majella in 1893, both Edmund Campion and Ralph Sherwin in 1886, Bernardino Realino on 12 January 1896, and Jeanne de Lestonnac on 23 September 1900. He also approved the cult of Cosmas of Aphrodisia. He also beatified several of the English martyrs in 1895.
Audiences.
One of the first audiences Leo XIII granted was to the professors and students of the Collegio Capranica, where in the first row knelt in front of him a young seminarian, Giacomo Della Chiesa, his eventual successor Pope Benedict XV, who would reign from 1914 to 1922.
While on a pilgrimage with her father and sister in 1887, the future Saint Thérèse of Lisieux attended a general audience with Pope Leo XIII and asked him to allow her to enter the Carmelite order. Even though she was strictly forbidden to speak to him because she was told it would prolong the audience too much, in her autobiography, "Story of a Soul", she wrote that after she kissed his slipper and he presented his hand, instead of kissing it, she took it in her own hand and said through tears, "Most Holy Father, I have a great favor to ask you. In honor of your Jubilee, permit me to enter Carmel at the age of 15!" Leo XIII answered, "Well, my child, do what the superiors decide." Thérèse replied, "Oh! Holy Father, if you say yes, everybody will agree!" Finally, the Pope said, "Go... go... "You will enter if God wills it"" [italics hers] after which time two guards lifted Thérèse (still on her knees in front of the Pope) by her arms and carried her to the door where a third gave her a medal of the Pope. Shortly thereafter, the Bishop of Bayeux authorized the prioress to receive Thérèse, and in April 1888, she entered Carmel at the age of 15.
While known for his cheerful personality, Leo XIII also had a gentle sense of humor as well. During one of his audiences, a man claimed to have had the opportunity to see Pius IX at one of his last audiences before his death in 1878. Upon hearing the remarkable story, Leo XIII smiled and replied to him, "If I had known that you were so dangerous to popes, I would have postponed this audience further".
Death.
Leo XIII was the first pope to be born in the 19th century and was also the first to die in the 20th century: he lived to the age of 93, the longest-lived pope. At the time of his death, Leo XIII was the second-longest reigning pope, exceeded only by his immediate predecessor, Pius IX.
Leo XIII was entombed in St. Peter's Basilica only very briefly after his funeral, but was later moved instead to the very ancient basilica of St. John Lateran, his cathedral church as the Bishop of Rome, and a church in which he took a particular interest. He was moved there in 1924.

</doc>
<doc id="50620" url="http://en.wikipedia.org/wiki?curid=50620" title="Scyphozoa">
Scyphozoa

The Scyphozoa are a class within the phylum Cnidaria, sometimes referred to as the "true jellyfish".
The class name Scyphozoa comes from the Greek word "skyphos" (σκύφος), denoting a kind of drinking cup and alluding to the cup shape of the organism.
Scyphozoans range in geological time from the Ediacaran period through the recent.
Biology.
Most species of Scyphozoa have two life history phases, including the planktonic medusa or jellyfish form, which is most evident in the warm summer months, and an inconspicuous, but longer-lived, bottom-dwelling polyp, which seasonally gives rise to new medusae. Most of the large, often colorful, and conspicuous jellyfish found in coastal waters throughout the world are Scyphozoa. They typically range from 2 to in diameter, but the largest species, "Cyanea capillata" can reach 2 m across. Scyphomedusae are found throughout the world's oceans, from the surface to great depths; no Scyphozoa occur in freshwater (or on land).
As medusae, they eat a variety of crustaceans and fish, which they capture using stinging cells called nematocysts. The nematocysts are located throughout the tentacles that radiate downward from the edge of the umbrella dome, and also cover the four or eight oral arms that hang down from the central mouth. Some species, however, are instead filter feeders, using their tentacles to strain plankton from the water.
Anatomy.
Scyphozoans usually display a four-part symmetry and have an internal gelatinous material called mesoglea, which provides the same structural integrity as a skeleton. The mesoglea includes mobile amoeboid cells originating from the epidermis. 
Scyphozoans have no durable hard parts, including no head, no skeleton, and no specialized organs for respiration or excretion. Marine jellyfish can consist of as much as 98% water, so are rarely found in fossil form. 
Unlike the hydrozoan jellyfish, Hydromedusae, Scyphomedusae lack a velum, which is a circular membrane beneath the umbrella that helps propels the (usually smaller) Hydromedusae through the water. However, a ring of muscle fibres is present within the mesoglea around the rim of the dome, and the jellyfish swims by alternately contracting and relaxing these muscles. The periodic contracting and relaxing propels the jellyfish through the water, allowing it to escape predation or catch its prey.
The mouth opens into a central stomach, from which four interconnected diverticula radiate outwards. In many species, this is further elaborated by a system of radial canals, with or without an additional ring canal towards the edge of the dome. Some genera, such as "Cassiopea", even have additional, smaller mouths in the oral arms. The lining of the digestive system includes further stinging nematocysts, along with cells that secrete digestive enzymes.
The nervous system usually consists of a distributed net of cells, although some species possess more organised nerve rings. In species lacking nerve rings, the nerve cells are instead concentrated into small structures called "rhopalia". There are between four and sixteen of these small lobes arranged around the rim of the umbrella, where they coordinate the muscular action allowing the animal to move. Each rhopalium is typically associated with a pair of sensory pits, a statocyst, and sometimes a pigment-cup ocellus.
Reproduction.
Most species appear to be gonochorists, with separate male and female individuals. The gonads are located in the stomach lining, and the mature gametes are expelled through the mouth. After fertilization, some species brood their young in pouches on the oral arms, but they are more commonly planktonic.
The fertilized egg produces a planular larva which, in most species, quickly attaches itself to the sea bottom. The larva develops into the hydroid stage of the lifecycle, a tiny sessile polyp called a scyphistoma. The scyphistoma reproduces asexually, producing similar polyps by budding, and then either transforming into a medusa, or budding several medusae off from its upper surface. The medusae are initially microscopic, and may take years to reach sexual maturity.
Commercial importance.
Scyphozoa include the moon jelly "Aurelia aurita", in the order Semaeostomeae, and the enormous "Nemopilema nomurai", in the order Rhizostomeae, found between Japan and China and which in some years causes major fisheries disruptions.
The jellyfish fished commercially for food are Scyphomedusae in the order Rhizostomeae. Most rhizostome jellyfish live in warm water.
Taxonomy.
Although the Scyphozoa were formerly thought to include the animals now referred to the classes Cubozoa and Staurozoa, it they are now thought to include just three extant orders (two of which are in Discomedusae, a subclass of Scyphozoa). About 200 extant species are recognized at present, but the true diversity is likely to be at least 400 species.
Class Scyphozoa

</doc>
<doc id="50621" url="http://en.wikipedia.org/wiki?curid=50621" title="History of Cyprus">
History of Cyprus

Human habitation of Cyprus dates back to the Paleolithic era. Cyprus's geographic position has caused Cyprus to be influenced by differing Eastern Mediterranean civilisations over the millennia.
Prehistory.
Cyprus was settled by humans in the Paleolithic period (known as the stone age) who coexisted with various dwarf animal species, such as dwarf elephants ("Elephas cypriotes") and pygmy hippos ("Hippopotamus minor") well into the Holocene. There are claims of an association of this fauna with artifacts of Epipalaeolithic foragers at Aetokremnos near Limassol on the southern coast of Cyprus. The first undisputed settlement occurred in the 9th (or perhaps 10th) millennium BC from the Levant. The first settlers were agriculturalists of the so-called PPNB (pre-pottery Neolithic B) era, but did not yet produce pottery (aceramic Neolithic). 
The dog, sheep, goats and possibly cattle and pigs were introduced, as well as numerous wild animals such as foxes ("Vulpes vulpes") and Persian fallow deer ("Dama mesopotamica") that were previously unknown on the island. The PPNB settlers built round houses with floors made of terrazzo of burned lime (e.g. Kastros, Shillourokambos) and cultivated einkorn and emmer. Pigs, sheep, goat and cattle were kept but remained, for the most part, behaviourally wild. Evidence of cattle such as that attested at Shillourokambos is rare, and when they apparently died out in the course of the 8th millennium they were not re-introduced until the ceramic Neolithic.
In the 6th millennium BC, the aceramic Khirokitia culture was characterised by roundhouses, stone vessels and an economy based on sheep, goats and pigs. Cattle were unknown, and Persian fallow deer were hunted. This was followed by the ceramic Sotira phase. The Eneolithic era is characterised by stone figurines with spread arms.
Water wells discovered by archaeologists in western Cyprus are believed to be among the oldest in the world, dated at 9,000 to 10,500 years old, putting them in the Stone Age. They are said to show the sophistication of early settlers, and their heightened appreciation for the environment.
In 2004, the remains of an 8-month-old cat were discovered buried with its human owner at a Neolithic archeological site in Cyprus. The grave is estimated to be 9,500 years old, predating Egyptian civilization and pushing back the earliest known feline-human association significantly.
Bronze Age.
In the Bronze Age the first cities, such as Enkomi, were built. Systematic copper mining began, and this resource was widely traded. Mycenaean Greeks were undoubtedly inhabiting Cyprus from the late stage of the Bronze Age, while the island's Greek name is already attested from the 15th century BC in the Linear B script.
The Cypriot syllabic script was first used in early phases of the late Bronze Age (LCIB) and continued in use for ca. 500 years into the LC IIIB, maybe up to the second half of the eleventh century BC. Most scholars believe it was used for a native Cypriot language (Eteocypriot) that survived until the 4th century BC, but the actual proofs for this are scant, as the tablets still have not been completely deciphered.
The LCIIC (1300–1200 BC) was a time of local prosperity. Cities such as Enkomi were rebuilt on a rectangular grid plan, where the town gates correspond to the grid axes and numerous grand buildings front the street system or newly founded.
Great official buildings constructed from ashlar masonry point to increased social hierarchisation and control. Some of these buildings contain facilities for processing and storing olive oil, such as Maroni-Vournes and Building X at Kalavassos-Ayios Dhimitrios. A Sanctuary with a horned altar constructed from ashlar masonry has been found at Myrtou-Pigadhes, other temples have been located at Enkomi, Kition and Kouklia (Palaepaphos). Both the regular layout of the cities and the new masonry techniques find their closest parallels in Syria, especially in Ugarit (modern Ras Shamra).
Rectangular corbelled tombs point to close contacts with Syria and Palestine as well.
The practice of writing spread and tablets in the Cypriot syllabic script have been found at Ras Shamra which was the Phoenician city of Ugarit. Ugaritic texts from Ras Shamra and Enkomi mention Ya, the Assyrian name of Cyprus, that thus seems to have been in use already in the late Bronze Age.
Copper ingots shaped like oxhides have been recovered from shipwrecks such as at Ulu Burun, Iria and Cape Gelidonya which attest to the widespread metal trade. Weights in the shape of animals found in Enkomi and Kalavassos follow the Syro-Palestinian, Mesopotamian, Hittite and Aegean standards and thus attest to the wide ranging trade as well.
Late Bronze Age Cyprus was a part of the Hittite empire but was a client state and as such was not invaded but rather merely part of the empire by association and governed by the ruling kings of Ugarit. As such Cyprus was essentially "left alone with little intervention in Cypriot affairs". However, during the reign of Tudhaliya, the island was briefly invaded by the Hittites for either reasons of securing the copper resource or as a way of preventing piracy. Shortly afterwards the island was reconquered by his son around 1200 BCE.
In the later phase of the late Bronze Age (LCIIIA, 1200–1100 BC) great amounts of 'Mycenean' IIIC:1b pottery were produced locally. New architectural features include cyclopean walls, found on the Greek mainland, as well and a certain type of rectangular stepped capitals, endemic on Cyprus. Chamber tombs are given up in favour of shaft graves. Large amounts of IIIC:1b pottery are found in Palestine during this period as well. While this was formerly interpreted as evidence of an invasion ('Sea Peoples'), this is seen more and more as an indigenous development, triggered by increasing trade relations with Cyprus and Crete. Evidence of early trade with Crete is found in archaeological recovery on Cyprus of pottery from Cydonia, a powerful urban center of ancient Crete.
Although Achaean Greeks were living in Cyprus from the 14th century, most of them inhabited the island after the Trojan war. Achaeans were colonizing Cyprus from 1210–1000 BC.
Dorian Greeks come at 1100 BC and unlike the Greek mainland they settle in Cyprus peacefully.
Another wave of Greek settlement is believed to have taken place in the following century (LCIIIB, 1100–1050), indicated, among other things, by a new type of graves (long dromoi) and Mycenean influences in pottery decoration.
Most authors claim that the Cypriot city kingdoms, first described in written sources in the 8th century BC were already founded in the 11th century BC. Other scholars see a slow process of increasing social complexity between the 12th and the 8th centuries, based on a network of chiefdoms. In the 8th century (geometric period) the number of settlements increases sharply and monumental tombs, like the 'Royal' tombs of Salamis appear for the first time. This could be a better indication for the appearance of the Cypriot kingdoms.
Iron Age.
The Iron Age follows the Submycenian period (1125–1050 BC) or Late Bronze Age and is divided into the:
Foundations myths documented by classical authors connect the foundation of numerous Cypriot towns with immigrant Greek heroes in the wake of the Trojan war. For example, Teucer, brother of Aias was supposed to have founded Salamis, and the Arcadian Agapenor of Tegea to have replaced the native ruler Kinyras and to have founded Paphos. Some scholars see this a memory of a Greek colonisation already in the 11th century. In the 11th century tomb 49 from Palaepaphos-Skales three bronze obeloi with inscriptions in Cypriot syllabic script have been found, one of which bears the name of Opheltas. This is first indication of the use of Greek language on the island.
Cremation as a burial rite is seen as a Greek introduction as well. The first cremation burial in Bronze vessels has been found at Kourion-Kaloriziki, tomb 40, dated to the first half of the 11th century (LCIIIB). The shaft grave contained two bronze rod tripod stands, the remains of a shield and a golden sceptre as well. Formerly seen as the Royal grave of first Argive founders of Kourion, it is now interpreted as the tomb of a native Cypriote or a Phoenician prince. The cloisonné enamelling of the sceptre head with the two falcons surmounting it has no parallels in the Aegean, but shows a strong Egyptian influence.
In the 8th century, several Phoenician colonies were founded, like Kart-Hadasht ('New Town'), present day Larnaca and Salamis. The oldest cemetery of Salamis has indeed produced children's burials in Canaanite jars, clear indication of Phoenician presence already in the LCIIIB 11th century. Similar jar burials have been found in cemeteries in Kourion-Kaloriziki and Palaepaphos-Skales near Kouklia. In Skales, many Levantine imports and Cypriote imitations of Levantine forms have been found and point to a Phoenician expansion even before the end of the 11th century.
Ancient history.
The first written source shows Cyprus under Assyrian rule. A stela found 1845 in Kition commemorates the victory of king Sargon II (721–705 BC) in 709 over the seven kings in the land of Ia', in the district of Iadnana or Atnana. The former is supposedly the Assyrian name of the island, while some authors take the latter to mean Greece (the Islands of the Danaoi). There are other inscriptions referring to Ia' in Sargon's palace at Khorsabad.
The ten kingdoms listed by an inscription of Esarhaddon in 673/2 BC have been identified as Salamis, Kition, Amathus, Kourion, Paphos and Soli on the coast and Tamassos, Ledra, Idalium and Chytri in the interior.
Cyprus gained independence for some time around 669 but was conquered by Egypt under Amasis (570–526/525). The island was conquered by the Persians around 545 BC. A Persian palace has been excavated in the territory of Marion on the North coast near Soli. The inhabitants took part in the Ionian rising. At the beginning of the 4th century BC, Euagoras I, King of Salamis, took control of the whole island and tried to gain independence from Persia. Another uprising took place in 350 but was crushed by Artaxerxes in 344.
During the siege of Tyre, the Cypriot Kings went over to Alexander the Great. In 321 four Cypriot kings sided with Ptolemy I Soter and defended the island against Antigonos. Ptolemy lost Cyprus to Demetrios Poliorketes in 306 and 294 BC, but after that it remained under Ptolemaic rule till 58 BC. It was ruled by a governor from Egypt and sometimes formed a minor Ptolemaic kingdom during the power-struggles of the 2nd and 1st centuries. Strong commercial relationships with Athens and Alexandria, two of the most important commercial centres of antiquity, developed.
Full Hellenisation only took place under Ptolemaic rule. Phoenician and native Cypriot traits disappeared, together with the old Cypriot syllabic script. A number of cities were founded during this time, e.g. Arsinoe that was founded between old and new Paphos by Ptolemy II.
Cyprus became a Roman province in 58 BC, according to Strabo because Publius Clodius Pulcher held a grudge against Ptolemy and sent Marcus Cato to conquer the island after he had become tribune. Marc Anthony gave the island to Cleopatra VII of Egypt and her sister Arsinoe IV, but it became a Roman province again after his defeat at the Battle of Actium (31 BC) in 30 BC. Since 22 BC it was a senatorial province, after the reforms of Diocletian it was placed under the Consularis Oriens. (From then it was governed by a proconsul.) The island suffered great losses during the Jewish rising of 115/116 AD. Several earthquakes led to the destruction of Salamis at the beginning of the 4th century, at the same time drought and famine hit the island.
Christianisation:
The apostle Paul is reported to have converted the people of Cyprus to Christianity. The Levite Barnabas, a Cypriot, travels to Cyprus and Anatolia with Paul (Acts. 12, 13). During the 5th century AD, the church of Cyprus achieved its independence from the Patriarch of Antioch at the Council of Ephesus in 431.
Middle Ages to High Middle Ages and Byzantine Renaissance.
After the division of the Roman Empire into an eastern half and a western half, Cyprus came under the rule of Byzantium. At that time, its bishop, while still subject to the Church, was made autocephalous by the Council of Ephesus.
The Arabs invaded Cyprus in force in the 650s, but in 688, the emperor Justinian II and the caliph Abd al-Malik reached an unprecedented agreement. For the next 300 years, Cyprus was ruled jointly by both the Arabs and the Byzantines as a condominium, despite the nearly constant warfare between the two parties on the mainland. The Byzantines recovered control over the island for short periods thereafter, but the "status quo" was always restored.
This period lasted until the year 965, when Niketas Chalkoutzes conquered the island for a resurgent Byzantium. In 1185, the last Byzantine governor of Cyprus, Isaac Comnenus of Cyprus from a minor line of the Imperial house, rose in rebellion and attempted to seize the throne. His attempted coup was unsuccessful, but Comnenos was able to retain control of the island.
Byzantine actions against Comnenos failed because he enjoyed the support of William II of Sicily. The Emperor had an agreement with the sultan of Egypt to close Cypriot harbours to the Crusaders. 
In the 12th century AD the island became a target of the crusaders. Richard the Lionheart landed in Limassol on 1 June 1191 in search of his sister and his bride Berengaria, whose ship had become separated from the fleet in a storm. Richard's army landed when Isaac refused to return the hostages (Richard's sister, his bride, and several shipwrecked soldiers), and forced Isaac to flee from Limassol. He eventually surrendered, conceding control of the island to the King of England. Richard married Berengaria in Limassol on 12 May 1192. She was crowned as Queen of England by John Fitzluke, Bishop of Évreux. The crusader fleet continued to St. Jean d'Acre (Syria) on 5 June.
The army of Richard the Lionheart continued to occupy Cyprus and raised taxes. He sold the island to the Knights Templar. Soon after that, the French (Lusignans) occupied the island, establishing the Kingdom of Cyprus. They declared Latin the official language, later replacing it with French; much later, Greek was recognised as a second official language. In 1196, the Latin Church was established, and the Orthodox Cypriot Church experienced a series of religious persecutions. Maronites settled on Cyprus during the crusades and still maintain some villages in the North.
Late Middle Ages and Renaissance.
Amalric I of Cyprus, received the royal crown and title from Henry VI, Holy Roman Emperor. A small minority Roman Catholic population of the island was mainly confined to some coastal cities, such as Famagusta, as well as inland Nicosia, the traditional capital. Roman Catholics kept the reins of power and control, while the Greek inhabitants lived in the countryside; this was much the same as the arrangement in the Kingdom of Jerusalem. The independent Eastern Orthodox Church of Cyprus, with its own archbishop and subject to no patriarch, was allowed to remain on the island, but the Latin Church largely displaced it in stature and holding property.
After the death of Amalric of Lusignan, the Kingdom continually passed to a series of young boys who grew up as king. The Ibelin family, which had held much power in Jerusalem prior its downfall, acted as regents during these early years. In 1229 one of the Ibelin regents was forced out of power by Frederick II, Holy Roman Emperor, who brought the struggle between the Guelphs and Ghibellines to the island. 
Frederick's supporters were defeated in this struggle by 1233, although it lasted longer in the Kingdom of Jerusalem and in the Holy Roman Empire. Frederick's Hohenstaufen descendants continued to rule as kings of Jerusalem until 1268 when Hugh III of Cyprus claimed the title and its territory of Acre for himself upon the death of Conrad III of Jerusalem, thus uniting the two kingdoms. The territory in Palestine was finally lost while Henry II was king in 1291, but the kings of Cyprus continued to claim the title.
Like Jerusalem, Cyprus had a Haute Cour (High Court), although it was less powerful than it had been in Jerusalem. The island was richer and more feudal than Jerusalem, so the king had more personal wealth and could afford to ignore the Haute Cour. The most important vassal family was the multi-branch House of Ibelin. However, the king was often in conflict with the Italian merchants, especially because Cyprus had become the centre of European trade with Africa and Asia after the fall of Acre in 1291.
The kingdom eventually came to be dominated more and more in the 14th century by the Genoese merchants. Cyprus therefore sided with the Avignon Papacy in the Western Schism, in the hope that the French would be able to drive out the Italians. The Mameluks then made the kingdom a tributary state in 1426; the remaining monarchs gradually lost almost all independence, until 1489 when the last Queen, Catherine Cornaro, was forced to sell the island to Venice. Ottomans started raiding Cyprus immediately afterwards, and captured it in 1571.
This is the historical setting to Shakespeare's "Othello", the play's title character being the commander of the Venetian garrison defending Cyprus against the Ottomans.
Venetian walls of Nicosia.
Cyprus under the Ottoman Empire.
Beginnings of Enosis.
During the Greek War of Independence the Greek people fought for independence from the Ottoman Empire who ruled them. A number of Greek Cypriots rebelled on Cyprus, in return the Ottoman rulers of Cyprus tried to keep control by using draconian means of suppression. 486 Greek Cypriots were executed on 9 July 1821, accused of conspiring with the rebelling Greeks, including four Bishops and numerous prominent citizens—all beheaded in the central square of Nicosia, while Archbishop Kyprianos was hanged. Actions during the short period that followed caused a strengthening of that Greek Cypriot desire to become part of Greece, known as "Enosis" ("Union").
Many Cypriots again sought the incorporation of Cyprus into Greece when Greece became independent in 1830, but it remained part of the Ottoman Empire. The Russo-Turkish War ended the Ottoman control of Cyprus in 1878 when Cyprus was left under the control of the United Kingdom; with its conditions set out in the Cyprus Convention, although sovereignty of the island continued to belong to the Ottoman Empire until the Kingdom annexed the island unilaterally in 1914, when it declared war against the Ottomans at the First World War.Following the World War I, under the provisions of the Lausanne Treaty, Turkey relinquished all claims and rights on Cyprus.
Under British rule the island began to enjoy a period of increased free-speech, something which allowed further development of the Greek Cypriots' ideas of Enosis.
Modern era.
In 1878, as the result of the Cyprus Convention, the United Kingdom took over the government of Cyprus as a protectorate from the Ottoman Empire. In 1914, at the beginning of World War I, Cyprus was annexed by the United Kingdom. In 1925, following the dissolution of the Ottoman Empire, Cyprus was made a Crown Colony. Between 1955–59 EOKA was created by Greek Cypriots and led by George Grivas to perform enosis (union of the island with Greece). However the EOKA campaign did not result union with Greece but rather an independent republic, The Republic of Cyprus, in 1960.
The 1960 constitution put in place a form of power-sharing, or consociational government, in which concessions were made to the Turkish Cypriot minority, including as a requirement that the vice-president of Cyprus and at least 30% of members of parliament be Turkish Cypriots. Archbishop Makarios would be the President and Dr. Fazıl Küçük would become Vice President. One of the articles in the constitution was the creation of separate local municipalities so that Greek and Turkish Cypriots could manage their own municipalities in large towns. 
Internal conflicts turned into full-fledged armed fighting between the two communities on the island which prompted the United Nations to send peacekeeping forces in 1964; these forces are still in place today. In 1974, Greek Cypriots performed a military coup with the support of military junta in Greece. Unable to secure multilateral support against the coup, Turkey invaded the northern portion of the island. Turkish forces remained after a cease-fire, resulting in the partition of the island. The intercommunal violence, the coup, and the subsequent invasion led to the displacement of hundreds of thousands of Cypriots.
The "de facto" state of Northern Cyprus was proclaimed in 1975 under the name of the Turkish Federated State of Northern Cyprus. The name was changed to its present form, the Turkish Republic of Northern Cyprus, on 15 November 1983.
In 2002 UN Secretary General Kofi Annan started a new round of negotiations for the unification of the island. In 2004 after long negotiations between both sides a plan for unification of the island emerged. The resulting plan was supported by UN, EU and the US. The nationalists in both sides campaigned for the rejection of the plan, the result being that Turkish Cypriots accepted the plan while Greek Cypriots rejected it.
After Cyprus became a member of the European Union in 2004, it adopted the euro as its currency on January 1, 2008, replacing the previously used Cypriot pound; Northern Cyprus continued to use the Turkish lira, and after January 1, 2008, now uses the new Turkish lira.
See also.
General:

</doc>
<doc id="50622" url="http://en.wikipedia.org/wiki?curid=50622" title="John Frederick, Margrave of Brandenburg-Ansbach">
John Frederick, Margrave of Brandenburg-Ansbach

John Frederick, Margrave of Brandenburg-Ansbach (18 October 1654 – 22 March 1686) succeeded his father Albert II as margrave of Ansbach in 1667. He married his second wife Eleonore Erdmuthe of Saxe-Eisenach on 4 November 1681. Their daughter Wilhelmine Charlotte Caroline, Margravine of Brandenburg-Ansbach (Caroline of Ansbach) married George II of Great Britain before he became king.
Issue.
By Margravine Johanna Elisabeth of Baden-Durlach, daughter of Frederick VI, Margrave of Baden-Durlach, and his wife Christina Magdalena of the Palatinate-Zweibrücken:
By Princess Eleonore Erdmuthe of Saxe-Eisenach:

</doc>
<doc id="50623" url="http://en.wikipedia.org/wiki?curid=50623" title="Chmod">
Chmod

In Unix-like operating systems, chmod is the command and system call which may change the access permissions to file system objects (files and directories). It may also alter special mode flags. The request is filtered by the umask. The name is an abbreviation of "change mode".
History.
A chmod command first appeared in AT&T Unix version 1.
As systems grew in number and types of users access_control_lists were added to many file systems in addition to these most basic modes to increase flexibility.
Command syntax.
Usual implemented options include:
If a symbolic link is specified, the target object is affected. File modes directly associated with symbolic links themselves are typically never used.
To view the file mode, the ls or stat commands may be used:
The r, w, and x specify the read, write, and execute access, respectively. The first letter denotes the file type; a hyphen represents a plain file. This script can be read, written to, and executed by the user, read and executed by other members of the "staff" group and can also be read by others.
Octal modes.
The chmod numerical format accepts up to four octal digits. The rightmost three (digits two until four) refer to permissions for the file owner, the group, and other users respectively. The optional first digit specifies the special setuid, setgid, and sticky flags.
Numerical permissions
Numeric example.
In order to permit all users who are members of the programmers group to update a file
Since the setuid, setgid and sticky bits are not specified, this is equivalent to:
Symbolic modes.
The chmod command also accepts a finer-grained symbolic notation, which allows modifying specific modes while leaving other modes untouched. The symbolic mode is composed of three components, which are combined to form a single string of text:
The references (or classes) are used to distinguish the users to whom the permissions apply. If no references are specified it defaults to “all” but modifies only the permissions allowed by the umask. The references are represented by one or more of the following letters:
The chmod program uses an operator to specify how the modes of a file should be adjusted. The following operators are accepted: 
The modes indicate which permissions are to be granted or removed from the specified classes. There are three basic modes which correspond to the basic permissions:
Multiple changes can be specified by separating multiple symbolic modes with commas (without spaces).
Symbolic examples.
Add write permission (w) to the group's(g) access modes of a directory,allowing users in the same group to add files:
Remove write permissions (w) for all classes (a),preventing anyone from writing to the file:
Set the permissions for the user and the group (ug) to read and execute (rx) only (no write permission) on referenceLib,preventing anyone other than the owner to add files.
Special modes.
The chmod command is also capable of changing the additional permissions or special modes of a file or directory. The symbolic modes use s to represent the "setuid" and "setgid" modes, and t to represent the "sticky" mode. The modes are only applied to the appropriate classes, regardless of whether or not other classes are specified.
Most operating systems support the specification of special modes using octal modes, but some do not. On these systems, only the symbolic modes can be used.
System call.
The POSIX standard defines the following function prototype:
 int chmod(const char *path, mode_t mode);
The "mode" parameter is a bitfield composed of various flags:
Where alternate flag names are given, one of the pair of names might not be supported on some OSs. The octal values of the flags are summed or combined in a bitwise OR operation to give the desired permission mode.
The function returns an error code.
Common Errors.
When you type in 'ls -l' and see bunch of question marks such as this:
this usually means that one of the parent folders does not have proper permissions set.
Unless you fix the permissions of the parent folder, commands like:
won't work.

</doc>
<doc id="50627" url="http://en.wikipedia.org/wiki?curid=50627" title="Conformal map">
Conformal map

In mathematics, a conformal map is a function that preserves angles locally. In the most common case, the function has a domain and an image in the complex plane.
More formally, a map
is called conformal (or angle-preserving) at a point formula_3 if it preserves oriented angles between curves through formula_3 with respect to their orientation (i.e. not just the magnitude of the angle). Conformal maps preserve both angles and the shapes of infinitesimally small figures, but not necessarily their size or curvature.
The conformal property may be described in terms of the Jacobian derivative matrix of a coordinate transformation. If the Jacobian matrix of the transformation is everywhere a scalar times a rotation matrix, then the transformation is conformal.
Conformal maps can be defined between domains in higher-dimensional Euclidean spaces, and more generally on a Riemannian or semi-Riemannian manifold.
Complex analysis.
An important family of examples of conformal maps comes from complex analysis. If "U" is an open subset of the complex plane, formula_5, then a function
is conformal if and only if it is holomorphic and its derivative is everywhere non-zero on "U". If "f" is antiholomorphic (that is, the conjugate to a holomorphic function), it still preserves angles, but it reverses their orientation.
In the literature, there is another definition of conformal maps; a map f defined on an open set is said to be conformal if it is one-to-one and holomorphic. Since a one-to-one map defined on a non-empty open set cannot be constant, the open mapping theorem forces the inverse function (defined on the image of f) to be holomorphic. Thus, under this definition, a map is conformal if and only if it is biholomorphic. The two definitions for conformal maps are not equivalent. Being one-to-one and holomorphic implies having a non-zero derivative. However, the exponential function is a holomorphic function with a non-zero derivative, but is not one-to-one since it is periodic. 
The Riemann mapping theorem, one of the profound results of complex analysis, states that any non-empty open simply connected proper subset of formula_5 admits a bijective conformal map to the open unit disk in formula_5.
A map of the extended complex plane (which is conformally equivalent to a sphere) onto itself is conformal if and only if it is a Möbius transformation. Again, for the conjugate, angles are preserved, but orientation is reversed.
An example of the latter is taking the reciprocal of the conjugate, which corresponds to circle inversion with respect to the unit circle. This can also be expressed as taking the reciprocal of the radial coordinate in circular coordinates, keeping the angle the same. See also inversive geometry.
Riemannian geometry.
In Riemannian geometry, two Riemannian metrics formula_9 and formula_10 on smooth manifold formula_11 are called conformally equivalent if formula_12 for some positive function formula_13 on formula_14. The function formula_15 is called the conformal factor.
A diffeomorphism between two Riemannian manifolds is called a conformal map if the pulled back metric is conformally equivalent to the original one. For example, stereographic projection of a sphere onto the plane augmented with a point at infinity is a conformal map.
One can also define a conformal structure on a smooth manifold, as a class of conformally equivalent Riemannian metrics.
Higher-dimensional Euclidean space.
A classical theorem of Joseph Liouville called Liouville's theorem shows the higher-dimensions have less varied conformal maps:
Any conformal map on a portion of Euclidean space of dimension greater than 2 can be composed from three types of transformation: a homothetic transformation, an isometry, and a special conformal transformation. (A "special conformal transformation" is the composition of a reflection and an inversion in a sphere.) Thus, the group of conformal transformations in spaces of dimension greater than 2 are much more restricted than the planar case, where the Riemann mapping theorem provides a large group of conformal transformations.
Uses.
If a function is harmonic (that is, it satisfies Laplace's equation formula_16) over a plane domain (which is two-dimensional), and is transformed via a conformal map to another plane domain, the transformation is also harmonic. For this reason, any function which is defined by a potential can be transformed by a conformal map and still remain governed by a potential. Examples in physics of equations defined by a potential include the electromagnetic field, the gravitational field, and, in fluid dynamics, potential flow, which is an approximation to fluid flow assuming constant density, zero viscosity, and irrotational flow. One example of a fluid dynamic application of a conformal map is the Joukowsky transform.
Conformal mappings are invaluable for solving problems in engineering and physics that can be expressed in terms of functions of a complex variable but that exhibit inconvenient geometries. By choosing an appropriate mapping, the analyst can transform the inconvenient geometry into a much more convenient one. For example, one may wish to calculate the electric field, formula_17 arising from a point charge located near the corner of two conducting planes separated by a certain angle (where formula_18 is the complex coordinate of a point in 2-space). This problem "per se" is quite clumsy to solve in closed form. However, by employing a very simple conformal mapping, the inconvenient angle is mapped to one of precisely pi radians, meaning that the corner of two planes is transformed to a straight line. In this new domain, the problem (that of calculating the electric field impressed by a point charge located near a conducting wall) is quite easy to solve. The solution is obtained in this domain, formula_19 and then mapped back to the original domain by noting that formula_20 was obtained as a function (viz., the composition of formula_21 and formula_20) of formula_23 whence formula_24 can be viewed as formula_25 which is a function of formula_23 the original coordinate basis. Note that this application is not a contradiction to the fact that conformal mappings preserve angles, they do so only for points in the interior of their domain, and not at the boundary.
A large group of conformal maps for relating solutions of Maxwell’s equations was identified by Ebenezer Cunningham (1908) and Harry Bateman (1910) (see spherical wave transformation). Their training at Cambridge University had given them facility with the method of image charges and associated methods of images for spheres and inversion. As recounted by Andrew Warwick (2003) "Masters of Theory":
Warwick highlights (pages 404 to 424) this "new theorem of relativity" as a Cambridge response to Einstein, and as founded on exercises using the method of inversion, such as found in James Hopwood Jeans textbook "Mathematical Theory of Electricity and Magnetism".
In cartography, several named map projections (including the Mercator projection) are conformal.
In general relativity, conformal maps are the simplest and thus most common type of causal transformations. Physically, these describe different universes in which all the same events and interactions are still (causally) possible, but a new additional force is necessary to effect this (that is, replication of all the same trajectories would necessitate departures from geodesic motion because the metric is different). It is often used to try to make models amenable to extension beyond curvature singularities, for example to permit description of the universe even before the big bang.
Alternative angles.
A "conformal map" is called that because it preserves the shapes of things (at an infinitesimal scale). The term is based on the Latin prefix "com-" (together, with, near) and the Latin noun "forma" (shape, appearance). The presumption often is that the shape being preserved is measured by the standard Euclidean angle, say parameterized in degrees or radians. However, in plane mapping there are two other angles to consider: the hyperbolic angle and the slope, which is the analogue of angle for dual numbers.
Suppose formula_27 is a mapping of surfaces parameterized by formula_28 and formula_29. The Jacobian matrix of formula_30 is formed by the four partial derivatives of formula_13 and formula_32 with respect to formula_33 and formula_34.
If the Jacobian "g" has a non-zero determinant, then formula_35 is "conformal in the generalized sense" with respect to one of the three angle types, depending on the real matrix expressed by the Jacobian "g".
Indeed, any such "g" lies in a particular "planar" commutative subring, and "g" has a polar coordinate form determined by parameters of radial and angular nature. The radial parameter corresponds to a similarity mapping and can be taken as 1 for purposes of conformal examination. The angular parameter of "g" is one of the three types, shear, hyperbolic, or Euclidean:
While describing analytic functions of a bireal variable, U. Bencivenga and G. Fox have written about conformal maps that preserve the hyperbolic angle. In general, a linear fractional transformation on any one of the types of complex plane listed provides a conformal map.

</doc>
<doc id="50628" url="http://en.wikipedia.org/wiki?curid=50628" title="James J. Hill">
James J. Hill

James Jerome Hill (September 16, 1838 – May 29, 1916), was a Canadian-American railroad executive. He was the chief executive officer of a family of lines headed by the Great Northern Railway, which served a substantial area of the Upper Midwest, the northern Great Plains, and Pacific Northwest. Because of the size of this region and the economic dominance exerted by the Hill lines, Hill became known during his lifetime as "The Empire Builder".
Biography.
Childhood and Youth.
Hill was born in Eramosa Township, Wellington County, Upper Canada (now Ontario). A childhood accident with a bow and arrow blinded him in the right eye. He had nine years of formal schooling. He attended the Rockwood Academy for a short while, where the head gave him free tuition. He was forced to leave school in 1852 due to the death of his father. By the time he had finished, he was adept at algebra, geometry, land surveying, and English. His particular talents for English and mathematics would be critical later in his life.
After working as a clerk in Kentucky (during which he learned bookkeeping), Hill decided to permanently move to the United States and settled in St. Paul, Minnesota, at the age of 18. His first job in St. Paul was with a steamboat company, where he worked as a bookkeeper. By 1860, he was working for wholesale grocers, for whom he handled freight transfers, especially dealing with railroads and steamboats. Through this work, he learned all aspects of the freight and transportation business. During this period, Hill began to work for himself for the first time. During the winter months when the Mississippi River was frozen and steamboats could not run, Hill started bidding on other contracts and won quite a few.
The young businessman.
Because of his previous experiences in shipping and fuel supply, Hill was able to enter both the coal and steamboat businesses. In 1870, he and his partners started the Red River Transportation Company, which offered steam boat transportation between St. Paul and Winnipeg. By 1879 he had a local monopoly by merging (with Norman Kittson). In 1867, Hill entered the coal business, and by 1879 it had expanded five times over, giving Hill a local monopoly in the anthracite coal business. During this same period, Hill also entered into banking and quickly managed to become member of several major banks' boards of directors. He also bought out bankrupt businesses, built them up again, and then resold them—often gaining a substantial profit.
Hill undertook to establish a monopoly of the steamboat business; he was monopolizing coal, socializing with bankers, and buying other businesses at the same time. Hill noted that the secret to success was, "Work, hard work, intelligent work, and then more work."
Entry into Gilded Era Railroading.
During the Panic of 1873, a number of railroads, including the St. Paul and Pacific Railroad (StP&P), had gone bankrupt. The StP&P in particular was caught in an almost hopeless legal muddle. For James Hill it was a golden opportunity. For three years, Hill researched the StP&P and finally concluded that it would be possible to make a good deal of money off of the StP&P, provided that the initial capital could be found. Hill teamed up with Norman Kittson (the man he had merged steamboat businesses with), Donald Smith, George Stephen and John Stewart Kennedy. Together they not only bought the railroad, they also vastly expanded it by bargaining for trackage rights with Northern Pacific Railway. In May 1879, the St. Paul, Minneapolis, and Manitoba Railway Co. (StPM&M) formed—with James J. Hill as general manager. His first goal was to expand and upgrade even more.
Hill was a hands-on, detail-obsessed manager. A Canadian himself of Scotch-Irish Protestant ancestry, he brought in many men with the same background into high management. He wanted people settling along his rail lines, so he sold homesteads to immigrants and then imported them to their new homes (on his rail lines, of course). He imported grains from Russia and sold this to farmers. He even sold wood to farmers in order to encourage them to buy his wheat. When he was looking for the best path for one of his tracks to take, he went on horseback and scouted it personally. Under his management, StPM&M prospered. In 1880, its net worth was $728,000; in 1885 it was $25,000,000.
One of his challenges at this point was the avoidance of federal action against railroads. If the federal government believed that the railroads were making too much profit, they might see this as an opportunity to force lowering of the railway tariff rates. Hill avoided this by investing a large portion of the railroad's profit back into the railroad itself—and charged those investments to operating expense. It was at this point that Hill became the official president of StPM&M (not that he hadn't been the man behind the curtain before), and decided to expand the rail lines.
The Empire Builder.
Between 1883 and 1889, Hill built his railroads across Minnesota, into Wisconsin, and across North Dakota to Montana.
When there was not enough industry in the areas Hill was building, Hill brought the industry in, often by buying out a company and placing plants along his railroad lines. By 1889, Hill decided that his future lay in expanding into a transcontinental railroad.
"What we want," Hill is quoted as saying, "is the best possible line, shortest distance, lowest grades, and least curvature we can build. We do not care enough for Rocky Mountains scenery to spend a large sum of money developing it." Hill got what he wanted, and in January 1893 his Great Northern Railway, running from St. Paul, Minnesota to Seattle, Washington — a distance of more than 1700 mi — was completed. The Great Northern was the first transcontinental built without public money and just a few land grants, and was one of the few transcontinental railroads not to go bankrupt.
Hill chose to build his railroad north of the competing Northern Pacific line, which had reached the Pacific Northwest over much more difficult terrain with more bridges, steeper grades, and tunnelling. Hill did much of the route planning himself, travelling over proposed routes on horseback. The key to the Great Northern line was Hill's use of the previously unmapped Marias Pass first discovered by Santiago Jameson. The pass had been discovered by John Frank Stevens, principal engineer of the Great Northern Railway, in December 1889, and offered an easier route across the Rockies than that taken by the Northern Pacific. The Great Northern reached Seattle in 1893.
In 1898 Hill purchased control of large parts of the Mesabi Range iron mining district in Minnesota, along with its rail lines. The Great Northern began large-scale shipment of ore to the steel mills of the Midwest.
Settlements.
The Great Northern energetically promoted settlement along its lines in North Dakota and Montana, especially by German and Scandinavians from Europe. The Great Northern bought its lands from the federal government—it received no land grants—and resold them to farmers one by one. It operated agencies in Germany and Scandinavia that promoted its lands, and brought families over at low cost. The rapidly increasing settlement in North Dakota's Red River Valley along the Minnesota border between 1871 and 1890 was a major example of large-scale "bonanza" farming.
The Hill Lines: The 1890s.
Six months after the railroad reached Seattle came the deep nationwide depression called the Panic of 1893. Hill's leadership became a case study in the successful management of a capital-intensive business during the economic downturn. In order to ensure that he did not lose his patronage during the crisis, Hill lowered rail tariff shipping rates for farmers and gave credit to many of the businesses he owned so they could continue paying their workers. He also took strong measures to economize—in just one year, Hill cut the railway's expense of carrying a ton of freight by 13%. Because of these measures, Hill not only stayed in business, but also increased the net worth of his railroad by nearly $10 million. Meanwhile, nearly every other transcontinental railroad went bankrupt. His ability to ride out the depression garnered him fame and admiration. Hill saved money by repeatedly cutting wages, although this was during a time of deflation when prices were falling generally.
Leonard says that after 1900 Hill exhibited poor business judgment regarding one Canadian subsidiary, the Vancouver, Westminster and Yukon Railway Company (VW&Y). He ousted its president John Hendry, thereby worsening the problems, prolonging the delays, and adding to the costs of taking over the VW&Y. Hill's top aides were careless about details, bookkeeping, correspondence, and reports.
The Northern Pacific and the "short squeeze" of 1901.
With 1901 and the start of the new century, James Hill now had control of both the Great Northern Railway, and the Northern Pacific (which he had obtained with the help of his friend J. P. Morgan, when that railroad went bankrupt in the depression of the mid-1890s). Hill also wanted control of the Chicago, Burlington and Quincy railroad because of its Midwestern lines and access to Chicago. The Union Pacific Railroad was the biggest competitor of Great Northern and Northern Pacific Railroads. Although Great Northern and Northern Pacific were backed by J. P. Morgan and James J. Hill, the Union Pacific was backed not only by its president, Edward H. Harriman, but by the extremely powerful William Rockefeller and Jacob Schiff.
Quietly, Harriman began buying stock in Northern Pacific with the intention of gaining control of Chicago, Burlington, and Quincy. He was within 40,000 shares of control when Hill learned of Harriman's activities and quickly contacted J. P. Morgan, who ordered his men to buy everything they could get their hands on.
The result was chaos on Wall Street. Northern Pacific stock was forced up to $1,000 per share. Many speculators, who had sold Northern Pacific "short" in the anticipation of a drop in the railroad's price, faced ruin. The threat of a real economic panic loomed. Neither side could win a distinct advantage, and the parties soon realized that a truce would have to be called. The winners of that truce were Hill and Morgan, who immediately formed the Northern Securities Company with the aim of tying together their three major rail lines As the Hill-Morgan alliance formed the Northern Securities Company, Theodore Roosevelt became president and turned his energies against the great trusts that were monopolizing trade.
The Hill Lines survive the trust-busting era.
Roosevelt sent his Justice Department to sue the Northern Securities Company in 1902. The Supreme Court in 1904 ordered it to be dissolved as a monopoly. (Ironically, the Burlington Route, Northern Pacific, and Great Northern would later merge in 1970 to form the Burlington Northern Railroad.) Hill moved on without the benefit of a central company, and acquired the Colorado and Southern Railway lines into Texas. He also built the Spokane, Portland and Seattle Railway. By the time of his death in 1916, James J. Hill was worth more than $53 million (almost $2.5 billion (2007) dollars).
The Great Northern Railway and the Northern Pacific tried to merge four times, in 1896, 1901, 1927, and 1955. This last attempt lasted from 1955 until final Supreme Court approval and merger in March, 1970, which created the Burlington Northern Railroad. In 1995, Burlington Northern merged with the Atchison, Topeka and Santa Fe Railway to become the Burlington Northern and Santa Fe Railway.
Family life.
In 1867, James J. Hill married Mary Theresa Mehegan, born in 1846 in New York City. They had ten children:
Death.
He died in his home in St. Paul, Minnesota on May 29, 1916. Mary Hill died in 1922 and was buried next to her husband by the shore of Pleasant Lake on their North Oaks farm. Because of vandals or curious admirers both graves were later moved to Resurrection Cemetery in St. Paul for safer keeping.
Hill's legacy.
Politically, Hill was a Bourbon Democrat. The Democratic Party's continued enchantment with the populist William Jennings Bryan led Hill to support Republican presidential candidates William McKinley (1896 and 1900), Theodore Roosevelt (1904), and William Howard Taft (1908 and 1912).
Hill was also a member of the Jekyll Island Club (aka The Millionaires Club) on Jekyll Island, Georgia along with J.P. Morgan and William Rockefeller.
Hill was a supporter of free trade and was one of the few supporters of free trade with Canada. In St. Paul, the city's main library building and the adjoining Hill Business Library were funded by him. In addition, he donated to numerous schools, including the Saint Paul Seminary.
In 1891, after three years of building, construction was completed on a new Hill family home on Summit Avenue in St. Paul. Over 400 workers labored on the project. Built at a cost of $930,000 and with 36000 sqft, the James J. Hill House was among the city's largest. As with his business dealings, Hill supervised the construction and design himself, hiring and firing several architects in the process. The house has many early electrical and mechanical systems that predate widespread adoption in modern domestic structures. After the death of Hill's wife in 1921, the house was donated to the Roman Catholic Archdiocese of Saint Paul and Minneapolis. It was obtained by the Minnesota Historical Society in 1978 and today is operated as a museum and gallery. Upon completion of the Summit Avenue residence, Hill had the family's old house, which he had constructed in 1878, razed.
Though a Protestant, Hill maintained a strong philanthropic relationship with the Catholic Church in St. Paul and through the northwest. Hill's historic home is located next to the Cathedral, largely due to the special relationship Hill's wife, a practicing Catholic, had with the Diocese. The Hills maintained close ties with Archbishop John Ireland and Hill was a major contributor to the Saint Paul Seminary, Macalester College, Hamline University, the University of St. Thomas, Carleton College, and other educational, religious and charitable organizations.
In order to generate business for his railroad, Hill encouraged European immigrants to settle along his line, often paying for Russian and Scandinavian settlers to travel from Europe. To promote settlement and revenue for his rail business, Hill experimented with agriculture and worked to hybridize Russian wheat for Dakota soil and weather conditions. He also ran model experimental farms in Minnesota such as North Oaks to develop superior livestock and crop yields for the settlers locating near his railroads.
An enthusiastic conservationist, Hill was invited by President Theodore Roosevelt to a governor's conference on conservation of natural resources, and later appointed to a lands commission.
Drawing on his experience in the development of Minnesota's Iron Range, Hill was, during 1911–1912, in close contact with Gaspard Farrer of Baring Brothers & Company of London regarding the formation of the Brazilian Iron Ore Company to tap that nation's rich mineral deposits.
Near the end of his life, Hill played what a recent biographer, Albro Martin, called his "last and greatest role." After the first punishing year of World War I, the Allied Powers desperately needed financial support to continue the war effort. To that end, Hill was a major figure in the effort launched by J.P. Morgan to float the Anglo-French Bond drive of 1915, which allowed the Allies to purchase much-needed foodstuffs and other supplies. In September 1915, the first public loan, the $500,000,000 Anglo-French loan, was floated. Concomitantly, the resulting trade in munitions with England and France carried the United States from a depression in 1914 to boom years in 1915 and 1916.
Hillsboro, North Dakota; Hill County, Montana; and Hillyard, Washington (now a neighborhood of Spokane) - are named for him. In 1929, the Great Northern Railway named its flagship passenger train the "Empire Builder" in his honor. The train continues as Amtrak's daily "Empire Builder," which uses former Great Northern tracks west of St. Paul, Minnesota. The James J. Hill House in St. Paul, Minnesota is a National Historic Landmark.
In 1887, the Great Northern's first company headquarters building was constructed in St. Paul. It was designed by James Brodie, who also built the Hill's house on Summit Avenue. The 1887 building was converted between 2000 and 2004 to a 53 unit condo in the Historic Lowertown District of St. Paul. Hill had seen the devastation done to downtown Chicago by the great fire of 1871. As a result, one feature Hill integrated into the construction of the 1887 company headquarters (the Great Northern General Office Building) was barrel vaulted ceilings constructed of brick and railroad steel rails that held up a layer of sand several inches deep. The theory was that if a fire broke out and the ceiling caved in, the sand would drop and retard or suppress the fire.
Hill was intimately involved in the planning and construction (1914–1916) of a new company headquarters in St. Paul (to be known as the Great Northern Office Building), which was to house the corporate staffs of the Great Northern, the Northern Pacific and Hill's banking enterprises. The 14-story building cost $14 million to construct.
Hill's heirs established the James J. Hill Reference Library in St. Paul, which is considered by the Small Business Administration the premier source for publicly accessible practical business information in the United States, and many SBA programs rely on the Hill Library's HillSearch service to provide business information resources to small businesses nationwide. The Hill Library has developed numerous online programs and now serves millions of small business owners worldwide.
In "The Great Gatsby", Hill is the man whom Gatsby's father says Gatsby would have equalled if he had lived long enough.
Hill and his railway are mentioned in the Harry McClintock song "Hallelujah! I'm a Bum."
Supercentenarian Walter Breuning, (09/21/1896 - 04/14/2011), began working as a railroader for Hill's company in 1913 at the age of 17. He stated that he had to constantly hide, as Hill did not want anyone under the age of 18.
Further reading.
Primary Sources
Secondary Sources
</dl>

</doc>
<doc id="50631" url="http://en.wikipedia.org/wiki?curid=50631" title="Ford Madox Ford">
Ford Madox Ford

Ford Madox Ford (17 December 1873 – 26 June 1939), born Ford Hermann Hueffer ( ), was an English novelist, poet, critic and editor whose journals, "The English Review" and "The Transatlantic Review", were instrumental in the development of early 20th-century English literature. He is now remembered best for his publications "The Good Soldier" (1915), the "Parade's End" tetralogy (1924–28) and "The Fifth Queen" trilogy (1906–08). "The Good Soldier" is frequently included among the great literature of the 20th century, including the Modern Library 100 Best Novels, The Observer's "100 Greatest Novels of All Time", and The Guardian's "1000 novels everyone must read".
Biography.
Ford was born in Wimbledon to Catherine Madox Brown and Francis Hueffer, the eldest of three; his brother was Oliver Madox Hueffer. His father, who became music critic for "The Times", was German and his mother English. His paternal grandfather was first to publish the fellow Westphalian poet and author Annette von Droste-Hülshoff, a Catholic aristocrat. He used the name of Ford Madox Hueffer and in 1919 changed it to Ford Madox Ford (allegedly, in the aftermath of World War I because "Hueffer" sounded too German) in honour of his grandfather, the Pre-Raphaelite painter Ford Madox Brown, whose biography he had written.
In 1894 he married his school girlfriend Elsie Martindale and together they had two daughters, Christina (born 1897) and Katharine (born 1900). Between 1918 and 1927 he lived with Stella Bowen, an Australian artist twenty years his junior. In 1920 they had a daughter, Julia Madox Ford. Ford spent the last years of his life teaching at Olivet College in Michigan, and died in Deauville, France, at the age of 65.
Literary life.
One of his most famous works is "The Good Soldier" (1915), a novel set just before World War I which chronicles the tragic lives of two "perfect couples" using intricate flashbacks. In the "Dedicatory Letter to Stella Ford”, that prefaces the novel, Ford reports that a friend pronounced "The Good Soldier" “the finest French novel in the English language!” Ford pronounced himself a "Tory mad about historic continuity" and believed the novelist's function was to serve as the historian of his own time.
Ford was involved in British war propaganda after the beginning of World War I. He worked for the War Propaganda Bureau, managed by C. F. G. Masterman, with other writers and scholars who were popular during that time, such as Arnold Bennett, G. K. Chesterton, John Galsworthy, Hilaire Belloc and Gilbert Murray. Ford wrote two propaganda books for Masterman, namely "When Blood is Their Argument: An Analysis of Prussian Culture" (1915), with the help of Richard Aldington, and "Between St Dennis and St George: A Sketch of Three Civilizations" (1915).
After writing the two propaganda books, Ford enlisted at 41 years of age into the Welch Regiment on 30 July 1915, and was sent to France, thus ending his cooperation with the War Propaganda Bureau. His combat experiences and his previous propaganda activities inspired his tetralogy "Parade's End" (1924–1928), set in England and on the Western Front before, during and after World War I.
Ford also wrote dozens of novels as well as essays, poetry, memoirs and literary criticism, and collaborated with Joseph Conrad on three novels, "The Inheritors" (1901), "Romance" (1903) and "The Nature of a Crime" (1924, although written much earlier). During the three to five years after this direct collaboration, Ford's best known achievement was The Fifth Queen trilogy (1906–1908), historical novels based on the life of Katharine Howard, which Conrad termed, at the time, "the swan song of historical romance." His poem "Antwerp" (1915) was praised by T.S. Eliot as "the only good poem I have met with on the subject of the war".
Ford's novel "Ladies Whose Bright Eyes" (1911, extensively revised in 1935) is, in a sense, the reverse of Twain's novel "A Connecticut Yankee in King Arthur's Court".
Promotion of literature.
In 1908, he founded "The English Review", in which he published works by Thomas Hardy, H. G. Wells, Joseph Conrad, Henry James, May Sinclair, John Galsworthy and William Butler Yeats, and gave debuts to Wyndham Lewis, D. H. Lawrence and Norman Douglas. In 1924, he founded "The Transatlantic Review", a journal with great influence on modern literature. Staying with the artistic community in the Latin Quarter of Paris, he befriended James Joyce, Ernest Hemingway, Gertrude Stein, Ezra Pound and Jean Rhys, all of whom he would publish (Ford is the model for the character Braddocks in Hemingway's "The Sun Also Rises)." As a critic, he is known for remarking "Open the book to page ninety-nine and read, and the quality of the whole will be revealed to you." George Seldes, in his book "Witness to a Century", describes Ford's recollection of his writing collaboration with Joseph Conrad, and the lack of acknowledgment by publishers of his status as co-author. Seldes recounts Ford's disappointment with Hemingway: "'and he disowns me now that he has become better known than I am.' Tears now came to Ford's eyes." Ford says, "I helped Joseph Conrad, I helped Hemingway. I helped a dozen, a score of writers, and many of them have beaten me. I'm now an old man and I'll die without making a name like Hemingway." Seldes observes, "At this climax Ford began to sob. Then he began to cry."
Hemingway devoted a chapter of his Parisian memoir "A Moveable Feast, Bantam Books, 1964" to an encounter with Ford at a café in Paris during the early 1920s. He describe Ford "as upright as an ambulatory, well clothed, up-ended hogshead."
During a later sojourn in the United States, he was involved with Allen Tate, Caroline Gordon, Katherine Anne Porter and Robert Lowell (who was then a student). Ford was always a champion of new literature and literary experimentation. In 1929, he published "The English Novel: From the Earliest Days to the Death of Joseph Conrad", a brisk and accessible overview of the history of English novels. He had an affair with Jean Rhys, which ended acrimoniously.

</doc>
<doc id="50637" url="http://en.wikipedia.org/wiki?curid=50637" title="Sequoiadendron giganteum">
Sequoiadendron giganteum

Sequoiadendron giganteum (giant sequoia, giant redwood, Sierra redwood, Sierran redwood, or Wellingtonia) is the sole living species in the genus "Sequoiadendron", and one of three species of coniferous trees known as redwoods, classified in the family Cupressaceae in the subfamily Sequoioideae, together with "Sequoia sempervirens" (coast redwood) and "Metasequoia glyptostroboides" (dawn redwood). The common use of the name "sequoia" generally refers to "Sequoiadendron giganteum", which occurs naturally only in groves on the western slopes of the Sierra Nevada Mountains of California. It is named after Sequoyah (1767–1843), the inventor of the Cherokee syllabary.
Description.
Giant sequoias are the world's largest single trees and largest living thing by volume.
Giant sequoias grow to an average height of 50 – and 6 – in diameter. Record trees have been measured to be 94.8 m in height and over 17 m in diameter. The oldest known giant sequoia based on ring count is 3,500 years old. Giant Sequoias are among the oldest living things on Earth. Sequoia bark is fibrous, furrowed, and may be 90 cm thick at the base of the columnar trunk. It provides significant fire protection for the trees. The leaves are evergreen, awl-shaped, 3 - long, and arranged spirally on the shoots. The seed cones are 4 - long and mature in 18–20 months, though they typically remain green and closed for up to 20 years; each cone has 30–50 spirally arranged scales, with several seeds on each scale, giving an average of 230 seeds per cone. The seed is dark brown, 4 - long and 1 mm broad, with a 1 mm wide, yellow-brown wing along each side. Some seeds are shed when the cone scales shrink during hot weather in late summer, but most are liberated when the cone dries from fire heat or is damaged by insects.
The giant sequoia regenerates by seed. Young trees start to bear cones at the age of 12 years. Trees up to about 20 years old may produce stump sprouts subsequent to injury, but unlike coast redwood, shoots do not form on the stumps of mature trees. Giant sequoias of all ages may sprout from their boles when branches are lost to fire or breakage.
At any given time, a large tree may be expected to have about 11,000 cones. Cone production is greatest in the upper portion of the canopy. A mature giant sequoia has been estimated to disperse 300,000–400,000 seeds per year. The winged seeds may be carried up to 180 m from the parent tree.
Lower branches die fairly readily from shading, but trees less than 100 years old retain most of their dead branches. Trunks of mature trees in groves are generally free of branches to a height of 20 -, but solitary trees will retain low branches.
Biology.
Because of its size, the tree has been studied for its water pull. Water from the roots can be pushed up only a few meters by osmotic pressure but can reach extreme heights by using a system of branching capillarity (capillary action) in the tree's xylem (the water tubules) and sub-pressure from evaporating water at the leaves. Sequoias supplement water from the soil with fog, taken up through air roots, at heights where the root water cannot be pulled to.
Distribution.
The natural distribution of giant sequoias is restricted to a limited area of the western Sierra Nevada, California. They occur in scattered groves, with a total of 68 groves (see list of sequoia groves for a full inventory), comprising a total area of only 144.16 km2. Nowhere does it grow in pure stands, although in a few small areas, stands do approach a pure condition. The northern two-thirds of its range, from the American River in Placer County southward to the Kings River, has only eight disjunct groves. The remaining southern groves are concentrated between the Kings River and the Deer Creek Grove in southern Tulare County. Groves range in size from 12.4 km2 with 20,000 mature trees, to small groves with only six living trees. Many are protected in Sequoia and Kings Canyon National Parks and Giant Sequoia National Monument.
The giant sequoia is usually found in a humid climate characterized by dry summers and snowy winters. Most giant sequoia groves are on granitic-based residual and alluvial soils. The elevation of the giant sequoia groves generally ranges from 1400 - in the north, to 1700 - to the south. Giant sequoias generally occur on the south-facing sides of northern mountains, and on the northern faces of more southerly slopes.
High levels of reproduction are not necessary to maintain the present population levels. Few groves, however, have sufficient young trees to maintain the present density of mature giant sequoias for the future. The majority of giant sequoias are currently undergoing a gradual decline in density since European settlement.
Ecology.
Giant sequoias are in many ways adapted to forest fires. Their bark is unusually fire resistant, and their cones will normally open immediately after a fire. The giant sequoias are having difficulty reproducing in their original habitat (and very rarely reproduce in cultivation) due to the seeds only being able to grow successfully in full sun and in mineral-rich soils, free from competing vegetation. Although the seeds can germinate in moist needle humus in the spring, these seedlings will die as the duff dries in the summer. They therefore require periodic wildfire to clear competing vegetation and soil humus before successful regeneration can occur. Without fire, shade-loving species will crowd out young sequoia seedlings, and sequoia seeds will not germinate. When fully grown, these trees typically require large amounts of water and are therefore often concentrated near streams.
Fires also bring hot air high into the canopy via convection, which in turn dries and opens the cones. The subsequent release of large quantities of seeds coincides with the optimal postfire seedbed conditions. Loose ground ash may also act as a cover to protect the fallen seeds from ultraviolet radiation damage.
Due to fire suppression efforts and livestock grazing during the early and mid 20th century, low-intensity fires no longer occurred naturally in many groves, and still do not occur in some groves today. The suppression of fires also led to ground fuel build-up and the dense growth of fire-sensitive white fir. This increased the risk of more intense fires that can use the firs as ladders to threaten mature giant sequoia crowns. Natural fires may also be important in keeping carpenter ants in check.
In 1970, the National Park Service began controlled burns of its groves to correct these problems. Current policies also allow natural fires to burn. One of these untamed burns severely damaged the second-largest tree in the world, the Washington tree, in September 2003, 45 days after the fire started. This damage made it unable to withstand the snowstorm of January 2005, leading to the collapse of over half the trunk.
In addition to fire, two animal agents also assist giant sequoia seed release. The more significant of the two is a longhorn beetle ("Phymatodes nitidus") that lays eggs on the cones, into which the larvae then bore holes. This cuts the vascular water supply to the cone scales, allowing the cones to dry and open for the seeds to fall. Cones damaged by the beetles during the summer will slowly open over the next several months. Some research indicates many cones, particularly higher in the crowns, may need to be partially dried by beetle damage before fire can fully open them. The other agent is the Douglas squirrel ("Tamiasciurus douglasi") that gnaws on the fleshy green scales of younger cones. The squirrels are active year round, and some seeds are dislodged and dropped as the cone is eaten.
Discovery and naming.
The giant sequoia was well known to Native American tribes living in its area. Native American names for the species include "wawona", "toos-pung-ish" and "hea-mi-withic", the latter two in the language of the Tule River Tribe.
The first reference to the giant sequoia by Europeans is in 1833, in the diary of the explorer J. K. Leonard; the reference does not mention any locality, but his route would have taken him through the Calaveras Grove. This discovery was not publicized. The next European to see the species was John M. Wooster, who carved his initials in the bark of the 'Hercules' tree in the Calaveras Grove in 1850; again, this received no publicity. Much more publicity was given to the "discovery" by Augustus T. Dowd of the Calaveras Grove in 1852, and this is commonly cited as the species' discovery. The tree found by Dowd, christened the 'Discovery Tree', was felled in 1853.
The first scientific naming of the species was by John Lindley in December 1853, who named it "Wellingtonia gigantea", without realizing this was an invalid name under the botanical code as the name "Wellingtonia" had already been used earlier for another unrelated plant ("Wellingtonia arnottiana" in the family Sabiaceae). The name "Wellingtonia" has persisted in England as a common name. The following year, Joseph Decaisne transferred it to the same genus as the coast redwood, naming it "Sequoia gigantea", but again this name was invalid, having been applied earlier (in 1847, by Endlicher) to the coast redwood. The name "Washingtonia californica" was also applied to it by Winslow in 1854, though this too is invalid, belonging to the palm genus "Washingtonia".
In 1907, it was placed by Carl Ernst Otto Kuntze in the otherwise fossil genus "Steinhauera", but doubt as to whether the giant sequoia is related to the fossil originally so named makes this name invalid.
The nomenclatural oversights were finally corrected in 1939 by J. Buchholz, who also pointed out the giant sequoia is distinct from the coast redwood at the genus level and coined the name "Sequoiadendron giganteum" for it.
John Muir wrote of the species in about 1870:
"Do behold the King in his glory, King Sequoia! Behold! Behold! seems all I can say. Some time ago I left all for Sequoia and have been and am at his feet, fasting and praying for light, for is he not the greatest light in the woods, in the world? Where are such columns of sunshine, tangible, accessible, terrestrialized?' 
Uses.
Wood from mature giant sequoias is highly resistant to decay, but due to being fibrous and brittle, it is generally unsuitable for construction. From the 1880s through the 1920s, logging took place in many groves in spite of marginal commercial returns. Due to their weight and brittleness, trees would often shatter when they hit the ground, wasting much of the wood. Loggers attempted to cushion the impact by digging trenches and filling them with branches. Still, as little as 50% of the timber is estimated to have made it from groves to the mill. The wood was used mainly for shingles and fence posts, or even for matchsticks.
Pictures of the once majestic trees broken and abandoned in formerly pristine groves, and the thought of the giants put to such modest use, spurred the public outcry that caused most of the groves to be preserved as protected land. The public can visit an example of 1880s clear-cutting at Big Stump Grove near General Grant Grove. As late as the 1980s, some immature trees were logged in Sequoia National Forest, publicity of which helped lead to the creation of Giant Sequoia National Monument.
The wood from immature trees is less brittle, with recent tests on young plantation-grown trees showing it similar to coast redwood wood in quality. This is resulting in some interest in cultivating giant sequoia as a very high-yielding timber crop tree, both in California and also in parts of western Europe, where it may grow more efficiently than coast redwoods. In the northwest United States, some entrepreneurs have also begun growing giant sequoias for Christmas trees. Besides these attempts at tree farming, the principal economic uses for giant sequoia today are tourism and horticulture.
Cultivation.
Giant sequoia is a very popular ornamental tree in many areas. It is successfully grown in most of western and southern Europe, the Pacific Northwest of North America north to southwest British Columbia, the southern United States, southeast Australia, New Zealand and central-southern Chile. It is also grown, though less successfully, in parts of eastern North America.
Trees can withstand temperatures of −31 °C (−25 °F) or colder for short periods of time, provided the ground around the roots is insulated with either heavy snow or mulch. Outside its natural range, the foliage can suffer from damaging windburn.
Since its discovery, a wide range of horticultural varieties have been selected, especially in Europe. There are, amongst others, weeping, variegated, pygmy, blue, grass green, and compact forms.
Europe.
The giant sequoia was brought into cultivation in 1853 by Scotsman John D. Matthew, who collected a small quantity of seed in the Calaveras Grove,and took it home to his noted Horticulturist father Patrick Matthew of Gourdiehill near Errol in Perth Shire arriving with it in Scotland in August 1853. A much larger shipment of seed collected (also in the Calaveras Grove) by William Lobb, acting for the Veitch Nursery at Budlake near Exeter, arrived in England in December 1853; seed from this batch was widely distributed throughout Europe.
Growth in Britain is very fast, with the tallest tree, at Benmore in southwest Scotland, reaching 56.4 min 2014 at age 150 years, and several others from 50 - tall; the stoutest is around 12 m in girth and 4 m in diameter, in Perthshire. The Royal Botanic Gardens at Kew in London also contains a large specimen. The General Sherman of California has a volume of 1489 m3; by way of comparison, the largest giant sequoias in Great Britain have volumes no greater than 90 -, one example being the 90 m3 specimen in the New Forest.
Numerous giant sequoia were planted in Italy from 1860 through 1905. Several regions contain specimens that range from 40 to in height. The largest tree is in Roccavione, in the Piedmont, with a basal circumference of 16 m. One notable tree survived a 200 m tall flood wave in 1963 that was caused by a landslide at Vajont Dam. There are numerous giant sequoia in parks and reserves.
Growth rates in some areas of Europe are remarkable. One young tree in Italy reached 22 m tall and 88 cm trunk diameter in 17 years (Mitchell, 1972). The tallest specimen measured in France is a tree near Ribeauvillé in France, at a height of 56.3 m.
Growth further northeast in Europe is limited by winter cold. In Denmark, where extreme winters can reach -32 C, the largest tree was 35 m tall and 1.7 m diameter in 1976 and is bigger today. One in Poland has purportedly survived temperatures down to -37 C with heavy snow cover.
Two members of the German Dendrology Society, E. J. Martin and Illa Martin, introduced the giant sequoia into German forestry at the Sequoiafarm Kaldenkirchen in 1952.
Twenty-nine giant sequoias, measuring around 30 m in height, grow in Belgrade's municipality of Lazarevac in Serbia.
The oldest "sequoiadendron" in the Czech Republic, at 44 m, grows in Ratměřice u Votic castle garden.
United States and Canada.
Giant sequoias are grown successfully in the Pacific Northwest and southern US, and less successfully in eastern North America. Giant sequoia cultivation is very successful in the Pacific Northwest from western Oregon north to southwest British Columbia, with fast growth rates. In Washington and Oregon, it is common to find giant sequoias that have been successfully planted in both urban and rural areas. In the Seattle area, large specimens (over 90 feet) are fairly common and exist in several city parks and many private yards (especially east Seattle including Capitol Hill, Washington Park, & Leschi/Madrona).
In the northeastern US there has been some limited success in growing the species, but growth is much slower there, and it is prone to "Cercospora" and "Kabatina" fungal diseases due to the hot, humid summer climate there. A tree at Blithewold Gardens, in Bristol, Rhode Island is reported to be 27 m tall, reportedly the tallest in the New England states. The tree at the Tyler Arboretum in Delaware County, Pennsylvania at 29.1 m may be the tallest in the northeast. Specimens also grow in the Arnold Arboretum in Boston, Massachusetts (planted 1972, 18 m tall in 1998), at Longwood Gardens near Wilmington, Delaware, in the New Jersey State Botanical Garden at Skylands in Ringwood State Park, Ringwood, New Jersey, and in the Finger Lakes region of New York. Private plantings of giant sequoias around the Middle Atlantic States are not uncommon. Since 2000, a small amateur experimental planting has been underway in the Lake Champlain valley of Vermont at the Vermont Experimental Cold-Hardy Cactus Garden where winter temperatures can reach −37 °C with variable snowcover. A few trees have been established in Colorado as well. Additionally, numerous sequoias have been planted with success in the state of Michigan.
A cold-tolerant cultivar 'Hazel Smith' selected in about 1960 is proving more successful in the northeastern US. This clone was the sole survivor of several hundred seedlings grown at a nursery in New Jersey.
Australia.
The Ballarat Botanical Gardens contain a significant collection, many of them about 150 years old. Jubilee Park and the Hepburn Mineral Springs Reserve in Daylesford, Cook Park in Orange, New South Wales and Carisbrook's Deep Creek park in Victoria both have specimens. Jamieson Township in the Victorian high country has 2 specimens which were planted in the early 1860s. In Tasmania specimens are to be seen in private and public gardens, as they were popular in the mid Victorian era. The Westbury Village Green has mature specimens with more in Deloraine. The Tasmanian Arboretum contains young wild collected material. The National Arboretum Canberra has begun a grove. They also grow in the abandoned arboretum at Mount Banda Banda in New South Wales.
New Zealand.
Several impressive specimens of "Sequoiadendron giganteum" can be found in the South Island of New Zealand. Notable examples include a set of trees in a public park of Picton, as well as robust specimens in the public and botanical parks of Queenstown. There is also a tree at Rangiora High School, which was planted for Queen Victoria's Golden Jubilee and is thus over 125 years old.
Superlatives.
Largest by trunk volume.
Some sequoias, such as the Mother of the Forest, were undoubtedly far larger than any living tree today. However, as of 2009, the top ten largest giant sequoias sorted by volume of their trunks are:
Further references.
</dl>

</doc>
<doc id="50640" url="http://en.wikipedia.org/wiki?curid=50640" title="Texas Longhorn">
Texas Longhorn

The Texas Longhorn is a breed of cattle known for its characteristic horns, which can extend to 7 ft tip to tip for steers and exceptional cows, and 36 to tip to tip for bulls. Similar cattle were imported by Spanish colonists into other parts of North America, including California and Florida. Horns can have a slight upward turn at their tips or even triple twist. Texas Longhorns are known for their diverse coloring. A longhorn can be any color or mix of colors, but dark red and white color mixes are the most dominant. Texas Longhorns with elite genetics can often fetch $40,000 or more at auction with the record of $170,000 in recent history for a cow. Due to their innate gentle disposition and intelligence, Texas Longhorns are increasingly being trained as riding steers.
Registries for the breed include the Texas Longhorn Breeders Association of America, the International Texas Longhorn Association, and the Cattlemen’s Texas Longhorn Registry.
History of the breed.
Genetic analyses show the Longhorn originated from an Iberian hybrid of two ancient cattle lineages: "taurine" descending from the domestication of the wild aurochs in the Middle East, and "indicine", descending from the domestication of the aurochs in India, 85% and 15% respectively by proportion. The Texas Longhorns are direct descendants of the first cattle in the New World. The ancestral cattle were first brought over by Christopher Columbus in 1493 to the Caribbean island of Hispaniola. Between 1493 and 1512, Spanish colonists brought additional cattle in subsequent expeditions. The cattle consisted of three different breeds; "Barrenda", "Retinto" and "Grande Pieto". Over the next two centuries the Spanish moved the cattle north, arriving in the area that would become Texas near the end of the 17th century. The cattle escaped or were turned loose on the open range, where they remained mostly feral for the next two centuries. Over several generations, descendants of these cattle evolved the high feed- and drought-stress tolerance and other "hardy" characteristics that Longhorns have become known for.
Early US settlers in Texas obtained feral Mexican cattle from the borderland between the Nueces River and the Rio Grande and mixed them with their own eastern cattle. The result was a tough, rangy animal with long legs and long horns extending up to seven feet. Although this interbreeding was of little consequence to the makeup of a Longhorn, it did alter color. The varieties of color ranged from bluish-grey, and various yellowish hues, to browns, black, ruddy and white, both cleanly bright and dirty-speckled. Portuguese cattle breeds, such as Alentejana and Mertolenga, are the closest relatives of Texas Longhorns.
As Texas became more heavily settled following annexation by the US, the frontier gave way to established farms and ranch lands. The leaner longhorn beef was not as attractive in an era where tallow was highly prized, and the longhorn's ability to survive on the poor vegetation of the open range was no longer as much of an issue. Other breeds demonstrated traits more highly valued by the modern rancher, such as the ability to gain weight quickly. The Texas Longhorn stock slowly dwindled, until in 1927 the breed was saved from near extinction by enthusiasts from the United States Forest Service, who collected a small herd of stock to breed on the Wichita Mountains Wildlife Refuge in Lawton, Oklahoma. A few years later, J. Frank Dobie and others gathered small herds to keep in Texas state parks. They were cared for largely as curiosities, but the stock's longevity, resistance to disease and ability to thrive on marginal pastures quickly revived the breed as beef stock. Today, the breed is used as a beef stock, though many Texas ranchers keep herds due to their link to Texas history.
Purpose.
Most breeds of cattle fall into either beef or dairy. The Texas Longhorn is principally a beef animal and is known for its lean beef, which is lower in fat, cholesterol and calories than most beef. 
These measurements can be adjusted to "horn per month of age" (HMA) which is calculated by dividing the number of months of age into the horn measurement. For example, a 48-month-old animal with 50" of horn would be 50 / 48 or 1.04" per month of age.
Commercial ranchers cross-breed longhorns with other breeds for increasing hybrid vigor and easy calving characteristics. Smaller birth weights reduce dystocia for first-calf heifers.

</doc>
