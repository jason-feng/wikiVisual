<doc id="52266" url="http://en.wikipedia.org/wiki?curid=52266" title="War of the Austrian Succession">
War of the Austrian Succession

The War of the Austrian Succession (1740–48) involved most of the powers of Europe over the question of Maria Theresa's succession to the realms of the House of Habsburg. The war included King George's War in North America, the War of Jenkins' Ear (which formally began on 23 October 1739), the First Carnatic War in India, and the First and Second Silesian Wars.
The war began under the pretext that Maria Theresa was ineligible to succeed to the Habsburg thrones of her father, Charles VI, because Salic law precluded royal inheritance by a woman—though in reality this was a convenient excuse put forward by Prussia and France to challenge Habsburg power. Austria was supported by Great Britain and the Dutch Republic, the traditional enemies of France, as well as the Kingdom of Sardinia and the Electorate of Saxony. France and Prussia were allied with the Electorate of Bavaria.
Spain, which had been at war with Britain over colonies and trade ever since 1739, entered the war on the Continent to re-establish its influence in northern Italy, further reversing an Austrian dominance over the Italian peninsula that had been achieved at Spain's expense as a consequence of Spain's war of succession earlier in the 18th century.
The war ended with the Treaty of Aix-la-Chapelle in 1748, by which Maria Theresa was confirmed as Archduchess of Austria and Queen of Hungary, but Prussia retained control of Silesia.
Background.
In 1740, after the death of her father, Charles VI, Maria Theresa succeeded him as Queen of Hungary, Croatia and Bohemia, Archduchess of Austria and Duchess of Parma. Her father had been Holy Roman Emperor, but Maria Theresa was not a candidate for that title, which had never been held by a woman; the plan was for her to succeed to the hereditary domains, and her husband, Francis Stephen, to be elected Holy Roman Emperor. The complications involved in a female Habsburg ruler had been long foreseen, and Charles VI had persuaded most of the states of Germany to agree to the Pragmatic Sanction of 1713.
Problems began when King Frederick II of Prussia violated the Pragmatic Sanction and invaded Silesia on 16 December 1740, using the 1537 Treaty of Brieg (under which the Hohenzollerns of Brandenburg were to inherit the Duchy of Brieg) as a pretext. Maria Theresa, as a woman, was perceived as weak, and other rulers (such as Charles Albert of Bavaria) put forward their own competing claims to the crown as male heirs with a clear genealogical basis to inherit the elected dignities of the great Imperial title.
Strategies.
For much of the eighteenth century, France approached its wars in the same way. It would let its colonies defend themselves, or would offer only minimal help (sending them only limited numbers of troops or inexperienced soldiers), anticipating that fights for the colonies would likely be lost anyway. This strategy was, to a degree, forced upon France: geography, coupled with the superiority of the British navy, made it difficult for the French navy to provide significant supplies and support to French colonies. Similarly, several long land borders made an effective domestic army imperative for any ruler of France. Given these military necessities, the French government, unsurprisingly, based its strategy overwhelmingly on the army in Europe: it would keep most of its army on the European continent, hoping that such a force would be victorious closer to home. At the end of the war, France gave back its European conquests, and recovered such lost overseas possessions as Louisburg, largely restoring the "status quo ante" as far as France was concerned.
The British—by inclination as well as for pragmatic reasons—had tended to avoid large-scale commitments of troops on the Continent. They sought to offset the disadvantage this created in Europe by allying themselves with one or more Continental powers whose interests were antithetical to those of their enemies, particularly France. For the War of the Austrian Succession, the British were allied with Austria; by the time of the Seven Years' War, they were allied with its enemy, Prussia. In marked contrast to France, Britain strove to actively prosecute the war in the colonies once it became involved in the war, taking full advantage of its naval power. The British pursued a dual strategy of naval blockade and bombardment of enemy ports, and also utilized their ability to move troops by sea to the utmost. They would harass enemy shipping and attack enemy outposts, frequently using colonists from nearby British colonies in the effort. This plan worked better in North America than in India, but set the stage for the Seven Years' War.
Silesian Campaign of 1740.
Prussia in 1740 was a small but well-organised emerging international power whose new, well-educated king, Frederick II wanted to unify the disparate and scattered crown holdings by gathering intervening lands into a unified, contiguous state. Prince Frederick had been only 28 years of age on 31 May 1740 when his father, Frederick William I died and Prince Frederick ascended to the throne. Although Prussia and Austria had been allies in the War of the Polish Succession (1733–1738), concluded only two years before, the interests of the two countries diverged when the Holy Roman Emperor, Charles VI, died on 20 October 1740.
Emperor Charles VI had made provision for the succession of his daughter, Maria Theresa to the throne as Empress of the Holy Roman Empire. Although the Salic law prevented succession through the female line, during his lifetime Emperor Charles VI obtained consent from several of the individual German states that were a part of the Holy Roman Empire for his Pragmatic Sanction of 1713 which circumvented the Salic Law to allow his daughter to succeed to the throne. While the Austrian Habsburg Monarchy was distracted by the succession, Frederick, upon coming to the throne, rejected the Pragmatic Sanction of 1713 and opportunistically invaded Silesia on 16 December 1740. In support of his invasion of Silesia, Frederick also used a questionable interpretation of a treaty (1537) between the Hohenzollerns and the Piasts of Brieg as pretext. What Frederick really feared was that other princes of Europe were preparing to exploit the succession struggle to acquire Habsburg possessions for themselves and/or diminish the power of the Holy Roman Empire. In particular, Frederick feared that Augustus III, Elector of Saxony and King of Poland, was preparing to seize Silesia for himself to unite Saxony and Poland.
The only recent combat experience of the Prussian Army was their participation in the War of the Polish Succession ("Rhine campaign of 1733–1735"), in which the Prussian Army had largely been kept out of combat. Nobody in the court of the Holy Roman Empire trusted the motives of the new rising power of Prussia and, therefore, the Holy Roman Emperor did not call on the Prussians, who were vassals of the Holy Roman Empire, for military support of the Empire. Accordingly, the Prussian Army had an uninspiring reputation and was counted as one of the many minor armies of the Holy Roman Empire. This reputation misrepresented the fact of a standing army of 80,000 soldiers representing 4% of the 2.2 million population of Prussia. Thus the Prussian Army was disproportionate to the size of the state it protected. By comparison, the Austrian Empire had 16 million citizens but had an army only half its authorised size because of financial restraints. Thus, in defending the vast territory of the Austrian Empire this small army was more of "a sieve" than a shield against foreign invasion.
Moreover, the Prussian army was better trained than other armies in Europe. King Frederick William I and "the guiding genius of the Prussian Army" Leopold I, Prince of Anhalt-Dessau or "Old Dessauer" had drilled the Prussian Army to a perfection previously unknown in Europe. The Prussian infantry soldier was so well-trained and well-equipped that he could fire 4 shots a minute to an Austrian's 3 shots per minute; while Prussian cavalry and artillery were comparatively less efficient, they were still of better quality than average. Furthermore, while the Austrians had to wait for conscription to complete the field forces, Prussian regiments took the field at once. With this army it might not have been surprising that Frederick was able to overrun Silesia. However, Frederick sought even more advantages in the war he was planning. Accordingly, he had his Foreign Minister--Heinrich von Podewils—secretly negotiate a treaty with France to put Austria in a two front war. In this way, Prussia could attack the Austrians in the east while France would attack Austria from the west. A treaty with France was signed in April 1739.
The Prussian army had massed quietly along the Oder river during early December, and on 16 December 1740, without declaration of war, Frederick moved his Army across the frontier into Silesia. The extant forces available to the local Austrian generals could do no more than garrison a few fortresses, and they necessarily fell back to the mountain frontier of Bohemia and Moravia with only a small remnant of their available forces left in the garrisons. Of these fortresses in Silesia, only Glogau, Breslau and Brieg remained in Austrian hands following Frederick's initial campaign. The Prussians were able to capture the fortress at Ohlau almost immediately and use it as winter quarters. Thus the Prussians were able to go into winter quarters, holding all Silesia and investing the strongholds of Glogau, Brieg and Neisse. In one step, Prussia had effectively doubled its population and made huge gains in its industrial productivity and resource base.
Nationalism as we know it today was not a factor, but an evolving concept just coming into its early years. Prussia benefited greatly from the apolitical nature of the society of the era, as the masses in central Germany would correspondingly suffer as the contending armies rampaged through their plains yet again.
Allies in Bohemia 1741.
Early in the year, a new Austrian field army under general Wilhelm Reinhard von Neipperg relieved Neisse and marched on Brieg, threatening to cut the Prussians off. On 10 April, Frederick's army caught the Austrians on the snow-covered fields near Mollwitz. This was the first time that Frederick had led troops into battle. The victory that Frederick attained at the Battle of Mollwitz was a learning experience for the young King.
Frederick obtained an alliance with the French against the Austrians, signing the Treaty of Breslau on 5 June. Accordingly, the French began to cross the Rhine on 15 August and joined the Bavarian Elector's forces on the Danube and advanced towards Vienna. The combined forces of the French and the Bavarians captured the Austrian town of Linz on 14 September. However, at this point, the objective was suddenly changed, and after many countermarches the anti-Austrian allies advanced, in three widely separated corps, on Prague. A French corps moved via Amberg and Pilsen. The Elector marched on Budweis, and the Saxons (who had now joined the allies against Austria) invaded Bohemia by the Elbe valley. The Austrians could at first offer little resistance, but before long a considerable force intervened at Tábor between the Danube and the allies, and Austrian troops including Neipperg were soon transferred from Silesia back to the west to defend the Austrian capital, Vienna, from the French.
With fewer Austrian troops in Silesia Frederick now had an easier time. The remaining fortresses in Silesia were taken by the Prussians. Before he left Silesia, Austrian General Neipperg had made a curious agreement with Frederick, the so-called Klein-Schnellendorf agreement (9 October 1741). By this agreement, the fortress at Neisse was surrendered after a mock siege, and the Prussians agreed to let the Austrians leave unmolested releasing Neipperg's army for service elsewhere. At the same time the Hungarians, moved to enthusiasm by the personal appeal, in September 1741, of Maria Theresia, had put into the field a "levée en masse," or "insurrection," which furnished the regular army with an invaluable force of 60,000 more light troops. A fresh army was collected under Field Marshal Khevenhüller at Vienna, and the Austrians planned an offensive winter campaign against the Franco-Bavarian forces in Bohemia and the small Bavarian army that remained on the Danube to defend the electorate.
The French in the meantime had stormed Prague on 26 November 1741, Francis Stephen, husband of Maria Theresa, who commanded the Austrians in Bohemia, moving too slowly to save the fortress. The Elector of Bavaria, who now styled himself Archduke of Austria, was crowned King of Bohemia (9 December 1741) and elected to the imperial throne as Charles VII (24 January 1742), but no active measures were undertaken.
In Bohemia the month of December was occupied in mere skirmishes. On the Danube, Khevenhüller, the best general in the Austrian service, advanced on 27 December, swiftly drove back the allies, shut them up in Linz, and pressed on into Bavaria. Munich itself surrendered to the Austrians on the coronation day of Charles VII.
At the close of this first act of the campaign the French, under the old Marshal de Broglie, maintained a precarious foothold in central Bohemia, menaced by the main army of the Austrians, and Khevenhüller was ranging unopposed in Bavaria. Frederick made a secret truce with Austria and thus, lay inactive in Silesia.
Campaigns of 1742.
Frederick had hoped by the truce to secure Silesia, for which alone he was fighting. But with the successes of Khevenhüller and the enthusiastic "insurrection" of Hungary, Maria Theresa's opposition became firmer, and she divulged the provisions of the truce, to compromise Frederick with his allies. The war recommenced. Frederick had not rested on his laurels. In the uneventful summer campaign of 1741 he had found time to begin the reorganisation of his cavalry. The training of the Prussian cavalry had been neglected by Frederick's father—King Frederick William I. Probably because he was an infantryman to his core, the training of the cavalry had also been overlooked by the "Old Dessauer" who was the true genius behind the Prussian Army. Frederick had been disappointed by the performance of his cavalry at the Battle of Mollwitz. However, as a result of Frederick's training of the cavalry, over the summer of 1741, the Prussian cavalry would soon acquit themselves much better in the coming battles of the First Silesian War. Before long the cavalry would be more efficient than the Prussian infantry.
The Emperor Charles VII, whose territories were overrun by the Austrians, asked him to create a diversion by invading Moravia. In December 1741, therefore, the Prussian general field marshal Kurt Christoph Graf von Schwerin crossed the border and captured Olmutz. Glatz also was invested, and the Prussian army was concentrated about Olomouc in January 1742. A combined plan of operations was made by the French, Saxons and Prussians for the rescue of Linz. But Linz soon fell. Broglie on the Vltava, weakened by the departure of the Bavarians to oppose Khevenhüller, and of the Saxons to join forces with Frederick, was in no condition to take the offensive, and large forces under Prince Charles of Lorraine lay in his front from Budweis to Jihlava (Iglau). Frederick's march was made towards Iglau in the first place. Brno was invested about the same time (February), but the direction of the march was changed, and instead of moving against Prince Charles, Frederick pushed on southwards by Znojmo and Mikulov. The extreme outposts of the Prussians appeared before Vienna. But Frederick's advance was a mere foray, and Prince Charles, leaving a screen of troops in front of Broglie, marched to cut off the Prussians from Silesia, while the Hungarian levies poured into Upper Silesia by the Jablunkov Pass. The Saxons, discontented and demoralised, soon marched off to their own country, and Frederick with his Prussians fell back by Svitavy and Litomyšl to Kutná Hora in Bohemia, where he was in touch with Broglie on the one hand and (Glatz having now surrendered) with Silesia on the other. No defence of Olomouc was attempted, and the small Prussian corps remaining in Moravia fell back towards Upper Silesia.
Prince Charles marched past Jihlava and Teutsch (Deutsch) Brod on Kutná Hora in pursuit of Frederick. On 17 May 1742 Frederick turned around and faced the Austrian forces that were pursuing him. He fought the Austrians in what has become known as the Battle of Chotusitz. After a severe struggle Frederick won a major Prussian victory. At Chotusitz, it was Frederick's newly reorganised and trained cavalry that really won the victory and compensated for their previous failings. The cavalry's conduct gave an earnest prospect of its future glory, not only by its charges on the battlefield, but by its vigorous pursuit of the defeated Austrians.
Almost at the same time the Battle of Chotusitz was occurring, French Field Marshal François Broglie fell upon a part of the Austrians left on the Vltava and won a small, but morally and politically important, success in the action of Sahay, near Budweis (24 May 1742). Frederick did not propose another combined movement. Frederick's victory at Chotusitz, along with the victory of Field Marshal Broglie, persuaded Maria Theresa to seek peace even if it meant ceding away Silesia to make good her position elsewhere. Accordingly, a separate peace between Prussia and Austria was signed at Breslau on 11 June 1742, which drew the First Silesian War to a close. However, the larger War of the Austrian Succession continued.
Campaign of 1743.
The year 1743 opened disastrously for the Holy Roman Emperor—Charles VII. The French and Bavarian armies were not working well together, and Field Marshal Broglie had been placed in command of the allied army in Bavaria. This created tension between Broglie and the Bavarian commanders. Broglie openly quarrelled with the Bavarian field marshal Friedrich Heinrich von Seckendorff. No connected resistance was offered to the converging march of Prince Charles's army along the Danube, Khevenhüller from Salzburg towards southern Bavaria, and Prince Lobkowitz from Bohemia towards the Naab. The Bavarians, under the command of Count Minuzzi, suffered a severe reverse at the town of Simbach near Braunau on 9 May 1743 at the hands of Prince Charles.
Now an Anglo–Allied army commanded by King George II retreated down the Main River to the village of Hanau. This army had been formed on the lower Rhine upon the withdrawal of the French (Westphalian) Army under the command of Count Maillebois. This British army became known as the "Pragmatic Army," because it was a confederation of states that supported the Pragmatic Sanction of 1713 agreements to recognize Maria Theresa as sovereign of the Habsburg Empire. The Pragmatic Army had been advancing southward up the Main into Neckar country prior to this retreat in the summer of 1743. A French army, under Marshal Noailles, was being collected on the middle Rhine to deal with this new force. Marshal Noailles correctly anticipated that given the problems faced by the Pragmatic Army, George II would take the entire British Pragmatic Army back down the Main. Marshal Noailles made plans to lay a trap for the Pragmatic Army and destroy it. However, Marshal Noailles's ally Marshal Broglie was now in full retreat. Strong places of Bavaria were surrendered one after the other to Prince Charles. Marshal Noailles's French and Bavarian army was suddenly outnumbered by the Pragmatic Army 35,000 soldiers to 26,000 soldiers. Before Marshal Noailles could order a retreat, his army was surprised and defeated on 27 June 1743 by King George at Dettingen.
King Frederick of Prussia was terrified by the defeat at Dettingen. Frederick saw that he now faced a coalition of enemies that included Austria, England, Saxony and Russia. However, Frederick soon realised that the coalition against him was not as strong as it first appeared. Neither Austria nor the English knew how to exploit their victory at Dettingen. Marshal Noailles was driven almost to the Rhine by King George. The French and Bavarian army had been completely outmanoeuvred and was in a position of the greatest danger between Aschaffenburg and Hanau in the defile formed by the Spessart Hills and the river Main. Yet the English Pragmatic Army did not quickly follow up the attack. Thus, Marshal Noailles had time to block the outlet and had posts all around. At this point, the allied troops had to force their way through the French and Bavarian lines. Still, because of the heavy losses inflicted on the French, the Battle of Dettingen and the follow up is justly reckoned as a notable victory of Anglo-Austrian-Hanoverian arms.
The coalition against Frederick was suddenly weakened when the St. Petersburg court discovered a plot to overthrow Tsarina Elisabeth and bring back the child Ivan VI as Tsar, with his mother Grand Duchess Anna Leopoldovna serving as regent for the child. Matters were made much worse for the allies against Frederick when an Austrian envoy Antoniotto Botta Adorno was found to be intimately involved in the plot. Indeed the plot became known as the "Botta Conspiracy." The Botta-attempted coup redounded badly not only against Austria, but also against the Saxon and British courts. Frederick's initial indifference to a treaty with Russia now changed to enthusiasm in the light of the fallout from the Botta Conspiracy.
Marshal Broglie, worn out by age and exertions, was soon replaced by Marshal Coigny. Both Broglie and Noailles were now on the strict defensive behind the Rhine. Not a single French soldier remained in Germany, and Prince Charles prepared to force the passage of the Rhine river in the Breisgau while George II, King of Britain, moved forward via Mainz to co-operate by drawing upon himself the attention of both the French marshals. The Anglo-allied army took Worms, but after several unsuccessful attempts to cross the Rhine river, Prince Charles went into winter quarters. The king followed his example, drawing his troops to the north, to deal, if necessary, with the army which the French were collecting on the frontier of the Southern Netherlands. Austria, Britain, the Dutch Republic and Sardinia were now allied. Saxony changed sides and the entry of Sweden had offset the loss of Russia to the allies. Thus, Sweden and Russia neutralised each other (Peace of Åbo, August 1743). Frederick was still quiescent. France, Spain and Bavaria actively continued the struggle against Maria Theresa.
While the Battle of Dettingen and Russian Botta plot were capturing all the attention during the summer of 1743, negotiations between the British, the Austrians and Piedmont-Sardinians were proceeding quietly in the city of Worms. The Austrians were desperately afraid that Frederick II would soon be invading the Austrian domains again. Thus, the Austrians sought a separate peace with Piedmont-Sardinia in Italy. Under the terms of the Treaty of Worms, which was signed on 13 September 1743, the Austrian Habsburgs surrendered all territory in Italy located west of the Ticino River and Lake Maggiore to Piedmont-Sardinia. Additionally some lands south of the Po River were also given to Piedmont-Sardinia. In exchange, Piedmont-Sardinia dropped out of the war.
Campaign of 1744.
With 1744 began the Second Silesian War. Frederick of Prussia was disquieted by the universal success of the Austrians and their separate peace with Piedmont-Sardinia. Accordingly, he secretly concluded another fresh alliance with Louis XV of France. France had posed hitherto as an auxiliary, its officers in Germany had worn the Bavarian cockade, and only with Britain was it officially at war. France now declared war directly upon Austria and Sardinia (April 1744).
At this point, France also planned a diversion which they hope would cause Britain to leave the war. A French army was assembled at Dunkirk to support the cause of Charles Edward Stuart (Bonnie Prince Charlie) in an invasion of Great Britain. Prince Charles was son of James Francis Edward Stuart, Stuart pretender to the British throne, who was the son of James II the last Catholic and last Stuart king of England. James II had been deposed as the King of England in 1688 in favour of his daughter, Mary, and her husband the Protestant Prince of Orange--William III of the house of Orange-Nassau. There remained a significant element of the population of the British Isles that hoped for the return of the Stuart family as monarchs. King Louis XIV of France had shown great support for Stuart cause. Indeed in 1715, France had sponsored an uprising in Scotland in 1715, which the pretender James had joined, but it was defeated. Forbidden to return to France by the new king, Louis XV, James sought sanctuary elsewhere. Finally, Pope Clement XI offered James and his family the use of Palazzo Muti and a lifetime annuity of 8,000 Roman scudi. It was here, in the Palazzo Muti, that Charles Edward Stuart, was born and had lived his whole life.
Charles had much more charisma than his father, and now Louis XV was favourably disposed toward helping him create another uprising in Scotland. Louis XV sent Drummond of Balhaldy as an emissary to the Stuart "court" in Rome. French plans called for Charles to be in Dunkirk, France, to assemble with the fleet on 10 January 1744, yet Balhaldy had only arrived in Rome on 19 December 1743. Thus there was very little time to waste. On 23 December 1743, Charles' father named him "Prince Regent" so that Charles could act in his own name. In the spring of 1744, Prince Charles secretly arrived in France and was about to board the ships that would take him to England. However, on the night before he was to board, a fierce storm blew up (this storm became known as the "Protestant Wind") and wrecked or dispersed the entire fleet. The violent storms had wrecked the crossing attempt, and the planned invasion was abandoned. However, Charles did not give up hope of restoring the Stuart family to the throne of England.
During naval operations that were possible preparations for the return of Charles Edward Stuart to England and a coordinated French invasion of England, there occurred, on 22 February 1744, the largest sea battle of the war. This naval battle took place in the Mediterranean off the coast of Toulon, France. A large British fleet under the command of Admiral Thomas Mathews with Rear Admiral Richard Lestock, second in command was blockading the French coast. A smaller French and Spanish naval force attacked the British blockade and damaged some of the British ships forcing the British to withdraw and seek repairs. Thus, the British blockade of the French coast was relieved and the Spanish fleet apparently controlled the Mediterranean Sea. A Spanish squadron took refuge in the harbour at Toulon. The British fleet watched this squadron carefully from a harbour a short distance to the east. On 21 February 1744, the Spanish ships put to sea with a French fleet. Admiral Mathews took his British fleet and attacked the Spanish fleet from 22 February until 23 February 1744 in what has become known as the Battle of Toulon. However, because of mis-communication and possibly treachery on the part of Rear Admiral Lestock, the smaller Spanish fleet was allowed to escape. With the knowledge that a larger French fleet was sailing to the rescue the British ships broke off combat and retreated to the northeast.
Although technically the Battle of Toulon was regarded as a victory for Britain, in Britain it was feared by the public that the combined French and Spanish ships were making for the Straits of Gibraltar and for a gathering of ships at Brest for a planned invasion of England. As a consequence, bitter recriminations were brought against Admiral Mathews for letting this Spanish-French fleet get away and, subsequently, placing England in danger of invasion. Consequently, both Mathews and Lestock were tried in naval court. Lestock was acquitted (unjustly according to some) while Mathews was found guilty (also regarded as an injustice by some commentators).
Meanwhile on the battlefields in northern Europe, Louis XV in person, with 90,000 men, invaded the Austrian Netherlands and took Menin and Ypres in July 1744. His presumed opponent, although shorn of the Russians, still consisted the same allied army, previously commanded by King George II, and composed of British, Dutch, German (Hanoverian) and Austrian troops.
The French put four armies into the field. On the Rhine, Marshal Coigny had 57,000 troops up against 70,000 allied troops under the command of Prince Charles. A fresh army of over 30,000 soldiers under the Prince de Conti was located between the Meuse and Moselle Rivers, which would later assist the Spaniards in Piedmont and Lombardy. This plan was, however, at once dislocated by the advance of Prince Charles, who, assisted by the veteran Marshal Traun, skillfully manoeuvred his allied army over the Rhine near Philippsburg on 1 July 1744 and captured the lines of Weissenburg, and cut off Marshal Coigny and his army from Alsace.
A third French Army composed of 17,000 men under the command of Duke d' Harcourt kept Luxembourg at bay Meanwhile, the fourth French army was the largest army that put to field in the summer of 1744. This was the Army of Flanders. Numbering 87,000 men and was officially under the command of the king of France—Louis XV, but in actuality Louis XV was being militarily advised by Marshal Noailles. As these French forces, invaded the Austrian Netherlands they outnumbered the allied armies by about a four to three ratio. Furthermore, as they marched into the Austrian Netherlands, they met a confused resistance offered by Dutch forces. Consequently, the French Army of Flanders made rapid progress across the Austrian Netherlands. The situation became so desperate for the Dutch, that the Dutch government sent an envoy to the king of France to seek peace. This plea for peace was rejected by the French.
However, the situation in the Austrian Netherlands was changed abruptly by the successful crossing of the Rhine on 30 June 1744 by Prince Charles and his 70,000-man allied army. Marshal Coigny, caught far in advance of the other French forces, cut his way back through the enemy at Weissenburg and withdrew towards Strasbourg. Louis XV now abandoned the invasion of the Southern Netherlands, and his army moved down to take a decisive part in the war in Alsace and Lorraine.
Finally on 12 July 1744, Frederick II of Prussia received confirmation that Prince Charles had taken his army beyond the Rhine and into France. Thus, Frederick knew that Prince Charles would not be able to present any immediate problem to him in the east. Consequently, on 15 August 1744, Frederick II, crossed the Austrian frontier into Bohemia. By late August all 80,000 of his troops were in Bohemia. The attention and resources of Austria had been fully occupied for some time on a renewal of the war in Silesia. However, neither Maria Theresa nor her advisers had expected the Prussians to march as quickly as they did. Accordingly, Frederick's invasion of Bohemia came as a surprise to the Austrian court and Frederick was almost unopposed in Bohemia. One column consisting of 40,000 troops, under Frederick's own command, passed through Saxony; another column of 16,000 men under the command of "Young Dessauer" passed through Lusatia, while a third consisting of 16,000 soldiers under Count Schwerin advanced from Silesia. The destination for all three columns was Prague. The objective, was reached on 2 September. The city was surrounded and besieged. Six days later the Austrian garrison was compelled to surrender. Scarcely had Prague surrendered to Frederick II than he was off marching southwards, leaving 5,000 soldiers under General Baron Gottfried Emanual von Einsiedel to garrison Prague. Three days after the fall of Prague, Frederick marched with his army marching southwards and seized Tabor, Budweis and Frauenberg.
Maria Theresa once again rose to the emergency, a new "insurrection" took the field in Hungary, and a corps of regulars was assembled to cover Vienna. Meanwhile, Austrian diplomats won over Saxony to the Austrian side. Because of Frederick's successful campaign in Bohemia, Prince Charles sought to withdraw from Alsace and cross the Rhine once again and strike at the Prussians. At this point the French had an excellent chance to strike at Prince Charles while he was in a vulnerable position—crossing the Rhine. However, the French military command was distracted and could not take any action at all and Prince Charles was able to cross the Rhine once again unmolested by the French. The French had been unable to act because they were thrown into confusion by King Louis XV suddenly becoming very ill with smallpox at Metz. The King's condition was so severe that many feared for his life. Only Count Seckendorf commander of the Bavarians pursued Prince Charles. No move was made by the French, and Frederick, thus, found himself isolated and exposed to the combined attack of the Austrians and Saxons. Count Traun, summoned from the Rhine, held the king in check in Bohemia with a united force of Austrians and Saxons. The Hungarian irregulars also inflicted numerous minor reverses on the Prussians. Finally Prince Charles arrived with the main army from the west. The campaign resembled that of 1742: the Prussian retreat was closely watched, and the rearguard pressed hard. Prague fell, and Frederick was completely outmanoeuvred by the united forces of Prince Charles and Count Traun. Frederick was forced to retreat to Silesia with heavy losses. However, the Austrians gained no foothold in Silesia itself. On the Rhine, Louis XV, now recovered, had besieged and taken Freiburg, after which the forces left in the north were reinforced and besieged the strong places of Southern Netherlands. There was also a slight war of manoeuvre on the middle Rhine.
Campaign of 1745.
The year 1745 saw three of the greatest battles of the war: Hohenfriedberg, Kesselsdorf and Fontenoy. The first event of the year was the formation of the Quadruple Alliance of Britain, Austria, the Dutch Republic and Saxony, concluded at Warsaw on 8 January 1745 by the Treaty of Warsaw. Twelve days later on 20 January 1745, the death of the Holy Roman Emperor--Charles VII—submitted the imperial title to a new election. Charles VII's son and heir, Maximilian III in Bavaria was not even considered a candidate for the Imperial throne. The Bavarian army was again unfortunate. Caught in its scattered winter quarters (action of Amberg, 7 Jan.), it was driven from point to point by a maneuver by the Austrian army under the joint command of Count Batthyány, Baron Bernklau and Count Browne. All Bavarian garrisons fled to the east. The Bavarian Army under the command of Count Törring was divided and paralyzed. The French in the area under Count Ségur marched to save the day. Count Sègur's force out-numbered the Austrian army under Count Batthyany, yet Sègur and the French army were defeated at the Battle of Pfaffenhofen. The young elector Maximilian III Joseph had to abandon Munich once more. The Peace of Füssen followed on 22 April 1745, by which Maximilian III secured his hereditary states on condition of supporting the candidature of the Grand-Duke Francis, consort of Maria Theresa. The "imperial" army ceased "ipso facto" to exist.
Frederick II of Prussia was again isolated. No help was to be expected from France, whose efforts this year were centred on the Flanders campaign. Indeed, on 31 March 1745, before Frederick ever took the field in the spring of 1745, Louis XV and the Marshal of France Maurice de Saxe, commanding an army of 95,000 men, the largest force in the war, marched down the Scheldt River valley and had besieged Tournay. Tournay was defended by a Dutch garrison of 7,000 soldiers. In May 1745, a British army under the command of the Duke of Cumberland attempted to break the French siege and relieve Tournay. Maurice (who had just recently been appointed a Marshal in the French army) had very good intelligence and knew the road that Cumberland was using to attack his forces besieging forces. Thus, Maurice could select the battlefield. Maurice chose to attack the British allied army on a plain on the east side of the Scheldt River about two miles southeast of Tournay near the town of Fontenoy. There the Battle of Fontenoy was fought on 11 May 1745. Fighting began at 5:00 AM with a French artillery barrage of the British-Allied forces, who were still attempting to move into their proper positions for their anticipated attack on Tournay. By noon, Cumberland's troops had ground to a halt and discipline had begun to dissolve. The British-Allied army sought cover in a retreat. It was a victory for the French that captured the attention of Europe because it overturned the mystique of British military superiority and it pointed out the importance of artillery. On 20 June 1745, after the Battle on Foutenoy, the fortress of Tournay surrendered to the French.
In the summer of 1745, the French once more decided to take up Charles Edward Stuart's claim to the British throne. The goal was to start a revolt in Scotland which would divert British attention away from the war on the mainland in Europe, and may even require Britain to leave the war altogether. On 23 July 1745, Charles landed on the island of Eriskay in the Hebrides, north-west of the mainland of Scotland. On 25 July 1745, Charles set sail again for the mainland. By the end of August 1745, they had landed Charles in Scotland and he had started issuing calls for troops loyal to the Jacobite cause of placing him on the throne. Already, Charles had collected 1,300 Scots prepared to fight in his Jacobite army. Defence of the Hanoverian rule of King George II in England fell to General Sir John Cope, a veteran of the Battle of Dettingen. On 31 August 1745, Cope marched north with about 2,000 British soldiers. Charles reached Perth on 18 September 1745 and Edinburgh surrendered to him on 27 September 1745. When Cope brought his army up to the town of Prestonpans, Scotland on 1 October 1745, he chose a stubble field that he felt was well protected, on which to encamp his troops. However, it was not as safe as he thought and at sunrise the next morning, 2 October 1745, Charles's Scottish troops attacked and defeated the British. With the British defeat at Prestonpans, it appeared that all Scotland belonged to Charles. By November 1745, his army consisted of 5,000 infantry men and 300 cavalry. In mid-November 1745 it crossed the border from Scotland and invaded England.
As the Jacobite army moved south into England, Charles kept assuring his troops that aid and reinforcements from English Jacobites would be arriving at any time. This aide and reinforcements were desperately needed as the Jacobites were badly outnumbered by the three British armies already in the field. Finally on 6 December 1745, at Derby in northern England, Charles was reluctantly persuaded by his senior officers to turn back to Scotland. Upon hearing about the turnabout in Derby, the French gave up on their plans for an invasion of England. The Jacobites felt they could more securely fight the British in a defensive battle on Scottish soil rather than fighting the British in England. On 17 January 1746, at the Battle of Falkirk Muir, 8,000 Scots, the largest amount of troops gathered by the Jacobite cause during the uprising defeated 7,000 of British troops. Ultimately, however, Charles Stuart and his uprising were defeated on 27 April 1746 at the Battle of Culloden.
The manoeuvres of the armies of both sides in the war on the upper Elbe occupied all the summer. Meanwhile, the political questions of the imperial election and of an understanding between Prussia and Britain were pending. The chief efforts of Austria were directed towards the valleys of the Main and of Lahn and Frankfurt, where the French and Austrian armies manoeuvred for a position from which to overawe the electoral body. Austrian Marshal Traun was successful, and, as a result, Francis, the husband/consort of Maria Theresa was elected Holy Roman Emperor on 13 September 1745. Frederick agreed with Britain to recognise the election a few days later, but Maria Theresa would not conform to the Treaty of Breslau of 1741, by which she had been forced to recognise Frederick's annexation of Silesia. Maria Theresa now tried a further appeal to the fortunes of war to get Silesia back. Saxony joined Austria in this last attempt to reconquer Silesia.
In May 1745, The main Prussian army was stationed at Frankenstein. This army consisted of 59,000 soldiers and was fitted with 54 heavy cannon. Frederick learned that a combined Austrian-Saxon army of about 70,000 troops under the command of Prince Charles, was on the march to the northeast towards Landeshut. To meet this threat to Silesia, Frederick II marched north toward Reichenbach. Before he reached Reichenbach, Frederick learned that Prince Charles was crossing the mountains from the west to the east side and that Prince Charles planned to occupy the town of Hohenfriedberg. Accordingly, Frederick encamped his army at Schweidwitz and waited for Prince Charles to come to him. At this site, Frederick laid a trap for the superior Austrian-Saxon forces. Indeed, Frederick was operating on the theory that "to catch a mouse, leave the trap open." At 6:30 AM on 4 June 1745, while the Austrian-Saxon troops were still recovering from their long march, the trap was sprung on them in the Battle of Hohenfriedberg. The Austrian-Saxon forces were no match for Frederick's army and especially his cavalry. At 9:00 AM, Prince Charles orders a full retreat back toward Reichenberg.
A new advance of Prince Charles quickly brought on the Battle of Soor on 30 September 1745. This battle was fought on ground destined to be famous in the war of 1866. Frederick commanded an army that at this time numbered only 20,000 soldiers in the vicinity of Soor. He was facing Prince Charles with an army of 41,000 troops. At first in a position of great peril, but his army changed front in the face of the advancing enemy and by its boldness and tenacity won a remarkable victory on 30 September 1745 at Soor.
But the campaign was not ended. An Austrian contingent from the Main joined the Saxons under Field Marshal Rutowsky (1702–1764), and a combined movement was made in the direction of Berlin by Rutowsky from Saxony and Prince Charles from Bohemia. The danger was very great. Frederick hurried up his forces from Silesia and marched as rapidly as possible on Dresden, in Saxony. Frederick won the actions of Katholisch-Hennersdorf on 24 November 1745 and Görlitz on 25 November. Prince Charles was thereby forced to abandon his plans to attack Silesia and hurry back to defend Saxony. A second Prussian army under the Old Dessauer advanced up the Elbe from Magdeburg to meet Rutowsky. The latter took up a strong position at Kesselsdorf between Meissen and Dresden, but the veteran Leopold attacked him directly and without hesitation on 14 December 1745. The Saxons and their allies were completely routed after a hard struggle in the Battle of Kesselsdorf. Maria Theresa was, at last, forced to give way. In the Peace of Dresden signed on 25 December 1745, she recognised Frederick's annexation of Silesia, as first recognised in the Peace of Breslau in 1741. Frederick on the other hand recognised the election of Maria Theresa's husband/consort—Francis I—as the Holy Roman Emperor.
Italian Campaigns 1741–47.
In central Italy an army of Spaniards and Neapolitans was collected for the purpose of conquering the Milanese. In 1741, the allied army of 40,000 Spaniards and Neapolitans under the command of the Duke of Montmar had advanced towards Modena, the Duke of Modena had allied himself with them, but the vigilant Austrian commander, Count Otto Ferdinand von Traun had out-marched them, captured Modena and forced the Duke to make a separate peace.
The aggressiveness of the Spanish in Italy forced Empress Maria Theresa of Austria and King Charles Emmanuel of Piedmont-Sardinia into negotiations in early 1742. These negotiations were held at Turin. Maria Theresa sent her envoy Count Schulenburg and King Charles Emmanuel sent the Marquis d'Ormea. On 1 February 1742, Schulenburg and Ormea signed the Convention of Turin which resolved (or postponed resolution) of many differences and formed an alliance between the two countries. In 1742, field marshal Count Traun held his own with ease against the Spanish and Neapolitans. On 19 August 1742, Naples was forced by the arrival of a British cavalry squadron in Naple's own harbour, to withdraw her 10,000 troops from the Montemar force to provide for home defence. The Spanish force under Montemar was now too weak to advance in the Po Valley and a second Spanish army was sent to Italy via France. Sardinia had allied herself with Austria in the Convention of Turin and at the same time neither state was at war with France and this led to curious complications, combats being fought in the Isère valley between the troops of Sardinia and of Spain, in which the French took no part. At the end of 1742, the Duke of Montemar was replaced as head of the Spanish forces in Italy by Count Gages.
In 1743, the Spanish on the Panaro had achieved a victory over Traun at Campo Santo on 8 February 1743. However, the next six months were wasted in inaction and Georg Christian, Fürst von Lobkowitz, joining Traun with reinforcements from Germany, drove back the Spanish to Rimini. Observing from Venice, Rousseau hailed the Spanish retreat as "the finest military manoeuvre of the whole century." The Spanish-Piedmontese War in the Alps continued without much result, the only incident of note being the first Battle of Casteldelfino (7–10 October 1743), when an initial French offensive was beaten off.
In 1744 the Italian war became serious. Prior to the War of the Spanish Succession (1701–1714) Spain and Austria had been ruled by the same (Habsburg) royal house. Consequently, the foreign policies of Austria and Spain in regards to Italy had a symmetry of interests and these interests were usually opposed to the interests of Bourbon controlled France. However, since the Treaty of Utrecht and the end of the War of the Spanish Succession, the childless last Habsburg monarch (Charles II) had been replaced by the Bourbon grandson of the French king Louis XIV Philip of Anjou, who became Philip V in Spain. Now the symmetry of foreign policy interests in regards to Italy existed between Bourbon France and Bourbon Spain with Habsburg Austria usually in opposition. King Charles Emmanuel of Savoy had followed the long-established foreign policy of Savoy of opposing Spanish interference in northern Italy. Now in 1744, Savoy was faced with a grandiose military plan of the combined Spanish and French armies (called the Gallispan army) for conquest of northern Italy.
However, in implementing this plan, the Gallispan generals at the front were hampered by the orders of their respective governments. For example, the commander of the Spanish army in the field, the Prince of Conti, could not get along with, or even reason with, the Marquis de La Mina, the Supreme commander of all Spanish forces. The Prince of Conti felt that the Marquis "deferred blindly to all orders coming from Spain" without any consideration of the realities on the ground. In preparation for the military campaign the Gallispan forces sought to cross the Alps in June 1744 and regroup the army in Dauphiné uniting there with the army on the lower Po.
The support of Genoa allowed a road into central Italy. While the Prince of Conti stayed in the north, Count Gages followed this road to the south. But then the Austrian commander, Prince Lobkowitz took the offensive and drove the Spanish army of the Count de Gages further southward towards the Neapolitan frontier near the small town of Velletri. Velletri just happened to be the birthplace of Caesar Augustus, but now from June through August 1744, Velletri became the scene of extensive military maneuvering between the French-Spanish army under the command of the Count Gages and the Austrian forces under the command of Prince Lobkowitz The King of Naples (the future Charles III of Spain) was increasingly worried about the Austrian army operating so close to his borders and decided to assist the Spaniards. Together a combined army of French, Spanish and Neapolitans surprised the Austrian army on the night of 16–17 June 1744. The Austrians were routed from three important hills around the town of Velletri during the attack. This battle is sometimes called the "Battle of Nemi" after the small town of Nemi located nearby. Because of this surprise attack, the combined army was able to take possession of the town of Velletri. Thus, the surprise attack has also been called the "first Battle of Velletri."
In early August 1744, the King of Naples paid a visit in person to the newly captured town of Velletri. Hearing about the presence of the King, the Austrians developed a plan for a daring raid on Velletri. During the predawn hours of 11 August 1744, about 6,000 Austrians under the direct command of Count Browne staged a surprise raid on the town of Velletri. They were attempting to abduct the King of Naples during his stay in the town. However, after occupying Velletri and searching the entire town, the Austrians found no hint of the King of Naples. The King had become aware of what was happening and had fled through a window of the palace where he was staying and rode off half-dressed on horseback out of the town. This was the second Battle of Velletri. The failure of the raid on Velletri meant that the Austrian march toward Naples was over. The defeated Austrians were ordered north where they could be used in the Piedmont of northern Italy to assist the King of Savoy against the Prince of Conti. Count de Gages followed the Austrians north with a weak force. Meanwhile, the King of Naples returned home.
The war in the Alps and the Apennines had already been keenly contested before the Prince of Conti and the Gallispan army had come down out of the Alps. Villefranche and Montalbán had been stormed by Conti on 20 April 1744. After coming down out of the Alps, Prince Conti began his advance into Piedmont on 5 July 1744. On 19 July 1744, the Gallispan army engaged the Piedmontese army in some desperate fighting at Peyre-Longue on 18 July 1744. As a result of the battle, the Gallispan army took control of Casteldelfino in the second Battle of Casteldelfino. Conti then moved on to Delmonte where on the night of 8–9 August 1744, (a mere 36 hours before the Spanish army in south of Italy fought the second Battle of Velletri, [as noted above]) the Gallispan army took the city of Delmonte from the Piedmontese in the Battle of Delmonte. The King of Sardinia (which included Piedmont) was defeated yet again by Conti in a great Battle at Madonna dell'Olmo on 30 September 1744 near Coni (Cuneo). Conti did not, however, succeed in taking the huge fortress at Coni and had to retire into Dauphiné for his winter quarters. Thus, the Gallispan army never did combine with the Spanish army under Prince Lobkowitz in the south and now the Austro-Sardinian army lay between them.
The campaign in Italy in 1745 was also no mere war of posts. The Convention of Turin of February 1742 (described above), which established a provisional relationship between Austria and Piedmont-Sardinia had caused some consternation in the Republic of Genoa. However, when this provisional relationship was given a more durable and reliable character in the signing of the Treaty of Worms (1743) signed on 13 September 1743, the government of Genoa became fearful. This fear of diplomatic isolation had caused the Genoese Republic to abandon its neutrality in the war and join the Bourbon cause. Consequently, the Genoese Republic signed a secret treaty with the Bourbon allies of France, Spain and Naples. On 26 June 1745, Genoa declared war on Piedmont-Sardinia.
Empress Maria Theresa, was frustrated with the failure of Lobkowitz to stop the advance of Gage. Accordingly Lobkowitz was replaced with Count Schulenburg. A change in the command of the Austrians, encouraged the Bourbon allies to strike first in the spring of 1745. Accordingly, Count de Gages moved from Modena towards Lucca, the Gallispan army in the Alps under the new command of Marshal Maillebois (Prince Conti and Marshal Maillebois had exchanged commands over the winter of 1744–1745) advanced through the Italian Riviera to the Tanaro. In the middle of July 1745, the two armies were at last concentrated between the Scrivia and the Tanaro. Together Count de Gage's army and the Gallispan army composed an unusually large number of 80,000 men. A swift march on Piacenza drew the Austrian commander thither and in his absence the allies fell upon and completely defeated the Sardinians at Bassignano on 27 September 1745, a victory which was quickly followed by the capture of Alessandria, Valenza and Casale Monferrato. Jomini calls the concentration of forces which effected the victory ""Le plus remarquable de toute la Guerre"."
The complicated politics of Italy, however, is reflected in the fact that Count Maillebois was ultimately unable to turn his victory to account. Indeed, early in 1746, Austrian troops, freed by the Austrian peace with Frederick II of Prussia, passed through the Tyrol into Italy. The Gallispan winter quarters at Asti, Italy, were brusquely attacked and a French garrison of 6,000 men at Asti was forced to capitulate. At the same time, Maximilian Ulysses Count Browne with an Austrian corps struck at the allies on the Lower Po, and cut off their communication with the main body of the Gallispan army in Piedmont. A series of minor actions thus completely destroyed the great concentration of Gallispan troops and the Austrians reconquered the duchy of Milan and took possession of much of northern Italy. The allies separated, Maillebois covering Liguria, the Spaniards marching against Browne. The latter was promptly and heavily reinforced and all that the Spaniards could do was to entrench themselves at Piacenza, Philip, the Spanish Infante as supreme commander calling up Maillebois to his aid. The French, skilfully conducted and marching rapidly, joined forces once more, but their situation was critical, for only two marches behind them the army of the King of Sardinia was in pursuit, and before them lay the principal army of the Austrians. The pitched Battle of Piacenza on 16 June 1746 was hard fought but ended in an Austrian victory, with the Spanish army heavily mauled. That the army escaped at all was in the highest degree creditable to Maillebois and to his son and chief of staff. Under their leadership the Gallispan army eluded both the Austrians and the Sardinians and defeated an Austrian corps in the Battle of Rottofreddo on 12 August 1746. Then the Austrian army made good its retreat back to Genoa.
Although the Austrian army was a mere shadow of its former self, when they returned to Genoa, the Austrians were soon masters of north Italy. The Austrians occupied the Republic of Genoa on 6 September 1746. But they met with no success in their forays towards the Alps. Soon Genoa revolted from the oppressive rule of the victors, rose and drove out the Austrians on 5–11 December 1746. As an Allied invasion of Provence stalled, and the French, now commanded by Charles Louis Auguste Fouquet, Duc de Belle-Isle, took the offensive (1747). Genoa held out against a second Austrian siege. As usual the plan of campaign had been referred to Paris and Madrid. A picked corps of the French army under the Chevalier de Belle-Isle (1684–1747), (the Chevalier de Belle-Isle was the younger brother of Marshal Belle-Isle), was ordered to storm the entrenched pass of Exilles on 10 July 1747. However, the defending army of the Worms allies (Austria and the Piedmont-Sardinia) handed the French army a stunning defeat at this battle, which became known as the (Colle dell'Assietta). At this battle, the chevalier, and with him much of the elite of the French nobility, were killed on the barricades. Desultory campaigns continued between the Worms allies and the French until the conclusion of peace at Aix-la-Chapelle.
Later campaigns.
The last three campaigns of the war in the Netherlands were illustrated by the now fully developed genius of Marshal Saxe. After Fontenoy, the French carried all before them. The withdrawal of most of the British to aid in suppressing the Jacobite rising of 1745 at home left their allies in a helpless position. In 1746, the Dutch and the Austrians were driven back towards the line of the Meuse, and most of the important fortresses were taken by the French and Brussels was captured in February 1746. In September the British launched a Raid on Lorient in an attempt to provide a diversion for the Allied forces in the Netherlands. The Battle of Roucoux (or Raucourt) near Liège, fought on 11 October between the allies under Prince Charles of Lorraine and the French under Saxe, resulted in a victory for the latter. The Dutch Republic itself was now in danger, and when in April 1747 Saxe's army, which had now conquered the Austrian Netherlands up to the Meuse, turned its attention to the United Provinces, the old fortresses on the frontier offered but slight resistance. Since August 1746, talks had been ongoing at the Congress of Breda to try and agree a peace settlement, but up to this point they had met with little success.
The Prince of Orange William IV and the Duke of Cumberland suffered a severe defeat at Lauffeld (Lawfeld, also called Val) on 2 July 1747, and Saxe, after his victory, promptly and secretly despatched a corps under Marshal Lowendahl (1700–1755) to besiege Bergen op Zoom. On 18 September, Bergen op Zoom was stormed by the French, and in the last year of the war Maastricht, attacked by the entire forces of Saxe and Lowendahl, surrendered on 7 May 1748. A large Russian army arrived to join the allies, but too late to be of use. The quarrel between Russia and Sweden had been settled by the Peace of Åbo in 1743, and in 1746 Russia had allied itself with Austria. Eventually, a large army marched from Moscow to the Rhine, an event which was not without military significance, and in a manner preluded the great invasions of 1813–1814 and 1815. The general Peace of Aix-la-Chapelle (Aachen) was signed on 18 October 1748.
Conclusion of the war.
The War of Austrian Succession concluded with the Treaty of Aix-la-Chapelle (1748). Maria Theresa and Austria survived" status quo ante bellum", sacrificing only the territory of Silesia, which Austria conceded to Prussia. The end of the war also sparked the beginning of the German dualism between Prussia and Austria, which would ultimately fuel German nationalism and the drive to unify Germany as a single entity.
Despite his victories, Louis XV of France, who wanted to appear as an arbiter and not as a conqueror, gave all his conquests back to the defeated enemies with chivalry, arguing that he was "king of France, not a merchant". This decision, largely misunderstood by his generals and by the French people, made the king unpopular. The French obtained so little of what they fought for that they adopted the expressions "Bête comme la paix" ("Stupid as the peace") and "Travailler pour le roi de Prusse" ("To work for the king of Prussia", i.e. working for nothing). The latter expression is still commonly used in French.
General character of the war in Europe.
The triumph of Prussia was in a great measure due to its fuller application of principles of tactics and discipline universally recognised though less universally enforced. The other powers reorganised their forces after the war, not so much on the Prussian model as on the basis of a stricter application of known general principles. Prussia, moreover, was far ahead of all the other continental powers in administration, and over Austria, in particular, its advantage in this matter was almost decisive. Added to this was the personal ascendancy of Frederick, as opposed to generals who were responsible for their men to their individual sovereigns.
The war, like other conflicts of the time, featured an extraordinary disparity between the end and the means. The political schemes to be executed by the French and other armies were as grandiose as any of modern times. Their execution, under the conditions of time and space, invariably fell short of expectations, and the history of the war proves, as that of the Seven Years' War was to prove, that the small standing army of the 18th century could conquer by degrees, but could not deliver a decisive blow. Frederick alone, with a definite end and proportionate means to achieve it, succeeded completely. Even less was to be expected when the armies were composed of allied contingents, sent to the war each for a different object. The allied national armies of 1813 (at the Battle of Leipzig) co-operated loyally, for they had much at stake and worked for a common object. Those of 1741 represented the divergent private interests of the several dynasties, and achieved nothing.
North America.
The war was also conducted in North America and India. In North America the conflict was known in the British colonies as King George's War, and did not begin until after formal war declarations of France and Britain reached the colonies in May 1744. The frontiers between New France and the British colonies of New England, New York, and Nova Scotia were the site of frequent small scale raids, primarily by French colonial troops and their Indian allies against British targets, although several attempts were made by British colonists to organise expeditions against New France. The most significant incident was the capture of the French Fortress Louisbourg on Cape Breton Island (Île Royale) by an expedition (29 April – 16 June 1745) of colonial militia organised by Massachusetts Governor William Shirley, commanded by William Pepperrell of Maine (then part of Massachusetts), and assisted by a Royal Navy fleet. A French expedition to recover Louisbourg in 1746 failed due to bad weather, disease, and the death of its commander. Louisbourg was returned to France in exchange for Madras, generating much anger among the British colonists, who felt they had eliminated a nest of privateers with its capture.
India.
The war marked the beginning of great power in England and the powerful struggle between Britain and France in India and of European military ascendancy and political intervention in the subcontinent. Major hostilities began with the arrival of a naval squadron under Mahé de la Bourdonnais, carrying troops from France. In September 1746 Bourdonnais landed his troops near Madras and laid siege to the port. Although it was the main British settlement in the Carnatic, Madras was weakly fortified and had only a small garrison, reflecting the thoroughly commercial nature of the European presence in India hitherto. On 10 September, only six days after the arrival of the French force, Madras surrendered. The terms of the surrender agreed by Bourdonnais provided for the settlement to be ransomed back for a cash payment by the British East India Company. However, this concession was opposed by Dupleix, the governor general of the Indian possessions of the Compagnie des Indes. When Bourdonnais was forced to leave India in October after the devastation of his squadron by a cyclone Dupleix reneged on the agreement. The Nawab of the Carnatic Anwaruddin Muhammed Khan intervened in support of the British and advanced to retake Madras, but despite vast superiority in numbers his army was easily and bloodily crushed by the French, in the first demonstration of the gap in quality that had opened up between European and Indian armies.
The French now turned to the remaining British settlement in the Carnatic, Fort St David at Cuddalore, which was dangerously close to the main French settlement of Pondichéry. The first French force sent against Cuddalore was surprised and defeated nearby by the forces of the Nawab and the British garrison in December 1746. Early in 1747 a second expedition laid siege to Fort St David but withdrew on the arrival of a British naval squadron in March. A final attempt in June 1748 avoided the fort and attacked the weakly fortified town of Cuddalore itself, but was routed by the British garrison.
With the arrival of a naval squadron under Admiral Boscawen, carrying troops and artillery, the British went on the offensive, laying siege to Pondichéry. They enjoyed a considerable superiority in numbers over the defenders, but the settlement had been heavily fortified by Dupleix and after two months the siege was abandoned.
The peace settlement brought the return of Madras to the British company, exchanged for Louisbourg in Canada. However, the conflict between the two companies continued by proxy during the interval before the outbreak of the Seven Years' War, with British and French forces fighting on behalf of rival claimants to the thrones of Hyderabad and the Carnatic.
Naval operations.
The naval operations of this war were entangled with the War of Jenkins' Ear, which broke out in 1739 in consequence of the long disputes between Britain and Spain over their conflicting claims in America. The war was remarkable for the prominence of privateering on both sides. It was carried on by the Spaniards in the West Indies with great success, and actively at home. The French were no less active in all seas. Mahé de la Bourdonnais's attack on Madras partook largely of the nature of a privateering venture. The British retaliated with vigour. The total number of captures by French and Spanish corsairs was in all probability larger than the list of British – as the French wit Voltaire drolly put it upon hearing his government's boast, namely, that more British merchants were taken because there were many more British merchant ships to take; but partly also because the British government had not yet begun to enforce the use of convoy so strictly as it did in later times.
The West Indies.
War on Spain was declared by Great Britain on 23 October 1739, which has become known as the War of Jenkins' Ear. A plan was laid for combined operations against the Spanish colonies from east and west. One force, military and naval, was to assault them from the West Indies under Admiral Edward Vernon. Another, to be commanded by Commodore George Anson, afterwards Lord Anson, was to round Cape Horn and to fall upon the Pacific coast of Latin America. Delays, bad preparations, dockyard corruption, and the squabbles of the naval and military officers concerned caused the failure of a hopeful scheme. On 21 November 1739, Admiral Vernon did, however, succeed in capturing the ill-defended Spanish harbour of Porto Bello in present-day Panama. When Vernon had been joined by Sir Chaloner Ogle with massive naval reinforcements and a strong body of troops, an attack was made on Cartagena in what is now Colombia (9 March – 24 April 1741). The delay had given the Spanish under Sebastián de Eslava and Blas de Lezo time to prepare. After two months of skilful defence by the Spanish, the British attack finally succumbed to a massive outbreak of disease and withdrew having suffered a dreadful loss of lives and ships.
The war in the West Indies, after two other unsuccessful attacks had been made on Spanish territory, died down and did not revive till 1748. The expedition under Anson sailed late, was very ill-provided, and less strong than had been intended. It consisted of six ships and left Britain on 18 September 1740. Anson returned alone with his flagship the "Centurion" on 15 June 1744. The other vessels had either failed to round the Horn or had been lost. But Anson had harried the coast of Chile and Peru and had captured a Spanish galleon of immense value near the Philippines. His cruise was a great feat of resolution and endurance.
After the failure of the British invasions and a Spanish counter invasion of Georgia in 1742, belligerent naval actions in the Caribbean were left to the privateers of both sides. Fearing great financial and economic losses should a treasure fleet be captured, the Spanish reduced the risk by increasing the number of convoys, thereby reducing their value. They also increased the number of ports they visited and reduced the predictability of their voyages.
The last year of the war saw two significant actions in the Caribbean. A second British assault on Santiago de Cuba which also ended in failure and a naval action which arose from an accidental encounter between two convoys. The action unfolded in a confused way with each side at once anxious to cover its own trade and to intercept that of the other. Capture was rendered particularly desirable for the British by the fact that the Spanish homeward-bound fleet would be laden with bullion from the American mines. The advantage lay with the British when one Spanish warship ran aground and another was captured but the British commander failed to capitalise and the Spanish fleet took shelter in Havana.
The Mediterranean.
While Anson was pursuing his voyage round the world, Spain was mainly intent on the Italian policy of the King. A squadron was fitted out at Cádiz to convey troops to Italy. It was watched by the British admiral Nicholas Haddock. When the blockading squadron was forced off by want of provisions, the Spanish admiral Don Juan José Navarro put to sea. He was followed, but when the British force came in sight of him Navarro had been joined by a French squadron under Claude-Elisée de La Bruyère de Court (December 1741). The French admiral announced that he would support the Spaniards if they were attacked and Haddock retired. France and Great Britain were not yet openly at war, but both were engaged in the struggle in Germany—Great Britain as the ally of the Queen of Hungary, Maria Theresa; France as the supporter of the Bavarian claimant of the empire. Navarro and de Court went on to Toulon, where they remained till February 1744. A British fleet watched them, under the command of Admiral Richard Lestock, till Sir Thomas Mathews was sent out as commander-in-chief and as Minister to the Court of Turin.
Sporadic manifestations of hostility between the French and British took place in different seas, but avowed war did not begin till the French government issued its declaration of 30 March, to which Great Britain replied on 31 March. This formality had been preceded by French preparations for the invasion of England, and by the Battle of Toulon between the British and a Franco-Spanish fleet. On 11 February, a most confused battle was fought, in which the van and centre of the British fleet was engaged with the Spanish rear and centre of the allies. Lestock, who was on the worst possible terms with his superior, took no part in the action. Mathews fought with spirit but in a disorderly way, breaking the formation of his fleet, and showing no power of direction, while Navarro's smaller fleet retained cohesion and fought off the energetic but confused attacks of its larger enemy until the arrival of the French fleet forced the heavily damaged British fleet to withdraw. The Spanish fleet then sailed to Italy where it delivered a fresh army and supplies that had a decisive impact upon the war. The mismanagement of the British fleet in the battle, by arousing deep anger among the people, led to a drastic reform of the British navy.
Northern waters.
The French scheme to invade Britain was arranged in combination with the Jacobite leaders, and soldiers were to be transported from Dunkirk. In February 1744, a French fleet of twenty sail of the line entered the English Channel under Jacques Aymar, comte de Roquefeuil, before the British force under Admiral John Norris was ready to oppose him. But the French force was ill-equipped, the admiral was nervous, his mind dwelt on all the misfortunes which might possibly happen, and the weather was bad. De Roquefeuil came up almost as far as The Downs, where he learnt that Sir John Norris was at hand with twenty-five sail of the line, and thereupon precipitately retreated. The military expedition prepared at Dunkirk to cross under cover of De Roquefeuil's fleet naturally did not start. The utter weakness of the French at sea, due to long neglect of the fleet and the bankrupt state of the treasury, was shown during the Jacobite rising of 1745, when France made no attempt to profit by the distress of the British government.
The Dutch, having by this time joined Great Britain, made a serious addition to the naval power opposed to France, though the Dutch Republic was compelled by the necessity for maintaining an army in Flanders to play a very subordinate part at sea. Not being stimulated by formidable attack, and having immediate interests both at home and in Germany, the British government was slow to make use of its latest naval strength. Spain, which could do nothing of an offensive character, was almost neglected. During 1745 the New England expedition which took Louisburg (30 April – 16 June) was covered by a British naval force, but little else was accomplished by the naval efforts of any of the belligerents.
In 1746 a British combined naval and military expedition to the coast of France – the first of a long series of similar ventures which in the end were derided as "breaking windows with guineas" – was carried out during August and October. The aim was the capture of the French East India Company's dockyard at Lorient, but it was not attained.
From 1747 until the close of the war in October 1748, the naval policy of the British government, without reaching a high level, was more energetic and coherent. A closer watch was kept on the French coast, and effectual means were taken to intercept communication between France and her American possessions. In the spring information was obtained that an important convoy for the East and West Indies was to sail from L'Orient. The convoy was intercepted by Anson on 3 May, and in the first Battle of Cape Finisterre, British admiral George Anson's fourteen ships of the line wiped out the French escort of six ships of the line and three armed Indiamen, although in the meantime the merchant ships escaped.
On 14 October, another French convoy, protected by a strong squadron, was intercepted by a well-appointed and well-directed squadron of superior numbers – the squadrons were respectively eight French and fourteen British – in the Bay of Biscay. In the second Battle of Cape Finisterre which followed, the French admiral, Henri-François des Herbiers-l'Étenduère (1681–1750), succeeded in covering the escape of most of the merchant ships, but Hawke's British squadron took six of his warships. Most of the merchantmen were later intercepted and captured in the West Indies. This disaster convinced the French government of its helplessness at sea, and it made no further effort.
The Indian Ocean.
In the East Indies, attacks on French commerce by a British squadron under Curtis Barnett in 1745 led to the despatch of a French squadron commanded by Mahé de la Bourdonnais. After an inconclusive clash off Negapatnam in July 1746, Edward Peyton, Barnett's successor, withdrew to Bengal, leaving Bourdonnais unopposed on the Coromandel Coast. He landed troops near Madras and besieged the port by land and sea, forcing it to surrender on 10 September 1746. In October the French squadron was devastated by a cyclone, losing four ships of the line and suffering heavy damage to four more, and the surviving ships withdrew. French land forces went on to make several attacks on the British settlement at Cuddalore, but the eventual replacement of the negligent Peyton by Thomas Griffin resulted in a return to British naval supremacy which put the French on the defensive. Despite the appearance of another French squadron, the arrival of large-scale British reinforcements under Edward Boscawen (who considered but did not make an attack on Île de France on the way) gave the British overwhelming dominance on land and sea, but the ensuing siege of Pondichéry organised by Boscawen was unsuccessful.

</doc>
<doc id="52268" url="http://en.wikipedia.org/wiki?curid=52268" title="Sabra and Shatila massacre">
Sabra and Shatila massacre

 
The Sabra and Shatila massacre was the killing of between 762 and 3,500 civilians, mostly Palestinians and Lebanese Shiites, by a militia close to the Kataeb Party, also called Phalange, a predominantly Christian Lebanese right-wing party in the Sabra neighborhood and the adjacent Shatila refugee camp in Beirut, Lebanon. From approximately 6:00 pm 16 September to 8:00 am 18 September 1982, a widespread massacre was carried out by the militia. The Phalanges, allies to the Israeli Defence Forces, were ordered by the IDF to clear out Sabra and Shatila from PLO fighters, as part of the IDF maneuvering into West Beirut. The IDF received reports of some of the Phalanges atrocities in Sabra and Shatila but failed to stop them.
The massacre was presented as retaliation for the assassination of newly elected Lebanese president Bachir Gemayel, the leader of the Lebanese Kataeb Party. It was wrongly assumed that Palestinian militants had carried out the assassination. In June 1982, the Israel Defence Forces had invaded Lebanon with the intention of rooting out the Palestine Liberation Organization (PLO). By mid-1982, under the supervision of the Multinational Force, the PLO withdrew from Lebanon following weeks of battles in West Beirut and shortly before the massacre took place. Various forces — Israeli, Phalangists and possibly also the South Lebanon Army (SLA) — were in the vicinity of Sabra and Shatila at the time of the slaughter, taking advantage of the fact that the Multinational Force had removed barracks and mines that had encircled Beirut's predominantly Muslim neighborhoods and kept the Israelis at bay during the Beirut siege. The Israeli advance over West Beirut in the wake of the PLO withdrawal, which enabled the Phalangist raid, was considered a violation of the ceasefire agreement between the various forces. The Israeli Army surrounded Sabra and Shatila and stationed troops at the exits of the area to prevent camp residents from leaving and, at the Phalangists' request, fired illuminating flares at night.
The direct perpetrators of the killings were the "Young Men", a gang recruited by Elie Hobeika, a prominent figure in the Phalanges, the Lebanese Forces intelligence chief and liaison officer with Mossad, from men who had been expelled from the Lebanese Forces for insubordination or criminal activities. The killings are widely believed to have taken place under Hobeika's direct orders. Hobeika's family and fiancée had been murdered by Palestinian militiamen, and their Lebanese allies, at the Damour massacre of 1976, itself a response to the 1976 Karantina massacre of Palestinians and Lebanese Muslims at the hands of Christian militants. Hobeika later became a long-serving Member of the Parliament of Lebanon and served in several ministerial roles. Other Phalangist commanders involved were Joseph Edde from South Lebanon, Dib Anasta, head of the Phalangist Military Police, Michael Zouein, and Maroun Mischalani from East Beirut. In all 300-400 militiamen were involved, including some from Sa'ad Haddad's South Lebanon Army.
In 1983, a commission chaired by Seán MacBride, the assistant to the UN Secretary General and President of United Nations General Assembly at the time, concluded that Israel, as the camp's occupying power, bore responsibility for the violence. The commission also concluded that the massacre was a form of genocide.
In 1983, the Israeli Kahan Commission, appointed to investigate the incident, found that Israeli military personnel, aware that a massacre was in progress, had failed to take serious steps to stop it. The commission deemed Israel indirectly responsible, and Ariel Sharon, then Defense Minister, bore personal responsibility "for ignoring the danger of bloodshed and revenge", forcing him to resign.
Background.
From 1975 to 1990, groups in competing alliances with neighboring countries fought against each other in the Lebanese Civil War. Infighting and massacres between these groups claimed several thousand victims. Examples: the Syrian-backed Karantina massacre (January 1976) by the Kataeb and its allies against Kurds, Syrians and Palestinians in this predominantly Muslim slum district of Beirut, Damour (January 1976) by the PLO against Christian Maronites, including the family and fiancée of the Lebanese Forces intelligence chief Elie Hobeika; and Tel al-Zaatar (August 1976) by Phalangists and their allies against Palestinian refugees living in a camp administered by UNRWA. The total death toll in Lebanon for the whole civil war period was around 150,000 victims.
The PLO had been attacking Israel from southern Lebanon and Israel had been bombing PLO positions in southern Lebanon since the early 1970s.
The casus belli cited by the Israeli side to declare war, however, was an assassination attempt, on 3 June 1982, made upon Israeli Ambassador to Britain Shlomo Argov. The attempt was the work of the Iraq-based Abu Nidal, possibly with Syrian or Iraqi involvement. Historians and observers such as David Hirst and Benny Morris have noticed the PLO could not be have been involved in the assault or even approved of it: Abu Nidal's group was, after all, a bitter rival to Arafat's PLO and even murdered some of its members, and the PLO condemned the attempted assassination of the Israeli ambassador. Nonetheless Israel used the assassination attempt as a justification to break the ceasefire with the PLO, and as a casus belli for a full-scale invasion of Lebanon. After the war, Israel tried to present it as a response to the "terrorism" being carried out by the PLO from several fronts, including from the border with Lebanon. However, it has been noticed that the PLO was respecting the ceasefire agreement then in force with Israel and keeping the border between the Jewish state and Lebanon more stable than it had been for a period of over a decade. During that ceasefire, which lasted 8 months, UNIFIL — the UN peacekeeping forces in Lebanon — had reported that not a single act of provocation against Israel was launched by the PLO. The Israeli tried out several excuses to justify ditching the ceasefire and attacking the PLO, at some point even eliciting accusations from the Israeli opposition that "demagogy" from the government threatened to pull Israel into war. All such justifications, before the attempted assassination of the ambassador, were also shot down by its ally, the United States, as insufficient reason to launch a war against the PLO.
On 6 June 1982, Israel invaded Lebanon moving northwards to surround the capital, Beirut. Following an extended siege of the city, the fighting was brought to an end with a U.S.-brokered agreement between the parties on 21 August 1982, which allowed for safe evacuation of the Palestinian fighters from the city under the supervision of Western nations and guaranteed the protection of refugees and the civilian residents of the refugee camps.
On 15 June 1982, 10 days after the start of the invasion, the Israeli Cabinet passed a proposal put forward by the Prime Minister, Menachem Begin, that the IDF should not enter West Beirut but this should be done by Lebanese Forces. Chief of Staff, Rafael Eitan, had already issued orders that the Lebanese predominantly Christian, right-wing militias should not take part in the fighting and the proposal was to counter public complaints that the IDF were suffering casualties whilst their allies were standing by.
The subsequent Israeli inquiry estimated the strength of militias in West Beirut, excluding Palestinians, to be around 7,000. They estimated the Phalange to be 5,000 when fully mobilized of whom 2,000 were full-time.
On 23 August 1982, Bachir Gemayel, leader of the right-wing Lebanese Forces, was elected President of Lebanon by the National Assembly. Israel had relied on Gemayel and his forces as a counterbalance to the PLO, and as a result, ties between Israel and Maronite groups, from which hailed many of the supporters of the Lebanese Forces, had grown stronger.
By 1 September, the PLO fighters had been evacuated from Beirut under the supervision of Multinational Force. The evacuation was conditional on the continuation of the presence of the MNF to provide security for the community of Palestinian refugees in Lebanon. Two days later the Israeli Premier Menachem Begin met Gemayel in Nahariya and strongly urged him to sign a peace treaty with Israel. According to some sources, Begin also wanted the continuing presence of the SLA in southern Lebanon (Haddad supported peaceful relations with Israel) in order to control attacks and violence, and action from Gemayel to move on the PLO fighters which Israel believed remained a hidden threat in Lebanon. However, the Phalangists, who were previously united as reliable Israeli allies, were now split because of developing alliances with Syria, which remained militarily hostile to Israel. As such, Gemayel rejected signing a peace treaty with Israel and did not authorize operations to root out the remaining PLO militants.
On 11 September 1982, the international forces that were guaranteeing the safety of Palestinian refugees left Beirut. Then on 14 September, Gemayel was assassinated in a massive explosion which demolished his headquarters. Eventually, the culprit, Habib Tanious Shartouni, a Lebanese Christian, confessed to the crime. He turned out to be a member of the Syrian Social Nationalist Party and an agent of Syrian intelligence. Palestinian and Lebanese Muslim leaders denied any connection to him.
On the evening of 14 September, following the news that Bashir Gemayel had been assassinated, Prime Minister Begin, Minister for Defence Sharon and Chief of Staff Eitan agreed that the Israeli army should invade West Beirut. The public reason given was to be that they were there to prevent chaos. In a separate conversation, at 8.30 pm that evening, Sharon and Eitan agreed that the IDF should not enter the Palestinian refugee camps but that the Phalange should be used. The only other member of the cabinet who was consulted was Foreign Minister Yitzhak Shamir. Shortly after 6.00 am 15 September, the Israeli army entered West Beirut. This Israeli action breached its agreement with the United States not to occupy West Beirut.
On 15 September 1982, 63 Palestinian intellectuals, notably lawyers, medical staff and teachers, were individually identified and killed by an Israeli unit called Sayeret Matkal.
The attack.
On the night of the 14/15 September 1982 the IDF chief of staff Eitan flew to Beirut where he went straight to the Phalangists' headquarters and instructed their leadership to order a general mobilisation of their forces and prepare to take part in the forth-coming Israeli attack on West Beirut. He also ordered them to impose a general curfew on all areas under their control and appoint a liaison officer to be stationed at the IDF forward command post. He told them that the IDF would not enter the refugee camps but that this would be done by the Phalangist forces. The militia leaders responded that the mobilisation would take them 24 hours to organise.
On morning of Wednesday 15 September Israeli Defence Minister, Sharon, who had also travelled to Beirut, held a meeting with Eitan at the IDF's forward command post, a five storey building 200 metres southwest of Shatila camp. Also in attendance were Sharon's aide Avi Duda'i, the Director of Military Intelligence -Yehoshua Saguy, a senior Mossad officer, General Amir Drori, General Amos Yaron, an Intelligence officer, the Head of GSS - Avraham Shalom, the Deputy Chief of Staff - General Moshe Levi and other senior officers. It was agreed that the Phalange should go into the camps.
Following the assassination of Lebanese Christian President Bachir Gemayel, the Phalangists sought revenge. By noon on 15 September, Sabra and Shatila had been surrounded by the IDF, which set up checkpoints at the exits and entrances, and used a number of multi-story buildings as observation posts. Amongst them was the seven-story Kuwaiti embassy which, according to TIME magazine, had "an unobstructed and panoramic view" of Sabra and Shatila. Hours later, IDF tanks began shelling Sabra and Shatila.
The following morning, 16 September, the sixth IDF order relating to the attack on West Beirut was issued. It specified: "The refugee camps are not to be entered. Searching and mopping up the camps will be done by the Phalangists/Lebanese Army".
According to Linda Malone of the Jerusalem Fund, Ariel Sharon and Chief of Staff Rafael Eitan met with Phalangist militia units and invited them to enter Sabra and Shatila, claiming that the PLO was responsible for Gemayel's assassination. The meeting concluded at 3:00 pm 16 September. Chatila had previously been one of the PLO's three main training camps for foreign fighters and the main training camp for European fighters. The Israelis maintained that 2,000 to 3,000 "terrorists" remained in the camps, but were unwilling to risk the lives of more of their soldiers after the Lebanese army repeatedly refused to "clear them out."
An hour later, 1,500 militiamen assembled at Beirut International Airport, then occupied by Israel. Under the command of Elie Hobeika, they began moving towards the area in IDF-supplied jeeps, some bearing weapons provided by Israel, following Israeli guidance on how to enter it. The forces were mostly Phalangist, though there were some men from Saad Haddad's "Free Lebanon forces". According to Ariel Sharon and Elie Hobeika's bodyguard, the Phalangists were given "harsh and clear" warnings about harming civilians. However, it was by then known that the Phalangists presented a special security risk for Palestinians. It was published in the September 1st edition of "Bamahane", the IDF newspaper, that a Phalangist told an Israeli official: "[T]he question we are putting to ourselves is — how to begin, by raping or killing?" A US envoy to the Middle East expressed horror after being told of Sharon's plans to send the Phalangists inside the camps, and Israeli officials themselves acknowledged the situation could trigger "relentless slaughter".
The first unit of 150 Phalangists entered Sabra and Shatila at 6:00 pm. A battle ensued that at times Palestinians claim involved lining up Palestinians for execution. During the night, the Israeli forces fired illuminating flares over the area. According to a Dutch nurse, the camp was as bright as "a sports stadium during a football game".
Two hours after the first Phalangist force entered Shatilla camp a mixed group of Phalangists and Israeli officers were observing the attack from the roof of the forward command post when one of the militia men in the camp radioed his commander Hobeika asking what to do with 50 women and children who had been taken prisoner. Hobeika's reply was overheard by an Israeli officer, who testified that he said: "This is the last time you're going to ask me a question like that; you know exactly what to do." Other Phalangists on the roof started laughing. Amongst the Israelis there was Brigadier General Yaron, Divisional Commander, who asked Lieutenant Elul, his Chef de Bureau, what the laughter was about and Elul translated what Hobeika had said. Yaron then had a five minute conversation, in English, with Hobeika. What was said is unknown.
At 11:00 pm the same evening a report was sent to the IDF headquarters in East Beirut, reporting the killings of 300 people, including civilians. The report was forwarded to headquarters in Tel Aviv and Jerusalem, where it was seen by more than 20 senior Israeli officers.
Later in the afternoon, a meeting was held between the Israeli Chief of Staff and the Phalangist staff. On the morning of Friday, 17 September, the Israeli Army surrounding Sabra and Shatila ordered the Phalange to halt their operation, concerned about reports of a massacre.
On 17 September, while Sabra and Shatila still were sealed off, a few independent observers managed to enter. Among them were a Norwegian journalist and diplomat Gunnar Flakstad, who observed Phalangists during their cleanup operations, removing dead bodies from destroyed houses in the Shatila camp.
Many of the bodies found had been severely mutilated. Many boys had been castrated, some were scalped, and some had the Christian cross carved into their bodies.
Janet Lee Stevens, an American journalist, later wrote to her husband, Dr. Franklin Lamb, "I saw dead women in their houses with their skirts up to their waists and their legs spread apart; dozens of young men shot after being lined up against an alley wall; children with their throats slit, a pregnant woman with her stomach chopped open, her eyes still wide open, her blackened face silently screaming in horror; countless babies and toddlers who had been stabbed or ripped apart and who had been thrown into garbage piles."
Before the massacre, it was reported that the leader of the PLO, Yasir Arafat, had requested the return of international forces, from Italy, France and the United States, to Beirut to protect civilians. Those forces had just supervised the departure of Arafat and his PLO fighters from Beirut. Italy expressed 'deep concerns' about 'the new Israeli advance', but no action was taken to return the forces to Beirut. The New York Times reported on September 1982: Yasir Arafat, leader of the Palestine Liberation Organization, demanded today that the United States, France and Italy send their troops back to Beirut to protect its inhabitants against Israel..."The dignity of three armies and the honor of their countries is involved," Mr. Arafat said at his news conference. "I ask Italy, France and the United States: What of your promise to protect the inhabitants of Beirut?"
Number of victims.
The Lebanese army's chief prosecutor investigated the killings and counted 460 dead (including 15 women and 12 children), Israeli intelligence estimated 700-800 dead, and the Palestinian Red Crescent claimed 2,000 dead. 1,200 death certificates were issued to anyone who produced three witnessing claiming a family member disappeared during the time of the massacre. 
U.N. condemnation.
On 16 December 1982, the United Nations General Assembly condemned the massacre and declared it to be an act of genocide.
The voting record on section D of Resolution 37/123 was: yes: 123; no: 0; abstentions: 22; non-voting: 12.
The delegate for Canada stated: "The term genocide cannot, in our view, be applied to this particular inhuman act". The delegate of Singapore – voting 'yes' – added: "My delegation regrets the use of the term 'an act of genocide' ... [as] the term 'genocide' is used to mean acts committed with intent to destroy, in whole or in part, a national, ethnic, racial or religious group." Canada and Singapore questioned whether the General Assembly was competent to determine whether such an event would constitute genocide. The Soviet Union, by contrast, asserted that: "The word for what Israel is doing on Lebanese soil is genocide. Its purpose is to destroy the Palestinians as a nation." The Nicaragua delegate asserted: "It is difficult to believe that a people that suffered so much from the Nazi policy of extermination in the middle of the twentieth century would use the same fascist, genocidal arguments and methods against other peoples."
The United States commented that "While the criminality of the massacre was beyond question, it was a serious and reckless misuse of language to label this tragedy genocide as defined in the 1948 Convention ...".
William Schabas, director of the Irish Centre for Human Rights at the National University of Ireland, to state: "the term genocide ... had obviously been chosen to embarrass Israel rather than out of any concern with legal precision".
MacBride commission.
The independent commission headed by Seán MacBride, however, did find that the concept of genocide applied to the case as it was the intention of those behind the massacre "the deliberate destruction of the national and cultural rights and identity of the Palestinian people". Individual Jews throughout the world also denounced the massacre as genocide.
The MacBride commission's report, "Israel in Lebanon," concluded that the Israeli authorities or forces were responsible in the massacres and other killings that have been reported to have been carried out by Lebanese militiamen in Sabra and Shatila in the Beirut area between 16 and 18 September. Unlike the Israeli commission, the McBride commission did not work with the idea of separate degrees of responsibility, viz., direct and indirect.
Israeli Kahan commission.
Israel's own Kahan commission found that only "indirect" responsibility befitted Israel's involvement. For British journalist David Hirst, Israel crafted the concept of indirect responsibility so as to make its involvement and responsibility seem smaller. He said of the Commission's verdict that it was only by means of errors and omissions in the analysis of the massacre that the Commission was able to reach it.
Sharon's "personal responsibility" for massacre.
The Kahan Commission concluded Israeli Defense minister Ariel Sharon bears personal responsibility[1] "for ignoring the danger of bloodshed and revenge" and "not taking appropriate measures to prevent bloodshed". Sharon's negligence in protecting the civilian population of Beirut, which had come under Israeli control, amounted to a non-fulfillment of a duty with which the Defense Minister was charged, and it was recommended that Sharon be dismissed as Defense Minister.
At first, Sharon refused to resign, and Begin refused to fire him. It was only after the death of Emil Grunzweig after a grenade was tossed into the dispersing crowd of a Peace Now protest march, which also injured ten others, that a compromise was reached: Sharon would resign as Defense Minister, but remain in the Cabinet as a minister without portfolio. Notwithstanding the dissuading conclusions of the Kahan report, Sharon would later become Prime Minister of Israel.
Other conclusions.
The Kahan commission also recommended the dismissal of Director of Military Intelligence Yehoshua Saguy, and the effective promotion freeze of Division Commander Brig. Gen. Amos Yaron for at least three years.
Role of Hobeika.
Robert Maroun Hatem, Elie Hobeika's bodyguard, stated in his book "From Israel to Damascus" that Hobeika ordered the massacre of civilians in defiance of Israeli instructions to behave like a "dignified" army.
Pierre Rehov, a documentary filmmaker who worked on the case with former Lebanese soldiers, while making his film "Holy Land: Christians in Peril", came to the conclusion that Hobeika was definitely responsible for the massacre, despite the orders he had received from Ariel Sharon to behave humanely.
Hobeika was assassinated by a car bomb in Beirut on 24 January 2002. Lebanese and Arab commentators blamed Israel for the murder of Hobeika, with alleged Israeli motive that Hobeika would be ‘apparently poised to testify before the Belgian court about Sharon’s role in the massacre (see section above). Prior to his assassination, Elie Hobeika had stated "I am very interested that the [Belgian] trial starts because my innocence is a core issue."
Sharon libel suit.
Ariel Sharon sued "Time" magazine for libel in American and Israeli courts in a $50 million libel suit, after "Time" published a story in its 21 February 1983, issue, implying that Sharon had "reportedly discussed with the Gemayels the need for the Phalangists to take revenge" for Bachir's assassination. The jury found the article false and defamatory, although "Time" won the suit in the U.S. court because Sharon's defense failed to establish that the magazine's editors and writers had "acted out of malice," as required under the U.S. libel law.
Relatives of victims sue Sharon.
After Sharon's 2001 election to the post of Prime Minister of Israel, relatives of the victims of the massacre filed a lawsuit On 24 September 2003, Belgium's Supreme Court dismissed the war crimes case against Ariel Sharon, since none of the plaintiffs had Belgian nationality at the start of the case.
Reprisal operations.
According to Robert Fisk, Osama bin Laden cited the Sabra and Shatila massacre as one of the motivations for the 1996 Khobar Towers bombing, in which al-Qaeda attacked an American Air Force housing complex in Saudi Arabia.

</doc>
<doc id="52269" url="http://en.wikipedia.org/wiki?curid=52269" title="Edward Heath">
Edward Heath

Sir Edward Richard George "Ted" Heath, KG, MBE (9 July 1916 – 17 July 2005) was Prime Minister of the United Kingdom from 1970 to 1974 and Leader of the Conservative Party from 1965 to 1975.
Born in Kent, Heath studied at Oxford University and served in the Second World War. He was first elected to Parliament in 1950 for Bexley, and was the Chief Whip from 1955 to 1959. Entering the Cabinet as Minister of Labour in 1959, he was later promoted to Lord Privy Seal and later became President of the Board of Trade. In 1965, Heath won the leadership of the Conservative Party against Reginald Maudling and Enoch Powell. The 1966 election months later saw the Labour Government of Harold Wilson win a large victory, although Heath remained leader.
Heath became Prime Minister after winning the 1970 election. In 1971, Heath oversaw the decimalisation of British coinage and in 1972, he implemented major reform to Britain's system of local government; these included a reduction in the number of local authorities across Britain as well as the creation of a number of new metropolitan counties. Possibly most significantly, Heath took Britain into the European Economic Community in 1973. Heath's Premiership also oversaw the height of The Troubles in Northern Ireland, with the suspension of the Stormont Parliament and the imposition of direct British rule. Unofficial talks with Provisional Irish Republican Army (IRA) delegates were unsuccessful, as was the Sunningdale Agreement of 1973, which caused the Ulster Unionist Party to withdraw from the Conservative whip.
Heath also attempted to curb the power of trade unions with the Industrial Relations Act 1971, and had hoped to deregulate the economy and make a transfer from direct to indirect taxation. However, rising unemployment in 1972 caused Heath to reflate the economy at the cost of high inflation, which he attempted to control by a prices and incomes policy. Two miners' strikes, in 1972 and another at the beginning of 1974, proved damaging to the government, with the latter causing the implementation of the Three-Day Week to conserve energy. Heath eventually called an election for February 1974 in an attempt to win a public mandate to face down the miners' wage demands, but this instead resulted in a hung parliament, in which the Tories had the most votes but Labour had slightly more seats. Following a failed attempt to establish a coalition government with the Liberal Party, Heath was forced to resign as Prime Minister in favour of Harold Wilson, whose minority government won a small majority in a second election in October that year.
Despite losing two general elections in quick succession, Heath vowed to continue as leader of his party. In February 1975, however, his former Education Secretary Margaret Thatcher challenged and defeated Heath to win the leadership. Returning to the backbenches, Heath became an active critic of Thatcher's policies as leader and, from 1979, as Prime Minister. He remained a backbench MP until retiring in 2001, serving as the Father of the House for his last nine years in Parliament. Outside of politics, Heath was a world-class yachtsman and a musician of near-professional standard. He was also one of only four British Prime Ministers never to have married.
Early life.
Edward Heath (known as "Teddy" as a young man) was born at 54 Albion Road, Broadstairs, Kent on 9 July 1916, the son of William George Heath, a carpenter and builder, and Edith Anne Heath (née Pantony), a maid. His father was later a successful small businessman. He was educated at Chatham House Grammar School in Ramsgate and in 1935 with the aid of a county scholarship he went up to study at Balliol College, Oxford. A talented musician, he won the college's organ scholarship in his first term (he had previously tried for the organ scholarships at St Catharine's College, Cambridge, and Keble College, Oxford) which enabled him to stay at the university for a fourth year; he eventually graduated with a Second Class Honours BA in Philosophy, Politics and Economics in 1939.
In later years, Heath's peculiar accent – with its "strangulated" vowel sounds, combined with his non-Standard pronunciation of "l" as "w" and "out" as "eout" – was satirised by the "Monty Python's Flying Circus" in the audio sketch "Teach Yourself Heath" (originally recorded for their 1972 LP "Monty Python's Previous Record" but not released at the time). Heath's biographer John Campbell speculates that his speech, unlike that of his father and younger brother, who both spoke with Kent accents, must have undergone "drastic alteration on encountering Oxford", although retaining elements of Kent speech.
While at university Heath became active in Conservative politics. On the key political issue of the day, foreign policy, he opposed the Conservative-dominated government of the day ever more openly. His first Paper Speech (i.e. a major speech listed on the order paper along with the visiting guest speakers) at the Oxford Union, in Michaelmas 1936, was in opposition to the appeasement of Germany by returning her colonies, confiscated after the First World War. In June 1937 he was elected President of the Oxford University Conservative Association as a pro-Spanish-Republican candidate, in opposition to the pro-Franco John Stokes (later a Conservative MP). In 1937–38 he was also chairman of the national Federation of University Conservative Associations, and in the same year (his third at university) he was Secretary then Librarian of the Oxford Union. At the end of the year he was defeated for the Presidency of the Oxford Union by another Balliol candidate, Alan Wood, on the issue of whether the Chamberlain government should give way to a left-wing Popular Front. On this occasion Heath supported the government.
In his final year Heath was President of Balliol College Junior Common Room, an office held in subsequent years by his near-contemporaries Denis Healey and Roy Jenkins, and as such was invited to support the Master of Balliol Alexander Lindsay, who stood as an anti-appeasement 'Independent Progressive' candidate against the official Conservative candidate, Quintin Hogg, in the Oxford by-election, 1938. Heath, who had himself applied to be the Conservative candidate for the by-election, accused the government in an October Union Debate of "turning all four cheeks" to Adolf Hitler, and was elected as President of the Oxford Union in November 1938, sponsored by Balliol, after winning the Presidential Debate that "This House has No Confidence in the National Government as presently constituted". He was thus President in Hilary Term 1939; the visiting Leo Amery described him in his diaries as "a pleasant youth".
As an undergraduate, Heath travelled widely in Europe. His opposition to appeasement was nourished by his witnessing first-hand a Nuremberg Rally in 1937, where he met top Nazis Hermann Göring, Joseph Goebbels and Heinrich Himmler at an SS cocktail party. He later described Himmler as "the most evil man I have ever met". In 1938 he visited Barcelona, then under attack from Spanish Nationalist forces during the Spanish Civil War. In the summer of 1939 he again travelled across Germany, returning to Britain just before the declaration of war.
Second World War.
Heath spent the winter of 1939–40 on a debating tour of the United States before being called up. On 22 March 1941, he received an emergency commission as a second lieutenant in the Royal Artillery. During the war he initially served with heavy anti-aircraft guns around Liverpool (which suffered heavy German bombing in May 1941) and by early 1942 was regimental adjutant, with the war substantive rank of captain. Later, as a temporary major commanding a battery of his own, he provided artillery support in the North-West Europe Campaign of 1944-1945, for which he received a mention in dispatches on 8 November 1945.
According to his autobiography Heath participated as an Adjutant in the Normandy Landings, where he met Maurice Schumann, French Foreign Minister under Pompidou.
Heath later remarked that, although he did not personally kill anybody, as the British forces advanced he saw the devastation caused by his unit's artillery bombardments. In September 1945 he commanded a firing squad that executed a Polish soldier convicted of rape and murder. He was appointed a Member of the Order of the British Empire, Military Division (MBE) on 24 January 1946. He was demobilised in August 1946 and promoted to the substantive rank of lieutenant-colonel on 1 May 1947. Heath joined the Honourable Artillery Company as a lieutenant-colonel on 1 September 1951, in which he remained active throughout the 1950s, rising to Commanding Officer of the Second Battalion; a portrait of him in full dress uniform still hangs in the HAC's Long Room. In April 1971, as Prime Minister, he wore his lieutenant-colonel's insignia to inspect troops.
Post war.
Before the war Heath had won a scholarship to Gray's Inn and had begun making preparations for a career at the Bar, but after the war he instead passed top into the Civil Service. He then became a civil servant in the Ministry of Civil Aviation (he was disappointed not to be posted to the Treasury, but declined an offer to join the Foreign Office, fearing that foreign postings might prevent him from entering politics). He joined a team under (later, Dame) Alison Munro tasked with drawing up a scheme for British airports using some of the many WW2 RAF bases, and was specifically charged with planning the home counties. Years later she attributed his evident enthusiasm for Maplin Airport to this work. Then much to the surprise of civil service colleagues, he sought adoption as the prospective parliamentary candidate for Bexley and resigned in November 1947.
After working as News Editor of the "Church Times" from February 1948 to September 1949, Heath worked as a management trainee at the merchant bankers Brown, Shipley & Co. until his election as Member of Parliament (MP) for Bexley in the February 1950 general election. In the election he defeated an old contemporary from the Oxford Union, Ashley Bramall, with a majority of 133 votes.
Member of Parliament.
Heath made his maiden speech in the House of Commons on 26 June 1950, in which he appealed to the Labour Government to participate in the Schuman Plan. As MP for Bexley, he gave enthusiastic speeches in support of the young, unknown candidate for neighbouring Dartford, Margaret Roberts, soon to become Margaret Thatcher.
In February 1951, Heath was appointed as an Opposition Whip by Winston Churchill. He remained in the Whip's Office after the Conservatives won the 1951 general election, rising rapidly to Joint Deputy Chief Whip, Deputy Chief Whip and, in December 1955, Government Chief Whip under Anthony Eden. Because of the convention that Whips do not speak in Parliament, Heath managed to keep out of the controversy over the Suez Crisis. On the announcement of Eden's resignation, Heath submitted a report on the opinions of the Conservative MPs regarding Eden's possible successors. This report favoured Harold Macmillan and was instrumental in eventually securing Macmillan the premiership in January 1957. Macmillan later appointed Heath Minister of Labour, a Cabinet Minister – as Chief Whip Heath had attended Cabinet but had not been formally a member – after winning the October 1959 election.
In 1960 Macmillan appointed Heath Lord Privy Seal with responsibility for the negotiations to secure the UK's first attempt to join the Common Market (as the European Community was then called). After extensive negotiations, involving detailed agreements about the UK's agricultural trade with Commonwealth countries such as New Zealand, British entry was vetoed by the French President, Charles de Gaulle, at a press conference in January 1963 – much to the disappointment of Heath, who was a firm supporter of European common market membership for the United Kingdom. However, he would oversee a successful application when serving in a higher position a decade later.
After this setback, a major humiliation for Macmillan's foreign policy, Heath was not a contender for the party leadership on Macmillan's retirement in October 1963. Under Prime Minister Sir Alec Douglas-Home he was President of the Board of Trade and Secretary of State for Industry, Trade and Regional Development, and oversaw the abolition of retail price maintenance.
Leader of the Opposition.
After the Conservative Party lost the general election of 1964, the defeated Home changed the party leadership rules to allow for a MP ballot vote, and then resigned. The following year, Heath – who was Shadow Chancellor at the time, and had recently won favourable publicity for leading the fight against Labour's Finance Bill – unexpectedly won the party's leadership contest, gaining 150 votes to Reginald Maudling's 133 and Enoch Powell's 15. Heath became the Tories' youngest leader and retained office after the party's defeat in the general election of 1966.
Heath sacked Enoch Powell from the Shadow Cabinet in April 1968, shortly after Powell made his controversial "Rivers of Blood" speech which criticised Commonwealth immigration to the United Kingdom. Heath never spoke to Powell again.
Prime Minister.
With another general election approaching in 1970 a Conservative policy document emerged from the Selsdon Park Hotel that, according to some historians, offered monetarist and free-market oriented policies as solutions to the country's unemployment and inflation problems. Heath stated that the Selsdon weekend only reaffirmed policies that had actually been evolving since he became leader of the Conservative Party. The prime minister, Harold Wilson, thought the document a vote-loser and dubbed it the product of "Selsdon Man" – after the supposedly prehistoric "Piltdown Man" – in order to portray it as reactionary. But Heath's Conservative Party won the general election of 1970 – 330 seats to Labour's 287. It was the only occasion since 1945 in which one party with a working majority had been replaced in a single election by another party with a working majority.
The new cabinet included Margaret Thatcher (Education and Science), William Whitelaw (Leader of the House of Commons) and the former prime minister Alec Douglas-Home (Foreign and Commonwealth Affairs).
During Heath's first year in office, higher charges were introduced for school meals, spectacles, dentistry, and prescriptions. Entitlement to state Sickness Benefit was also changed so that it would only be paid after the first three days of sickness. As a result of the squeeze in the education budget, Margaret Thatcher acted on the late Iain Macleod's wishes by ending the provision of free school milk for 8- to 11-year-olds (the preceding Labour Government having removed it from secondary schools three years before), for which the tabloid press christened her "Thatcher the Milk Snatcher". Despite these measures, however, the Heath Government encouraged a significant increase in welfare spending, and Thatcher blocked Macleod's other posthumous Education policy: the abolition of the Open University, which had recently been founded by the preceding Labour Government.
Provision was made under the National Insurance (Old Persons’ and Widows’ Pensions and Attendances Allowances) Act 1970 for pensions to be paid to old people who had been excluded from the pre-1948 pension schemes and were accordingly excluded from the comprehensive scheme that was introduced in 1948. About 100,000 people were affected by this change, half of whom were receiving Supplementary Benefit under the social security scheme. The Act also made improvements to the Widows’ Pension scheme by introducing a scale that started at 30 shillings a week for women widowed at the age of 40 and rose to the full rate of £5 at the age of 50.
Considerable support was provided for nursery school building, and a long-term capital investment programme in school building was launched. A Family Fund was set up to provide assistance to families with children who had congenital conditions, while new benefits were introduced benefiting hundreds of thousands of disabled persons whose disabilities had been caused neither by war nor by industrial injury. An Attendance Allowance was introduced for those needing care at home, together with Invalidity Benefit for the long-term sick, while a higher Child Allowance was made available where invalidity allowance was paid. Widow's Benefits were introduced for those aged between forty and fifty years of age, improved subsidies for slum clearance were made available, while Rent Allowances were introduced for private tenants. In April 1971, the right to education was given to all children with Down’s syndrome for the first time.
The school leaving age was raised to 16, while Family Income Supplement was introduced to boost the incomes of low-income earners. Families who received this benefit were exempted from NHS charges while the children in such families were eligible for Free School Meals. Non-contributory pensions were also introduced for all persons aged eighty and above, while the Social Security Act 1973 was passed which introduced benefit indexation in the United Kingdom for the first time by index-linking benefits to prices to maintain their real value.
In Great Britain, Scottish and Welsh nationalism also grew as political forces, while the decimalisation of British coinage, begun under the previous Labour Government, was completed eight months after Heath came to power. The Central Policy Review Staff was established by Heath in February 1971, while the 1972 Local Government Act changed the boundaries of Britain's counties and created "Metropolitan Counties" around the major cities (e.g. Merseyside around Liverpool): this caused significant public anger. Heath did not divide England into regions, choosing instead to await the report of the Crowther Commission on the constitution; the ten Government Office Regions were eventually set up by the Major government in 1994.
Heath's time in office was as difficult as that of all British prime ministers in the 1970s. The government suffered an early blow with the death of Chancellor of the Exchequer Iain Macleod on 20 July 1970; his replacement was Anthony Barber. Heath's planned economic policy changes (including a significant shift from direct to indirect taxation) remained largely unimplemented: the Selsdon policy document was more or less abandoned as unemployment increased considerably by 1972. By January that year, the unemployment rate reached a million, the highest level for more than two decades. Opposed to unemployment on moral grounds, Heath encouraged a famous "U-Turn" in economic policy that precipitated what became known as the "Barber boom." This was a two-range process involving the budgets of 1972 and 1973, the former of which pumped £2.5 billion into the economy in increased pensions and benefits and tax reductions. By early 1974, as a result of this Keynesian economic strategy, unemployment had fallen to under 550,000. The economic boom did not last, however, and the Heath Government implemented various cuts that led to the abandonment of policy goals such as a planned expansion of nursery education.
Heath attempted to rein in the increasingly militant trade union movement, which had so far managed to stop attempts to curb their power by legal means. His Industrial Relations Act 1971 set up a special court under the judge Lord Donaldson, whose imprisonment of striking dockworkers was a public relations disaster that the Thatcher Government of the 1980s would take pains to avoid repeating (relying instead on confiscating the assets of unions found to have broken new anti-strike laws). Heath's attempt to confront trade union power resulted in a political battle, hobbled as the government was by inflation and high unemployment. Especially damaging to the government's credibility were the two miners' strikes of 1972 and 1974, the latter of which resulted in much of the country's industry working a Three-Day Week in an attempt to conserve energy. The National Union of Mineworkers won its case but the energy shortages and the resulting breakdown of domestic consensus contributed to the eventual downfall of his government.
As mentioned above, Heath's government oversaw two years of a steep rise in unemployment, which they later successfully reversed. His Labour predecessor as prime minister, Harold Wilson, had inherited an unemployment count of around 400,000 at the time of his general election win of October 1964 but seen unemployment peak at 631,000 during the spring of 1967, though it had fallen to 582,000 by the time Heath seized power in June 1970. Like Wilson and Labour, Heath and the Tories were pledged to "full employment" but within a year it became clear that they were losing that battle, as the official unemployment count crept towards 1,000,000 and some newspapers suggested that it was even higher. In January 1972, it was officially confirmed that unemployment had risen above 1,000,000 – a level not seen for more than 30 years. Various other reports around this time suggested that unemployment was higher still, with "The Times" newspaper claiming that "nearly 3,000,000" people were jobless by March of that year.
Foreign policy.
Upon entering office in June 1970, Heath immediately set about trying to reverse Wilson's policy of ending Britain's military presence East of Suez. Heath took the United Kingdom into Europe with the European Communities Act 1972 in October (21 Eliz. II c.68). He publicly supported the massive US bombing of Hanoi and Haiphong in April 1972.
In October 1973, he placed a British arms embargo on all combatants in the Arab-Israeli Yom Kippur war, which mostly affected the Israelis by preventing them obtaining spares for their Centurion tanks. Heath refused to allow US intelligence gathering from British bases in Cyprus, resulting in a temporary halt in the US signals intelligence tap. He also refused permission for the US to use any British bases for resupply.
He favoured links with the People's Republic of China, visiting Mao Zedong in Beijing in 1974 and 1975 and remaining an honoured guest in China on frequent visits thereafter and forming a close relationship with Mao's successor Deng Xiaoping. Heath also maintained a good relationship with US President Richard Nixon and figures in the Iraqi Ba'ath Party.
Northern Ireland.
Heath governed during a bloody period in the history of the Northern Ireland Troubles. On Bloody Sunday in 1972, 14 men were killed by British soldiers during a civil rights march in Derry. In early 1971 Heath sent in a Secret Intelligence Service officer, Frank Steele, to talk to the IRA and find out what common ground there was for negotiations. Steele had carried out secret talks with Jomo Kenyatta ahead of the British withdrawal from Kenya. In July 1972, Heath permitted his Secretary of State for Northern Ireland, William Whitelaw, to hold unofficial talks in London with an IRA delegation by Seán Mac Stiofáin. In the aftermath of these unsuccessful talks, the Heath government pushed for a peaceful settlement with the democratic political parties.
The 1973 Sunningdale Agreement, which proposed a power-sharing deal, was strongly repudiated by many Unionists and the Ulster Unionist Party who withdrew its MPs at Westminster from the Conservative whip. The proposal was finally brought down by the Loyalist Ulster Workers' Council strike in 1974 (although by then Heath was no longer in office).
Heath was targeted by the IRA for introducing internment in Northern Ireland. In December 1974, the Balcombe Street ASU threw a bomb onto the first-floor balcony of his home in Wilton Street, Belgravia where it exploded. Heath had been conducting a Christmas carol concert at Broadstairs and arrived home 10 minutes after the bomb exploded. No one was injured in the attack, but a landscape portrait painted by Winston Churchill – given to Heath as a present – was damaged.
In January 2003, Heath gave evidence to the Saville Inquiry and stated that he had never sanctioned unlawful lethal force in Northern Ireland.
Fall from power.
1974 general election.
Heath tried to bolster his government by calling a general election for 28 February 1974, using the election slogan "Who governs Britain?". The result of the election was inconclusive with no party gaining an overall majority in the House of Commons; the Tories had the most votes but Labour had slightly more seats. Heath began negotiations with Jeremy Thorpe, leader of the Liberal Party but, when these failed, he resigned as Prime Minister on 4 March 1974, and was replaced by Wilson's minority Labour government, eventually confirmed, though with a tiny majority, in a second election in October of the same year.
The Centre for Policy Studies, a Conservative group closely involved with the 1970 Selsdon document, began to formulate a new monetarist and free-market policy, initially led by Sir Keith Joseph. Although Margaret Thatcher was associated with the CPS she was initially seen as a potential moderate go-between by Heath's lieutenant James Prior.
Rise of Thatcher.
Heath came to be seen as a liability by many Conservative MPs, party activists and newspaper editors. His personality was cold and aloof, annoying even to his friends. He resolved to remain Conservative leader, even after two general election defeats in one year, and at first it appeared that by calling on the loyalty of his front bench colleagues he might prevail. In the weeks following the second election defeat, Heath came under tremendous pressure to concede a review of the rules and agreed to establish a commission to propose changes and to seek re-election. There was no clear challenger after Enoch Powell had left the party and Keith Joseph had ruled himself out after controversial statements implying that the working classes should be encouraged to use more birth control. Joseph's close friend and ally Margaret Thatcher, who believed an adherent to CPS philosophy should stand, joined the leadership contest in his place alongside the outsider Hugh Fraser. Aided by Airey Neave's campaigning amongst back-bench MPs – whose earlier approach to William Whitelaw had been rebuffed out of loyalty to Heath – she emerged as the only serious challenger.
The new rules permitted new candidates to enter the ballot in a second round of voting should the first be inconclusive, so Thatcher's challenge was considered by some to be that of a stalking horse. Neave deliberately understated Thatcher's support in order to attract wavering votes from MPs who were keen to see Heath replaced even though they did not necessarily want Thatcher to replace him.
On 4 February 1975, Thatcher defeated Heath in the first ballot by 130 votes to 119, with Fraser coming in a distant third with 16 votes. This was not a big enough margin to give Thatcher the 15% majority necessary to win on the first ballot, but having finished in second place Heath immediately resigned and did not contest the next ballot. His favoured candidate, William Whitelaw, lost to Thatcher in the second vote one week later (Thatcher 146, Whitelaw 79, Howe 19, Prior 19, Peyton 11). The vote polarized along right-left lines, with in addition the region, experience and education of the MP having their effects. Heath and Whitelaw were stronger on the left, among Oxbridge and public school graduates, and in MPs from Northern England or Scotland.
Thatcher had promised Heath a seat in the Shadow Cabinet, and planned to offer him whatever post he wanted. His advisors agreed he should wait at least six months, so he declined. He never relented and his refusal was called "the incredible sulk." Thatcher nonetheless visited Heath at his home shortly after her election as leader, and had to stay for coffee with his PPS Tim Kitson so that the waiting press would not realize how brief the visit had been. Heath claimed that he had simply declined her request for advice about how to handle the press, whilst Thatcher claimed that she offered him any Shadow Cabinet position he wanted and asked him to lead the Conservative campaign in the upcoming EEC referendum, only to be rudely rebuffed.
Later career.
Heath for many years persisted in criticism of the party's new ideological direction. At the time of his defeat he was still popular with rank and file Conservative members and was warmly applauded at the 1975 Party Conference. He played a leading role in the 1975 referendum campaign in which Britain voted to remain part of the EEC and remained active on the international stage, serving on the Brandt Commission investigation into developmental issues, particularly on North-South projects (Brandt Report).
His relations with Thatcher remained negative, and in 1979–80 he turned down her offers of ambassador to the U.S. and secretary-general of NATO. He continued as a central figure on the left of the party and, at the 1981 Conservative Party conference, openly criticised the government's economic policies – namely monetarism, which had seen inflation cut from 27% in 1979 to 4% by 1983, but had seen unemployment double from around 1,500,000 to a postwar high of more than 3,000,000 during that time. In 1990 he flew to Baghdad to attempt to negotiate the release of aircraft passengers and other British nationals taken hostage when Saddam Hussein invaded Kuwait. After Black Wednesday in 1992 he stated in the House of Commons that government should build a fund of reserves to counter currency speculators.
In the 1960s Heath had lived at a flat in the Albany, off Piccadilly; at the unexpected end of his premiership he took the flat of a Conservative MP Tim Kitson for some months. In February 1985 Heath moved to Salisbury, where he resided until his death 20 years later. In 1987 he was nominated in the election for the Chancellorship of the University of Oxford but lost to Roy Jenkins as a result of splitting the Conservative vote with Lord Blake.
Heath continued to serve as a back bench MP for the London constituency of Old Bexley and Sidcup and was, from 1992, the longest-serving MP ("Father of the House") and the oldest British MP. As Father of the House he oversaw the election of two Speakers of the Commons, Betty Boothroyd and Michael Martin. Heath was created a Knight of the Garter on 23 April 1992. He retired from Parliament at the 2001 general election. He and Tony Benn were the last two serving MPs to have been elected under George VI, with Heath being the only one to have served continuously since 1950.
Parliament broke with precedent by commissioning a bust of Heath while he was still alive. The 1993 bronze work, by Martin Jennings, was moved to the Members' Lobby in 2002. On 29 April 2002, in his 86th year, he made a public appearance at Buckingham Palace alongside the then prime minister Tony Blair and the three other surviving former prime ministers, as well as relatives of deceased prime ministers, for a dinner which was part of the Golden Jubilee of Elizabeth II. This was to be one of his last public appearances, as the following year saw a decline in his health.
Illness and death.
In August 2003, at the age of 87, Heath suffered a pulmonary embolism while on holiday in Salzburg, Austria. He never fully recovered, and owing to his declining health and mobility made very few public appearances in the final two years of his life. His last public appearance was at the unveiling of a set of gates to Sir Winston Churchill at St Paul's Cathedral on 30 November 2004.
Heath paid tribute to James Callaghan who died on 26 March 2005, saying that "James Callaghan was a major fixture in the political life of this country during his long and varied career. When in opposition he never hesitated to put firmly his party's case. When in office he took a smoother approach towards his supporters and opponents alike. Although he left the House of Commons in 1987 he continued to follow political life and it was always a pleasure to meet with him. We have lost a major figure from our political landscape".
This was his last public statement. Heath died from pneumonia on the evening of 17 July 2005, at the age of 89. He was cremated on 25 July 2005 at a funeral service attended by 1,500 people. The day after his death the BBC Parliament channel showed the BBC results coverage of the 1970 election. A memorial service was held for Heath in Westminster Abbey on 8 November 2005 which was attended by two thousand people. Three days later his ashes were interred in Salisbury Cathedral. In a tribute to him, the then Prime Minister Tony Blair stated "He was a man of great integrity and beliefs he held firmly from which he never wavered".
"Arundells".
In January 2006, it was announced that Heath had left his house and contents to the value of £5 million in his will, most of it to a charitable foundation to conserve his 18th-century house, "Arundells", opposite Salisbury Cathedral, as a museum to his career. The house is open to the public for guided tours from March to October, and displayed is a large collection of personal effects as well as Heath's personal library, photo collections and paintings by Winston Churchill.
In his will Heath, who had had no descendants, left only two legacies: £20,000 to his brother's widow, and £2,500 to his housekeeper.
Personal life.
Yachting.
Heath was a keen yachtsman. He bought his first yacht "Morning Cloud" in 1969 and won the Sydney to Hobart Yacht Race that year. He captained Britain's winning team for the Admiral's Cup in 1971 – while Prime Minister – and also captained the team in the 1979 Fastnet race. He was a member of the Sailing Club in his home town, Broadstairs. Heath's hobby is referred to in the 2008 film "The Bank Job" where it is said that the Prime Minister himself may meet with the bank robbers ""if you can drag him off his yacht"."
Conductor.
Heath also maintained an interest in orchestral music as an organist and conductor, famously installing a Steinway grand in 10 Downing Street – bought with his £450 Charlemagne Prize money, awarded for his unsuccessful efforts to bring Britain into the EEC in 1963, and chosen on the advice of his friend, the pianist Moura Lympany – and conducting Christmas carol concerts in Broadstairs every year from his teens until old age. Heath often played the organ for services at Holy Trinity Church Brompton in his early years.
Heath conducted the London Symphony Orchestra, notably at a gala concert at the Royal Festival Hall in November 1971, at which he conducted Sir Edward Elgar's overture "Cockaigne (In London Town)". He also conducted the Royal Liverpool Philharmonic and the English Chamber Orchestra, as well as orchestras in Germany and the United States. Heath received honorary degrees from the Royal College of Music and Royal College of Organists. During his premiership, Heath invited musician friends, such as Isaac Stern, Yehudi Menuhin, Clifford Curzon and the Amadeus Quartet, to perform either at Chequers or 10 Downing Street. Heath was the founding President of the European Community Youth Orchestra (in 1976), now the European Union Youth Orchestra.
In 1988, Heath recorded Beethoven's Triple Concerto, Op. 56 (with members of the Trio Zingara as soloists) and Boccherini's Cello Concerto in G major, G480.
Performing arts.
Heath enjoyed the performing arts as a whole. In particular, he gave a great deal of support to performing arts causes in his constituency and was known to be proud of the fact that his constituency boasted two of the country's leading performing arts schools. Rose Bruford College and Bird College are both situated in Sidcup, and a purpose built facility for the latter was officially opened by Heath in 1979.
Heath also wrote a book called "The Joy of Christmas: A Collection of Carols", published in 1978 by Oxford University Press and including the music and lyrics to a wide variety of Christmas carols each accompanied by a reproduction of a piece of religious art and a short introduction by Heath.
Football.
Heath was a supporter of the Lancashire football club Burnley, and just after the end of his term as prime minister in 1974 he opened the £450,000 Bob Lord Stand at the club's Turf Moor stadium.
Author.
He wrote three non-political books, "Sailing", "Music", and "Travels", and an autobiography, "The Course of My Life" (1998). Heath's "Daily Telegraph" obituary noted that his autobiography "had involved dozens of researchers and writers (some of whom he never paid) over many years".
Private life.
Heath was a lifelong bachelor. Heath's interest in music kept him on friendly terms with a number of female musicians including Moura Lympany, and he always had the company of women when social circumstances required. Lympany had thought he would marry her, but when asked about the most intimate thing he had done, replied, "He put his arm around my shoulder." Bernard Levin wrote at the time in "The Observer" that "an ironical comment" on the permissive society was that the UK had had to wait until the 1970s for a prime minister who was a virgin.
John Campbell, who published a biography of Heath in 1993, devoted four pages to a discussion of the evidence concerning Heath's sexuality. Whilst acknowledging that Heath was often assumed by the public to be gay, not least because it is "nowadays ... whispered of any bachelor" he found "no positive evidence" that this was so "except for the faintest unsubstantiated rumour" (the footnote refers to a mention of a "disturbing incident" at the beginning of the Second World War in a 1972 biography by Andrew Roth).
Heath had been expected to marry childhood friend Kay Raven, who reportedly tired of waiting and married a RAF officer whom she met on holiday in 1950. In a terse four-sentence paragraph of his memoirs, Heath claimed that he had been too busy establishing a career after the war and had "perhaps ... taken too much for granted". In a 1998 TV interview with Michael Cockerell, Heath admitted that he had kept her photograph in his flat for many years afterwards.
In 2007, Brian Coleman, the Conservative Party London Assembly member for Barnet and Camden, claimed that Heath, in order to protect his career, stopped cottaging for gay sex in the 1950s. Coleman claimed it was "common knowledge" among Conservatives that Heath had been given a stern warning by police when he underwent background checks for the post of Privy Councillor. Heath's biographer Philip Ziegler writes that Coleman was able to provide “little or no evidence” to back up this statement, that no man has ever claimed to have had a sexual relationship with Heath, nor is any trace of homosexuality to be found in his papers, and that “those who knew him well” insist that he had no such inclination. He believes Heath to have been “asexual”.
Charles Moore's authorised biography of Margaret Thatcher states that Bill Deedes believed that Thatcher “seem(ed) convinced” Heath was homosexual, whilst Moore believes it is “possible” that Thatcher’s reference, in interview in 1974, to Heath not having a family, was a deliberate hint that he was gay, in order to discredit him.
In 2014, Jeremy Norman, owner of the chain 'Soho Gyms' and friend of Heath, claimed that Heath was "most likely gay".
Personality.
Heath's rudeness was much noted. In 1975 his "brusqueness, his gaucherie, his lack of small or indeed any talk, his sheer bad manners" were among the factors costing him the support of Conservative backbenchers.
Geoffrey Wheatcroft wrote that – ironically given his "tact and patience" as Chief Whip as a younger man – "he later became a byword for graceless petulance and sheer rudeness," (quoting his official biographer Philip Ziegler) "at dinner 'apt to relapse into morose silence or completely ignore the woman next to him and talk across her to the nearest man' ".
Grocer Heath.
Heath led the successful fight to abolish retail price maintenance, which grocers and small shopkeepers wanted to keep in place so that large stores could not sell items cheaper. "Private Eye," a humour magazine thereupon persistently ridiculed him as "The Grocer", or "Grocer Heath," emphasizing his lower middle class origins.

</doc>
<doc id="52270" url="http://en.wikipedia.org/wiki?curid=52270" title="Pica">
Pica

Pica or PICA may refer to: 

</doc>
<doc id="52271" url="http://en.wikipedia.org/wiki?curid=52271" title="Harold Wilson">
Harold Wilson

James Harold Wilson, Baron Wilson of Rievaulx, KG, OBE, FRS, FSS, PC (11 March 1916 – 24 May 1995) was a British Labour Party politician who served as the Prime Minister of the United Kingdom from 1964 to 1970 and 1974 to 1976. He won four general elections, and is the most recent British Prime Minister to have served non-consecutive terms.
First entering Parliament in 1945, Wilson was immediately appointed the Parliamentary Secretary to the Ministry of Works and rose quickly through the ministerial ranks, becoming the Secretary for Overseas Trade in 1947 and being appointed to the Cabinet just months later as the President of the Board of Trade. Later, in the Labour Shadow Cabinet, he served first as Shadow Chancellor of the Exchequer from 1955 to 1961 and then as the Shadow Foreign Secretary from 1961 to 1963, when he was elected Leader of the Labour Party after the sudden death of Hugh Gaitskell. Wilson narrowly won the 1964 election, going on to win a much increased majority in a snap 1966 election.
Wilson's first period as Prime Minister coincided with a period of low unemployment and relative economic prosperity, though also of significant problems with Britain's external balance of payments. In 1969 Wilson sent British troops to Northern Ireland. After losing the 1970 general election to Edward Heath, he spent four years as Leader of the Opposition before the February 1974 general election resulted in a hung parliament. After Heath's talks with the Liberals broke down, Wilson returned to power as leader of a minority government until there was a second general election in the autumn, which resulted in a narrow Labour victory. A period of economic crisis was now beginning to hit most Western countries, and in 1976 Wilson suddenly announced his resignation as Prime Minister.
Wilson's own approach to socialism was moderate, with emphasis on increasing opportunity within society, for example through change and expansion within the education system, allied to the technocratic aim of taking better advantage of rapid scientific progress, rather than on the more controversial socialist goal of promoting wider public ownership of industry. He took little action to pursue the Labour Party constitution's stated dedication to such nationalisation, though he did not formally disown it. Himself a member of the Labour Party's "soft left", Wilson joked about leading a Cabinet that was made up mostly of social democrats, comparing himself to a Bolshevik revolutionary presiding over a Tsarist cabinet, but there was arguably little to divide him ideologically from the cabinet majority.
Wilson's first period in office, in particular, was notable for substantial legal changes in a number of social areas; though they were generally not at the top of his personal agenda. These included the liberalisation of laws on censorship, divorce, homosexuality, immigration, and abortion; as well as the abolition of capital punishment, which was due in part to the initiatives of backbench MPs who had the support of Roy Jenkins during his time as Home Secretary. Overall, Wilson is seen to have managed a number of difficult political issues with considerable tactical skill, including such potentially divisive issues for his party as the role of public ownership, British membership of the European Community, and the Vietnam War, in which he consistently resisted US pressure to involve Britain and send British troops, while continuing to maintain a costly military presence East of Suez. Nonetheless, his stated ambition of substantially improving Britain's long-term economic performance remained largely unfulfilled.
Early life.
Wilson was born at 4 Warneford Road, Huddersfield, in the West Riding of Yorkshire, England on 11 March 1916. He came from a political family: his father James Herbert Wilson (December 1882 – 1971) was a works chemist who had been active in the Liberal Party and then joined the Labour Party. His mother Ethel ("née" Seddon; 1882–1957) was a schoolteacher before her marriage. When Wilson was eight, he visited London and a later-to-be-famous photograph was taken of him standing on the doorstep of 10 Downing Street.
He was a supporter of his hometown football club, Huddersfield Town.
Education.
Wilson won a scholarship to attend Royds Hall Grammar School, his local grammar school (now a comprehensive school) in Huddersfield in Yorkshire. His education was disrupted in October 1930 when he contracted typhoid fever after drinking contaminated milk on a Scouts' outing. It took him three months to recover. In December 1930, his father, working as an industrial chemist, was made redundant and it took him nearly two years to find work. He moved to Spital on the Wirral, Cheshire in order to do so. Wilson was educated in the Sixth Form at the Wirral Grammar School for Boys, where he became Head Boy.
Wilson did well at school and, although he missed getting a scholarship, he obtained an exhibition; which, when topped up by a county grant, enabled him to study Modern History at Jesus College, Oxford, from 1934. At Oxford, Wilson was moderately active in politics as a member of the Liberal Party but was later influenced by G. D. H. Cole to join the Labour Party. After his first year, he changed his field of study to Philosophy, Politics and Economics. He graduated with "an outstanding first class Bachelor of Arts degree, with alphas on every paper" in the final examinations. A popular urban myth at Oxford University states that Wilson's grade in his final examination was the highest ever recorded up to that date.
Although Wilson had two abortive attempts at an All Souls Fellowship, he continued in academia, becoming one of the youngest Oxford University dons of the century at the age of 21. He was a lecturer in Economic History at New College from 1937, and a Research Fellow at University College.
Marriage.
On New Year's Day 1940, in the chapel of Mansfield College, Oxford, he married Mary Baldwin who remained his wife until his death. Mary Wilson became a published poet. They had two sons, Robin and Giles (named after Giles Alington); Robin became a Professor of Mathematics, and Giles became a teacher. In their twenties, his sons were under a kidnap threat from the IRA because of their father's prominence.
Second World War.
On the outbreak of the Second World War, Wilson volunteered for service but was classed as a specialist and moved into the civil service instead. For much of this time, he was a research assistant to William Beveridge, the Master of the College, working on the issues of unemployment and the trade cycle. He later became a statistician and economist for the coal industry. He was Director of Economics and Statistics at the Ministry of Fuel and Power 1943–44, and received an OBE for his services.
He was to remain passionately interested in statistics. As President of the Board of Trade, he was the driving force behind the Statistics of Trade Act 1947, which is still the authority governing most economic statistics in Great Britain. He was instrumental as Prime Minister in appointing Claus Moser as head of the Central Statistical Office, and was president of the Royal Statistical Society in 1972–73.
Member of Parliament.
As the war drew to an end, he searched for a seat to fight at the impending general election. He was selected for the constituency of Ormskirk, then held by Stephen King-Hall. Wilson agreed to be adopted as the candidate immediately rather than delay until the election was called, and was therefore compelled to resign from his position in the Civil Service. He served as Praelector in Economics at University College between his resignation and his election to the House of Commons. He also used this time to write "A New Deal for Coal", which used his wartime experience to argue for nationalisation of the coal mines on the grounds of the improved efficiency he predicted would ensue.
In the 1945 general election, Wilson won his seat in the Labour landslide. To his surprise, he was immediately appointed to the government by Prime Minister Clement Attlee as Parliamentary Secretary to the Ministry of Works. Two years later, he became Secretary for Overseas Trade, in which capacity he made several official trips to the Soviet Union to negotiate supply contracts.
In the general election of 1950, his Ormskirk constituency was significantly altered and he was narrowly elected for the new seat of Huyton near Liverpool, where he served for 33 years until 1983.
Cabinet.
On 29 September 1947 Wilson was appointed President of the Board of Trade, at 31 becoming the youngest member of a British Cabinet in the 20th century. He took a lead in abolishing some wartime rationing, which he referred to as a "bonfire of controls". His role in internal debates during the summer of 1949 over whether or not to devalue sterling, in which he was perceived to have played both sides of the issue, tarnished his reputation in both political and official circles.
Wilson was becoming known in the Labour Party as a left-winger and joined Aneurin Bevan and John Freeman in resigning from the government in April 1951 in protest at the introduction of National Health Service (NHS) medical charges to meet the financial demands imposed by the Korean War. After the Labour Party lost the 1951 election, he became the Chairman of Keep Left, Bevan's political group, but soon after he began to distance himself from Bevan.
Shadow Cabinet.
Wilson was appointed to the Shadow Cabinet by Labour Party Leader Hugh Gaitskell in 1955 as Shadow Chancellor of the Exchequer, and he proved to be very effective. One of his procedural moves caused a substantial delay to the progress of the Government's Finance Bill in 1955, and his speeches as Shadow Chancellor from 1956 were widely praised for their clarity and wit. He coined the term "Gnomes of Zurich" to describe Swiss bankers whom he accused of pushing the pound down by speculation. As well as his role as Shadow Chancellor, he conducted an inquiry into the Labour Party's organisation following its defeat in the 1955 general election, which compared Labour's organisation to an antiquated "penny farthing" bicycle, and made various recommendations for improvements. Unusually, Wilson combined the job of Chairman of the House of Commons' Public Accounts Committee with that of Shadow Chancellor from 1959, holding that position until 1963.
Wilson steered a course in intra-party matters in the 1950s and early 1960s that left him neither fully accepted nor trusted by the left and the right in the Labour Party. Despite his earlier association with the left-wing Aneurin Bevan, in 1955 he backed Gaitskell, considered the right-of-centre candidate in internal Labour Party terms, against Bevan for the party leadership. He then launched an opportunistic but unsuccessful challenge to Gaitskell's leadership in November 1960 in the wake of the Labour Party's 1959 defeat, Gaitskell's controversial attempt to ditch Labour's commitment to nationalisation by scrapping Clause Four, and Gaitskell's defeat at the 1960 Party Conference over a motion supporting unilateral nuclear disarmament. Wilson would later be moved to the position of Shadow Foreign Secretary in 1961, before he challenged for the deputy leadership in 1962 but was defeated by George Brown.
Gaitskell died in January 1963, just as the Labour Party had begun to unite and appeared to have a very good chance of winning the next election, with the Macmillan Government running into trouble. Wilson was adopted as the left-wing candidate for the leadership, defeating Brown and James Callaghan to become the Leader of the Labour Party and the Leader of the Opposition.
At the Labour Party's 1963 Annual Conference, Wilson made both his best-remembered speech, on the implications of scientific and technological change. He argued that "the Britain that is going to be forged in the white heat of this revolution will be no place for restrictive practices or for outdated measures on either side of industry". This speech did much to set Wilson's reputation as a technocrat not tied to the prevailing class system.
Labour's 1964 election campaign was aided by the Profumo Affair, a ministerial sex scandal that had mortally wounded Harold Macmillan and was to taint his successor Sir Alec Douglas-Home, even though Home had not been involved in the scandal. Wilson made capital without getting involved in the less salubrious aspects. (Asked for a statement on the scandal, he reportedly said "No comment ... in glorious Technicolor!"). Home was an aristocrat who had given up his title as Lord Home to sit in the House of Commons and become Prime Minister upon Macmillan's resignation. To Wilson's comment that he was the 14th Earl of Home, Home retorted, "I suppose Mr. Wilson is the fourteenth Mr. Wilson".
First term as Prime Minister.
Labour won the 1964 general election with a narrow majority of four seats, and Wilson became Prime Minister, the youngest person to hold that office since Lord Rosebery 70 years earlier. During 1965, by-election losses reduced the government's majority to a single seat; but in March 1966 Wilson took the gamble of calling another general election. The gamble paid off, because this time Labour achieved a 96-seat majority over the Conservatives, who the previous year had made Edward Heath their leader.
Domestic affairs.
Economic policies.
In economic terms, Wilson's first three years in office were dominated by an ultimately doomed effort to stave off the devaluation of the pound. He inherited an unusually large external deficit on the balance of trade. This partly reflected the preceding government's expansive fiscal policy in the run-up to the 1964 election, and the incoming Wilson team tightened the fiscal stance in response. Many British economists advocated devaluation, but Wilson resisted, reportedly in part out of concern that Labour, which had previously devalued sterling in 1949, would become tagged as "the party of devaluation". In the latter half of 1967, however, an attempt was made to prevent the recession in activity from going too far in the form of a stimulus to consumer durable spending through an easing of credit, which in turn prevented a winter rise in unemployment.
After a costly battle, market pressures forced the government into devaluation in 1967. Wilson was much criticised for a broadcast in which he assured listeners that the "pound in your pocket" had not lost its value. It was widely forgotten that his next sentence had been "prices will rise". Economic performance did show some improvement after the devaluation, as economists had predicted. The devaluation, with accompanying austerity measures, successfully restored the balance of payments to surplus by 1969. This unexpectedly turned into a small deficit again in 1970. The bad figures were announced just before polling in the 1970 general election, and are often cited as one of the reasons for Labour's defeat.
A main theme of Wilson's economic approach was to place enhanced emphasis on "indicative economic planning". He created a new Department of Economic Affairs to generate ambitious targets that were in themselves supposed to help stimulate investment and growth (the government also created a Ministry of Technology (shortened to Mintech) to support the modernisation of industry). The DEA itself was in part intended to serve as an expansionary counter-weight to what Labour saw as the conservative influence of the Treasury, though the appointment of Wilson's deputy, George Brown, as the Minister in charge of the DEA was something of a two-edged sword, in view of Brown's reputation for erratic conduct; in any case the government's decision over its first three years to defend sterling's parity with traditional deflationary measures ran counter to hopes for an expansionist push for growth. Though now out of fashion, the faith in indicative planning as a pathway to growth, embodied in the DEA and Mintech, was at the time by no means confined to the Labour Party – Wilson built on foundations that had been laid by his Conservative predecessors, in the shape, for example, of the National Economic Development Council (known as "Neddy") and its regional counterparts (the "little Neddies"). Government intervention in industry was greatly enhanced, with the National Economic Development Office greatly strengthened, with the number of "little Neddies" was increased, from eight in 1964 to twenty-one in 1970. The government's policy of selective economic intervention was later characterised by the establishment of a new super-ministry of technology, under Tony Benn.
The continued relevance of industrial nationalisation (a centrepiece of the post-War Labour government's programme) had been a key point of contention in Labour's internal struggles of the 1950s and early 1960s. Wilson's predecessor as leader, Hugh Gaitskell, had tried in 1960 to tackle the controversy head-on, with a proposal to expunge Clause Four (the public ownership clause) from the party's constitution, but had been forced to climb down. Wilson took a characteristically more subtle approach. He threw the party's left wing a symbolic bone with the renationalisation of the steel industry, but otherwise left Clause Four formally in the constitution but in practice on the shelf.
Wilson made periodic attempts to mitigate inflation through wage-price controls, better known in Britain as "prices and incomes policy" (as with indicative planning, such controls—though now generally out of favour – were widely adopted at that time by governments of different ideological complexions, including the Nixon administration in the United States). Partly as a result of this reliance, the government tended to find itself repeatedly injected into major industrial disputes, with late-night "beer and sandwiches at Number Ten" an almost routine culmination to such episodes. Among the most damaging of the numerous strikes during Wilson's periods in office was a six-week stoppage by the National Union of Seamen, beginning shortly after Wilson's re-election in 1966, and conducted, he claimed, by "politically motivated men".
With public frustration over strikes mounting, Wilson's government in 1969 proposed a series of changes to the legal basis for industrial relations (labour law), which were outlined in a White Paper "In Place of Strife" put forward by the Employment Secretary Barbara Castle. Following a confrontation with the Trades Union Congress, which strongly opposed the proposals, and internal dissent from Home Secretary James Callaghan, the government substantially backed-down from its intentions. Some elements of these changes were subsequently to be enacted (in modified form) during the premiership of Margaret Thatcher.
Wilson's government made a variety of changes to the tax system. Largely under the influence of the Hungarian-born economists Nicholas Kaldor and Thomas Balogh, an idiosyncratic Selective Employment Tax (SET) was introduced that was designed to tax employment in the service sectors while subsidising employment in manufacturing (the rationale proposed by its economist authors derived largely from claims about potential economies of scale and technological progress, but Wilson in his memoirs stressed the tax's revenue-raising potential). The SET did not long survive the return of a Conservative government. Of longer term significance, Capital Gains Tax (CGT) was introduced across the UK on 6 April 1965. Across his two periods in office, Wilson presided over significant increases in the overall tax burden in the UK. In 1974, three weeks after forming a new government, Wilson's new chancellor Denis Healey partially reversed the 1971 reduction in the top rate of tax from 90% to 75%, increasing it to 83% in his first budget, which came into law in April 1974. This applied to incomes over £20,000 (£ in 2015), and combined with a 15% surcharge on 'un-earned' income (investments and dividends) could add to a 98% marginal rate of personal income tax. In 1974, as many as 750,000 people were liable to pay the top-rate of income tax. Labour's identification with high tax rates was to prove one of the issues that helped the Conservative Party under Margaret Thatcher and John Major dominate British politics during the 1980s and early-to-mid-1990s.
Wilson had entered power at a time when unemployment stood at around 400,000. It still stood 371,000 by early 1966 after a steady fall during 1965, but by March 1967 it stood at 631,000. It fell again towards the end of the decade, standing at 582,000 by the time of the general election in June 1970.
Social issues.
A number of liberalising social reforms were passed through parliament during Wilson's first period in government. These included the abolition of capital punishment, decriminalisation of sex between men in private, liberalisation of abortion law and the abolition of theatre censorship. The Divorce Reform Act 1969 was passed by Parliament (and came into effect in 1971). Such reforms were mostly via private member's bills on 'free votes' in line with established convention, but the large Labour majority after 1966 was undoubtedly more open to such changes than previous parliaments had been.
Wilson personally, coming culturally from a provincial non-conformist background, showed no particular enthusiasm for much of this agenda (which some linked to the "permissive society"), but the reforming climate was especially encouraged by Roy Jenkins during his period at the Home Office. The franchise was also extended with the reduction of the voting age from twenty-one to eighteen in 1969.
Wilson's 1966–70 term witnessed growing public concern over the level of immigration to the United Kingdom. The issue was dramatised at the political level by the famous "Rivers of Blood speech" by the Conservative politician Enoch Powell, warning against the dangers of immigration, which led to Powell's dismissal from the Shadow Cabinet. Wilson's government adopted a two-track approach. While condemning racial discrimination (and adopting legislation to make it a legal offence), Wilson's Home Secretary James Callaghan introduced significant new restrictions on the right of immigration to the United Kingdom.
Education.
Education held special significance for a socialist of Wilson's generation, in view of its role in both opening up opportunities for children from working-class backgrounds and enabling Britain to seize the potential benefits of scientific advances. Under the first Wilson government, for the first time in British history, more money was allocated to education than to defence. Wilson continued the rapid creation of new universities, in line with the recommendations of the Robbins Report, a bipartisan policy already in train when Labour took power. The economic difficulties of the period deprived the tertiary system of the resources it needed. Nevertheless, university expansion remained a core policy. One notable effect was the first entry of women into university education in significant numbers. More broadly, higher education overall was significantly expanded, with a distinct bias towards the non-university sector. During Wilson’s time in office from 1964-1970, some 30 polytechnics were set up to provide vocationally-oriented courses that were not fully provided by universities. In addition, student participation rates were increased from 5% to 10%.
Wilson also deserves credit for grasping the concept of an Open University, to give adults who had missed out on tertiary education a second chance through part-time study and distance learning. His political commitment included assigning implementation responsibility to Baroness Lee, the widow of Aneurin Bevan, the charismatic leader of Labour's left wing whom Wilson had joined in resigning from the Attlee cabinet. The Open University worked through summer schools, postal tuition and television programmes. By 1981, 45,000 students had received degrees through the Open University. Money was also channelled into local-authority run colleges of education.
Wilson's record on secondary education is, by contrast, highly controversial. A fuller description is in the article Education in England. Two factors played a role. Following the Education Act 1944 there was disaffection with the tripartite system of academically oriented Grammar schools for a small proportion of "gifted" children, and Technical and Secondary Modern schools for the majority of children. Pressure grew for the abolition of the selective principle underlying the "eleven plus", and replacement with Comprehensive schools which would serve the full range of children (see the article Debates on the grammar school). Comprehensive education became Labour Party policy. From 1966 to 1970, the proportion of children in comprehensive schools increased from about 10% to over 30%. There was also a move in primary schools towards "child-centred" or individual learning, in keeping with the recommendations of the 1967 Plowden Report on improving the education system. Polytechnics were established in 1965 through the amalgamation of existing institutions such as colleges of technology, art, and commerce. A new external examination, designed for children of middling intellectual ability and leading to a Certificate of Secondary Education (CSE), was also introduced that same year. Advanced level courses in further education were also expanded by the government much faster than under the previous Conservative government.
Labour pressed local authorities to convert grammar schools into comprehensives. Conversion continued on a large scale during the subsequent Conservative Heath administration, although the Secretary of State, Margaret Thatcher, ended the compulsion of local governments to convert.
A major controversy that arose during Wilson's first government was the decision that the government could not fulfil its long-held promise to raise the school leaving age to 16, because of the investment required in infrastructure, such as extra classrooms and teachers. Baroness Lee considered resigning in protest, but narrowly decided against this in the interests of party unity. It was left to Thatcher to carry out the change, during the Heath government.
Attempts were also made to improve the provision of nursery education. In 1960, as a means of saving money, the Conservative government issued a circular which forbade the expansion of nursery education. This restriction was slightly relaxed just before the July 1964 election, when authorities were allowed to provide places "where this would enable married women to return to teaching." In 1965, the Labour government provided a further relaxation which allowed authorities to expand "so long as they provided some extra places for teachers to whom priority was to be given." Nevertheless, the number of children under five in maintained nursery, primary, and special schools increased only slightly, from 222,000 in 1965 to 239,000 in 1969.
Overall, public expenditure on education rose as a proportion of GNP from 4.8% in 1964 to 5.9% in 1968, and the number of teachers in training increased by more than a third between 1964 and 1967. The percentage of students staying on at school after the age of sixteen increased similarly, and the student population increased by over 10% each year. Pupil-teacher ratios were also steadily reduced. As a result of the first Wilson government's educational policies, opportunities for working-class children were improved, while overall access to education in 1970 was broader than in 1964. As summarised by Brian Lapping,
"The years 1964–70 were largely taken up with creating extra places in universities, polytechnics, technical colleges, colleges of education: preparing for the day when a new Act would make it the right of a student, on leaving school, to have a place in an institution of further education."
In 1966, Wilson was created the first Chancellor of the newly created University of Bradford, a position he held until 1985.
Housing.
Housing was a major policy area under the first Wilson government. During Wilson's time in office from 1964 to 1970, more new houses were built than in the last six years of the previous Conservative government. The proportion of council housing rose from 42% to 50% of the total, while the number of council homes built increased steadily, from 119,000 in 1964 to 133,000 in 1965 and to 142,000 in 1966. Allowing for demolitions, 1.3 million new homes were built between 1965 and 1970, To encourage home ownership, the government introduced the Option Mortgage Scheme (1968), which made low-income housebuyers eligible for subsidies (equivalent to tax relief on mortgage interest payments). This scheme had the effect of reducing housing costs for buyers on low incomes and enabling more people to become owner occupiers. In addition, house owners were exempted from capital gains tax. Together with the Option Mortgage Scheme, this measure stimulated the private housing market. To improve conditions for homeless people, a joint circular of the Ministry of Health, Home Office, and Ministry of Local Government of 1966 recommended that families “ought not to be spilt at reception centres, and that more family privacy was desirable.” According to one study, the “great majority” of local authorities incorporated these suggestions into their policies.”
The government also accepted most of the recommendations of the 1961 Parker Morris Report for significantly improved standards of space and amenities new local authority dwellings. The first Wilson government made Parker Morris recommendations mandatory for public sector housing in new towns in 1967 and for local authorities in 1969. By 1967, almost 85% of council dwellings were being built to the standards laid out by the 1961 Parker Morris Report, and from January 1969 Parker Morris space and heating standards became mandatory. in public housing design.
Significant emphasis was also placed on town planning, with new conservation areas introduced and a new generation of new towns built, notably Milton Keynes. The New Towns Acts of 1965 and 1968 together gave the government the authority (through its ministries) to designate any area of land as a site for a New Town. The government also combined its push for the construction of more new housing with encouragement and subsidisation of the renovation of old houses (as an alternative to their destruction and replacement). The Housing Improvement Act 1969, for example, made it easier to turn old houses into new homes by encouraging rehabilitation and modernisation through increased grants to property owners. The Act sought to place the economics of housing improvement in a much better relationship to those of redevelopment. Under the Act, local authorities were provided with powers to designate "improvement areas" and to pursue a policy of area-wide improvement. An area could be declared an improvement area if 50% or more of the dwellings within its bounds lacked at least one of the following standard amenities, which included hot and cold running water, an inside toilet, a sink, a wash basin, and a fixed bath or shower. Local authorities in the area could encourage householders in the area to improve their dwellings with the aid of grants. The legislation also introduced major financial changes, including an increase in the normal total standard grant from £155 to £200, an increase from £400 to £1000 in the maximum improvement grant that might be given at the discretion of the local authority, and a new Exchequer grant to local authorities of 50% of the expense for environmental improvement on costs of up to £100 per dwelling in newly designated improvement areas. The legislation introduced special grants for installing amenities in houses in multi-occupation and government grants towards environmental improvement up to an expenditure of £100 per dwelling, while approved works of repair and replacement became eligible for grant aid for the first time ever. Altogether, between 1965 and 1970, over 2 million homes had been constructed (almost half of which were council properties), more than in any other five-year period since 1918.
The Protection from Eviction Act 1964 outlawed the eviction of tenants without a court order, and according to Colin Crouch and Martin Wolf, did much "to stem the rising tide of homelessness," especially in London. The Rent Act 1965 extended security of tenure, introduced registration of rents, and protection from eviction for private tenants, making illegal the harassment of tenants. This legislation was attributed to fall in number of homeless families taken into welfare accommodation each year in the LCC area, from 2,000 in 1962–64 to 1,300 in 1965 and 1,500 in 1966. The Leasehold Reform Act 1967 was passed in order to enable holders of long leases to purchase the freehold of their homes. This legislation provided about one million leaseholders with the right to purchase the freehold of their homes. Controls were introduced over increases in the rents of council accommodation, a new Rent Act 1965 froze the rent for most unfurnished accommodation in the private sector while providing tenants with greater security of tenure and protection against harassment, and a system was introduced whereby independent arbitrators had the power to fix fair rents. In addition, the First Wilson government encouraged the introduction of discretionary local authority rent rebates to assist with housing costs, and also initiated the possibility of paying rates by instalments. In November 1969, legislation was passed by the government limiting rent rises following anti-eviction protests. 
Generous new subsides were introduced by the government to encourage authorities to construct many more houses and to build them to Parker Morris standards. In 1967, the government issued a circular which urged authorities to adopt and publicise rent rebate schemes. As a result of this circular, the number of authorities adopting such schemes rose from 40% before the circular to 53% by March 1968. About 70% of tenants were covered, though not necessarily in receipt of rebates:
"... 495 authorities operated rent rebate schemes, and the £9.5 million total rebate went to over a quarter of a million tenants, representing nearly 12 per cent of the total housing stock. The average rebate, 13s 9d, amounted to one third of the average rent."
Legislation was introduced which regulated tenancies for properties with a rateable value of up to £200 per year (£400 in London), which meant that tenants were not only to be protected from intimidation, but that evictions would now require court orders. It also restructured the housing subsidy system such that the borrowing charges of local authorities of individual local authorities would be pegged to 4% interest. The Rating Act 1966 introduced the rating of empty properties and provided for the payment of rates in instalments. The Local Government Act 1966 introduced a "domestic" element in the new Rate Support Grant, by providing relief to domestic ratepayers on a rising scale, so that as local expenditure rose, government grant was geared to outpace it. As noted by one historian,
"The amount of grant in the domestic element would be calculated as sufficient to subsidise domestic ratepayers to the extent of a fivepenny rate in the first year, tenpence in the second, and so on."
The Housing (Slum Clearance Compensation) Act 1965 continued a provision for home owners of unfit dwellings purchased between 1939 and 1955 to be compensated at market values. The Building Control Act 1966 introduced building licensing to give priority to housing construction. Under the Supplementary Benefit Act 1966, an owner occupier on benefits was entitled to an allowance for repairs, insurance, rates, and "reasonable" interest charges on a mortgage. A Land Commission was also established to purchase land for building and therefore prevent profiteering in land values, although it only had limited success. The aim of the Land Commission was to purchase land for public goods such as housing or shopping redevelopment (compulsorily, if the need arose), and investigated the planning needs of a particular area in conjunction with the Ministry of Housing and some planning authorities to see if any land in any particular area would be needed for such developmental schemes. Although the Land Commission purchased substantial quantities of land, it did not become the dominant influence in the land market that the government had hoped for.
The Housing Subsidies Act 1967 fixed interest rates at 4% for councils borrowing to build homes. It also provided financial assistance to local authorities for conversions and improvements, while also reforming the standard of fitness for human habitation. The 1967 Act increased subsides on new houses to such an extent that it became the largest individual source of subsidy after a previous Housing Subsidy Act 1946. For a period, as part of the prices and incomes standstill introduced by the government, local authorities were not permitted to raise rents. Thereafter, a limit was set on the extent of increases that were permitted.
The Town and Country Planning Act 1968 provided more local autonomy in town planning. This piece of legislation aimed for greater flexibility and speed in the planning of land use, and made public participation a statutory requirement in the preparation of development plans. The Act also introduced a new system of process planning under which the spatial distribution of social and economic trends superseded physical standards as the principal concern of planners. According to Maureen Rhoden, this effectively meant that the development control system operated by local authorities 'policed' new housing demand. This allowed for new development on infill sites or on the edge of larger towns and villages, "but preventing development in the open countryside and in designated areas such as green belts and Areas of Outstanding Natural Beauty." In addition, opportunities for public participation in the planning process were also increased by the Act, partly in response to opposition to some features of urban housing and planning policies. By September 1970, general improvement areas covering 23,254 dwellings had been declared, with work having been completed on 683 dwellings. In addition, the Labour government went further than previous administrations in aiming to safeguard the housing programme from wider economic problems.
Social Services and Welfare.
According to A.B. Atkinson, social security received much more attention from the first Wilson government than it did during the previous thirteen years of Conservative government. Following its victory in the 1964 general election, Wilson's government began to increase social benefits. Prescription charges for medicines were abolished immediately, while pensions were raised to a record 21% of average male industrial wages. In 1966, the system of National Assistance (a social assistance scheme for the poor) was overhauled and renamed Supplementary Benefit. The means test was replaced with a statement of income, and benefit rates for pensioners (the great majority of claimants) were increased, granting them a real gain in income. Before the 1966 election, the widow's pension was tripled and redundancy payments for laid-off workers were introduced. Due to austerity measures following an economic crisis, prescription charges were re-introduced in 1968 as an alternative to cutting the hospital building programme, although those sections of the population who were most in need (including supplementary benefit claimants, the long-term sick, children, and pensioners) were exempted from charges. The widow's earning rule was also abolished.
Altogether, the increases made in pensions and other benefits during Wilson's first year in office were the largest ever real term increases carried out up until that point. Social security benefits were markedly increased during Wilson's first two years in office, as characterised by a budget passed in the final quarter of 1964 which raised the standard benefit rates for old age, sickness and invalidity by 18.5%. In 1965, the government increased the national assistance rate to a higher level relative to earnings, and via annual adjustments, broadly maintained the rate at between 19% and 20% of gross industrial earnings until the start of 1970. In the five years from 1964 up until the last increases made by the First Wilson Government, pensions went up by 23% in real terms, supplementary benefits by 26% in real terms, and sickness and unemployment benefits by 153% in real terms (largely as a result of the introduction of earnings-related benefits in 1967).
Increased funds were allocated to social services during the first Wilson government's time in office. Between 1963 and 1968, spending on housing increased by 9.6%, social security by 6.6%, health by 6%, and education by 6.9%, while from 1964 to 1967 social spending increased by 45%. In terms of the social wage, by 1968, spending on health had gone up by 47%, education by 47%, public sector housing by 63%, and the social security budget by 58%. During the six years of the first Wilson government, spending on social services rose much faster than real personal incomes, and from 1964 to 1969, spending on social services rose from 14.6% to 17.6% of GNP, an increase of nearly 20%. Altogether, from 1964 to 1970, spending on the social services rose from 16% to 23% of national wealth between 1964 and 1970. As noted by the historian Richard Whiting, spending on social services under Wilson rose faster than the growth in GNP, by 65% (excluding housing) as against 37% for GNP, "a substantially better record than that achieved by the preceding Conservative governments."
In terms of social security, the welfare state was significantly expanded through substantial increases in national insurance benefits (which rose in real terms by 20% from 1964 to 1970) and the creation of new social welfare benefits. A variety of measures was introduced under Wilson which improved the living standards of many people with low incomes.
Short-term unemployment benefits were increased, while the National Assistance Board was merged with the Ministry of Pensions and National Insurance to become the new Department of Social Security, which replaced national assistance with supplementary benefit, improved benefit scale rates, and provided a statutory right to benefit for the out-of-work needy. Although people were kept above a new unofficial poverty line, however, many thousands lived only just above it.
The government also succeeded in persuading people to draw assistance to which they were entitled to but hadn't claimed before. The number of elderly Britons receiving home helps rose by over 15% from 1964 to 1969, while nearly three times as many meals on wheels were served in 1968 as in 1964. In 1968, the Ministry of Health and the Ministry of Social Security were amalgamated into the Department of Health and Social Security, the purpose of which was to coordinate benefits in cash with benefits in kind since "the services needed to deal with social insecurity are not cash benefits only, but health and welfare as well." An Act was passed which replaced National Assistance with Supplementary Benefits. The new Act laid down that people who satisfied its conditions were entitled to these noncontributory benefits. Unlike the National Assistance scheme, which operated like state charity for the worst-off, the new Supplementary Benefits scheme was a right of every citizen who found himself or herself in severe difficulties. Those persons over the retirement age with no means who were considered to be unable to live on the basic pension (which provided less than what the government deemed as necessary for subsistence) became entitled to a "long term" allowance of an extra few shillings a week. Some simplification of the procedure for claiming benefits was also introduced.
The new Supplementary Benefit scheme included a fixed basic weekly rate that those with an income below this level would now have a right to claim, while extra payments were made available on a discretionary basis for additional needs. In 1966, the period for which flat rate unemployment benefit was payable was extended to twelve months, while the earnings limit for pensioners was extended. A long term addition of 9 shillings (which was subsequently increased to 10 shillings) a week was provided for the allowances of all pensioners and for the long term sick, while the real value of most existing benefits was increased, (such as family allowances, which were substantially raised in 1967 and 1968) with benefits rising at roughly the same rate as salaries over the course of the first Wilson government, while family allowances were significantly increased. By 1969, family allowances were worth 72% more in real terms to a low income family with three children than in 1964. The single pension was raised by 12s 6d in March 1965, by 10s in 1967 and by a further 10s in 1969. From April 1964 to April 1970, family allowances for four children increased as a percentage of male manual workers aged 21 and above from 8% to 11.3%.
The First Wilson Government kept the old age pension rising roughly as fast as average earnings during its time in office, while campaigns were launched by the government to encourage people to take up means-tested benefits to which they were entitled to. For instance, a publicity campaign launched by the government increased the fraction of children eligible to get free school meals.
From 1966 to 1968, various reforms were carried out by the government to improve social security benefits, and which improved the real standards of insurance and assistance provision for low earners. The purchasing power of family allowances were improved, which as a percentage of the gross earnings of an average manual worker with five children rising from 9.5% to 17.% between 1966 and 1968, while improvements were made in additional payments for children via social insurance, with adults receiving a more modest improvement in flat rate payments. In addition, the real level of income guaranteed through assistance was enhanced, with rates increased in the autumn of 1966, 1967, and 1968. Taking into account averages for manual workers, and comparing the last quarter of 1968 with the first quarter of 1966, basic minimum benefits went up in real terms during that period by 4% for a family with 2 children, 18% for a family with 3 children, and 47% for a family with 5 children, while the total benefit went up by 50% for a family with 2 children, by 45% for a family with 3 children, and by 36% for a family with 5 children.
Under the Social Security Act 1966, newly unemployed individuals were no longer denied assistance during their first month of unemployment, while men who had had their Unemployment Benefit disallowed for six weeks (on the grounds that they had been at fault for losing their job) were no longer subjected to a harsh rule applied by the National Assistance confining their payments to below "benefit rate." Instead, a policy was adopted of paying these individuals their full entitlement less 15 shillings. The Act also introduced a long term addition of 9 shillings for all pensioners receiving supplementary benefit and for others (with the exception of those required register for employment) receiving supplementary benefits for two years. In 1967, the earnings limits for retirement pensioners were raised, while other changes were made in the administration of the earnings rule. From Autumn 1966 onwards, part of a widowed mother’s pension was not counted as income when the level of income was determined. In 1966-67 the Ministry of Social Security allowed elderly persons to receive supplementary pensions from the same book as retirement pensions, which led to a marked rise in the rate of applications for supplementary pensions.
Redundancy payments were introduced in 1965 to lessen the impact of unemployment, earnings-related benefits for maternity, unemployment, sickness, industrial injuries and widowhood were introduced in 1966, followed by the replacement of flat-rate family allowances with an earnings-related scheme in 1968. In 1968, the universal family allowance was raised for the first time in a decade. This measure was considered to be redistributive to some degree,
"from richer to poorer and from mainly male taxpayers to mothers who received family allowances, a tentative move towards what Roy Jenkins called 'civilised selectivity'".
The National Insurance Act 1966, which introduced supplementary earnings-related benefits for short-term sickness and unemployment, had far-reaching distributional consequences by "guaranteeing that insurance benefits rose at the same rate as wages in the late 1960s." Trade unions were supportive of the advances made in social protection by the Wilson government, which had a considerable impact on the living standards of the lowest quintile of the population. A statement by the TUC argued that the unions' acquiescence to the government's incomes policy was justified given that "the government had deliberately refrained from attacking the social services."
The introduction of earnings-related unemployment and sickness benefits significantly reduced inequalities between those in work and those who were unemployed. In 1964, the net income received by the average wage earner, when on unemployment or sickness benefit, was only 45% of what he received at work, whereas by 1968 the figure had increased to 75%. The earnings-related supplement for unemployment benefits was made available to those who had earned at least £450 in the previous financial year. The supplement was paid after a twelve-day waiting period, and the rate was one-third the amount by which the average weekly earnings (up to £30) exceeded £9. The earnings-related supplement was based on the assertion that a person's commitments for mortgages, rents, and hire purchase agreements were related to their normal earnings and could not be adjusted quickly when experiencing a loss of normal income. As a result of this supplement, the total benefit of a married man with two children went up by 52%, and that of a single man by 117.% The duration was limited to 26 weeks, while the total benefit was restricted to 85% of average weekly earnings in the preceding financial year. From 1965 to 1970, including Earnings Related Supplement, unemployment or sickness benefits as a percentage of net income at average earnings rose from 27% to 53.3% for a single person, 41.2% to 65.2% for a married couple, and from 49.3% to 72.7% for a married couple with two children.
As a result of the introduction of earnings related supplements to sickness and unemployment benefits and widows' allowances, the total benefit for a man earning £30 a week now represented 50% of his earnings rather than 27% with just the flat rate (for a married couple).
Personal social services were integrated, expenditure increased and their responsibilities broadened following the enactment of the Children and Young Persons' Act 1969 and the Local Authority Social Services Act 1970. The Children and Young Persons Act 1969 reformed the juvenile court system and extended local authority duties to provide community homes for juvenile offenders. The legislation provided that "remand homes," "approved schools," and local authority and voluntary children's homes became part of a comprehensive system of community homes for all children in care. This provided that children who got into trouble with the police should more certainly and quickly than ever before receive special educational assistance, social work help or any other form of assistance (financial or otherwise) that the community could provide. Under the Health Services and Public Health Act 1968, largely as a result of their insistence, local authorities were granted powers to "promote the welfare" of elderly people in order to allow them greater flexibility in the provision of services. Health and welfare services for the elderly were improved, with about 15,000 new places provided in homes for the elderly between 1965 and 1968. From 1964 to 1966, the number of home helps rose from 28,237 to 30,244. Efforts were also made to improve provisions for mentally handicapped adults and children. From 1965 up until the end of 1966, the number of places available in adult training centres rose from 15,000 to 19,000, while for mentally handicapped children there were over 20,000 places in junior training centres by 1966, compared with less than 5,000 in 1960. In addition, spending of local authorities on the mentally ill doubled from £10 million in 1963/64 to £20 million in 1967/68.
Agriculture.
Under the First Wilson Government, subsidies for farmers were increased. Farmers who wished to leave the land or retire became eligible for grants or annuities if their holdings were sold for approved amalgamations, and could receive those benefits whether they wished to remain in their farmhouses or not. A Small Farmers Scheme was also extended, and from the 1st of December 1965 40,000 more farmers became eligible for the maximum £1,000 grant. New grants to agriculture also encouraged the voluntary pooling of smallholdings, and in cases where their land was purchased for non-commercial purposes, tenant-farmers could now receive double the previous “disturbance compensation.” A Hill Land Improvement Scheme, introduced by the Agriculture Act of 1967, provided 50% grants for a wide range of land improvements, along with a supplementary 10% grant on drainage works benefitting hill land. The Agriculture Act 1967 also provided grants to promote farm amalgamation and to compensate outgoers.
Health.
The proportion of GNP spent on the NHS rose from 4.2% in 1964 to about 5% in 1969. This additional expenditure provided for an energetic revival of a policy of building health centres for GPs, extra pay for doctors who served in areas particularly short of them, a significant growth in hospital staffing, and a significant increase in a hospital building programme. Far more money was spent each year on the NHS than under the 1951–64 Conservative governments, while much more effort was put into modernising and reorganising the health service. Stronger central and regional organisations were established for bulk purchase of hospital supplies, while some efforts were made to reduce inequalities in standards of care. In addition, the government increased the intake to medical schools.
The 1966 Doctor's Charter introduced allowances for rent and ancillary staff, significantly increased the pay scales, and changed the structure of payments to reflect "both qualifications of doctors and the form of their practices, i.e. group practice." These changes not only led to higher morale, but also resulted in the increased use of ancillary staff and nursing attachments, a growth in the number of health centres and group practices, and a boost in the modernisation of practices in terms of equipment, appointment systems, and buildings. The charter introduced a new system of payment for GPs, with refunds for surgery, rents, ands rates, to ensure that the costs of improving his surgery did not diminish the doctor's income, together with allowances for the greater part of ancillary staff costs. In addition, a Royal Commission on medical education was set up, partly to draw up ideas for training GPS (since these doctors, the largest group of all doctors in the country, had previously not received any special training, "merely being those who, at the end of their pre-doctoral courses, did not go on for further training in any speciality).
In 1967, local authorities were empowered to provide family planning advice to any who requested it and to provide supplies free of charge. In addition, medical training was expanded following the Todd Report on medical education in 1968. In addition, National Health expenditure rose from 4.2% of GNP in 1964 to 5% in 1969 and spending on hospital construction doubled. The Health Services and Public Health Act 1968 empowered local authorities to maintain workshops for the elderly either directly or via the agency of a voluntary body. A Health Advisory Service was later established to investigate and confront the problems of long-term psychiatric and mentally subnormal hospitals in the wave of numerous scandals. The Family Planning Act 1967 empowered local authorities to set up a family planning service with free advice and means-tested provision of contraceptive devices while the Clean Air Act 1968 extended powers to combat air pollution. More money was also allocated to hospitals treating the mentally ill. In addition, a Sports Council was set up to improve facilities. Direct government expenditure on sports more than doubled from £0.9 million in 1964/65 to £2 million in 1967/68, while 11 regional Sports Councils had been set up by 1968. In Wales, five new health centres had been opened by 1968, whereas none had been opened from 1951 to 1964, while spending on health and welfare services in the region went up from £55.8 million in 1963/64 to £83.9 million in 1967/68. 
Workers.
The Industrial Training Act 1964 set up an Industrial Training Board to encourage training for people in work, and within 7 years there were “27 ITBs covering employers with some 15 million workers.” From 1964 to 1968, the number of training places had doubled. The Docks and Harbours Act (1966) and the Dock Labour Scheme (1967) reorganised the system of employment in the docks in order to put an end to casual employment. The changes made to the Dock Labour Scheme in 1967 ensured a complete end to casual labour on the docks, effectively giving workers the security of jobs for life. Trade unions also benefited from the passage of the Trade Dispute Act 1965. This restored the legal immunity of trade union officials, thus ensuring that they could no longer be sued for threatening to strike. The First Wilson Government also encouraged married women to return to teaching and improved Assistance Board Concessionary conditions for those teaching part-time, “by enabling them to qualify for pension rights and by formulating a uniform scale of payment throughout the country." Soon after coming into office, midwives and nurses were given an 11% pay increase, and according to one MP, nurses also benefited from the largest pay rise they had received in a generation. In May 1966, Wilson announced 30% pay rises for doctors and dentists - a move which did not prove popular with unions, as the national pay policy at the time was for rises of between 3% and 3.5%.
Much needed improvements were made in junior hospital doctors' salaries. From 1959 to 1970, while the earnings of manual workers increased by 75%, the salaries of registrars more than doubled while those of house officers more than trebled. Most of these improvements, such as for nurses, came in the pay settlements of 1970. On a limited scale, reports by the National Board for Prices and Incomes encouraged incentive payments schemes to be development in local government and elsewhere. In February 1969, the government accepted an "above the ceiling" increase for farmworkers, a low-paid group. Some groups of professional workers, such as nurses, teachers, and doctors, gained substantial awards.
Improvements were also made in conditions for nursing staff following the publication of a report by the NBPI in 1968 on the pay of nurses. This led to the introduction of a far more substantial pay lead for nurses in geriatric and psychiatric hospitals, together with (for the first time) premium rates for weekend and night work. Some progress was also made increasing the pay of NHS manual workers through incentive schemes. Despite these improvements, however, the NHS retained a reputation of being a low-wage employer by the end of the first Wilson government's time in office.
The National Insurance Act 1966 introduced more generous provisions for the assessment of certain types of serious disablement caused by industrial injury. That same year, a Pneumoconiosis, Byssinosis and Miscellaneous Diseases Benefit Scheme was introduced. The Employers' Liability (Compulsory Insurance) Act 1969 was passed, requiring employers to insure their liability to their employees for personal injury, disease or death sustained in their place of work. The Asbestos Regulations of 1969 sought to protect people in the workplace from exposure to asbestos, while the Employer's Liability (Defective Equipment) Act 1969 introduced that same year made employers liable for injuries caused to employees by defective equipment. In addition, the Agriculture Act of 1967 gave the Agricultural Wages Board the power to fix minimum rates of sick pay for farmworkers. 
Wilson's government also ensured that low-income earners improved their position relative to that of average-income earners during its time in office. One of the principles of the government's prices and incomes policy was that low-paid workers would be given special consideration, and between 1965 and 1969 the earnings of the lowest paid workers increased slightly faster than the average (the increase in inflation in 1969–70 caused by devaluation, however, led to a deterioration in the position of low-paid workers). The Prices and Incomes Board was successful in directing some "above the norm" pay rises to low-paid groups such as local government employees and agricultural workers. However, the large increases in pay given to manual workers in local government in September 1969 (such as street sweepers and dustmen) subsequently set off a spiral of wage demands in industry, which meant that the improvement in the relative position of the local government manual worker was not sustained.
In 1966, extensions and improvements were made in the allowances payable out of the Industrial Injuries Fund to people who had been injured before 5 July 1948 and who were entitled to weekly payments of worker's compensation. In 1968, various steps were taken to reduce the severity with which the "wage-stop" operated, a regressive mechanism which restricted the amount of assistance payable to an unemployed person. For miners, the Coal Industry Act 1965 introduced aid towards severance payments for miners about to be made redundant or for the vocational retraining of staff, while the Coal Industry Act 1967 provided subsidisation of redundancy and early retirement. Under the National Insurance (Industrial Injuries) (Amendment) Act 1967, men who were diagnosed as having over 50% disablement through pneumoconiosis “were allowed to have their accompanying bronchitis and emphysema treated as part of the disease,” although only 3,000 men “fell into this category.”
Transport.
The Travel Concessions Act of 1964, one of the first Acts passed by the First Wilson Government, provided concessions to all pensioners travelling on buses operated by municipal transport authorities. The Transport Act 1968 established the principle of government grants for transport authorities if uneconomic passenger services were justified on social grounds. A National Freight Corporation was also established to provide integrated rail freight and road services. Public expenditure on roads steadily increased and stricter safety precautions were introduced, such as the breathalyser test for drunken driving, under the 1967 Road Traffic Act. The Transport Act gave a much needed financial boost to British Rail, treating them like they were a company which had become bankrupt but could now, under new management, carry on debt-free. The act also established a national freight corporation and introduced government subsidy for passenger transport on the same basis as existing subsidies for roads to enable local authorities to improve public transport in their areas. The road building programme was also expanded, with capital expenditure increased to 8% of GDP, "the highest level achieved by any post-war government". Central government expenditure on roads went up from £125 million in 1963/64 to £225 million in 1967/68, while a number of road safety regulations were introduced, covering seat belts, lorry drivers’ hours, car and lorry standards, and an experimental 70 mile per hour speed limit. In Scotland, spending on trunk roads went up from £6.8 million in 1963/64 to £15.5 million in 1966/67, while in Wales, spending on Welsh roads went up from £21.2 million in 1963/64 to £31.4 million in 1966/67.
Regional development.
Encouragement of regional development was given increased attention under the first Wilson government, with the aim of narrowing economic dispratiies between the various regions. A policy was introduced in 1965 whereby any new government organisation should be established outside London and in 1967 the government decided to give preference to development areas. A few government departments were also moved out of London, with the Royal Mint moved to South Wales, the Giro and Inland Revenue to Bootle, and the Motor Tax Office to Swansea. A new Special Development Status was also introduced in 1967 to provide even higher levels of assistance. In 1966, five development areas (covering half the population in the UK) were established, while subsidies were provided for employers recruiting new employees in the Development Areas. A Highlands and Islands Development Board was also set up to “re-invigorate” the north of Scotland.
The Industrial Development Act 1966 changed the name of Development Districts (parts of the country with higher levels of unemployment than the national average and which governments sought to encourage greater investment in) to Development Areas and increased the percentage of the workforce covered by development schemes from 15% to 20%, which mainly affected rural areas in Scotland and Wales. Tax allowances were replaced by grants in order to extend coverage to include firms which were not making a profit, and in 1967 a Regional Employment Premium was introduced. Whereas the existing schemes tended to favour capital-intensive projects, this aimed for the first time at increasing employment in depressed areas. Set at £1.50 a man per week and guaranteed for seven years, the Regional Employment Premium subsidised all manufacturing industry (though not services) in Development Areas.
Regional unemployment differentials were narrowed, and spending on regional infrastructure was significantly increased. Between 1965–66 and 1969–70, yearly expenditure on new construction (including power stations, roads, schools, hospitals and housing) rose by 41% in the United Kingdom as a whole. Subsidies were also provided for various industries (such as shipbuilding in Clydeside), which helped to prevent a number of job losses. It is estimated that, between 1964 and 1970, 45,000 government jobs were created outside London, 21,000 of which were located in the Development Areas. The Local Employment Act, passed in March 1970, embodied the government's proposals for assistance to 54 "intermediate" employment exchange areas not classified as full "development" areas.
Funds allocated to regional assistance more than doubled, from £40 million in 1964/65 to £82 million in 1969/70, and from 1964 to 1970, the number of factories completed was 50% higher than from 1960 to 1964, which helped to reduce unemployment in development areas. In 1970, the unemployment rate in development areas was 1.67 times the national average, compared to 2.21 times in 1964. Although national rates of unemployment were higher in 1970 than in the early 1960s, unemployment rates in the development areas were lower and had not increased for three years. Altogether, the impact of the first Wilson government's regional development policies was such that, according to one historian, the period 1963 to 1970 represented "the most prolonged, most intensive, and most successful attack ever launched on regional problems in Britain."
Urban renewal.
A number of subsidies were allocated to local authorities faced with acute areas of severe poverty (or other social problems). The Housing Act 1969 provided local authorities with the duty of working out what to do about 'unsatisfactory areas'. Local authorities could declare 'general improvement areas' in which they would be able to buy up land and houses, and spend environmental improvement grants. On the same basis, taking geographical areas of need, a package was developed by the government which resembled a miniature poverty programme. In July 1967, the government decided to pour money into what the Plowden Committee defined as Educational Priority Areas, poverty-stricken areas where children were environmentally deprived. A number of poor inner-city areas were subsequently granted EPA status (despite concerns that Local Education Authorities would be unable to finance Educational Priority Areas). From 1968 to 1970, 150 new schools were built under the educational priority programme.
Section 11 of the Local Government Act 1966 enabled local authorities to claim grants to recruit additional staff to meet special needs of Commonwealth immigrants. According to Brian Lapping, this was the first step ever taken towards directing help to areas with special needs, "the reversal of the former position under which ministers had passed the burden of social help measures in housing, education and health to local authorities without passing them any money."
In 1967, Wilson's government decided to spend £16 million, mainly in "Educational Priority Areas", over the next two years. Over a two-year period, £16 million was allocated by the government for construction of schools in EPAs, while teachers in 572 primary schools "of exceptional difficulty" were selected for additional increments. After negotiations with teachers' unions, £400,000 of this money was set aide to pay teachers an additional £75 per annum for working in "schools of exceptional difficulty", of which 570 schools were designated. In April 1966, the government authorised school building projects in 57 authorities in England and Wales. It also sponsored an action research project, an experiment in five of the EPAs to try to devise the most effective ways of involving communities, according to Brian Lapping,
"in the work of their schools, compensating the children for the deprivation of their background, seeing whether, in one area pre-school play groups, in another intensive language tuition, in another emphasis on home-school relations, would be most effective."
The first Wilson government made assistance to deprived urban communities a specific policy of national government in 1969 with the passage of the Local Government Grants (Social Need) Act, which empowered the Home Secretary to dispense grants to assist local authorities in providing extra help to areas "of special social need." The Urban Aid Programme was subsequently launched to provide community and family advice centres, centres for the elderly, money for schools and other services, thereby alleviating urban deprivation. In introducing the Urban Aid Programme, the then Home Secretary James Callaghan stated that the goal of the legislation was to
"provide for the care of our citizens who live in the poorest overcrowded parts of our cities and towns. It is intended to arrest ... and reverse the downward spiral which afflicts so many of these areas. There is a deadly quagmire of need and poverty."
Under the Urban Aid Programme, funds were provided for centres for unattached youngsters, family advice centres, community centres, centres for the elderly, and in one case for an experimental scheme for rehabilitating methylated spirit drinkers. Central government paid 75% of the costs of these schemes, which were nominated by local authorities in areas of 'acute social need'. As a result of this legislation, many ideas were put into practice such as language classes for immigrants, daycentres for the elderly or disabled, day nurseries, adventure playgrounds, and holidays for deprived or handicapped children. The schemes therefore proved successful in making extra social provision while encouraging community development. In January 1969, 23 local authorities were awarded a total of £3 million mainly for nursery education but also for children’s homes and day nurseries. The second phase, in July 1969, agreed to finance some 500 projects in 89 authorities to a total of £4.5 million, and while the emphasis again stressed education with teachers; centres, nursery schools and language classes for immigrants, aid was also given through the local authorities to voluntary societies to run adventure playgrounds, play centres, and play groups.
Twelve Community Development Projects (CDPs) were set up in areas with high levels of deprivation to encourage self-help and participation by local residents in order to improve their communication and access to local government, together with improving the provision of local services. In the years that followed, these action-research projects increasingly challenged existing ideas about the causes of inner-city deprivation, arguing that the roots of poverty in such areas could be traced to changes in the political economy of inner-city areas, such as the withdrawal of private capital (as characterised by the decline of manufacturing industries).
The Community Development Projects involved co-operation between specially created local teams of social workers, who were supported by part-timers (such as policemen and youth employment officers). The task given to these groups (who were watched over by their own action research teams) was to ascertain how much real demand t here was for support from the social services in their areas of choice, based on the theory that workers in social services usually failed to communicate what they had to offer or to make themselves available, thereby resulting in many deprived people failing to acquire the services that they so desperately needed.
As noted by Brian Lapping, the Community Development Projects were also designed to test the view that within poor communities local residents could articulate local grievances, get conditions in their areas improved, and provide some kind of political leadership, in a way that the existing political structure had failed to do, "largely because these areas of intense poverty were rarely big enough to be electorally important." In assessing the first Wilson government's efforts to uplift the poorest members of British society via the establishment of the Community Development Projects and the designation of the Educational Priority Areas, Brian Lapping noted that
"The determination expressed in the diverse policies to give this unfortunate group the help it needed was among the most humane and important initiatives of the 1964–70 government."
International development.
A new Ministry of Overseas Development was established, with its greatest success at the time being the introduction of interest-free loans for the poorest countries. The Minister of Overseas Development, Barbara Castle, set a standard in interest relief on loans to developing nations which resulted in changes to the loan policies of many donor countries, "a significant shift in the conduct of rich white nations to poor brown ones." Loans were introduced to developing countries on terms that were more favourable to them than those given by governments of all other developed countries at that time. In addition, Castle was instrumental in setting up an Institute of Development Studies at the University of Sussex to devise ways of tackling global socio-economic inequalities. Overseas aid, however, bore a major brunt of the austerity measures introduced by the first Wilson government in its last few years in office, with British aid as a percentage of GNP falling from 0.53% in 1964 to 0.39% in 1969.
Taxation.
Various changes were also made to the tax system which benefited workers on low and middle incomes. Married couples with low incomes benefited from the increases in the single personal allowance and marriage allowance. In 1965, the regressive allowance for national insurance contributions was abolished and the single personal allowance, marriage allowance and wife's earned income relief were increased. These allowances were further increased in the tax years 1969–70 and 1970–71. Increases in the age exemption and dependant relative's income limits benefited the low-income elderly. In 1967, new tax concessions were introduced for widows.
Increases were made in some of the minor allowances in the 1969 Finance Act, notably the additional personal allowance, the age exemption and age relief and the dependent relative limit. Apart from the age relief, further adjustments in these concessions were implemented in 1970.
1968 saw the introduction of aggregation of the investment income of unmarried minors with the income of their parents. According to Michael Meacher, this change put an end to a previous inequity whereby two families, in otherwise identical circumstances, paid differing amounts of tax "simply because in one case the child possessed property transferred to it by a grandparent, while in the other case the grandparent's identical property was inherited by the parent."
In the 1969 budget, income tax was abolished for about 1 million of the lowest paid and reduced for a further 600,000 people, while in the government's last budget (introduced in 1970), two million small taxpayers were exempted from paying any income tax altogether.
Liberal reforms.
A wide range of liberal measures were introduced during Wilson's time in office. The Matrimonial Proceedings and Property Act 1970 made provision for the welfare of children whose parents were about to divorce or be judicially separated, with courts (for instance) granted wide powers to order financial provision for children in the form of maintenance payments made by either parent. This legislation allowed courts to order provision for either spouse and recognised the contribution to the joint home made during marriage. That same year, spouses were given an equal share of household assets following divorce via the Matrimonial Property Act. The Race Relations Act 1968 was also extended in 1968 and in 1970 the Equal Pay Act 1970 was passed. Another important reform, the 1967 Welsh Language Act, granted 'equal validity' to the declining Welsh language and encouraged its revival. Government expenditure was also increased on both sport and the arts. The Mines and Quarries (Tips) Act 1969, passed in response to the Aberfan tragedy, made provision for preventing disused tips from endangering members of the public. In 1967, corporal punishment in borstals and prisons was abolished. 7 regional associations were established to develop the arts, and government expenditure on cultural activities rose from £7.7 million in 1964/64 to £15.3 million in 1968/69. A Criminal Injuries Compensation Board was also set up, which had paid out over £2 million to victims of criminal violence by 1968.
The Commons Registration Act 1965 provided for the registration of all common land and village greens, whilst under the Countryside Act 1968, local authorities could provide facilities "for enjoyment of such lands to which the public has access". The Family Provision Act 1966 amended a series of pre-existing estate laws mainly related to persons who died interstate. The legislation increased the amount that could be paid of surviving spouses if a will hadn't been left, and also expanded upon the jurisdiction of county courts, which were given the jurisdiction of high courts under certain circumstances when handling matters of estate. The rights of adopted children were also improved with certain wording changed in the Inheritance (Family Provision) Act 1938 to bestow upon them the same rights as natural-born children. In 1968, the Nurseries and Child-Minders Regulation Act 1948 was updated to include more categories of childminders. A year later, the Family Law Reform Act 1969 was passed, which allowed people born outside marriage to inherit on the intestacy of either parent. In 1967, homosexuality was decriminalised by the passage of the Sexual Offences Act. The first Wilson government also introduced a thirty-year rule for access to public records, replacing a previous fifty-year rule.
The Race Relations Act 1965 outlawed direct discrimination on the grounds of race, colour, and ethnic or national origin in some public places. The legislation also set up a Race Relations Board. A centrally financed network of local officers was provided to smooth inter-racial relations by conciliation, education, and informal pressure, while a National Committee for Commonwealth Immigrants was established (under the chairmanship of the Archbishop of Canterbury) to encourage and help finance staff "for local voluntary, good-neighbour type bodies." A further Race Relations Act 1968 was passed, which made discrimination in letting or advertising housing illegal, together with discrimination in hiring and promotion. The legislation also provided a strengthened Race Relations Board with powers to "conciliate" in cases of discrimination, which meant persuading discriminators to stop such acts and, if they refused to stop, legal action could be taken against them as an ultimate sanction. The legislation also replaced the National Committee for Commonwealth Immigrants with the Community Relations Commission, a statutory body. This body was provided with an annual grant (beginning at £300,000) for social work, propaganda, and education as a means of bringing about good race relations. The Criminal Justice Act 1967 introduced suspended prison sentences and allowed a ten to two majority vote for jury decisions. An Ombudsman (Parliamentary Commissioner) was appointed in 1967 to consider complaints against government departments and to impose remedies, while censorship of plays by the Lord Chamberlain was abolished (1969). In addition, the law on Sunday Observance was relaxed, and a strengthening of legal aid was carried out.
The Nuclear Installations Act 1965 placed a "strict" statutory duty on the operators of nuclear facilities to ensure that any exposure to radiation resulting from operations did not cause injury or damage. Under the legislation, claimants did not have to prove fault to receive compensation under the Act, only causation. From 1966, a circular from several Whitehall ministries was sent to local authorities across the country urging them to provide permanent caravan sites for gypsies. This was followed by the Caravan Sites Act, introduced by the Liberal MP Eric Lubbock in 1968, which obliged local authorities to carry out the recommendations of the 1966 circular. Under the Act, gypsies became entitled to settle in many areas as well as to enjoy regular visiting rights for their caravans in others. The Civic Amenities Act 1967 was aimed at improving and safeguarding buildings of architectural or historical interest, together with the planting and preservation of trees. The Administration of Justice Act 1970 introduced (amongst other measures) a new Family Division of the High Court.
A number of private members' bills related to consumer affairs, put forward by Co-operative MPs, became law under the first Wilson government, and much of the consumer legislation taken for granted by contemporary British shoppers can be attributed to the legislation passed during this period. In 1968, the Trade Descriptions Act 1968 (the "shoppers' charter") was enacted by parliament, and a farm and garden chemicals bill also became law that same year. Other co-operative bills enacted during this period included a new Clean Air Act, a bill removing restrictions on off-licences, and a bill to promote agriculture co-operatives passed in 1967, which established "A scheme administered by a new Central Council for Agriculture and Horticulture Co-operation with a budget to organise and promote co-operation with agriculture and horticulture". The 1970 Chronically Sick & Disabled Persons Act, regarded as a groundbreaking measure, was the first kind of legislation in the world to recognise and give rights to disabled people, and set down specific provisions to improve access and support for people with disabilities. The government effectively supported the passage of these bills by granting them the necessary parliamentary time.
Record of first term on income distribution.
Despite the economic difficulties faced by the first Wilson government, it succeeded in maintaining low levels of unemployment and inflation during its time in office. Unemployment was kept below 2.7%, and inflation for much of the 1960s remained below 4%. Living standards generally improved, while public spending on housing, social security, transport, research, education and health went up by an average of more than 6% between 1964 and 1970. The average household grew steadily richer, with the number of cars in the United Kingdom rising from one to every 6.4 persons to one for every five persons in 1968, representing a net increase of three million cars on the road. The rise in the standard of living was also characterised by increased ownership of various consumer durables from 1964 to 1969, as demonstrated by television sets (from 88% to 90%), refrigerators (from 39% to 59%), and washing machines (from 54% to 64%).
By 1970, income in Britain was more equally distributed than in 1964, mainly because of increases in cash benefits, including family allowances.
According to one historian,
"In its commitment to social services and public welfare, the Wilson government put together a record unmatched by any subsequent administration, and the mid-sixties are justifiably seen as the 'golden age' of the welfare state".
As noted by Ben Pimlott, the gap between those on lowest incomes and the rest of the population "had been significantly reduced" under Wilson's first government. The first Wilson government thus saw the distribution of income became more equal, while reductions in poverty took place. These achievements were mainly brought about by several increases in social welfare benefits, such as supplementary benefit, pensions and family allowances, the latter of which were doubled between 1964 and 1970 (although most of the increase in family allowances did not come about until 1968). A new system of rate rebates was introduced, which benefited one million households by the end of the 1960s. Increases in national insurance benefits in 1965, 1967, 1968 and 1969 ensured that those dependant on state benefits saw their disposable incomes rise faster than manual wage earners, while income differentials between lower income and higher income workers were marginally narrowed. Greater progressivity was introduced in the tax system, with greater emphasis on direct (income-based) as opposed to indirect (typically expenditure-based) taxation as a means of raising revenue, with the amount raised by the former increasing twice as much as that of the latter. Also, in spite of an increase in unemployment, the poor improved their share of the national income while that of the rich was slightly reduced. Despite various cutbacks after 1966, expenditure on services such as education and health was still much higher as a proportion of national wealth than in 1964. In addition, by raising taxes to pay their reforms, the government paid careful attention to the principle of redistribution, with disposable incomes rising for the lowest paid while falling amongst the wealthiest during its time in office.
Between 1964 and 1968, benefits in kind were significantly progressive, in that over the period those in the lower half of the income scale benefited more than those in the upper half. On average those receiving state benefits benefited more in terms of increases in real disposable income than the average manual worker or salaried employee between 1964 and 1969. From 1964 to 1969, low-wage earners did substantially better than other sections of the population. In 1969, a married couple with two children were 11.5% per cent richer in real terms, while for a couple with three children, the corresponding increase was 14.5%, and for a family with four children, 16.5%. From 1965 to 1968, the income of single pensioner households as a percentage of other one adult households rose from 48.9% to 52.5%. For two pensioner households, the equivalent increase was from 46.8% to 48.2%. In addition, mainly as a result of big increases in cash benefits, unemployed persons and large families gained more in terms of real disposable income than the rest of the population during Wilson's time in office.
As noted by Paul Whiteley, pensions, sickness, unemployment, and supplementary benefits went up more in real terms under the First Wilson Government than under the preceding Conservative administration:
“To compare the Conservative period of office with the Labour period, we can use the changes in benefits per year as a rough estimate of comparative performance. For the Conservatives and Labour respectively increases in supplementary benefits per year were 3.5 and 5.2 percentage points, for sickness and unemployment benefits 5.8 and 30.6 percentage points, for pensions 3.8 and 4.6, and for family allowances -1.2 and -2.6. Thus the poor, the retired, the sick and the unemployed did better in real terms under Labour than they did under Conservatives, and families did worse.”
Between 1964 and 1968, cash benefits rose as a percentage of income for all households but more so for poorer than for wealthier households. As noted by the economist Michael Stewart,
"it seems indisputable that the high priority the Labour Government gave to expenditure on education and the health service had a favourable effect on income distribution."
For a family with two children in the income range £676 to £816 per annum, cash benefits rose from 4% of income in 1964 to 22% in 1968, compared with a change from 1% to 2% for a similar family in the income range £2,122 to £2,566 over the same period. For benefits in kind the changes over the same period for similar families were from 21% to 29% for lower income families and from 9% to 10% for higher income families. When taking into account all benefits, taxes and Government expenditures on social services, the first Wilson government succeeded in bringing about a reduction in income inequality. As noted by the historian Kenneth O. Morgan,
"In the long term, therefore, fortified by increases in supplementary and other benefits under the Crossman regime in 1968–70, the welfare state had made some impact, almost by inadvertence, on social inequality and the maldistribution of real income".
Public expenditure as a percentage of GDP rose significantly under the 1964–1970 Labour government, from 34% in 1964–65 to nearly 38% of GDP by 1969–70, whilst expenditure on social services rose from 16% of national income in 1964 to 23% by 1970. These measures had a major impact on the living standards of low-income Britons, with disposable incomes rising faster for low-income groups than for high-income groups during the course of the 1960s. When measuring disposable income after taxation but including benefits, the total disposable income of those on the highest incomes fell by 33%, whilst the total disposable income of those on the lowest incomes rose by 104%. As noted by one historian,
"the net effect of Labour's financial policies was indeed to make the rich poorer and the poor richer".
External affairs.
Europe.
Among the more challenging political dilemmas Wilson faced during his two terms in government and his two spells in Opposition before 1964 and between 1970 and 1974 was the issue of British membership of the European Community, the forerunner of the present European Union. An entry attempt had been issued in July 1961 by the Macmillan government, and negotiated by Edward Heath as Lord Privy Seal, but was vetoed in 1963 by French President Charles de Gaulle. The Labour Party in Opposition had been divided on the issue, with former party leader Hugh Gaitskell having come out in 1962 in opposition to Britain joining the Community.
After initially hesitating over the issue, Wilson's Government in May 1967 lodged the UK's second application to join the European Community. Like the first, though, it was vetoed by de Gaulle in November that year.
Following his victory in the 1970 election (and helped by de Gaulle's fall from power in 1969), the new prime minister Edward Heath negotiated Britain's admission to the EC, alongside Denmark and Ireland in 1973. The Labour Party in opposition continued to be deeply divided on the issue, and risked a major split. Leading opponents of membership included Richard Crossman, who was for two years (1970–72) the editor of "New Statesman", at that time the leading left-of-centre weekly journal, which published many polemics in support of the anti-EC case. Prominent among Labour supporters of membership was Roy Jenkins.
Wilson in opposition showed political ingenuity in devising a position that both sides of the party could agree on, opposing the terms negotiated by Heath but not membership in principle. Labour's 1974 manifesto included a pledge to renegotiate terms for Britain's membership and then hold a referendum on whether to stay in the EC on the new terms. This was a constitutional procedure without precedent in British history.
Following Wilson's return to power, the renegotiations with Britain's fellow EC members were carried out by Wilson himself in tandem with Foreign Secretary James Callaghan, and they toured the capital cities of Europe meeting their European counterparts (some commentators have suggested that their co-operation in this exercise may have been the source of a close relationship between the two men which is claimed to have assisted a smooth change-over when Wilson retired from office). The discussions focused primarily on Britain's net budgetary contribution to the EC. As a small agricultural producer heavily dependent on imports, Britain suffered doubly from the dominance of:
During the renegotiations, other EEC members conceded, as a partial offset, the establishment of a significant European Regional Development Fund (ERDF), from which it was clearly agreed that Britain would be a major net beneficiary.
In the subsequent referendum campaign, rather than the normal British tradition of "collective responsibility", under which the government takes a policy position which all cabinet members are required to support publicly, members of the Government were free to present their views on either side of the question. The electorate voted on 5 June 1975 to continue membership, by a substantial majority.
Asia.
Prior United States military involvement in Vietnam intensified following the Gulf of Tonkin Resolution in 1964. US President Lyndon Johnson brought pressure to bear for at least a token involvement of British military units in the Vietnam War. Wilson consistently avoided any commitment of British forces, giving as reasons British military commitments to the Malayan Emergency and British co-chairmanship of the 1954 Geneva Conference which agreed the cessation of hostilities and called for internationally supervised elections in Vietnam. His government offered some rhetorical support for the US position (most prominently in the defence offered by the Foreign Secretary Michael Stewart in a much-publicised "teach-in" or debate on Vietnam). On at least one occasion the British government made an unsuccessful effort to mediate in the conflict, with Wilson discussing peace proposals with Alexei Kosygin, the Chairman of the USSR Council of Ministers. On 28 June 1966 Wilson 'dissociated' his Government from American bombing of the cities of Hanoi and Haiphong. In his memoirs, Wilson writes of "selling LBJ a bum steer", a reference to Johnson's Texas origins, which conjured up images of cattle and cowboys in British minds. Wilson's approach of maintaining close relations with the US while pursuing an independent line on Vietnam has attracted new interest in the light of the different approach taken by the Blair government vis-a-vis Britain's participation in the Iraq War (2003).
Since the Second World War, Britain's presence in the Far East had gradually been run down. Former British colonies, whose defence had provided much of the rationale for a British military presence in the region, moved towards independence under British governments of both parties. Successive UK Governments also became conscious of the cost to the exchequer and the economy of maintaining major forces abroad (in parallel, several schemes to develop strategic weaponry were abandoned on the grounds of cost, for example, the Blue Streak missile and the TSR2 aircraft).
Part of the price paid by Wilson after talks with President Johnson in June 1967 for US assistance with the UK economy was his agreement to maintain a military presence East of Suez. In July 1967 Defence Secretary Denis Healey announced that Britain would abandon her mainland bases East of Suez by 1977, although airmobile forces would be retained which could if necessary be deployed in the region. Shortly afterward, in January 1968, Wilson announced that the proposed timetable for this withdrawal was to be accelerated, and that British forces were to be withdrawn from Singapore, Malaysia, and the Persian Gulf by the end of 1971. However, Wilson's successor Edward Heath sought to reverse this policy, and British forces remained in Singapore and Malaysia until the mid-1970s. Whilst widely criticised at the time, over the longer term the decision can be seen as a logical culmination of the withdrawal from Britain's colonial-era political and military commitments in Asia, the Middle East, Africa and elsewhere that had been underway under British governments of both parties since the Second World War – and of the parallel switch of Britain's emphasis to its European identity.
Wilson was known for his strong pro-Israel views. He was a particular friend of Israeli Premier Golda Meir, though her tenure largely coincided with Wilson's 1970–1974 hiatus. Another associate was West German Chancellor Willy Brandt; all three were members of the Socialist International.
Africa.
In 1960, Prime Minister Harold Macmillan made his important Wind of Change speech to the Parliament of South Africa in Cape Town. This heralded independence for many British colonies in Africa. The British "retreat from Empire" had made headway by 1964 and was to continue during Wilson's administration. The Federation of Rhodesia and Nyasaland came to present serious problems.
The Federation was set up in 1953, and was an amalgamation of the British Colonies of Northern Rhodesia and Nyasaland and the self-governing colony of Southern Rhodesia. The Federation was dissolved in 1963 and the states of Zambia and Malawi were granted independence. Southern Rhodesia, which had been the economic powerhouse of the Federation, was not granted independence, principally because of the régime in power. The country bordered South Africa to the south and its governance was influenced by the apartheid régime, then headed by Hendrik Verwoerd. Wilson refused to grant independence to the white minority government headed by Rhodesian Prime Minister Ian Smith which not willing to extend unqualified voting rights to the native African population. His government's view was that the native population was ingenuous, and doing that would lay them open to undue influence and intimidation. The franchise was open to those who had achieved a certain (fairly low) standard of education, and to property owners, and to people of "importance", i.e., chiefs and indunas – in other words, you needed to qualify for a vote – which many natives did.
Smith's defiant response was a Unilateral Declaration of Independence, timed to coincide with Armistice Day at 11.00 am on 11 November 1965, an attempt to garner support in the UK by reminding people of the contribution of the colony to the war effort (Smith himself had been a Spitfire pilot). Smith was personally vilified in the British media. Wilson's immediate recourse was to the United Nations, and in 1965, the Security Council imposed sanctions, which were to last until official independence in 1979. This involved British warships blockading the port of Beira to try to cause economic collapse in Rhodesia. Wilson was applauded by most nations for taking a firm stand on the issue (and none extended diplomatic recognition to the Smith régime). A number of nations did not join in with sanctions, undermining their efficiency. Certain sections of public opinion started to question their efficacy, and to demand the toppling of the régime by force. Wilson declined to intervene in Rhodesia with military force, believing the British population would not support such action against their "kith and kin". The two leaders met for discussions aboard British warships, "Tiger" in 1966 and "Fearless" in 1968. Smith subsequently attacked Wilson in his memoirs, accusing him of delaying tactics during negotiations and alleging duplicity; Wilson responded in kind, questioning Smith's good faith and suggesting that Smith had moved the goal-posts whenever a settlement appeared in sight. The matter was still unresolved at the time of Wilson's resignation in 1976.
Elsewhere in Africa, trouble developed in Nigeria, brought about by the ethnic hatreds and Biafra's efforts to become independent. Wilson supported the established new governments in former colonies and refused to countenance breakaway movements. He supported the government of General Yakubu Gowon during the Nigerian Civil War of 1967–1970.
Defeat and return to opposition.
By 1969, the Labour Party was suffering serious electoral reverses, and by the turn of 1970 had lost a total of 16 seats in by-elections since the previous general election.
Although the First Wilson Government had enacted a wide range of social reforms and arguably did much to reduce social inequalities during its time in office, the economic difficulties that it faced led to austerity measures being imposed on numerous occasions, forcing the government to abandon some of its key policy goals. Amongst the controversial austerity measures introduced included higher dental charges, the abolition of free school milk in all secondary schools in 1968, increased weekly National Insurance Contributions, the postponement of the planned rise in the school leaving age to 16, and cuts in road and housing programmes, which meant that the government's house-building target of 500,000 per year was never met. The government also failed to meet its 1964 manifesto commitment to tie increases in national insurance benefits to increases in average earnings, although this reform would later be implemented during Wilson's second premiership in 1975. There was also much controversy over the government's decision to reintroduce Prescription charges in 1968 (after having abolished them in 1964), although the blow of this measure was arguably by softened by the fact that many people were exempted from charges. In 1968, arguably in response to sensationalist stories about supposed "scroungers" and "welfare cheats," the government made the decision to introduce a controversial new rule terminating benefits for single men under the age of 45. Under this rule, young, single, unskilled men who lived in areas of low unemployment would have their supplementary benefits stopped after four weeks. Wilson’s government also failed to maintain the real value of family allowances during its time in office, which (despite being doubled under Wilson) fell by 13% in real terms between 1964 and 1969. In addition, tax allowances were reduced in 1968 to pay in part for increases in family allowances, and despite inflation were not increased again until 1971. Family allowances were increased for the fourth and subsequent children from 50p to 75p per week in October 1966, and then in April 1967 to 75p for the second child and 85p for each subsequent child. According to one writer, however, this policy did not help single mothers with only one child, Supplementary Benefit payments were reduced “by the amount of this increase,” and tax allowances were adjusted “to recover the cost of family allowances from taxpayers, including some low earners.”
In the field of housing, the First Wilson Government has received criticism by historians for encouraging the building of high-rise council flats, continuing the high-rise boom launched by the preceding Conservative administration in 1956 when it introduced a progressive storey-height subsidy that gave large increments for four-, five-, and six-storey flats and a fixed increment for every additional storey above that. In 1966, tall flats accounted 25.6% of all approved starts, compared with only 3% in 1954. From 1964 to 1966, the percentage of homes built in England and Wales by local authorities and New Towns in the form of flats in buildings of 5 storeys or more rose from 22.4% to 25.7%, falling to 9.9% in 1970.
According to the historian Andrew Thorpe, much of the high-rise and high-density housing that was erected proved to be poorly constructed and unpopular with tenants, and social and extended family networks were disrupted by rehousing, leading to increased strain on social services and therefore public expenditure as older, informal support networks were ruptured. As argued by Thorpe, Labour's accomplishments "were equivocal, and in retrospect many would see its policies as leading to significant social problems."
According to another historian, Eric Shaw, in the rush to build, and to overcome shortages in funds, the First Wilson Government "succumbed to the fashion for high-rise blocks of flats." For Shaw, the housing drive demonstrated "flaws in Labour's centralist brand of social democracy," the assumption that the interests of ordinary people could be safeguarded by public officials without needing to consult them, "a well-intentioned but short-sighted belief that pledges could be honored by spreading resources more thinly; and a 'social engineering' approach to reform in which the calculation of the effects of institutional reform neglected their impact upon the overall quality of people's lives." This approach resulted in people being wrenched from their local communities and transferred to isolating and forbidding environments which often lacked basic social and commercial amenities and which hindered the revival of community networks. High-rise council flats, according to Shaw, intensified class inequalities by becoming a low-grad reserve for the poorer sections of the working class, which reflecting the "extent to which Keynesian social democracy had departed from the traditions of ethical socialism, with its aspirations to construct institutions which would foster greater fellowship, a communal spirit and more altruistic forms of behaviour." As further noted by Shaw, the "new soulless working-class estates" became the breeding grounds of a host of social evils, "as socialists from an older generation like William Morris could have predicted."
A plan to boost economic growth to 4% a year was never met, while development aid was cut severely as a result of austerity measures. A proposed "minimum income guarantee" for widows and pensioners was never implemented, together with Richard Crossman's compulsory national superannuation scheme. This scheme, a system of universal secondary pensions, was aimed at providing British pensioners with an income closer to what they enjoyed during the best years of their working life, when their earnings were at their highest. According to Brian Lapping, this would have been Wilson's largest reform of social security, had it been carried out. A proposed means-tested Family Supplement debated in cabinet and supported by the (then) Chancellor James Callaghan. never came into being, although it was later introduced by the Heath Government under the name “Family Income Supplement.” In June 1969, the government announced plans for introducing two new benefits for the disabled an “attendance allowance” for the very severely disabled, and an “invalidity pension” for people forced to retire early due to illness. Neither benefit, however, came into being during the remainder of the First Wilson Government’s time in office, although the proposed attendance allowance would later be introduced by the Heath Government and the invalidity pension by the Second Wilson Government. In addition, the government's austerity measures led to an unpopular squeeze on consumption in 1968 and 1969.
By 1970, the economy was showing signs of improvement, and by May that year, Labour had overtaken the Conservatives in the opinion polls. Wilson responded to this apparent recovery in his government's popularity by calling a general election, but, to the surprise of most observers, was defeated at the polls by the Conservatives under Heath.
Wilson survived as leader of the Labour party in opposition. Economic conditions during the 1970s were becoming more difficult for Britain and many other western economies as a result of the ending of the Bretton Woods Agreement and the 1973 oil shock, and the Heath government in its turn was buffeted by economic adversity and industrial unrest (notably including confrontation with the coalminers which led to the Three-day week) towards the end of 1973, and on 7 February 1974 (with the crisis still ongoing) Heath called a snap election for 28 February.
Second term as Prime Minister.
Labour won more seats (though fewer votes) than the Conservative Party in February 1974. As Heath was unable to persuade the Liberals to form a coalition, Wilson returned to 10 Downing Street on 4 March 1974 as Prime Minister of a minority Labour Government. He gained a three-seat majority in another election later that year, on 10 October 1974. One of the key issues addressed during his second period in office was the referendum on British membership of the EEC (see "Europe", above).
Domestic affairs.
The Second Wilson Government made a major commitment to the expansion of the British welfare state, with increased spending on education, health, and housing rents. To pay for it, it imposed controls and raised taxes on the rich. It partially reversed the 1971 reduction in the top rate of tax from 90% to 75%, increasing it to 83% in the first budget from new chancellor Denis Healey, which came into law in April 1974. Also implemented was an investment income surcharge which raised the top rate on investment income to 98%, the highest level since the Second World War. In March 1974, an additional £2 billion were announced for benefits, food subsidies, and housing subsidies, including a record 25% increase in the pension. Council house rents were also frozen. That same year, national insurance benefits were increased by 13%, which brought pensions as a proportion of average earnings "up to a value equivalent to the previous high, which was reached in 1965 as a result of Labour legislation." In order to maintain the real value of these benefits in the long term, the government introduced legislation which linked future increases in pensions to higher incomes or wages. In 1974–75, social spending was increased in real terms by 9%. In 1974, pensions were increased in real terms by 14%, while in early 1975 increases were made in family allowances. There were also significant increases in rate and rent subsidies, together with £500 million worth of food subsidies.
An Independent Advisory, Conciliation and Arbitration Service (now simply called Acas) (regarded as very much the brainchild of the trade union leader Jack Jones) was set, which according to Robert Taylor continues to provide "an impartial and impressive function in resolving disputes and encouraging good industrial relations practice." A Manpower Services Commission was set up to encourage a more active labour market policy to improve job placements and deal with unemployment. The Pay Board was abolished, while the Price Commission was provided with greater powers to control and delay price increases. In addition, the Housing Rents and Subsidies Act 1975 gave power over rents back to local authorities.
To help those with disabilities, the government introduced an Invalid Care Allowance, a Mobility Allowance, a Non-Contributory Invalidity Pension for those unable to contribute through national insurance, and other measures. To combat child poverty, legislation to create a universal Child Benefit was passed in 1975 (a reform later implemented by the Callaghan Government). To raise the living standards of those dependant on national insurance benefits, the government index-linked short-term benefits to the rate of inflation, while pensions and long-term benefits were tied to increases in prices or earnings, whichever was higher.
In 1975, a State Earnings Related Pension Scheme (SERPS) was introduced. A new pension, which was inflation-proofed and linked to earnings, was added to the basic pension which was to increase in line with earnings for the first time ever. This reform assisted women by the linking of pensions to the 'twenty best years' of earnings, and those who worked at home caring for children or others were counted as contributors. This scheme was later eroded by the subsequent Thatcher Government, and insufficient pension rights had been built up by that time to establish resistance to its erosion. The Sex Discrimination Act 1975 gave women the right in principle to equal access to jobs and equal treatment at work with men, while the Employment Protection Act 1975 introduced Statutory Maternity Leave. That same year, the wage stop was finally abolished. In addition, differentials between skilled and unskilled workers were narrowed as a result of egalitarian pay policies involving flat-rate increases.
The 1975 Social Security Pensions Act provided for equal access by men and women to employers’ pension schemes and also included a home responsibilities provision ensuring that parents and those looking after elderly dependents could retain their pension rights in spite of employment breaks. As a means of combating sex discrimination within the social security system, the Act provided that in future married women would receive the same level of personal sickness or unemployment benefit. The Housing Finance Act 1974 increased aid to local authorities for slum clearance, introduced a system of "fair rents" in public and private sector unfurnished accommodation, and introduced rent rebates for council tenants. The Housing Act 1974 improved the Renovation Grants scheme, provided increased levels of aid to housing associations, and extended the role of the Housing Corporation. The Rent Act 1974 extended security of tenure to tenants of furnished properties and allowed access to rent tribunals. The Community Land Act 1975 allowed for the taking into public control of development land, while the Child Benefits Act 1975 introduced an extra payment for lone parents. A Resource Allocation Working Party (RAWP) was also set up to produce a formula for a more equitable distribution of health care expenditure. Anthony Crosland, while serving as a minister during Wilson's second government, made a decision to reform the level of Rate Support Grant, introducing a standard level of relief across the country to benefit poorer urban areas.
Circular 4/74 (1974) renewed pressure for moves towards comprehensive education (progress of which had stalled under the Heath Government), while the industrial relations legislation passed under Edward Heath was repealed. The Health and Safety at Work Act 1974 set up a Health and Safety Commission and Executive and a legal framework for health and safety at work. The Employment Protection Act 1975 set up the Advisory, Conciliation and Arbitration Services (ACAS) to arbitrate in industrial disputes, enlarged the rights of employees and trade unions, extended the redundancy payments scheme, and provided redress against unfair dismissal. The legislation also provided for paid maternity leave and outlawed dismissal for pregnancy. The Act also obliged employers to pay their workers a minimum guaranteed payment “if they are laid off through no fault of their own.” The Social Security Act 1975 introduced a maternity allowance fund, while the Sex Discrimination Act 1975 set up an Equal Opportunities Commission and outlawed gender discrimination (both indirect and direct).
The Woodworking Machines Regulations 1974, replacing the 1922 Regulations, came into operation on in November 1974. These regulations raised the standard of guarding of the most dangerous machines. Improvements were made in mine-workers' pensions, while the Coal Mines (Respirable Dust) Regulations 1975, which came into operation in October that year, were aimed at reducing the incidence of coal miners' pneumoconiosis. They prescribed permitted amounts of respirable dust at workplaces in coal mines as well as arrangements for the suppression and continuous sampling of dust, and they include a scheme for the medical supervision of workers at risk. The Protection of Eyes Regulations 1974 and 1975, replacing the 1938 Regulations, extended protection to those employed on construction sites as well as in factories. In addition, the Policyholders Protection Act 1975 introduced safeguards for customers of failed insurance companies.
Despite its achievements in social policy, however, Wilson's government came under scrutiny in 1975 for the rise in the unemployment rate, with the total number of Britons out of work passing 1,000,000 by April of that year.
Northern Ireland.
Wilson's earlier government had witnessed the outbreak of The Troubles in Northern Ireland. In response to a request from the Government of Northern Ireland, Wilson agreed to deploy the British Army in August 1969 in an effort to restore the peace.
Out of office in the autumn of 1971, Wilson formulated a 16-point, 15-year programme that was designed to pave the way for the unification of Ireland. The proposal was not adopted by the then Heath government.
In May 1974, when back in office as leader of a minority government, Wilson condemned the Unionist-controlled Ulster Workers Council Strike as a "sectarian strike", which was "being done for sectarian purposes having no relation to this century but only to the seventeenth century". However he refused to pressure a reluctant British Army to face down the loyalist paramilitaries who were intimidating utility workers. In a televised speech later, he referred to the loyalist strikers and their supporters as "spongers" who expected Britain to pay for their lifestyles. The strike was eventually successful in breaking the power-sharing Northern Ireland executive.
On 11 September 2008, BBC Radio Four's "Document" programme claimed to have unearthed a secret plan – codenamed "Doomsday" – which proposed to cut all of the United Kingdom's constitutional ties with Northern Ireland and transform the province into an independent dominion. "Document" went on to claim that the Doomsday plan was devised mainly by Wilson and was kept a closely guarded secret. The plan then allegedly lost momentum, due in part, it was claimed, to warnings made by both the then Foreign Secretary, James Callaghan, and the then Irish Minister for Foreign Affairs Garret FitzGerald who admitted the 12,000-strong Irish army would be unable to deal with the ensuing civil war.
In 1975 Wilson secretly offered Libya's dictator Muammar Gaddafi £14 million (£500 million in 2009 values) to stop arming the IRA, but Gaddafi demanded a far greater sum of money. This offer did not become publicly known until 2009.
Resignation.
On 16 March 1976, Wilson surprised the nation by announcing his resignation as Prime Minister (taking effect on 5 April 1976). He claimed that he had always planned on resigning at the age of 60, and that he was physically and mentally exhausted. As early as the late 1960s, he had been telling intimates, like his doctor Sir Joseph Stone (later Lord Stone of Hendon), that he did not intend to serve more than eight or nine years as Prime Minister. Roy Jenkins has suggested that Wilson may have been motivated partly by the distaste for politics felt by his loyal and long-suffering wife, Mary. His doctor had detected problems which would later be diagnosed as colon cancer, and Wilson had begun drinking brandy during the day to cope with stress. In addition, by 1976 he might already have been aware of the first stages of early-onset Alzheimer's disease, which was to cause both his formerly excellent memory and his powers of concentration to fail dramatically.
Queen Elizabeth II came to dine at 10 Downing Street to mark his resignation, an honour she has bestowed on only one other Prime Minister, Sir Winston Churchill.
Wilson's Prime Minister's Resignation Honours included many businessmen and celebrities, along with his political supporters. His choice of appointments caused lasting damage to his reputation, worsened by the suggestion that the first draft of the list had been written by his political secretary Marcia Williams on lavender notepaper (it became known as the "Lavender List"). Roy Jenkins noted that Wilson's retirement "was disfigured by his, at best, eccentric resignation honours list, which gave peerages or knighthoods to some adventurous business gentlemen, several of whom were close neither to him nor to the Labour Party." Some of those whom Wilson honoured included Lord Kagan, the inventor of Gannex, who was eventually imprisoned for fraud, and Sir Eric Miller, who later committed suicide while under police investigation for corruption.
Six candidates stood in the first ballot to replace him, in order of votes they were: Michael Foot, James Callaghan, Roy Jenkins, Tony Benn, Denis Healey and Anthony Crosland. In the third ballot on 5 April, Callaghan defeated Foot in a parliamentary vote of 176 to 137, thus becoming Wilson's successor as Prime Minister and leader of the Labour Party, and he continued to serve as Prime Minister until May 1979, when Labour lost the general election to the Conservatives and Margaret Thatcher became Britain's first female prime minister.
As Wilson wished to remain an MP after leaving office, he was not immediately given the peerage customarily offered to retired Prime Ministers, but instead was created a Knight of the Garter. On leaving the House of Commons after the 1983 general election he was created Baron Wilson of Rievaulx, after Rievaulx Abbey, in the north of his native Yorkshire.
Retirement and death.
Shortly after resigning as Prime Minister, Wilson was signed by David Frost to host a series of interview/chat show programmes. The pilot episode proved to be a flop as Wilson appeared uncomfortable with the informality of the format. Wilson also hosted two editions of the BBC chat show "Friday Night, Saturday Morning". He famously floundered in the role, and in 2000, Channel 4 chose one of his appearances as one of the 100 Moments of TV Hell. Wilson also coined the name of charity War on Want.
A lifelong Gilbert and Sullivan fan, in 1975, Wilson joined the Board of Trustees of the D'Oyly Carte Trust at the invitation of Sir Hugh Wontner, who was then the Lord Mayor of London. At Christmas 1978, Wilson appeared on the "Morecambe and Wise" Christmas Special. Eric Morecambe's habit of appearing not to recognise the guest stars was repaid by Wilson, who referred to him throughout as 'Morry-camby' (the mis-pronunciation of Morecambe's name made by Ed Sullivan when the pair appeared on his famous American television show). Wilson appeared on the show again in 1980.
Wilson was not especially active in the House of Lords, although he did initiate a debate on unemployment in May 1984. His last speech was in a debate on marine pilotage in 1986, when he commented as an elder brother of Trinity House. In the same year, he played himself as Prime Minister in an Anglia Television drama, "Inside Story".
He continued regularly attending the House of Lords until just over a year before his death; the last sitting he attended was on 27 April 1994. Wilson died from colon cancer and Alzheimer's disease in May 1995, aged 79. His memorial service was held in Westminster Abbey on 13 July 1995. His death came mere months before the death of his predecessor. It was attended by Prince Charles, former Prime Ministers Edward Heath, James Callaghan and Margaret Thatcher, then Prime Minister John Major and future Prime Minister Tony Blair. Wilson was buried at St. Mary's Old Church, St. Mary's on the Isles of Scilly on 6 June. His epitaph is "Tempus Imperator Rerum" ("Time the Commander of All Things").
Political style.
Wilson regarded himself as a "man of the people" and did much to promote this image, contrasting himself with the stereotypical aristocratic conservatives who had preceded him. Features of this portrayal included his working man's Gannex raincoat, his pipe (the British Pipesmokers' Council voted him Pipe Smoker of the Year in 1965 and Pipeman of the Decade in 1976, though in private he smoked cigars), his love of simple cooking and fondness for popular British relish HP Sauce, and his support for his home town's football team, Huddersfield Town. He spoke with a studied working class Yorkshire accent, although this was not part of his background, as his father had spoken "upper class" English. Eschewing continental holidays, he returned every summer with his family to the Isles of Scilly. His first general election victory relied heavily on associating these down-to-earth attributes with a sense that the UK urgently needed to modernise, after "thirteen years of Tory mis-rule ...". These characteristics were exaggerated in "Private Eye's" satirical column "Mrs Wilson's Diary".
Wilson exhibited his populist touch in June 1965 when he had The Beatles honoured with the award of MBE (such awards are officially bestowed by The Queen but are nominated by the Prime Minister of the day). The award was popular with young people and contributed to a sense that the Prime Minister was "in touch" with the younger generation. There were some protests by conservatives and elderly members of the military who were earlier recipients of the award, but such protesters were in the minority. Critics claimed that Wilson acted to solicit votes for the next general election (which took place less than a year later), but defenders noted that, since the minimum voting age at that time was 21, this was hardly likely to impact many of the Beatles' fans who at that time were predominantly teenagers. It cemented Wilson's image as a modernistic leader and linked him to the burgeoning pride in the 'New Britain' typified by the Beatles. The Beatles mentioned Wilson rather negatively, naming both him and his opponent Edward Heath in George Harrison's song "Taxman", the opener to 1966's "Revolver"—recorded and released after the MBEs.
In 1967, Wilson had a different interaction with a musical ensemble. He sued the pop group The Move for libel after the band's manager Tony Secunda published a promotional postcard for the single "Flowers In The Rain", featuring a caricature depicting Wilson in bed with his female assistant, Marcia Williams. Gossip had hinted at an improper relationship, though these rumours were never substantiated. Wilson won the case, and all royalties from the song (composed by Move leader Roy Wood) were assigned in perpetuity to a charity of Wilson's choosing.
Wilson coined the term 'Selsdon Man' to refer to the anti-interventionist policies of the Conservative leader Edward Heath, developed at a policy retreat held at the Selsdon Park Hotel in early 1970. This phrase, intended to evoke the 'primitive throwback' qualities of anthropological discoveries such as Piltdown Man and Swanscombe Man, was part of a British political tradition of referring to political trends by suffixing 'man'. Another famous quote is "A week is a long time in politics": this signifies that political fortunes can change extremely rapidly. Other memorable phrases attributed to Wilson include "the white heat of the [technological] revolution." In his broadcast after the 1967 devaluation of the pound, Wilson said: "This does not mean that the pound here in Britain – in your pocket or purse – is worth any less ...", and the phrase "the pound in your pocket" subsequently took on a life of its own.
Reputation.
Despite his successes and one-time popularity, Harold Wilson's reputation took a long time to recover from the low ebb reached immediately following his second premiership. Some accuse him of undue deviousness, some claim he did not do enough to modernise the Labour Party's policy positions on issues such as the respective roles of the state and the market or the reform of industrial relations. This line of argument partly blames Wilson for the civil unrest of the late 1970s (during Britain's Winter of Discontent), and for the electoral success of the Conservative party and its ensuing 18-year rule. His supporters argue that Wilson's skilful management (on issues such as nationalisation, Europe and Vietnam) allowed an otherwise fractious party to stay politically united and govern. This co-existence did not long survive his leadership, and the factionalism that followed contributed greatly to the Labour Party's electoral weakness during the 1980s. The reinvention of the Labour Party would take the better part of two decades, at the hands of Neil Kinnock, John Smith and – electorally, most conclusively – Tony Blair.
In 1964, when Wilson took office, the mainstream of informed opinion (in all the main political parties, in academia and the media, etc.) strongly favoured the type of technocratic, "indicative planning" approach that Wilson endeavoured to implement. Radical market-orientated reforms, of the kind eventually adopted by Margaret Thatcher, were in the mid-1960s backed only by a 'fringe' of enthusiasts (such as the leadership of the later-influential Institute of Economic Affairs), and had almost no representation at senior levels even of the Conservative Party. Fifteen years later, disillusionment with Britain's weak economic performance and troubled industrial relations, combined with active spadework by figures such as Sir Keith Joseph, had helped to make a radical market programme politically feasible for Thatcher (which was in turn to influence the subsequent Labour leadership, especially under Blair).
An opinion poll in September 2011 found that Wilson came in third place when respondents were asked to name the best post-war Labour Party leader. He was beaten only by John Smith and Tony Blair.
Possible plots and conspiracy theories.
MI5 plots.
In 1963, Soviet defector Anatoliy Golitsyn is said to have secretly claimed that Wilson was a KGB agent. The majority of intelligence officers did not believe that Golitsyn was credible in this and various other claims, but a significant number did (most prominently James Jesus Angleton, Deputy Director of Operations for Counter-Intelligence at the U.S. Central Intelligence Agency) and factional strife broke out between the two groups. Former MI5 officer Peter Wright claimed in his memoirs, "Spycatcher", that 30 MI5 agents then collaborated in an attempt to undermine Wilson.
In March 1987, James Miller, a former agent, claimed that the Ulster Workers Council Strike of 1974 had been promoted by MI5 in order to help destabilise Wilson's government. In July 1987, Labour MP Ken Livingstone used his maiden speech to raise the 1975 allegations of a former Army Press officer in Northern Ireland, Colin Wallace, who also alleged a plot to destabilise Wilson. Chris Mullin, MP, speaking on 23 November 1988, argued that sources other than Peter Wright supported claims of a long-standing attempt by MI5 to undermine Wilson's government.
In 2009, " The Defence of the Realm", the authorised history of MI5 by Christopher Andrew, held that while MI5 kept a file on Wilson from 1945, when he became an MP – because communist civil servants claimed that he had similar political sympathies – there was no bugging of his home or office, and no conspiracy against him. In 2010 newspaper reports made detailed allegations that the bugging of 10 Downing Street had been omitted from the history for "wider public interest reasons". In 1963 on Macmillan's orders following the Profumo Affair, MI5 bugged the cabinet room, the waiting room, and the prime minister's study until the devices were removed in 1977 on Callaghan's orders. From the records it is unclear if Wilson or Heath knew of the bugging, and no recorded conversations were retained by MI5 so possibly the bugs were never activated. Professor Andrew had previously recorded in the preface of the history that "One significant excision as a result of these requirements (in the chapter on The Wilson Plot) is, I believe, hard to justify" giving credence to these new allegations.
Other conspiracy theories.
Richard Hough, in his 1980 biography of Mountbatten, indicates that Mountbatten was approached during the 1960s in connection with a scheme to install an "emergency government" in place of Wilson's administration. The approach was made by Cecil Harmsworth King, the chairman of the International Publishing Corporation (IPC), which published the "Daily Mirror" newspaper. Hough bases his account on conversations with the "Mirror"'s long-time editor Hugh Cudlipp, supplemented by the recollections of the scientist Solly Zuckerman and of Mountbatten's valet, William Evans. Cudlipp arranged for Mountbatten to meet King on 8 May 1968. King had long yearned to play a more central political role, and had personal grudges against Wilson (including Wilson's refusal to propose King for the hereditary earldom that King coveted). He had already failed in an earlier attempt to replace Wilson with James Callaghan. With Britain's continuing economic difficulties and industrial strife in the 1960s, King convinced himself that Wilson's government was heading towards collapse. He thought that Mountbatten, as a member of the Royal Family and a former Chief of the Defence Staff, would command public support as leader of a non-democratic "emergency" government. Mountbatten insisted that his friend, Zuckerman, be present (Zuckerman says that he was urged to attend by Mountbatten's son-in-law, Lord Brabourne, who worried King would lead Mountbatten astray). King asked Mountbatten if he would be willing to head an emergency government. Zuckerman said the idea was treason and Mountbatten in turn rebuffed King. He does not appear to have reported the approach to Downing Street.
The question of how serious a threat to democracy may have existed during these years continues to be contentious—a key point at issue being who of any consequence would have been ready to move beyond grumbling about the government (or spreading rumours) to actively taking unconstitutional action. Cecil King himself was an inveterate schemer but an inept actor on the political stage. Perhaps significantly, when King penned a strongly worded editorial against Wilson for the "Daily Mirror" two days after his abortive meeting with Mountbatten, the unanimous reaction of IPC's directors was to fire him with immediate effect from his position as Chairman. King's resignation was considered a serious enough matter for the BBC to have senior journalist William Hardcastle announce it in a news flash. More fundamentally, Denis Healey, who served for six years as Wilson's Secretary of State for Defence, has argued that actively serving senior British military officers would not have been prepared to overthrow a constitutionally elected government.
By the time of his resignation, Wilson's own perceptions of any threat may very well have been exacerbated by the onset of Alzheimer's disease; his inherent tendency to chariness was undoubtedly stoked by some in his inner circle, including Marcia Williams. He reportedly shared with a surprised George H. W. Bush, at the time the Director of the CIA, his fear that some of the portraits in 10 Downing Street (specifically including Gladstone's portrait in the Cabinet Room) concealed listening devices being used to bug his discussions. Files released on 1 June 2005 show that Wilson was concerned that, while on the Isles of Scilly, he was being monitored by Russian ships disguised as trawlers. MI5 found no evidence of this, but told him not to use a walkie-talkie.
Wilson's Government took strong action against the controversial, self-styled "Church" of Scientology in 1967, banning foreign Scientologists from entering the country, a prohibition which remained in force until 1980. In response, L. Ron Hubbard, Scientology's founder, accused Wilson of being in cahoots with Soviet Russia and an international conspiracy of psychiatrists and financiers. Wilson's Minister of Health, Kenneth Robinson, subsequently won a libel suit against the Scientologists and Hubbard.
Honours.
Statues and other tributes.
A portrait of Harold Wilson, painted by the famous Scottish portrait artist Cowan Dobson, hangs today at University College, Oxford. Two statues of Harold Wilson stand in prominent places. The first, unveiled by the then Prime Minister Tony Blair in July 1999, stands outside Huddersfield railway station in St George's Square, Huddersfield. Costing £70,000, the statue, designed by sculptor Ian Walters, is based on photographs taken in 1964 and depicts Wilson in walking pose at the start of his first term as Prime Minister. His widow, Mary requested that the eight-foot tall monument did not show Wilson holding his famous pipe as she feared it would make the representation a caricature.
In September 2006, Tony Blair unveiled a second bronze statue of Wilson in the latter's former constituency of Huyton, near Liverpool. The statue was created by Liverpool sculptor, Tom Murphy, and Blair paid tribute to Wilson's legacy at the unveiling, including the Open University. He added: "He also brought in a whole new culture, a whole new country. He made the country very, very different".
Also in 2006, a street on a new housing development in Tividale, West Midlands, was named Wilson Drive in honour of Wilson. Along with neighbouring new development Callaghan drive (named after James Callaghan), it formed part of a large housing estate developed since the 1960s where all streets were named after former prime ministers or senior parliamentary figures.
Bibliography.
There is an extensive bibliography on Harold Wilson. He is the author of a number of books. He is the subject of many biographies (both light and serious) and academic analyses of his career and various aspects of the policies pursued by the governments he led. He features in many "humorous" books. He was the Prime Minister in the so-called "Swinging London" era of the 1960s, and therefore features in many of the books about this period of history.

</doc>
<doc id="52272" url="http://en.wikipedia.org/wiki?curid=52272" title="Creative Commons">
Creative Commons

Creative Commons (CC) is a non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share. The organization has released several copyright-licenses known as Creative Commons licenses free of charge to the public. These licenses allow creators to communicate which rights they reserve, and which rights they waive for the benefit of recipients or other creators. An easy-to-understand one-page explanation of rights, with associated visual symbols, explains the specifics of each Creative Commons license. Creative Commons licenses do not replace copyright, but are based upon it. They replace individual negotiations for specific rights between copyright owner (licensor) and licensee, which are necessary under an "all rights reserved" copyright management, with a "some rights reserved" management employing standardized licenses for re-use cases where no commercial compensation is sought by the copyright owner. The result is an agile, low-overhead and low-cost copyright-management regime, profiting both copyright owners and licensees. Wikipedia uses one of these licenses.
The organization was founded in 2001 by Lawrence Lessig, Hal Abelson, and Eric Eldred with the support of Center for the Public Domain. The first article in a general interest publication about Creative Commons, written by Hal Plotkin, was published in February 2002. The first set of copyright licenses was released in December 2002. The founding management team that developed the licenses and built the Creative Commons infrastructure as we know it today included Molly Shaffer Van Houweling, Glenn Otis Brown, Neeru Paharia, and Ben Adida. Matthew Haughey and Aaron Swartz also played a significant role in the early stages of the project. As of November 2014 there were an estimated 880 million works licensed under the various Creative Commons licenses. As of March 2015, Flickr alone hosts over 306 million Creative Commons licensed photos. Creative Commons is governed by a board of directors. Their licenses have been embraced by many as a way for creators to take control of how they choose to share their copyrighted works.
Aim and influence.
Creative Commons has been described as being at the forefront of the copyleft movement, which seeks to support the building of a richer public domain by providing an alternative to the automatic "all rights reserved" copyright, and has been dubbed "some rights reserved." David Berry and Giles Moss have credited Creative Commons with generating interest in the issue of intellectual property and contributing to the re-thinking of the role of the "commons" in the "information age". Beyond that, Creative Commons has provided "institutional, practical and legal support for individuals and groups wishing to experiment and communicate with culture more freely."
Creative Commons attempts to counter what Lawrence Lessig, founder of Creative Commons, considers to be a dominant and increasingly restrictive permission culture. Lessig describes this as "a culture in which creators get to create only with the permission of the powerful, or of creators from the past". Lessig maintains that modern culture is dominated by traditional content distributors in order to maintain and strengthen their monopolies on cultural products such as popular music and popular cinema, and that Creative Commons can provide alternatives to these restrictions.
Governance and staff.
Creative Commons staff include two full time legal counsel, as well as a number of open education, free culture and free software veterans including:
s of 2015[ [update]], the Board of Creative Commons consists of:
The Advisory Board consists of:
CC's Audit Committee has three members, who are also members of the Board. s of 2015[ [update]], they are Laurie Racine, Eric Saltzman and Chris Sprigman.
Affiliate network.
As of 2015, there are more than 100 affiliates working in over 75 jurisdictions to support and promote CC activities around the world.
South Korea.
Creative Commons Korea (CC Korea) is the affiliated network of Creative Commons in South Korea. In March 2005, CC Korea was initiated by Jongsoo Yoon (in Korean: 윤종수), a Presiding Judge of Incheon District Court, as a project of Korea Association for Infomedia Law (KAFIL). The major Korean portal sites, including Daum and Naver, have been participating in the use of Creative Commons licences. In January 2009, the Creative Commons Korea Association was consequently founded as a non-profit incorporated association. Since then, CC Korea has been actively promoting the liberal and open culture of creation as well as leading the diffusion of Creative Commons in the country.
Bassel Khartabil.
Bassel Khartabil is a Palestinian Syrian open source software developer and has served as project lead and public affiliate for Creative Commons Syria. Since March 15, 2012 he has been detained by the Syrian government in Damascus at Adra Prison.
Criticism.
All current CC licenses (except the CC0 Public Domain Dedication tool) require attribution, which can be inconvenient for works based on multiple other works. Critics feared that Creative Commons could erode the copyright system over time or allow "some of our most precious resources — the creativity of individuals — to be simply tossed into the commons to be exploited by whomever has spare time and a magic marker."
Critics also worried that the lack of rewards for content producers will dissuade artists from publishing their work, and questioned whether Creative Commons is the commons that it purports to be.
Creative Commons founder Lawrence Lessig countered that copyright laws have not always offered the strong and seemingly indefinite protection that today's law provides. Rather, the duration of copyright used to be limited to much shorter terms of years, and some works never gained protection because they did not follow the now-abandoned compulsory format.
The maintainers of Debian, a GNU and Linux distribution known for its rigid adherence to a particular definition of software freedom, rejected the Creative Commons Attribution License prior to version 3 as incompatible with the Debian Free Software Guidelines (DFSG) due to the license's anti-DRM provisions (which might, due to ambiguity, be covering more than DRM) and its requirement that downstream users remove an author's credit upon request from the author. Version 3.0 of the Creative Commons licenses addressed these concerns and is considered to be compatible with the DFSG.
License proliferation and incompatibility.
Mako Hill asserted that Creative Commons fails to establish a "base level of freedom" that all Creative Commons licenses must meet, and with which all licensors and users must comply. "By failing to take any firm ethical position and draw any line in the sand, CC is a missed opportunity... CC has replaced what could have been a call for a world where 'essential rights are unreservable' with the relatively hollow call for 'some rights reserved.'" He also argued that Creative Commons worsens license proliferation, by providing multiple licenses that are incompatible.
The Creative Commons website states, "Since each of the six CC licenses functions differently, resources placed under different licenses may not necessarily be combined with one another without violating the license terms." Works licensed under incompatible licenses may not be recombined in a derivative work without obtaining permission from the copyright owner.
Richard Stallman of the FSF stated in 2005 that he couldn’t support Creative Commons as an activity because "it adopted some additional licenses which do not give everyone that minimum freedom", that freedom being "the freedom to share, noncommercially, any published work". Those licenses have since been "retired" by Creative Commons.
License misuse.
Creative Commons is only a service provider for standardized license text, not a party in any agreement. Abusive users can brand the copyrighted works of legitimate copyright holders with Creative Commons licenses and re-upload these works to the internet. No central database of Creative Commons works is controlling all licensed works and the responsibility of the Creative Commons system rests entirely with those using the licences. This situation is, however, not specific to Creative Commons. All copyright owners must individually defend their rights and no central database of copyrighted works or existing license agreements exists. The United States Copyright Office does keep a database of all works registered with it, but absence of registration does not imply absence of copyright.
Although Creative Commons offers multiple licenses for different uses, some critics suggested that the licenses still do not address the differences among the media or among the various concerns that different authors have.
Lessig wrote that the point of Creative Commons is to provide a middle ground between two extreme views of copyright protection—one demanding that all rights be controlled, and the other arguing that none should be controlled. Creative Commons provides a third option that allows authors to pick and choose which rights they want to control and which they want to grant to others. The multitude of licenses reflects the multitude of rights that can be passed on to subsequent creators.
Criticism of the non-commercial license.
Erik Möller raised concerns about the use of Creative Commons' non-commercial license. Works distributed under the Creative Commons Non-Commercial license are not compatible with many open-content sites, including Wikipedia, which explicitly allow and encourage some commercial uses. Möller explained that "the people who are likely to be hurt by an -NC license are not large corporations, but small publications like weblogs, advertising-funded radio stations, or local newspapers."
Lessig responded that the current copyright regime also harms compatibility and that authors can lessen this incompatibility by choosing the least restrictive license. Additionally, the non-commercial license is useful for preventing someone else from capitalizing on an author's work when the author still plans to do so in the future. The non-commercial licenses have also been criticized for being too vague about which uses count as "commercial" and "non-commercial".

</doc>
<doc id="52273" url="http://en.wikipedia.org/wiki?curid=52273" title="Crow Nation">
Crow Nation

The Crow, called the Apsáalooke in their own Siouan language, or variants including Absaroka, are Native Americans, who in historical times lived in the Yellowstone River valley, which extends from present-day Wyoming, through Montana and into North Dakota, where it joins the Missouri River. Today, they are enrolled in the federally recognized Crow Tribe of Montana.
Pressured by the Ojibwe and Cree peoples (the Iron Confederacy), who had earlier and better access to guns through the fur trade, they had migrated there from the Ohio Eastern Woodland area to settle south of Lake Winnipeg, Canada. From there, they were pushed to the west by the Cheyennes. Both the Crow and the Cheyennes were then pushed farther west by the Lakota (Sioux), who took over the territory from the Black Hills of South Dakota to the Big Horn Mountains of Montana; the Cheyennes finally became close allies of the Sioux, but the Crows remained bitter enemies of both Sioux and Cheyennes. The Crow were generally friendly with the whites and managed to retain a large reservation of over 9300 km2 despite territorial losses.
Since the 19th century, Crow people have been concentrated on their reservation established south of Billings, Montana. They also live in several major, mainly western, cities. Tribal headquarters are located at Crow Agency, Montana.
History.
The name of the tribe, Apsáalooke ], meaning "children of the large-beaked bird", was given to them by the Hidatsa, a neighboring Siouan tribe. French interpreters translated the name as "gens du corbeaux" ("people of [the] crows"), and they became known in English as the Crow. Other tribes also refer to the Apsáalooke as "crow" or "raven" in their own languages.
In 1743 the Absaroka encountered their first people of European descent, the two La Vérendrye brothers from New France. The explorers called the Apsáalooke "beaux hommes" (handsome men). The Crow called the French explorers "baashchíile" (persons with yellow eyes).
In the Northern Plains.
The early home of the Crow-Hidatsa ancestral tribe was in the Ohio country, near Lake Erie. Driven from there by better armed, aggressive neighbors, they settled for a while south of Lake Winnipeg in Manitoba. Later the people moved to the Devil's Lake region of North Dakota before the Crow split from the Hidatsa and moved westward. The Crow were largely pushed westward due to intrusion and influx of the Cheyenne subsequently the Sioux.
To acquire control of their new home, they warred against Shoshone bands (called "Bikkaashe"—"People of the Grass Lodges"), and drove them westward. They allied with local Kiowa and Kiowa Apache bands. The Kiowa and Kiowa Apache bands later migrated southward, and the Crow remained dominant in their established area through the 18th and 19th centuries, the era of the fur trade.
Their tribal territory stretched from what is now Yellowstone National Park and the headwaters of the Yellowstone River ("E-chee-dick-karsh-ah-shay"—"Elk River") in the west, north to the Musselshell River, then northeast to the Yellowstone's mouth at the Missouri River, then southeast to the confluence of the Yellowstone and Powder Rivers ("Bilap chashee"—"Powder River" or "Ash River"), south along the South Fork of the Powder River, confined in the SE by the Rattlesnake Mountains and westwards in the SW by the Wind River Range. Their tribal area included the river valleys of the Judith River ("Buluhpa'ashe"—"Plum River"), Powder River, Tongue River, Big Horn River and Wind River as well as the Bighorn Mountains ("Iisiaxpúatachee Isawaxaawúua"), Pryor Mountains ("Baahpuuo Isawaxaawúua"), Wolf Mountains ("Cheetiish"—"Wolf Teeth Mountains") and Absaroka Range (also called "Absalaga Mountains").
Once established in the Valley of the Yellowstone River and its tributaries on the Northern Plains in Montana and Wyoming, the Crow divided into four groups: the Mountain Crow, River Crow, Kicked in the Bellies and Beaver Dries its Fur. Formerly semi-nomad hunters and farmers in the northeastern woodland, they picked up the nomadic lifestyle of the Plains Indians as hunters and gatherers and hunted bison. Before 1700, they were using dog travois for carrying goods. They obtained horses from the Spanish.
Enemies and allies.
From about 1740, the Plains tribes rapidly adopted the horse, which allowed them to move out on to the Plains and hunt buffalo more actively. However, the severe winters in the North kept their herds smaller than those of Plains tribes in the South. The Crow, Hidatsa, Eastern Shoshone and Northern Shoshone soon became noted as horse breeders and dealers, and developed relatively large horse herds. At the time, other eastern and northern tribes were also moving on to the Plains, in search of game for the fur trade, bison, and more horses. The Crow were subject to raids and horse thefts by horse-poor tribes including the powerful Blackfoot Confederacy, Gros Ventre, Assiniboine, Pawnee, and Ute. Later they had to face the Lakota and their allies, the Arapaho and Cheyenne, who also stole horses from their enemies. Their greatest enemies became the tribes of the Blackfoot Confederacy and the Lakota-Cheyenne-Arapaho alliance.
The Crow were generally friendly with the northern Plains tribes of the Flathead (although sometimes they had conflicts); Nez Perce, Kutenai, Shoshone, Kiowa and Kiowa Apache. The powerful Iron Confederacy ("Nehiyaw-Pwat"), an alliance of northern plains Indian nations based around the fur trade developed as enemies of the Crow. It was named after the dominating Plains Cree and Assiniboine peoples, and latter included the Stoney, Saulteaux, Ojibwe, and Métis.
Historical subgroups.
The "Apsaalooke" by the early 19th century were divided into three independent groupings, who came together only for common defense:
The oral tradition of the "Apsaalooke" mentions a fourth group, the Bilapiluutche (‘Beaver Dries its Fur’), who are believed to have merged with the Kiowa in the second half of the eighteenth century.
Gradual displacement from tribal lands.
When white Americans arrived in numbers, the Crows were resisting heavy pressure from enemies who greatly outnumbered them. In the 1850s, a vision by Plenty Coups, a Crow boy who later became their greatest chief, was interpreted by tribal elders as meaning that the whites would become dominant over the entire country, and that the Crows, if they were to retain any of their land, would need to remain on good terms with the whites.
By 1851 the more numerous Lakota and Cheyenne were established just to the south and east of Crow territory in Montana. These enemy tribes coveted the hunting lands of the Crow and warred against them. By right of conquest, they took over the eastern hunting lands of the Crow, including the Powder and Tongue River valleys, and pushed the less numerous Crow to the west and northwest upriver on the Yellowstone. After about 1860, the Lakota Sioux claimed all the former Crow lands from the Black Hills of South Dakota to the Big Horn Mountains of Montana. They demanded that the Americans deal with them regarding any intrusion into these areas.
The Fort Laramie Treaty of 1851 with the United States confirmed as Crow lands a large area centered on the Big Horn Mountains: the area ran from the Big Horn Basin on the west, to the Musselshell River on the north, and east to the Powder River; it included the Tongue River basin. But for two centuries the Cheyenne and many bands of Lakota Sioux had been steadily migrating westward across the plains, and were still pressing hard on the Crows.
Red Cloud's War (1866–1868) was a challenge by the Lakota Sioux to the United States military presence on the Bozeman Trail, a route along the eastern edge of the Big Horn Mountains to the Montana gold fields. Red Cloud's War ended with victory for the Lakota Sioux. The Treaty of Fort Laramie (1868) with the United States confirmed the Lakota control over all the high plains from the Black Hills of the Dakotas westward across the Powder River Basin to the crest of the Big Horn Mountains. Thereafter bands of Lakota Sioux led by Sitting Bull, Crazy Horse and others, along with their Northern Cheyenne allies, hunted and raided throughout the length and breadth of eastern Montana and northeastern Wyoming, which had been for a time ancestral Crow territory.
On June 25, 1876 the Lakota Sioux and Cheyenne achieved a major victory over army forces under Colonel George A. Custer at the Battle of the Little Big Horn, but the Great Sioux War (1876–1877) ended in the defeat of the Sioux and their Cheyenne allies. Crow warriors enlisted with the US Army for this war. The Sioux and allies were forced from eastern Montana and Wyoming: some bands fled to Canada, while others suffered forced removal to distant reservations, primarily in present-day Montana and Nebraska west of the Missouri River.
In 1918, the Crow organized a gathering to display their culture, and they invited members of other tribes. The Crow Fair is now celebrated yearly on the third weekend of August, with wide participation from other tribes.
Culture.
Subsistence.
The main food source for the Crow was the American bison which was hunted in a variety of ways. Before the use of horses the bison were hunted on foot and required hunters to stalk close to the bison, often with a wolf-pelt disguise, then pursue the animals quickly on foot before killing them with arrows or lances. The horse allowed the Crow to hunt bison more easily as well as hunt more at one time. Riders would panic the herd into a stampede and shoot the targeted animals with arrows or bullets from horseback or lance them through the heart. In addition to bison the Crow also hunted bighorn sheep, mountain goats, deer, elk, bear, and other game. Buffalo meat was often roasted or boiled in a stew with prairie turnips. The rump, tongue, liver, heart, and kidneys all were considered delicacies. Dried bison meat was ground with fat and berries to make "pemmican". In addition to meat, wild edibles were gathered and eaten such as elderberries, wild turnip, and Saskatoon berries.
The Crow often hunted bison by utilizing buffalo jumps. "Where Buffaloes are Driven Over Cliffs at Long Ridge" was a favorite spot for meat procurement by the Crow Indians for over a century, from 1700 to around 1870 when modern weapons were introduced. The Crow used this place annually in the autumn, a place of multiple cliffs along a ridge that eventually sloped to the creek. Early in the morning the day of the jump a medicine man would stand on the edge of the upper cliff, facing up the ridge. He would take a pair of bison hindquarters and pointing the feet along the lines of stones he would sing his sacred songs and call upon the Great Spirit to make the operation a success. After this invocation the medicine man would give the two head drivers a pouch of incense. As the two head drivers and their helpers headed up the ridge and the long line of stones they would stop and burn incense on the ground repeating this process four times. The ritual was intended to make the animals come to the line where the incense was burned, then bolt back to the ridge area.
Habitation and Transportation.
The traditional Crow shelter is the tipi or skin lodge made with bison hides stretched over wooden poles. The Crow are historically known to construct some of the largest tipis. Tipi poles were harvested from the lodgepole pine which acquired its name from its use as support for tipis. Inside the tipi, mattresses and buffalo-hide seats were arranged around the edge, with a fireplace in the center. The smoke from the fire escaped through a hole or smoke-flap in the top of the tipi. At least one entrance hole with collapsible flap allowed entry into the tipi. Often hide paintings adorned the outside and inside of tipis with specific meanings attached to the images. Often specific tipi designs were unique to the individual owner, family, or society that resided in the tipi. Tipis are easily raised and collapsed and are lightweight, which is ideal for nomadic people like the Crow who move frequently and quickly. Once collapsed, the tipi poles are used to create a travois. Travois are a horse-pulled frame structure used by plains Indians to carry and pull belongings as well as small children. Many Crow families still own and use the tipi, especially when traveling. The annual Crow Fair has been described as the largest gathering of tipis in the world.
The most widely used form of transportation used by the Crow was the horse. Horses were acquired through raiding and trading with other Plains nations. People of the northern plains like the Crow mostly got their horses from people from the southern plains such as the Comanche and Kiowa who originally got their horses from the Spanish and southwestern Indians such as the various Pueblo people. The Crow had large horse herds which were among the largest owned by Plains Indians; in 1914 they had approximately thirty to forty thousand head. By 1921 the number of mounts had dwindled to just one thousand due to increased raiding from Crow enemies such as Cheyenne, Sioux, and Blackfeet. Like other plains people the horse was central to the Crow economy and were a highly valuable trade item and were frequently stolen from other tribes to gain wealth and prestige as a warrior. The horse allowed the Crow to become powerful and skilled mounted warriors, being able to perform daring maneuvers during battle including hanging underneath a galloping horse and shooting arrows by holding onto its mane. They also had many dogs; one source counted five to six hundred. Dogs were used as guards and pack animals to carry belongings and pull travois. The introduction of horses into Crow society allowed them to pull heavier loads faster, greatly reducing the number of dogs used as pack animals.
Clothing and Beadwork.
The Crow wore clothing distinguished by gender. Women wore dresses made of deer and buffalo skins, decorated with elk teeth or shells. They covered their legs with leggings during winter and their feet with moccasins. Crow women wore their hair in two braids. Male clothing usually consisted of a shirt, trimmed leggings with a belt, a long breechcloth, and moccasins. Robes made from the furred hide of a bison were often worn in winter. Leggings were either made of animal hide which the Crow made for themselves or made of wool which were highly valued trade items made specifically for Indians in Europe. Their hair was worn long, in some cases reaching the ground. The Crow are famous for often wearing their hair into a pompadour which is often coloured white with paint. Crow men were notable for wearing two hair pipes made from beads on both sides of their hair. Men often wore their hair in two braids wrapped in the fur from beavers or otters. Bear grease is used to give shine to hair. Stuffed birds were often worn in the hair of warriors and medicine men. Like other plains Indians the Crow wear feathers from eagles, crows, owls, and other birds in their hair for symbolic reasons. The Crow wear a variety of headdresses including the famous eagle feather headdress, bison scalp headdress with horns and beaded rim, and split horn headdress. The split horn headdress is made from a single bison horn split in half and polished into two nearly identical horns which are attached to a leather cap and decorated with feathers and beadwork. Traditional clothing worn by the Crow is still worn today with varying degrees of regularity.
The Crow People are well known for their intercut beadwork. They adorned basically every aspect of their lives with these beads, giving special attention to ceremonial and ornamental items. Their clothing, horses, cradles, ornamental and ceremonial gear, in addition to leather cases of all shapes, sizes and uses were decorated in beadwork. They gave reverence to the animals they ate by using as much of it as they could. The leather for their clothing, robes and pouches were created from the skin of buffalo, deer and elk. The work was done by the tribeswomen, with some being considered experts and were often sought by the younger, less experienced women for design and symbolic advice. The Crow are an innovative people and are credited with developing their own style of stitch-work for adhering beads. This stitch, which is contemporarily called the over-lay, is even still also known as the "Crow Stitch". In their beadwork, geometric shapes were primarily used with triangles, diamonds and hour-glass structures being the most prevalent. A wide range of colors were utilized by the Crow, but blues and various shades of pink were the most dominantly used. To intensify or to draw out a certain color or shape, they would surround that figure or color in a white outline.
The colors chosen were not just merely used to be aesthetically pleasing, but rather had a deeper symbolic meaning. Pinks represented the various shades of the rising sun with yellow being the East the origin of the sun's arrival. Blues are symbolic of the sky; red represented the setting sun or the West; green symbolizing mother earth, black the slaying of an enemy and white representing clouds, rain or sleet. Although most colors had a common symbolism, each piece's symbolic significance was fairly subjective to its creator, especially when in reference to the individual shapes. One person's triangle might symbolize a teepee, a spear head to a different individual or a range of mountains to yet another. Regardless of the individual significance of each piece, the Crow People give reverence to the land and sky with the symbolic references found in the various colors and shapes found on their ornamental gear and even clothing.
Some of the clothing that the Crow People decorated with beads included robes, vests, pants, shirts, moccasins and various forms of celebratory and ceremonial gear. In addition to creating a connection with the land, from which they are a part, the various shapes and colors reflected one's standing and achievements. For example if a warrior were to slay, wound or disarm an enemy, he would return with a blackened face. The black color would then be incorporated in the clothing of that man, most likely in his war attire. A beaded robe, which was often given to a bride to be, could take over a year to produce and was usually created by the bride's mother-in-law or another female relative-in-law. These robes were often characterized by a series of parallel horizontal lines, usually consisting of light blue. The lines represented the young women's new role as a wife and mother; also the new bride was encouraged to wear the robe at the next ceremonial gathering to symbolize her addition and welcoming to a new family. In modern times the Crow still often decorate their clothing with intricate bead designs for powwow and everyday clothing.
Gender and kinship system.
The Crow had a matrilineal system. After marriage, the couple was matrilocal (the husband moved to the wife's mother's house upon marriage). Women held a significant role within the tribe.
Crow kinship is a system used to describe and define family members. Identified by Lewis Henry Morgan in his 1871 work "Systems of Consanguinity and Affinity of the Human Family", the Crow system is one of the six major types which he described: Eskimo, Hawaiian, Iroquois, Crow, Omaha, and Sudanese.
Like other Plains tribes, the Crow historically had three defined gender roles: male, female and "baté" (trans female / "two-spirit"). Osh-Tisch was a well-known Crow "baté".
The modern Crow Tribe Apsáalooke Nation.
Geography.
The Crow Indian Reservation in south-central Montana is a large reservation covering approximately 2300000 acre of land area, the fifth-largest Indian reservation in the United States. The reservation is primarily in Big Horn and Yellowstone counties with ceded lands in Rosebud, Carbon, and Treasure Counties. The Crow Indian Reservation's eastern border is the 107th meridian line, except along the border line of the Northern Cheyenne Indian Reservation.
The southern border is from the 107th meridian line west to the east bank of the Big Horn River. The line travels downstream to Bighorn Canyon National Recreation Area and west to the Pryor Mountains and north-easterly to Billings. The northern border travels east and through Hardin, Montana, to the 107th meridian line. The 2000 census reported a total population of 6,894 on reservation lands. Its largest community is Crow Agency.
Government.
Prior to the 2001 Constitution, the Crow Nation was governed by a 1948 Constitution. The former constitution organized the tribe as a General Council (Tribal Council). The General Council in essence held the executive, legislative, and judicial powers of the government, and was composed of all enrolled members of the Crow Nation, provided that females were 18 years or older and males were 21 or older. The General Council was a direct democracy, comparable to that of ancient Athens.
The Crow Nation, or Crow Tribe of Indians, established a three-branch government at a 2001 Council Meeting. The new government is known as the 2001 Constitution. The General Council remains the governing body of the tribe; however, the powers were distributed to a three-branch government. In theory, the General Council is still the governing body of the Crow Nation, yet in reality the General Council has not convened since the establishment of the 2001 Constitution.
The Executive Branch has four officials. These officials are known as the Chairperson, Vice-Chairperson, Secretary, and Vice-Secretary. The Executive Branch officials are also the officials within the Crow Tribal General Council, which has not met since July 15, 2001. These officials established the 2001 Constitution. The Chairman is Darrin Old Coyote.
The Legislative Branch consists of three members from each district on the Crow Indian Reservation. The Crow Indian Reservation is divided into six districts known as The Valley of the Chiefs, Reno, Black Lodge, Mighty Few, Big Horn, and Pryor Districts. The Valley of the Chiefs District is the largest district by population.
The Judicial Branch consists of all courts established by the Crow Law and Order Code and in accordance with the 2001 Constitution. The Judicial Branch has jurisdiction over all matters defined in the Crow Law and Order Code. The Judicial Branch attempts to be a separate and distinct branch of government from the Legislative and Executive Branches of Crow Tribal Government. The Judicial Branch consists of an elected Chief Judge and two Associate Judges. The Crow Court of Appeals, similar to State Court of Appeals, receives all appeals from the lower courts. The Chief Judge of the Crow Nation is Julie Yarlott.
Constitution controversy.
According to the 1948 Constitution, Resolution 63-01,(Please note; in a letter of communication from Phileo Nash, then Commissioner of Indian Affairs, to the B.I.A. Area Director- as stated in the letter And confirmed that 63-01 is an Ordinance in said letter) all constitutional amendments must be voted on by secret ballot or referendum vote. In 2001, major actions were taken by the former Chairperson Birdinground without complying with those requirements. The quarterly council meeting on July 15, 2001 passed all resolutions by voice vote, including the measure to repeal the current constitution and approve a new constitution. An opposition has arisen to challenge the new constitution's validity. The challenge is now in Crow Tribal Courts awaiting a decision.
Critics contend the new constitution is contrary to the spirit of the Crow Nation, as it provides authority for the US Bureau of Indian Affairs (BIA) to approve Crow legislation and decisions. The Crow people have guarded their sovereignty and Treaty Rights. The alleged New Constitution was not voted on to add it to the agenda of the Tribal Council. The former constitution mandated that constitutional changes be conducted by referendum vote, using the secret ballot election method and criteria. In addition, a constitutional change can only be conducted in a specially called election, which was never approved by council action for the 2001 Constitution. The agenda was not voted on or accepted at the council.
The only vote taken at the council was whether to conduct the voting by voice vote or walking through the line. Critics say the Chairman ignored and suppressed attempts to discuss the Constitution. This council and constitutional change was never ratified by any subsequent council action. The Tribal Secretary, who was removed from office by the BirdinGround Administration, was the leader of the opposition. All activity occurred without his signature.
When the opposition challenged, citing the violation of the Constitutional Process and the Right to Vote, the Birdinground Administration sought the approval of the United States Department of the Interior (USDOI), Bureau of Indian Affairs (BIA). The latter stated it could not interfere in an internal tribal affair. The federal court also ruled that the constitutional change was an internal tribal matter.
Leadership.
The seat of government and capital of the Crow Indian Reservation is Crow Agency, Montana.
The Crow Nation has traditionally elected a chairperson of the Crow Tribal Council biennially; however, in 2001, the term of office was extended to four years. The previous chairperson was Carl Venne. The chairperson serves as chief executive officer, speaker of the council, and majority leader of the Crow Tribal Council. The constitutional changes of 2001 created a three-branch government. The chairperson serves as the head of the executive branch, which includes the offices of vice-chairperson, secretary, vice-secretary, and the tribal offices and departments of the Crow Tribal Administration. Notable chairs are Clara Nomee, Edison Real Bird, and Robert "Robie" Yellowtail.
On May 19, 2008, Hartford and Mary Black Eagle of the Crow Nation adopted U.S. Senator (now President) Barack Obama into the tribe on the date of the first visit of a U.S. presidential candidate to the nation. Crow representatives also took part in President Obama's inaugural parade. In 2009 Dr. Joseph Medicine Crow was one of 16 people awarded the Presidential Medal of Freedom.
During the United States federal government shutdown of 2013, the Crow Nation furloughed 316 employees and suspended programs providing health care, bus services and improvements to irrigation.
Popular culture.
Edgar Allan Poe's 1838 novel "The Narrative of Arthur Gordon Pym of Nantucket" features a character, Dirk Peters, who is the son of an Upsaroka (Absaroka) mother and a French father.
The cover of the popular music album "America," which contained the Top Ten song, "Horse With No Name", featured the three group members sitting on the floor in front of a mural of "Eight Crows".
The tribe hosts a large pow wow, rodeo, and parade annually; the 86th Crow Fair was held in Crow Agency from August 17–21, 2006. Called "Baasaxpilue" (to make much noise), it is the largest and most spectacular of Indian celebrations in the northern Plains. The photographer Elsa Spear Byron photographed the Crow Fair from 1911 to the 1950s.
Angus Young, a Crow elder and historian, and professor at Little Big Horn College, was featured on the 2006 installment of the PBS television series "Frontier House".
In the documentary "Native Spirit and the Sun Dance Way" (2007), Thomas Yellowtail, a Crow medicine man and Sun Dance chief for more than 30 years, describes and explains the ancient Sun Dance ceremony, which is sacred to the Crow tribe. In the 1994 film "Legends of the Fall", based on the 1979 novella of the same name by Jim Harrison, actor Gordon Tootoosis spoke Yellowtail's words to examine the preservation of a cultural and spiritual world before the coming of European settlers.
In 2007 Medicine Crow's grandson Joe Medicine Crow appeared on Ken Burns PBS series "The War" (documentary).
The Crow are the main antagonist in the 1972 film Jeremiah Johnson based on the real-life mountain man John Liver-Eating Johnston who, like in the movie, fought against the Crow earning him the nickname "Crow killer". Liver-Eating Johnson later made allies with the Crow as in the film.

</doc>
<doc id="52274" url="http://en.wikipedia.org/wiki?curid=52274" title="Richard Rodgers">
Richard Rodgers

Richard Charles Rodgers (June 28, 1902 – December 30, 1979) was an American composer of music for more than 900 songs and for 43 Broadway musicals. He also composed music for films and television. He is best known for his songwriting partnerships with the lyricists Lorenz Hart and Oscar Hammerstein II. His compositions have had a significant impact on popular music up to the present day, and have an enduring broad appeal.
Rodgers was the first person to win what are considered the top show business awards in television, recording, movies and Broadway—an Emmy, a Grammy, an Oscar, and a Tony—now known collectively as an EGOT. He has also won a Pulitzer Prize, making him one of two people (Marvin Hamlisch is the other) to receive each award.
Biography.
Early life and education.
Born into a prosperous German Jewish family in Arverne, Queens, New York City, Rodgers was the son of Mamie (Levy) and Dr. William Abrahams Rodgers, a prominent physician who had changed the family name from Abrahams. Richard began playing the piano at age six. He attended P.S. 10, Townsend Harris Hall and DeWitt Clinton High School. Rodgers spent his early teenage summers in Camp Wigwam (Waterford, Maine) where he composed some of his first songs.
Rodgers, Lorenz Hart, and later collaborator Oscar Hammerstein II all attended Columbia University. At Columbia, Rodgers joined the Pi Lambda Phi fraternity. In 1921, Rodgers shifted his studies to the Institute of Musical Art (now Juilliard). Rodgers was influenced by composers such as Victor Herbert and Jerome Kern, as well as by the operettas his parents took him to see on Broadway when he was a child.
Career.
Rodgers and Hart.
In 1919, Richard met Lorenz Hart, thanks to Phillip Leavitt, a friend of Richard's older brother. Rodgers and Hart struggled for years in the field of musical comedy, writing several amateur shows. They made their professional debut with the song "Any Old Place With You", featured in the 1919 Broadway musical comedy "A Lonely Romeo". Their first professional production was the 1920 "Poor Little Ritz Girl". Their next professional show, "The Melody Man", did not premiere until 1924.
When he was just out of college Rodgers worked as musical director for Lew Fields. Among the stars he accompanied were Nora Bayes and Fred Allen. Rodgers was considering quitting show business altogether to sell children's underwear, when he and Hart finally broke through in 1925. They wrote the songs for a benefit show presented by the prestigious Theatre Guild, called "The Garrick Gaieties", and the critics found the show fresh and delightful. Only meant to run one day, the Guild knew they had a success and allowed it to re-open later. The show's biggest hit — the song that Rodgers believed "made" Rodgers and Hart — was "Manhattan". The two were now a Broadway songwriting force.
Throughout the rest of the decade, the duo wrote several hit shows for both Broadway and London, including "Dearest Enemy" (1925), "The Girl Friend" (1926), "Peggy-Ann" (1926), "A Connecticut Yankee" (1927), and "Present Arms" (1928). Their 1920s shows produced standards such as "Here in My Arms", "Mountain Greenery", "Blue Room", "My Heart Stood Still" and "You Took Advantage of Me".
With the Depression in full swing during the first half of the 1930s, the team sought greener pastures in Hollywood. The hardworking Rodgers later regretted these relatively fallow years, but he and Hart did write some classic songs and film scores while out west, including "Love Me Tonight" (1932) (directed by Rouben Mamoulian, who would later direct Rodgers' "Oklahoma!" on Broadway), which introduced three standards: "Lover", "Mimi", and "Isn't It Romantic?". Rodgers also wrote a melody for which Hart wrote three consecutive lyrics which either were cut, not recorded or not a hit. The fourth lyric resulted in one of their most famous songs, "Blue Moon". Other film work includes the scores to "The Phantom President" (1932), starring George M. Cohan, "Hallelujah, I'm a Bum" (1933), starring Al Jolson, and, in a quick return after having left Hollywood, "Mississippi" (1935), starring Bing Crosby and W.C. Fields.
In 1935, they returned to Broadway and wrote an almost unbroken string of hit shows that ended only with Hart's death in 1943. Among the most notable are "Jumbo" (1935), "On Your Toes" (1936, which included the ballet "Slaughter on Tenth Avenue", choreographed by George Balanchine), "Babes in Arms" (1937), "I Married an Angel" (1938), "The Boys from Syracuse" (1938), "Pal Joey" (1940), and their last original work, "By Jupiter" (1942). Rodgers also contributed to the book on several of these shows.
Many of the songs from these shows are still sung and remembered, including "The Most Beautiful Girl in the World", "My Romance", "Little Girl Blue", "I'll Tell the Man in the Street", "There's a Small Hotel", "Where or When", "My Funny Valentine", "The Lady Is a Tramp", "Falling in Love with Love", "Bewitched, Bothered and Bewildered", and "Wait till You See Her".
In 1939, he wrote the ballet "Ghost Town" for the Ballet Russe de Monte Carlo, with choreography by Marc Platoff.
Rodgers and Hammerstein.
His partnership with Hart having problems because of the lyricist's unreliability and declining health, Rodgers began working with Oscar Hammerstein II, with whom he had previously written songs (before ever working with Lorenz Hart). Their first musical, the groundbreaking hit, "Oklahoma!" (1943), marked the beginning of the most successful partnership in American musical theatre history. Their work revolutionized the form. What was once a collection of songs, dances and comic turns held together by a tenuous plot became an integrated masterpiece.
The team went on to create four more hits that are among the most popular of all musicals and were each made into successful films: "Carousel" (1945), "South Pacific" (1949, winner of the 1950 Pulitzer Prize for Drama), "The King and I" (1951), and "The Sound of Music" (1959). Other shows include the minor hit, "Flower Drum Song" (1958), as well as relative failures "Allegro" (1947), "Me and Juliet" (1953) and "Pipe Dream" (1955). They also wrote the score to the film "State Fair" (1945) (which was remade in 1962 with Pat Boone), and a special TV musical of "Cinderella" (1957).
Their collaboration produced many well-known songs, including "Oh, What a Beautiful Mornin'", "People Will Say We're in Love", "Oklahoma!" (which also became the state song of Oklahoma), "If I Loved You", "You'll Never Walk Alone", "It Might as Well Be Spring", "Some Enchanted Evening", "Getting to Know You", "My Favorite Things", "The Sound of Music", "Sixteen Going on Seventeen", "Climb Ev'ry Mountain", "Do-Re-Mi", and "Edelweiss", Hammerstein's last song.
Much of Rodgers's work with both Hart and Hammerstein was orchestrated by Robert Russell Bennett. Rodgers composed twelve themes, which Bennett used in preparing the orchestra score for the 26-episode World War II television documentary "Victory at Sea" (1952–53). This NBC production pioneered the "compilation documentary"—programming based on pre-existing footage — and was eventually broadcast in dozens of countries. The melody of the popular song "No Other Love" was later taken from the "Victory at Sea" theme entitled "Beneath the Southern Cross". Rodgers won an Emmy for the music for the ABC documentary "Winston Churchill: The Valiant Years", scored by Eddie Sauter, Hershy Kay, and Robert Emmett Dolan. He composed the theme music, "March of the Clowns", for the 1963–64 television series "The Greatest Show on Earth", which ran for 30 episodes. He also contributed the main-title theme for the 1963–64 historical anthology television series "The Great Adventure".
In 1950, Rodgers and Hammerstein received The Hundred Year Association of New York's Gold Medal Award "in recognition of outstanding contributions to the City of New York." Rodgers, Hammerstein, and Joshua Logan won the Pulitzer Prize for Drama for "South Pacific". Rodgers and Hammerstein had won a special Pulitzer Prize in 1944 for "Oklahoma!".
In 1954, Rodgers conducted the New York Philharmonic Orchestra in excerpts from "Victory at Sea", "Slaughter on Tenth Avenue" and the "Carousel Waltz" for a special LP released by Columbia Records.
Rodgers and Hammerstein musicals earned a total of 35 Tony Awards, 15 Academy Awards, two Pulitzer Prizes, two Grammy Awards, and two Emmy Awards.
After Hammerstein.
After Hammerstein's death in 1960, Rodgers wrote both words and music for his first new Broadway project "No Strings" (1962, which earned two Tony Awards). The show was a minor hit and featured the song, "The Sweetest Sounds".
Rodgers also wrote both the words and music for two new songs used in the film version of "The Sound of Music". (Other songs in that film were from Rodgers and Hammerstein.)
Rodgers went on to work with lyricists Stephen Sondheim ("Do I Hear a Waltz?"), a protégé of Hammerstein; Martin Charnin ("Two by Two," "I Remember Mama"); and Sheldon Harnick ("Rex").
At its 1978 commencement ceremonies, Barnard College awarded Rodgers its highest honor, the Barnard Medal of Distinction.
Death and legacy.
Rodgers died in 1979 at age 77 after surviving cancer of the jaw, a heart attack, and a laryngectomy. He was cremated and his ashes were scattered at sea.
In 1990, the 46th Street Theatre was renamed "The Richard Rodgers Theatre" in his memory. In 1999, Rodgers and Hart were each commemorated on United States postage stamps. 2002 was the centennial year of Rodgers's birth, celebrated worldwide with books, retrospectives, performances, new recordings of his music, and a Broadway revival of "Oklahoma!". The BBC Proms that year devoted an entire evening to Rodgers' music including a concert performance of "Oklahoma!"
Several American schools are named after Richard Rodgers.
Alec Wilder wrote the following about Rodgers:
 Of all the writers whose songs are considered and examined in this book, those of Rodgers show the highest degree of consistent excellence, inventiveness, and sophistication...[A]fter spending weeks playing his songs, I am more than impressed and respectful: I am astonished.
Richard Rodgers is a member of the American Theatre Hall of Fame.
Relationship with performers.
Rosemary Clooney recorded a version of "Falling In Love With Love" by Rodgers, using a swing style. After the recording session Richard Rodgers told her pointedly that it should be sung as a waltz. The 1961 doo-wop arrangement of the Rodgers and Hart song "Blue Moon" by The Marcels so incensed Rodgers that he took out full page newspaper ads urging people not to buy it. His efforts were unsuccessful as it reached #1 on the charts. After Doris Day recorded "I Have Dreamed" in 1961, he wrote to her and her arranger, James Harbert, that theirs was the most beautiful rendition of his song he had ever heard.
After Peggy Lee recorded her version of "Lover", a Rodgers song with a dramatically different arrangement from that originally conceived by him, Rodgers said, "I don't know why Peggy picked on me, she could have fucked up "Silent Night". Mary Martin said that Richard Rodgers composed songs for her for "South Pacific", knowing she had a small vocal range, and the songs generally made her look her best. She also said that Rodgers and Hammerstein listened to all her suggestions and she worked extremely well with them. Rodgers & Hammerstein wanted Doris Day for the lead in the film version of "South Pacific" and she reportedly wanted the part. They discussed it with her, but after her manager/husband, Martin Melcher, would not budge on his demand for a high salary for her, the role went to Mitzi Gaynor.
Personal life.
In 1930, Rodgers married Dorothy Belle Feiner (1909 - 1992). Their daughter, Mary (1931 - 2014), was the composer of "Once Upon a Mattress" and an author of children's books. The Rodgerses later lost a daughter at birth, but another daughter, Linda, was born in the 1930s.
Mary Rodgers' son and Richard Rodgers' grandson, Adam Guettel (b.1964), also a musical theatre composer, won Tony Awards for Best Score and Best Orchestrations for "The Light in the Piazza" in 2005.
Peter Melnick (b. 1958), Linda Rodgers' son and thus another grandson to Richard Rodgers, is the composer of "Adrift In Macao," which debuted at the Philadelphia Theatre Company in 2005 and was produced Off-Broadway in 2007.

</doc>
<doc id="52275" url="http://en.wikipedia.org/wiki?curid=52275" title="Piegan Blackfeet">
Piegan Blackfeet

The Piegan (Blackfoot: "Piikáni") are an Algonquian people from the North American Great Plains. They were the largest of three Blackfoot-speaking groups that made up the Blackfoot Confederacy with the Siksika and the Kainai, and dominated much of the northern plains during the nineteenth century. In the nineteenth century after their homelands were divided between Canada and the United States of America, the Piegan people were forced to sign treaties with one of those two countries, settle in reservations on one side or the other of the border, and be enrolled in one of two government-like bodies sanctioned by North American nation-states. These two successor groups are the Blackfeet Nation a "federally-recognized tribe" in Montana, USA and the Piikani Nation, a recognized "Indian band" in Alberta, Canada.
Today many Piegan live on the Blackfeet Reservation in northwestern Montana, with tribal headquarters in Browning. There were 32,234 Blackfeet counted by the 1990 US census.
Terminology.
The "Piegan" (also known as the Pikuni, Pikani, and Piikáni) are one of the three original "tribes" of the Blackfoot Confederacy (a "tribe" here refers to an ethnic or cultural group with a shared name and identity). The Piegan are closely related to the Kainai Nation (also known as the "Blood Tribe"), and the Siksika Nation (also called the "Blackfoot Nation"); together they are sometimes collectively referred to as "the Blackfoot" or "the Blackfoot Confederacy". Ethnographic literature most commonly uses "Blackfoot people", and Canadian Blackfoot people use the singular Blackfoot. The tribal governments and the US government use the term "Blackfeet", as in Blackfeet Indian Reservation and Blackfeet Nation, as used on their official tribe website. The term "Siksika", derived from "Siksikáíkoan" (a Blackfoot person), may also be used as self-identification. In English, an individual may say, "I am Blackfoot" or "I am a member of the Blackfeet tribe."
Traditionally, Plains peoples were divided in to "bands": groups of families who migrated together for hunting and defence. The bands of the Piegan, as given by Grinnell, are : Ahahpitape, Ahkaiyikokakiniks, Kiyis, Sikutsipmaiks, Sikopoksimaiks, Tsiniksistsoyiks, Kutaiimiks, Ipoksimaiks, Silkokitsimiks, Nitawyiks, Apikaiviks, Miahwahpitsiks, Nitakoskitsipupiks, Nitikskiks, Inuksiks, Miawkinaiyiks, Esksinaitupiks, Inuksikahkopwaiks, Kahmitaiks, Kutaisotsiman, Nitotsiksisstaniks, Motwainaiks, Mokumiks, and Motahtosiks. Hayden gives also Susksoyiks.
Since the 1870s, Piegan people have been members of either the Blackfeet Nation in the US or the Piikani Nation (Northern Piegan) in Canada. They are closely related to the Kainai (also known as the Blood) and the Siksika nations. All speak dialects of the Blackfoot language and are sometimes collectively referred to as the Blackfoot or the Blackfoot Confederacy. Ethnographic literature most commonly uses "Blackfoot people", and Canadian Blackfoot people use the singular Blackfoot. The US and tribal governments officially use "Blackfeet", as in Blackfeet Indian Reservation and Blackfeet Nation.
Relations and history.
In 2014, researchers reported on their sequencing of the DNA of a 12,500+-year-old infant skeleton in west-central Montana, found in close association with several Clovis culture artifacts. It showed strong affinities with all existing Native American populations.
There is preliminary evidence of human habitation in north central Montana, which became part of the Piegan territory, that may date as far back as 5000 years. with evidence of substantial use of buffalo jumps dating to AD 300. The Piegan people may be more recent arrivals in the area, as there is strong evidence that their Algonquian-speaking ancestors migrated southwest from what today is Saskatchewan beginning about 1730.
The linguistic connection of the Blackfoot language to others in the Algonquian-language family indicate that the Blackfoot had long lived in an area west of the Great Lakes. Like others in this language family, the Blackfoot language is agglutinative.
The people practiced some agriculture and were partly nomadic. They moved westward after they adopted use of horses and guns, which gave them a larger range for bison hunting. They became part of the Plains Indians cultures in the early 19th century. According to tribal oral histories, humans lived near the Rocky Mountain Front for thousands of years before European contact. The Blackfoot creation story is set near Glacier National Park in an area now known as the "Badger-Two Medicine".
The introduction of the horse is placed at about 1730, when raids by the Shoshoni prompted the Piegan to obtain horses from the Kutenai, Salish and Nez Perce. Early accounts of contact with European-descended people date to the late eighteenth century. The fur trader James Gaddy and the Hudson's Bay Company explorer David Thompson, the first Whites to see Bow River, camped with a group of Piegan during the 1787-1788 winter.
In 1900, there were an estimated 20,000 Blackfoot. In the early 21st century, there are more than 35,000. The population was at times dramatically lower when the Blackfeet people suffered declines due to infectious disease epidemics. They had no natural immunity to Eurasian diseases, and the 1837 smallpox epidemic on the Plains killed 6,000 Blackfeet. They also suffered from starvation because of disruption of food supplies and war. When the last buffalo hunt failed in 1882, that year became known as the starvation year. The Blackfeet had controlled large portions of Alberta and Montana. Today the Blackfeet Reservation in Montana is the size of Delaware, and the three Blackfoot reserves in Alberta have a much smaller area.
The Blackfeet hold belief "in a sacred force that permeates all things, represented symbolically by the sun whose light sustains all things."
The Blackfoot do not have well documented male Two-Spirits, but they do have "manly-hearted women". These were recorded as acting in many of the social roles of men. This includes a willingness to sing alone, usually considered "immodest", and using a men's singing style.
In 1858 the Piegan in the United States were estimated to number 3,700. Three years later, Hayden estimated the population at 2,520. In 1906 there were 2,072 under the Blackfeet agency in Montana, and 493 under the Piegan band in Alberta, Canada. In the 2010 census, 105,304 people claimed Piegan Blackfeet ancestry, 27,279 of them full-blooded.

</doc>
<doc id="52277" url="http://en.wikipedia.org/wiki?curid=52277" title="Jansenism">
Jansenism

Jansenism was a Catholic theological movement, primarily in France, that emphasized original sin, human depravity, the necessity of divine grace, and predestination. The movement originated from the posthumously published work of the Dutch theologian Cornelius Jansen, who died in 1638. It was first popularized by Jansen's friend Abbot Jean Duvergier de Hauranne, of Saint-Cyran-en-Brenne Abbey, and after Duvergier's death in 1643, was led by Antoine Arnauld. Through the 17th and into the 18th centuries, Jansenism was a distinct movement within the Catholic Church. The theological centre of the movement was the convent of Port-Royal Abbey, Paris, which was a haven for writers including Duvergier, Arnauld, Pierre Nicole, Blaise Pascal, and Jean Racine.
Jansenism was opposed by many in the Catholic hierarchy, especially the Jesuits. Although the Jansenists identified themselves only as rigorous followers of Augustine of Hippo's teachings, Jesuits coined the term "Jansenism" to identify them as having Calvinist affinities. The apostolic constitution "Cum occasione" promulgated by Pope Innocent X in 1653, condemned five cardinal doctrines of Jansenism as heresy—especially the relationship between human 
free will and efficacious grace, wherein the teachings of Augustine, as presented by the Jansenists, contradicted the teachings of the Jesuit School. Jansenist leaders endeavored to accommodate the pope's pronouncements while retaining their uniqueness, and enjoyed a measure of peace in the late 17th century under Pope Clement IX. However, further controversy led to the apostolic constitution "Unigenitus Dei Filius", promulgated by Pope Clement XI in 1713, 
Origins.
The origins of Jansenism lie in the friendship of Jansen and Duvergier, who met in the early 17th century when both were studying theology at the University of Leuven. Duvergier was Jansen's patron for a number of years, getting Jansen a job as a tutor in Paris in 1606. Two years later, he got Jansen a position teaching at the bishop's college in Duvergier's hometown of Bayonne. The duo studied the Church Fathers together, with a special focus on the thought of Augustine of Hippo, until both left Bayonne in 1617.
Duvergier became abbot of Saint Cyran Abbey in Brenne and was known as the "Abbé de Saint-Cyran" for the rest of his life. Jansen returned to the University of Leuven, where he completed his doctorate in 1619 and was named professor of exegesis. Jansen and Duvergier continued to correspond about Augustine, especially Augustine's teachings on grace. Upon the recommendation of King Philip IV of Spain, Jansen was consecrated as bishop of Ypres in 1636.
Jansen died in a 1638 epidemic. On his deathbed, he committed a manuscript to his chaplain, ordering him to consult with Libert Froidmont, theology professor at Leuven, and Henri Calenus, canon at the metropolitan church, and to publish the manuscript if they agreed it should be published, adding "If, however, the Holy See wishes any change, I am an obedient son, and I submit to that Church in which I have lived to my dying hour. This is my last wish."
This manuscript, published in 1640 as "Augustinus", expounded Augustine's system and formed the basis for the subsequent Jansenist Controversy. It consisted of three volumes:
Jansenist theology.
Even before the publication of "Augustinus", Duvergier publicly preached Jansenism. Jansen emphasized a particular reading of Augustine's idea of efficacious grace which stressed that only a certain portion of humanity were predestined to be saved. Jansen insisted that the love of God was fundamental, and that only perfect contrition, and not imperfect contrition (or attrition) could save a person (and that, in turn, only an efficacious grace could tip that person toward God and such a contrition). This debate on the respective roles of contrition and attrition, which had not been settled by the Council of Trent (1545–1563), was one of the motives of the imprisonment in May 1638 of Duvergier, the first leader of Port-Royal, by order of Cardinal Richelieu. Duvergier was not released until after Richelieu's death in 1642, and he died shortly thereafter, in 1643.
Jansen also insisted on justification by faith, although he did not contest the necessity of revering saints, of confession, and of frequent Communion. Jansen's opponents (mainly Jesuits) condemned his teachings for their alleged similarities to Calvinism (though, unlike Calvinism, Jansen rejected the doctrine of assurance and taught that even the justified could lose their salvation). Blaise Pascal's "Écrits sur la grâce", attempted to conciliate the contradictory positions of Molinists and Calvinists by stating that both were partially right: Molinists, who claimed God's choice concerning a person's sin and salvation was "a posteriori" and contingent, while Calvinists claimed that it was "a priori" and necessary. Pascal himself claimed that Molinists were correct concerning the state of humanity before the Fall, while Calvinists were correct regarding the state of humanity after the Fall.
The heresy of Jansenism, as stated by subsequent Roman Catholic doctrine, lay in denying the role of free will in the acceptance and use of grace. Jansenism asserts that God's role in the infusion of grace cannot be resisted and does not require human assent. Catholic doctrine, in the "Catechism of the Catholic Church", is that "God's free initiative demands man's free response,"—that is, humans freely "assent or refuse" God's gift of grace.
Controversy and papal condemnation: 1640–1653.
"Augustinus" was widely read in theological circles in France, Belgium, and the Netherlands in 1640, and a new edition quickly appeared in Paris under the approbation of ten professors at the College of Sorbonne (the theological college of the University of Paris).
However, on August 1, 1642, the Holy Office issued a decree condemning "Augustinus" and forbidding its reading. In 1642, Pope Urban VIII followed up with a papal bull entitled "In eminenti", which condemned "Augustinus" because it was published in violation of the order that no works concerning grace should be published without the prior permission of the Holy See; and renewed the censures by Pope Pius V, in "Ex omnibus afflictionibus" in 1567, and Pope Gregory XIII, of several propositions of Baianism which were repeated in "Augustinus".
In 1602, Marie Angélique Arnauld become abbess of Port-Royal-des-Champs, a Cistercian convent in Magny-les-Hameaux. There, she reformed discipline after a conversion experience in 1608. In 1625, most of the nuns moved to Paris, forming the convent of "Port-Royal de Paris", which from then on was commonly known simply as Port-Royal. In 1634, Duvergier had become the spiritual adviser of Port-Royal-des-Champs and good friend of Angélique Arnauld; he convinced her of the rightness of Jansen's opinions. The two convents thus became major strongholds of Jansenism. Under Angélique Arnauld, later with Duvergier's support, Port-Royal-des-Champs developed a series of elementary schools, known as the "Little Schools of Port-Royal" ("Les Petites-Écoles de Port-Royal"); the most famous product of these schools was the playwright Jean Racine.
Through Angélique Arnauld, Duvergier had met her brother, Antoine Arnauld, and brought him to accept Jansen's position in "Augustinus". Following Duvergier's death in 1643, Antoine Arnauld became the chief proponent of Jansenism. That same year he published "De la fréquente Communion" ("On Frequent Communion") which presented Jansen's ideas in a way more accessible to the public (e.g., it was written in the vernacular, whereas "Augustinus" was written in Latin). The book focused on a related topic in the dispute between Jesuits and Jansenists. The Jesuits encouraged Catholics, including those struggling with sin, to receive Holy Communion frequently, arguing that Christ instituted it as a means to holiness for sinners, and stating that the only requirement for receiving Communion (apart from baptism) was that the communicant be free of mortal sin at the time of reception. The Jansenists, in line with their deeply pessimistic theology, discouraged frequent Communion, arguing that a high degree of perfection, including purification from attachment to venial sin, was necessary before approaching the Sacrament.
The faculty of the College of Sorbonne formally accepted the bull "In eminenti" in 1644, and Cardinal Jean François Paul de Gondi, archbishop of Paris, formally proscribed "Augustinus"; the work nevertheless continued to circulate.
The Jesuits then attacked the Jansenists, charging them with heresy similar to Calvinism.
Arnauld answered with "Théologie morale des Jésuites" ("Moral Theology of the Jesuits").
The Jesuits then designated Nicolas Caussin (former confessor to Louis XIII) to write "Réponse au libelle intitulé La Théologie morale des Jésuites" ("Response to the libel titled Moral Theology of the Jesuits") in 1644.
Another Jesuit response was "Les Impostures et les ignorances du libelle intitulé: La Théologie Morale des Jésuites" ("The impostures and ignorance of the libel titled Moral Theology of the Jesuits") by François Pinthereau, under the pseudonym of "abbé de Boisic", also in 1644. Pinthereau also wrote a critical history of Jansenism, "La Naissance du Jansénisme découverte à Monsieur le Chancelier" ("The Birth of Jansenism Revealed to the Chancellor") in 1654.
During the 1640s, Duvergier's nephew, Martin de Barcos, who was once a theology student under Jansen, wrote several works defending Duvergier.
In 1649, Nicolas Cornet, syndic of the Sorbonne, frustrated by the continued circulation of "Augustinus", drew up a list of five propositions from "Augustinus" and two propositions from "De la fréquente Communion" and asked the Sorbonne faculty to condemn the propositions. Before the faculty could do so, the "Parlement de Paris" intervened and forbade the faculty to consider the propositions. The faculty then submitted the propositions to the Assembly of the French clergy in 1650, which submitted the matter to Pope Innocent X. Eleven bishops opposed this and asked Innocent X to appoint a commission similar to the "Congregatio de Auxiliis" to resolve the situation. Innocent X agreed to the majority's request, but in an attempt to accommodate the view of the minority, appointed an advisory committee consisting of five cardinals and thirteen consultors to report on the situation. Over the next two years, this commission held 36 meetings including 10 presided by Innocent X.
The supporters of Jansenism on the commission drew up a table with three heads: the first listed the Calvinist position (which were condemned as heretical), the second listed the Pelagian/Semipelagian position (as taught by the Molinists), and the third listed the correct Augustinian position (according to the Jansenists).
Jansenism's supporters suffered a decisive defeat when the apostolic constitution "Cum occasione" was promulgated by Innocent X in 1653 which condemned the following five propositions:
Formulary controversy.
Background: 1654–1664.
Antoine Arnauld condemned the five propositions listed in "Cum occasione". However, he contended that "Augustinus" did not argue in favour of the five propositions condemned as heretical in "Cum occasione". Rather, he argued that Jansen intended his statements in "Augustinus" in the same sense that Augustine of Hippo had offered his opinions, and, Arnauld equivocated, that since Innocent X would certainly not have wished to condemn Augustine's opinions, Innocent X had not condemned Jansen's actual opinions.
Replying to Arnauld, in 1654, 38 French bishops condemned Arnauld's position to the pope. Opponents of Jansenism in the church refused absolution to Roger du Plessis, duc de Liancourt for his continued protection of the Jansenists. In response to this onslaught, Arnauld articulated a distinction as to how far the Church could bind the mind of a Catholic. He argued that there is a distinction between "de jure" and "de facto": that a Catholic was obliged to accept the Church's opinion as to a matter of law (i.e., as to a matter of doctrine) but not as to a matter of fact. Arnauld argued that, while he agreed with the doctrine propounded in "Cum occasione", he was not bound to accept the pope's determination of fact as to what doctrines were contained in Jansen's work.
In 1656, the theological faculty at the Sorbonne moved against Arnauld. This was the context in which Blaise Pascal wrote his famous "Lettres provinciales" in defense of Arnauld's position in the dispute at the Sorbonne, and denouncing the "relaxed morality" of Jesuitism (However, unlike Arnauld, Pascal did not accede to "Cum occasione" but believed that the condemned doctrines were orthodox. Nevertheless, he emphasised Arnauld's distinction about matters of doctrine vs. matters of fact.) The Letters were also scathing in their critique of the casuistry of the Jesuits, echoing Arnauld's "Théologie morale des Jésuites".
However, Pascal did not convince the Sorbonne's theological faculty, which voted 138–68 to degrade Arnauld together with 60 other theologians from the faculty. Later that year, the French Assembly of the Bishops voted to condemn Arnauld's distinction of the pope's ability to bind the mind of believers in matters of doctrine but not in matters of fact; they asked Pope Alexander VII to condemn Arnauld's proposition as heresy. Alexander VII responded, in the apostolic constitution "Ad sanctam beati Petri sedem" promulgated in 1656, that "We declare and define that the five propositions have been drawn from the book of Jansenius entitled "Augustinus", and that they have been condemned in the sense of the same Jansenius and we once more condemn them as such."
In 1657, relying on "Ad sanctam beati Petri sedem", the French Assembly of the Clergy drew up a formula of faith condemning Jansenism and declared that subscription to the formula was obligatory. Many Jansenists remained firmly committed to Arnauld's proposition; they condemned the propositions in "Cum occasione" but disagreed that the propositions were contained in "Augustinus". In retaliation, Gondi interdicted the convent of Port Royal from receiving the Sacraments. In 1660, the elementary schools run by Port-Royal-des-Champs were closed by bull, and in 1661, the monastery at Port-Royal-des-Champs was forbidden to accept new novices, which guaranteed the convent would eventually die out.
Formulary: 1664.
Four bishops sided with Port-Royal, arguing that the Assembly of the French clergy could not command French Catholics to subscribe to something which was not required by the pope. At the urging of several bishops, and at the personal insistence of King Louis XIV, Pope Alexander VII sent to France the apostolic constitution "Regiminis Apostolici" in 1664 which required, according to the "Enchiridion symbolorum", "all ecclesiastical personnel and teachers" to subscribe to an included formulary, the "Formula of Submission for the Jansenists".(n. 2020)
Formulary controversy: 1664–1669.
The "Formula of Submission for the Jansenists" was the basis of the Formulary Controversy. Many Jansenists refused to sign it; while some did sign, they made it known that they were agreeing only to the doctrine (questions of law "de jure"), not the allegations asserted by the bull (questions of fact "de facto"). The latter category included the four Jansenist-leaning bishops, who communicated the bull to their flocks along with messages which maintained the distinction between doctrine and fact. This angered both Louis XIV and Alexander VII. Alexander VII commissioned nine French bishops to investigate the situation.
Alexander VII died in 1667 before the commission concluded its investigation and his successor, Pope Clement IX, initially appeared willing to continue the investigation of the nine Jansenist-leaning bishops. However, in France, Jansenists conducted a campaign arguing that allowing a papal commission of this sort would be ceding the traditional liberties of the Gallican Church, thus playing on traditional French opposition to ultramontanism. They convinced one member of the cabinet (Lyonne) and nineteen bishops of their position, these bishops argued, in a letter to Clement IX, that the infallibility of the Church applied only to matters of revelation, and not to matters of fact. They asserted that this was the position of Caesar Baronius and Robert Bellarmine. They also argued, in a letter to Louis XIV, that allowing the investigation to continue would result in political discord.
Under these circumstances, the papal nuncio to France recommended that Clement IX accommodate the Jansenists. Clement agreed, and appointed César d'Estrées, bishop of Laon, as mediator in the matter. D'Estrées convinced the four bishops: Arnauld, Choart de Buzenval, Caulet, and Pavillon, to sign the "Formula of Submission for the Jansenists" (though it seems they may have believed that signing the formulary did not mean assent to the matters of fact it contained). The pope, initially happy that the four bishops had signed, became angry when he was informed that they had done so with reservations. Clement IX ordered his nuncio to conduct a new investigation. Reporting back, the nuncio declared: "they have condemned and caused to be condemned the five propositions with all manner of sincerity, without any exception or restriction whatever, in every sense in which the Church has condemned them". However, he reported that the four bishops continued to be evasive as to whether they agreed with the pope as to the matter of fact. In response, Clement IX appointed a commission of twelve cardinals to further investigate the matter. This commission determined that the four bishops had signed the formula in a less than entirely sincere manner, but nevertheless recommended that the matter should be dropped in order to forestall further divisions in the Church. The pope agreed and thus issued four briefs, declaring the four bishops' agreement to the formula was acceptable, thus instituting the "Peace of Clement IX" (1669–1701).
Case of Conscience and aftermath: 1701–1709.
Although the Peace of Clement IX was a lull in the public theological controversy, a number of clergy remained attracted to Jansenism. Three major groups were:
The quasi-Jansenists served as protectors of the "duped Jansenists" and the "fins Jansénistes".
The tensions generated by the continuing presence of these elements in the French church came to a head in the Case of Conscience of 1701. The case involved the question of whether or not absolution should be given to a cleric who refused to affirm the infallibility of the Church in matters of fact (even though he did not preach against it but merely maintained a "respectful silence"). A provincial conference, consisting of forty theology professors from the Sorbonne, headed by Noël Alexandre, declared that the cleric should receive absolution.
The publication of this "Case of Conscience" provoked outrage among the anti-Jansenist elements in the Catholic Church. The decision given by the scholars was condemned by several French bishops; by Cardinal Louis Antoine de Noailles, archbishop of Paris; by the theological faculties at Leuven, Douai, and eventually Paris; and, finally, in 1703, by Pope Clement XI. The scholars who had signed the Case of Conscience now backed away, and all of the signatories withdrew their signatures and the theologian who had championed the result of the Case of Conscience, Nicolas Petitpied, was expelled from the Sorbonne.
Louis XIV and his grandson, Philip V of Spain, now asked the pope to issue a papal bull condemning the practice of maintaining a respectful silence as to the issue of the infallibility of the Church in matters of dogmatic fact.
The pope obliged, issuing the apostolic constitution "Vineam Domini Sabaoth", dated July 16, 1705. At the subsequent Assembly of the French Clergy, all those present, except P.-Jean-Fr. de Percin de Montgaillard, bishop of Saint-Pons, voted to accept "Vineam Domini Sabaoth" and Louis XIV promulgated it as binding law in France.
Louis also sought the dissolution of Port-Royal-des-Champs, the stronghold of Jansenist thought, and this was achieved in 1708, when the pope issued a bull dissolving Port-Royal-des-Champs. The remaining nuns were forcibly removed in 1709 and dispersed among various other French convents and the buildings were razed in 1709. The convent of Port-Royal Abbey, Paris, remained in existence until it was closed in the general dechristianisation of France during the French Revolution.
Case of Quesnel.
Pasquier Quesnel had been a member of the Oratory of Saint Philip Neri in Paris from 1657 until 1681, when he was expelled for Jansenism. He sought the protection of Pierre du Cambout de Coislin, bishop of Orléans, who harbored Quesnel for four years, at which point Quesnel joined Antoine Arnauld in Brussels, Flanders. In 1692, Quesnel published "Réflexions morales sur le Nouveau Testament", a devotional guide to the New Testament which laid out the Jansenist position in strong terms. Following Arnauld's death in 1694, Quesnel was widely regarded as the leader of the Jansenists. In 1703, Quesnel was imprisoned by Humbertus Guilielmus de Precipiano, archbishop of Mechelen, but escaped several months later and lived in Amsterdam for the remainder of his life.
"Réflexions morales" did not initially arouse controversy; in fact, it was approved for publication by Félix Vialart de Herse, bishop of Châlons-sur-Marne, and recommended by Noailles. However, in the years that followed, several bishops became aware of the book's Jansenist tendencies and issued condemnations: Joseph-Ignace de Foresta, bishop of Apt, in 1703; Charles-Béningne Hervé, bishop of Gap, in 1704; and both François-Joseph de Grammont, bishop of Besançon, and Édouard Bargedé, bishop of Nevers, in 1707. When the Holy Office drew the "Réflexions morales" to the attention of Clement XI, he issued the papal brief "Universi dominici" (1708), proscribing the book for "savouring of the Jansenist heresy"; as a result, in 1710, Jean-François de l'Escure de Valderil, bishop of Luçon, and Étienne de Champflour, bishop of La Rochelle, forbade the reading of the book in their dioceses.
 The result was the apostolic constitution "Unigenitus Dei Filius", promulgated by Pope Clement XI on September 8, 1713. It was written with the contribution of Gregorio Selleri, a lector at the College of Saint Thomas, the future Pontifical University of Saint Thomas Aquinas, "Angelicum", and later Master of the Sacred Palace, fostered the condemnation of Jansenism by condemning 101 propositions from the "Réflexions morales" of Quesnel as heretical, and as identical with propositions already condemned in the writings of Jansen.
Those Jansenists who accepted "Unigenitus Dei Filius" became known as "Acceptants".
After examining the 101 propositions condemned by "Unigenitus Dei Filius", Noailles determined that as set out in "Unigenitus Dei Filius" and apart from their context in the "Réflexions morales", some of the propositions condemned by "Unigenitus Dei Filius" were in fact orthodox. He therefore refused to accept the apostolic constitution and instead sought clarifications from the pope.
In the midst of this dispute, Louis XIV died in 1715, and the government of France was taken over by Philippe II, Duke of Orléans, regent for the 5-year-old Louis XV of France. Unlike Louis XIV, who had stood solidly behind "Unigenitus Dei Filius", Philippe II expressed ambivalence during the "Régence" period. With the change in political mood, three theological faculties which had previously voted to accept "Unigenitus Dei Filius" – Paris, Nantes, and Reims – voted to rescind their acceptance.
In 1717, the bishops were joined by hundreds of French priests, monks and nuns, and were supported by the "parlements". In 1718, Clement XI responded vigorously to this challenge to his authority by issuing the bull "Pastoralis officii" by which he excommunicated everyone who had called Far from disarming the French clergy, many of whom were now advocating conciliarism, the clergy who had appealed "Unigenitus Dei Filius" to a general council, now In total, one cardinal, 18 bishops, and 3,000 clergy of France However, the majority of clergy in France (four cardinals, 100 bishops, 100,000 clergymen) stood by the pope. The schism carried on for some time, however, and it was not until 1728 that Noailles submitted to the pope.
Factionalism.
Jansenism persisted in France for many years but split "into antagonistic factions" in the late-1720s.
One faction developed from the "convulsionnaires" of Saint-Médard, who were religious pilgrims who went into frenzied religious ecstasy at the grave of François de Pâris, a Jansenist deacon in the parish cemetery of Saint-Médard in Paris. The connection between the larger French Jansenist movement and the smaller, more radical "convulsionnaire" phenomenon is difficult to state with precision. Brian Strayer noted, in "Suffering Saints", almost all "convulsionnaires" were Jansenists, but very few Jansenists embraced the "convulsionnaire" phenomenon.(p236)
"The format of their seances changed perceptibly after 1732," according to Strayer. "Instead of emphasizing prayer, singing, and healing miracles, believers now participated in 'spiritual marriages' (which occasionally bore earthly children), encouraged violent convulsions [...] and indulged in the "secours" (erotic and violent forms of torture), all of which reveals how neurotic the movement was becoming." The movement descended into brutal cruelties that "clearly had sexual overtones" in their practices of penance and mortification of the flesh. In 1735 the "parlements" regained jurisdiction over the convulsionary movement which changed into an underground movement of clandestine sects. The next year "an alleged plot" by "convulsionnaire" revolutionaries to overthrow the "parlements" and assassinate Louis XV was thwarted. The "Augustinian "convulsionnaires"" then absconded from Paris to avoid police surveillance. This "further split the Jansenist movement."(pp257–265)
According to Strayer, by 1741 the leadership was "dead, exiled, or imprisoned," and the movement divided into three groups. The police role increased and the "parlements" role decreased "in the social control of Jansenism" but cells continued engaging in seances, torture, and apocalyptic and treasonous rhetoric. By 1755 there were less than 800 "convulsionnaires" in France. In 1762 the "parlements" criminalized some of their practices "as 'potentially dangerous' to human life."(pp266–269, 272) The last crucifixion was documented in 1788.(p282)
Legacy.
"Unigenitus Dei Filius" marks the official end of toleration of Jansenism in the Church in France, though quasi-Jansenists would occasionally stir in the following decades. By the mid-18th century, Jansenism proper had totally lost its battle to be a viable theological position within Catholicism. However, certain ideas tinged with Jansenism remained in circulation for much longer; in particular, the Jansenist idea that Holy Communion should be received very infrequently, and that reception required much more than freedom from mortal sin, remained influential until finally condemned by Pope Pius X, who endorsed frequent communion, as long as the communicant was free of mortal sin, in the early 20th century.
In 1677, a pro-Baianism faction from the theological faculty at Louvain submitted 116 propositions of moral laxity for censure to Pope Innocent XI, who selected 65 propositions from the submission and "limited himself to condemning the deviations of moral doctrine."(p. 466) 
On the other hand, through the Holy Office, those 65 propositions in 1679,(nn. 2101–2167) "without naming the probabilism prevalent in Jesuit circles." All 65 propositions were censured and prohibited "as at least scandalous and pernicious in practice."(n. 2167)
Jansenism was a factor in the formation of the independent Old Catholic Church of the Netherlands from 1702 to 1723, and 
In Quebec, Canada, in the 1960s, many people rejected the Church, and many of its institutions were secularized. 
Further reading.
·Strayer .E. Brain, ‘’Suffering Saints: Jensenits and Convulsionaries in France, 1640-1799’’, (Eastborne , Sussex Academic Press, 2008)
·Crichton. D. J , ‘Saints or Sinners? : Jansenism and Jansenisers in Seventeenth Century France’’, (Dublin, Veritas Publications, 1996)
·Swann Julian, ‘Politics and the Parliament of Paris under Louis XV 1754-1774’ , ( Cambridge, Cambridge University Press, 1995 )
·Doyle William, ‘Jansenism: Catholic Resistance to Authority from the Reformation to the French Revolution - Studies in European History’, (Basingstoke, Macmillan Press Ltd,2000)

</doc>
<doc id="52278" url="http://en.wikipedia.org/wiki?curid=52278" title="James Callaghan">
James Callaghan

Leonard James Callaghan, Baron Callaghan of Cardiff, KG, PC (27 March 1912 – 26 March 2005) was the Prime Minister of the United Kingdom from 1976 to 1979 and Leader of the Labour Party from 1976 to 1980. Callaghan is to date the only politician in history to have served in all four of the "Great Offices of State", having been Chancellor of the Exchequer from 1964 to 1967, Home Secretary from 1967 to 1970, and Foreign Secretary from 1974, until his appointment as Prime Minister in 1976.
Callaghan's period as Chancellor of the Exchequer coincided with a turbulent period for the British economy, during which he had to wrestle with a balance of payments deficit and speculative attacks on the pound sterling. In November 1967, the Government was forced to devalue the pound sterling despite having previously denied that this would be necessary. Callaghan offered to resign over the matter, but instead swapped ministerial positions with Roy Jenkins to become Home Secretary. In that capacity, Callaghan took the decision to use the Army to support the police in Northern Ireland, after a request from the Northern Ireland Government.
After Labour lost the 1970 election, Callaghan played a key role in the Shadow Cabinet before returning to office as Foreign Secretary in 1974, taking responsibility for renegotiating the terms of Britain's membership of the European Economic Community, and supporting a "Yes" vote in the 1975 referendum for the UK to remain in the EEC. When Harold Wilson suddenly resigned as Prime Minister in 1976, Callaghan defeated five other candidates to be elected as his replacement. Labour had already lost its small majority in the House of Commons by the time he became Prime Minister, and further by-elections and defections forced Callaghan to deal with minor parties such as the Liberal Party, particularly in the "Lib-Lab pact" from 1977 to 1978. Industrial disputes and widespread strikes in the 1978 "Winter of Discontent" made Callaghan's government unpopular, and the defeat of the referendum on devolution for Scotland led to the successful passage of a motion of no confidence on 28 March 1979. This was followed by a defeat in the ensuing general election.
Callaghan remained Leader of the Labour Party until 1980, to reform the process by which the Party elected its leader, before returning to the backbenches where he remained until retiring as an MP in 1987. That year he was made a life peer as Baron Callaghan of Cardiff. He died in 2005, one day before his 93rd birthday.
1912 to 1944: early life and career.
James Callaghan was born at 38 Funtington Road, Copnor, Portsmouth, England on 27 March 1912. He was named after his father, also James Callaghan (1877–1921), who was a Royal Navy Chief Petty Officer. His mother was Charlotte Callaghan (née Cundy; 1879–1961). His paternal grandmother was Jewish.
He had an older sister, Dorothy Gertrude Callaghan (1904–82). He attended Portsmouth Northern Secondary School (now Mayfield School). He gained the Senior Oxford Certificate in 1929, but could not afford entrance to university and instead sat the civil service Entrance Exam.
At the age of 17, Callaghan left to work as a clerk for the Inland Revenue. While working as a tax inspector, Callaghan was instrumental in establishing the Association of Officers of Taxes as a trade union for those in his profession and became a member of its national executive. While at the Inland Revenue offices in Kent, in 1931, he joined the Maidstone branch of the Labour Party. In 1934, he was transferred to Inland Revenue offices in London. Following a merger of unions in 1936, Callaghan was appointed a full-time union official and to the post of Assistant Secretary of the Inland Revenue Staff Federation and resigned from his Civil Service duties.
His union position at the Inland Revenue Federation brought Callaghan into contact with Harold Laski, the Chairman of the Labour Party's National Executive Committee and an academic at the London School of Economics. Laski encouraged him to stand for Parliament, although later on he requested Callaghan several times to study and lecture at the LSE. Callaghan joined the Royal Navy Volunteer Reserve as an Ordinary Seaman in World War II from 1942 where he served in the East Indies Fleet and was promoted to the rank of Lieutenant in April 1944. While training for his promotion, his medical examination revealed that he was suffering from tuberculosis so he was admitted to the Royal Naval Hospital Haslar in Gosport near Portsmouth. After he recovered, he was discharged and assigned to duties with the Admiralty in Whitehall. He was assigned to the Japanese section and wrote a service manual for the Royal Navy "The Enemy Japan". Callaghan would become (as of 2015) the last British Prime Minister to be an armed forces veteran.
Whilst on leave, Callaghan was selected as a Parliamentary candidate for Cardiff South. He narrowly won the local party ballot with twelve votes against the next highest candidate George Thomas with eleven. He was encouraged to put his name forward for the Cardiff South seat by his friend Dai Kneath, a member of the IRSF National executive from Swansea, who was in turn an associate and friend of the local Labour Party secretary Bill Headon. During 1945 he was assigned to the East Indies Fleet and served on HMS "Queen Elizabeth" in the Indian Ocean. After VE Day, along with other prospective candidates he returned to the United Kingdom to stand in the general election.
1945 to 1976: parliament and cabinet.
Labour won a landslide victory on 26 July 1945 bringing Clement Attlee to power. Callaghan won his Cardiff South seat in the 1945 UK general election (and would hold a Cardiff-area seat continuously until 1987). He defeated the sitting Conservative incumbent candidate, Sir Arthur Evans, by 17,489 votes to 11,545. He campaigned on such issues as the rapid demobilisation of the armed forces and for a new housing construction programme. At the time of his election, his son Michael was born.
Callaghan was soon appointed Parliamentary Secretary to the Ministry of Transport in 1947 where, advised by the young chief constable of Hertfordshire, Sir Arthur Young, his term saw important improvements in road safety, notably the introduction of zebra crossings, and an extension in the use of cat's eyes. He moved to be Parliamentary and Financial Secretary to the Admiralty from 1950 where he was a delegate to the Council of Europe and resisted plans for a European army.
Callaghan was popular with Labour MPs and was elected to the Shadow Cabinet every year while the Labour Party was in opposition from 1951 to 1964. He was Parliamentary Adviser to the Police Federation from 1955 to 1960 when he negotiated an increase in police pay with the then general secretary Arthur Charles Evans. He ran for the Deputy Leadership of the party in 1960 as an opponent of unilateral nuclear disarmament, and despite the other candidate of the Labour right (George Brown) agreeing with him on this policy, he forced Brown to a second vote. In November 1961, Callaghan became shadow chancellor. When Hugh Gaitskell died in January 1963, Callaghan ran to succeed him, but came third in the leadership contest, which was won by Harold Wilson. However, he did gain the support of right-wingers, such as Denis Healey and Anthony Crosland, who wanted to prevent Wilson from being elected leader but who also did not trust George Brown.
Chancellor of the Exchequer.
In October 1964, Conservative Prime Minister Sir Alec Douglas-Home (who had only been in power for 12 months since the resignation of Harold Macmillan) called a general election. It was a tough election, but Labour won a narrow majority, gaining 56 seats (a total of 317 to the Conservatives' 304). The new Labour government under Harold Wilson immediately faced economic problems and Wilson acted within his first hours to appoint Callaghan as Chancellor of the Exchequer. The new government had to cope with a balance of payments deficit and speculative attacks on Sterling. It was the policy of the whole government, and one in which Callaghan concurred, that devaluation should be avoided for as long as possible and he managed to arrange loans from other central banks and some tax rises to stabilise the economy. Callaghan's time as chancellor was to be during a time of crisis; with high inflation, high unemployment and an unstable economy with a deficit in the budget, a deficit in the balance of import and exports and most importantly conflict over the value of the pound.
On 11 November, Callaghan gave his first budget and announced increases in income tax, petrol tax and the introduction of a new capital gains tax, actions which most economists deemed necessary to take the heat out of the balance and sterling deficit, though international bankers disagreed.
On 23 November, it was decided to increase the bank rate from 2% to 7% which generated a large amount of criticism. Handling the crisis was made more difficult by the attitude of Lord Cromer, the Governor of the Bank of England, who argued against the fiscal policies of the new Labour government. When Callaghan and Wilson threatened to call a new general election, the governor soon raised a £3 billion loan to stabilise the reserves and the deficit. His second budget came on 6 April 1965, in which he announced efforts to deflate the economy and reduce home import demand by £250 million. Shortly afterwards, the bank rate was reduced from 7% down to 6%. For a brief time, the economy and British financial market stabilised, allowing in June for Callaghan to visit the United States and to discuss the state of the British economy with President Lyndon Baines Johnson and the International Monetary Fund (IMF).
In July, the pound came under extreme pressure and Callaghan was forced to create harsh temporary measures to demonstrate control of the economy. These include suspending all current government building projects and postponing new pension plans. The alternative was to allow the pound to float or to devalue it. Callaghan and Wilson however were again adamant that a devaluation of the pound would create new social and economic problems and continued to take a firm stance against it. The government continued to struggle both with the economy and with the slender majority which, by 1966, had been reduced to one. On 28 February, Harold Wilson formally announced an election for 31 March 1966. On 1 March, Callaghan gave a 'little budget' to the Commons and announced the historic decision that the UK would adopt decimal currency. It was actually not until 1971, under a Conservative government, that the United Kingdom moved from the system of pounds, shillings and pence to a decimal system of 100 pence to the pound. He also announced a short-term mortgage scheme which allowed low-wage earners to maintain mortgage schemes in the face of economic difficulties. Soon afterwards, Labour won 363 seats compared to 252 seats against the Conservatives, giving the Labour government a large majority of 97.
Callaghan introduced his next Budget on 4 May. He had informed the house that he would bring a full Budget to the House when he made his 'little budget' speech prior to the election. The main point of his budget was the introduction of a Selective Employment Tax, penalising the service industry and favouring the manufacturing industry. Twelve days after the budget, the National Union of Seamen called a national strike and the problems facing Sterling were multiplied. Additional strikes caused the balance of payments deficit to increase and the 3.3 billion loan was now due. Unemployment was also rising; it had been just over 300,000 when Labour came to power, but two years later it was climbed to more than 500,000.
On 14 July, the bank rate was increased again to seven percent. On 20 July, Callaghan announced an emergency ten-point programme with a six-month freeze on wage and salary increases. By 1967, the economy had begun to stabilise once again and the bank rate was reduced to 6% in March and 5.5% in May.
It was under these conditions that Callaghan beat Michael Foot in a vote to become Treasurer of the Labour Party.
The economy was soon in turmoil again, with the Middle East crisis between Egypt and Israel raising oil prices. Furthermore, the economy was hit in mid-September when a national dock strike lasted for eight weeks. A run on Sterling began with the six-day war and with the closure of the Suez Canal and with the dock strike, the balance of payments deficit grew to a critical level. A Common Market report suggested that the pound could not be sustained as a reserve currency and it was suggested again that the pound should be devalued. Wilson and Callaghan refused a contingency fund offered from the IMF because of several conditions attached. On Wednesday 15 November, the historic decision was taken to commit the government to a 14.3% devaluation. The situation was a great political controversy at the time. As Denis Healey in his autobiography, notes:
Before the devaluation, Jim Callaghan had announced publicly to the Press and the House of Commons that he would not devalue, something he later said was necessary to maintain confidence in the pound and avoid creating jitters in the financial markets. Callaghan immediately offered his resignation as Chancellor and increasing political opposition forced Wilson to accept it. Wilson then moved Roy Jenkins, the Home Secretary, to the Chancellor of the Exchequer and Callaghan became the new Home Secretary on 30 November 1967.
Home Secretary.
Callaghan's tenure as Home Secretary was marked by the emerging conflict in Northern Ireland and it was as Home Secretary that he took the decision to deploy British Army troops in the province after a request from the Ulster Unionist Government of Northern Ireland.
Callaghan was also responsible for the Commonwealth Immigrants Act 1968; a controversial piece of legislation prompted by Conservative assertions that an influx of Kenyan Asians would soon inundate the country. It passed through the Commons in a week and placed entry controls on holders of British passports who had "no substantial connection" with Britain by setting up a new system. In his memoirs "Time and Chance", Callaghan wrote that introducing the Commonwealth Immigrants Bill had been an unwelcome task but that he did not regret it. He claimed the Asians had "discovered a loophole" and he told a BBC interviewer: "Public opinion in this country was extremely agitated, and the consideration that was in my mind was how we could preserve a proper sense of order in this country and, at the same time, do justice to these people – I had to balance both considerations". An opponent of the Act, Conservative MP Ian Gilmour, asserted that it was "brought in to keep the blacks out. If it had been the case that it was 5,000 white settlers who were coming in, the newspapers and politicians, Callaghan included, who were making all the fuss would have been quite pleased".
Also significant was the passing of the Race Relations Act in the same year, making it illegal to refuse employment, housing or education on the basis of ethnic background. The Act extended the powers of the Race Relations Board at the time, to deal with complaints of discrimination and unfair attitudes. It also set up a new supervisory body, the Community Relations Commission, to promote "harmonious community relations". Presenting the Bill to Parliament, the Home Secretary, Jim Callaghan, said, "The House has rarely faced an issue of greater social significance for our country and our children."
In 1969, Callaghan, a strong supporter of the Labour–Trade Union link, led the successful opposition in a divided cabinet to Barbara Castle's White Paper "In Place of Strife" which sought to modify Trade Union law. Amongst its numerous proposals were plans to force unions to call a ballot before a strike was held and the establishment of an Industrial Board to enforce settlements in industrial disputes. Ironically, if the proposals had become law, many of the activities of the trades unions during the Winter of Discontent a decade later would have been illegal.
Following Wilson's unexpected defeat by Edward Heath in the 1970 General Election, Callaghan declined to challenge him for the leadership despite Wilson's vulnerability. This did much to rehabilitate him in Wilson's eyes. He was in charge of drawing up a new policy statement in 1972 which contained the idea of the Social Contract between the government and trade unions. He also did much to ensure that Labour opposed the Heath government's bid to enter the Common Market—forcing Wilson's hand by making his personal opposition clear without consulting the Party Leader.
Foreign Secretary.
When Wilson won the next general election and returned as Prime Minister in March 1974, he appointed Callaghan as Foreign Secretary which gave him responsibility for renegotiating the terms of the United Kingdom's membership of the Common Market. When the talks concluded, Callaghan led the Cabinet in declaring the new terms acceptable and he supported a 'Yes' vote in the 1975 referendum.
Election as Leader of the Labour Party.
Barely two years after beginning his second spell as prime minister, Wilson announced his surprise resignation on 16 March 1976, and unofficially endorsed Callaghan as his successor. Callaghan was the favourite to win the leadership election, although he was the oldest candidate; he was also the most experienced and least divisive. Popularity with all parts of the Labour movement saw him through the ballot of Labour MPs to win the leadership vote. On 5 April 1976, at the age of 64 years and 9 days, Callaghan became Prime Minister – the oldest person to become Prime Minister at time of appointment since Winston Churchill.
Prime Minister.
Callaghan was the only Prime Minister to have held all three leading Cabinet positions – Chancellor of the Exchequer, Home Secretary and Foreign Secretary – prior to becoming Prime Minister.
During his first year in office, Callaghan started what has since become known as 'The Great Debate', when he spoke at Ruskin College, Oxford about the 'legitimate concerns' of a public about education as it took place in the nation's maintained schools. This discussion led to greater involvement of the government, through its ministries, in the curriculum and administration of state education, leading to the eventual introduction of the National Curriculum some ten years later.
Callaghan's time as Prime Minister was dominated by the troubles in running a Government with a minority in the House of Commons: he was forced to make deals with minor parties to survive – including the Lib-Lab pact, and he had been forced to accept referendums on devolution in Scotland and Wales (the former went in favour but did not reach the required majority, and the latter went heavily against). He also became prime minister at a time when Britain was suffering from double-digit percentage inflation and rising unemployment. He responded to the economic crises by adopting deflationary policies to reduce inflation, and cutting public expenditure – a precursor to the monetarist economic policies that the next government, a Conservative one led by Margaret Thatcher, would pursue to ease the crises.
Callaghan and his ministers did, however, introduce a number of reforms during their time in office. The Supplementary Benefits Act 1976 gave every person over the age of 16, whose resources were not enough to meet his or her basic needs, the right to claim a supplementary pension if he or she had reached state-pension age, and a supplementary allowance if he or she was less than this age. The Rent (Agricultural) Act 1976 provided security of tenure for agricultural workers in tied accommodation, while the Bail Act 1976 reformed bail conditions with courts having to explain refusal of bail. The Police Act 1976 set up a Police Complaints Board "to formalise the procedure for dealing with public complaints." The Education Act 1976 limited the taking up of independent and Direct grant school places and required all local authorities who had failed to do so "to submit proposals for comprehensive schools," while the Housing (Homeless Persons) Act 1977 extended local council responsibility "to provide accommodation for homeless people in their area," and instituted the right of homeless families to a permanent local council tenancy. In addition, efforts were made under the Environment Secretary Peter Shore to redistribute resources toward deprived urban areas. The Inner Urban Areas Act 1978 allowed local authorities to assist declining industrial areas and central government provided new subsidies to those inner city areas with the most problems, while the 1978 Finance Act introduced profit-sharing schemes. In April 1976, a Child Interim Benefit for single-parent families was introduced, followed by a universal Child Benefit scheme the following year. 
The Callaghan Government also introduced a range of measures aimed at moderating pressures for wage rises and to create a favourable climate “for an orderly restoration of collective bargaining.” These included the granting of family income supplements to bring the incomes of lower-paid workers up to the level of social security benefits, the lowering of marginal tax rates on smaller incomes by rises in personal allowances, and increases in children’s allowances (which were payable to the mother). However, child tax allowances were lowered, which had the effect of reducing the take home pay of fathers. The impact of consumer price rises was also mitigated by higher income limits for free school meals, an increased milk subsidy, and a substantial reduction in the duty on petrol. In addition, electricity prices were lowered for families in receipt of supplementary benefits.
The Training Opportunities Scheme, under which more than 90 000 people completed their training in 1976 and which catered mainly for people over 19 years old, was extended during 1977 to include provisions for training persons for self-employment. In addition, technician training was extended and the network of skillcentres continued to expand. In August 1977, a scheme for voluntary early retirement was introduced in the coal industry for men aged 62 or more with 20 or more year's underground service, with weekly payments up to normal pensionable age. In January 1977, unions became authorized to lodge a claim on behalf of workers with the Advisory, Conciliation and Arbitration Service for an improvement in terms and conditions of employment on the grounds that existing terms and conditions were less favourable than the relevant recognized terms and conditions for the trade in the area or, where these did not exist, the general level. In February, sections of the Employment Act 1975 were brought into operation dealing with the qualifying hours for part-time work, thereby entitling large numbers of part-time workers to the same rights and job security as full-time workers. Also in February, employees became entitled to receive guarantee payments from their employers when laid off or on short time, while in April sections of the 1975 Employment Act were activated giving employees the right to paid time off work in order to perform certain public duties. The main provisions of the Race Relations Act 1976 came into force in June 1977, making it unlawful for an employer to discriminate in recruitment or dismissal or in the treatment of existing employees in matters of promotion, transfer, training or other benefits on the grounds of nationality, race, colour, colour, or ethic or national origins. A Commission for Racial Equality was established to work towards the elimination of discrimination the promotion of equality of opportunity, and good relations between persons of different racial groups.
In Scotland, the Community Service by Offenders Act 1978 introduced provisions whereby offenders might, under certain circumstances, be ordered by courts to undertake community work as an alternative to a prison sentence. This legislation brought Scotland in line with England and Wales where similar provisions already apply. The Mines (Precautions Against Inrushes) Regulations 1979 applied to all types of mines and made provision for measures to be taken against the hazard of inrushes of water or gas or material which flows when wet.
In housing policy, a shift of emphasis in housing policy towards rehabilitation was evident in the further increase in the number of General Improvement Areas and the number of Housing Action Areas declared. An Act of March 1977 makes provision, for a limited period, for benefits to be paid from the age of 64 to workers who agree to retire in order to free jobs for young unemployed people. A number of other improvements were introduced in 1977, with Attendance Allowances extended to cover handicapped foster children and non-contributory disablement pensions extended to married women whose invalidity prevented them from carrying out their household tasks. In January 1977, regulations were issued which brought about a change in the administration of legislation governing fire precautions at places of work. Under these regulations the Health and Safety Executive retained full responsibility for fire safety in certain 'special' premises such as nuclear installations, coalmines and chemical plants, whereas responsibility for general fire precautions at places of work was transferred to local fire authorities. In July 1977, an experimental Job Introduction Scheme was introduced to provide financial assistance enabling certain disabled people to undertake a trial period of employment with an employer, where there was reasonable doubt as to the person’s ability to perform a particular job. In July 1978, a revised and simplified scheme designed to assist severely disabled people with their travel-to-work costs was introduced.
The Safety Representatives and Committees Regulations of 1977 made provision for recognised trade unions to appoint health and safety representatives “and gave such representatives rights to representation and consultation on health and safety as well as rights to access to training and facilities to support them in undertaking these tasks.” The Homes Insulation Act 1978 provided for grants to occupiers towards the cost of thermal insulation of their dwellings, while under the Safety Representatives and Safety Committees Regulations recognized trade unions were allowed to appoint safety representatives who would have certain rights and functions. As part of an extension in external consultation on the prevention of industrial accidents and occupationally induced diseases the Health and Safety Commission established three Industry Advisory Committees for construction, railways and oil and regulations were issued in March 1978 dealing with the packaging and labelling of some 800 dangerous chemicals commonly used at work and in the home. Improvements to the Mineworkers Sick Pay Scheme were also introduced from 1978, with improvement in the formula for calculating benefit improved and the period of 'waiting days' reduced from seven to three. The Home Purchase Assistance and Housing Corporation Guarantee Act 1978, gave help to first-time home buyers. The Consumer Safety Act 1978 protected consumers from purchasing potentially harmful goods, while the 1979 Credit Unions Act, the last piece of legislation passed by the Labour government, set up a legal structure for credit unions.
Despite the economic difficulties faced by the government, over the summer of 1978 (shortly after the end of the Lib-Lab pact) most opinion polls showed Labour ahead, and the expectation grew that Callaghan would call an autumn election that would have given him a second term in office until autumn 1983. The economy had also started to show signs of recovery by this time. 1978 was a year of economic recovery for Britain, with inflation falling to single digits, unemployment declining during the year, and general living standards going up by more than 8%. Famously, he strung along the opposition and was expected to make his declaration of election in a broadcast on 7 September 1978. His decision to put off the election, at the time, seen by many as a sign of his domination of the political scene and he ridiculed his opponents by singing old-time music hall star Vesta Victoria's song "Waiting at the Church" at that month's Trades Union Congress meeting: now seen as one of the greatest moments of hubris in modern British politics, but celebrated at the time. Callaghan intended to convey the message that he had not promised an election, but most observers misread his message as an assertion that he would call an election, and the Conservatives would not be ready for it.
"Winter of Discontent".
Callaghan's method of dealing with the long-term economic difficulties involved pay restraint which had been operating for four years with reasonable success. He gambled that a fifth year would further improve the economy and allow him to be re-elected in 1979, and so attempted to hold pay rises to 5% or less. The trade unions rejected continued pay restraint and in a succession of strikes over the winter of 1978–79 (known as the Winter of Discontent) secured higher pay. The industrial unrest made his government extremely unpopular, and Callaghan's response to one interview question only made it worse. Returning to the United Kingdom from an economic summit held in Guadeloupe in early 1979, Callaghan was asked, "What is your general approach, in view of the mounting chaos in the country at the moment?"
Callaghan replied, "Well, that's a judgement that you are making. I promise you that if you look at it from outside, and perhaps you're taking rather a parochial view at the moment, I don't think that other people in the world would share the view that there is mounting chaos." This reply was reported in "The Sun" under the headline "Crisis? What Crisis?". Callaghan also later admitted in regard to the Winter of Discontent that he had "let the country down".
The Winter of Discontent saw Labour's performance in the opinion polls slump dramatically. They had topped most of the pre-winter opinion polls by several points, but in February 1979 at least one opinion poll was showing the Tories 20 points ahead of Labour and it appeared certain that Labour would lose the forthcoming election.
In the buildup to the election, the "Daily Mirror" and "The Guardian" supported Labour, while "The Sun", the "Daily Mail", the "Daily Express", and "The Daily Telegraph" supported the Conservatives.
On 28 March 1979, the House of Commons passed a Motion of No Confidence by one vote, 311–310, which forced Callaghan to call a general election that was held on 3 May. The Conservatives under Margaret Thatcher ran a campaign on the slogan "Labour Isn't Working" and won the election.
Callaghan's failure to call an election during 1978 was widely seen as a political miscalculation; indeed, he himself later admitted that not calling an election was an error of judgement. After losing power in 1979, Labour would spend the next 18 years in opposition.
Resignation, backbenches and retirement.
Notwithstanding electoral defeat, Callaghan stayed on as Labour leader until 15 October 1980, shortly after the party conference had voted for a new system of election by electoral college involving the individual members and trade unions. His resignation ensured that his successor would be elected by MPs only. After a campaign that laid bare the deep internal divisions of the Parliamentary Labour Party, Michael Foot narrowly beat Denis Healey on 10 November in the second round of the election to succeed Callaghan as leader.
In 1982, along with his friend Gerald Ford, he co-founded the annual AEI World Forum.
In 1983, he attacked Labour's plans to reduce defence, and the same year became Father of the House as the longest continuously serving member of the Commons. He was by this time one of only three survivors of the 1945 general election, but the only one with continuous service. Michael Foot and Ian Mikardo also remained of the 1945 intake, but Michael Foot had been out of the House from 1955 to 1960 and Mikardo from 1959 to 1964.
In 1987, he was made a Knight of the Garter and stood down at the 1987 general election after 42 years as a member of the Commons. Shortly afterwards, he was elevated to the House of Lords as Baron Callaghan of Cardiff, of the City of Cardiff in the Royal County of South Glamorganshire. In 1987, his autobiography, "Time and Chance", was published. He also served as a non-executive director of the Bank of Wales.
Callaghan's wife Audrey, a former chairman (1969–1982) of Great Ormond Street Hospital, spotted a letter to a newspaper which pointed out that the copyright of "Peter Pan", which had been assigned by J. M. Barrie to the hospital, was going to expire at the end of that year, 1987 (50 years after Barrie's death, the current copyright term). In 1988, Callaghan moved an amendment to the Copyright Designs & Patents Act, then under consideration in the House of Lords, to grant the hospital a right to royalty in perpetuity despite the lapse of copyright, and this was passed by the government.
In July 1996, he was awarded an honorary degree from the Open University as Doctor of the University.
In October 1999, Callaghan told "The Oldie Magazine" that he would not be surprised to be considered as Britain's worst prime minister in 200 years. He also admitted in this interview that he "must carry the can" for the Winter of Discontent.
Personal life.
Callaghan's interests included rugby, tennis and agriculture. He married Audrey Elizabeth Moulton, whom he had met when they both worked as Sunday School teachers at the local Baptist church, in July 1938 and had three children – one son and two daughters. 
Although there is much doubt about how much belief Callaghan retained into adult life, the Baptist nonconformist ethic was a profound influence throughout all of his public and private life. In the mid-1980s Callaghan told an interviewer that he was an atheist.
One of his final public appearances came on 29 April 2002, when at the age of 90 he sat alongside the then prime minister Tony Blair and the three other surviving former prime ministers at the time at Buckingham Palace for a dinner which formed part of the celebrations for the Golden Jubilee of Elizabeth II, alongside his daughter Margaret, Baroness Jay, who had served as Leader of the House of Lords from 1998 to 2001.
Callaghan died on 26 March 2005 at Ringmer, East Sussex, of lobar pneumonia, cardiac failure and kidney failure. He passed away 11 days after his wife's death on 15 March 2005 and the day before what would have been his 93rd birthday. He died as the longest-lived British former prime minister, having beaten Harold Macmillan's record 39 days earlier. Lord Callaghan was cremated and his ashes were scattered in a flower bed around the base of the Peter Pan statue near the entrance of London's Great Ormond Street Hospital, where his wife had formerly been chair of the board of governors.
In popular culture.
The song "Time for Truth" from The Jam's debut album, "In the City", a scathing critique of the state of the British nation, directly addresses Callaghan: "I think it's time for truth, and the truth is you lost, Uncle Jimmy."
The Callaghan Library at Ruskin College, Oxford is named after Callaghan. The library was opened in 2011: the ceremony was attended by James Callaghan's son Michael and grandson Joe, the latter being a student at the college.
The vote of no confidence that led to the 1979 general election is a plot point in the first episode of the second series of Utopia.
References.
Books:
</dl>
Biographies:
</dl>

</doc>
<doc id="52279" url="http://en.wikipedia.org/wiki?curid=52279" title="Ben Elton">
Ben Elton

Benjamin Charles "Ben" Elton (born 3 May 1959) is an English-Australian comedian, author, playwright, actor and director. He was a part of London's alternative comedy movement of the 1980s and became a writer on series such as "The Young Ones" and "Blackadder", as well as continuing as a stand-up comedian on stage and television. His style in the 1980s was left-wing political satire. Since then he has published 15 novels and written the musicals "We Will Rock You" (2002) and "Love Never Dies" (2010), the sequel to "The Phantom of the Opera".
Early life and education.
Elton was born at University College Hospital in Fitzrovia, London, the son of Mary (née Foster), an English teacher from Cheshire, and physicist and educational researcher Professor Lewis Elton. He is the nephew of the historian Sir G. R. Elton. Elton's father is from a German Jewish family and Elton's mother, who was raised in the Church of England, is of English background.
Elton grew up in Catford, south London, before moving with his family to Guildford, Surrey in 1968. Raised in a non-religious home he identifies as an atheist.
Elton studied at Stillness Junior School and Godalming Grammar School in Surrey, South Warwickshire College in Stratford-upon-Avon and the University of Manchester.
Work.
Television.
His first television appearance was a stand-up performance on the BBC1 youth and music programme, The "Oxford Road Show". His first TV success though was at 23 as co-writer of the television sitcom, "The Young Ones", in which he occasionally appeared.
In 1983/84 he wrote and appeared in Granada Television's sketch show "Alfresco", which was also notable for early appearances by Stephen Fry, Hugh Laurie, Emma Thompson and Robbie Coltrane. In 1985, Elton produced his first solo script for the BBC with his comedy-drama series "Happy Families", starring Jennifer Saunders and Adrian Edmondson. Elton appeared in the fifth episode as a liberal prison governor. Shortly afterwards, he reunited Rik Mayall and Edmondson with their "Young Ones" co-star Nigel Planer for the showbiz send-up sitcom "Filthy Rich and Catflap".
In 1985 Elton began his writing partnership with Richard Curtis. Together they wrote "Blackadder II", "Blackadder the Third" (in one episode, Elton appeared as a bomb-wielding anarchist) and "Blackadder Goes Forth". "Blackadder", starring Rowan Atkinson, was a worldwide hit, winning four BAFTAs and an Emmy.
Elton and Curtis were inspired to write "Blackadder Goes Forth" upon finding World War I to be apt for a situation comedy. This series, which dealt with greater, darker themes than prior "Blackadder" episodes, was praised for Curtis's and Elton's scripts, in particular the final episode. Before writing the series, the pair read about the war and found that
Elton and Curtis also wrote Atkinson's 1986 stage show, "The New Review", and Mr Bean's "exam" episode.
Elton became a stand-up comedian primarily to showcase his own writing, but became one of Britain's biggest live acts. After a regular slot on "Saturday Live" – later moved and renamed "Friday Night Live " – which was seen as a UK version of the US's "Saturday Night Live", he became the host of the programme.
In 1990 he starred in his own stand-up comedy and sketch series, "The Man from Auntie", which had a second series in 1994. (The title plays on "The Man from UNCLE"; "Auntie" is a nickname for the BBC). In 1989 Elton won the Royal Television Society Writers' Award.
"The Ben Elton Show" (1998) followed a format similar to "The Man from Auntie" and featured Ronnie Corbett, a comedian of the old guard that the "alternative comedians" of the 1980s were the direct alternative to, as a regular guest. It was Elton's last high-profile network programme in the UK as a stand-up comedian.
In April 2007, "Get a Grip", a new show, began on ITV1. Featuring comic sketches similar to those on "The Ben Elton Show" and staged studio discussion between Elton and 23-year-old Alexa Chung, the show's aim was to "contrast Elton's middle-aged viewpoint with Chung's younger perspective" (although Elton was responsible for the scripts).
In "Third Way Magazine", Elton accused the BBC of allowing jokes about vicars but not imams. "And I believe that part of it is due to the genuine fear that the authorities and the communities have about provoking the radical elements of Islam".
On 10 October 2010, Elton headlined the first episode of "Dave's One Night Stand".
Elton worked on "Ben Elton Live From Planet Earth", a live one-hour variety show which debuted on 8 February 2011 on the Nine Network in Australia. Live from Planet Earth was axed by the Nine Network on Wednesday 23 February 2011 after three episodes, despite having six commissioned. The show's final airing rated 200,000 viewers.
Behind the camera.
Elton wrote and produced "The Thin Blue Line", a studio-based sitcom set in a police station, also starring Rowan Atkinson, which ran for two series in 1995 and 1996. A prime-time family show, its traditional format and characters won it the 1995 British Comedy Award and both the public and professional Jury Awards at Reims.
He also wrote the six-part sitcom "Blessed", starring Ardal O'Hanlon, on BBC1 in 2005. No further series was commissioned.
In 2012 a new sitcom for BBC1 was commissioned written and produced by Elton starring David Haig. Filming for a full six part series of the sitcom, "The Wright Way" (formerly known as Slings and Arrows) was completed in late February 2013. It debuted in April 2013 to negative reviews.
Radio.
Elton starred with Adrian Edmondson in a sitcom based on the song "Teenage Kicks" for BBC Radio 2. A television version of "Teenage Kicks" for ITV has been made; Elton appeared in the pilot but was replaced by Mark Arden when it went to series production.
Novels.
He has published 15 novels since 1989, published by Simon and Schuster (the first four) and the rest by Transworld.
On a publicity tour for "Past Mortem" in 2004, Elton mused on the high school reunion theme and his own drama college reunion: "We’d had a very happy time all together, so there were no old scores to be settled really, we’d been a pretty happy bunch. And yet one person, who’d been a bit of a golden boy – he certainly went out with a girl I was besotted and unrequitedly in love with – he came up and he said, ‘Why did you come? Was it to show off?’ That really surprised me, that anyone would think that ... he came kind of carrying my agenda. It was weird. I hasten to add I didn’t think my life to be more successful than anybody else’s. If you’re happy and honest and fulfilled in what you do, then you’re having a successful life."
Films.
Ben Elton appeared in amateur dramatic productions as a youth, notably as The Artful Dodger in the musical "Oliver!" 
While in bit parts in his own TV series, he began professional film acting as CD in "Stark", the Australian/BBC TV series adaptation of his novel, in 1993. This was directed by Nadia Tass and filmed in Australia.
Elton played Verges in Kenneth Branagh's film adaptation of William Shakespeare's "Much Ado About Nothing", also in 1993.
Behind the camera.
Elton wrote and directed the film adaptation of his novel "Inconceivable", under the title "Maybe Baby" (2000) starring Hugh Laurie and Joely Richardson. It was a moderate UK success and distributed globally. The film was also nominated for a prize at Germany's Emden Film Festival.
Musicals.
Elton collaborated with Andrew Lloyd Webber on "The Beautiful Game" in 2000, writing the book and lyrics (Lloyd Webber wrote the music). "The Beautiful Game" won the London Critics Circle Award for best new musical. 
Elton went on to write compilation shows featuring popular songs from the catalogues of pop/rock artists. The first was the musical "We Will Rock You" with music by Queen. Despite unfavourable early reaction, this was successful in London and won the 2003 Theatregoers' Choice Award for Best New Musical. It has since opened in the US, Australia, Russia, Spain, South Africa, Japan, Germany, Switzerland, Sweden, Canada, and The Netherlands. Elton also directed the 10th Anniversary Arena tour, in 2013. The musical ran for 12 years in London.
"Tonight's the Night", based on the songs of Rod Stewart, opened in November 2003.
Elton worked with Andrew Lloyd Webber on the sequel to his 1986 "The Phantom of the Opera", "Love Never Dies".
Stage.
Elton studied drama at the University of Manchester and has written three West End plays.
Stand-up comedy.
In 1981, when his live act took off, Elton was hired by The Comedy Store as compère.
He made two albums of stand-up comedy, "Motormouth" (1987) and "Motorvation" (1988).
In 2005 Elton did his first stand-up tour since 1997, touring the UK with "Get a Grip". He toured Australia and New Zealand with the same show in 2006.
Awards.
Ben Elton has been awarded an Honorary Rose for lifetime achievement at the Rose d'Or festival. He was also made a Companion of the Liverpool Institute for Performing Arts, in recognition of his work with students, and has an honorary doctorate from The University of Manchester. He has won 3 BAFTAs for Best Comedy Series for "The Young Ones", "Blackadder the Third" and "Blackadder Goes Forth". "Popcorn" and "We Will Rock You" each won an Olivier Award and "The Beautiful Game" was awarded the Best Musical at the Critics Circle Awards. "The Man From Auntie" won him a Royal Television Society Writer's Award and "The Thin Blue Line" picked up a British Comedy Award as well as Jury Award at Reims. His books are also award winning. Awards include the Crime Writers Association Gold Dagger Award for Crime Fiction ("Popcorn"), the Swedish Kaliber Award ("Popcorn"), WH Smiths People’s Choice Fiction Award ("High Society") and Prix Polar International Crime Writer Award ("Amitiès Mortelles" for "Past Mortem", French edition).
Personal life.
Elton married Australian bass guitar player Sophia Gare, whom he met in 1987, in 1994 and has three children, twins Lottie and Bert, who were born in 1999 and younger son Fred, who was born in 2001. He has lived in Fremantle, Australia; in East Sussex, England; and London, England. Since 2009, Elton and his family have lived in Fremantle. Elton has held dual British/Australian citizenship since 2004. He said he would like to move back to London when his children have finished school.
Elton has psoriasis and his wife has undergone IVF treatments.
Politics.
Elton champions left wing opinions. Prior to the 1987 UK general election, Elton supported Red Wedge by participating in a comedy tour organised by the campaign.
He was a Labour Party supporter and was named in a list of the biggest private financial donors to the party.
However, Elton distanced himself with the Party under Tony Blair and 'New Labour', instead donating and voting for the Green Party, although in April 2015 has stated he is "back with Labour" for the 2015 General Election.
Criticism.
Stewart Lee devoted about 10 minutes to his dislike of Elton's later work in his 2005 act, "Stand-Up Comedian," and provides a transcript with footnotes further specifying his complaints in his 2010 book "How I Escaped My Certain Fate". Lee expresses his disappointment at Elton's abandonment of political ideology in favour of commercial work and his dislike of Elton's work on the musical "We Will Rock You." The live set included an extended back-and-forth with the audience in which Lee brings them to the comical conclusion that Elton is less liked than Osama bin Laden, as bin Laden appeared to have lived his life according to a more consistent set of ethical principles!
Elton has been criticised for writing a musical with Conservative supporter Andrew Lloyd Webber. In his defence, Elton said "If I were to refuse to talk to Tories, I would narrow my social and professional scope considerably. If you judge all your relationships on a person's voting intentions, I think you miss out on the varieties of life." He is also one of the few items to have been put into "Room 101" twice: first by Anne Robinson in 2001 and then by Mark Steel.
Elton says of criticism "I would have loved a honeymoon period, but I've been irritating journos from the beginning. Originally I was knocked for being too left-wing, and now apparently I've sold out and I'm too right-wing, but all the time I've been being me, and that certainly isn't the person I recognise in anything that's written about me." He denies being anti-establishment though, "I wrote a sitcom for the BBC when I was 21! How the fuck can I be anti-establishment? From the first interview I ever did, I talked about Morecambe and Wise, and every time they wanted me to talk about Lenny Bruce I'd say, 'Yeah, he's fine, but he doesn't make me laugh the way Eric 'n' Ernie do." He also points out he was a socialist at a time when "the media was on the whole slavishly worshipping of Thatcher". He said of his political views "I believe in the politics of Clement Attlee. I'm a Welfare State Labour voter."
He parodied himself though in the sketch 'Benny Elton' for "Harry Enfield and Chums" in 1994, sending up his 'right on' socialist image as a politically correct spoilsport chasing Page Three models around a park to chastise them and tricking heterosexual couples into becoming gay.

</doc>
<doc id="52280" url="http://en.wikipedia.org/wiki?curid=52280" title="Perthshire">
Perthshire

Perthshire, officially the County of Perth (; Scottish Gaelic: "Siorrachd Pheairt"), is a registration county in central Scotland. It extends from Strathmore in the east, to the Pass of Drumochter in the north, Rannoch Moor and Ben Lui in the west, and Aberfoyle in the south. It was a local government county from 1890 to 1930.
Perthshire is known as the "big county" and has a wide variety of landscapes, from the rich agricultural straths in the east, to the high mountains of the southern Highlands.
Administrative History.
Perthshire was an administrative county between 1890 and 1975, governed by a county council. This Local Government council was superseded in 1930, when a joint Local Government council was formed with the neighbouring small county of Kinross-shire, linking the two.
In 1975 this Local Government council was in turn superseded by the Local Government (Scotland) Act 1973 and split between the Local Government Central and Tayside Regions:
The two-tier system introduced in 1975 was superseded by a system of unitary authorities in 1996.
The area of the former council is now divided between the Local Government council areas of Clackmannanshire, Perth and Kinross and Stirling.
The area included in Dundee in 1975 was transferred to Perth and Kinross.
Boundaries.
Prior to the 1890s Perthshire's boundaries were irregular: the parishes of Culross and Tulliallan formed an exclave some miles away from the rest of the county, on the boundaries of Clackmannanshire and Fife; while the northern part of the parish of Logie formed an enclave of Stirlingshire within the county.
Following the recommendations of the council boundary commission appointed under the Local Government (Scotland) Act 1889, Culross and Tulliallan were transferred to Fife, and the entire parish of Logie was included in Stirlingshire.
Coat of arms.
The coat of arms of the County of Perth appears to have been granted for use on the colours and standards of the volunteer and militia units of the county raised at the end of the eighteenth century. Robert Hay Drummond, a native of Perthshire, and commanding officer of the Perthshire Gentlemen and Yeomanry Cavalry, was also Lord Lyon King of Arms at the time, and he presented the arms to the county in 1800. The grant document was discovered in the Lyon Office in 1890, and forwarded to the newly formed Perth County Council.
It is blazoned thusly:
"Or, a lion rampant gules armed and langued azure holding a scimitar proper upon a mount vert, all within a double tressure flory-counter-flory of the second; also a canton of the third charged with the Palace of Scone ensigned by a crown proper. Upon a wreath of the colours the first and second is, for crest, a demi-highlander, holding aloft a broadsword proper in a menacing position. The escutcheon supported by an eagle regardant proper and a horse argent with trappings of the second. Below the escutcheon an escrol bearing the motto “pro lege et libertate” (for law and liberty)."
Burghs.
By the 1890s the county contained the following burghs, which were largely outside the county council's jurisdiction:
The Local Government (Scotland) Act 1929 divided burghs into two classes from 1930: large burghs, which were to gain extra powers from the county council, and small burghs which lost many of their responsibilities.
Of the twelve burghs in Perthshire, only Perth was made a large burgh. There were ten small burghs: Blairgowrie and Rattray being united into a single burgh. In 1947 Pitlochry was created a small burgh.
Civil parishes.
In 1894 parish councils were established for the civil parishes, replacing the previous parochial boards. 
The parish councils were in turn replaced by district councils in 1930.
Following the boundary changes caused by the Local Government (Scotland) Act 1889, the county contained the following civil parishes:
Towns and villages.
Perthshire includes the City of Perth and the following other towns and villages (see also Civil Parish list):
Other towns and villages
Some others listed in alphabetical order in the Land Register Counties : 
Districts.
In 1930 the "landward" area of the Local Government councils (the part outside of burgh boundaries) was divided into five districts, replacing the parish councils established in 1894:
Parliamentary constituencies.
Following the Act of Union, Perthshire returned members to the House of Commons of the Parliament of the United Kingdom from 1708.
1885 - 1918.
In 1885 seats in the House of Commons were redistributed: Perthshire received three seats.
1918 - 1975.
In 1918 there was a further redistribution. Perthshire was combined with Kinross-shire to form a parliamentary county, divided into two constituencies:
These boundaries continued in use until 1983, when new constituencies were formed based on the Local Government regions and districts created in 1975.
Surnames.
Most common surnames in Perthshire at the time of the United Kingdom Census of 1881, by order of incidence:

</doc>
<doc id="52281" url="http://en.wikipedia.org/wiki?curid=52281" title="Clackmannanshire">
Clackmannanshire

Clackmannanshire ( or   ) is a historic county and council area in Scotland, bordering the council areas of Stirling, Fife and Perth & Kinross.
The name is derived from the Scottish Gaelic: "Siorrachd Chlach Mhannainn" meaning "Stone of Manau". As Britain's smallest historic county, it is often nicknamed "The Wee County". When written, Clackmannanshire is commonly abbreviated to Clacks. Clackmannanshire was the first to declare during the Scottish Independence Referendum.
Administrative history.
The County of Clackmannan is one of Scotland's 33 historic local government counties, bordering on Perthshire, Kinross-shire, Stirlingshire and Fife. The county town was originally Clackmannan, but by 1822 neighbouring Alloa had outgrown Clackmannan and replaced it as the county town. Some rationalisation of the county boundaries was undertaken in 1889-1890, and in 1971 the Muckhart and Glendevon areas, formerly in Perthshire, were transferred to Clackmannanshire.
In 1975, under the Local Government (Scotland) Act 1973, the 33 historic counties lost their administrative status, and a new hierarchy of regions and districts was created. Clackmannanshire became part of the Central Region, under the name Clackmannan District, together with Stirling District and Falkirk District. 
The historic name was restored in 1996, under the Local Government etc (Scotland) Act 1994. The area was to have the name "Clackmannan", but following strong local pressure this was changed to "Clackmannanshire" by the council using its own powers. 
Clackmannanshire today.
In terms of population, Clackmannanshire is the smallest council area in mainland Scotland, with a population of 48,630 (in 2005), around half of whom live in the main town and administrative centre, Alloa.
The motto of Clackmannanshire is "Look Aboot Ye" ("Circumspice" in Latin). In 2007 a re-branding exercise led to the area adopting the slogan "More Than You Imagine".
In the 18 September 2014 Scottish Independence Referendum, Clackmannanshire reported a turnout of 88.6%. It became the first area to announce its result with 16,350 people (46.20%) voting in favour of independence and 19,036 (53.80%) voting against.
Geography.
The Ochil Hills lie in the northern part of the area. Strathdevon is immediately to the south of the steep escarpment formed by the Ochil Fault, along which the Hillfoots Villages are located. Strathdevon mostly comprises a lowland plain a few hundred metres either side of the River Devon, which joins the Forth near Cambus. There is also the Black Devon river that flows past the town of Clackmannan to join the Forth near Alloa. This confluence once had a small pier, for portage to Dunmore pier on the south shore, and anchorage of smaller sailing ships, while others of greater tonnage could be accepted at Dunmore pier on the opposite banks of the Forth.
Coat of arms.
Clackmannanshire's coat of arms is blazoned:
"Or, a saltire gules; upon a chief vert, between two gauntlets proper, a pale argent charged with a pallet sable."
The red saltire on gold is taken from the arms of the Clan Bruce. According to legend, Robert Bruce mislaid his gauntlets while visiting the county, and upon asking where he could find them was told to "look aboot ye" (hence the motto). The green chief represents the county's agriculture, while the black and white pale is taken from the arms of the Clan Erskine whose chief the Earl of Mar lives at Alloa Tower.
Economy.
The main industries are agriculture, brewing, and formerly coal mining. In 2006, permission was given for a waterfront development of the Docks area of Alloa, which has been in decline since the 1960s. There is a large glass works at Alloa.
Transport.
Alloa railway station reopened in May 2008. A new railway line was completed which connected Kincardine and Stirling, and thus reconnecting Alloa to the national rail network for the first time since 1968, was opened to the public. Scheduled passenger services operate only between Alloa and Stirling and onwards to Glasgow and Edinburgh; the line to Kincardine is normally used by freight trains only but some special excursion trains are run by charter operators. An opening ceremony was held on Thursday 15 May, with the first fully functioning passenger service commencing in the new summer timetable on 19 May 2008. The service provides an hourly connection between Alloa, Stirling and Glasgow Queen Street.
History.
Clackmannan, the old county town, is named after the ancient stone associated with the pre-Christian deity Manau or Mannan.
The stone now rests on a larger stone beside the Tollbooth and Mercat Cross at the top of Main street, Clackmannan.
Clackmannanshire became known for the weaving mills powered by the Hillfoots burns. Other industries included brewing, glass manufacture, mining and ship building. Now capitalising on its central position and transport links, Clackmannanshire attracts service industries and tourism.

</doc>
<doc id="52285" url="http://en.wikipedia.org/wiki?curid=52285" title="Charles Lee (general)">
Charles Lee (general)

Charles Lee (6 February 1732 [O.S. 26 January 1731] – 2 October 1782) served as a General of the Continental Army during the American War of Independence. Lee also served earlier in the British Army during the Seven Years War. After the war he sold his commission and served for a time in the Polish army of King Stanislaus II. In 1773 Lee, who had Whig views, moved to America and bought an estate in Virginia. When the fighting broke out in the American War of Independence in 1775 he volunteered to serve with rebel forces. Lee's ambitions to become Commander in Chief of the Continental Army were thwarted by the appointment of George Washington.
During 1776, forces under his command repulsed a British attempt to capture Charleston, which boosted his standing with the army and Congress. Later that year he was captured by British cavalry under Banastre Tarleton and held as a prisoner until exchanged in 1778. During the decisive Battle of Monmouth later that year, Lee led an assault on the British which miscarried. He was subsequently court-martialed and his military service brought to an end. He died in Philadelphia in 1782.
Early life.
Lee was born on 6 February 1732 [O.S. 26 January 1731] in Dernhall, Cheshire, England, the son of Major General John Lee and Isabella Bunbury (daughter of Sir Henry Bunbury, 3rd Baronet). He was sent to free grammar school in Bury St Edmunds and later to Switzerland and became proficient in several languages, including Latin, Greek and French. On 9 April 1747 his father, then Colonel of the 55th Foot (later renumbered the 44th), purchased a commission for Charles as an Ensign in the same regiment.
Seven Years' War and after.
North America.
After completing his schooling, Lee reported for duty with his regiment in Ireland. Shortly after his father's death, on 2 May 1751 he received (or purchased) a Lieutenant's commission in the 44th. He was sent with the regiment to America in 1754 for service in the French and Indian War under Major General Edward Braddock, and was at his defeat at the Battle of the Monongahela in 1755. During this time in America, he married the daughter of a Mohawk Indian chief. His wife (name unknown) gave birth to twins. Lee was known to the Mohawks as "Ounewaterika", or "Boiling Water".
On 11 June 1756 Lee purchased a Captain's commission in the 44th for the sum of £900. The following year he took part in an expedition against the French fortress of Louisbourg, and on 1 July 1758 he was wounded in a failed assault on Fort Ticonderoga. He was sent to Long Island to recuperate where an attempt on his life was made by a surgeon he had earlier rebuked and thrashed. After recovering, he took part in the capture of Fort Niagara in 1759 and Montreal in 1760, which brought the war on the North American theater to an end by completing the Conquest of Canada.
Portugal.
Lee went back to Europe, transferred to the 103rd Foot as a major, and served as a Lieutenant Colonel in the Portuguese army, fighting against the Spanish invasion of Portugal (1762) in which he distinguished himself under John Burgoyne at the Battle of Vila Velha.
Poland.
He returned to England in 1763 following the Peace of Paris which ended the Seven Years' War. His regiment was disbanded and he was retired on half pay as a Major. In May 1772, although still inactive, he was promoted to Lieutenant Colonel.
In 1765 he fought in Poland, serving as an aide-de-camp under King Stanislaus II. After many adventures he came home to England. Unable to secure promotion in the British Army, in 1769 he returned to Poland and saw action in the Russo-Turkish War, and lost two fingers in a duel in which he killed his opponent. Returning to England once again, he found that he was sympathetic to the American colonists in their quarrel with Britain. He moved to the colonies in 1773 and in 1775 purchased an estate worth £3,000 in Berkeley County in Virginia, an area now part of West Virginia, near the home of his friend Horatio Gates. He spent ten months travelling through the colonies and acquainting himself with patriots.
American Revolution.
Continental Army.
When war appeared inevitable he resigned his Royal commission and volunteered his services to the colonies. He expected to be named Commander-in-Chief of the Continental Army, being the most experienced candidate. On the other hand, he was born in Britain, somewhat eccentric, slovenly in appearance, coarse in language, and perhaps most of all, he wanted to be paid: by joining the rebellion, he forfeited all his properties in England, and wanted to be compensated. George Washington, on the other hand, was sober, steady, calm, and best of all, would work without pay. Washington also was a good political choice: a southern commander to pair with a primarily New England fighting force. Washington received the appointment, and Lee was offered the subordinate rank of Major General. Because of this, Lee had nothing but the utmost disdain for his superior. He once remarked, "Washington is not fit enough to command a Sergeant's Guard". Lee was often considered second in command of the Continental forces, although Artemas Ward, who was not in good health, officially held this position.
During the encampment at Valley Forge in late 1777 and early 1778, his headquarters was at the David Harvard House.
Southern command.
Lee also received various other titles: in 1776, he was named Commander of the so-called Canadian Department, although he never got to serve in this capacity. Instead, he was appointed as the first Commander of the Southern Department. He served in this post for six months, until he was recalled to the main army. During his time in the South, the British sent an expedition under Henry Clinton to recover Charleston, South Carolina. Lee oversaw the fortification of the city. Fort Sullivan was a fortification built out of palmetto logs, later named for commander Col. William Moultrie. Lee ordered the army to evacuate the fort because as he said it would only last thirty minutes and all soldiers would be killed. Governor John Rutledge forbade Moultrie to evacuate and the fort held. The spongy palmetto logs repelled the cannonball from the British ships. Thus a British assault on Sullivan's Island was driven off and Clinton then abandoned his attempts to capture the city. Even though the credit of the defence was not his he was now called "hero of Charleston".
New York.
When he arrived in New York to join General Washington and the main part of the Continental Army, Washington chose to show his appreciation of General Lee (who was a very popular general officer among not only the army, but Congress) by changing the name of Fort Constitution, which was located on the New Jersey side of the Hudson opposite Fort Washington, to Fort Lee.
Toward the end of 1776, Lee's animosity for Washington began to show. During the retreat from Forts Washington and Lee, he dawdled with his army, and intensified a letter campaign to convince various Congress members that he should replace Washington as Commander-in-Chief.
Capture.
Although his army was supposed to join that of Washington's in Pennsylvania, Lee set a very slow pace. On the night of December 12, Lee and a dozen of his guard inexplicably stopped for the night at "White's Tavern" in Basking Ridge, New Jersey, some three miles from his main army. The next morning, a British patrol of two dozen mounted soldiers found Lee writing letters in his dressing gown, and captured him. Among the members of the British patrol was Cornet Banastre Tarleton. Lee returned to service after he was exchanged for General Richard Prescott.
Battle of Monmouth.
During the Battle of Monmouth in June 1778. Washington needed a secondary commander to lead the frontal assault. He unwillingly chose to put Lee in charge, as he was the most senior of his generals. At first Lee was so reluctant to take part in the attack that Washington bestowed command onto Marquis de Lafayette. Upon this, Lee had a change of heart and requested de Lafayette to cede command, which he gladly did. Washington ordered him to attack the retreating enemy, but instead, Lee ordered a retreat after only one volley of fire. After seeing this, de Lafayette sent a messenger to Washington informing him of this behaviour. Lee's troops retreated directly into Washington and his troops, who were advancing, and Washington dressed him down publicly. Lee responded with insubordination for which he was arrested.
On 2 July 1778 Lee was court-martialled at Brunswick, New Jersey by a jury presided by Lord Stirling on three charges: 1. disobedience of orders in not attacking the enemy; 2. misbehaviour before the enemy in making an unnecessary, disorderly, and shameful retreat; 3. disrespect to the commander-in-chief. Lee was found guilty, and he was relieved of command for a period of one year. (In his "Narrative", Joseph Plumb Martin recounted that Washington rode up during the retreat and asked Martin's officers "by whose order the troops [are] retreating". Being told it was "by Gen. Lee's", he said something that Martin wasn't close enough to hear. Martin was told later, by those who had been close enough, that Washington had said "damn him!" as he rode off, which Martin found unusual but plausible since Washington had been "in a great passion" because of the retreat.)
It is not clear that Lee had made a bad strategic decision: he believed himself outnumbered (which he in fact was: British commander Sir Henry Clinton had 10,000 troops to Lee's 5,440), and that a retreat was reasonable. However, he disobeyed his orders, and he publicly expressed disrespect to his Commander-in-Chief. Furthermore, Washington had wanted to test the abilities of Lee's troops, since they were among the first to be trained in European tactics by Baron von Steuben.
Treachery may have been the reason for Lee's retreat at the Battle of Monmouth. While Lee had been held prisoner by the British General Sir William Howe, 5th Viscount Howe in March 1777, Lee drafted a plan for British military operations against the Americans. At the time, Lee was under a threat of being tried as a deserter from the British Army, because he hadn't resigned his British commission as Lieutenant-Colonel until several days after he accepted an American commission. The plan in Lee's handwriting was found in the Howe family archives in 1857.
Later life.
Lee tried to get Congress to overturn the court-martial's verdict, and when this failed, he resorted to open attacks on Washington's character. Lee's popularity then plummeted. Colonel John Laurens, an aide to Washington, challenged him to a duel, in which Lee was wounded in his side. He was challenged to many more duels. Lee was released from his duty on January 10, 1780. He retired to his estate in the Shenandoah Valley where he bred horses and dogs. While visiting Philadelphia, he was stricken with fever and died in a tavern on October 2, 1782. He was buried there in the churchyard of Christ Church. Lee left his property to his sister, Sidney Lee, who died unmarried in 1788.
Fort Lee, New Jersey, on the west side of the Hudson River (across the water from Fort Washington, New York) was named for him, as were Lee, Massachusetts, Lee, New Hampshire, and Leetown, West Virginia.
Popular culture.
Lee was featured as one of the main antagonists in "Assassin's Creed III", serving as one of the Grand Masters of the Knights Templar after Haytham Kenway's death.
Lee is a character in the 2014 AMC television series "Turn", where he treacherously informs the British of the path of a Continental Army patrol that is then massacred by British-affiliated Rogers' Rangers. Next, Lee tells the British the location of a Continental Army safe house which the Continental Army then uses to surprise and kill almost the entire British detachment sent to attack it. British intelligence officer Major John André uses a honeypot to trap and arrest Lee at an inn in New Jersey and deceive and blackmail him to further help the British cause.
Lee is also a character in the most recent book of Diana Gabaldon's "Outlander" series, "Written in My Own Heart's Blood".

</doc>
<doc id="52291" url="http://en.wikipedia.org/wiki?curid=52291" title="Herod">
Herod

Herod is a name used of several kings belonging to the Herodian Dynasty of the Roman province of Judaea:

</doc>
<doc id="52293" url="http://en.wikipedia.org/wiki?curid=52293" title="Origami">
Origami

Origami (折り紙, from "ori" meaning "folding", and "kami" meaning "paper" ("kami" changes to "gami" due to rendaku) is the art of paper folding, which is often associated with Japanese culture. In modern usage, the word "origami" is used as an inclusive term for all folding practices, regardless of their culture of origin. The goal is to transform a flat sheet square of paper into a finished sculpture through folding and sculpting techniques. Modern origami practitioners generally discourage the use of cuts, glue, or markings on the paper. Origami folders often use the Japanese word "kirigami" to refer to designs which use cuts, although cutting is more characteristic of Chinese papercrafts.
The small number of basic origami folds can be combined in a variety of ways to make intricate designs. The best-known origami model is the Japanese paper crane. In general, these designs begin with a square sheet of paper whose sides may be of different colors, prints, or patterns. Traditional Japanese origami, which has been practiced since the Edo period (1603–1867), has often been less strict about these conventions, sometimes cutting the paper or using nonsquare shapes to start with. The principles of origami are also used in stents, packaging and other engineering applications.
History.
Distinct paperfolding traditions arose in Europe, China, and Japan which have been well-documented by historians. These seem to have been mostly separate traditions, until the 20th century.
In China, traditional funerals often include the burning of folded paper, most often representations of gold nuggets (yuanbao). The practice of burning paper representations instead of full-scale wood or clay replicas dates from the Sung Dynasty (905–1125 CE), though it's not clear how much folding was involved. Traditional Chinese funeral practices were banned during the Cultural Revolution, so most of what we know about Chinese paperfolding comes from the modern-day continuation of these practices in Taiwan.
In Japan, the earliest unambiguous reference to a paper model is in a short poem by Ihara Saikaku in 1680 which mentions a traditional butterfly design used during Shinto weddings. Folding filled some ceremonial functions in Edo period Japanese culture; noshi were attached to gifts, much like greeting cards are used today. This developed into a form of entertainment; the first two instructional books published in Japan are clearly recreational.
In Europe, there was a well-developed genre of napkin-folding, which flourished during the 17th and 18th centuries. After this period, this genre declined and was mostly forgotten; historian Joan Sallas attributes this to the introduction of porcelain, which replaced complex napkin folds as a dinner-table status symbol among nobility. However, some of the techniques and bases associated with this tradition continued to be a part of European culture; folding was a significant part of Friedrich Froebel's "Kindergarten" method, and the designs published in connection with his curriculum are stylistically similar to the napkin fold repertoire.
When Japan opened its borders in the 1860s, as part of a modernization strategy, they imported Froebel's Kindergarten system—and with it, German ideas about paperfolding. This included the ban on cuts, and the starting shape of a bicolored square. These ideas, and some of the European folding repertoire, were integrated into the Japanese tradition. Before this, traditional Japanese sources use a variety of starting shapes, often had cuts; and if they had color or markings, these were added after the model was folded.
In the early 1900s, Akira Yoshizawa, Kosho Uchiyama, and others began creating and recording original origami works. Akira Yoshizawa in particular was responsible for a number of innovations, such as wet-folding and the Yoshizawa–Randlett diagramming system, and his work inspired a renaissance of the art form. During the 1980s a number of folders started systematically studying the mathematical properties of folded forms, which led to a rapid increase in the complexity of origami models.
Techniques and materials.
Techniques.
Many origami books begin with a description of basic origami techniques which are used to construct the models. This includes simple diagrams of basic folds like valley and mountain folds, pleats, reverse folds, squash folds, and sinks. There are also standard named bases which are used in a wide variety of models, for instance the bird base is an intermediate stage in the construction of the flapping bird.
Additional bases are the preliminary base (square base), fish base, waterbomb base, and the frog base.
Origami paper.
Almost any laminar (flat) material can be used for folding; the only requirement is that it should hold a crease.
Origami paper, often referred to as "kami" (Japanese for paper), is sold in prepackaged squares of various sizes ranging from 2.5 cm (1 in) to 25 cm (10 in) or more. It is commonly colored on one side and white on the other; however, dual coloured and patterned versions exist and can be used effectively for color-changed models. Origami paper weighs slightly less than copy paper, making it suitable for a wider range of models.
Normal copy paper with weights of 70–90 g/m2 can be used for simple folds, such as the crane and waterbomb. Heavier weight papers of (19–24&nb 100 g/m2 (approx. 25 lb) or more can be wet-folded. This technique allows for a more rounded sculpting of the model, which becomes rigid and sturdy when it is dry.
Foil-backed paper, as its name implies, is a sheet of thin foil glued to a sheet of thin paper. Related to this is tissue foil, which is made by gluing a thin piece of tissue paper to kitchen aluminium foil. A second piece of tissue can be glued onto the reverse side to produce a tissue/foil/tissue sandwich. Foil-backed paper is available commercially, but not tissue foil; it must be handmade. Both types of foil materials are suitable for complex models.
Washi (和紙) is the traditional origami paper used in Japan. Washi is generally tougher than ordinary paper made from wood pulp, and is used in many traditional arts. Washi is commonly made using fibres from the bark of the gampi tree, the mitsumata shrub ("Edgeworthia papyrifera"), or the paper mulberry but can also be made using bamboo, hemp, rice, and wheat.
Artisan papers such as unryu, lokta, hanji, gampi, kozo, saa, and abaca have long fibers and are often extremely strong. As these papers are floppy to start with, they are often backcoated or resized with methylcellulose or wheat paste before folding. Also, these papers are extremely thin and compressible, allowing for thin, narrowed limbs as in the case of insect models.
Paper money from various countries is also popular to create origami with; this is known variously as Dollar Origami, Orikane, and Money Origami.
Tools.
It is common to fold using a flat surface, but some folders like doing it in the air with no tools, especially when displaying the folding. Many folders believe that no tool should be used when folding. However a couple of tools can help especially with the more complex models. For instance a bone folder allows sharp creases to be made in the paper easily, paper clips can act as extra pairs of fingers, and tweezers can be used to make small folds. When making complex models from origami crease patterns, it can help to use a ruler and ballpoint embosser to score the creases. Completed models can be sprayed so they keep their shape better, and a spray is needed when wet folding.
Types.
Action origami.
Origami not only covers still-life, there are also moving objects; Origami can move in clever ways. Action origami includes origami that flies, requires inflation to complete, or, when complete, uses the kinetic energy of a person's hands, applied at a certain region on the model, to move another flap or limb. Some argue that, strictly speaking, only the latter is really "recognized" as action origami. Action origami, first appearing with the traditional Japanese flapping bird, is quite common. One example is Robert Lang's instrumentalists; when the figures' heads are pulled away from their bodies, their hands will move, resembling the playing of music.
Modular origami.
Modular origami consists of putting a number of identical pieces together to form a complete model. Normally the individual pieces are simple but the final assembly may be tricky. Many of the modular origami models are decorative balls like kusudama, the technique differs though in that kusudama allows the pieces to be put together using thread or glue.
Chinese paper folding includes a style called Golden Venture Folding where large numbers of pieces are put together to make elaborate models. It is most commonly known as "3D origami", however, that name did not appear until Joie Staff published a series of books titled "3D Origami", "More 3D Origami", and "More and More 3D Origami". Sometimes paper money is used for the modules. This style originated from some Chinese refugees while they were detained in America and is also called Golden Venture folding from the ship they came on.
Wet-folding.
Wet-folding is an origami technique for producing models with gentle curves rather than geometric straight folds and flat surfaces. The paper is dampened so it can be moulded easily, the final model keeps its shape when it dries. It can be used, for instance, to produce very natural looking animal models. Size, an adhesive that is crisp and hard when dry, but dissolves in water when wet and becoming soft and flexible, is often applied to the paper either at the pulp stage while the paper is being formed, or on the surface of a ready sheet of paper. The latter method is called external sizing and most commonly uses Methylcellulose, or MC, paste, or various plant starches.
Pureland origami.
Pureland origami adds the restrictions that only simple mountain/valley folds may be used, and all folds must have straightforward locations. It was developed by John Smith in the 1970s to help inexperienced folders or those with limited motor skills. Some designers also like the challenge of creating within the very strict constraints.
Origami tessellations.
This branch of origami is one that has grown in popularity recently. A tessellation is a collection of figures filling a plane with no gaps or overlaps. In origami tessellations, pleats are used to connect molecules such as twist folds together in a repeating fashion.
During the 1960s, Shuzo Fujimoto was the first to explore twist fold tessellations in any systematic way, coming up with dozens of patterns and establishing the genre in the origami mainstream. Around the same time period, Ron Resch patented some tessellation patterns as part of his explorations into kinetic sculpture and developable surfaces, although his work was not known by the origami community until the 1980s. Chris Palmer is an artist who has extensively explored tessellations after seeing the Zilij patterns in the Alhambra, and has found ways to create detailed origami tessellations out of silk. Robert Lang and Alex Bateman are two designers who use computer programs to create origami tessellations.
Kirigami.
Kirigami is a Japanese term for paper cutting. Cutting was often used in traditional Japanese origami, but modern innovations in technique have made the use of cuts unnecessary. Most origami designers no longer consider models with cuts to be origami, instead using the term Kirigami to describe them. This change in attitude occurred during the 1960s and 70s, so early origami books often use cuts, but for the most part they have disappeared from the modern origami repertoire; most modern books don't even mention cutting.
Mathematics and technical origami.
Mathematics and practical applications.
The practice and study of origami encapsulates several subjects of mathematical interest. For instance, the problem of "flat-foldability" (whether a crease pattern can be folded into a 2-dimensional model) has been a topic of considerable mathematical study.
A number of technological advances have come from insights obtained through paper folding. For example, techniques have been developed for the deployment of car airbags and stent implants from a folded position.
The problem of rigid origami ("if we replaced the paper with sheet metal and had hinges in place of the crease lines, could we still fold the model?") has great practical importance. For example, the Miura map fold is a rigid fold that has been used to deploy large solar panel arrays for space satellites.
Origami can be used to construct various geometrical designs not possible with compass and straightedge constructions. For instance paper folding may be used for angle trisection and doubling the cube.
There are plans for an origami airplane to be launched from space. A prototype passed a durability test in a wind tunnel on March 2008, and Japan's space agency adopted it for feasibility studies.
Technical origami.
Technical origami, known in Japanese as origami sekkei (折り紙設計), is an origami design approach in which the model is conceived as an engineered crease pattern, rather than developed through trial-and-error. With advances in origami mathematics, the basic structure of a new origami model can be theoretically plotted out on paper before any actual folding even occurs. This method of origami design was developed by Robert Lang, Meguro Toshiyuki and others, and allows for the creation of extremely complex multi-limbed models such as many-legged centipedes, human figures with a full complement of fingers and toes, and the like.
The crease pattern is a layout of the creases required to form the structure of the model. Paradoxically enough, when origami designers come up with a crease pattern for a new design, the majority of the smaller creases are relatively unimportant and added only towards the completion of the model. What is more important is the allocation of regions of the paper and how these are mapped to the structure of the object being designed. By opening up a folded model, you can observe the structures that comprise it; the study of these structures led to a number of crease-pattern-oriented design approaches
The pattern of allocations is referred to as the 'circle-packing' or 'polygon-packing'. Using optimization algorithms, a circle-packing figure can be computed for any uniaxial base of arbitrary complexity. Once this figure is computed, the creases which are then used to obtain the base structure can be added. This is not a unique mathematical process, hence it is possible for two designs to have the same circle-packing, and yet different crease pattern structures.
As a circle encloses the maximum amount of area for a given perimeter, circle packing allows for maximum efficiency in terms of paper usage. However, other polygonal shapes can be used to solve the packing problem as well. The use of polygonal shapes other than circles is often motivated by the desire to find easily locatable creases (such as multiples of 22.5 degrees) and hence an easier folding sequence as well. One popular offshoot of the circle packing method is box-pleating, where squares are used instead of circles. As a result, the crease pattern that arises from this method contains only 45 and 90 degree angles, which often makes for a more direct folding sequence.
Origami-related computer programs.
A number of computer aids to origami such as TreeMaker and Oripa, have been devised. Treemaker allows new origami bases to be designed for special purposes and Oripa tries to calculate the folded shape from the crease pattern.
Ethics.
Copyright in origami designs and the use of models has become an increasingly important issue in the origami community, as the internet has made the sale and distribution of pirated designs very easy. It is considered good ettiquette to always credit the original artist and the folder when displaying origami models. All commercial rights to designs and models are typically reserved by origami artists; however, the degree to which this can be enforced has been disputed. Normally a person who folds a model using a legally obtained design can publicly display the model unless such rights are specifically reserved, whereas folding a design for money or commercial use of a photo for instance would require consent. The group was set up to represent the copyright interests of origami artists and facilitate permissions requests.
Gallery.
These pictures show examples of various types of origami.

</doc>
<doc id="52302" url="http://en.wikipedia.org/wiki?curid=52302" title="Monticello">
Monticello

Monticello was the primary plantation of Thomas Jefferson, the third President of the United States, who, after inheriting quite a large amount of land from his father, started building Monticello when he was 26 years old. Located just outside Charlottesville, Virginia, in the Piedmont region, the plantation was originally 5000 acre, used for extensive cultivation of tobacco and mixed crops, with labor by slaves. Like other planters, Jefferson shifted from tobacco cultivation to a wheat plantation to respond to changing markets. 
The house, which Jefferson designed, was based on the neoclassical principles described in the books of the Italian Renaissance architect Andrea Palladio. He reworked it through much of his presidency to include design elements popular in late 18th-century Europe. It contains many of his own design solutions. The house is situated on the summit of an 850 ft-high peak in the Southwest Mountains south of the Rivanna Gap. Its name comes from the Italian "little mountain". The plantation, in time, included numerous outbuildings for specialized functions, a nailery, and quarters for domestic slaves along Mulberry Row near the house; gardens for flowers, produce, and Jefferson's experiments in plant breeding; plus tobacco fields and mixed crops. Cabins for field slaves were located further from the mansion.
At Jefferson's direction, he was buried on the grounds, an area now designated as the Monticello Cemetery, which is owned by the Monticello Association, a lineage society of his descendants through Martha Wayles Skelton Jefferson. After Jefferson's death, his daughter Martha Jefferson Randolph sold the property. In 1834 it was bought by Uriah P. Levy, a commodore in the U.S. Navy, who admired Jefferson and spent his own money to preserve the property. His nephew Jefferson Monroe Levy took over the property in 1879; he also invested considerable money to restore and preserve it. He held it until 1923, when he sold it to the "Thomas Jefferson Foundation", which operates it as a house museum and educational institution. It has been designated a National Historic Landmark. In 1987 Monticello and the nearby University of Virginia, also designed by Jefferson, were together designated a UNESCO World Heritage Site.
Design and building.
Jefferson's home was built to serve as a plantation house, which ultimately took on the architectural form of a villa. It has many architectural antecedents but Jefferson went beyond them to create something very much his own. He consciously sought to create a new architecture for a new nation.
Work began on what historians would subsequently refer to as "the first Monticello" in 1768, on a plantation of 5,000 acres (2,000 hectares). Jefferson moved into the South Pavilion (an outbuilding) in 1770, where his new wife Martha Wayles Skelton joined him in 1772. Jefferson continued work on his original design, but how much was completed is of some dispute.
After his wife's death in 1782, Jefferson left Monticello in 1784 to serve as Minister of the United States to France. During his several years' in Europe, he had an opportunity to see some of the classical buildings with which he had become acquainted from his reading, as well as to discover the "modern" trends in French architecture that were then fashionable in Paris. His decision to remodel his own home may date from this period. In 1794, following his service as the first U.S. Secretary of State (1790–93), Jefferson began rebuilding his house based on the ideas he had acquired in Europe. The remodeling continued throughout most of his presidency (1801–09). Although generally completed by 1809, Jefferson continued work on the present structure until his death in 1826.
Jefferson added a center hallway and a parallel set of rooms to the structure, more than doubling its area. He removed the second full-height story from the original house and replaced it with a mezzanine bedroom floor. The interior is centered on two large rooms, which served as an entrance-hall-museum, where Jefferson displayed his scientific interests, and a music-sitting room. The most dramatic element of the new design was an octagonal dome, which he placed above the west front of the building in place of a second-story portico. The room inside the dome was described by a visitor as "a noble and beautiful apartment," but it was rarely used—perhaps because it was hot in summer and cold in winter, or because it could only be reached by climbing a steep and very narrow flight of stairs. The dome room has now been restored to its appearance during Jefferson's lifetime, with "Mars yellow" walls and a painted green floor.
Before his death, Monticello had begun to show signs of disrepair. The attention Jefferson's university project in Charlottesville demanded, and family problems diverted his focus. The most important reason for the mansion's deterioration was his accumulating debts. In the last few years of Jefferson's life, much went without repair in Monticello. A witness, Samuel Whitcomb Jr., who visited Jefferson in 1824, thought it run down. He said, "His house is rather old and going to decay; appearances about his yard and hill are rather slovenly. It commands an extensive prospect but it being a misty cloudy day, I could see but little of the surrounding scenery."
History.
After Jefferson died on July 4, 1826, his only surviving daughter Martha Jefferson Randolph inherited Monticello. The estate was encumbered with debt and Martha Randolph had financial problems in her own family because of her husband's mental illness. In 1831 she sold Monticello to James Turner Barclay, a local apothecary. Barclay sold it in 1834 to Uriah P. Levy, the first Jewish Commodore (equivalent to today's admiral) in the United States Navy. A fifth-generation American whose family first settled in Savannah, Georgia, Levy greatly admired Jefferson. He used his private funds to repair, restore and preserve the house. During the American Civil War, the house was seized and sold by the Confederate government because it was owned by a Northern officer under the sequestration terms of the Alien Enemies Act of 1861 which called for the removal of all residents of northern states from the Confederacy and authorized the Confederate States to take possession and ownership of property in the South owned by ousted northerners. Uriah Levy's estate recovered the property after the war.
Levy's heirs argued over his estate, but their lawsuits were settled in 1879, when Uriah Levy's nephew, Jefferson Monroe Levy, a prominent New York lawyer, real estate and stock speculator (and later member of Congress), bought out the other heirs for $10,050, and took control of the property. Like his uncle, Jefferson Levy commissioned repairs, restoration and preservation at Monticello, which had been deteriorating seriously while the lawsuits wound their way through the courts in New York and Virginia. Together, the Levys preserved Monticello for nearly 100 years.
In 1923, a private non-profit organization, the Thomas Jefferson Foundation, purchased the house from Jefferson Levy with funds raised by Theodore Fred Kuper and others. They managed additional restoration under architects including Fiske Kimball and Milton L. Grigg. Since that time, other restoration has been done at Monticello.
The Foundation operates Monticello and its grounds as a house museum and educational institution. Visitors can view rooms in the cellar and ground floor, but the second and third floors are not open to the general public due to fire code restrictions. Visitors can tour the third floor (Dome), while on a Signature Tour.
Monticello is a National Historic Landmark. It is the only private home in the United States to be designated a UNESCO World Heritage Site. Included in that designation are the original grounds and buildings of Jefferson's University of Virginia. From 1989 to 1992, a team of architects from the Historic American Buildings Survey (HABS) of the United States created a collection of measured drawings of Monticello. These drawings are held by the Library of Congress.
Among Jefferson's other designs are Poplar Forest, his private retreat near Lynchburg (which he intended for his daughter Maria, who died at age 25); the University of Virginia, and the Virginia State Capitol in Richmond.
Decoration and furnishings.
Much of Monticello's interior decoration reflect the personal ideas and ideals of Jefferson.
The original main entrance is through the portico on the east front. The ceiling of this portico incorporates a wind plate connected to a weather vane, showing the direction of the wind. A large clock face on the external east-facing wall has only an hour hand since Jefferson thought this was accurate enough for outdoor laborers (slaves). The clock reflects the time shown on the "Great Clock", designed by Jefferson, in the entrance hall. The entrance hall contains recreations of items collected by Lewis and Clark on the cross-country expedition commissioned by Jefferson to explore the Louisiana Purchase. Jefferson had the floorcloth painted a "true grass green" upon the recommendation of artist Gilbert Stuart, so that Jefferson's "essay in architecture" could invite the spirit of the outdoors into the house.
The south wing includes Jefferson's private suite of rooms. The library holds many books from his third library collection. His first library was burned in an accidental plantation fire, and he 'ceded' (or sold) his second library in 1815 to the United States Congress to replace the books lost when the British burned the Capitol in 1814. This second library formed the nucleus of the Library of Congress.
As "larger than life" as Monticello seems, the house has approximately 11000 sqft of living space. Jefferson considered much furniture to be a waste of space, so the dining room table was erected only at mealtimes, and beds were built into alcoves cut into thick walls that contain storage space. Jefferson's bed opens to two sides: to his cabinet (study) and to his bedroom (dressing room).
The west front ("illustration") gives the impression of a villa of modest proportions, with a lower floor disguised in the hillside.
The north wing includes two guest bedrooms and the dining room. It has a dumbwaiter incorporated into the fireplace, as well as dumbwaiters (shelved tables on castors) and a pivoting serving door with shelves.
Slave quarters on Mulberry Row.
Jefferson located one set of his slaves' quarters on Mulberry Row, a one-thousand foot road of slave, service, and industrial structures. Mulberry Row was situated three-hundred feet (100 m) south of Monticello, with the slave quarters facing the Jefferson mansion. These slave cabins were occupied by the slaves who worked in the mansion or in Jefferson's manufacturing ventures, and not by those who labored in the fields.
Archaeology of the site shows that the rooms of the slave cabins were much larger in the 1770s than in the 1790s. Researchers disagree as to whether this indicates that more slaves were crowded into a smaller spaces, or that fewer people lived in the smaller spaces. Earlier slave houses had a two-room plan, one family per room, with a single, shared doorway to the outside. But from the 1790s on, all rooms/families had independent doorways. Most of the cabins are free-standing, single-room structures.
By the time of Jefferson's death, some slave families had labored and lived for four generations at Monticello. Six families and their descendants are featured in the exhibit, "Slavery at Jefferson's Monticello: Paradox of Liberty" (January to October 2012) at the Smithsonian's National Museum of American History, which also examines Jefferson as slaveholder. Developed as a collaboration between the National Museum of African American History and Culture and Monticello, it is the first exhibit on the national mall to address these issues.
In February 2012, Monticello opened a new outdoor exhibit on its grounds: "Landscape of Slavery: Mulberry Row at Monticello," to convey more about the lives of the hundreds of slaves who lived and worked at the plantation.
Outbuildings and plantation.
The main house was augmented by small outlying pavilions to the north and south. A row of outbuildings (dairy, a washhouse, store houses, a small nail factory, a joinery etc.) and slave's quarters (log cabins), known as Mulberry Row, lay nearby to the south. A stone weaver's cottage survives, as does the tall chimney of the joinery, and the foundations of other buildings. A cabin on Mulberry Row was, for a time, the home of Sally Hemings, the household slave who is widely believed to have had a 38-year relationship with the widower Jefferson and to have borne six children by him, four of whom survived to adulthood. The genealogist Helen F.M. Leary concluded that "the chain of evidence securely fastens Sally Hemings's children to their father, Thomas Jefferson." Later Hemings lived in a room in the "south dependency" below the main house.
On the slope below Mulberry Row, slaves maintained an extensive vegetable garden for Jefferson and the main house. In addition to growing flowers for display and producing crops for eating, Jefferson used the gardens of Monticello for experimenting with different species. The house was the center of a plantation of 5000 acre tended by some 150 slaves. There are also two houses included in the whole.
Programming.
In recent decades, the TJF has created programs to more fully interpret the lives of slaves at Monticello. Beginning in 1993, researchers interviewed descendants of Monticello slaves for the "Getting Word Project", a collection of oral history that provided much new insight into the lives of slaves at Monticello and their descendants. (Among findings were that no slaves adopted Jefferson as a surname, but many had their own surnames as early as the 18th century.)
New research, publications and training for guides has been added since 2000, when the Foundation's Research Committee concluded it was highly likely that Jefferson had fathered Sally Hemings' children.
Some of Mulberry Row has been designated as archeological sites, where excavations and analysis are revealing much about slave life at the plantation. In the winter of 2000-2001, the slave burial ground at Monticello was discovered. In the fall of 2001, the Thomas Jefferson Foundation held a commemoration of the burial ground, in which the names of known slaves of Monticello were read aloud. Additional archeological work is providing information about African-American burial practices.
In 2003 Monticello welcomed a reunion of descendants of Jefferson from both the Wayles' and Hemings' sides of the family. It was organized by the descendants, who have created a new group called the Monticello Community. Additional and larger reunions have been held.
Land purchase.
In 2004, the trustees acquired Mountaintop Farm (also known locally as Patterson's or Brown's Mountain), the only property that overlooks Monticello. Jefferson had called the taller mountain Montalto. To prevent development of new homes on the site, the trustees spent $15 million to purchase the property. Jefferson had owned it as part of his plantation, but it was sold off after his death. In the 20th-century, its farmhouses were divided into apartments for many University of Virginia students. The officials at Monticello had long considered the property an eyesore, and planned to buy it when it came on the market.
Architecture.
The house is similar in appearance to Chiswick House, a Neoclassical house inspired by the architect Andrea Palladio built in 1726-9 in London.
Representation in other media.
Monticello was featured in Bob Vila's A&E Network production, "Guide to Historic Homes of America," in a tour which included Honeymoon Cottage and the Dome Room, which is open to the public during a limited number of tours each year.
Replicas.
A replica has now been built in Somers, Connecticut. It can be seen on Rte 186 also known as Hall Hill Rd Oct. 2014. 
A replica of Monticello was constructed in Chickasha, Oklahoma.
The entrance pavilion of the Naval Academy Jewish Chapel at Annapolis is modeled on Monticello.
Chamberlin Hall at Wilbraham & Monson Academy in Wilbraham, Massachusetts, built in 1962 and modeled on Monticello, serves as the location of the Academy's Middle School.
Scheduled to be complete in July, 2015, Dallas Baptist University is building one of the largest replicas of Monticello known to date. The building will be approximately 23,000 square feet, and will be the home of the Gary Cook School of Leadership.
Legacy.
Monticello's image has appeared on U.S. currency and postage stamps. An image of the west front of Monticello by Felix Schlag has been featured on the reverse of the nickel minted since 1938 (with a brief interruption in 2004 and 2005, when designs of the Westward Journey series appeared instead). It was also used as the title for the 2015 play "Jefferson's Garden", which centred on his life.
Monticello also appeared on the reverse of the two-dollar bill from 1928 to 1966, when the bill was discontinued. The current bill was introduced in 1976 and retains Jefferson's portrait on the obverse but replaced Monticello on the reverse with an engraved modified reproduction of John Trumbull's painting "Declaration of Independence". The gift shop at Monticello hands out two-dollar bills as change.

</doc>
<doc id="52303" url="http://en.wikipedia.org/wiki?curid=52303" title="Day of the Dead">
Day of the Dead

Day of the Dead (Spanish: "") is a Mexican holiday celebrated throughout Mexico, in particular the Central and South regions, and acknowledged around the world in other cultures. The holiday focuses on gatherings of family and friends to pray for and remember friends and family members who have died, and help support their spiritual journey. In 2008 the tradition was inscribed in the Representative List of the Intangible Cultural Heritage of Humanity by UNESCO.
It is particularly celebrated in Mexico where the day is a public holiday. Prior to Spanish colonization in the 16th century, the celebration took place at the beginning of summer. It was moved to October 31, November 1 and November 2 to coincide with the Roman Catholic triduum festival of Allhallowtide: All Hallows' Eve, Hallowmas, and All Souls' Day. Traditions connected with the holiday include building private altars called "ofrendas", honoring the deceased using sugar skulls, marigolds, and the favorite foods and beverages of the departed, and visiting graves with these as gifts. Visitors also leave possessions of the deceased at the graves.
Scholars trace the origins of the modern Mexican holiday to indigenous observances dating back hundreds of years and to an Aztec festival dedicated to the goddess Mictecacihuatl. The holiday has spread throughout the world, being absorbed within other deep traditions for honoring the dead. It happens to be a holiday that has become a national symbol and as such is taught (for educational purposes) in the nation's schools, but there are families who are more inclined to celebrate a traditional "All Saints Day" associated with the Catholic Church.
Originally, the Day of the Dead as such was not celebrated in northern Mexico, where it was even unknown until the 20th century; before that the people and the church rejected it in northeastern Mexico because they perceived the day was a result of syncretizing pagan elements with Catholicism. They held the traditional 'All Saints Day' in the same way as other Catholics in the world. This is due to the limited or nonexistent Mesoamerican influence in this region, and the relatively few indigenous inhabitants from the regions of Southern Mexico. In the early 21st century in northern Mexico, Día de Muertos is observed because the Mexican government made it a national holiday by its educational policies from the 1960s and has tried to use it as a unifying national tradition in the north of the country.
In Brazil, "Dia de Finados" is a public holiday that many Brazilians celebrate by visiting cemeteries and churches. In Spain, festivals and parades are frequently held and people often gather at cemeteries and pray for their deceased loved ones at the end of the day. Similar observances occur elsewhere in Europe, and similarly themed celebrations appear in many Asian and African cultures.
In France and some other European countries, All Souls Day was observed by visits of families to the graves of loved ones, where they left chrysanthemums. Writer Marguerite Yourcenar observed that 
"autumnal rites are among the oldest celebrated on earth. It appears that in every country the Day of the Dead occurs at the year's end, after the last harvests, when the barren earth is though to give passage to the souls lying beneath it."She also notes exceptions to the autumn season, such as the Buddhist Bon festival which is held in summer. But similarly themed celebrations of honoring the dead have been practiced since prehistoric times in many Asian and African cultures.
Observance in Mexico.
Origins.
The Day of the Dead celebrations in Mexico developed from ancient traditions among its pre-Columbian cultures. Rituals celebrating the deaths of ancestors had been observed by these civilizations perhaps for as long as 2,500–3,000 years. 
The festival that developed into the modern Day of the Dead fell in the ninth month of the Aztec calendar, about the beginning of August, and was celebrated for an entire month. The festivities were dedicated to the goddess known as the "Lady of the Dead", corresponding to the modern La Calavera Catrina.
By the late 20th century in most regions of Mexico, the practices had developed to honor dead children and infants on November 1, and to honor deceased adults on November 2. November 1 is generally referred to as "Día de los Inocentes" ("Day of the Innocents") but also as "Día de los Angelitos" ("Day of the Little Angels"); November 2 is referred to as "Día de los Muertos" or "Día de los Difuntos" ("Day of the Dead").
Beliefs.
Frances Ann Day summarizes the three-day celebration, the Day of the Dead:
People go to cemeteries to be with the souls of the departed and build private altars containing the favorite foods and beverages, as well as photos and memorabilia, of the departed. The intent is to encourage visits by the souls, so the souls will hear the prayers and the comments of the living directed to them. Celebrations can take a humorous tone, as celebrants remember funny events and anecdotes about the departed.
Plans for the day are made throughout the year, including gathering the goods to be offered to the dead. During the three-day period families usually clean and decorate graves; most visit the cemeteries where their loved ones are buried and decorate their graves with "ofrendas" (altars), which often include orange Mexican marigolds ("Tagetes erecta") called "cempasúchil" (originally named "cempoaxochitl", Nāhuatl for "twenty flowers"). In modern Mexico the marigold is sometimes called "Flor de Muerto" (Flower of Dead). These flowers are thought to attract souls of the dead to the offerings.
Toys are brought for dead children ("los angelitos", or "the little angels"), and bottles of "tequila, mezcal" or "pulque" or jars of "atole" for adults. Families will also offer trinkets or the deceased's favorite candies on the grave. "Ofrendas" are also put in homes, usually with foods such as candied pumpkin, "pan de muerto" ("bread of dead"), and sugar skulls and beverages such as "atole". The "ofrendas" are left out in the homes as a welcoming gesture for the deceased. Some people believe the spirits of the dead eat the "spiritual essence" of the "ofrendas" food, so though the celebrators eat the food after the festivities, they believe it lacks nutritional value. Pillows and blankets are left out so the deceased can rest after their long journey. In some parts of Mexico such as the towns of Mixquic, Pátzcuaro and Janitzio, people spend all night beside the graves of their relatives. In many places people have picnics at the grave site, as well.
Some families build altars or small shrines in their homes; these sometimes feature Christian cross, statues or pictures of the Blessed Virgin Mary, pictures of deceased relatives and other persons, scores of candles and an "ofrenda". Traditionally, families spend some time around the altar, praying and telling anecdotes about the deceased. In some locations celebrants wear shells on their clothing, so when they dance, the noise will wake up the dead; some will also dress up as the deceased.
Public schools at all levels build altars with "ofrendas", usually omitting the religious symbols. Government offices usually have at least a small altar, as this holiday is seen as important to the Mexican heritage.
Those with a distinctive talent for writing sometimes create short poems, called "calaveras" (skulls), mocking epitaphs of friends, describing interesting habits and attitudes or funny anecdotes. This custom originated in the 18th or 19th century, after a newspaper published a poem narrating a dream of a cemetery in the future, "and all of us were dead", proceeding to read the tombstones. Newspapers dedicate "calaveras" to public figures, with cartoons of skeletons in the style of the famous "calaveras" of José Guadalupe Posada, a Mexican illustrator. Theatrical presentations of "Don Juan Tenorio" by José Zorrilla (1817–1893) are also traditional on this day.
José Guadalupe Posada created a famous print of a figure he called "La Calavera Catrina" ("The Elegant Skull") as a parody of a Mexican upper-class female. Posada's striking image of a costumed female with a skeleton face has become associated with the Day of the Dead, and Catrina figures often are a prominent part of modern Day of the Dead observances.
A common symbol of the holiday is the skull (in Spanish "calavera"), which celebrants represent in masks, called "calacas" (colloquial term for skeleton), and foods such as sugar or chocolate skulls, which are inscribed with the name of the recipient on the forehead. Sugar skulls as gifts can be given to both the living and the dead. Other holiday foods include "pan de muerto", a sweet egg bread made in various shapes from plain rounds to skulls and rabbits, often decorated with white frosting to look like twisted bones.
The traditions and activities that take place in celebration of the Day of the Dead are not universal, often varying from town to town. For example, in the town of Pátzcuaro on the Lago de Pátzcuaro in Michoacán, the tradition is very different if the deceased is a child rather than an adult. On November 1 of the year after a child's death, the godparents set a table in the parents' home with sweets, fruits, "pan de muerto", a cross, a rosary (used to ask the Virgin Mary to pray for them) and candles. This is meant to celebrate the child's life, in respect and appreciation for the parents. There is also dancing with colorful costumes, often with skull-shaped masks and devil masks in the plaza or garden of the town. At midnight on November 2, the people light candles and ride winged boats called "mariposas" (butterflies) to Janitzio, an island in the middle of the lake where there is a cemetery, to honor and celebrate the lives of the dead there.
In contrast, the town of Ocotepec, north of Cuernavaca in the State of Morelos, opens its doors to visitors in exchange for "veladoras" (small wax candles) to show respect for the recently deceased. In return the visitors receive tamales and "atole". This is only done by the owners of the house where someone in the household has died in the previous year. Many people of the surrounding areas arrive early to eat for free and enjoy the elaborate altars set up to receive the visitors from Mictlán.
In some parts of the country (especially the cities, where in recent years other customs have been displaced) children in costumes roam the streets, knocking on people's doors for a "calaverita", a small gift of candies or money; they also ask passersby for it. This relatively recent custom is similar to that of Halloween's trick-or-treating.
Some people believe possessing Day of the Dead items can bring good luck. Many people get tattoos or have dolls of the dead to carry with them. They also clean their houses and prepare the favorite dishes of their deceased loved ones to place upon their altar or "ofrenda".
Observances outside Mexico.
Europe.
Marguerite Yourcenar observed that "autumnal rites are among the oldest celebrated on earth. It appears that in every country the Day of the Dead occurs at the year's end, after the last harvests, when the barren earth is thought to give passage to the souls lying beneath it."
In Christian Europe, Roman Catholic customs absorbed pagan traditions, and All Saints Day and All Souls Day became the autumnal celebration of the dead. Over many centuries, rites were moved from cultivated fields to cemeteries.
In many countries with a Roman Catholic heritage All Saints Day and All Souls Day have evolved traditions in which people take the day off work, go to cemeteries with candles and flowers, and give presents to children, usually sweets and toys. In Portugal and Spain "ofrendas" ("offerings") are made on this day. In Spain, the play "Don Juan Tenorio" is traditionally performed. In Belgium, France, Ireland, Italy, the Netherlands, Portugal, and Spain, people bring flowers (typically chrysanthemums in France) to the graves of dead relatives and say prayers over the dead.
In Austria, Croatia, Finland, Germany, Hungary, Lithuania, Norway, Poland, Romania, Slovakia, Slovenia, and Sweden, the tradition is to light candles and visit the graves of deceased relatives.
In Brittany, a place with deep Celtic traditions, people flock to the cemeteries at nightfall to kneel, bareheaded, at the graves of their loved ones and to anoint the hollow of the tombstone with holy water or to pour libations of milk on it. At bedtime, in the house supper is left on the table for the souls. In Tyrol cakes are left for the dead on the table, and the room is kept warm for their comfort.
As part of a promotion by the Mexican embassy in Prague, Czech Republic, some local citizens join in a Mexican-style Day of the Dead. A theatre group produces an events featuring masks, candles, and sugar skulls.
Latin America.
"Dia de los ñatitas" ("Day of the Skulls") is a festival celebrated in La Paz, Bolivia, on May 5. In pre-Columbian times indigenous Andeans had a tradition of sharing a day with the bones of their ancestors on the third year after burial. Today families keep only the skulls for such rituals. Traditionally, the skulls of family members are kept at home to watch over the family and protect them during the year. On November 9, the family crowns the skulls with fresh flowers, sometimes also dressing them in various garments, and making offerings of cigarettes, coca leaves, alcohol, and various other items in thanks for the year's protection. The skulls are also sometimes taken to the central cemetery in La Paz for a special Mass and blessing.
The Brazilian public holiday of "Finados" (Day of the Dead) is celebrated on November 2. Similar to other Day of the Dead celebrations, people go to cemeteries and churches with flowers and candles, and offer prayers. The celebration is intended as a positive honoring of the dead. Memorializing the dead draws from indigenous, African and European Catholic origins.
In Ecuador the Day of the Dead is observed to some extent by all parts of society, though it is especially important to the indigenous Kichwa peoples, who make up an estimated quarter of the population. "Indigena" families gather together in the community cemetery with offerings of food for a day-long remembrance of their ancestors and lost loved ones. Ceremonial foods include "colada morada", a spiced fruit porridge that derives its deep purple color from the Andean blackberry and purple maize. This is typically consumed with "guagua de pan", a bread shaped like a swaddled infant, though variations include many pigs—the latter being traditional to the city of Loja. The bread, which is wheat flour-based today, but was made with masa in the pre-Columbian era, can be made savory with cheese inside or sweet with a filling of guava paste. These traditions have permeated into mainstream society, as well, where food establishments add both "colada morada" and "gaugua de pan" to their menus for the season. Many non-indigenous Ecuadorians partake in visiting the graves of the deceased, cleaning and bringing flowers, or preparing the traditional foods, too.
Guatemalan celebrations of the Day of the Dead are highlighted by the construction and flying of giant kites in addition to the traditional visits to grave sites of ancestors. A big event also is the consumption of "fiambre", which is made only for this day during the year.
In Haiti voodoo traditions mix with Roman Catholic observances as, for example, loud drums and music are played at all-night celebrations at cemeteries to waken Baron Samedi, the Loa of the dead, and his mischievous family of offspring, the Gede.
Oceania.
Mexican-style Day of the Dead celebrations occur in major cities in Australia, Fiji, and Indonesia. Additionally, prominent celebrations are held in Wellington, New Zealand, complete with altars celebrating the deceased with flowers and gifts.
The Philippines.
In predominantly Roman Catholic Philippines, All Saints' Day and All Souls' Day are family-orientated, solemn celebrations. Both days are taken to be a single occasion (with some urban areas including Halloween) that is traditionally termed Allhallowtide in English and known in the Philippines as Undás (from the Spanish "andas", or possibly "honra"), Todos los Santos (Spanish, "All Saints' Day"), or Araw ng mga Patáy (Tagalog, "Day of the Dead")—the last two terms being proper to the first and second days of November, respectively.
The holiday is often ranked as second in importance after Christmas and Holy Week, as these three observances are the most popular seasons in the year that people return to their respective hometowns. Only 1 November is a considered regular holiday, but the government normally declares adjacent dates as special non-working holidays to allow for travel.
While ancestor veneration is an ancient part of Filipino culture, the modern observance is believed to have been imported from Mexico when the islands (as part of the Spanish East Indies) were governed from Mexico City as part of the Viceroyalty of New Spain. During the holiday, Filipinos customarily visit family tombs and other graves, which they then repair and clean. Entire families spend a night or two at their loved ones' tombs, passing time with card games, eating, drinking, singing and dancing—activities that would be considered improper in some cultures. Prayers such as the rosary are often said for the deceased, who are normally offered candles, flowers, food and even liquor. Some Catholic Chinese Filipino families additionally offer joss sticks to the dead, and observe customs otherwise associated with the Hungry Ghost Festival.
United States.
In many American communities with Mexican residents Day of the Dead celebrations are very similar to those held in Mexico. In some of these communities, such as in Texas, and Arizona, the celebrations tend to be mostly traditional. For example, the All Souls Procession has been an annual Tucson, Arizona event since 1990. The event combines elements of traditional Day of the Dead celebrations with those of pagan harvest festivals. People wearing masks carry signs honoring the dead and an urn in which people can place slips of paper with prayers on them to be burned. Likewise, Old Town San Diego, California annually hosts a very traditional two-day celebration culminating in a candlelight procession to the historic El Campo Santo Cemetery.
In other communities interactions between Mexican traditions and American culture are resulting in celebrations in which Mexican traditions are being extended to make artistic or sometimes political statements. For example, in Los Angeles, California, the Self Help Graphics & Art Mexican-American cultural center presents an annual Day of the Dead celebration that includes both traditional and political elements, such as altars to honor the victims of the Iraq War highlighting the high casualty rate among Latino soldiers. An updated, intercultural version of the Day of the Dead is also evolving at Hollywood Forever Cemetery. There, in a mixture of Mexican traditions and Hollywood hip, conventional altars are set up side-by-side with altars to Jayne Mansfield and Johnny Ramone. Colorful native dancers and music intermix with performance artists, while sly pranksters play on traditional themes.
Similar traditional and intercultural updating of Mexican celebrations are held in San Francisco. For example, the Galería de la Raza, SomArts Cultural Center, Mission Cultural Center, de Young Museum and altars at Garfield Square by the Marigold Project. Oakland is home to "Corazon Del Pueblo" in the Fruitvale district. "Corazon Del Pueblo" has a shop offering handcrafted Mexican gifts and a museum devoted to Day of the Dead artifacts. Also, the Fruitvale district in Oakland serves as the hub of the "Dia de Los Muertos" annual festival which occurs the last weekend of October. Here, a mix of several Mexican traditions come together with traditional Aztec dancers, regional Mexican music, and other Mexican artisans to celebrate the day. In Missoula, Montana skeletal celebrants on stilts, novelty bicycles, and skis parade through town. The festival also occurs annually at historic Forest Hills Cemetery in Boston's Jamaica Plain neighborhood. Sponsored by Forest Hills Educational Trust and the folkloric performance group La Piñata, the Day of the Dead festivities celebrate the cycle of life and death. People bring offerings of flowers, photos, mementos, and food for their departed loved ones, which they place at an elaborately and colorfully decorated altar. A program of traditional music and dance also accompanies the community event.
The Smithsonian Institution, in collaboration with the University of Texas at El Paso and Second Life, have created a Smithsonian Latino Virtual Museum and accompanying multimedia e-book: "Día de los Muertos: Day of the Dead". The project's website contains some of the text and images which explain the origins of some of the customary core practices surrounding the Day of the Dead, such as the background beliefs and the "offrenda" (the special altar commemorating one's deceased loved one). The Made For iTunes multimedia e-book version provides additional content, such as further details; additional photogalleries; pop-up profiles of influential Latino artists and cultural figures over the decades; and video clips of interviews with artists who make Dia de los Muertos-themed artwork, explanations and performances of Aztec and other traditional dances, an animation short that explains the customs to children, virtual poetry readings in English and Spanish.
Similar traditions.
Many other cultures around the world have similar traditions of a day set aside to visit the graves of deceased family members. Often included in these traditions are celebrations, food and beverages, in addition to prayers and remembrances of the departed.
In some African cultures, visits to ancestors' graves, the leaving of food and gifts, and the asking of protection from them serve as important parts of traditional rituals, such as one ritual that is held just before the start of the hunting season.
The Qingming Festival () is a traditional Chinese festival usually occurring around April 5 of the Gregorian calendar. Along with Double Ninth Festival on the 9th day of the 9th month in the Chinese calendar, it is a time to tend to the graves of departed ones. In addition, in the Chinese tradition, the seventh month in the Chinese calendar is called the Ghost Month (鬼月), in which ghosts and spirits come out from the underworld to visit earth.
The Bon Festival (O-bon (お盆), or only Bon (盆)), is a Japanese Buddhist holiday held in August to honor the spirits of departed ancestors. It is derived in part from the Chinese observance of the Ghost Month, and was affixed to the solar calendar along with other traditional Japanese holidays.
In Korea, "Chuseok" (추석, 秋夕; also called "Hangawi") is a major traditional holiday. People go where the spirits of their ancestors are enshrined, and perform ancestral worship rituals early in the morning; they visit the tombs of immediate ancestors to trim plants, clean the area around the tomb, and offer food, drink, and crops to their ancestors.
During the Nepali holiday of "Gai Jatra" ("Cow Pilgrimage"), every family who has lost a member during the previous year creates a "tai" out of bamboo branches, cloth, and paper decorations, in which are placed portraits of the deceased. As a cow traditionally leads the spirits of the dead into the afterlife, an actual or symbolic cow is used depending on local custom. The festival is also a time to dress up in costume reminiscent of the western Halloween, with popular subjects including political commentary and satire.

</doc>
<doc id="52309" url="http://en.wikipedia.org/wiki?curid=52309" title="Rafflesia">
Rafflesia

Rafflesia is a genus of parasitic flowering plants. It contains approximately 28 species (including four incompletely characterized species as recognized by Willem Meijer in 1997), all found in southeastern Asia, on Indonesia, Malaysia, Thailand and the Philippines.
"Rafflesia" was found in the Indonesian rain forest by an Indonesian guide working for Dr. Joseph Arnold in 1818, and named after Sir Thomas Stamford Raffles, the leader of the expedition. It was discovered even earlier by Louis Deschamps in Java between 1791 and 1794, but his notes and illustrations, seized by the British in 1803, were not available to western science until 1861.
The plant has no stems, leaves or true roots. It is a holoparasite of vines in the genus "Tetrastigma" (Vitaceae), spreading its absorptive organ, the haustorium, inside the tissue of the vine. The only part of the plant that can be seen outside the host vine is the five-petaled flower. In some species, such as "Rafflesia arnoldii", the flower may be over 100 cm in diameter, and weigh up to 10 kg. Even the smallest species, "R. baletei", has 12 cm diameter flowers. The flowers look and smell like rotting flesh, hence its local names which translate to "corpse flower" or "meat flower" (see below). The foul odor attracts insects such as flies, which transport pollen from male to female flowers. Most species have separate male and female flowers, but a few have hermaphroditic flowers. Little is known about seed dispersal. However, tree shrews and other forest mammals eat the fruits and disperse the seeds. "Rafflesia" is the official state flower of Indonesia, the Sabah state in Malaysia, and of the Surat Thani Province, Thailand.
The name "corpse flower" applied to "Rafflesia" can be confusing because this common name also refers to the titan arum ("Amorphophallus titanum") of the family Araceae. Moreover, because "Amorphophallus" has the world's largest unbranched inflorescence, it is sometimes mistakenly credited as having the world's largest flower. Both "Rafflesia" and "Amorphophallus" are flowering plants, but they are only distantly related. "Rafflesia arnoldii" has the largest "single" flower of any flowering plant, at least in terms of weight. "A. titanum" has the largest "unbranched" inflorescence, while the talipot palm ("Corypha umbraculifera") forms the largest "branched" inflorescence, containing thousands of flowers; the talipot is monocarpic, meaning the individual plants die after flowering.
Classification.
Comparison of mitochondrial DNA (mtDNA) sequences of "Rafflesia" with other angiosperm mtDNA indicated this parasite evolved from photosynthetic plants of the order Malpighiales. Another study from that same year confirmed this result using both mtDNA and nuclear DNA sequences, and showed the three other groups traditionally classified in Rafflesiaceae were unrelated. A more recent study found "Rafflesia" and its relatives to be embedded within the family Euphorbiaceae, which is surprising, as members of that family typically have very small flowers. According to their analysis, the rate of flower size evolution was more or less constant throughout the family except at the origin of Rafflesiaceae, where the flowers rapidly evolved to become much larger before reverting to the slower rate of change.
Malaysian species.
Species native to Malaysia include "Rafflesia arnoldii", "Rafflesia cantleyi", "Rafflesia hasseltii", "Rafflesia keithii", "Rafflesia kerrii", "Rafflesia pricei", and "Rafflesia tengku-adlinii". "R. arnoldii" boasts the world's largest single bloom. Some endemic Malaysian species, such as "R. keithii", begin blooming at night and start to decompose only two to three days later. The time from bud emergence to flowering is six to nine months. Male and female flowers must be open simultaneously for pollination to occur, hence successful pollination and fruit production are quite rare. In addition to habitat loss, these reproductive limitations are contributing factors to why many species are endangered. "R. keithii" is found along the eastern slopes of Mount Kinabalu in the Lohan Valley of Sabah. "Rafflesia tuan-mudae" is endemic to only Gunung Gading National Park in Sarawak.
Loss of the chloroplast genome.
Research published in the journal "Molecular Biology and Evolution" revealed that one Philippine Rafflesia species from the island of Luzon, "Rafflesia lagascae" (formerly described as "R. manillana") has lost the genome of its chloroplast and it is speculated that the loss happened due to the parasitic lifestyle of the plant. This discovery makes Rafflesia the first land plant without a chloroplast genome, which had once thought to be impossible. 

</doc>
<doc id="52313" url="http://en.wikipedia.org/wiki?curid=52313" title="MathML">
MathML

Mathematical Markup Language (MathML) is a mathematical markup language, an application of XML for describing mathematical notations and capturing both its structure and content. It aims at integrating mathematical formulae into World Wide Web pages and other documents. It is a recommendation of the W3C math working group and part of HTML5.
History.
MathML 1 was released as a W3C recommendation in April 1998 as the first XML language to be recommended by the W3C. Version 1.01 of the format was released in July 1999 and version 2.0 appeared in February 2001.
In October 2003, the second edition of MathML Version 2.0 was published as the final release by the W3C math working group.
MathML was originally designed before the finalization of XML namespaces. However it was assigned a namespace immediately after the Namespace Recommendation was completed, and for XML use, the elements should be in the namespace with namespace URI http://www.w3.org/1998/Math/MathML. When MathML is used in HTML (as opposed to XML) this namespace is automatically inferred by the HTML parser and need not be specified in the document.
MathML version 3.
Version 3 of the MathML specification was released as a W3C Recommendation on 20 October 2010. A recommendation of "A MathML for CSS Profile" was later released on 7 June 2011; this is a subset of MathML suitable for CSS formatting. Another subset, "Strict Content MathML", provides a subset of content MathML with a uniform structure and is designed to be compatible with OpenMath. Other content elements are defined in terms of a transformation to the strict subset. New content elements include <bind> which associates bound variables (<bvar>) to expressions, for example a summation index. The new <share> element allows structure sharing.
The development of MathML 3.0 went through a number of stages. In June 2006 the W3C rechartered the MathML Working Group to produce a MathML 3 Recommendation until February 2008 and in November 2008 extended the charter to April 2010. A sixth Working Draft of the MathML 3 revision was published in June 2009. On 10 August 2010 version 3 graduated to become a "Proposed Recommendation" rather than a draft.
The Second Edition of MathML 3.0 was published as a W3C Recommendation on April 10, 2014.
Presentation and semantics.
MathML deals not only with the "presentation" but also the "meaning" of formula components (the latter part of MathML is known as “Content MathML”). Because the meaning of the equation is preserved separate from the presentation, how the content is communicated can be left up to the user. For example, web pages with MathML embedded in them can be viewed as normal web pages with many browsers, but visually impaired users can also have the same MathML read to them through the use of screen readers (e.g. using the MathPlayer plugin for Internet Explorer, Opera 9.50 build 9656+ or the Fire Vox extension for Firefox).
Presentation MathML.
Presentation MathML focuses on the display of an equation, and has about 30 elements. The elements' names all begin with codice_1. A Presentation MathML expression is built up out of "tokens" that are combined using higher-level elements, which control their layout (there are also about 50 attributes, which mainly control fine details).
Token elements generally only contain characters (not other elements). They include:
Note however that these token elements may be used as extension points, allowing markup in host languages.
MathML in HTML5 allows most inline HTML markup in mtext, and
is conforming, with the HTML markup being used within the MathML to mark up the embedded text (making the first word bold in this example).
These are combined using layout elements, that generally contain only elements. They include:
As usual in HTML and XML, many entities are available for specifying special symbols by name, such as codice_14 and codice_15. An interesting feature of MathML is that entities also exist to express normally-invisible operators, such as codice_16 for implicit multiplication. They are:
U+2061 FUNCTION APPLICATION; U+2062 INVISIBLE TIMES; U+2063 INVISIBLE SEPARATOR; and U+2064 INVISIBLE PLUS. The full specification of MathML entities is closely coordinated with the corresponding specifications for use with HTML and XML in general.
Thus, the expression formula_1 requires two layout elements: one to create the overall horizontal row and one for the superscripted exponent. Including only the layout elements and the (not yet marked up) bare tokens, the structure looks like this:
However, the individual tokens also have to be identified as identifiers (mi), operators (mo), or numbers (mn). Adding the token markup, the full form ends up as:
A valid MathML document typically consists of the XML declaration, DOCTYPE declaration, and document element.
The document body then contains MathML expressions which appear in formula_2 and codice_23 represents formula_3. The elements representing operators and functions are empty elements, because their operands are the other elements under the containing codice_17.
The expression formula_1 could be represented as
Content MathML is nearly isomorphic to expressions in a functional language such as Scheme. codice_26 amounts to Scheme's codice_27, and the many operator and function elements amount to Scheme functions. With this trivial literal transformation, plus un-tagging the individual tokens, the example above becomes:
This reflects the long-known close relationship between XML element structures, and LISP or Scheme S-expressions.
Example and comparison to other formats.
The well-known quadratic formula:
would be marked up using LaTeX syntax like this:
in troff/eqn like this:
 x={-b +- sqrt{b sup 2 – 4ac}} over 2a
in Apache OpenOffice Math and LibreOffice Math like this (all three are valid):
x={-b ± sqrt {b^2 – 4ac}} over 2a
x={-b +- sqrt {b^2 – 4ac}} over 2a
in ASCIIMathML like this:
 x = (-b +- sqrt(b^2 – 4ac)) / (2a)
The above equation could be represented in Presentation MathML as an expression tree made up from layout elements like "mfrac" or "msqrt" elements:
This example uses the codice_28 element, which can be used to embed a semantic annotation in non-XML format, for example to store the formula in the format used by an equation editor such as StarMath or the markup using LaTeX syntax.
Although less compact than TeX, the XML structuring promises to make it widely usable and allows for instant display in applications such as Web browsers and facilitates a straightforward interpretation of its meaning in mathematical software products. MathML is not intended to be written or edited directly by humans.
Embedding MathML in HTML/XHTML files.
MathML, being XML, can be embedded inside other XML files such as XHTML files using XML namespaces. Recent browsers such as Firefox 3+ and Opera 9.6+ (support incomplete) can display Presentation MathML embedded in XHTML.
Inline MathML is also supported in HTML5 files in the current versions of WebKit (Safari), Gecko (Firefox). There is no need to specify namespaces like in the XHTML.
Software support.
Web browsers.
Of the major web browsers, Gecko-based browsers (e.g., Firefox and Camino) have the most complete native support for MathML.
While the WebKit layout engine has a development version of MathML, this feature is only available in version 5.1 and higher of Safari, Chrome 24 but not in later versions of Chrome. Google removed support of MathML claiming architectural security issues and low usage do not justify their engineering time. s of October 2013[ [update]], the WebKit/Safari implementation has numerous bugs.
Opera, between version 9.5 and 12, supports MathML for CSS profile, but is unable to position diacritical marks properly. Prior to version 9.5 it required User JavaScript or custom stylesheets to emulate MathML support. Starting with Opera 14, Opera drops support for MathML by switching to the Chromium 25 engine.
Internet Explorer does not support MathML natively. Support for IE6 through IE9 can be added by installing the MathPlayer plugin. IE10 has some crashing bugs with MathPlayer and Microsoft decided to completely disable in IE11 the binary plug-in interface that MathPlayer needs.<Ref></ref> MathPlayer has a license that may limit its use or distribution in commercial webpages and software. Using or distributing the MathPlayer plugin to display HTML content via the WebBrowser control in commercial software may also be forbidden by this license.
The KHTML-based Konqueror currently does not provide support for MathML.
The quality of rendering of MathML in a browser depends on the installed fonts. The STIX Fonts project have released a comprehensive set of mathematical fonts under an open license. The Cambria Math font supplied with Microsoft Windows had a slightly more limited support.
According to a member of the MathJax team, none of the major browser makers paid any of their developers for any MathML-rendering work; whatever support exists is overwhelmingly the result of unpaid volunteer time/work.
Editors.
Some editors with native MathML support (including copy and paste of MathML) are MathFlow and MathType from Design Science, MathMagic, Publicon from Wolfram Research, and WIRIS. Full MathML editor list at W3C.
MathML is also supported by major office products such as Apache OpenOffice (via OpenOffice Math), LibreOffice (via LibreOffice Math), Calligra Suite (former KOffice), and MS Office 2007, as well as mathematical software products such as Mathematica, Maple and the Windows version of the Casio ClassPad 300. The W3C Browser/Editor Amaya can also be mentioned as a WYSIWYG MathML-as-is editor.
Firemath, an addon for Firefox, provides a WYSIWYG MathML editor.
Most editors will only produce presentation MathML. The MathDox formula editor is an
OpenMath editor also providing presentation and content MathML. Formulator MathML Weaver uses WYSIWYG style to edit Presentation, Content and mixed markups of MathML.
Handwriting recognition.
 can convert handwriting to MathML. Windows 7 has a built-in tool called Math Input Panel. It converts handwriting to MathML. (Unlike the Microsoft Office suite, the Math Input Panel does not use the OMML format, but Office applications can covert/paste from MathML into their preferred internal format.) The underlying technology is also exposed for use in other applications as an ActiveX control called Math Input Control.
Conversion.
Several utilities for converting to and from MathML are available. W3.org maintains a list of MathML related software for download.
Web conversion.
ASCIIMathML provides a JavaScript library to rewrite a convenient Wiki-like text syntax used inline in web pages into MathML on the fly; it works in Gecko-based browsers, and Internet Explorer with MathPlayer. LaTeXMathML does the same for (a subset of) the standard LaTeX mathematical syntax. ASCIIMathML syntax would also be quite familiar to anyone used to electronic scientific calculators.
MathJax, a JavaScript library for inline rendering of mathematical formulae, can be used to translate LaTeX into MathML for direct interpretation by the browser.
Equation Server for .NET from soft4science can be used on the server side (ASP.NET) for TeX-Math (Subset of LaTeX math syntax) to MathML conversion. It can also create bitmap images (Png, Jpg, Gif, etc.) from TeX-Math or MathML input.
jqMath is a JavaScript module that dynamically converts a simple TeX-like syntax to MathML if the browser supports it, else simple HTML and CSS.
LaTeXML is a perl utility to convert LaTeX documents to HTML, optionally either using MathML or converting mathematical expressions to bitmap images.
Support of software developers.
Support of MathML format accelerates software application development in such various topics, as computer-aided education (distance learning, electronic textbooks and other classroom materials); automated creation of attractive reports; computer algebra systems; authoring, training, publishing tools (both for web and desktop-oriented), and many other applications for mathematics, science, business, economics, etc. Several software vendors propose a component edition of their MathML editors, thus providing the easy way for software developers to insert mathematics rendering/editing/processing functionality in their applications. For example, Formulator ActiveX Control from Hermitech Laboratory can be incorporated into an application as a MathML-as-is editor, Design Science offer a toolkit for building web pages that include interactive math (MathFlow Developers Suite,).
Other standards.
Another standard called OpenMath that has been designed (largely by the same people who devised Content MathML) more specifically for storing formulae semantically can also be used to complement MathML. OpenMath data can be embedded in MathML using the codice_29 element. OpenMath "content dictionaries" can be used to define the meaning of codice_30 elements. The following would define "P"1("x") to be the first Legendre polynomial
The OMDoc format has been created for markup of larger mathematical structures than formulae, from statements like definitions, theorems, proofs, or example, to theories and text books. Formulae in OMDoc documents can either be written in Content MathML or in OpenMath; for presentation, they are converted to Presentation MathML.
The ISO/IEC standard Office Open XML (OOXML) defines a different XML math syntax, derived from Microsoft Office products. However, it is partially compatible through relatively simple XSL Transformations.

</doc>
<doc id="52315" url="http://en.wikipedia.org/wiki?curid=52315" title="Peter Waldo">
Peter Waldo

Peter Waldo, Valdo, Valdes, or Waldes (c. 1140 – c. 1205), also Pierre Vaudès or de Vaux, is credited as the founder of the Waldensians, a Christian spiritual movement of the Middle Ages, descendants of which still exist in various regions of southern Europe. 
Life and work.
Specific details of his life are largely unknown. Extant sources relate that he was a wealthy clothier and merchant from Lyons and a man of some learning. Sometime shortly before the year 1160 he was inspired by a series of events, firstly, after hearing a sermon on the life of St. Alexius, secondly, rejection of transubstantiation when it was considered a capital crime to do so, thirdly, the sudden and unexpected death of a friend during an evening meal. From this point onward he began living a radical Christian life giving his property over to his wife, while the remainder of his belongings he distributed as alms to the poor.
At about this time, Waldo began to preach and teach publicly, based on his ideas of simplicity and poverty, notably that "No man can serve two masters, God and Mammon" accompanied by strong condemnations of Papal excesses and Catholic dogmas, including purgatory and transubstantiation, while accusing them of being the harlot from the book of Revelation. By 1170 he had gathered a large number of followers who were referred to as "the Poor of Lyons", "the Poor of Lombardy", or "the Poor of God" who would spread their teaching abroad while disguised as peddlers. Often referred to as the Waldensians (or Waldenses), they were distinct from the Albigensians or Cathari.
The Waldensian movement was characterized from the beginning by lay preaching, voluntary poverty and strict adherence to the Bible. Between 1175-1185 Peter Waldo either commissioned a cleric from Lyons to translate the New Testament into the vernacular, the Arpitan (Franco-Provençal) language or was himself involved in this translation work. Regardless of the source of translation, he is credited with providing to Europe the first translation of the Bible in a 'modern tongue' outside of Latin.
In 1179, Waldo and one of his disciples went to Rome where they were welcomed by Pope Alexander III, and the Roman Curia. They had to explain their faith before a panel of three clergymen, including issues which were then debated within the Church, as the universal priesthood, the gospel in the vulgar tongue, and the issue of self-imposed poverty. The results of the meeting were inconclusive, and Waldo's ideas, but not the movement itself, were condemned at the Third Lateran Council in the same year, though the leaders of the movement had not been yet excommunicated.
Driven away from Lyons, Waldo and his followers settled in the high valleys of Piedmont, and in France, in the Luberon, as they continued in their pursuit of Christianity based on the New Testament. Finally, Waldo was excommunicated by Pope Lucius III during the synod held at Verona in 1184, and the doctrine of the Poor of Lyons was again condemned by the Fourth Lateran Council in 1215 where they are mentioned by name for the first time, and regarded as heresy. The Roman Catholic Church began to persecute the Waldensians, and many were tried and sentenced to death in various European countries during the 12th, 13th, and 14th centuries. These Christians persisted by fleeing to the Alps and hiding there. Centuries after Waldo's death, this Christian movement connected with the Genevan or Reformed branch of the Protestant Reformation.

</doc>
<doc id="52316" url="http://en.wikipedia.org/wiki?curid=52316" title="Mood disorder">
Mood disorder

Mood disorder is a group of diagnoses in the Diagnostic and Statistical Manual of Mental Disorders classification system where a disturbance in the person's mood is hypothesized to be the main underlying feature. The classification is known as mood (affective) disorders in International Classification of Diseases (ICD).
English psychiatrist Henry Maudsley proposed an overarching category of "affective disorder". The term was then replaced by "mood disorder", as the latter term refers to the underlying or longitudinal emotional state, whereas the former refers to the external expression observed by others.
Mood disorders fall into the basic groups of elevated mood such as mania or hypomania, depressed mood of which the best-known and most researched is major depressive disorder (MDD) (commonly called clinical depression, unipolar depression, or major depression), and moods which cycle between mania and depression known as bipolar disorder (BD) (formerly known as manic depression). There are several sub-types of depressive disorders or psychiatric syndromes featuring less severe symptoms such as dysthymic disorder (similar to but milder than MDD) and cyclothymic disorder (similar to but milder than BD). Mood disorders may also be substance-induced or occur in response to a medical condition.
Classification.
Depressive disorders.
Depressive disorder is frequent in primary care and general hospital practice but is often undetected. Unrecognized depressive disorder may slow recovery and worsen prognosis in physical illness, therefore it is important that all doctors be able to recognize the condition, treat the less severe cases, and identify those requiring specialist care.
Bipolar disorders.
A minority of people with bipolar disorder have high creativity, artistry or a particular gifted talent. Before the mania phase becomes too extreme, its energy, ambition, enthusiasm and grandiosity often bring people with this type of mood disorder life's masterpieces.
Substance-induced mood disorders.
A mood disorder can be classified as substance-induced if its etiology can be traced to the direct physiologic effects of a psychoactive drug or other chemical substance, or if the development of the mood disorder occurred contemporaneously with substance intoxication or withdrawal. Also, an individual may have a mood disorder coexisting with a substance abuse disorder. Substance-induced mood disorders can have features of a manic, hypomanic, mixed, or depressive episode. Most substances can induce a variety of mood disorders. For example, stimulants such as amphetamine, methamphetamine, and cocaine can cause manic, hypomanic, mixed, and depressive episodes.
Alcohol-induced mood disorders.
High rates of major depressive disorder occur in heavy drinkers and those with alcoholism. Controversy has previously surrounded whether those who abused alcohol and developed depression were self-medicating their pre-existing depression. But recent research has concluded that, while this may be true in some cases, alcohol misuse directly causes the development of depression in a significant number of heavy drinkers. Participants studied were also assessed during stressful events in their lives and measured on a "Feeling Bad Scale." Likewise, they were also assessed on their affiliation with " deviant " peers, unemployment, and their partner’s substance use and criminal offending.
 High rates of suicide also occur in those who have alcohol-related problems. It is usually possible to differentiate between alcohol-related depression and depression that is not related to alcohol intake by taking a careful history of the patient. Depression and other mental health problems associated with alcohol misuse may be due to distortion of brain chemistry, as they tend to improve on their own after a period of abstinence.
Benzodiazepine-induced mood disorders.
The long-term use of benzodiazepines, such as diazepam and chlordiazepoxide, may have a similar effect on the brain as alcohol, and are also implicated in depression. Major depressive disorder can also develop as a result of chronic use of benzodiazepines or as part of a protracted withdrawal syndrome. Benzodiazepines are a class of medication commonly used to treat insomnia, anxiety, and muscular spasms. As with alcohol, the effects of benzodiazepine on neurochemistry, such as decreased levels of serotonin and norepinephrine, are believed to be responsible for the increased depression. Major depressive disorder may also occur as part of the benzodiazepine withdrawal syndrome. In a long-term follow-up study of patients dependent on benzodiazepines, it was found that 10 people (20%) had taken drug overdoses while on chronic benzodiazepine medication despite only two people ever having had any pre-existing depressive disorder. A year after a gradual withdrawal program, no patients had taken any further overdoses. Depression resulting from withdrawal from benzodiazepines usually subsides after a few months but in some cases may persist for 6–12 months.
Due to another medical condition.
"Mood disorder due to a general medical condition" is used to describe manic or depressive episodes which occur secondary to a medical condition. There are many medical conditions that can trigger mood episodes, including neurological disorders (e.g. dementias), metabolic disorders (e.g. electrolyte disturbances), gastrointestinal diseases (e.g. cirrhosis), endocrine disease (e.g. thyroid abnormalities), cardiovascular disease (e.g. heart attack), pulmonary disease (e.g. chronic obstructive pulmonary disease), cancer, and autoimmune diseases (e.g. rheumatoid arthritis).
Mood disorder not otherwise specified.
Mood disorder not otherwise specified (MD-NOS) is a mood disorder that is impairing but does not fit in with any of the other officially specified diagnoses. In the DSM-IV MD-NOS is described as "any mood disorder that does not meet the criteria for a specific disorder." MD-NOS is not used as a clinical description but as a statistical concept for filing purposes.
Most cases of MD-NOS represent hybrids between mood and anxiety disorders, such as mixed anxiety-depressive disorder or atypical depression. An example of an instance of MD-NOS is being in minor depression frequently during various intervals, such as once every month or once in three days. There is a risk for MD-NOS not to get noticed, and for that reason not to get treated.
Origin.
A number of authors have suggested that mood disorders are an evolutionary adaptation. A low or depressed mood can increase an individual's ability to cope with situations in which the effort to pursue a major goal could result in danger, loss, or wasted effort. In such situations, low motivation may give an advantage by inhibiting certain actions. This theory helps to explain why negative life incidents precede depression in around 80 percent of cases, and why they so often strike people during their peak reproductive years. These characteristics would be difficult to understand if depression were a dysfunction.
A depressed mood is a predictable response to certain types of life occurrences, such as loss of status, divorce, or death of a child or spouse. These are events that signal a loss of reproductive ability or potential, or that did so in humans' ancestral environment. A depressed mood can be seen as an adaptive response, in the sense that it causes an individual to turn away from the earlier (and reproductively unsuccessful) modes of behavior.
A depressed mood is common during illnesses, such as influenza. It has been argued that this is an evolved mechanism that assists the individual in recovering by limiting his/her physical activity. The occurrence of low-level depression during the winter months, or seasonal affective disorder, may have been adaptive in the past, by limiting physical activity at times when food was scarce. It is argued that humans have retained the instinct to experience low mood during the winter months, even if the availability of food is no longer determined by the weather.
Sociocultural aspects.
Kay Redfield Jamison and others have explored the possible links between mood disorders — especially bipolar disorder — and creativity. It has been proposed that a "ruminating personality type may contribute to both [mood disorders] and art."
Jane Collingwood notes an Oregon State University study that
In Liz Paterek’s article "Bipolar Disorder and the Creative Mind" she wrote
The relationship between depression and creativity appears to be especially strong among poets.
Epidemiology.
According to a substantial amount of epidemiology studies conducted, women are twice as likely to develop certain mood disorders, such as major depression. Although there is an equal number of men and women diagnosed with bipolar II disorder, women have a slightly higher frequency of the disorder.
In 2011, mood disorders were the most common reason for hospitalization among children aged 1–17 years in the United States, with approximately 112,000 stays. Mood disorders were top principal diagnosis for Medicaid super-utilizers in the United States in 2012. Further, a study of 18 States found that mood disorders accounted for the highest number of hospital readmissions among Medicaid patients and the uninsured, with 41,600 Medicaid patients and 12,200 uninsured patients being readmitted within 30 days of their index stay—a readmission rate of 19.8 per 100 admissions and 12.7 per 100 admissions, respectively. In 2012, mood and other behavioral health disorders were the most common diagnoses for Medicaid-covered and uninsured hospital stays in the United States (6.1% of Medicaid stays and 5.2% of uninsured stays).
A study conducted in 1988 to 1994 amongst young American adults involved a selection of demographic and health characteristics. A population-based sample of 8,602 men and women ages 17–39 years participated. Lifetime prevalence were estimated based on six mood measures: 
Treatment.
There are different types of treatments available for mood disorders, such as therapy and medications. Behavior Therapy, Cognitive Behavior Therapy and Interpersonal Therapy have all shown to be potential psychological treatments for depression. Major depressive disorder medications usually include antidepressants, while bipolar disorder medications can consist of antipsychotics, mood stabilizers and/or lithium.
DSM-5.
The DSM-5, released in May 2013, separates the mood disorder chapter from the DSM-TR-IV into two sections: Depressive and Related Disorders and Bipolar and Related Disorders. Bipolar Disorders falls in between Depressive Disorders and Schizophrenia Spectrum and Related Disorders “in recognition of their place as a bridge between the two diagnostic classes in terms of symptomatology, family history and genetics” (Ref. 1, p 123). Bipolar Disorders underwent a few changes in the DSM-5, most notably the addition of more specific symptomology related to hypomanic and mixed manic states. Depressive Disorders underwent the most changes, the addition of three new disorders: disruptive mood dysregulation disorder, persistent depressive disorder (previously dysthymia), and premenstrual dysphoric disorder (previously in Appendix B, the section for disorders needing further research). Disruptive mood dysregulation disorder is meant as a diagnosis for children and adolescents who would normally be diagnosed with bipolar disorder as a way to limit the bipolar diagnosis in this age cohort. Major depressive disorder (MDD) also underwent a notable change, in that the bereavement clause has been removed. Those previously exempt from a diagnosis of MDD due to bereavement are now candidates for the MDD diagnosis.

</doc>
<doc id="52318" url="http://en.wikipedia.org/wiki?curid=52318" title="Representative democracy">
Representative democracy

Representative democracy (also indirect democracy or sephocracy) is a variety of democracy founded on the principle of elected officials representing a group of people, as opposed to direct democracy. All modern Western-style democracies are types of representative democracies; for example, the United Kingdom is a constitutional monarchy and Germany is a parliamentary republic.
It is an element of both the parliamentary system or presidential system of government and is typically used in a lower chamber such as the House of Commons (UK) or Bundestag (Germany), and may be curtailed by constitutional constraints such as an upper chamber. It has been described by some political theorists as Polyarchy. In it the power is in the hands of the elected representatives who are elected by the people in elections.
Powers of representatives.
Representatives are elected by the public, as in national elections for the national legislature. Elected representatives may hold the power to select other representatives, presidents, or other officers of government or of the legislature, as the Prime Minister in the latter case. (indirect representation).
The power of representatives is usually curtailed by a constitution (as in a constitutional democracy or a constitutional monarchy) or other measures to balance representative power:
Theorists such as Edmund Burke believe that part of the duty of a representative was not simply to communicate the wishes of the electorate but also to use their own judgement in the exercise of their powers, even if their views are not reflective of those of a majority of voters:
...it ought to be the happiness and glory of a representative to live in the strictest union, the closest correspondence, and the most unreserved communication with his constituents. Their wishes ought to have great weight with him; their opinion, high respect; their business, unremitted attention. It is his duty to sacrifice his repose, his pleasures, his satisfactions, to theirs; and above all, ever, and in all cases, to prefer their interest to his own. But his unbiassed opinion, his mature judgment, his enlightened conscience, he ought not to sacrifice to you, to any man, or to any set of men living. These he does not derive from your pleasure; no, nor from the law and the constitution. They are a trust from Providence, for the abuse of which he is deeply answerable. Your representative owes you, not his industry only, but his judgment; and he betrays, instead of serving you, if he sacrifices it to your opinion.
History.
The Roman Republic was the first government in the western world to have a representative government, despite taking the form of a direct government in the Roman assemblies. The Roman model of governance inspired many political thinkers over the centuries, and today's modern representative democracies imitate more the Roman than the Greek models because it was a state in which supreme power was held by the people and their elected representatives, and which had an elected or nominated leader. Representative democracy is a form of democracy in which people vote for representatives who then vote on policy initiatives as opposed to a direct democracy, a form of democracy in which people vote on policy initiatives directly. A European medieval tradition of selecting representatives from the various estates (classes, but not as we know them today) to advise/control monarchs led to relatively wide familiarity with representative systems inspired by Roman systems.
Representative democracy came into particular general favour in post-industrial revolution nation states where large numbers of subjects or (latterly) citizens evinced interest in politics, but where technology and population figures remained unsuited to direct democracy. As noted above, Edmund Burke in his speech to the electors of Bristol classically analysed their operation in Britain and the rights and duties of an elected representative.
Globally, a majority of the world's people live in representative democracies including constitutional monarchies with strong representative branches.
Research on Representation Per Se.
Separate but related, and very large, bodies of research in political philosophy and social science investigate how and how well elected representatives, such as legislators, represent the interests or preferences of one another constituency. Some of the particular lines of such research and the findings therein are described under the topic Representation (politics).
Criticisms.
In his book "Political Parties", written in 1911, Robert Michels argues that most representative systems deteriorate towards an oligarchy or particracy. This is known as the "Iron Law of Oligarchy."
Representative democracies which are stable have been analysed by Adolf Gasser and compared to the unstable representative democracies in his book "Gemeindefreiheit als Rettung Europas" which was published in 1943 (first edition in German) and a second edition in 1947 (in German). Adolf Gasser stated the following requirements for a representative democracy in order to remain stable, unaffected by the "Iron Law of Oligarchy": 
A drawback to this type of government is that elected officials are not required to fulfill promises made before their election.
The system of stochocracy has been proposed as an improved system compared to the system of representative democracy, where representatives are elected. Stochocracy aims to at least reduce this degradation by having all representatives appointed by lottery instead of by voting. Therefore this system is also called lottocracy. The system was proposed by the writer Roger de Sizif in 1998 in his book "La Stochocratie". Choosing officeholders by lot was also the standard practice in ancient Athenian democracy. The rationale behind this practice was to avoid lobbying and electioneering by economic oligarchs.

</doc>
<doc id="52319" url="http://en.wikipedia.org/wiki?curid=52319" title="Livable Netherlands">
Livable Netherlands

Livable Netherlands (Dutch: "Leefbaar Nederland", LN) was a Dutch political party. Pim Fortuyn began his political career in the party.
Party history.
Historically there have always been parties in States-Provincial (provincial legislatures) and Gemeenteraad (municipal assemblies or city councils) that were independent from the national party system. In the predominantly Catholic south of the Netherlands during the 1950s, the Catholic People's Party gained eighty percent of the vote in national elections, local and provincial groups of independents were organized.
During the 1980s, independent parties began to spread to other parts of the Netherlands. Some successful groups like Independent Rijswijk began to move towards a national movement. A congress for this purpose held in 1989 did not result in much progress towards this goal, although independent parties were still very successful in municipal elections especially in Hilversum and Utrecht.
Some of them choose the name "Leefbaar" (liveable). It became a distinctive political movement. It is not a party in itself but consisted of many municipal branches. These branches had no formal ties, and often have radically different programs, sharing only their disdain for the political establishment.
In 1999, prominent media personalities Henk Westbroek and Jan Nagel, chairs of the highly successful Leefbaar Utrecht and Leefbaar Hilversum parties respectively, founded Leefbaar Nederland as a spin-off from their local parties. Nagel became the party's chair. Nagel had previously been chair of the VARA, a broadcaster linked to the PvdA.
In 2001 the party's support and visibility began to increase. They came to be seen as an opposition movement against the second Kok cabinet. In November 2001 Pim Fortuyn was elected as the party's lijsttrekker. On February 10, a few months before the election he was discharged because of a controversial interview published in the Volkskrant newspaper. Fortuyn subsequently organized his own party, the Lijst Pim Fortuyn. On March 10, the Amsterdam public prosecutor Fred Teeven was chosen as new lijsttrekker. In the elections of May 2002 the party won only two seats in the House of Representatives.
In the elections of 2003, self-help guru Emile Ratelband was put forward by the party board as their candidate for the lijsttrekkers' position. Teeven, who had gained some recognition as MP, withdrew his candidacy for the position when a motion of no confidence was not supported by the party's congress. During a tumultuous congress, the 22-year-old Haitske van der Linde, the daughter of TV personality Wubbo van der Linde and candidate of the party's youth movement J@L, was elected lijsttrekker. She was unable to hold on to the two seats the party held, and the party left parliament. Ratelband, who had formed his own list, was still less successful in the polls. The party tried to dissolve itself, but there were not enough members present at the congress to do that, and the party left the public spotlight. In 2006 the party announced that it would disband itself: it owed a large debt to the Ministry of Home Affairs, who had lent them money for the 2003 elections, and only a handful of paying members were left.
Name.
The term "Leefbaar" was turned into a political brand by the Leefbaar Utrecht and Hilversum parties; the founders tried to reproduce this success by taking over the name.
Ideology and Issues.
The party was a populist party, oriented at democratizing society and solving several difficult political issues pragmatically. The party saw itself as a movement against the 'old parties' and especially those cooperating in the Kok II cabinet.
They had a ten-point plan which included:
Representation.
This table shows the results of the LN in elections to the House of Representatives, Senate and European elections, as well as the party's political leadership: the fractievoorzitter, is the chair of the parliamentary party and the lijsttrekker is the party's top candidate in the general election, these posts are normally taken by the party's leader.
Municipal and Provincial Government.
The municipal and local Leefbaar parties were not an official part of the party. Some of these parties, most notably Leefbaar Rotterdam were however founded around the same time hoping to gain from the same momentum. Many of these parties are represented in provincial and municipal legislatures and cooperated in several municipal executives. They were highly successful in the 2002 municipal elections, but lost much of their support in the 2006 municipal elections.
Electorate.
The party was supported by many independent voters, who no longer felt connected to a particular party.
Organization.
Organizational structure.
The highest organ of LN was the Congress in which every member could participate. It convened once every year. It appointed the party board and decided the order of the House of Representatives, Senate and European Parliament candidate lists and had the final say over the party program.
Linked organisations.
The party's youth organisation was called Jong@Leefbaar.nl (Young@Livable.nl; J@L). The party published "De Leefbaar Koerier" (Livable Courier). The scientific institute of the party was called Foundation Scientific Bureau Livable Netherlands, which published "De Fundering" (The Foundation).
International Comparison.
Internationally, Leefbaar Nederland may be compared to Forza Italia, a populist party centered around a prominent media personality.

</doc>
<doc id="52321" url="http://en.wikipedia.org/wiki?curid=52321" title="Freedom House">
Freedom House

Freedom House is a U.S.-based non-governmental organization (NGO) that conducts research and advocacy on democracy, political freedom, and human rights. Freedom House was founded in October 1941. Wendell Willkie and Eleanor Roosevelt served as its first honorary chairpersons. It describes itself as a "clear voice for democracy and freedom around the world".
The organization's annual "Freedom in the World" report, which assesses each country's degree of political freedoms and civil liberties, is frequently cited by political scientists, journalists, and policy-makers. "Freedom of the Press" and "Freedom of the Net", which monitor censorship, intimidation and violence against journalists, and public access to information, are among its other signature reports.
History.
Freedom House was incorporated October 31, 1941.:293 Among its founders were Eleanor Roosevelt, Wendell Willkie, Mayor Fiorello H. La Guardia, Elizabeth Cutter Morrow, Dorothy Thompson, George Field, Herbert Agar, Herbert Bayard Swope, Ralph Bunche, Father George B. Ford, Roscoe Drummond and Rex Stout. George Field (1904–2006) was executive director of the organization until his retirement in 1967.
According to its website, Freedom House "emerged from an amalgamation of two groups that had been formed, with the quiet encouragement of President Franklin D. Roosevelt, to encourage popular support for American involvement in World War II at a time when isolationist sentiments were running high in the United States." Several groups, in fact, were aggressively supporting U.S. entry into the war and in early fall 1941, when various group activities began to overlap, the Fight for Freedom Committee began exploring a mass merger. George Field then conceived the idea of all of the groups maintaining their separate identities under one roof — Freedom House — to promote the concrete application of the principles of freedom.:293
Freedom House had physical form in a New York City building that represented the organization's goals. A converted residence at 32 East 51st Street opened January 22, 1942,:293 as a center "where all who love liberty may meet, plan their programs and encourage one another". Furnished as a gift of the Allied nations, the 19-room building included a broadcasting facility.
Freedom House sponsored influential radio programs including "The Voice of Freedom" (1942–43) and "Our Secret Weapon" (1942–43), a CBS radio series created to counter Axis shortwave radio propaganda broadcasts. Rex Stout, chairman of the Writers' War Board and representative of Freedom House, would rebut the most entertaining lies of the week. The series was produced by Paul White, founder of CBS News.:529:305
In 1945 an elegant building at 20 West 40th Street was purchased to house the organization. It was named the Willkie Memorial Building.
After the war, as its website states, "Freedom House took up the struggle against the other twentieth century totalitarian threat, Communism... The organization's leadership was convinced that the spread of democracy would be the best weapon against totalitarian ideologies." Freedom House supported the Marshall Plan and the establishment of NATO. Freedom House also supported the Johnson Administration's Vietnam War policies. 
Freedom House was highly critical of McCarthyism. During the 1950s and 1960s, it supported the U.S. Civil Rights Movement and its leadership included several prominent civil rights activists—though it was critical of civil rights leaders such as Martin Luther King, Jr. for their anti-war activism. It supported Andrei Sakharov, other Soviet dissidents, and the Solidarity movement in Poland. Freedom House assisted the post-Communist societies in the establishment of independent media, non-governmental think tanks, and the core institutions of electoral politics.
The organization describes itself currently as a clear voice for democracy and freedom around the world. Freedom House states that it:
has vigorously opposed dictatorships in Central America and Chile, apartheid in South Africa, the suppression of the Prague Spring, the Soviet war in Afghanistan, genocide in Bosnia and Rwanda, and the brutal violation of human rights in Cuba, Burma, the People's Republic of China, and Iraq. It has championed the rights of democratic activists, religious believers, trade unionists, journalists, and proponents of free markets.
In 1967, Freedom House absorbed Books USA, which had been created several years earlier by Edward R. Murrow, as a joint venture between the Peace Corps and the United States Information Service.
More recently, Freedom House has supported citizens involved in challenges to the existing regimes in Serbia, Ukraine, Kyrgyzstan, Egypt, Tunisia and elsewhere. The organization states, "From South Africa to Jordan, Kyrgyzstan to Indonesia, Freedom House has partnered with regional activists in bolstering civil society; worked to support women’s rights; sought justice for victims of torture; defended journalists and free expression advocates; and assisted those struggling to promote human rights in challenging political environments." Freedom House was critical of Saudi Arabia and Chile under Augusto Pinochet, classifying them as "Not Free". It was also strongly critical of the apartheid in South Africa and military dictatorships in Latin America.
In 2001 Freedom House had income of around $11m, increasing to over $26m in 2006. Much of the increase was due to an increase between 2004 and 2005 in US government federal funding, from $12m to $20m. Federal funding fell to around $10m in 2007, but still represented around 80% of Freedom House's budget. As of 2010, grants awarded from the US government accounted for most of Freedom House's funding; the grants were not earmarked by the government but allocated through a competitive process.
Organization.
Freedom House is a nonprofit organization. Headquartered in Washington, D.C., it has field offices in about a dozen countries, including Ukraine, Hungary, Serbia, Jordan, Mexico, and also countries in Central Asia.
Freedom House states that its Board of Trustees is composed of "business and labor leaders, former senior government officials, scholars, writers, and journalists". All board members are current residents of the United States. It does not identify itself with either of the American Republican or the Democratic parties. Members of the organization's board of directors include Kenneth Adelman, Farooq Kathwari, Azar Nafisi, Mark Palmer, P.J. O'Rourke and Lawrence Lessig, while past board-members have included Zbigniew Brzezinski, Jeane Kirkpatrick, Samuel Huntington, Mara Liasson, Otto Reich, Donald Rumsfeld, Whitney North Seymour, Paul Wolfowitz, Steve Forbes and Bayard Rustin.
Reports.
"Freedom in the World".
Since 1972 (1978 in book form), Freedom House publishes an annual report, "Freedom in the World", on the degree of democratic freedoms in nations and significant disputed territories around the world, by which it seeks to assess the current state of civil and political rights on a scale from 1 (most free) to 7 (least free).
Until 2003, states where the average for political and civil liberties differed from 1.0 to 2.5 were considered "free". States with values from 3.0 to 5.5 were considered "partly free" and those with values between 5.5 and 7.0 as "not free". Since 2003 the scope of the "partly free" ranges from 3.0 to 5.0, "not free" from 5.5 to 7.0.
These reports are often used by political scientists when doing research. The ranking is highly correlated with several other ratings of democracy also frequently used by researchers.
In its 2003 report, for example, Canada (judged as fully free and democratic) got a perfect score of a "1" in civil liberties and a "1" in political rights, earning it the designation of "free." Nigeria got a "5" and a "4," earning it the designation of "partly free," while North Korea scored the lowest rank of "7-7," and was thus dubbed "not free." Nations are scored from 0 to 4 on several questions and the sum determines the rankings. Example questions: "Is the head of state and/or head of government or other chief authority elected through free and fair elections?", "Is there an independent judiciary?", "Are there free trade unions and peasant organizations or equivalents, and is there effective collective bargaining? Are there free professional and other private organizations?" Freedom House states that the rights and liberties of the survey are derived in large measure from the Universal Declaration of Human Rights.
The research and ratings process involved two dozen analysts and more than a dozen senior-level academic advisors. The eight members of the core research team headquartered in New York, along with 16 outside consultant analysts, prepared the country and territory reports. The analysts used a broad range of sources of information—including foreign and domestic news reports, academic analyses, nongovernmental organizations, think tanks, individual professional contacts, and visits to the region—in preparing the reports.
The country and territory ratings were proposed by the analyst responsible for each related report. The ratings were reviewed individually and on a comparative basis in a series of six regional meetings — Asia-Pacific, Central and Eastern Europe and the Former Soviet Union, Latin America and the Caribbean, Middle East and North Africa, Sub-Saharan Africa, and Western Europe — involving the analysts, academic advisors with expertise in each region, and Freedom House staff. The ratings were compared to the previous year's findings, and any major proposed numerical shifts or category changes were subjected to more intensive scrutiny. These reviews were followed by cross-regional assessments in which efforts were made to ensure comparability and consistency in the findings. Many of the key country reports were also reviewed by the academic advisers.
The survey's methodology is reviewed periodically by an advisory committee of political scientists with expertise in methodological issues.
Freedom House also produces annual reports on press freedom ("Press Freedom Survey"), governance in the nations of the former Soviet Union ("Nations in Transit"), and countries on the borderline of democracy ("Countries at the Crossroads"). In addition, one-time reports have included a survey of women's freedoms in the Middle East.
Freedom House's methods (around 1990) and other democracy-researchers were mentioned as examples of an expert-based evaluation by sociologist Kenneth A. Bollen, who is also an applied statistician. Bollen writes that expert-based evaluations are prone to statistical bias of an unknown direction, that is, not known either to agree with U.S. policy or to disagree with U.S. policy: "Regardless of the direction of distortions, it is highly likely that every set of indicators formed by a single author or organization contains systematic measurement error. The origin of this measure lies in the common methodology of forming measures. Selectivity of information and various traits of the judges fuse into a distinct form of bias that is likely to characterize all indicators from a common publication."
"Freedom of the Press".
The "Freedom of the Press" index is an annual survey of media independence that assesses the degree of print, broadcast, and internet freedom throughout the world. It provides numerical rankings and rates each country's media as "Free," "Partly Free," or "Not Free." Individual country narratives examine the legal environment for the media, political pressures that influence reporting, and economic factors that affect access to information.
The annual survey, which provides analytical reports and numerical ratings for 196 countries and territories in 2011, continues a process conducted since 1980. The findings are widely used by governments, international organizations, academics, and the news media in many countries. Countries are given a total score from 0 (best) to 100 (worst) on the basis of a set of 23 methodology questions divided into three subcategories: legal environment, political environment, and the economic environment. Assigning numerical points allows for comparative analysis among the countries surveyed and facilitates an examination of trends over time. Countries scoring 0 to 30 are regarded as having “Free” media; 31 to 60, “Partly Free” media; and 61 to 100, “Not Free” media. The ratings and reports included in each annual report cover events that took place during the previous year, for example "Freedom of the Press 2011" covers events that took place between January 1, 2010 and December 31, 2010.
The study is based on universal criteria and recognizes cultural differences, diverse national interests, and varying levels of economic development. The starting point is the smallest, most universal unit of concern: the individual. The survey uses a multilayered process of analysis and evaluation by a team of regional experts and scholars, including an internal research team and external consultants. The diverse nature of the methodology questions seeks to encompass the varied ways in which pressure can be placed upon the flow of information and the ability of print, broadcast, and internet-based media to operate freely and without fear of repercussions. The report provides a picture of the entire “enabling environment” in which the media in each country operate. Degree of news and information diversity available to the public is also addressed.
An independent review of press freedom studies, commissioned by the Knight Foundation in 2006, found that FOP is the best in its class of Press Freedom Indicators.
"Freedom on the Net".
The "Freedom on the Net" reports provide analytical reports and numerical ratings regarding the state of Internet freedom for countries worldwide. The countries surveyed represent a sample with a broad range of geographical diversity and levels of economic development, as well as varying levels of political and media freedom. The surveys ask a set of questions designed to measure each country’s level of Internet and digital media freedom, as well as the access and openness of other digital means of transmitting information, particularly mobile phones and text messaging services. Results are presented for three areas: 
The results from the three areas are combined into a total score for a country (from 0 for best to 100 for worst) and countries are rated as "Free" (0 to 30), "Partly Free" (31 to 60), or "Not Free" (61 to 100) based on the totals.
As of April 2015 Freedom House has produced five editions of the report, the first in 2009 surveyed 15 countries, the second in 2011 surveyed 37 countries, the third in 2012 surveyed 47 countries, the fourth in 2013 surveyed 60 countries, and the fifth in 2014 surveyed 65 countries.
In addition the 2012 report identified seven countries that were at particular risk of suffering setbacks related to Internet freedom in late 2012 and in 2013: Azerbaijan, Libya, Malaysia, Pakistan, Rwanda, Russia, and Sri Lanka. In most of these countries the Internet is currently a relatively open and unconstrained space for free expression, but the countries also typically feature a repressive environment for traditional media and have recently considered or introduced legislation that would negatively affect Internet freedom.
Other annual reports.
Freedom House also produces these annual reports:
Special reports.
Freedom House has produced more than 85 special reports since 2002, including: 
Other activities.
In addition to these reports, Freedom House participates in advocacy initiatives, currently focused on North Korea, Africa, and religious freedom. It has offices in a number of countries, where it promotes and assists local human rights workers and non-government organizations.
On January 12, 2006, as part of a crackdown on unauthorized nongovernmental organizations, the Uzbek government ordered Freedom House to suspend operations in Uzbekistan. Resource and Information Centers managed by Freedom House in Tashkent, Namangan, and Samarkand offered access to materials and books on human rights, as well as technical equipment, such as computers, copiers and Internet access. The government warned that criminal proceedings could be brought against Uzbek staff members and visitors following recent amendments to the criminal code and Code on Administrative Liability of Uzbekistan. Other human rights groups have been similarly threatened and obliged to suspend operations.
Freedom House is a member of the International Freedom of Expression Exchange, a global network of more than 80 non-governmental organizations that monitors free expression violations around the world and defends journalists, writers and others who are persecuted for exercising their right to freedom of expression. Freedom House also publishes the , a weekly analysis on press freedom in and related to the People's Republic of China.
Criticism.
The "Financial Times" has reported that Freedom House is one of several organizations selected by the State Department to receive funding for 'clandestine activities' inside Iran. In a research study, Freedom House sets out its conclusions: "Far more often than is generally understood, the change agent is broad-based, non-violent civic resistance - which employs tactics such as boycotts, mass protests, blockades, strikes and civil disobedience to de-legitimate authoritarian rulers and erode their sources of support, including the loyalty of their armed defenders."
On June 8, 2006, the vice-chairman of Freedom House's board of trustees asked the U.S. Senate to increase the share of NGO funding aimed at helping support non-violent foreign democratic activists organize for potential overthrows of their non-democratic governments. Palmer argued in favor of shifting funding away from NGOs working in already democratic nations to fund this effort.
Cuban, Sudanese and Chinese criticism.
In May 2001, the Committee on Non-Governmental Organizations of the United Nations heard arguments for and against Freedom House. Representatives of Cuba alleged that the organization is a U.S. foreign policy instrument linked to the CIA and "submitted proof of the politically motivated, interventionist activities the NGO (Freedom House) carried out against their Government". They also claimed a lack of criticism of U.S. human rights violations in the annual reports. Cuba also claimed that these violations are well documented by other reports, such as those of Human Rights Watch. Other countries such as China and Sudan also gave criticism. The Russian representative inquired "why this organization, an NGO which defended human rights, was against the creation of the International Criminal Court."
The U.S. representative stated that alleged links between Freedom House and the CIA were "simply not true." The representative said he agreed that the NGO receives funds from the United States Government, but said this is disclosed in its reports. The representative said the funds were from the United States Agency for International Development (USAID), which was not a branch of the CIA. The representative said his country had a law prohibiting the government from engaging in the activities of organizations seeking to change public policy, such as Freedom House. The representative said his country was not immune from criticism from Freedom House, which he said was well documented. The US representative further argued that Freedom House was a human rights organization which sought to represent those who did not have a voice. The representative said he would continue to support NGOs who criticized his government and those of others.
Russia.
Russia, identified by Freedom House as "Not Free", called Freedom House biased and accused the group of serving U.S. interests. Sergei Markov, an MP from the United Russia party, called Freedom House a "Russophobic" organization. "You can listen to everything they say, except when it comes to Russia," Markov argued. "There are many Russophobes there," he asserted. In response, Christopher Walker, director of studies at Freedom House, argued that Freedom House made its evaluations based on objective criteria explained on the organization's web site, and he denied that it had a pro-U.S. agenda. "If you look closely at the 193 countries that we evaluate, you'll find that we criticize what are often considered strategic allies of the United States," he said.
Daniel Treisman, a UCLA political scientist, has criticised Freedom House's assessment of Russia. Treisman has pointed out that Freedom House ranks Russia's political rights on the same level as the United Arab Emirates, which, according to Freedom House, is a federation of absolute monarchies with no hint of democracy anywhere in the system. Freedom House also ranks Russia's civil liberties on the same scale as those of Yemen. In Yemen, according to the constitution, Sharia law is the only source of legislation, and allows assaults and killings of women for alleged immoral behaviour. Criticising the president is illegal in Yemen. Treisman contrasts Freedom House's ranking with the Polity IV scale used by academics and in which Russia has a much better score. In the Polity IV scale, Saudi Arabia is a consolidated autocracy (-10), while the United States is a consolidated democracy (+10); Russia has a score of +4, while United Arab Emirates has a score of -8.
U.S. domestic criticism.
On December 7, 2004, U.S. House Representative Ron Paul criticized Freedom House for allegedly administering a U.S.-funded program in Ukraine where "much of that money was targeted to assist one particular candidate." Paul said that
"one part that we do know thus far is that the U.S. government, through the U.S. Agency for International Development (USAID), granted millions of dollars to the Poland-America-Ukraine Cooperation Initiative (PAUCI), which is administered by the U.S.-based Freedom House. PAUCI then sent U.S. Government funds to numerous Ukrainian non-governmental organizations (NGOs). This would be bad enough and would in itself constitute meddling in the internal affairs of a sovereign nation. But, what is worse is that many of these grantee organizations in Ukraine are blatantly in favor of presidential candidate Viktor Yushchenko."
Noam Chomsky and Edward S. Herman have criticized the organization for excessively criticizing states opposed to US interests while being unduly sympathetic to regimes supportive of US interests. For example, Freedom House described the Rhodesian general election of 1979 as "fair", but described the Southern Rhodesian 1980 elections as "dubious", and it found El Salvador's 1982 election to be "admirable".
Alleged partiality toward Uzbekistan.
Craig Murray, the British ambassador to Uzbekistan from 2002 to 2004, wrote that the executive director of Freedom House told him in 2003 that the group decided to back off from its efforts to spotlight human rights abuses in Uzbekistan, because some Republican board members (in Murray’s words) "expressed concern that Freedom House was failing to keep in sight the need to promote freedom in the widest sense, by giving full support to U.S. and coalition forces". Human rights abuses in Uzbekistan at the time included treatment of prisoners who were killed by "immersion in boiling liquid," and by strapping on a gas mask and blocking the filters, Murray reported. Jennifer Windsor, the executive director of Freedom House in 2003, replied that Murray's "characterization of our conversation is an inexplicable misrepresentation not only of what was said at that meeting, but of Freedom House’s record in Uzbekistan ... Freedom House has been a consistent and harsh critic of the human rights situation in Uzbekistan, as clearly demonstrated in press releases and in our annual assessments of that country".
Systematic evaluations.
In several studies of the methodology used by Raymond D. Gastil and others to create "Freedom in the World" report, Kenneth A. Bollen found that "no criticisms ... have demonstrated a systematic bias in all the ratings. Most of the evidence consists of anecdotal evidence of relatively few cases. Whether there is a systematic or sporadic slant in Gastil's ratings is an open question". Bollen studied the question of ideological bias using multivariate statistics. Using their factor-analytic model for human-rights measurements, Bollen and Paxton estimate that Gastil's method produces a bias of 0.38 standard deviations (s.d.) against Marxist–Leninist countries and a larger bias, 0.5 s.d., favoring Christian countries. In contrast, another method by a critic of "Freedom in the World" produced a bias for Leftist countries during the 1980s of at least 0.8 s.d., a bias that is "consistent with the general finding that political scientists are more favorable to leftist politics than is the general population".
In 1990, Gastil discussed criticisms of "Freedom in the World", stating that "generally such criticism is based on opinions about Freedom House rather than detailed examination of survey ratings".
The definition of freedom in Gastil (1982) and Freedom House (1990) emphasized liberties rather than the exercise of freedom, according to Adam Przeworski, who gave the following example: In the United States, citizens are free to form political parties and to vote, yet even in presidential elections only half of U.S. "citizens" vote; in the U.S., "the same two parties speak in a commercially sponsored unison".
Another study by Mainwaring, Brink, and Perez-Linanhe found the Freedom Index of "Freedom in the World" to have a strong positive correlation (at least 80%) with three other democracy indices. Mainwaring "et al." wrote that Freedom House's index had "two systematic biases: scores for leftist were tainted by political considerations, and changes in scores are sometimes driven by changes in their criteria rather than changes in real conditions". Nonetheless, when evaluated on Latin American countries yearly, Freedom House's index were strongly and positively correlated with the index of Adam Przeworski and with the index of the authors themselves: They evaluated Pearson's coefficient of linear correlation between their index and Freedom House's index, which was 0.82; among these indices and the two others studied, the correlations were all between 0.80 and 0.86.
Freedom House maintains that its methodology is systematic and not culturally biased:
Freedom House does not maintain a culture-bound view of freedom. The methodology of the survey is grounded in basic standards of political rights and civil liberties, derived in large measure from relevant portions of the Universal Declaration of Human Rights. These standards apply to all countries and territories, irrespective of geographical location, ethnic or religious composition, or level of economic development.
Recognition.
Former US President Bill Clinton, giving a speech at a Freedom House breakfast, said: I'm honored to be here with all of you and to be here at Freedom House. For more than 50 years, Freedom House has been a voice for tolerance for human dignity. People all over the world are better off because of your work. And I'm very grateful that Freedom House has rallied this diverse and dynamic group. It's not every day that the Carnegie Endowment, the Progressive Policy Institute, the Heritage Foundation, and the American Foreign Policy Council share the same masthead.
Speaking at a reception hosted by Freedom House to honor human rights defenders, U.S. Representative Jim McGovern said:
I want to thank Freedom House for all the incredible work that they do to assist human rights defenders around the world. We rely a lot on Freedom House not only for information, advice and counsel, but also for their testimony when we do our hearings. And I’m a big fan.
Speaking at a screening of film "The Magnitsky Files", Senator John McCain said:
 Thank you for everything that Freedom House continues to do on behalf of people around the world who suffer oppression and persecution. I'm honored to have known you and to have the opportunity to work with you around the world...We rely on organizations like Freedom House to make judgments about corruption and the persecution of minorities...
Writing in the conservative "National Review" Online, John R. Miller states:
Freedom House has unwaveringly raised the standard of freedom in evaluating fascist countries, Communist regimes, and plain old, dictatorial thugocracies. Its annual rankings are read and used in the United Nations and other international organizations, as well as by the U.S. State Department. Policy and aid decisions are influenced by Freedom House’s report. Those fighting for freedom in countries lacking it are encouraged or discouraged by what Freedom House’s report covers. And sometimes — most importantly — their governments are moved to greater effort."
Miller nevertheless criticized the organization in 2007 as not paying enough attention to slavery in its reports. He wrote that repressive regimes, and even democracies such as Germany and India, needed to be held to account for their lack of enforcement of laws against human trafficking and the bondage of some foreign workers.

</doc>
<doc id="52322" url="http://en.wikipedia.org/wiki?curid=52322" title="Multichannel">
Multichannel

Multichannel may refer to: 

</doc>
<doc id="52324" url="http://en.wikipedia.org/wiki?curid=52324" title="Non-departmental public body">
Non-departmental public body

In the United Kingdom, non-departmental public body (NDPB) is a classification applied by the Cabinet Office, Treasury, the Scottish Government and the Northern Ireland Executive to quangos (quasi-autonomous non-governmental organisations). NDPBs are not an integral part of any government department and carry out their work at arm's length from ministers, although ministers are ultimately responsible to Parliament for the activities of bodies sponsored by their department.
The term includes the four types of NDPB (executive, advisory, tribunal and independent monitoring boards) but excludes public corporations and public broadcasters (BBC, Channel 4 and S4C).
Types of body.
The UK Government classifies bodies into four main types. The Scottish Government also has a fifth category- NHS bodies.
Advisory NDPBs.
These bodies consist of boards which advise ministers on particular policy areas. They are often supported by a small secretariat from the parent department and any expenditure is paid for by that department.
Executive NDPBs.
These bodies usually deliver a particular public service and are overseen by a board rather than ministers. Appointments are made by ministers following the Code of Practice of the Commissioner for Public Appointments. They employ their own staff and are allocated their own budgets.
Tribunal NDPBs.
These bodies have jurisdiction in an area of the law. They are co-ordinated by the Tribunals Service, an executive agency of the Ministry of Justice, and supervised by the Administrative Justice and Tribunals Council, itself a NDPB sponsored by the Ministry of Justice.
Independent monitoring boards.
These bodies were formerly known as "boards of visitors" and are responsible for the state of prisons, their administration and the treatment of prisoners. The Home Office is responsible for their costs, and has to note all expenses.
Contrast with executive agencies, non-ministerial departments and quangos.
NDPB differ from executive agencies as they are not created to carry out ministerial orders or policy, instead they are more or less self-determining and enjoy greater independence. They are also not directly part of government like a non-ministerial government department being at a remove from both ministers and any elected assembly or parliament. Typically an NDPB would be established under statute and be accountable to Parliament rather than to Her Majesty's Government. This arrangement allows more financial independence since the government is obliged to provide funding to meet statutory obligations.
NDPBs are commonly referred to as quangos. However, this term originally referred to bodies that are, at least ostensibly, non-government organisations, but nonetheless perform governmental functions.
History, numbers and powers.
In March 2009 there were nearly 800 public bodies that were sponsored by the UK Government. This total included 198 executive NDPBs, 410 advisory bodies, 33 tribunals, 21 public corporations, the Bank of England, 2 public broadcasting authorities and 23 NHS bodies. However, the classification is conservative and does not include bodies that are the responsibility of devolved government, various lower tier boards (including a considerable number within the NHS), and also other boards operating in the public sector (e.g. school governors and police authorities). 
These appointed bodies performed a large variety of tasks, for example health trusts, or the Welsh Development Agency, and by 1992 were responsible for some 25% of all government expenditure in the UK. According to the Cabinet Office their total expenditure for the financial year 2005–06 was £167 billion.
Criticism.
Critics argued that the system was open to abuse as most NDPBs had their members directly appointed by government ministers without an election or consultation with the people. The press, critical of what was perceived as the Conservatives' complacency in power in the 1990s, presented much material interpreted as evidence of questionable government practices. 
This concern led to the formation of a Committee on Standards in Public Life (the Nolan Committee) which first reported in 1995 and recommended the creation of a "public appointments commissioner" to make sure that appropriate standards were met in the appointment of members of QUANGOs. The Government accepted the recommendation, and the Office of the Commissioner for Public Appointments was established in November 1995.
The use of NDPBs continued under the Labour government in office from 1997 to 2010, though the political controversy associated with NDPBs in the mid-1990s for the most part died away. Before 1997, the incoming Labour Government promised to reduce the number and power of NDPBs.
In 2010 the UK's Conservative-Liberal coalition published a review of NDPBs recommending closure or merger of nearly two hundred bodies, and the transfer of others to the private sector. This process is colloquially termed the "bonfire of the quangos".
Classification in national accounts.
NDPBs are classified under code S.13112 of the European System of Accounts (ESA.95). However Statistics UK does not break out the detail for these bodies and they are consolidated into General Government (S.1311).

</doc>
<doc id="52327" url="http://en.wikipedia.org/wiki?curid=52327" title="Cyclic group">
Cyclic group

In algebra, a cyclic group is a group that is generated by a single element. That is, it consists of a set of elements with a single invertible associative operation, and it contains an element "g" such that every other element of the group may be obtained by repeatedly applying the group operation or its inverse to "g". Each element can be written as a power of "g" in multiplicative notation, or as a multiple of "g" in additive notation. This element "g" is called a "generator" of the group.
Every infinite cyclic group is isomorphic to the additive group of Z, the integers. Every finite cyclic group of order "n" is isomorphic to the additive group of Z/"n"Z, the integers modulo "n". Every cyclic group is an abelian group (meaning that its group operation is commutative), and every finitely generated abelian group is a direct product of cyclic groups.
Definition.
A group "G" is called cyclic if there exists an element "g" in "G" such that "G" = ⟨"g"⟩ = { "g""n" | "n" is an integer }. Since any group generated by an element in a group is a subgroup of that group, showing that the only subgroup of a group "G" that contains "g" is "G" itself suffices to show that "G" is cyclic.
For example, if "G" = { "g"0, "g"1, "g"2, "g"3, "g"4, "g"5 } is a group, then "g"6 = "g"0, and "G" is cyclic. In fact, "G" is essentially the same as (that is, isomorphic to) the set { 0, 1, 2, 3, 4, 5 } with addition modulo 6. For example, 1 + 2 ≡ 3 (mod 6) corresponds to "g"1 · "g"2 = "g"3, and 2 + 5 ≡ 1 (mod 6) corresponds to "g"2 · "g"5 = "g"7 = "g"1, and so on. One can use the isomorphism χ defined by χ("g""i") = "i".
The name "cyclic" may be misleading: it is possible to generate infinitely many elements and not form any literal cycles; that is, every "g""n" is distinct. (It can be thought of as having one infinitely long cycle.) A group generated in this way (for example, the first frieze group, p1) is called an infinite cyclic group, and is isomorphic to the additive group of the integers, (Z, +).
Examples.
Integer and modular addition.
The set of integers, with the operation of addition, forms a group. It is an infinite cyclic group, because all integers can be written as a finite sum or difference of copies of the number 1. In this group, 1 and −1 are the only generators. Every infinite cyclic group is isomorphic to this group.
For every positive integer "n", the set of integers modulo "n", again with the operation of addition, forms a finite cyclic group, the group Z/"n".
An element "g" is a generator of this group if "g" is relatively prime to "n".
Thus, the number of different generators is φ("n"), where φ is the Euler totient function, the function that counts the number of numbers modulo "n" that are relatively prime to "n".
Every finite cyclic group is isomorphic to a group Z/"n", where "n" is the order of the group.
The integer and modular addition operations, used to define the cyclic groups, are both the addition operations of commutative rings, also denoted Z and Z/"n". If "p" is a prime, then Z/"p" is a finite field, and is usually instead written as F"p" or GF("p"). Every field with "p" elements is isomorphic to this one.
Modular multiplication.
For every positive integer "n", the subset of the integers modulo "n" that are relatively prime to "n", with the operation of multiplication, forms a finite group that for many values of "n" is again cyclic.
It is the group under multiplication modulo "n", and it is cyclic whenever "n" is 1, 2, 4, a power of an odd prime, or twice a power of an odd prime.
Its elements are the units of the ring Z/"nZ; there are φ("n") of them, where again φ is the totient function. This group is written as (Z/"nZ)×. For example, (Z/6Z)× has as its elements {1,5}; 6 is twice a prime, so this is a cyclic group. In contrast, (Z/8Z)× (with elements {1,3,5,7}) is the Klein group and is not cyclic. When (Z/"nZ)× is cyclic, every generator of (Z/"nZ)× is called a primitive root modulo "n".
The cyclic group (Z/"pZ)× for a prime number "p", is also written (Z/"pZ)* because it consists of the non-zero elements of the finite field of order "p". More generally, every finite subgroup of the multiplicative group of any field is cyclic.
Rotational symmetries.
The set of rotational symmetries of a polygon forms a finite cyclic group. If there are "n" different ways of mapping the polygon to itself by a rotation (including the null rotation) then this group is isomorphic to Zn. In three or higher dimensions there can exist other finite symmetry groups that are cyclic, but that do not form the set of rotations around a single axis.
The group "S"1 of all rotations of a circle (the circle group) is "not" cyclic. Unlike the infinite cyclic group, it is not even countable. There also exist other infinite rotation groups (such as the set of rotations by rational angles) that are countable but not cyclic.
Galois theory.
An "n"th root of unity may be thought of as a complex number whose "n"th power is 1. That is, it is a root of the polynomial "x""n" − 1.
The "n"th roots of unity form a cyclic group of order "n" under multiplication. For example, the polynomial 0 = "z"3 − 1 factors as ("z" − "s"0)("z" − "s"1)("z" − "s"2), where "s" = "e"2π"i"/3; the set {"s"0, "s"1, "s"2} forms a cyclic group under multiplication. The Galois group of the field extension of the rational numbers generated by the "n"th roots of unity forms a different group. It is isomorphic to the multiplicative group modulo "n", which has order φ("n") and is cyclic for some but not all "n".
A field extension is called a cyclic extension if its Galois group is a cyclic group. The Galois group of every finite extension of a finite field is finite and cyclic, with an iterate of the Frobenius endomorphism as its generator. Conversely, given a finite field "F" and a finite cyclic group "G", there is a finite field extension of "F" whose Galois group is "G".
Subgroups and notation.
All subgroups and quotient groups of cyclic groups are cyclic. Specifically, all subgroups of Z are of the form "m"Z, with "m" an integer ≥0. All of these subgroups are distinct from each other, and apart from the trivial group (for "m" = 0) all are isomorphic to Z. The lattice of subgroups of Z is isomorphic to the dual of the lattice of natural numbers ordered by divisibility. In particular, because the prime numbers are the numbers with no nontrivial divisors, a cyclic group is simple if and only if its order (the number of its elements) is prime.
Since the cyclic groups are abelian, they are often written additively and denoted Z"n" with the identity written 0. However, this notation can be problematic for number theorists because it conflicts with the usual notation for "p"-adic number rings or localization at a prime ideal. The quotient notations Z/"n"Z, Z/"n", and Z/("n") are standard alternatives.
One may instead write the group multiplicatively, and denote it by C"n", where "n" is the order for finite groups and by C for the infinite cyclic group. For example, "g"2"g"4 = "g"1 in C5, whereas 2 + 4 = 1 in Z/5Z.
All quotient groups of Z are finite, except for the trivial exception Z/{0} = Z/0Z. For every positive divisor "d" of "n", the quotient group Z/"n"Z has precisely one subgroup of order "d", the one generated by the residue class of "n"/"d". There are no other subgroups.
Using the quotient group formalism, Z/"nZ is a standard notation for the additive cyclic group with "n" elements. In ring terminology, the subgroup "nZ is also the ideal ("n"), so the quotient can also be written Z/("n") or Z/"n" without abuse of notation. These alternatives do not conflict with the notation for the "p"-adic integers. The last form is very common in informal calculations; it has the additional advantage that it reads the same way that the group or ring is often described verbally in English, "Zee mod en".
Additional properties.
Every cyclic group is abelian. That is, its group operation is commutative: "gh" = "hg" (for all "g" and "h" in "G"). This is clear for the groups of integer and modular addition since "r" + "s" ≡ "s" + "r" (mod "n"), and it follows for all cyclic groups since they are all isomorphic to a group generated by an addition operation.
For a finite cyclic group of order "n", and every element "e" of the group, "e""n" is the identity element of the group. This again follows by using the isomorphism to modular addition, since "kn" ≡ 0 (mod "n") for every integer "k".
If "d" is a divisor of "n", then the number of elements in Z/"n" which have order "d" is φ("d"), and the number of elements whose order divides "d" is exactly "d".
If "G" is a finite group in which, for each "n" > 0, "G" contains at most "n" elements of order dividing "n", then "G" must be cyclic.
The order of an element "m" of the group is "n"/gcd("n","m").
The direct product of two cyclic groups Z/"n" and Z/"m" is cyclic if and only if "n" and "m" are coprime. Thus e.g. Z/12 is the direct product of Z/3 and Z/4, but not the direct product of Z/6 and Z/2.
If "p" is a prime number, then the only group (up to isomorphism) with "p" elements is Z/"p".
it is called a primary cyclic group. The fundamental theorem of abelian groups states that every finitely generated abelian group is the direct product of finitely many finite primary cyclic and infinite cyclic groups.
A number "n" is called a cyclic number if it has the property that Z/"n" is the only group of order "n", which is true exactly when gcd("n",φ("n")) = 1. The cyclic numbers include all prime numbers, but also include some composite numbers such as 15.
The definition immediately implies that cyclic groups have group presentation C∞ = ⟨"x" |⟩ and C"n" = ⟨"x" | "x""n"⟩ for finite "n".
Associated objects.
Representations.
The representation theory of the cyclic group is a critical base case for the representation theory of more general finite groups. In the complex case, a representation of a cyclic group decomposes into a direct sum of linear characters, making the connection between character theory and representation theory transparent. In the positive characteristic case, the indecomposable representations of the cyclic group form a model and inductive basis for the representation theory of groups with cyclic Sylow subgroups and more generally the representation theory of blocks of cyclic defect.
Cycle graph.
A cycle graph illustrates the various cycles of a group and is particularly useful in visualizing the structure of small finite groups. A cycle graph for a cyclic group is simply a circular graph, where the group order is equal to the number of nodes. A single generator defines the group as a directional path on the graph, and the inverse generator defines a backwards path. Trivial paths (identity) can be drawn as a loop but are usually suppressed. Z2 is sometimes drawn with two curved edges as a multigraph.
Cyclic groups Z"n", order "n", is a single cycle graphed simply as an "n"-sided polygon with the elements at the vertices. A cyclic group Z"n" can be decomposed into a direct product Z"a"×Z"b" where "n"="ab", where "a" and "b" are relatively prime (gcd(a,b)=1).
Cayley graph.
A Cayley graph is a graph defined from a pair ("G","S") where "G" is a group and "S" is a set of generators for the group; it has a vertex for each group element, and an edge for each product of an element with a generator. In the case of a finite cyclic group, with its single generator, the Cayley graph is a cycle graph, and for an infinite cyclic group with its generator the Cayley graph is a doubly-infinite path graph. However, Cayley graphs can be defined from other sets of generators as well. The Cayley graphs of cyclic groups with arbitrary generator sets are called circulant graphs. These graphs may be represented geometrically as a set of equally-spaced points on a circle or on a line, with each point connected to neighbors with the same set of distances as each other point. They are exactly the vertex-transitive graphs whose symmetry group includes a transitive cyclic group.
Endomorphisms.
The endomorphism ring of the abelian group Z/"nZ is isomorphic to Z/"nZ itself as a ring. Under this isomorphism, the number "r" corresponds to the endomorphism of Z/"nZ that maps each element to the sum of "r" copies of it. This is a bijection if and only if "r" is coprime with "n", so the automorphism group of Z/"nZ is isomorphic to the unit group (Z/"n"Z)×.
Similarly, the endomorphism ring of the additive group of Z is isomorphic to the ring Z. Its automorphism group is isomorphic to the group of units of the ring Z, i.e. to ({−1, +1}, ×) ≅ C2.
Related classes of groups.
Several other classes of groups have been defined by their relation to the cyclic groups:
Virtually cyclic groups.
A group is called virtually cyclic if it contains a cyclic subgroup of finite index (the number of cosets that the subgroup has). In other words, any element in a virtually cyclic group can be arrived at by applying a member of the cyclic subgroup to a member in a certain finite set. Every cyclic group is virtually cyclic, as is every finite group. An infinite group is virtually cyclic if and only if it is finitely generated and has exactly two ends; an example of such a group is the product of Z/"n" and Z, in which the Z factor has finite index "n". Every abelian subgroup of a Gromov hyperbolic group is virtually cyclic.
Locally cyclic groups.
A locally cyclic group is a group in which each finitely generated subgroup is cyclic.
An example is the additive group of the rational numbers: every finite set of rational numbers is a set of integer multiples of a single unit fraction, the inverse of their lowest common denominator, and generates as a subgroup a cyclic group of integer multiples of this unit fraction.
A group is locally cyclic if and only if its lattice of subgroups is a distributive lattice.
Cyclically ordered groups.
A cyclically ordered group is a group together with a cyclic order preserved by the group structure.
Every cyclic group can be given a structure as a cyclically ordered group, consistent with the ordering of the integers (or the integers modulo the order of the group).
Every finite subgroup of a cyclically ordered group is cyclic.
Metacyclic and polycyclic groups.
A metacyclic group is a group containing a cyclic normal subgroup whose quotient is also cyclic.
These groups include the cyclic groups, the dicyclic groups, and the direct products of two cyclic groups.
The polycyclic groups generalize metacyclic groups by allowing more than one level of group extension. A group is polycyclic if it has a finite descending sequence of subgroups, each of which is normal in the previous subgroup with a cyclic quotient, ending in the trivial group. Every finitely generated abelian group or nilpotent group is polycyclic.

</doc>
<doc id="52328" url="http://en.wikipedia.org/wiki?curid=52328" title="Stock market">
Stock market

A stock market or equity market is the aggregation of buyers and sellers (a loose network of economic transactions, not a physical facility or discrete entity) of stocks (also called shares); these may include securities listed on a stock exchange as well as those only traded privately.
Size of the market.
Stocks can be categorized in various ways. One common way is, by the country where the company is domiciled. For example, Nestlé and Novartis are domiciled in Switzerland, so they may be considered as part of the Swiss stock market, although their stock may also be traded at exchanges in other countries.
At the close of 2012, the size of the world stock market (total market capitalisation) was about US$55 trillion. By country, the largest market was the United States (about 34%), followed by Japan (about 6%) and the United Kingdom (about 6%). This went up more in 2013.
Stock exchange.
A stock exchange is a place or organization by which stock traders (people and companies) can trade stocks. Companies may want to get their stock listed on a stock exchange. Other stocks may be traded "over the counter", that is, through a dealer. A large company will usually have its stock listed on many exchanges across the world.
Exchanges may also cover other types of security such as fixed interest securities or indeed derivatives.
Trade.
Trade in stock markets means the transfer for money of a stock or security from a seller to a buyer. This requires these two parties to agree on a price. Equities (Stocks or shares) confer an ownership interest in a particular company.
Participants in the stock market range from small individual stock investors to larger traders investors, who can be based anywhere in the world, and may include banks, insurance companies or pension funds, and hedge funds. Their buy or sell orders may be executed on their behalf by a stock exchange trader.
Some exchanges are physical locations where transactions are carried out on a trading floor, by a method known as open outcry. This method is used in some stock exchanges and commodity exchanges, and involves traders entering oral bids and offers simultaneously. An example of such an exchange is the New York Stock Exchange. The other type of stock exchange is a virtual kind, composed of a network of computers where trades are made electronically by traders. An example of such an exchange is the NASDAQ.
A potential buyer "bids" a specific price for a stock, and a potential seller "asks" a specific price for the same stock. Buying or selling "at market" means you will accept "any" ask price or bid price for the stock, respectively. When the bid and ask prices match, a sale takes place, on a first-come-first-served basis if there are multiple bidders or askers at a given price.
The purpose of a stock exchange is to facilitate the exchange of securities between buyers and sellers, thus providing a marketplace (virtual or real). The exchanges provide real-time trading information on the listed securities, facilitating price discovery.
The New York Stock Exchange (NYSE) is a physical exchange, with a hybrid market for placing orders both electronically and manually on the trading floor. Orders executed on the trading floor enter by way of exchange members and flow down to a floor broker, who goes to the floor trading post specialist for that stock to trade the order. The specialist's job is to match buy and sell orders using open outcry. If a spread exists, no trade immediately takes place—in this case the specialist should use his/her own resources (money or stock) to close the difference after his/her judged time. Once a trade has been made the details are reported on the "tape" and sent back to the brokerage firm, which then notifies the investor who placed the order. Although there is a significant amount of human contact in this process, computers play an important role, especially for so-called "program trading".
The NASDAQ is a virtual listed exchange, where all of the trading is done over a computer network. The process is similar to the New York Stock Exchange. However, buyers and sellers are electronically matched. One or more NASDAQ market makers will always provide a bid and ask price at which they will always purchase or sell 'their' stock.
The Paris Bourse, now part of Euronext, is an order-driven, electronic stock exchange. It was automated in the late 1980s. Prior to the 1980s, it consisted of an open outcry exchange. Stockbrokers met on the trading floor or the Palais Brongniart. In 1986, the CATS trading system was introduced, and the order matching process was fully automated.
People trading stock will prefer to trade on the most popular exchange since this gives the largest number of potential counterparties (buyers for a seller, sellers for a buyer). and probably the best price. However, there have always been alternatives such as brokers trying to bring parties together to trade outside the exchange. Some third markets that were popular are Instinet, and later Island and Archipelago. One advantage is that this avoids the commissions of the exchange. However, it also has problems such as adverse selection. Financial regulators are probing dark pools.
Market participant.
Market participants include individual retail investors, institutional investors such as mutual funds, banks, insurance companies and hedge funds, and also publicly traded corporations trading in their own shares. Some studies have suggested that institutional investors and corporations trading in their own shares generally receive higher risk-adjusted returns than retail investors.
A few decades ago, worldwide, buyers and sellers were individual investors, such as wealthy businessmen, usually with long family histories to particular corporations. Over time, markets have become more "institutionalized"; buyers and sellers are largely institutions (e.g., pension funds, insurance companies, mutual funds, index funds, exchange-traded funds, hedge funds, investor groups, banks and various other financial institutions).
The rise of the institutional investor has brought with it some improvements in market operations. There has been a gradual tendency for "fixed" (and exorbitant) fees being reduced for all investors, partly from falling administration costs but also assisted by large institutions challenging brokers' oligopolistic approach to setting standardised fees.
History.
In 12th century France the "courretiers de change" were concerned with managing and regulating the debts of agricultural communities on behalf of the banks. Because these men also traded with debts, they could be called the first brokers. A common misbelief is that in late 13th century Bruges commodity traders gathered inside the house of a man called "Van der Beurze", and in 1409 they became the "Brugse Beurse", institutionalizing what had been, until then, an informal meeting, but actually, the family Van der Beurze had a building in Antwerp where those gatherings occurred; the Van der Beurze had Antwerp, as most of the merchants of that period, as their primary place for trading. The idea quickly spread around Flanders and neighboring countries and "Beurzen" soon opened in Ghent and Rotterdam.
In the middle of the 13th century, Venetian bankers began to trade in government securities. In 1351 the Venetian government outlawed spreading rumors intended to lower the price of government funds. Bankers in Pisa, Verona, Genoa and Florence also began trading in government securities during the 14th century. This was only possible because these were independent city states not ruled by a duke but a council of influential citizens. Italian companies were also the first to issue shares. Companies in England and the Low Countries followed in the 16th century.
The Dutch East India Company (founded in 1602) was the first joint-stock company to get a fixed capital stock and as a result, continuous trade in company stock occurred on the Amsterdam Exchange. Soon thereafter, a lively trade in various derivatives, among which options and repos, emerged on the Amsterdam market. Dutch traders also pioneered short selling – a practice which was banned by the Dutch authorities as early as 1610.
There are now stock markets in virtually every developed and most developing economies, with the world's largest markets being in the United States, United Kingdom, Japan, India, Pakistan, China, Canada, Germany (Frankfurt Stock Exchange), France, South Korea and the Netherlands.
Importance of stock market.
Function and purpose.
The stock market is one of the most important ways for companies to raise money, along with debt markets which are generally more imposing but do not trade publicly. This allows businesses to be publicly traded, and raise additional financial capital for expansion by selling shares of ownership of the company in a public market. The liquidity that an exchange affords the investors enables their holders to quickly and easily sell securities. This is an attractive feature of investing in stocks, compared to other less liquid investments such as property and other immoveable assets. Some companies actively increase liquidity by trading in their own shares.
History has shown that the price of stocks and other assets is an important part of the dynamics of economic activity, and can influence or be an indicator of social mood. An economy where the stock market is on the rise is considered to be an up-and-coming economy. In fact, the stock market is often considered the primary indicator of a country's economic strength and development.
Rising share prices, for instance, tend to be associated with increased business investment and vice versa. Share prices also affect the wealth of households and their consumption. Therefore, central banks tend to keep an eye on the control and behavior of the stock market and, in general, on the smooth operation of financial system functions. Financial stability is the raison d'être of central banks.
Exchanges also act as the clearinghouse for each transaction, meaning that they collect and deliver the shares, and guarantee payment to the seller of a security. This eliminates the risk to an individual buyer or seller that the counterparty could default on the transaction.
The smooth functioning of all these activities facilitates economic growth in that lower costs and enterprise risks promote the production of goods and services as well as possibly employment. In this way the financial system is assumed to contribute to increased prosperity, although some controversy exists as to whether the optimal financial system is bank-based or market-based.
Recent events such as the Global Financial Crisis have prompted a heightened degree of scrutiny of the impact of the structure of stock markets (called market microstructure), in particular to the stability of the financial system and the transmission of systemic risk.
Relation of the stock market to the modern financial system.
The financial system in most western countries has undergone a remarkable transformation. One feature of this development is disintermediation. A portion of the funds involved in saving and financing, flows directly to the financial markets instead of being routed via the traditional bank lending and deposit operations. The general public interest in investing in the stock market, either directly or through mutual funds, has been an important component of this process.
Statistics show that in recent decades, shares have made up an increasingly large proportion of households' financial assets in many countries. In the 1970s, in Sweden, deposit accounts and other very liquid assets with little risk made up almost 60 percent of households' financial wealth, compared to less than 20 percent in the 2000s. The major part of this adjustment is that financial portfolios have gone directly to shares but a good deal now takes the form of various kinds of institutional investment for groups of individuals, e.g., pension funds, mutual funds, hedge funds, insurance investment of premiums, etc.
The trend towards forms of saving with a higher risk has been accentuated by new rules for most funds and insurance, permitting a higher proportion of shares to bonds. Similar tendencies are to be found in other developed countries. In all developed economic systems, such as the European Union, the United States, Japan and other developed nations, the trend has been the same: saving has moved away from traditional (government insured) "bank deposits to more risky securities of one sort or another".
A second transformation is the move to electronic trading to replace human trading of listed securities.
United States S&P stock market returns.
Compared to Other Asset Classes Over the long term, investing in a well diversified portfolio of stocks such as an S&P 500 Index outperforms other investment vehicles such as Treasury Bills and Bonds, with the S&P 500 having a geometric annual average of 9.55% from 1928-2013.
Behavior of the stock market.
From experience it is known that investors may 'temporarily' move financial prices away from their long term aggregate price 'trends'. Over-reactions may occur—so that excessive optimism (euphoria) may drive prices unduly high or excessive pessimism may drive prices unduly low. Economists continue to debate whether financial markets are 'generally' efficient.
According to one interpretation of the efficient-market hypothesis (EMH), only changes in fundamental factors, such as the outlook for margins, profits or dividends, ought to affect share prices beyond the short term, where random 'noise' in the system may prevail. (But this largely theoretic academic viewpoint—known as 'hard' EMH—also predicts that little or no trading should take place, contrary to fact, since prices are already at or near equilibrium, "having priced in all public knowledge.") The 'hard' efficient-market hypothesis is sorely tested and does not explain the cause of events such as the crash in 1987, when the Dow Jones Industrial Average plummeted 22.6 percent—the largest-ever one-day fall in the United States.
This event demonstrated that share prices can fall dramatically even though, to this day, it is impossible to fix a generally agreed upon definite cause: a thorough search failed to detect "any" 'reasonable' development that might have accounted for the crash. (But note that such events are predicted to occur strictly by chance, although very rarely.) It seems also to be the case more generally that many price movements (beyond that which are predicted to occur 'randomly') are "not" occasioned by new information; a study of the fifty largest one-day share price movements in the United States in the post-war period seems to confirm this.
A 'soft' EMH has emerged which does not require that prices remain at or near equilibrium, but only that market participants not be able to "systematically" profit from any momentary market 'inefficiencies'. Moreover, while EMH predicts that all price movement (in the absence of change in fundamental information) is random (i.e., non-trending), many studies have shown a marked tendency for the stock market to trend over time periods of weeks or longer. Various explanations for such large and apparently non-random price movements have been promulgated. For instance, some research has shown that changes in estimated risk, and the use of certain strategies, such as stop-loss limits and value at risk limits, "theoretically could" cause financial markets to overreact. But the best explanation seems to be that the distribution of stock market prices is (in which case EMH, in any of its current forms, would not be strictly applicable).
Other research has shown that psychological factors may result in "exaggerated" (statistically anomalous) stock price movements (contrary to EMH which assumes such behaviors 'cancel out'). Psychological research has demonstrated that people are predisposed to 'seeing' patterns, and often will perceive a pattern in what is, in fact, just "noise". (Something like seeing familiar shapes in "clouds" or "ink blots".) In the present context this means that a succession of good news items about a company may lead investors to overreact positively (unjustifiably driving the price up). A period of good returns also boosts the investors' self-confidence, reducing their (psychological) risk threshold.
Another phenomenon—also from psychology—that works against an objective assessment is "group thinking". As social animals, it is not easy to stick to an opinion that differs markedly from that of a majority of the group. An example with which one may be familiar is the reluctance to enter a restaurant that is empty; people generally prefer to have their opinion validated by those of others in the group.
In one paper the authors draw an analogy with gambling. In normal times the market behaves like a game of roulette; the probabilities are known and largely independent of the investment decisions of the different players. In times of market stress, however, the game becomes more like poker (herding behavior takes over). The players now must give heavy weight to the psychology of other investors and how they are likely to react psychologically.
The stock market, as with any other business, is quite unforgiving of amateurs. Inexperienced investors rarely get the assistance and support they need. In the period running up to the 1987 crash, less than 1 percent of the analyst's recommendations had been to sell (and even during the 2000–2002 bear market, the average did not rise above 5%). In the run up to 2000, the media amplified the general euphoria, with reports of rapidly rising share prices and the notion that large sums of money could be quickly earned in the so-called new economy stock market. (And later amplified the gloom which descended during the 2000–2002 bear market, so that by summer of 2002, predictions of a DOW average below 5000 were quite common)
On the other hand, Stock markets play an essential role in growing industries that ultimately affect the economy through transferring available funds from units that have excess funds (savings) to those who are suffering from funds deficit (borrowings) (Padhi and Naik, 2012). In other words, capital markets facilitate funds movement between the above-mentioned units. This process leads to the enhancement of available financial resources which in turn affects the economic growth positively. Moreover, both of economic and financial theories argue that stocks‘ prices are affected by the performance of main macroeconomic variables, see .
Irrational behavior.
Sometimes, the market seems to react irrationally to economic or financial news, even if that news is likely to have no real effect on the fundamental value of securities itself. But, this may be more apparent than real, since often such news has been anticipated, and a counterreaction may occur if the news is better (or worse) than expected. Therefore, the stock market may be swayed in either direction by press releases, rumors, euphoria and mass panic.
Over the short-term, stocks and other securities can be battered or buoyed by any number of fast market-changing events, making the stock market behavior difficult to predict. Emotions can drive prices up and down, people are generally not as rational as they think, and the reasons for buying and selling are generally obscure. Behaviorists argue that investors often behave 'irrationally' when making investment decisions thereby incorrectly pricing securities, which causes market inefficiencies, which, in turn, are opportunities to make money. However, the whole notion of EMH is that these non-rational reactions to information cancel out, leaving the prices of stocks rationally determined.
The Dow Jones Industrial Average biggest gain in one day was 936.42 points or 11 percent, this occurred on October 13, 2008.
Crashes.
A stock market crash is often defined as a sharp dip in share prices of stocks listed on the stock exchanges. In parallel with various economic factors, a reason for stock market crashes is also due to panic and investing public's loss of confidence. Often, stock market crashes end speculative economic bubbles.
There have been famous stock market crashes that have ended in the loss of billions of dollars and wealth destruction on a massive scale. An increasing number of people are involved in the stock market, especially since the social security and retirement plans are being increasingly privatized and linked to stocks and bonds and other elements of the market. There have been a number of famous stock market crashes like the Wall Street Crash of 1929, the stock market crash of 1973–4, the Black Monday of 1987, the Dot-com bubble of 2000, and the Stock Market Crash of 2008.
One of the most famous stock market crashes started October 24, 1929 on Black Thursday. The Dow Jones Industrial Average lost 50% during this stock market crash. It was the beginning of the Great Depression. Another famous crash took place on October 19, 1987 – Black Monday. The crash began in Hong Kong and quickly spread around the world.
By the end of October, stock markets in Hong Kong had fallen 45.5%, Australia 41.8%, Spain 31%, the United Kingdom 26.4%, the United States 22.68%, and Canada 22.5%. Black Monday itself was the largest one-day percentage decline in stock market history – the Dow Jones fell by 22.6% in a day. The names "Black Monday" and "Black Tuesday" are also used for October 28–29, 1929, which followed Terrible Thursday—the starting day of the stock market crash in 1929.
The crash in 1987 raised some puzzles – main news and events did not predict the catastrophe and visible reasons for the collapse were not identified. This event raised questions about many important assumptions of modern economics, namely, the theory of rational human conduct, the theory of market equilibrium and the efficient-market hypothesis. For some time after the crash, trading in stock exchanges worldwide was halted, since the exchange computers did not perform well owing to enormous quantity of trades being received at one time. This halt in trading allowed the Federal Reserve System and central banks of other countries to take measures to control the spreading of worldwide financial crisis. In the United States the SEC introduced several new measures of control into the stock market in an attempt to prevent a re-occurrence of the events of Black Monday.
Since the early 1990s, many of the largest exchanges have adopted electronic 'matching engines' to bring together buyers and sellers, replacing the open outcry system. Electronic trading now accounts for the majority of trading in many developed countries. Computer systems were upgraded in the stock exchanges to handle larger trading volumes in a more accurate and controlled manner. The SEC modified the margin requirements in an attempt to lower the volatility of common stocks, stock options and the futures market. The New York Stock Exchange and the Chicago Mercantile Exchange introduced the concept of a circuit breaker. The circuit breaker halts trading if the Dow declines a prescribed number of points for a prescribed amount of time. In February 2012, the Investment Industry Regulatory Organization of Canada (IIROC) introduced single-stock circuit breakers.
Stock market prediction.
Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in "Scientific Reports", suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.
Stock market index.
The movements of the prices in a market or section of a market are captured in price indices called stock market indices, of which there are many, e.g., the S&P, the FTSE and the Euronext indices. Such indices are usually market capitalization weighted, with the weights reflecting the contribution of the stock to the index. The constituents of the index are reviewed frequently to include/exclude stocks in order to reflect the changing business environment.
Derivative instruments.
Financial innovation has brought many new financial instruments whose pay-offs or values depend on the prices of stocks. Some examples are exchange-traded funds (ETFs), stock index and stock options, equity swaps, single-stock futures, and stock index futures. These last two may be traded on futures exchanges (which are distinct from stock exchanges—their history traces back to commodity futures exchanges), or traded over-the-counter. As all of these products are only "derived" from stocks, they are sometimes considered to be traded in a (hypothetical) derivatives market, rather than the (hypothetical) stock market.
Leveraged strategies.
Stock that a trader does not actually own may be traded using short selling; margin buying may be used to purchase stock with borrowed funds; or, "derivatives" may be used to control large blocks of stocks for a much smaller amount of money than would be required by outright purchase or sales.
Short selling.
In short selling, the trader borrows stock (usually from his brokerage which holds its clients' shares or its own shares on account to lend to short sellers) then sells it on the market, betting that the price will fall. The trader eventually buys back the stock, making money if the price fell in the meantime and losing money if it rose. Exiting a short position by buying back the stock is called "covering." This strategy may also be used by unscrupulous traders in illiquid or thinly traded markets to artificially lower the price of a stock. Hence most markets either prevent short selling or place restrictions on when and how a short sale can occur. The practice of naked shorting is illegal in most (but not all) stock markets.
Margin buying.
In margin buying, the trader borrows money (at interest) to buy a stock and hopes for it to rise. Most industrialized countries have regulations that require that if the borrowing is based on collateral from other stocks the trader owns outright, it can be a maximum of a certain percentage of those other stocks' value. In the United States, the margin requirements have been 50% for many years (that is, if you want to make a $1000 investment, you need to put up $500, and there is often a maintenance margin below the $500).
A margin call is made if the total value of the investor's account cannot support the loss of the trade. (Upon a decline in the value of the margined securities additional funds may be required to maintain the account's equity, and with or without notice the margined security or any others within the account may be sold by the brokerage to protect its loan position. The investor is responsible for any shortfall following such forced sales.)
Regulation of margin requirements (by the Federal Reserve) was implemented after the Crash of 1929. Before that, speculators typically only needed to put up as little as 10 percent (or even less) of the total investment represented by the stocks purchased. Other rules may include the prohibition of "free-riding:" putting in an order to buy stocks without paying initially (there is normally a three-day grace period for delivery of the stock), but then selling them (before the three-days are up) and using part of the proceeds to make the original payment (assuming that the value of the stocks has not declined in the interim).
New issuance.
Global issuance of equity and equity-related instruments totaled $505 billion in 2004, a 29.8% increase over the $389 billion raised in 2003. Initial public offerings (IPOs) by US issuers increased 221% with 233 offerings that raised $45 billion, and IPOs in Europe, Middle East and Africa (EMEA) increased by 333%, from $9 billion to $39 billion.
ASX Share Market Game.
ASX Share Market Game is a platform for Australian school students and beginners to learn about trading stocks. The game is a free service hosted on ASX (Australian Securities Exchange) website. Each year more than 70,000 students enroll in the game. For vast majority, this is an introduction to Stock Market investing. Students once enrolled, are given $50,000 virtual money and can buy and sell up to 20 times a day. The game runs for 10 weeks. This ridiculously short time frame turns the game into a lottery, encouraging people to take huge risks with their virtual $50,000, breaking the laws of commonsense investing in the process.
Investment strategies.
One of the many things people always want to know about the stock market is, "How do I make money investing?" There are many different approaches; two basic methods are classified by either fundamental analysis or technical analysis. Fundamental analysis refers to analyzing companies by their financial statements found in SEC filings, business trends, general economic conditions, etc. Technical analysis studies price actions in markets through the use of charts and quantitative techniques to attempt to forecast price trends regardless of the company's financial prospects. One example of a technical strategy is the Trend following method, used by John W. Henry and Ed Seykota, which uses price patterns, utilizes strict money management and is also rooted in risk control and diversification.
Additionally, many choose to invest via the index method. In this method, one holds a weighted or unweighted portfolio consisting of the entire stock market or some segment of the stock market (such as the S&P 500 or Wilshire 5000). The principal aim of this strategy is to maximize diversification, minimize taxes from too frequent trading, and ride the general trend of the stock market (which, in the U.S., has averaged nearly 10% per year, compounded annually, since World War II).
Taxation.
According to much national or state legislation, a large array of fiscal obligations are taxed for capital gains. Taxes are charged by the state over the transactions, dividends and capital gains on the stock market, in particular in the stock exchanges. However, these fiscal obligations may vary from jurisdictions to jurisdictions because, among other reasons, it could be assumed that taxation is already incorporated into the stock price through the different taxes companies pay to the state, or that tax free stock market operations are useful to boost economic growth.
Further reading.
</dl>

</doc>
<doc id="52329" url="http://en.wikipedia.org/wiki?curid=52329" title="Allegany County, New York">
Allegany County, New York

Allegany County is a county located in the U.S. state of New York. As of the 2010 census, the population was 48,946. Its county seat is Belmont. Its name derives from a Delaware Indian word, applied by settlers of Western New York State to a trail that followed the Allegheny River.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1034 sqmi, of which 1029 sqmi is land and 5.1 sqmi (0.5%) is water.
Allegany County is in the southwestern part of New York State, along the Pennsylvania border. Allegany County does not lie along the Allegheny River, as its name would suggest. The highest point in the county is Alma Hill with an elevation of 2,548' above sea level. This is the highest point west of the Catskill Mountains in New York State. The highest point of Interstate 86 is located in the Town of West Almond with an elevation of 2,110'. This is also believed to be the highest point of any Interstate in the New York.
The Genesee River bisects the county from south to north. In June 1972 the remnants of Hurricane Agnes stalled over the area, dropping more than 20 in of rain. There was memorable flooding in Wellsville, Belmont, Belfast and other valley communities of the county.
The Genesee River is extremely popular with canoeists (as it was a favored route for Native Americans) and the river abounds in smallmouth bass, trout and panfish.
Demographics.
As of the census of 2000, there were 49,927 people, 18,009 households, and 12,192 families residing in the county. The population density was 48 people per square mile (19/km²). There were 24,505 housing units at an average density of 24 per square mile (9/km²). The racial makeup of the county was 97.03% White, 0.72% Black or African American, 0.28% Native American, 0.72% Asian, 0.37% from other races, and 0.88% from two or more races. 0.91% of the population were Hispanic or Latino of any race. 22.3% were of German, 16.6% English, 13.8% Irish, 11.9% American and 7.0% Italian ancestry according to Census 2000. 96.5% spoke English and 1.3% Spanish as their first language.
There were 18,009 households out of which 31.50% had children under the age of 18 living with them, 54.20% were married couples living together, 9.00% had a female householder with no husband present, and 32.30% were non-families. 26.00% of all households were made up of individuals and 11.30% had someone living alone who was 65 years of age or older. The average household size was 2.53 and the average family size was 3.04.
In the county the population was spread out with 24.40% under the age of 18, 15.50% from 18 to 24, 23.90% from 25 to 44, 22.20% from 45 to 64, and 14.00% who were 65 years of age or older. The median age was 35 years. For every 100 females there were 99.80 males. For every 100 females age 18 and over, there were 98.10 males.
The median income for a household in the county was $32,106, and the median income for a family was $38,580. Males had a median income of $30,401 versus $21,466 for females. The per capita income for the county was $14,975. About 10.50% of families and 15.50% of the population were below the poverty line, including 19.20% of those under age 18 and 7.50% of those age 65 or over.
Government and politics.
Allegany County is considered a 'red', that is, a conservative county. In 2004, it voted for George Bush over John Kerry by a margin of 63 to 34 and in 2008 it voted for John McCain over Barack Obama by a margin of 59 to 39. It has been reported that in the last 170 years the only Democratic candidates to win were Franklin Pierce in 1852 and Lyndon B. Johnson in 1964. In 2006, neither Eliot Spitzer or Hillary Clinton carried it in their landslide elections. Eliot Spitzer lost 48.98% to John Faso's 49.03%. Hillary Clinton lost the county by 3 points. In 2010, Andrew Cuomo lost by a wide margin while Senator Chuck Schumer carried it by a narrow margin of 49.46% to Jay Townsend's 48.86% a margin of 78 votes. It was one of only two counties Senator Kirsten Gillibrand lost to Wendy Long in 2012.
Allegany is part of New York's 29th congressional district, which has a Cook Partisan Voting Index of R+5. In the New York State Senate it is part of the 57th district and is represented by Republican Catharine Young. In the New York State Assembly the County is in the 148th Assembly District represented by Republican Joseph Giglio.
The Allegany Board of Legislators consists of 15 members from five three-member districts. s of 2010[ [update]], the Board consists of 14 Republicans and one Independent. The current chairman is Curtis W. Crandall. The County Administrator is Mitchell M. Alger.
Allegany County is divided into 29 towns and 10 villages. There are no cities as designated by New York State Law. The Village of Almond has the distinction of residing in Two Countys: Allegany and Steuben. The towns and villages by County Legislative District are as follows:
District I: Angelica, Belfast, Caneadea, Centerville, Granger, Hume, Rushford and the Village of Angelica.
District II: Amity, Clarksville, Cuba, Friendship, New Hudson, Ward and the Villages of Belmont & Cuba.
District III: Alma, Bolivar, Genesee, Independence, Scio, Willing, Wirt and the Villages of Bolivar & Richburg.
District IV: Andover, Wellsville and the Villages of Andover and Wellsville.
District V: Alfred, Allen, Almond, Birdsall, Burns, Grove, West Almond and the Villages of Alfred, Almond & Canaseraga.
The Oil Springs Reservation is an Indian Reservation of the Seneca Nation shared with Cattaraugus County having a total area of only 1 sqmi. This is the site of the famed spring described by the Franciscan Missionary Joseph DeLa Roch D'Allion in 1627, the first recorded mention of oil on the North American Continent. In 1927, the New York State Oil Producers Association sponsored the dedication of a monument at the site describing the history of the oil industry in North America. There is a small park with parking and a foot bridge to the monument. The remainder of the reservation is mostly utilized for cottages on Cuba Lake, Seneca run gas stations and woodlands.
Recreation.
While fishing in the Genesee and other area streams is excellent, Wiscoy Creek in the northern part of the county (also in Wyoming County) is one of the most famous trout streams in the area, drawing fishermen from across northeastern USA. Both wild and stocked brown trout are to be found in various stretches of this stream.
Education.
Higher education facilities include Alfred University, Alfred State College, and Houghton College.
Communities.
Allegany County comprises 29 Towns and 10 Villages.

</doc>
<doc id="52330" url="http://en.wikipedia.org/wiki?curid=52330" title="Robber baron">
Robber baron

A robber baron or robber knight is a historic term and title of disdain that was applied to the behavior and practices of a group of unscrupulous and despotic landowners (nobles) of the medieval period in Europe. They hindered commerce by imposing unauthorized tolls and tariffs and at times by sometimes ransoming or hijacking the goods outright of (pack-animal-dependent) caravans and riverine traffic amidst the poorly roaded tracts of the vast and far-flung demesnes of the Holy Roman Empire, in particular. The term has slightly different meanings in different countries, and has changed somewhat over time. In modern U.S. parlance, the term since the mid-nineteenth century had also come to be used to describe unscrupulous industrialists ("see robber baron (industrialist)") and stock speculators, who like the Germanic robber barons enriched their own pockets without adding to the common good by adding value.
Germany.
Early development.
For one thousand years, from around 800 AD to 1800 AD, tolls were collected from ships sailing on the River Rhine in Europe. During this time, various feudal lords, among them archbishops who held fiefs from the Holy Roman Emperor, collected tolls from passing cargo ships to bolster their finances.
Only the Holy Roman Emperor could authorise the collection of such tolls. Allowing the nobility and Church to collect tolls from the busy traffic on the Rhine seems to have been an attractive alternative to other means of taxation and funding of government functions.
Often iron chains were stretched across the river to prevent passage without paying the toll, and strategic towers were built to facilitate this.
The Holy Roman Emperor and the various noblemen and archbishops who were authorised to levy tolls seem to have worked out an informal way of regulating this process.
Among the decisions involved in managing the collection of tolls on the Rhine were:
While this decision process was made no less complex by being informal, common factors included the local power structure (archbishops and nobles being the most likely recipients of a charter to collect tolls), space between toll stations (authorized toll stations seem to have been at least five kilometres apart), and ability to be defended from attack (some castles through which tolls were collected were tactically useful until the French invaded in 1689 and levelled them).
Tolls were standardized either in terms of an amount of silver coin allowed to be charged or an "in-kind" toll of cargo from the ship.
In contrast, the men who came to be known as robber barons or robber knights (German: "Raubritter") violated the structure under which tolls were collected on the Rhine either by charging higher tolls than the standard or by operating without authority from the Holy Roman Emperor altogether.
Writers of the period referred to these practices as "unjust tolls," and not only did the robber barons thereby violate the prerogatives of the Holy Roman Emperor, they also went outside of the society's behavioural norms, since merchants were bound both by law and religious custom to charge a "just price" for their wares.
Interregnum period.
During the period in the history of the Holy Roman Empire known as the Interregnum (1250–1273), when there was no Emperor, the number of tolling stations exploded in the absence of imperial authority. In addition, robber barons began to earn their newly coined term of "opprobrium" by robbing ships of their cargoes, stealing entire ships and even kidnapping.
In response to this organized, military lawlessness, the "Rheinischer Bund," or Rhine League was formed by and from the nobility, knights, and lords of the Church, all of whom held large stakes in the restoration of law and order to the Rhine.
Officially launched in 1254, the Rhine League wasted no time putting robber barons out of business by the simple expedient of taking and destroying their castles. In the next three years, four robber barons were targeted and between ten and twelve robber castles destroyed or inactivated.
The Rhine League was not only successful in suppressing illicit collection of tolls and river robbery. On at least one occasion, they intervened to rescue a kidnap victim who had been kidnapped by the Baron of Rietberg.
The procedure pioneered by the Rhine League for dealing with robber barons – to besiege, capture and destroy their castles – survived long after the League self-destructed from political strife over the election of a new Emperor and military reversals against unusually strong robber barons.
When the Interregnum ended, the new Emperor Rudolf of Habsburg applied the lessons learned by the Rhine League to the destruction of the highway robbers at Sooneck, torching their castles and hanging them. While robber barony never entirely ceased, especially during the Hundred Years' War, the excesses of their heyday during the Interregnum never recurred.
England.
The reign of King Stephen of England (1096–1154) was a long period of civil unrest commonly known as "The Anarchy". In the absence of strong central kingship, the nobility of England were a law unto themselves, as characterised in this excerpt from the "Anglo-Saxon Chronicle":
When the traitors saw that Stephen was a mild good humoured man who inflicted no punishment, then they committed all manner of horrible crimes. They had done him homage and sworn oaths of fealty to him, but not one of their oaths was kept. They were all forsworn and their oaths broken. For every great man built him castles and held them against the king; they sorely burdened the unhappy people of the country with forced labour on the castles; and when the castles were built they filled them with devils and wicked men. By night and by day they seized those they believed to have any wealth, whether they were men or women; and in order to get their gold or silver, they put them into prison and tortured them with unspeakable tortures, for never were martyrs tortured as they were. They hung them up by the feet and smoked them with foul smoke. They strung them up by the thumbs, or by the head, and hung coats of mail on their feet. They tied knotted cords round their heads and twisted it until it entered the brain. They put them in dungeons wherein were adders and snakes and toads and so destroyed them. Many thousands they starved to death.
United States.
The term robber baron was popularized by U.S. political and economic commentator Matthew Josephson during The Great Depression in a 1934 book. He attributed its first use to an 1880 anti-monopoly pamphlet in which Kansas farmers applied the term to railroad magnates.
Literature references.
Michael Heller refers to the original robber barons to illustrate his tragedy of the anticommons in his 2008 book. The tragedy of the anticommons is a type of coordination breakdown, in which a single resource has numerous rightsholders who prevent others from using it, frustrating what would be a socially desirable outcome.

</doc>
<doc id="52331" url="http://en.wikipedia.org/wiki?curid=52331" title="Broome County, New York">
Broome County, New York

Broome County is a county located in the U.S. state of New York. As of the 2010 census, the population was 200,600. Its county seat and largest city is Binghamton. The county was named in honor of John Broome, who was lieutenant governor in 1806 when Broome County was established.
Broome County is part of the Binghamton, NY Metropolitan Statistical Area.
The current county executive is Debra A. Preston. Broome County is also home to Binghamton University, one of four university centers in the SUNY system.
History.
When counties were established in the Province of New York in 1683, the present Broome County was part of Albany County. This was an enormous county, including the northern part of New York State as well as all of the present State of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to Montgomery County in honor of the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, replacing the name of the hated British governor.
In 1789, Montgomery County was reduced in size by the splitting off of Ontario County. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne Counties.
In 1791, Tioga County split off from Montgomery County, along with Herkimer and Otsego Counties. Tioga County was at this time much larger than the present county and included the present Broome and Chemung Counties and parts of Chenango and Schuyler Counties.
In 1798, Tioga County was reduced in size by the splitting off of Chemung County (which also included part of the present Schuyler County) and by the combination of a portion with a portion of Herkimer County to create Chenango County.
In 1806, the present-day Broome County was split off from Tioga County.
Geography.
According to the U.S. Census Bureau, the county has a total area of 716 sqmi, of which 706 sqmi is land and 9.7 sqmi (1.4%) is water.
Broome County is located in south-central New York, directly north of the border with Pennsylvania in a section of the state called the Southern Tier. The Chenango River joins the Susquehanna River, which flows through the county.
The western half of the county is hilly but has wide valleys that accommodate Binghamton and its suburbs. In the northern portion Interstate 81 takes advantage of another glacial valley. To the east, however, the terrain becomes much more rugged as the land tilts up to the Catskills.
The highest elevation is a U.S. National Geodetic Survey benchmark known as Slawson atop an unnamed hill in the Town of Sanford. It is approximately 2087 feet (636 m) above sea level. An area due east on the Delaware County line in Oquaga Creek State Park also lies within the same elevation contour line. The lowest point is 864 feet (263 m) above sea level, along the Susquehanna at the Pennsylvania state line.
Demographics.
As of the census of 2000, there were 200,536 people, 80,749 households, and 50,225 families residing in the county. The population density was 284 people per square mile (110/km²). There were 88,817 housing units at an average density of 126 per square mile (49/km²). The racial makeup of the county was 91.33% White, 3.28% Black or African American, 0.19% Native American, 2.79% Asian, 0.03% Pacific Islander, 0.79% from other races, and 1.59% from two or more races. 1.99% of the population were Hispanic or Latino of any race. 16.1% were of Irish, 13.3% Italian, 12.3% German, 11.6% English, 6.4% American and 5.7% Polish ancestry according to Census 2000 . 91.4% spoke English, 2.0% Spanish and 1.1% Italian as their first language.
There were 80,749 households out of which 28.20% had children under the age of 18 living with them, 47.60% were married couples living together, 10.80% had a female householder with no husband present, and 37.80% were non-families. 31.00% of all households were made up of individuals and 12.40% had someone living alone who was 65 years of age or older. The average household size was 2.37 and the average family size was 2.97.
In the county the population was spread out with 23.00% under the age of 18, 11.00% from 18 to 24, 26.80% from 25 to 44, 22.80% from 45 to 64, and 16.40% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 93.20 males. For every 100 females age 18 and over, there were 89.90 males.
The median income for a household in the county was $35,347, and the median income for a family was $45,422. Males had a median income of $34,426 versus $24,542 for females. The per capita income for the county was $19,168. About 8.80% of families and 12.80% of the population were below the poverty line, including 15.90% of those under age 18 and 7.20% of those age 65 or over.
Government and politics.
Broome County's offices are housed in the Edwin L. Crawford County Office Building of Government Plaza located at 60 Hawley Street in Downtown Binghamton.
Legislature.
The Broome County Legislature consists of 15 members. All fifteen members of the legislature are elected from individual districts. Currently, there are 12 Republicans and 3 Democrats.
Education.
The four primary institutes of higher education in Broome County include:

</doc>
<doc id="52332" url="http://en.wikipedia.org/wiki?curid=52332" title="Cattaraugus County, New York">
Cattaraugus County, New York

Cattaraugus County is a county located in the western part of the U.S. state of New York, with one side bordering Pennsylvania. As of the 2010 census, the population was 80,317. The county seat is Little Valley. The county was created in 1808 and later organized in 1817.
Cattaraugus County comprises the Olean, NY Micropolitan Statistical Area, which is included in the Buffalo-Cheektowaga, NY Combined Statistical Area. Within its boundaries are the Allegany Indian Reservation of the Seneca Nation of New York, and a large part of the Allegany National Forest. The Allegany River runs through the county.
History.
The largely unsettled territory was the traditional homeland of the now-extinct Wenrohronon Indians and later occupied by the Seneca people, one of the five Nations of the Haudenosaunee. During the colonial era, it was claimed by at least three territories: New York Colony, Massachusetts Bay Colony and Pennsylvania Colony, each of which extended their colonial claims to the west until after the Revolutionary War. 
When counties were established in the province of New York in 1683, the territory of Cattaraugus County was included within the very large Albany County. This was an enormous county, including the northern part of New York as well as all of the present state of Vermont and, in theory, extending westward to the Pacific Ocean. As additional areas were settled, this county was reduced on July 3, 1766 by the creation of Cumberland County, and on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady. The county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County is represented in the 21st century by 37 counties of New York. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled across the Niagara Frontier into modern day Ontario, Canada. In 1784, following the peace treaty that ended the American Revolutionary War (and a treaty with Massachusetts that finally settled who owned Western New York), the name of Tryon County was changed to Montgomery County in honor of the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec. This replaced the name of the hated British governor.
In practice, however, these counties did not cover modern Cattaraugus County or Western New York. Most of that area lay within the Indian Reserve established in the Treaty of Fort Stanwix by the British; it was intended to be reserved for Native Americans and was ruled off-limits to European settlement. 
The newly independent United States changed the rules, after the British ceded their territory east of the Mississippi River to the United States. This included Iroquois territory in New York, without consultation with the Six Nations. The four nations that had been allies of the British mostly relocated to Ontario, where the Crown gave them land grants in some compensation for losses. 
Ontario County was split from Montgomery County in 1789 as a result of the establishment of the Morris Reserve. In turn, Genesee County was split from Ontario County in 1802 as a result of the Holland Purchase. This period was the beginning of more significant European-American settlement of this western territory. Shortly afterward, Genesee County was reduced in 1806 by the creation of Allegany County.
Cattaraugus County was formed in 1808, split off from Genesee County. At first there was no county government due to the sparse population. From 1812 to 1814, Cattaraugus County was incorporated in Allegany County; from 1814 to 1817, records of the county were divided between Belmont (Allegany County) and Buffalo (then in Niagara County). The name "Cattaraugus" derives from a Seneca word for "bad smelling banks," and was named for the "odor of natural gas leaking from rock seams." In 1817, a county government was established for Cattaraugus County.
Numerous towns in the county are named after agents of the Holland Land Company, including Ellicottville (Joseph Ellicott), Franklinville (William Temple Franklin, a speculator and grandson of Benjamin Franklin), and Otto and East Otto (Jacob Otto). The first settlement in the county was in Olean. The original county seat was designated as Ellicottville. After 1860, the county seat was moved to Little Valley.
The Allegany Indian Reservation is within the county boundaries and is one of two controlled by the federally recognized Seneca Nation of New York in the western part of the state. South of Salamanca, New York, a small city located within the reservation, is part of the large Allegany National Forest, which extends into Pennsylvania.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1322 sqmi, of which 1308 sqmi is land and 14 sqmi (1.1%) is water.
Cattaraugus County is in the southwestern part of the state, immediately north of the Pennsylvania border. The southern part of Cattaraugus County is the only area of western New York that was not covered by the last ice age glaciation . It is noticeably more rugged than neighboring areas that had peaks rounded and valleys filled by the glacier. The entire area is a dissected plateau of Pennsylvanian and Mississippian age, but appears mountainous to the casual observer. 
The plateau is an extension of the Allegany Plateau from nearby Pennsylvania. Southern Cattaraugus County is part of the same oil field, and petroleum was formerly a resource of the area. It is now mostly played out, but natural gas continues to be extracted.
A continental divide between the Mississippi River and Great Lakes watersheds runs through Cattaraugus County
The northern border of the county is formed by Cattaraugus Creek and the Allegany River arises in the county.
Transportation.
Airports.
Great Valley Airport is located in Cattaraugus County, one nautical mile (1.85 km) southeast of the central business district of Great Valley.
Cattaraugus County-Olean Airport is located outside of Olean located in the Town of Ischua .
Demographics.
As of the census of 2000, there were 83,955 people, 32,023 households, and 21,647 families residing in the county. The population density was 64 people per square mile (25/km²). There were 39,839 housing units at an average density of 30 per square mile (12/km²). The racial makeup of the county was 94.63% White, 1.06% Black or African American, 2.60% Native American, 0.46% Asian, 0.02% Pacific Islander, 0.23% from other races, and 1.01% from two or more races. 0.94% of the population were Hispanic or Latino of any race. 26.8% were of German, 13.2% Irish, 11.3% English, 9.1% Polish, 8.2% Italian and 7.4% American ancestry according to Census 2000. 95.2% spoke English and 1.4% Spanish as their first language.
There were 32,023 households out of which 32.10% had children under the age of 18 living with them, 52.30% were married couples living together, 10.80% had a female householder with no husband present, and 32.40% were non-families. 26.80% of all households were made up of individuals and 11.60% had someone living alone who was 65 years of age or older. The average household size was 2.52 and the average family size was 3.05.
In the county the population was spread out with 26.20% under the age of 18, 9.30% from 18 to 24, 26.50% from 25 to 44, 23.50% from 45 to 64, and 14.60% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 95.90 males. For every 100 females age 18 and over, there were 92.50 males.
The median income for a household in the county was $33,404, and the median income for a family was $39,318. Males had a median income of $30,901 versus $22,122 for females. The per capita income for the county was $15,959. About 10.00% of families and 13.70% of the population were below the poverty line, including 18.60% of those under age 18 and 9.90% of those age 65 or over.
Government and politics.
Cattaraugus County is run by a unicameral legislature who appoint a county "administrator." This individual serves many executive duties of a county executive but has no legislative veto power and is not elected. 
The legislators are elected independently by districts (while some also serve as mayors of the villages they represent, their status as mayors does not affect in any way their seats on the legislature and thus it is not a Board of Supervisors). The votes of each legislator are counted in proportion to the number of people that said legislator represents (for example, if a legislator from Olean has 10,000 people in his/her district, and the legislator serving Lyndon only has 5,000, the Lyndon legislator's vote has only half the worth of the Olean legislator's). There are currently 21 members of the legislature with 15 Republicans and 6 Democrats; the county is in the process of reducing that number to 17. Each legislator serves a four-year term, with a limit of three terms.
A majority of the county council are Republicans. Norman L. Marsh of Little Valley serves as Chairman, and James J. Snyder of Olean serves as Vice-Chairman.
Cattaraugus County is entirely within the boundaries of the 148th New York State Assembly District (served currently by Joseph Giglio), the 57th New York State Senate District (served currently by Catharine Young), and the U.S. House of Representatives 29th district (served currently by Tom Reed). In the last of these, Cattaraugus County's votes proved pivotal in the 2008 elections: incumbent Republican Randy Kuhl, who had strongly carried the county in 2004 and 2006, lost Cattaraugus County to Democrat Eric Massa in the 2008 elections. 
The county is generally considered a "red county," with Republicans usually outvoting Democrats in most statewide and national offices (for instance, in 2004 George W. Bush defeated John Kerry in Cattaraugus County by a 60-40 margin) although Bill Clinton won the Cattaraugus County very narrowly in 1996. In 2006, the county narrowly chose Eliot Spitzer over John Faso by a margin of about 1% in the governor's race, and Hillary Clinton defeated John Spencer in the county by a 10 percentage point margin. (That being said, in all other positions up for office that year, Republicans won—including all congressional and state legislature representatives.) 
In 2008 the county voted for the Republican presidential candidate, with John McCain defeating Barack Obama by a 55-44% margin. In 2010, Republican Carl Paladino carried Cattaraugus County over Democrat (and eventual winner) Andrew Cuomo 65% to 31%. Senator Kirsten Gillibrand and Senator Chuck Schumer, both incumbent Democrats, carried the county by 51% to 46% and 54% to 43% margins, respectively.
An emerging trend among Cattaraugus County's villages has been consolidation with their surrounding towns. Limestone voters approved dissolution into the Town of Carrollton in September 2009; that took effect at the beginning of 2011. On March 18, 2010, three other villages (East Randolph, Randolph and Perrysburg) followed suit and approved dissolution into their surrounding towns (Town of Randolph for the first two, and Town of Perrysburg for the third); these three villages dissolved at the beginning of 2012.
Additional facts.
Two geological formations, both called "Rock City," have the appearance of a town laid out with streets. One is in Olean and the other is in Little Valley.
Olean is the largest city in the county and is the major center for business. Ski country runs through Cattaraugus County; two ski resorts, popular with Canadians, lie in the town of Ellicottville; in addition, several snowmobile trails cross the county, including the Pat McGee Trail, a flagship for the county's trail system, and the North Country Trail.
Cattaraugus County's current promotional nickname is the "Enchanted Mountains.," The county's heights are mostly described as hills; it has only two comparatively small "mountains" (Mount Seneca and Mount Tuscarora, both in Allegany State Park). During the 1980s, the county used the slogan "Naturally Yours to Enjoy."
Cattaraugus County is considered part of Appalachia, as well as Western New York, upstate New York, the Southern Tier, the Twin Tiers and the Buffalo-Niagara-Cattaraugus Combined Statistical Area. As a result of being at a geographic crossroads, the people of Cattaraugus County speak a variety of accents, ranging from mild variants of Appalachian English to Inland Northern American English, with a handful of people speaking in the more loud and nasal Buffalo English.
The sales tax in Cattaraugus County is 8% (4% from New York, 4% from the county).
A large Amish community is located in the western part of the county. They have maintained houses with indoor plumbing, giving the county a high rate of such homes compared to nearby counties.
Education.
A branch of Jamestown Community College, in Olean provides higher education for residents. Olean Business Institute provided specialized education and is also in Olean; it closed in 2013 due to financial and enrollment declines. Jamestown Business College operates a satellite campus in Salamanca. Cornplanter College, a tribally controlled college, opened in 2014 in Salamanca. St. Bonaventure University is located in its own census-designated place just west of Olean.

</doc>
<doc id="52333" url="http://en.wikipedia.org/wiki?curid=52333" title="Cayuga County, New York">
Cayuga County, New York

Cayuga County is a county located in the U.S. state of New York. As of the 2010 census, the population was 80,026. Its county seat is Auburn. The county was named for one of the tribes of Indians in the Iroquois Confederation.
Cayuga County comprises the Auburn, NY Micropolitan Statistical Area, which is also included in the Syracuse-Auburn, NY Combined Statistical Area.
History.
When counties were established in the Province of New York in 1683, the present Cayuga County was part of Albany County. This was an enormous county, including the northern part of the present state of New York and all of the present state of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770, by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to Montgomery County in honor of the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, replacing the name of the hated British governor.
In 1789, Montgomery County was reduced in size by the splitting off of Ontario County. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne Counties. Harriet Tubman visited Auburn, New York, located in Cayuga County, in 1887 with her daughter (adopted) and her 2nd husband.
Herkimer County was one of three counties split off from Montgomery County (the others being Otsego and Tioga Counties) in 1791.
Onondaga County was formed in 1794 by the splitting of Herkimer County.
Cayuga County was formed in 1799 by the splitting of Onondaga County. This county was, however, much larger than the present Cayuga County. It then included the present Seneca and Tompkins Counties.
In 1804, Seneca County was formed by the splitting of Cayuga County. Then in 1817, in turn, a portion of Seneca County was combined with a piece of the remainder of Cayuga County to form Tompkins County.
Geography.
According to the U.S. Census Bureau, the county has a total area of 864 sqmi, of which 692 sqmi is land and 172 sqmi (20%) is water.
Cayuga County is located in the west central part of the state, in the Finger Lakes region. Owasco Lake is in the center of the county, and Cayuga Lake forms part of the western boundary. Lake Ontario is on the northern border, and Skaneateles Lake and Cross Lake form part of the eastern border. Cayuga County has more waterfront land than any other county in the state not adjacent to the Atlantic Ocean.
Demographics.
As of the census of 2000, there were 81,963 people, 30,558 households, and 20,840 families residing in the county. The population density was 118 people per square mile (46/km²). There were 35,477 housing units at an average density of 51 per square mile (20/km²). The racial makeup of the county was 93.34% White, 3.99% Black or African American, 0.31% Native American, 0.42% Asian, 0.02% Pacific Islander, 0.88% from other races, and 1.03% from two or more races. 1.97% of the population were Hispanic or Latino of any race. 16.3% were of Irish, 16.0% English, 15.7% Italian, 11.3% German, 9.5% American and 6.3% Polish ancestry according to Census 2000. 94.9% spoke English, 2.0% Spanish and 1.0% Italian as their first language.
There were 30,558 households out of which 32.60% had children under the age of 18 living with them, 52.00% were married couples living together, 11.00% had a female householder with no husband present, and 31.80% were non-families. 26.20% of all households were made up of individuals and 11.90% had someone living alone who was 65 years of age or older. The average household size was 2.53 and the average family size was 3.04.
In the county the population was spread out with 25.10% under the age of 18, 8.20% from 18 to 24, 29.70% from 25 to 44, 22.60% from 45 to 64, and 14.40% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 102.20 males. For every 100 females age 18 and over, there were 101.80 males.
The median income for a household in the county was $37,487, and the median income for a family was $44,973. Males had a median income of $33,356 versus $23,919 for females. The per capita income for the county was $18,003. About 7.80% of families and 11.10% of the population were below the poverty line, including 14.90% of those under age 18 and 8.20% of those age 65 or over.
Government and politics.
Cayuga County is considered a swing county in national elections. In 2000, Democrat Al Gore won Cayuga County with 50% of the vote to Bush's 44%. In 2004 however Republican George W. Bush defeated John Kerry by a margin of only 0.58% or 49.22% to 48.64%. In statewide elections it has gone for Democrats both Eliot Spitzer and Hillary Clinton won it in 2006 with over 60% of the vote. In 2008 it was won by Democrat Barack Obama with 53% of the vote to Republican John McCain's 45%. In 2010, Democrat Andrew Cuomo defeated Republican Carl Paladino 53% to 40% with 3% going to Green Party candidate Howie Hawkins. Also in 2010, both Senators Kirsten Gillibrand and Chuck Schumer carried Cayuga County. Gillibrand won 54% of the vote while Schumer won 61%.
The Cayuga County Legislature consists of 15 members each of whom are elected from individual districts.

</doc>
<doc id="52334" url="http://en.wikipedia.org/wiki?curid=52334" title="Chautauqua County, New York">
Chautauqua County, New York

Chautauqua County is the westernmost county in the U.S. state of New York. As of the 2010 census, the population was 134,905. Its county seat is Mayville, and its largest city is Jamestown. Its name may be a contraction of a Seneca Indian word meaning "bag tied in the middle". The county was created in 1808 and later organized in 1811.
Chautauqua County comprises the Jamestown-Dunkirk-Fredonia, NY Micropolitan Statistical Area. It is located south of Lake Erie and includes part of the Cattaraugus Reservation of the Seneca.
History.
Long part of the homeland of the Seneca Nation, one of the six nations of the Iroquois Confederacy, Chautauqua County was organized by the state legislature during the development of western New York after the American Revolutionary War. It was officially separated from Genesee County on March 11, 1808. This partition was performed under the same terms that produced Cattaraugus and Niagara counties. The partition was done for political purposes, but the counties were not properly organized for self-government, so they were all administered as part of Niagara County.
On February 9, 1811, Chautauqua was completely organized, and its separate government was launched. This established Chautauqua as a county of 1,100 square miles (2,850 square km) of land. Chautauqua has not been altered since.
Geography.
According to the U.S. Census Bureau, the county has a total area of 1500 sqmi, of which 1060 sqmi is land and 440 sqmi (29%) is water.
Chautauqua County, in the southwestern corner of New York State, along the New York-Pennsylvania border, is the westernmost of New York's counties. Chautauqua Lake is located in the center of the county, and Lake Erie is its northern border.
Part of the Eastern Continental Divide runs through Chautauqua County. The area that drains into the Conewango Creek (including Chautauqua Lake) eventually empties into the Gulf of Mexico; the rest of the county's watershed empties into Lake Erie and via Lake Ontario and the St. Lawrence Seaway into the North Atlantic Ocean. This divide, known as the Chautauqua Ridge, can be used to mark the border between the Southern Tier and the Niagara Frontier. It is also a significant dividing point in the county's geopolitics, with the "North County" being centered on Dunkirk and the "South County" centered on Jamestown each have their own interests.
The county is generally composed of rolling hills and valleys, with elevations ranging anywhere between 1100 and 2100 feet, although the land within a few miles of Lake Erie is generally flat and at an elevation of 1000 feet or lower. The lowest point in the county is Lake Erie, at 571 feet (174 meters), and the highest point is Gurnsey Benchmark at 2180 feet (664 meters).
Demographics.
As of the census of 2000, there were 139,750 people, 54,515 households, and 35,979 families residing in the county. The population density was 132 people per square mile (51/km²). There were 64,900 housing units at an average density of 61 per square mile (24/km²). The racial makeup of the county was 94.04% White, 2.18% Black or African American, 0.43% Native American, 0.36% Asian, 0.03% Pacific Islander, 1.73% from other races, and 1.23% from two or more races. 4.22% of the population were Hispanic or Latino of any race. 17.3% were of German, 15.1% Italian, 11.6% Swedish, 10.9% English, 9.3% Polish, 9.2% Irish and 5.6% American ancestry according to Census 2000. 93.0% spoke English and 3.8% Spanish as their first language.
There were 54,515 households out of which 30.50% had children under the age of 18 living with them, 50.90% were married couples living together, 10.80% had a female householder with no husband present, and 34.00% were non-families. 28.10% of all households were made up of individuals and 12.60% had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 2.99.
In the county the population was spread out with 24.50% under the age of 18, 10.30% from 18 to 24, 26.30% from 25 to 44, 23.00% from 45 to 64, and 16.00% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 95.20 males. For every 100 females age 18 and over, there were 92.20 males.
The median income for a household in the county was $33,458, and the median income for a family was $41,054. Males had a median income of $32,114 versus $22,214 for females. The per capita income for the county was $16,840. About 9.70% of families and 13.80% of the population were below the poverty line, including 19.30% of those under age 18 and 8.20% of those age 65 or over.
As of the 2010 Census, there were 134,905 people in the county. The population density was 127 people per square mile (49/km²). The racial makeup of the county was 92.57% (124,875 people) white, 2.37% (3,197 people) African-American, 0.51% (688 people) Asian, 0.51% (689 people) Native American/Alaskan, 0.03% (34 people) Native Hawaiian/Pacific Islander, 1.98% (2,669 people) other, and 2.04% (2,751 people) two or more races. The Hispanic/Latino population of any race was 6.11% (8,241 people).
In the county, the population was spread out with 21.83% of the population under the age of 18, 3.82% (5,155 people) ages 18 and 19, 7.50% (10,113 people) ages 20–24, 10.37% (13,985 people) ages 25–34, 18.83% (25,406 people) ages 35–49, 21.07% (28,419 people) ages 50–64, and 16.59% (22,381 people) over the age of 65. Of the population, 49.3% (66,509 people) were male and 50.7% (68,396 people) were female.
Government and politics.
Chautauqua County is one of 19 "charter counties" in New York, which grants the county greater leeway in conducting its own affairs.
Chautauqua County was governed by a board of supervisors until 1975, when a new county charter went into effect with provisions for a county executive and a 13-seat county legislature. The county council currently consists of 19 members, down from 25, each elected from single member districts.
All of the county is in the 150th New York State Assembly district, represented by Andy Goodell. The entire county is within the bounds of New York's 23rd congressional district (served by Tom Reed) and the New York State Senate 57th district (served by Catharine Young). Prior to 2013, the county was part of New York's 27th congressional district. Prior to 2003, the county was part of New York's 31st congressional district (now the 29th), but was controversially redistricted out of that district and into what was the 27th, and was replaced in the 29th district by Rochester suburbs that had never before been part of the district. Chautauqua County, at the same time, joined southern Erie County and portions of the City of Buffalo in the 27th, areas that had also never been in the same district with each other. In both cases, the suburban additions were significantly more Democratic populations than the rural 31st was, leading to Democrats winning both portions of the divided territory and accusations of cracking-based gerrymandering. The 2012 redistricting process moved all of Chautauqua County into Goodell's assembly district, while the county also rejoined the former 31st (renumbered the 23rd) congressional district along with Cattaraugus and Allegany Counties.
Chautauqua County had been a perfect bellwether county from 1980 to 2008, correctly voting for the winner of each presidential election in all eight elections in that time frame. Its 2012 vote (in which it voted for Republican Mitt Romney instead of incumbent Democrat Barack Obama) was its first miss since 1976.
Education.
Jamestown Community College has two campuses in the county at Jamestown and Dunkirk. The State University of New York at Fredonia is located in the northern part of the county. Jamestown Business College offers two year degrees, certificates, and a four year degree in Jamestown.

</doc>
<doc id="52335" url="http://en.wikipedia.org/wiki?curid=52335" title="Idiolect">
Idiolect

In linguistics, an idiolect is an individual's distinctive and unique use of language, including speech. This unique usage encompasses vocabulary, grammar, and pronunciation. 
Idiolect is the variety of language unique to an individual. This differs from a dialect, a common set of linguistic characteristics shared among some group of people.
Idiolect and language.
The notion of "language" is used as an abstract description of the "language use", and of the abilities of individual speakers and listeners. According to this view, a language is an "ensemble of idiolects . . . rather than an entity per se." Linguists study particular languages, such as English or Xhosa, by examining the utterances produced by the people who speak the language.
This contrasts with a view among non-linguists, at least in the United States, that languages as ideal systems exist outside the actual practice of language users. Based on work done in the US, Nancy Niedzielski and Dennis Preston describe a language ideology that appears to be common among American English speakers. According to Niedzielski and Preston, many of their subjects believe that there is one "correct" pattern of grammar and vocabulary that underlies Standard English, and that individual usage derives from this external system.
Linguists who understand particular languages as a composite of unique, individual idiolects must nonetheless account for the fact that members of large speech communities, and even speakers of different dialects of the same language, can understand one another. All human beings seem to produce language in essentially the same way. This has led to searches for universal grammar, as well as attempts to further define the nature of particular languages.
Forensic linguistics.
The scope of forensic linguistics includes attempts to identify whether a certain person did or did not produce a given text by comparing the style of the text with the idiolect of the individual. The forensic linguist may conclude that the text is consistent with the individual, rule out the individual as the author, or deem the comparison inconclusive.

</doc>
<doc id="52340" url="http://en.wikipedia.org/wiki?curid=52340" title="Aitutaki">
Aitutaki

Aitutaki, also traditionally known as Araura, Ararau and Utataki, is one of the Cook Islands, north of Rarotonga. It has a population of approximately 2,000. Aitutaki is the second most visited island of the Cook Islands. The main village is Arutanga (Arutunga) on the west side.
Geography.
Aitutaki is an "almost atoll". It has a maximum elevation of approximately 123 metres with the hill known as Maunga Pu close to its northernmost point. The land area of the atoll is 18.05 km2, of which the main island occupies 16.8 km2. The Ootu Peninsula, protruding east from the main island in a southerly direction along the eastern rim of the reef, takes up 1.75 km2 out of the main island. For the lagoon, area figures between 50 and are found. Satellite image measurement suggests that the larger figure also includes the reef flat, which is commonly not considered part of a lagoon.
The barrier reef that forms the basis of Aitutaki is roughly the shape of an equilateral triangle with sides 12 km in length. The southern edge of the triangle is almost totally below the surface of the ocean, and the eastern side is composed of a string of small islands including Mangere, Akaiami, and Tekopua.
The western side of the atoll contains many of Aitutaki's important features including a boat passage through the barrier reef allowing for anchorage close to shore at Arutanga. Towards the south of the side is a small break in the barrier reef, allowing access for small boats to the lagoon which covers most of the southern part of the triangle. Further to the north is the bulk of the main island. Its fertile volcanic soil provide tropical fruits and vegetables. Two of Aitutaki's 15 islets (motus) are also volcanic. The rest are made of coral.
Aitutaki Airport is located close to the triangle's northern point. There is an area suitable to land flying boats in the southeastern part of the lagoon.
Subdivisions.
Aitutaki is subdivided in 8 districts. The districts are further subdivided into 19 tapere (land holdings by tribe lineages).
The eight villages are:
The eight districts are subdivided into 19 tapere as follows:
History.
Polynesians probably first settled Aitutaki around AD 900. The first known European contact was with Captain Bligh and the crew of the when they discovered Aitutaki on April 11, 1789, prior to the infamous mutiny.
Aitutaki was the first of the Cook Islands to accept Christianity, after London Missionary Society (LMS) missionary John Williams visited in 1821. The oldest church in the country, the Cook Islands Christian Church in Arutanga, was built by Papeiha (Bora Bora) and Vahapata (Raiatea), two LMS teachers Williams had left behind.
On 8-9 October 1900 seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people; and by a British Proclamation issued at the same time the cessions were accepted, the islands being declared parts of Her Britanic Majesty's dominions. Uniquely, these instruments did not include Aitutaki. It appears that, though the inhabitants of Aitutaki regarded themselves as British subjects, the Crown's title was uncertain, and the island was formally annexed by Proclamation dated 9 October 1900. It was the only island in the Cook Islands that was annexed rather than ceded. 
In 1942 New Zealand and American forces were stationed on the island, building the two-way airstrip that can be seen today. This airport, and one on the northernmost Penrhyn Island, were to be used as bases by the Allies during World War II. The first aircraft, an American light bomber, landed on November 22, 1942. When the war ended some of the servicemen remained and married the locals.
During the 1950s Aitutaki's lagoon was used as a stopover for TEAL (Tasman Empire Airways Limited) flying boats on the famous Coral Route. The islet of Akaiami was used as a resting stop for passengers, who often lay about until the aircraft was refuelled for two hours. These operations ceased in 1960, and the only reminder are the remains of the purpose-built jetty on Akaiami. The flying boat "Aranui", which was part of this service, is now on display at the Museum of Transport and Technology in Auckland, New Zealand.
Two of Aitutaki's motus (small islands), Rapota and Moturakau, were the locations of the first series of the UK reality television program "Shipwrecked" in 2000.
More recently, in 2001, Steve Fossett passed over just south of Aitutaki in the balloon Solo Spirit during his round-the-world trip.
In 2006, the island was used as the location for the tribal council in the US TV program "". Surrounding islands were used for tribal camps and crew locations. One of the tribes was named "Aitutaki" (or 'Aitu') after the island.
Then, not long afterwards, "Shipwrecked" returned again, with "". This was filmed on the same islands as before. One year later, Aitutaki was the locale of an episode of "Survivorman".
On February 10–11, 2010, Aitutaki was hit by Cyclone Pat. The high winds of the storm ripped the roofs off of most houses and damaged other buildings including a school and a hospital. At least 60% of houses were damaged. There were no reported deaths but a few minor injuries were reported. An Air Force Hercules cargo plane and an army engineering team were provided by New Zealand along with an initial $200,000. Cook Island MP Teina Bishop said "New Zealand aid should have been sent to the devastated area much sooner.".
In June 2010 the island was nominated "the world's most beautiful island" by Tony Wheeler the founder of Lonely Planet travel guide.
The delegates of the 2012 Pacific Islands Forum, which had its main venue in Rarotonga, travelled to One Foot Island for a 2-day retreat.
Demographics.
The population of Aitutaki is 2,194.
Places of interest.
Aitutaki is famous for its turquoise central lagoon, uninhabited islands and palm-fringed beaches. Another advantage is that until now it has been spared by mass tourism. Noteworthy also are an old church (the oldest in the Cook Islands) and some gigantic Banyan trees "(Ficus prolixa)".
Tapuaetai (One Foot Island), a small islet in the south-east of the lagoon, is often said to be the most important attraction. It is regarded as providing the visitor with the best views of the Aitutaki lagoon and, depending on the tide, one is able to walk on a sandbank a decent distance away from Tapuaetai. The trip to this island is the most frequented trip available on Aitutaki. One Foot Island was awarded "Australasia's Leading Beach" at the World Travel Awards held in Sydney in June 2008.
Air Rarotonga offers daily flights and a day tour from Rarotonga.
Economy.
Government jobs are the highest paying jobs on the island for the locals followed closely by tourism.
Tourism numbers have started to climb recently with the exposure Aitutaki has been given by travel programmes.
The recent addition of a newly built resort has helped to increase tourism numbers.
Sports.
The most popular sport on Aitutaki is Rugby union and netball, followed closely by volleyball. With a population of 2,000 residing on the island and 50,000 overseas, there are four clubs on Aitutaki and eight teams (each club having a first team and a reserve team). The best players on the island play for the Aitutaki island team against their main rivals Rarotonga.
Minor islands of Aitutaki.
The main island of Aitutaki occupies the northern part of the atoll, which is roughly triangular in shape. The minor islands form part of the perimeter of the lagoon. All islands, including the main island and its peninsula Ootu, are listed starting clockwise from the northernmost point of the atoll:
Ootu Peninsula is of coral formation, but attached to the main volcanic island, thus a peninsula. If it were an island, it would be the largest of the minor islands. Ootu Peninsula does belong to tapere and district of "Vaitupa". The minor islands are not allocated to any districts or tapere, but they do form part of the larger constituencies.
All minor islets, except Akitua and Maina, are part of Vaipae-Tautu Constituency. Akitua is part of Amuri-Ureia Constituency, as is Ootu Peninsula, just north of Akitua. Maina is part of Arutanga-Reureu-Nikaupara Constituency. The main island is equally divided among the three constituencies Arutanga-Reureu-Nikaupara (southwest), Vaipae-Tautu (southeast), and Amuri-Ureia (north).
Education.
Araura College is the only secondary school on Aitutaki. The school has the role of teaching approximately 200 students from Year 7 (Form 1) to Year 12 (Form 6).
The island has two government schools and one church school: Araura Primary school, Vaitau Primary School and Tekaaroa Primary School. Tekaaroa Primary School is a private special character school which is the designated Seventh Day Adventist (SDA) school. Araura Primary is the larger of the primary schools, catering for the mostly populated part of the island and Vaitau Primary caters mostly for the Vaipae and Tautu villages. Tekaaroa Primary School caters for the Seventh Day Adventist children on the island.
All schools follow the Cook Islands Curriculum Framework. This is largely based on the New Zealand Curriculum Framework, as most of the young people will eventually migrate to New Zealand to study there at universities or polytechnics. Students from Year 11 (Form 5) and Year 12 (Form 6) are the exception, as they study from the New Zealand Curriculum Framework at NCEA Levels 1, 2 and 3.

</doc>
<doc id="52343" url="http://en.wikipedia.org/wiki?curid=52343" title="Brahmic scripts">
Brahmic scripts

The Brahmic scripts are a family of abugida writing systems. They are used throughout South Asia (excluding Pakistan and Afghanistan), Southeast Asia, and parts of Central and East Asia, and are descended from the Brāhmī script of ancient India. They are used by languages of several language families: Indo-European, Dravidian, Tibeto-Burman, Mongolic (Soyombo alphabet), Austroasiatic, Austronesian, Tai. They were also the source of the dictionary order of Japanese "kana".
History.
Brahmic scripts are descended from the Brahmi script. Brahmi is clearly attested from the 3rd century BC during the reign of Ashoka, who used the script for imperial edicts, but there are some claims of earlier epigraphy found on pottery in South India and Sri Lanka. The most reliable of these were short Brahmi inscriptions dated to the 4th century BC and published by Coningham et al., but scattered press reports have claimed both dates as early as the 6th century BC and that the characters are identifiably Tamil Brahmi, though these latter claims do not appear to have been published academically. Northern Brahmi gave rise to the Gupta script during the Gupta period, which in turn diversified into a number of cursives during the Middle Ages, including Siddham, Sharada and Nagari.
The Siddham script was especially important in Buddhism, as many sutras were written in it. The art of Siddham calligraphy survives today in Japan. The syllabic nature and dictionary order of the modern "kana" system of Japanese writing is believed to be descended from the Indic scripts, most likely through the spread of Buddhism.
Southern Brahmi evolved into Old-Kannada, Pallava and Vatteluttu scripts, which in turn diversified into other scripts of South India and Southeast Asia.
Bhattiprolu was a great centre of Buddhism during 3rd century BCE and from where Buddhism spread to east Asia. The present Telugu script is derived from Bhattiprolu Script or 'Kannada-Telugu script' or Kadamba script, also known as 'old Telugu script', owing to its similarity to the same.
Initially, minor changes were made which is now called Tamil Brahmi which has far fewer letters than some of the other Indic scripts as it has no separate aspirated or voiced consonants.
Characteristics.
Some characteristics, which are present in most but not all the scripts, are:
Comparison.
Below are comparison charts of several of the major Indic scripts, organised on the principle that glyphs in the same column all derive from the same Brahmi glyph. Accordingly:
The transliteration is indicated in ISO 15919.
Vowels.
Vowels are presented in their independent form on the left of each column, and in their corresponding dependent form (vowel sign) combined with the consonant "k" on the right. A glyph for "ka" is an independent consonant letter itself without any vowel sign, where the vowel "a" is inherent. When used to write their own languages, Khmer and Thai script can have either an "a" or an "o" as the inherent vowel, following the rules of their respective orthographies. Thai and Lao script do not have independent vowel forms, for syllables starting with a vowel sound, a zero consonant, อ and ອ respectively, is used as a placeholder.
Note: Glyphs for "r̥̄", "l̥", "l̥̄" and a few other glyphs are obsolete or very rarely used.
List of Brahmic scripts.
Scripts derived from Brahmi.
Historical.
The Brahmi script was already divided into regional variants at the time of the earliest surviving epigraphy around the 3rd century BC. Cursives of the Brahmi script began to diversify further from around the 5th century AD and continued to give rise to new scripts throughout the Middle Ages. The main division in antiquity was between northern and southern Brahmi. In the northern group, the Gupta script was very influential, and in the southern group the Vatteluttu and Old-Kannada/Pallava scripts with the spread of Hinduism sent Brahmic scripts throughout Southeast Asia.

</doc>
<doc id="52346" url="http://en.wikipedia.org/wiki?curid=52346" title="Nebuchadnezzar II">
Nebuchadnezzar II

Nebuchadnezzar II (; Aramaic: ܢܵܒܘܼ ܟܘܼܕܘܼܪܝܼ ܐܘܼܨܘܼܪ‎; Hebrew: נְבוּכַדְנֶצַּר "Nəḇūḵaḏneṣṣar"; Ancient Greek: Ναβουχοδονόσωρ "Naboukhodonósôr"; Arabic: نِبُوخَذنِصَّر "nibūḫaḏniṣṣar"; c. 634 – 562 BC) was an Assyrian king of the Neo-Babylonian Empire, who reigned c. 605 BC – 562 BC. Both the construction of the Hanging Gardens of Babylon and the destruction of Jerusalem's temple are ascribed to him. He is featured in the Book of Daniel and is mentioned in several other books of the Bible.
The Akkadian name, Nabû-kudurri-uṣur, means "O god Nabu, preserve/defend my firstborn son". Nabu, son of the god Marduk, is the Babylonian deity of wisdom. In an inscription, Nebuchadnezzar styles himself as Nabu's "beloved" and "favourite". His name has previously been mistakenly interpreted as "O Nabu, defend my "kudurru"", in which sense a "kudurru" is an inscribed stone deed of property. However, when contained in a ruler's title, "kudurru" approximates to "firstborn son" or "oldest son". Variations of the Hebrew form include נְבוּכַדְנֶאצַּר and נְבוּכַדְרֶאצַּר ("Nəḇuḵaḏreṣṣar"). He is also known as "Bakhat Nasar", which means "winner of the fate", or literally, "fate winner".
Life.
Nebuchadnezzar was the oldest son and successor of Nabopolassar, who delivered Babylon from its three centuries of vassalage to its fellow Mesopotamian state, Assyria, and in alliance with the Medes, Persians, Scythians, and Cimmerians, laid Nineveh in ruins. According to Berossus, some years before he became king of Babylon, Babylonian dynasties were united. There are conflicting accounts of Nitocris of Babylon being either his wife or daughter.
Nabopolassar was intent on annexing the western provinces of Syria (ancient Aram) from Necho II (whose own dynasty had been installed as vassals of Assyria, and who was still hoping to help restore Assyrian power), and to this end dispatched his son westward with a large army. In the ensuing Battle of Carchemish in 605 BC, the Egyptian and Assyrian army was defeated and driven back, and the region of Syria and Phoenicia were brought under the control of Babylon. Nabopolassar died in August that year, and Nebuchadnezzar returned to Babylon to ascend the throne.
After the defeat of the Cimmerians and Scythians, previous allies in the defeat of Assyria, Nebuchadnezzar's expeditions were directed westward. The powerful Median empire lay to the north. Nebuchadnezzar's political marriage to Amytis of Media, the daughter of the Median king, had ensured peace between the two empires.
Nebuchadnezzar engaged in several military campaigns designed to increase Babylonian influence in Aramea (modern Syria) and Judah. An attempted invasion of Egypt in 601 BC was met with setbacks, however, leading to numerous rebellions among the Phoenician and Canaanite states of the Levant, including Judah. Nebuchadnezzar soon dealt with these rebellions, capturing Jerusalem in 597 BC and deposing King Jehoiakim, then destroying the city in 587 BC due to rebellion, and deporting many of the prominent citizens along with a sizable portion of the Jewish population of Judea to Babylon. These events are described in the Prophets ("Nevi'im") and Writings ("Ketuvim"), sections of the Hebrew Bible (in the books 2 Kings and Jeremiah, and 2 Chronicles, respectively). After the destruction of Jerusalem, Nebuchadnezzar engaged in a thirteen-year siege of Tyre (circa 586–573) which ended in a compromise, with the Tyrians accepting Babylonian authority.
Following the pacification of the Phoenician state of Tyre, Nebuchadnezzar turned again to Egypt. A clay tablet, now in the British Museum, states: "In the 37th year of Nebuchadnezzar, king of the country of Babylon, he went to Mitzraim (Egypt) to wage war. Amasis, king of Egypt, collected [his army], and marched and spread abroad." Having completed the subjugation of Phoenicia, and a campaign against Egypt, Nebuchadnezzar set himself to rebuild and adorn the city of Babylon, and constructed canals, aqueducts, temples and reservoirs.
According to Babylonian tradition, towards the end of his life, Nebuchadnezzar prophesied the impending ruin of the Chaldean Dynasty (Berossus and Abydenus in Eusebius, "Praeparatio Evangelica", 9.41). He died in Babylon between the second and sixth months of the forty-third year of his reign, and was succeeded by Amel-Marduk.
Construction activity.
During the last century of Nineveh's existence, Babylon had been greatly devastated, not only at the hands of Sennacherib and Assurbanipal, but also as a result of Babylon's repeated rebellions. Nebuchadnezzar, continuing his father's work of reconstruction, aimed at making his capital one of the world's wonders. Old temples were restored; new edifices of incredible magnificence were erected to the many gods of the Babylonian pantheon (Diodorus of Sicily, 2.95; Herodotus, 1.183). To complete the royal palace begun by Nabopolassar, nothing was spared, neither "cedar-wood, nor bronze, gold, silver, rare and precious stones"; an underground passage and a stone bridge connected the two parts of the city separated by the Euphrates; the city itself was rendered impregnable by the construction of a triple line of walls. The bridge across the Euphrates is of particular interest, in that it was supported on asphalt covered brick piers that were streamlined to reduce the upstream resistance to flow, and the downstream turbulence that would otherwise undermine the foundations. Nebuchadnezzar's construction activity was not confined to the capital; he is credited with the restoration of the Lake of Sippar, the opening of a port on the Persian Gulf, and the building of the Mede wall between the Tigris and the Euphrates to protect the country against incursions from the north. These undertakings required a considerable number of laborers; an inscription at the great temple of Marduk suggests that the labouring force used for his public works was most likely made up of captives brought from various parts of western Asia.
Nebuchadnezzar is credited by Berossus with the construction of the Hanging Gardens, for his homesick wife Amyitis (or Amytis) to remind her of her homeland, Medis (Media) in Persia. He is also credited for the construction of the Ishtar Gate, one of the eight gates leading into the city of Babylon. However, some scholars argue that the Gardens may have been constructed by the Assyrian king Sennacherib in his capital city, Nineveh.
Portrayal in the Bible.
Nebuchadnezzar is widely known through his portrayal in the Bible, especially the Book of Daniel. The Bible discusses events of his reign and his conquest of Jerusalem. Daniel 2 contains an account attributed to the second year of his reign, in which Nebuchadnezzar dreams of a huge image made of various materials (gold, silver, bronze, iron and clay). The prophet Daniel tells him God's interpretation, that it stands for the rise and fall of world powers, starting with Nebuchadnezzar's own as the golden head.
Daniel 3 is an account of Nebuchadnezzar erecting a large idol made of gold for worship during a public ceremony on the plain of Dura. When three Jews, whose names were Hananiah, Mishael, and Azariah (respectively renamed Shadrach, Meshach, and Abednego by their captors, to facilitate their assimilation into Babylonian culture), refuse to take part, he has them cast into a fiery furnace. They are protected by what Nebuchadnezzar describes as "a son of the gods" (Daniel 3:25 NIV) and emerge unscathed without even the smell of smoke. Daniel 3 goes on to say that Nebuchadnezzar realized that no man-made god has the power to save and praised the God of Shadrach, Meshach, and Abednego. He then made a decree that anyone of any nation that would make any accusation against God would be mutilated and their homes be destroyed. Daniel 4 contains an account of Nebuchadnezzar's dream about an immense tree, which Daniel interprets to mean that Nebuchadnezzar will go insane for seven years because of his pride. The chapter is written from the perspective of king Nebuchadnezzar.
Bout of insanity.
While boasting about his achievements, Nebuchadnezzar is humbled by God. The king loses his sanity and lives in the wild like an animal for seven years. After this, his sanity and position are restored and he praises and honors God. Theologians have interpreted this story in several ways. Origen attributed the metamorphosis as a representation of the fall of Lucifer, Bodin and Cluvier maintained it was a metamorphosis of both soul and body, Tertullian confined the transformation to the body only, without the loss of reason, cases of which Augustine stated were reported in Italy, but gave them little credit. Gaspard Peucer asserted that the transformation of men into wolves was common in Livonia. Some Jewish rabbis asserted there was an exchange of souls between the man and ox, while others argued for an apparent or docetic change which was not real. The most generally received opinion, which was also held by Jerome, was that the madman was under the influence of hypochondriachal monomania by which God could humble the pride of kings.
Modern writers have speculated that the biblical account might refer to an illness with a natural organic cause. Some consider it to have been an attack of clinical lycanthropy or alternatively porphyria. Psychologist Henry Gleitman wrote that Nebuchadnezzar's insanity was a result of general paresis or paralytic dementia seen in advanced cases of syphilis.
Some scholars think that Nebuchadnezzar's portrayal by Daniel is a mixture of traditions about Nebuchadnezzar and about Nabonidus ("Nabuna'id") who became confused with him. For example, Nabonidus was the natural, or paternal father of Belshazzar, and the seven years of insanity could be related to Nabonidus' sojourn in Tayma in the desert. Fragments from the Dead Sea Scrolls, written from 150 BC to 70 AD state that it was Nabonidus (N-b-n-y) who was smitten by God with a fever for seven years of his reign while his son Belshazzar ruled.
The Book of Jeremiah contains a prophecy about the arising of a "destroyer of nations", commonly regarded as a reference to Nebuchadnezzar ("Jer." 4:7), as well as an account of Nebuchadnezzar's siege of Jerusalem and looting and destruction of the temple ("Jer." 52).
Helel, Son of the Morning.
Chapter 14 of the Book of Isaiah refers to what Jewish exegesis of the prophetic vision of Isaiah 14:12-15 identifies as King Nebuchadnezzar II; the Hebrew text says הֵילֵל בֶּן-שָׁחַר ("Helel ben Shaḥar", "day-star, son of the morning"). It is a taunting prophecy against an oppressive king. In Isaiah 14, the king is being mocked, as he is struck through with a sword, killed, and thrown into a common grave. Mainstream Christianity reads into this passage to the fall of Lucifer because verse 20 says that this king of Babylon will not be "joined with them [all the kings of the nations] in burial, because thou hast destroyed thy land, thou hast slain thy people; the seed of evil-doers shall not be named for ever", but rather be cast out of the grave, while "All the kings of the nations, all of them, sleep in glory, every one in his own house". "Helel ben Shaḥar" may refer to the Morning Star, but Isaiah gives no indication that Helel is a star.
Portrayal in medieval Muslim sources.
According to Tabari, Nebuchadnezzar, whose Persian name was Bukhtrashah, was of Persian descent, from the progeny of Jūdharz. Some believe he lived as long as 300 years. While much of what is written about Nebuchadnezzar depicts a ruthless warrior, some texts show a ruler who was concerned with both spiritual and moral issues in life and was seeking divine guidance.
Nebuchadnezzar was seen as a strong, conquering force in Islamic texts and historical compilations, like Tabari. The Babylonian leader used force and destruction to grow an empire. He conquered kingdom after kingdom, including Phoenicia, Philistia, Judah, Ammon, Moab, Jerusalem, and more. The most notable events that Tabari’s collection focuses on is the destruction of Jerusalem.
Destruction of Jerusalem.
Nebuchadnezzar was sent from Balkh by Luhrāsb, a Persian ruler, to defeat the Jews in Jerusalem. Some sources believe that Luhrāsb’s son, Bahman, is the one who sent Nebuchadnezzar to exile the Jews from Jerusalem. According to one source in Tabari, at the time Nebuchadnezzar was summoned to defeat Jerusalem, he was finishing a peace agreement with the people of Damascus. Because of this, he sent an officer to ease the tension in Jerusalem and create a peace treaty. The officer successfully met with the king of Jerusalem and made a peace treaty. As was custom for the Babylonians, the officer took hostages with him as and began the return journey to Nebuchadnezzar. When the officer reached Tiberias, he heard that the Israelites had revolted against their king and killed him because the king had given the Babylonians hostages. The hostages were then beheaded and Nebuchadnezzar made his way to Jerusalem.
According to stories in the Old Testament Nebuchadnezzar ravaged the town, killed and enslaved the people, and then discovered the prophet Jeremiah in a prison. He had been jailed for about three years for prophesying Jerusalem's fate as told to him by God. Jeremiah warned the Israelites and told them to repent, but they had instead jailed him. The Testament states that God sent an angel to ask Jeremiah if the Israelites must be destroyed and Jeremiah agreed, beginning the attack by Nebuchadnezzar. When Jeremiah told this story to Nebuchadnezzar, he replied, “Wretched people, they defied their lord’s messenger”. He then released Jeremiah. Nebuchadnezzar does not worship the god of the Bible but Marduk, a Babylonian god most often related to judgement. The Testament suggests that Nebuchadnezzar respected Jeremiah and his beliefs, allowing Jeremiah to be an ally and help Nebuchadnezzar with policy.
Nebuchadnezzar then goes on to attack Egypt. After releasing Jeremiah from prison, the remaining Israelites apologized to Jeremiah but still do not listen to him when he tells them to stay in Jerusalem. They instead flee to Egypt, where the king takes them in even after Nebuchadnezzar has asked that they be returned. Nebuchadnezzar then conquers Egypt and moves further north in Africa before returning home with treasures and hoards of slaves. Nebuchadnezzar’s victories display the period of growth that Babylonians were experiencing. Every new victory resulted in a further accumulation of wealth and prisoners of war, both of which were used to strengthen the empire even more. It is also suggested that Nebuchadnezzar took royals hostage but treated them well so that when they were released, they would be supportive and complimentary of the Babylonian Empire.
After Nebuchadnezzar leaves, there is a disconnect between the sources. One says that Jeremiah speaks to God, who tells him that the city will be rebuilt. He then puts Jeremiah to sleep for a hundred years. The Israelites return and begin to rebuild the city and then God wakes Jeremiah from his slumber.
Other sources say that Nebuchadnezzar puts Zedekiah in power. Jeremiah provides support and counsel to Zedekiah for the two years he is in charge because Zedekiah knows that the city is doomed. Jeremiah stays by his side, realizing that it is better to be Babylon’s ally than enemy. After ruling for two years, Zedekiah tried to make an alliance with Egypt, leading to his demise. Nebuchadnezzar puts an end to the alliance and the cities.
Some accounts say that Bahman took over after Nebuchadnezzar conquered Jerusalem, eventually passing off the power to Cyrus. These accounts do not say a lot about Nebuchadnezzar or make him seem as powerful.
Yet another source say that God let Nebuchadnezzar rule as long as he wanted. Near the end of his reign, and life, Nebuchadnezzar has a dream but he cannot remember it when he awakes. He calls Daniel to pray and talk to God to figure out what he had dreamed. He comes back and tells Nebuchadnezzar about his dream: He saw a statue, made up of many different materials. The feet were formed out of clay and the materials got stronger and stronger the higher they were on the body, with the head and neck being made from gold. These different substances symbolized the different reigns of rulers. Then, a rock was sent down from heaven and smashed the statue. This was meant to symbolize God sending a prophet to smash the reign of Nebuchadnezzar. Soon after this, Nebuchadnezzar addressed the Israelites. He talked about how powerful he was, so powerful that he had destroyed God’s house and people and that when he died, he would take over God’s kingdom. God, having heard this, pitied his people. He allowed them to return to Judea and multiply. One of the captives, Ezra, was distraught about the fact that the Israelites’ scripture had been destroyed with the temple. God returns the scripture to him and his people and the Israelites live on under their own leadership.
Sources.
While researchers rely mostly on firsthand accounts to learn about Nebuchadnezzar, that is not the only way they can get information. Much of the biographical information collected by historians about Nebuchadnezzar is taken from inscriptions on buildings that were erected during the rebuilding of Babylon.
Portrayal in Baha'i Faith.
Nebuchadnezzar is mentioned in the Kitáb-i-Íqán, the primary theological work of the Bahá'í Faith. There he is described as having "delivered Jerusalem to the flames."
Nebuchadnezzar in fiction.
Voltaire interprets the legacy of Nebuchadnezzar and his relationship with Amasis in a short story entitled "The White Bull".

</doc>
<doc id="52350" url="http://en.wikipedia.org/wiki?curid=52350" title="Gaspar Corte-Real">
Gaspar Corte-Real

Gaspar Corte-Real (c. 1450 – c. 1501?) was a pre-Columbian trans-oceanic Portuguese explorer.
Early life.
He was the youngest of three sons of João Vaz Corte-Real, also a Portuguese explorer, and had accompanied his father on his expeditions to North America. His brothers were also explorers.
Career.
In 1500, King Manuel I of Portugal sent Gaspar to discover lands and search for a Northwest Passage to Asia.
He reached Greenland, believing it to be east Asia, but chose not to land. He set out on a second voyage to Greenland in 1501, with his brother Miguel Corte-Real and three caravels. Encountering frozen sea, they changed course to the south and reached land, believed to be Labrador and Newfoundland. There they captured 57 native men, who would later be sold as slaves. Gaspar then sent his brother and two ships back to Portugal before continuing southwards. 
Death.
Nothing more was ever heard of Gaspar Corte-Real after 1501. His brother Miguel attempted to find him in 1502, but he too never returned.
Tribute.
The statue of Gaspar Corte-Real ("pictured") is located in front of the Confederation Building in St. John's, Newfoundland. It was donated by the Portuguese Fisheries Organisation in 1965 in recognition of the hospitality of Newfoundlanders towards Portuguese Grand Banks fishermen.
References.
"(required)"

</doc>
<doc id="52351" url="http://en.wikipedia.org/wiki?curid=52351" title="Ogimachi">
Ogimachi

Ogimachi may refer to:

</doc>
<doc id="52354" url="http://en.wikipedia.org/wiki?curid=52354" title="Trinity College">
Trinity College

Trinity College may refer to:
United Kingdom.
Trinity College is generally used as a term to refer to the colleges at one of the English "ancient universities":
It may also refer to:

</doc>
<doc id="52356" url="http://en.wikipedia.org/wiki?curid=52356" title="German Museum of Technology">
German Museum of Technology

Deutsches Technikmuseum Berlin (German Museum of Technology) in Berlin, Germany is a museum of science and technology, and exhibits a large collection of historical technical artifacts. The museum's main emphasis originally was on rail transport, but today it also features exhibits of various sorts of industrial technology. In 2003, it opened both maritime and aviation exhibition halls in a newly-built extension. The museum also contains a science center called Spectrum.
History.
The Museum of Traffic and Technology ("Museum für Verkehr und Technik") was founded in 1982 and assumed the tradition of the Royal Museum of Traffic and Construction ("Königliches Verkehrs- und Baumeseum") which was opened in the former Hamburger Bahnhof station building in 1906. The present-day museum is located on the former freight yard attached to the Anhalter Bahnhof in the Kreuzberg district of Berlin, including two historic roundhouses and several office buildings.
Renamed "Deutsches Technikmuseum" in 1996, the exhibition area was gradually expanded. An adjacent new building complex was inaugurated in 2003, topped by a prominent US Air Force Douglas C-47B "Raisin Bomber", which can be seen with ease from the top of the Fernsehturm and formerly at Tempelhof Airport.
Collections.
Locomotives.
An extensive railway collection opened in 1987/88 in the rebuilt 19th century roundhouses of the Anhalter Bahnhof locomotive depot ("Bahnbetriebswerk") that had laid derelict for about 30 years. The 33 tracks illustrate the history of rail transport, including the deportations of Jews and others by the "Deutsche Reichsbahn" in the Holocaust. The exhibition also features a H0 scale model of the Anhalter Bahnhof track installations.
Locomotives on display include:
Aircraft.
A large aviation section beside the C-47 houses numerous aircraft from the single Jeannin Stahltaube to a "Lufthansa" Junkers Ju 52 and an Arado Ar 79. The museum addresses the flight enthusiasm of the early 20th century and its abuse in the German re-armament building up the "Luftwaffe", documented by a Arado Ar 96, a wrecked Junkers Ju 87 "Stuka" dive bomber and the current restoration of a Focke-Wulf Fw 200 Condor as well as by one of three preserved Messerschmitt Bf 110, a Flak cannon, and a V-1 flying bomb built by Mittelbau-Dora concentration camp inmates at the Mittelwerk site. Post-war aircraft including a VFW-Fokker 614 and the Cessna 172P that Mathias Rust flew to the Moscow Red Square during the Cold War have also been added to the exhibition.
The remains of Lancaster B III "JA914" are displayed. This aircraft served with 57 Squadron as DX-O. It was shot down over Berlin in September 1943 and crashed into a lake opposite Zahrensdorf.
Computers.
On 15 May 2002, a special exhibition opened which featured the inventions of computer pioneer Konrad Zuse, including a replica of the Z1 and several other Zuse computers.
Buildings.
The museum has two windmills (one German, one Dutch), a brewery, and a forge powered by a waterwheel.

</doc>
<doc id="52357" url="http://en.wikipedia.org/wiki?curid=52357" title="Eroticism">
Eroticism

Eroticism (from the Greek ἔρως, "eros"—"desire") is a quality that causes sexual feelings, as well as a philosophical contemplation concerning the aesthetics of sexual desire, sensuality and romantic love. That quality may be found in any form of artwork, including painting, sculpture, photography, drama, film, music or literature. It may also be found in advertising. The term may also refer to a state of sexual arousal or anticipation of such – an insistent sexual impulse, desire, or pattern of thoughts.
As French novelist Honoré de Balzac stated, eroticism is dependent not just upon an individual's sexual morality, but also the culture and time in which an individual resides.
Definitions.
Because the nature of what is erotic is fluid, early definitions of the term attempted to conceive eroticism as some form of sensual or romantic love or as the human sex drive (libido); for example, the "Encyclopédie" of 1755 states that the erotic "is an epithet ork: 1972) critics have often confused eroticism with pornography, going so far as to say: "[Eroticism] is simply high-class pornography; better produced, better conceived, better executed, better packaged, designed for a better class of consumer." 
However, because eroticism is wholly dependent on the viewer's culture and personal tastes pertaining to what, exactly, defines the erotic, This confusion, as Lynn Hunt writes, "demonstrate the difficulty of drawing...a clear generic demarcation between the erotic and the pornographic": indeed arguably "the history of the separation of pornography from eroticism...remains to be written".
Psychoanalytical approach.
For a psychoanalytical definition, as early as Freud psychotherapists have turned to the ancient Greek philosophy's "overturning of mythology" as a definition to understanding of the heightened aesthetic. For Plato, Eros takes an almost transcendent manifestation when the subject seeks to go beyond itself and form a communion with the objectival other: "the true order of going...to the things of love, is to use the beauties of earth as steps...to all fair forms, and from fair forms to fair actions, and from fair actions to fair notions, until from fair notions he arrives at the notion of absolute beauty".
French philosophy.
Modern French conceptions of eroticism can be traced to The Enlightenment, when "in the eighteenth century, dictionaries defined the erotic as that which concerned love...eroticism was the intrusion into the public sphere of something that was at base private". This theme of intrusion or transgression was taken up in the twentieth century by the French philosopher Georges Bataille, who argued that eroticism performs a function of dissolving boundaries between human subjectivity and humanity, a transgression that dissolves the rational world but is always temporary, as well as that, "Desire in eroticism is the desire that triumphs over the taboo. It presupposes man in conflict with himself". For Bataille, as well as many French theorists, "Eroticism, unlike simple sexual activity, is a psychological quest...eroticism is assenting to life even in death".
Non-heterosexual.
Queer theory and LGBT studies consider the concept from a non-heterosexual perspective, viewing psychoanalytical and modernist views of eroticism as both archaic and heterosexist, written primarily by and for a "handful of elite, heterosexual, bourgeois men" who "mistook their own repressed sexual proclivities" as the norm.
Theorists like Eve Kosofsky Sedgwick, Gayle S. Rubin and Marilyn Frye all write extensively about eroticism from a heterosexual, lesbian and separatist point of view, respectively, seeing Eroticism as both a political force and cultural critique for marginalized groups, or as Mario Vargas Llosa summarized: "Eroticism has its own moral justification because it says that pleasure is enough for me; it is a statement of the individual's sovereignty".

</doc>
<doc id="52358" url="http://en.wikipedia.org/wiki?curid=52358" title="Imaginary unit">
Imaginary unit

The term imaginary unit or unit imaginary number refers to a solution to the equation . As a convention, the solution is usually denoted i. Since there is no real number with this property, it extends the real numbers, and under the assumption that the familiar properties of addition and multiplication (namely closure, associativity, commutativity and distributivity) continue to hold for this extension, the complex number system is generated by including it.
Imaginary numbers are an important mathematical concept, which extends the real number system ℝ to the complex number system ℂ, which in turn provides at least one root for every nonconstant polynomial "P"("x") (see Algebraic closure and Fundamental theorem of algebra). The term "imaginary" is used because there is no real number having a negative square.
There are two complex square roots of −1, namely "i" and −"i", just as there are two complex square roots of every real number other than zero, which has one double square root.
In contexts where i is ambiguous or problematic, j or the Greek "ι" is sometimes used (see ). In the disciplines of electrical engineering and control systems engineering, the imaginary unit is often denoted by j instead of i, because i is commonly used to denote electric current.
For the history of the imaginary unit, see .
Definition.
The imaginary number i is defined solely by the property that its square is −1:
With i defined this way, it follows directly from algebra that i and −"i" are both square roots of −1.
Although the construction is called "imaginary", and although the concept of an imaginary number may be intuitively more difficult to grasp than that of a real number, the construction is perfectly valid from a mathematical standpoint. Real number operations can be extended to imaginary and complex numbers by treating i as an unknown quantity while manipulating an expression, and then using the definition to replace any occurrence of "i"2 with −1. Higher integral powers of i can also be replaced with −"i", 1, i, or −1:
Similarly, as with any non-zero real number:
As a complex number, i is equal to 0 + "i", having a unit imaginary component and no real component (i.e., the real component is zero). In polar form, i is 1 cis π/2, having an absolute value (or magnitude) of 1 and an argument (or angle) of π/2. In the complex plane (also known as the Cartesian plane), i is the point located one unit from the origin along the imaginary axis (which is at a right angle to the real axis).
i and −"i".
Being a quadratic polynomial with no multiple root, the defining equation has "two" distinct solutions, which are equally valid and which happen to be additive and multiplicative inverses of each other. More precisely, once a solution i of the equation has been fixed, the value −"i", which is distinct from i, is also a solution. Since the equation is the only definition of i, it appears that the definition is ambiguous (more precisely, not well-defined). However, no ambiguity results as long as one or other of the solutions is chosen and labelled as "i", with the other one then being labelled as −"i". This is because, although −"i" and i are not "quantitatively" equivalent (they "are" negatives of each other), there is no "algebraic" difference between i and −"i". Both imaginary numbers have equal claim to being the number whose square is −1. If all mathematical textbooks and published literature referring to imaginary or complex numbers were rewritten with −"i" replacing every occurrence of +"i" (and therefore every occurrence of −"i" replaced by ), all facts and theorems would continue to be equivalently valid. The distinction between the two roots x of "x"2 + 1 = 0 with one of them labelled with a minus sign is purely a notational relic; neither root can be said to be more primary or fundamental than the other, and neither of them is "positive" or "negative".
The issue can be a subtle one. The most precise explanation is to say that although the complex field, defined as ℝ["x"]/("x"2 + 1), (see complex number) is unique up to isomorphism, it is "not" unique up to a "unique" isomorphism — there are exactly 2 field automorphisms of ℝ["x"]/("x"2 + 1) which keep each real number fixed: the identity and the automorphism sending "x" to −"x". See also Complex conjugate and Galois group.
A similar issue arises if the complex numbers are interpreted as 2 × 2 real matrices (see matrix representation of complex numbers), because then both
are solutions to the matrix equation
In this case, the ambiguity results from the geometric choice of which "direction" around the unit circle is "positive" rotation. A more precise explanation is to say that the automorphism group of the special orthogonal group SO (2, ℝ) has exactly 2 elements — the identity and the automorphism which exchanges "CW" (clockwise) and "CCW" (counter-clockwise) rotations. See orthogonal group.
All these ambiguities can be solved by adopting a more rigorous definition of complex number, and explicitly "choosing" one of the solutions to the equation to be the imaginary unit. For example, the ordered pair (0, 1), in the usual construction of the complex numbers with two-dimensional vectors.
Proper use.
The imaginary unit is sometimes written √−1 in advanced mathematics contexts (as well as in less advanced popular texts). However, great care needs to be taken when manipulating formulas involving radicals. The notation is reserved either for the principal square root function, which is "only" defined for real "x" ≥ 0, or for the principal branch of the complex square root function. Attempting to apply the calculation rules of the principal (real) square root function to manipulate the principal branch of the complex square root function will produce false results:
Attempting to correct the calculation by specifying both the positive and negative roots only produces ambiguous results:
Similarly:
The calculation rules
and
are only valid for real, non-negative values of a and b.
These problems are avoided by writing and manipulating "i"√7, rather than expressions like √−7. For a more thorough discussion, see Square root and Branch point.
Properties.
Square roots.
The square root of i can be expressed as either of two complex numbers
Indeed, squaring the right-hand side gives
This result can also be derived with Euler's formula
by substituting , giving
Taking the square root of both sides gives
which, through application of Euler's formula to , gives
Similarly, the square root of −"i" can be expressed as either of two complex numbers using Euler's formula:
by substituting , giving
Taking the square root of both sides gives
which, through application of Euler's formula to , gives
Multiplying the square root of i by i also gives:
Multiplication and division.
Multiplying a complex number by i gives:
Dividing by i is equivalent to multiplying by the reciprocal of i:
Using this identity to generalize division by i to all complex numbers gives:
Powers.
The powers of i repeat in a cycle expressible with the following pattern, where n is any integer:
This leads to the conclusion that
where "mod" represents the modulo operation. Equivalently:
i raised to the power of i.
Making use of Euler's formula, "i""i" is
where formula_35, the set of integers.
The principal value (for "k" = 0) is "e"−π/2 or approximately 0.207879576...
Factorial.
The factorial of the imaginary unit i is most often given in terms of the gamma function evaluated at 1 + "i":
Also,
Other operations.
Many mathematical operations that can be carried out with real numbers can also be carried out with i, such as exponentiation, roots, logarithms, and trigonometric functions. However, it should be noted that all of the following functions are complex multi-valued functions, and it should be clearly stated which branch of the Riemann surface the function is defined on in practice. Listed below are results for the most commonly chosen branch.
A number raised to the n"i" power is:
The n"i"th root of a number is:
The imaginary-base logarithm of a number is:
As with any complex logarithm, the log base {{mvar|i}} is not uniquely defined.
The cosine of {{mvar|i}} is a real number:
And the sine of {{mvar|i}} is purely imaginary:
Matrices.
When 2 × 2 real matrices "m" are used for a source, and the number one (1) is identified with the identity matrix, and minus one (−1) with the negative of the identity matrix, then there are many solutions to "m"2 = −1. In fact, there are many solutions to "m"2 = +1 and "m"2 = 0 also. Any such m can be taken as a basis vector, along with 1, to form a planar algebra.

</doc>
<doc id="52360" url="http://en.wikipedia.org/wiki?curid=52360" title="Arapaho">
Arapaho

The Arapaho (in French: "Arapahos, Gens de Vache") are a tribe of Native Americans historically living on the plains of Colorado and Wyoming. They were close allies of the Cheyenne tribe and loosely aligned with the Lakota and Dakota. The Arapaho language, "Heenetiit", is an Algonquian language closely related to Gros Ventre (Ahe/A'ananin), whose people are seen as an early offshoot of the Arapaho. Blackfeet and Cheyenne are the other Algonquian-speakers on the Plains, but their languages are quite different from Arapaho.
By the 1850s, Arapaho bands formed two tribes: the Northern Arapaho and Southern Arapaho. Since 1878 the Northern Arapaho have lived with the Eastern Shoshone on the Wind River Reservation in Wyoming and are federally recognized as the Arapahoe Tribe of the Wind River Reservation. The Southern Arapaho live with the Southern Cheyenne in Oklahoma. Together their members are enrolled as the federally recognized Cheyenne and Arapaho Tribes.
Name.
The Arapaho autonym is Hinono'eino or Inun-ina (“our people” or “people of our own kind”), when referring to the tribe they use Hinono'eiteen (Arapaho Nation).
They were also known as "Hitanwo'iv" or "Hetanevoeo/Hetanevo'eo'o" (“People of the Sky” or “Cloud People”) by their Cheyenne allies or "Mahpíyato" (“Blue Cloud Men”) by Dakota, "Maȟpíya tho"́ (“Blue Sky People”) by Lakota and Assiniboine. 
The Caddo ("Toniibeenenno"' or "Toniibeeneseino' " - “pierced nose People”) called them "Detseka'yaa", the Wichita ("Hinosouno' ") "Nia'rhari's-kûrikiwa'ahûski", and the Comanche "Saria Tʉhka / Säretika (Sata Teichas)", all names signifying “dog-eaters”. To Pawnee and Ute and other tribes they were also known as “dog-eaters”.
The Northern Arapaho who called themselves "Nank'haanseine'nan" or "Nookhose'iinenno" ("white sage men") were known as "Baantcline'nan" or "Bo'oociinenno' " ("red willow men") to the Southern Arapaho, whereas the latter were called by their northern kin "Nawathi'neha" or "Noowunenno' " ("Southerners"). The Northern Arapaho were also known as "BSakuune'na' (Bee'eekuunnenno') " ("blood-soup men").
The Cheyenne adapted the Arapaho terms and referred to the Northern Arapaho as "Vanohetan" or "Vanohetaneo/ Váno'étaneo'o" (“Sage (Brush) People”) and to the Southern Arapaho as "Nomsen'nat" or "Nomsen'eo" (“Southerners”).
Historic political and dialect Arapaho divisions and bands.
The Arapaho recognize among themselves five main divisions, each speaking a different dialect and apparently representing as many originally distinct but cognate tribes. Through much of Arapaho history, each tribal-nation maintained a separate ethnic identity, although they occasionally came together and acted as political allies. Each also spoke mutually intelligible dialects, but there existed a degree of difference from
Arapaho proper. Dialectically, the Haa'ninin, Beesowuunenno', and Hinono'eino were
closely related. Arapaho elders claimed that the Hánahawuuena dialect was most
difficult to comprehend of all the dialects. In the classic ethnographic study, Alfred Kroeber identified these five nations from south to north:
Before their historic geo-political ethnogensis, each tribal-nation had a principal
headman. The exact date of the ethnic fusion or fission of each social division is not known, but elders state that the "Hinono'eino" (“Arapaho proper”) and "Beesowuunenno' " (“Big Lodge People” or “Brush-Hut/Shelter People”) fought over the tribal symbols - the sacred pipe and lance. Both sacred objects traditionally were kept by the "Beesowuunenno' ". The different tribal-nations lived together and the "Beesowuunenno' " have dispersed for at least 150 years among the former distinct Arapaho tribal groups. By the
late eighteenth century the four divisions south of the "Haa'ninin" (“White Clay People” or “Lime People”) or "Gros Ventre (Atsina)" consolidated into the Arapaho, so that only the Arapaho and Gros Ventre (Atsina) were recognized as separate tribal-nations.
While living on the Great Plains, the "Hinono'eino" (all Arapaho bands south of the "Haa'ninin") divided 
historically into two geo-political social divisions:
History.
Early history.
The ancestral Arapaho speaking people ("Heeteinono'eino' ") lived in the western Great Lakes region along the Red River Valley in what is present day Manitoba and Minnesota around 3000 years ago. In the Great Lakes region, the Arapaho were an agricultural people who grew crops including maize. Together with the early Cheyenne people ("Hitesiino"') the Arapaho were pushed westward onto the eastern Great Plains by the Ojibwe who were numerous and obtained guns earlier from their French allies. The ancestors of the Arapaho people entered the Great Plains from the western Great Lakes region sometime before 1700. During their early history on the plains the Arapaho lived on the northern plains from the South Saskatchewan River in Canada south to Montana, Wyoming, and western South Dakota. Before the Arapaho acquired horses, they used domestic dogs as pack animals to pull their travois. Horses were first acquired in the early 1700s by the Arapaho and became central to their way of life as a nomadic people. The introduction of the horse to Arapaho society allowed them to pull greater loads, hunt more meat, hunt easier, travel faster, and travel farther. Gradually, the Arapaho moved farther south, split into the closely allied Northern and Southern Arapaho, and established a large joint territory spanning land in southern Montana, most of Wyoming, the Nebraska Panhandle, central and eastern Colorado, western Oklahoma, and extreme western Kansas. A large group of Arapaho split from the main tribe and became their own independent people commonly known as the Gros Ventre or Atsina. The name Gros Ventre meaning “Big Bellies” in French was a misinterpretation of sign language between an Indian guide and French explorers. The Gros Ventre spoke a similar language to the Arapaho after the division and referred to themselves as "A'aninin" meaning ″White Clay people″. The Arapaho often viewed the Gros Ventre as inferior and referred to them as "Hitúnĕna" or "Hitouuteen", meaning "beggars".
Expansion on the plains.
Once established, the Arapaho began to expand their presence on the plains through trade, warfare, and alliances with other plains tribes. Around 1811 the Arapaho made an alliance with the Cheyenne ("Hítesííno’" - ″scarred one″). Their strong alliance with the Cheyenne allowed the Arapaho to greatly expand their hunting territory. By 1826, the Lakota, Dakota, Cheyenne, and Arapaho pushed the Kiowa ("Niiciiheihiinenno"'; Kiowa tribe: "Niiciiheihiiteen") and invading Comanche south. Conflict with the allied Comanche and Kiowa ended in 1840 when the two large tribes made peace with the Arapaho and Southern Cheyenne and became their allies. Chief Little Raven was the most notable Arapaho chief that helped mediate peace among the nomadic southern plains tribes and would retain his reputation as a peace chief throughout the Indian Wars and reservation period. The alliance with the Comanche and Kiowa led to the most southern of the Arapaho bands being able to enter the Llano Estacado in the Texas Panhandle. One band of Southern Arapaho became so closely allied with the Comanche that they were absorbed into the tribe, adopted the Comanche language, and became their own band of Comanche known as the "Saria Tʉhka (Sata Teichas)" or "Dog-Eaters band".
They had active trade relationships with the farming villages of the Arikara, Mandan, and Hidatsa on the Missouri, trading them excess meat and hides for corn squash and beans. The Arikara called them "Colored Stone Village (People)" possibly because gemstones from the Southwest were among the trade items. The Hidatsa called them "E-tah-leh" or "Ita-Iddi" (″Bison-Path People″).
Conflict with Euro-American traders and explorers was light, and the Arapaho freely entered various trading posts and trade fairs to exchange mostly bison hides and beaver furs for European goods such as firearms. The Arapaho frequently came into contact with fur traders in the foothills of the Rocky Mountains, and the headwaters of the Platte and Arkansas and became well-known traders on the plains and bordering Rocky Mountains; the name ″Arapaho″ may have come from the Pawnee word "Tirapihu" (or "Larapihu"), probably applied to the tribe from the fact that they were the trading group in the Great Plains region for "he buys or trades" or "traders". Another source for their tribal name could be, that the white traders referred to them by their Crow (Apsáalooke aliláau) name "Alappaho"' which meant "People with many tattoos". The Arapaho had tattooed small circles on their bodies. The traders' pronunciation of Alappaho' soon led to the widespread use of the name Arapaho.
Enemies and warrior culture.
A large part of Arapaho society was based around the warrior, which most young men strived to, but were not obligated to become. After the introduction of the horse the Arapaho quickly became master horsemen and highly skilled at fighting on horseback. The role of the warrior in Arapaho society was much more than just combat. Warriors were expected to keep peace among the camps, provide food and wealth for their families, and guard the camps from attacks. Like other plains Indians including their Cheyenne allies the Arapaho have a number of distinct military societies. Each of the eight Arapaho military societies had their own unique initiation rites, pre and post battle ceremonies and songs, regalia, and style of combat. Unlike their Cheyenne, Lakota, and Dakota allies, the Arapaho military societies were age based. Each age level had its own society for prestigious or promising warriors of the matching age. As the warriors aged they may graduate to the next society. Warriors often painted their face and bodies with war paint as well as their horses for reasons of empowerment. Each warrior had a unique design for their war paint they often wore into battle. Feathers from birds, particularly eagle feathers, were also worn in to battle as symbols of prestige and for reasons similar to war paint. Before setting out for war the warriors organized themselves into war parties. War parties were made up of individual warriors and a selected war chief. The title of war chief must be earned through a specific number of acts of bravery in battle known as Counting coup. Coups may include stealing horses unseen, touching a living enemy, or stealing a gun from an enemy’s grasp. Arapaho warriors used a variety of weapons including war-clubs, lances, knives, tomahawks, bows, shotguns, rifles, and pistols. Guns were acquired through trade at trading posts or trade fairs in addition to raiding soldiers or other tribes.
The Arapaho fought with the Pawnee ("Hooxeihiinenno' " - "wolf people"), Omaha ("Howohoono' "), Ho-chunk, Osage ("Wosootiinen", "Wosoo3iinen" or "Wosoosiinen"), Ponca (same as Omaha: "Howohoono' "), and Kaw ("Honoho' ") east of their territory. North of Arapaho territory they fought with the Crow ("Houunenno' "), Blackfoot Confederacy ("Woo'teenixteet" or "Woo'teenixtee3i"' - ″people wearing black-feet″), Gros Ventre ("Hitouunenno' ", Gros Ventre tribe: "Hitouuteen"), Flathead ("Kookee'ei3i' "), Arikara ("Koonoonii3i" ' - ″people whose jaws break in pieces″), Iron Confederacy (Nehiyaw-Pwat) (Assiniboine ("Nihooneihteenootineihino' " - "yellow-footed Sioux"), Plains/Woods Cree ("Nooku(h)nenno" '; Plains Cree tribe: "Nookuho' " - "rabbit people"), Saulteaux (Plains Ojibwa) and Nakoda (Stoney)). To the west they fought with eastern Shoshone ("Sosoni'ii"; Shoshone tribe: "Sosoni'iiteen") and the Ute ("Wo'(o)teenehi3i' " - ″cut throats″; Ute tribe: "Wo'(o)teennehhiiteen"). South of their territory they occasionally fought with the Navajo ("Coohoh'oukutoo3i' " - ″those who tie their hair in back of the head or in bunches″), Apache ("Coo3o' " - "enemy" or "Teebe'eisi3i' " - "they have their hair cut straight, hanging straight down", "Ti'iihiinen" - "killdeer people" refers especially to Jicarilla Apache) and various Pueblo peoples ("Cooh'ookutoo3i' " - "they tie their hair in a bundle"). The Cheyenne ("Hitesiino' "), Sioux ("Nootineihino' "), Kiowa ("Niiciiheihiinenno' " - ″river people″ or "Koh’ówuunénno’" - ″creek people″; Kiowa tribe: "Niiciiheihiiteen" or "Koh’ówuunteen"), Plains Apache ("3oxooheinen" - "pounder people"), and Comanche ("Coo3o' " - sg. and pl., means: "enemy", like Apache) were enemies of the Arapaho initially but became their allies. Together with their allies the Arapaho also fought with invading US soldiers, miners, and settlers across Arapaho territory and the territory of their allies.
Sand Creek Massacre.
In November 1864, a small village of Cheyenne and Arapaho became victims of the Sand Creek massacre, an attack by the Colorado militia, led by Colonel John Chivington. According to an historical narrative on the event titled "Chief Left Hand", by Margaret Coel, contributing factors that led to the massacre were: Governor Evans' desire to hold title to the resource rich Denver-Boulder area; government trust officials' avoidance of Chief Left Hand (a linguistically gifted Southern Arapaho chief), when executing a legal treaty that transferred title of the area away from Indian Trust; a local cavalry stretched thin by the demands of the Civil War; the hijacking of their supplies by a few stray Indian warriors who had lost respect for their chiefs and followers of Chief Left Hand (including a group of Cheyenne and Arapaho elders, a few well behaved warriors, and mostly women and children), who had received a message to report to Fort Lyon with the promise of safety and food at the Fort, or risk being considered "hostile" and ordered killed by the cavalry. (The tribe had been deprived of their normal wintering grounds in the Boulder area.)
Upon arrival at Fort Lyon, Chief Left Hand and his followers were accused of violence by Colonel Chivington. Chief Left Hand and his people got the message that only those Indians that reported to Fort Lyon would be considered peaceful and all others would be considered hostile and ordered killed. Confused, Chief Left Hand and his followers turned away and traveled a safe distance away from the Fort to camp. A traitor gave Colonel Chivington directions to the camp. He and his battalion stalked and attacked the camp early the next morning. Rather than heroic, Colonel Chivington's efforts were considered a gross embarrassment to the Cavalry since he attacked peaceful elders, women and children. As a result of his war efforts, instead of receiving the promotion to which he aspired, he was relieved of his duties.
Eugene Ridgely, a Cheyenne–Northern Arapaho artist, is credited with bringing to light the fact that Arapahos were among the victims of the massacre. His children, Gail Ridgely, Benjamin Ridgley and, Eugene "Snowball" Ridgely, were instrumental in having the massacre site designated as a National Historic Site. In 1999, Benjamin and Gail Ridgley organized a group of Northern Arapaho runners to run from Limon, Colorado, to Ethete, Wyoming, in memory of their ancestors who were forced to run for their lives after being attacked and pursued by Colonel Chivington and his battalion. Their efforts will be recognized and remembered by the "Sand Creek Massacre" signs that appear along the roadways from Limon to Casper, Wyoming, and then to Ethete.
Indian Wars on the Southern Plains.
The events at Sand Creek sparked outrage among the Arapaho and Cheyenne resulting in three decades of war between them and the United States. Much of the hostilities took place in Colorado leading to many of the events being referred to as part of the so-called Colorado War. Battles and hostilities elsewhere on the southern plains such as in Kansas and Texas are often included as part of the “Comanche Wars”. During the wars the Arapaho and Cheyenne allies the Kiowa, Comanche, and Plains Apache would participate in some battles alongside them. The Lakota from the north came downwards into northern Colorado to help the Arapaho and Cheyenne there. The Battle of Julesburg resulted from a force of about 1000 allied Northern Arapaho, Cheyenne (mostly from the Dog Soldiers warrior society), and Lakota from the Brulé and Oglala sub-tribes. The point of the raid was retaliation for the events at the Sand Creek Massacre months earlier. The allied Indian forces attacked settlers and US Army forces around the valley of the South Platte River near Julesburg, Colorado. The battle was a decisive Indian victory resulting in 14 soldiers and 4 civilians dead and probably no Indian casualties. A force of around 3000 Southern Arapaho, Northern Cheyenne, and Lakota attacked soldiers and civilians at a bridge crossing the North Platte River resulting in the Battle of Platte Bridge. The battle was another victory for the Indians resulting in 29 soldiers killed with at least 8 Indian casualties. Arapaho, Cheyenne, Comanche, Kiowa, and Plains Apaches seeking peace were offered to sign the Medicine Lodge Treaty in October 1867. The treaty allotted the Southern Arapaho a reservation with the Southern Cheyenne between the Arkansas and Cimarron rivers in Indian Territory (present-day Oklahoma). Among those that signed the treaty was Chief Little Raven. Those that did not sign the treaty were called “hostile” and were continually pursued by the US Army and their Indian scouts. The last major battle between the Arapaho and the US on the southern plains was the Battle of Summit Springs in northernmost Colorado. The battle involved a force of around 450 Arapaho, Cheyenne, and Lakota warriors and 244 US soldiers and around 50 Pawnee scouts under Frank North. The most prominent Indian leader at the battle was Tall Bull, a leader of the Dog Soldiers warrior society of the Cheyenne. The battle was a US victory resulting in around 35 warriors killed (including Tall Bull) and a further 17 captured. The soldiers suffered only a single casualty. The death of Tall Bull was a major loss for the Dog Soldiers.
Powder River Expedition.
After the Sand Creek Massacre and a number of other skirmishes the Northern Arapaho, Cheyenne, and Lakota moved many of their bands to the remote Powder River country in Wyoming and southern Montana. Along the way they participated in the Battle of Mud Springs, a minor incident in the Nebraska Panhandle involving a force of between 500 and 1,000 Arapaho, Cheyenne, and Lakota warriors and 230 US soldiers. The battle resulted the capturing of some army horses and a herd of several hundred cattle with a single US casualty. An attempt was made by the army to recapture their stolen livestock and attack the Indians which resulted in the Battle of Rush Creek. The battle was inconclusive, resulting in only one Indian casualty and three US soldiers dead (with a further eight wounded). Lt. Col. William O. Collins, commander of the army forces stated that pursuing the Indian forces any further through the dry Sand Hills area would be "injudicious and useless." Once in the area of the Powder River the Arapaho noticed an increase in travelers moving along the established Bozeman trail, which led to the Montana gold fields. Settlers and miners traveling on the Bozeman Trail through the Powder River country were viewed as threats by the Indians as they were numerous and were often violent towards encountered Indians and competed for food along the trail.
Hostilities in the Powder River area led to Major General Grenville M. Dodge to order the Powder River Expedition as a punitive campaign against the Arapaho, Lakota, and Cheyenne. The expedition was inconclusive with neither side gaining a definitive victory. The allied Indian forces mostly evaded the soldiers except for raids on their supplies which left most soldiers desperately under equipped. The most significant battle was the Battle of the Tongue River were Brigadier General Patrick Edward Connor ordered Frank North and his Pawnee Scouts to find a camp of Arapaho Indians under the leadership of Chief Black Bear. Once located Connor sent in 200 soldiers with two howitzers and 40 Omaha and Winnebago and 30 Pawnee scouts, and marched that night toward the village. Indian warriors acting as scouts for the US Army came from the Pawnee, Omaha, and Winnebago tribes who were traditional enemies of the Arapaho and their Cheyenne and Lakota allies. With mountain man Jim Bridger leading the forces they charged the camp. Most of the Arapaho warriors were gone on a raid against the Crow and the battle was a US victory resulting in 63 Arapaho dead, mostly women and children. The few warriors present at the camp put up a strong defense and covered the women and children as most escaped beyond the reach of the soldiers and Indian scouts. After the battle the soldiers burned and looted the abandoned tipis. Connor singled out four Winnebago, including chief Little Priest, plus North and 15 Pawnee for bravery. The Pawnee made off with 500 horses from the camps herd as payback for previous raids by the Arapaho. The Arapaho were not intimidated by the attack and launched a counterattack resulting in the Sawyers Fight where Arapaho warriors attacked a group of surveyors resulting in three dead and no Arapaho loses.
Red Cloud’s War.
Red Cloud’s War was a war fought between soldiers of the United States and the allied Lakota, Northern Cheyenne, and Northern Arapaho between 1866 and 1868. The war was named after the prominent Oglala Lakota chief Red Cloud who led many followers into battle with the invading soldiers. The war was a response to the large number of miners and settlers passing through the Bozeman Trail, which was the fastest and easiest trail from Fort Laramie to the Montana gold fields. The Bozeman Trail passed right through the Powder River Country which was near the center of Arapaho, Cheyenne, Lakota, and Dakota territory in Wyoming and southern Montana. The large number of miners and settlers competed directly with the Indians for resources such as food along the trail.
The most significant battle during Red Cloud’s War was the Fetterman Fight also known as Battle of The Hundred in the Hand to the Indian forces fought on December 21, 1866. The Battle involved Capt. William J. Fetterman who led a force of 79 soldiers and two civilians after a group of ten Indian decoys planning on luring Fetterman's forces into an ambush. The ten decoys consisted of two Arapaho, two Cheyenne, and six Lakota. Fetterman was well known for his boastful nature and his inexperience fighting Indian warriors and despite orders to not pursue the decoys did so anyways. Famous Mountain Man and guide to the soldiers stationed at Fort Laramie Jim Bridger commented on how the soldiers “don't know anything about fighting Indians". After about a half mile pursuit the decoys signaled the hidden warriors to ambush Fetterman and his forces. Warriors from both sides of the trail charged Fetterman and forced them into nearby rocks where the battle soon became hand-to-hand combat giving the Indians the upper hand due to their skill in fighting with hand held weapons such as tomahawks and war-clubs. The Indian forces killed all of Fetterman’s infantry as well as the following cavalry with a total of 81 killed. The battle was the greatest military defeat by the US on the Great Plains until the Battle of the Little Bighorn ten years later. Red Cloud’s War ended in a victory for the Arapaho, Cheyenne, Lakota, and Dakota. The Treaty of Fort Laramie guarantied legal control of the Powder River country to the Indians.
Great Sioux War of 1876–77.
The Great Sioux War of 1876-77 also known as the Black Hills War or Great Cheyenne War was a major conflict that was fought between the Lakota Sioux, Cheyenne, and Arapaho alliance and the US Army. The war was started after miners and settlers traveled into the Black Hills area and found gold, resulting in increased numbers of Non-Indians illegally entering designated Indian lands. A large part of Cheyenne and Arapaho territory and most of Sioux territory known as the Great Sioux Reservation was guaranteed legally to the tribes by the Treaty of Fort Laramie after they defeated the US during Red Cloud's War in 1868. The Black Hills in particular are viewed as sacred to the Lakota and Dakota peoples, and the presence of settlers illegally occupying the area caused great unrest within the tribes. Instead of evicting the settlers the US Army broke the treaty and invaded Sioux, Cheyenne, and Arapaho territory in order to protect American settlers and put the allied tribes on smaller reservations or wipe them out.
After Red Cloud’s War many Northern Arapaho moved to the Red Cloud Agency in Dakota Territory and lived among the Lakota as well as many Cheyenne. Among the most influential and respected Arapaho chiefs living on the Agency was Chief Black Coal who gained prominence as a warrior and leader against white settlers in the Powder River country. Other important Arapaho chiefs living in the area included Medicine Man, Black Bear, Sorrel Horse, Little Shield, Sharp Nose, Little Wolf, Plenty Bear, and Friday. The Arapaho chief “Friday” (unlikely to be his real name) was well regarded for his intelligence and served as an interpreter between the tribe and the Americans. Black Coal guaranteed to the Americans that he and his people would remain peaceful during the tense times when the settlers were illegally entering Indian land in hopes of securing recognized territory of their own in Wyoming. Many of the warriors and families that disagreed with Black Coal’s ideals drifted southward to join up with the southern division of Arapahos. Many Arapaho, particularly those in Chief Medicine Man’s band did not wish to reside among the Sioux “for fear of mixing themselves up with other tribes”. Their peaceful stance and willingness to help American soldiers strained once strong relations between them and the Lakota and Cheyenne which took an aggressive stance and fled the reservation. Attitudes towards the Arapaho from the “hostile” Lakota and Cheyenne were similar to the attitudes they had towards members of their own tribes which took similar peaceful stances and remained as “reservation Indians”. Despite their unwillingness to take up the war path the Arapaho were unwilling to cede their territory, particularly the Black Hills area which they have a strong spiritual attachment to similar to the Lakota.
You have come here to speak with us about the Black Hills, and, without discussing anything that we say, and without changing anything that we say, we wish to tell the Great father [President of the United States] when you get back that this is the country in which we were brought up, and it has also been given to us by treaty by the great father. And I am here to take care of the country, and therefore, not only the Dakota [Sioux] Indians, but my people have an interest in the Black Hills that we have come to speak about today.—Black Coal
During this time of great unrest the tribe found itself deteriorating in leadership with many chiefs holding little sway among their bands. In order to regain strength as leaders and further negotiations for land in Wyoming many chiefs and their warriors enlisted as army scouts for the United States and campaigned against their allies. Another chief considered as influential and equal to Black Coal named Chief Sharp Nose was noted as “the inspiration of the battlefield…He handled men with rare judgment and coolness, and was as modest as he was brave”. Despite their overall stance as allies for the Americans some Arapaho warriors fought against the United States in key battles during the war.
Like in previous wars the US recruited Indian warriors from tribes that were enemies with the Arapaho-Cheyenne-Lakota-Dakota alliance to act as Indian scouts, most notably from the Crow, Arikara, and Shoshone. Unlike previous conflicts involving the Lakota-Dakota-Cheyenne-Arapaho alliance and the United States the Great Sioux War ended in a victory for the United States. The bison herds which were the center of life for the Indians were considerably smaller due to government supported whole-scale slaughter in order to prevent collisions with railroads, conflict with ranch cattle, and to force nomadic plains Indians to adopt reservation life living off government handouts. Decreased resources and starvation was the major reason for the surrendering of individual Indian bands and the end of the Great Sioux War.
The most significant battle of the war was the Battle of The Little Bighorn on June 25–26, 1876. The battle was fought between warriors from the Lakota, Cheyenne, and Arapaho (as well as individual Dakota warrior) and the 7th Cavalry Regiment of the US Army. The battle was fought along the Little Bighorn River in eastern Montana. The soldiers attempted to ambush the large camp of Indians along the river bottom despite the warnings from the Crow Scouts who knew that Custer severely underestimated the number of warriors in the camp. The US Seventh Cavalry, including the Custer Battalion, a force of 700 men led by George Armstrong Custer, suffered a severe defeat. Five of the Seventh Cavalry's companies were annihilated. The total US casualty count, including scouts, was 268 dead including Custer and 55 injured. Only five Arapaho were present at the battle and their presence was by chance. The Arapaho present were four Northern Arapaho warriors named Yellow Eagle, Yellow Fly, Left Hand, and Water Man. The fifth Arapaho was a Southern Arapaho named Well-Knowing One (Sage) but also known as Green Grass. The five Arapaho set out as a war party from near Fort Robinson to raid the Shoshone but by chance came across a small party of young Sioux warriors. The Sioux thought the Arapaho were United States Army Indian Scouts and invited them back to their camp along the Little Bighorn River where they were captured and had their guns taken from them. The Lakota and Dakota threatened to kill the Arapaho but Cheyenne chief Two Moons recognized the men as Arapaho and ordered their release. The next day was the battle and, despite being viewed with suspicion, the five Arapaho actively fought in the battle. Water Man wore a large eagle feather headdress, a white shirt, beaded leggings, a breech cloth, and painted his face red and yellow during the battle. Water Man claimed killing one soldier while charging up the steep river banks but did not take his scalp because most Arapaho refused to take a scalp from someone with short hair. Water Man claimed to have watched Custer die.
When I reached the top of the hill I saw Custer. He was dressed in buckskin, coat and pants, and was on his hands and knees. He had been shot through the side and there was blood coming from his mouth. He seemed to be watching the Indians moving around him. Four soldiers were sitting up around him, but they were all badly wounded. All the other soldiers were down. Then the Indians closed in around him, and I did not see any more. Most of the dead soldiers had been killed by arrows, as they had arrows sticking in them. The next time I saw Custer he was dead, and some Indians were taking his buckskin clothes.—-Water Man
The Arapaho warrior Left Hand accidentally killed a Lakota warrior that he mistook for an Arikara scout, and despite further anger from the Lakota left the battle alive along with the other four Arapaho. After the battle the Arapaho quietly slipped away and headed back to the Fort Robinson area.
Economic development.
In July 2005, Northern Arapahos won a contentious court battle with the State of Wyoming to get into the gaming or casino industry. The 10th Circuit Court ruled that the State of Wyoming was acting in bad faith when it would not negotiate with the Arapahos for gaming. Presently, the Arapaho Tribe owns and operates high-stakes, Class III gaming at the Wind River Casino, Little Wind Casino and 789 Smoke Shop and Casino. They are regulated by a Gaming Commission composed of three tribal members. The Northern Arapaho Tribe opened the first casinos in Wyoming.
Meanwhile, the Cheyenne and Arapaho Tribes operate three casinos: the Lucky Star Casino in Clinton, the Feather Warrior Casino in Watonga, and the Feather Warrior Casino in Canton.

</doc>
<doc id="52369" url="http://en.wikipedia.org/wiki?curid=52369" title="Lincoln Memorial">
Lincoln Memorial

The Lincoln Memorial is an American national monument built to honor the 16th President of the United States, Abraham Lincoln. It is located on the western end of the National Mall in Washington, D.C., across from the Washington Monument. The architect was Henry Bacon; the designer of the primary statue – "Abraham Lincoln", 1920 – was Daniel Chester French; the Lincoln statue was carved by the Piccirilli Brothers; and the painter of the interior murals was Jules Guerin. Dedicated in 1922, it is one of several monuments built to honor an American president. It has always been a major tourist attraction and since the 1930s has been a symbolic center focused on race relations.
The building is in the form of a Greek Doric temple and contains a large seated sculpture of Abraham Lincoln and inscriptions of two well-known speeches by Lincoln, "The Gettysburg Address" and his "Second Inaugural Address". The memorial has been the site of many famous speeches, including Martin Luther King's "I Have a Dream" speech, delivered on August 28, 1963, during the rally at the end of the March on Washington for Jobs and Freedom.
Like other monuments on the National Mall – including the nearby Vietnam Veterans Memorial, Korean War Veterans Memorial, and National World War II Memorial – the memorial is administered by the National Park Service under its National Mall and Memorial Parks group. It has been listed on the National Register of Historic Places since October 15, 1966. It is open to the public 24 hours a day. In 2007, it was ranked seventh on the "List of America's Favorite Architecture" by the American Institute of Architects. Approximately 6 million people visit the memorial annually.
History.
The first public memorial to Abraham Lincoln in Washington, D.C., was a statue by Lot Flannery erected in front of the District of Columbia City Hall in 1868, three years after Lincoln's assassination. Demands for a fitting national memorial had been voiced since the time of Lincoln's death. In 1867, Congress passed the first of many bills incorporating a commission to erect a monument for the sixteenth president. An American sculptor, Clark Mills, was chosen to design the monument. His plans reflected the nationalistic spirit of the time, and called for a 70 ft structure adorned with six equestrian and 31 pedestrian statues of colossal proportions, crowned by a 12 ft statue of Abraham Lincoln. Subscriptions for the project were insufficient.
The matter lay dormant until the start of the 20th century, when, under the leadership of Senator Shelby M. Cullom of Illinois, six separate bills were introduced in Congress for the incorporation of a new memorial commission. The first five bills, proposed in the years 1901, 1902, and 1908, met with defeat because of opposition from Speaker Joe Cannon. The sixth bill (Senate Bill 9449), introduced on December 13, 1910, passed. The Lincoln Memorial Commission had its first meeting the following year and U.S. President William H. Taft was chosen as the commission's president. Progress continued at a steady pace and by 1913 Congress had approved of the Commission's choice of design and location.
There were questions regarding the commission's plan. Many thought that architect Henry Bacon's Greek temple design was far too ostentatious for a man of Lincoln's humble character. Instead they proposed a simple log cabin shrine. The site too did not go unopposed. The recently reclaimed land in West Potomac Park was seen by many to be either too swampy or too inaccessible. Other sites, such as Union Station, were put forth. The Commission stood firm in its recommendation, feeling that the Potomac Park location, situated on the Washington Monument-Capitol axis, overlooking the Potomac River and surrounded by open land, was ideal. Furthermore, the Potomac Park site had already been designated in the McMillan Plan of 1901 to be the location of a future monument comparable to that of the Washington Monument.
With Congressional approval and a $300,000 allocation, the project got underway. On February 12, 1914, a dedication ceremony was conducted and the following month the actual construction began. Work progressed steadily according to schedule. Some changes were made to the plan. The statue of Lincoln, originally designed to be 10 ft tall, was enlarged to 19 ft to prevent it from being overwhelmed by the huge chamber. As late as 1920, the decision was made to substitute an open portal for the bronze and glass grille which was to have guarded the entrance. Despite these changes, the Memorial was finished on schedule. Commission president William H. Taft – who was then Chief Justice of the United States – dedicated the Memorial on May 30, 1922 and presented it to President Warren G. Harding, who accepted it on behalf of the American people. Lincoln's only surviving son, 78-year-old Robert Todd Lincoln, was in attendance.
The Memorial was listed on the National Register of Historic Places on October 15, 1966.
Vandalism.
In September 1962, vandals painted the words "nigger lover" in foot-high pink letters on the rear wall.
On the morning of July 26, 2013, the memorial was shut down after the statue's base and legs were splashed with green paint. It reopened later that day. A 58-year-old Chinese national, Jiamei Tian, was later found responsible for the vandalism. Following her arrest at the Washington National Cathedral, she was admitted to St. Elizabeths Hospital, a psychiatric facility, and was later found to be incompetent to stand trial.
Exterior.
The exterior of the Memorial echoes a classic Greek temple and features Yule marble from Colorado. The structure measures 189.7 by and is 99 ft tall. It is surrounded by a peristyle of 36 fluted Doric columns, one for each of the 36 states in the Union at the time of Lincoln's death, and two columns in-antis at the entrance behind the colonnade. The columns stand 44 ft tall with a base diameter of 7.5 ft. Each column is built from 12 drums including the capital. The columns, like the exterior walls and facades, are inclined slightly toward the building's interior. This is to compensate for perspective distortions which would otherwise make the memorial appear to bulge out at the top when compared with the bottom, a common feature of Ancient Greek architecture.
Above the colonnade, inscribed on the frieze, are the names of the 36 states in the Union at the time of Lincoln's death and the dates in which they entered the Union. Their names are separated by double wreath medallions in bas-relief. The cornice is composed of a carved scroll regularly interspersed with projecting lions' heads and ornamented with palmetto cresting along the upper edge. Above this on the attic frieze are inscribed the names of the 48 states present at the time of the Memorial's dedication. A bit higher is a garland joined by ribbons and palm leaves, supported by the wings of eagles. All ornamentation on the friezes and cornices was done by Ernest C. Bairstow.
The Memorial is anchored in a concrete foundation, 44 to in depth, constructed by M. F. Comer and Company and the National Foundation and Engineering Company, and is encompassed by a 187 by rectangular granite retaining wall measuring 14 ft in height.
Leading up to the shrine on the east side are the main steps. Beginning at the edge of the Reflecting Pool, the steps rise to the Lincoln Memorial Circle roadway surrounding the edifice, then to the main portal, intermittently spaced with a series of platforms. Flanking the steps as they approach the entrance are two buttresses each crowned with an 11 ft tall tripod carved from pink Tennessee marble by the Piccirilli Brothers.
Interior.
The area where the statue stands is 60 feet wide, 74 feet long, and 60 feet high. The interior of the Memorial is divided into three chambers by two rows of Ionic columns. These columns, four in each row, are 50 ft tall and 5.5 ft in diameter at their base. The north and south side chambers contain carved inscriptions of Lincoln's second inaugural address and his Gettysburg Address. Bordering these inscriptions are pilasters ornamented with fasces, eagles, and wreaths. The inscriptions and adjoining ornamentation were done by Evelyn Beatrice Longman.
The Memorial is filled with symbolism: the 36 columns represent the states in the union at the time of Lincoln's death, the 48 stone festoons on the attic above the columns represent the 48 states in 1922. Above each of the inscriptions is a 60 by mural painted by Jules Guerin graphically portraying governing principles evident in Lincoln's life. On the south wall mural, Freedom, Liberty, Immortality, Justice, and the Law are pictured, while the north wall portrays Unity, Fraternity, and Charity. Both scenes contain a background of cypress trees, the emblem of Eternity. The murals were crafted with a special mixture of paint which included elements of kerosene and wax to protect the exposed artwork from fluctuations in temperature and moisture conditions.
The ceiling of the Memorial, 60 ft above the floor, is composed of bronze girders, ornamented with laurel and oak leaves. Between the girders are panels of Alabama marble, saturated with paraffin to increase their translucency. Despite the increased light from this device, Bacon and French felt the statue required even more light. They decided upon an artificial lighting system in which a louvered lighting panel would be set in the ceiling with metal slats to conceal the great floodlights. Custodians could adjust the lights from a control room, varying them according to the outside light. Funds for this expensive system were appropriated by Congress in 1926, and in 1929, seven years after the dedication, the statue was properly lighted. Since that time, only one major alteration has taken place in the Memorial's design. This was the addition of an elevator within the structure to aid handicapped visitors, which was installed in the mid-1970s.
Statue.
Lying between the north and south chambers is the central hall containing the solitary figure of Lincoln sitting in contemplation. The statue was carved by the Piccirilli Brothers under the supervision of the sculptor, Daniel Chester French, and took four years to complete. The statue, originally intended to be only 10 ft tall, was, on further consideration, enlarged so that it finally stood 19 ft tall from head to foot, the scale being such that if Lincoln were standing, he would be 28 ft tall. The extreme width of the statue is the same as its height. The Georgia white marble sculpture weighs 175 ST and had to be shipped in 28 separate pieces.
The statue rests upon an oblong pedestal of Tennessee marble 10 ft high, 16 ft wide, and 17 ft deep. Directly beneath this lies a platform of Tennessee marble about 34.5 ft long, 28 ft wide, and 6.5 in high. Lincoln's arms rest on representations of Roman fasces, a subtle touch that associates the statue with the Augustan (and imperial) theme (obelisk and funerary monuments) of the Washington Mall. The statue is discretely bordered by two pilasters, one on each side. Between these pilasters and above Lincoln's head stands the engraved epitaph, composed by Royal Cortissoz, shown in the box to the left.
Sculptural features.
The sculpture has been at the center of two urban legends. Some have claimed that the face of General Robert E. Lee was carved onto the back of Lincoln's head, and looks back across the Potomac toward his former home, Arlington House, now within the bounds of Arlington National Cemetery. Another popular legend is that Lincoln is shown using sign language to represent his initials, with his left hand shaped to form an "A" and his right hand to form an "L", the president's initials. The National Park Service denies both legends.
However, historian Gerald Prokopowicz writes that, while it is not clear that sculptor Daniel Chester French intended Lincoln's hands to be formed into sign language versions of his initials, it is possible that French did intend it, because he was familiar with American Sign Language, and he would have had a reason to do so, that is, to pay tribute to Lincoln for having signed the federal legislation giving Gallaudet University, a university for the deaf, the authority to grant college degrees. The National Geographic Society's publication, "Pinpointing the Past in Washington, D.C." states that Daniel Chester French had a son who was deaf and that the sculptor was familiar with sign language. Historian James A. Percoco has observed that, although there are no extant documents showing that French had Lincoln's hands carved to represent the letters "A" and "L" in American Sign Language, "I think you can conclude that it's reasonable to have that kind of summation about the hands."
Sacred space.
As Sandage (1993) demonstrates, the Memorial has become a symbolically sacred venue especially for the Civil Rights movement. In 1939, the Daughters of the American Revolution refused to allow the African-American contralto Marian Anderson to perform before an integrated audience at the organization's Constitution Hall. At the suggestion of Eleanor Roosevelt, the wife of President Franklin D. Roosevelt, Harold L. Ickes, the Secretary of the Interior, arranged for a performance on the steps of the Lincoln Memorial on Easter Sunday of that year, to a live audience of 70,000, and a nationwide radio audience.
On August 28, 1963, the memorial grounds were the site of the March on Washington for Jobs and Freedom, which proved to be a high point of the American Civil Rights Movement. It is estimated that approximately 250,000 people came to the event, where they heard Martin Luther King, Jr., deliver his historic speech, "I Have a Dream", before the memorial honoring the president who had issued the Emancipation Proclamation 100 years earlier. King's speech, with its language of patriotism and its evocation of Lincoln's Gettysburg Address, was meant to match the symbolism of the Lincoln Memorial as a monument to national unity. The D.C. police also appreciated the location because it was surrounded on three sides by water, so that any incident could be easily contained. Twenty years later, on August 28, 1983, crowds gathered again to mark the 20th Anniversary Mobilization for Jobs, Peace and Freedom, to reflect on progress in gaining civil rights for African Americans and to commit to correcting continuing injustices. The "I Have a Dream" speech is such a part of the Lincoln Memorial story, that the spot on which King stood, on the landing eighteen steps below Lincoln's statue, was engraved in 2003 in recognition of the 40th anniversary of the event.
At the memorial on May 9, 1970, President Richard Nixon had a middle-of-the-night impromptu, brief meeting with protesters who, just days after the Kent State shootings, were preparing to march against the Vietnam War.
Depictions on U.S. currency.
From 1959 to 2008, the Lincoln Memorial was shown on the reverse of the United States one cent coin, which bears Lincoln's portrait bust on the front. The statue of Lincoln can be seen in the monument. This was done to mark the 150th anniversary of Lincoln's birth.
The memorial also appears on the back of the U.S. five dollar bill, the front of which bears Lincoln's portrait.
In popular culture.
Literature
Films
Television
Video games
Music videos
References.
Notes
Bibliography

</doc>
<doc id="52371" url="http://en.wikipedia.org/wiki?curid=52371" title="Titanic (1997 film)">
Titanic (1997 film)

Titanic is a 1997 American epic romantic disaster film directed, written, co-produced, and co-edited by James Cameron. A fictionalized account of the sinking of the RMS "Titanic", it stars Leonardo DiCaprio and Kate Winslet as members of different social classes who fall in love aboard the ship during its ill-fated maiden voyage.
Cameron's inspiration for the film came from his fascination with shipwrecks; he felt a love story interspersed with the human loss would be essential to convey the emotional message of the disaster. Production began in 1995, when Cameron shot footage of the actual "Titanic" wreck. The modern scenes on the research vessel" "were shot on board the "Akademik Mstislav Keldysh", which Cameron had used as a base when filming the wreck. Scale models, computer-generated imagery, and a reconstruction of the "Titanic" built at Playas de Rosarito in Baja California were used to re-create the sinking. The film was partially funded by Paramount Pictures and 20th Century Fox; at the time, it was the most expensive film ever made, with an estimated budget of $200 million.
Upon its release on December 19, 1997, "Titanic" achieved critical and commercial success. Nominated for fourteen Academy Awards, it tied "All About Eve" (1950) for the most Oscar nominations, and won eleven, including the awards for Best Picture and Best Director, tying "Ben Hur" (1959) for the most Oscars won by a single film. With an initial worldwide gross of over $1.84 billion, "Titanic" was the first film to reach the billion-dollar mark. It remained the highest-grossing film of all time until Cameron's 2009 film "Avatar" surpassed it in 2010. A 3D version of "Titanic," released on April 4, 2012 to commemorate the centennial of the sinking, earned it an additional $343.6 million worldwide, pushing the film's worldwide total to $2.18 billion. It became the second film to gross more than $2 billion worldwide (after "Avatar").
Plot.
In 1996, treasure hunter Brock Lovett and his team aboard the research vessel Akademik Mstislav Keldysh search the wreck of RMS "Titanic" for a necklace with a rare diamond, the Heart of the Ocean. They recover a safe containing a drawing of a young woman wearing only the necklace dated April 14, 1912, the day the ship struck the iceberg. Rose Dawson Calvert, the woman in the drawing, is brought aboard "Keldysh" and tells Lovett of her experiences aboard "Titanic".
In 1912 Southampton, 17-year-old first-class passenger Rose DeWitt Bukater, her fiancé Cal Hockley, and her mother Ruth board "Titanic". Ruth emphasizes that Rose's marriage will resolve the DeWitt Bukaters' financial problems. Distraught over the engagement, Rose considers jumping from the stern; Jack Dawson, a penniless artist, convinces her not to. Discovered with Jack, Rose tells a concerned Cal that she was peering over the edge and Jack saved her from falling. When Cal becomes indifferent, she suggests to him that Jack deserves a reward. He invites Jack to dine with them in first class the following night. Jack and Rose develop a tentative friendship, though Cal and Ruth are wary of him. Following dinner, Rose secretly joins Jack at a party in third class.
Aware of Cal and Ruth's disapproval, Rose rebuffs Jack's advances, but realizes she prefers him over Cal. After rendezvousing on the bow at sunset, Rose takes Jack to her state room; at her request, Jack sketches Rose posing nude wearing Cal's engagement present, the Heart of the Ocean necklace. They evade Cal's bodyguard and have sex in an automobile inside the cargo hold. On the forward deck, they witness a collision with an iceberg and overhear the officers and designer discussing its seriousness.
Cal discovers Jack's sketch of Rose and a mocking note from her in his safe along with the necklace. When Jack and Rose attempt to tell Cal of the collision, he has his butler slip the necklace into Jack's pocket and accuses him of theft. Jack is arrested, taken to the Master-at-arms' office, and handcuffed to a pipe. Cal puts the necklace in his own coat pocket.
With the ship sinking, Rose flees Cal and her mother, who has boarded a lifeboat, and rescues Jack. On the boat deck, Cal and Jack encourage her to board a lifeboat; Cal claims he can get himself and Jack off safely. After Rose boards one, Cal tells Jack the arrangement is only for himself. As her boat lowers, Rose decides that she cannot leave Jack and jumps back on board. Cal takes his bodyguard's pistol and chases Rose and Jack into the flooding first class dining saloon. After using up his ammunition, Cal realizes he gave his coat and consequently the necklace to Rose. He later boards a collapsible lifeboat by carrying a lost child.
After braving several obstacles, Jack and Rose return to the boat deck. The lifeboats have departed and passengers are falling to their deaths as the stern rises out of the water. The ship breaks in half, lifting the stern into the air. Jack and Rose ride it into the ocean and he helps her onto a wooden panel only buoyant enough for one person. He assures her that she will die an old woman, warm in her bed. He dies of hypothermia but she is saved.
With Rose hiding from Cal en route, the RMS "Carpathia" takes the survivors to New York City where Rose gives her name as Rose Dawson. She later learns that Cal committed suicide after losing everything in the 1929 Wall Street Crash.
Lovett abandons his search after hearing Rose's story. Alone on the stern of "Keldysh", Rose takes out the Heart of the Ocean — in her possession all along — and drops it into the sea over the wreck site. While she is seemingly asleep in her bed, photos on her dresser depict a life of freedom and adventure inspired by Jack. A young Rose reunites with Jack at the "Titanic"'s Grand Staircase, applauded by those who perished.
Cast.
Historical characters.
Although not—and not intended to be—an entirely historically accurate depiction of events, the film does include dramatizations of certain historical characters:
Cameos.
Several crew members of the "Akademik Mstislav Keldysh" appear in the film, including Anatoly Sagalevich, creator and pilot of the "MIR" self-propelled Deep Submergence Vehicle. Anders Falk, who filmed a documentary about the film's sets for the "Titanic" Historical Society, makes a cameo appearance in the film as a Swedish immigrant whom Jack Dawson meets when he enters his cabin; Edward Kamuda and Karen Kamuda, then President and Vice President of the Society who served as film consultants, were cast as extras in the film.
Pre-production.
Writing and inspiration.
James Cameron had a fascination with shipwrecks, and, for him, the RMS "Titanic" was "the Mount Everest of shipwrecks." He was almost past the point in his life when he felt he could consider an undersea expedition, but said he still had "a mental restlessness" to live the life he had turned away from when he switched from the sciences to the arts in college. So when an IMAX film was made from footage shot of the wreck itself, he decided to seek Hollywood funding to "pay for an expedition and do the same thing." It was "not because I particularly wanted to make the movie," Cameron said. "I wanted to dive to the shipwreck."
Cameron wrote a scriptment for a "Titanic" film, met with 20th Century Fox executives including Peter Chernin, and pitched it as "Romeo and Juliet on the "Titanic"". Cameron stated, "They were like, 'Oooooohkaaaaaay – a three-hour romantic epic? Sure, that's just what we want. Is there a little bit of "Terminator" in that? Any Harrier jets, shoot-outs, or car chases?' I said, 'No, no, no. It's not like that.'" The studio was dubious about the idea's commercial prospects, but, hoping for a long term relationship with Cameron, they gave him a greenlight.
Cameron convinced Fox to promote the film based on the publicity afforded by shooting the "Titanic" wreck itself, and organized several dives to the site over a period of two years. "My pitch on that had to be a little more detailed," said Cameron. "So I said, 'Look, we've got to do this whole opening where they're exploring the "Titanic" and they find the diamond, so we're going to have all these shots of the ship." Cameron stated, "Now, we can either do them with elaborate models and motion control shots and CG and all that, which will cost X amount of money – or we can spend X plus 30 per cent and actually go shoot it at the real wreck." The crew shot at the real wreck in the Atlantic Ocean twelve times in 1995 and actually spent more time with the ship than its passengers. At that depth, with a water pressure of 6,000 pounds per square inch, "one small flaw in the vessel's superstructure would mean instant death for all on board." Not only were the dives high-risk, but adverse conditions prevented Cameron from getting the high quality footage that he wanted. During one dive, one of the submersibles collided with "Titanic"‍ '​s hull, damaging both sub and ship and leaving fragments of the submersible's propeller shroud scattered around the superstructure. The external bulkhead of Captain Smith's quarters collapsed, exposing the interior. The area around the entrance to the Grand Staircase was also damaged.
Descending to the actual site made both Cameron and crew want "to live up to that level of reality... But there was another level of reaction coming away from the real wreck, which was that it wasn't just a story, it wasn't just a drama," he said. "It was an event that happened to real people who really died. Working around the wreck for so much time, you get such a strong sense of the profound sadness and injustice of it, and the message of it." Cameron stated, "You think, 'There probably aren't going to be many filmmakers who go to "Titanic." There may never be another one – maybe a documentarian." Due to this, he felt "a great mantle of responsibility to convey the emotional message of it – to do that part of it right, too".
After filming the underwater shots, Cameron began writing the screenplay. He wanted to honor the people who died during the sinking, so he spent six months researching all of the "Titanic"‍ '​s crew and passengers. "I read everything I could. I created an extremely detailed timeline of the ship's few days and a very detailed timeline of the last night of its life," he said. "And I worked within that to write the script, and I got some historical experts to analyze what I'd written and comment on it, and I adjusted it." He paid meticulous attention to detail, even including a scene depicting the "Californian"‍ '​s role in "Titanic"‍‍ '​‍s demise, though this was later cut (see below). From the beginning of the shoot, they had "a very clear picture" of what happened on the ship that night. "I had a library that filled one whole wall of my writing office with ""Titanic" stuff," because I wanted it to be right, especially if we were going to dive to the ship," he said. "That set the bar higher in a way – it elevated the movie in a sense. We wanted this to be a definitive visualization of this moment in history as if you'd gone back in a time machine and shot it."
Cameron felt the "Titanic" sinking was "like a great novel that really happened", but that the event had become a mere morality tale; the film would give audiences the experience of living the history. The treasure hunter Brock Lovett represented those who never connected with the human element of the tragedy, while the blossoming romance of Jack and Rose, Cameron believed, would be the most engaging part of the story: when their love is finally destroyed, the audience would mourn the loss. He said: "All my films are love stories, but in "Titanic" I finally got the balance right. It's not a disaster film. It's a love story with a fastidious overlay of real history." Cameron framed the romance with the elderly Rose to make the intervening years palpable and poignant. For him, the end of the film is left open to interpretation regarding whether or not elderly Rose is dreaming or has died in her sleep. He said that although he knows what he intended with the ending, he will not reveal its intention, adding, "The answer has to be something you supply personally; individually."
Scale modeling.
Harland and Wolff, the RMS "Titanic"‍‍ '​‍s builders, opened their private archives to the crew, sharing blueprints that were thought lost. For the ship's interiors, production designer Peter Lamont's team looked for artifacts from the era. The newness of the ship meant every prop had to be made from scratch. Fox acquired 40 acres of waterfront south of Playas de Rosarito in Mexico, and began building a new studio on May 31, 1996. A horizon tank of seventeen million gallons was built for the exterior of the reconstructed ship, providing 270 degrees of ocean view. The ship was built to full scale, but Lamont removed redundant sections on the superstructure and forward well deck for the ship to fit in the tank, with the remaining sections filled with digital models. The lifeboats and funnels were shrunk by ten percent. The boat deck and A-deck were working sets, but the rest of the ship was just steel plating. Within was a fifty-foot lifting platform for the ship to tilt during the sinking sequences. Towering above was a 162 ft tall tower crane on 600 ft of rail track, acting as a combined construction, lighting, and camera platform.
The sets representing the interior rooms of the "Titanic" were reproduced exactly as originally built, using photographs and plans from the "Titanic"‍‍ '​‍s builders. "The liner's first class staircase, which figures prominently in the script was constructed out of real wood and actually destroyed in the filming of the sinking." The rooms, the carpeting, design and colors, individual pieces of furniture, decorations, chairs, wall paneling, cutlery and crockery with the White Star Line crest on each piece, completed ceilings, and costumes were among the designs true to the originals. Cameron additionally hired two "Titanic" historians, Don Lynch and Ken Marschall, to authenticate the historical detail in the film.
Production.
The modern day scenes of the expedition were shot on the "Akademik Mstislav Keldysh" in July 1996. Principal photography for "Titanic" began in September 1996 at the newly built Fox Baja Studios. The poop deck was built on a hinge which could rise from zero to ninety degrees in a few seconds as the ship's stern rose during the sinking. For the safety of the stuntmen, many props were made of foam rubber. By November 15, the boarding scenes were being shot. Cameron chose to build his RMS "Titanic" on the starboard side as a study of weather data showed prevailing north-to-south wind which blew the funnel smoke aft. This posed a problem for shooting the ship's departure from Southampton, as it was docked on its port side. Any writing on props and costumes had to be reversed, and if someone walked to their right in the script, they had to walk left during shooting. In post-production, the film was flipped to the correct direction.
A full-time etiquette coach was hired to instruct the cast on the manners of the upper class gentility in 1912. Despite this, several critics picked up on anachronisms in the film, not least involving the two main stars.
Cameron sketched Jack's nude portrait of Rose for a scene which he feels has the backdrop of repression. "You know what it means for her, the freedom she must be feeling. It's kind of exhilarating for that reason," he said. The nude scene was DiCaprio and Winslet's first scene together. "It wasn't by any kind of design, although I couldn't have designed it better. There's a nervousness and an energy and a hesitance in them," Cameron stated. "They had rehearsed together, but they hadn't shot anything together. If I'd had a choice, I probably would have preferred to put it deeper into the body of the shoot." He said he and his crew "were just trying to find things to shoot" because the big set was not yet ready. "It wasn't ready for months, so we were scrambling around trying to fill in anything we could get to shoot." After seeing the scene on film, Cameron felt it worked out considerably well.
Other times on the set were not as smooth. The shoot was an arduous experience that "cemented Cameron's formidable reputation as 'the scariest man in Hollywood'. He became known as an uncompromising, hard-charging perfectionist" and a "300-decibel screamer, a modern-day Captain Bligh with a megaphone and walkie-talkie, swooping down into people's faces on a 162ft crane". Winslet chipped a bone in her elbow during filming, and had been worried that she would drown in the 17m-gallon water tank the ship was to be sunk in. "There were times when I was genuinely frightened of him. Jim has a temper like you wouldn't believe," she said. "'God damn it!' he would yell at some poor crew member, 'that's exactly what I didn't want!'" Her co-star, Bill Paxton, was familiar with Cameron's work ethic from his earlier experience with him. "There were a lot of people on the set. Jim is not one of those guys who has the time to win hearts and minds," he said. The crew felt that Cameron had an evil alter ego, and nicknamed him "Mij" (Jim spelt backwards). In response to the criticism, Cameron stated, "Film-making is war. A great battle between business and aesthetics."
During shooting on the "Akademik Mstislav Keldysh", an angry crew member put the dissociative drug PCP into the soup that Cameron and various others ate one night, which sent more than 50 people to the hospital. "There were people just rolling around, completely out of it. Some of them said they were seeing streaks and psychedelics," said actor Lewis Abernathy. Cameron managed to vomit before the drug took a full hold. Abernathy was shocked at the way he looked. "One eye was completely red, like the Terminator eye. A pupil, no iris, beet red. The other eye looked like he'd been sniffing glue since he was four." The person behind the poisoning was never caught.
The filming schedule was intended to last 138 days but grew to 160. Many cast members came down with colds, flu, or kidney infections after spending hours in cold water, including Winslet. In the end, she decided she would not work with Cameron again unless she earned "a lot of money". Several others left and three stuntmen broke their bones, but the Screen Actors Guild decided, following an investigation, that nothing was inherently unsafe about the set. Additionally, DiCaprio said there was no point when he felt he was in danger during filming. Cameron believed in a passionate work ethic and never apologized for the way he ran his sets, although he acknowledged:I'm demanding, and I'm demanding on my crew. In terms of being kind of militaresque, I think there's an element of that in dealing with thousands of extras and big logistics and keeping people safe. I think you have to have a fairly strict methodology in dealing with a large number of people.
The costs of filming "Titanic" eventually began to mount, and finally reached $200 million. Fox executives panicked, and suggested an hour of specific cuts from the three-hour film. They argued the extended length would mean fewer showings, thus less money even though long epics are more likely to help directors win Oscars. Cameron refused, telling Fox, "You want to cut my movie? You're going to have to fire me! You want to fire me? You're going to have to kill me!" The executives did not want to start over, because it would mean the loss of their entire investment, but they also initially rejected Cameron's offer of forfeiting his share of the profits as an empty gesture; they felt that profits would be unlikely. Cameron explained forfeiting his share as complex. "...the short version is that the film cost proportionally much more than "" and "True Lies." Those films went up seven or eight percent from the initial budget. "Titanic" also had a large budget to begin with, but it went up a lot more," said Cameron. "As the producer and director, I take responsibility for the studio that's writing the checks, so I made it less painful for them. I did that on two different occasions. They didn't force me to do it; they were glad that I did."
Post-production.
Effects.
Cameron wanted to push the boundary of special effects with his film, and enlisted Digital Domain to continue the developments in digital technology which the director pioneered while working on "The Abyss" and "". Many previous films about the RMS "Titanic" shot water in slow motion, which did not look wholly convincing. He encouraged them to shoot their 45 ft long miniature of the ship as if "we're making a commercial for the White Star Line". Afterwards, digital water and smoke were added, as were extras captured on a motion capture stage. Visual effects supervisor Rob Legato scanned the faces of many actors, including himself and his children, for the digital extras and stuntmen. There was also a 65 ft long model of the ship's stern that could break in two repeatedly, the only miniature to be used in water. For scenes set in the ship's engines, footage of the SS "Jeremiah O'Brien"‍ '​s engines were composited with miniature support frames and actors shot against a greenscreen. In order to save money, the first class lounge was a miniature set incorporated into a greenscreen backdrop.
An enclosed 5000000 usgal tank was used for sinking interiors, in which the entire set could be tilted into the water. In order to sink the Grand Staircase, 90000 usgal of water were dumped into the set as it was lowered into the tank. Unexpectedly, the waterfall ripped the staircase from its steel-reinforced foundations, although no one was hurt. The 744 ft long exterior of the RMS "Titanic" had its first half lowered into the tank, but being the heaviest part of the ship meant it acted as a shock absorber against the water; to get the set into the water, Cameron had much of the set emptied and even smashed some of the promenade windows himself. After submerging the dining saloon, three days were spent shooting Lovett's ROV traversing the wreck in the present. The post-sinking scenes in the freezing Atlantic were shot in a 350000 usgal tank, where the frozen corpses were created by applying a powder on actors that crystallized when exposed to water, and wax was coated on hair and clothes.
The climactic scene, which features the breakup of the ship directly before it sinks, as well as its final plunge to the bottom of the Atlantic, involved a tilting full-sized set, 150 extras and 100 stunt performers. Cameron criticized previous "Titanic" films for depicting the final plunge of the liner as sliding gracefully underwater. He "wanted to depict it as the terrifyingly chaotic event that it really was". When carrying out the sequence, people needed to fall off the increasingly tilting deck, plunging hundreds of feet below and bouncing off of railings and propellers on the way down. A few attempts to film this sequence with stunt people resulted in some minor injuries and Cameron halted the more dangerous stunts. The risks were eventually minimized "by using computer generated people for the dangerous falls".
Editing.
There was one "crucial historical fact" Cameron chose to omit from the film – the ship that was close to the "Titanic", but had turned off its radio for the night and did not hear their SOS calls. "Yes, the [SS] "Californian." That wasn't a compromise to mainstream filmmaking. That was really more about emphasis, creating an emotional truth to the film," stated Cameron. He said there were aspects of retelling the sinking that seemed important in pre and post-production, but turned out to be less important as the film evolved. "The story of the "Californian" was in there; we even shot a scene of them switching off their Marconi radio set," said Cameron. "But I took it out. It was a clean cut, because it focuses you back onto that world. If "Titanic" is powerful as a metaphor, as a microcosm, for the end of the world in a sense, then that world must be self-contained."
During the first assembly cut, Cameron altered the planned ending, which had given resolution to Brock Lovett's story. In the original version of the ending, Brock and Lizzy see the elderly Rose at the stern of the boat, and fear she is going to commit suicide. Rose then reveals that she had the "Heart of the Ocean" diamond all along, but never sold it, in order to live on her own without Cal's money. She tells Brock that life is priceless and throws the diamond into the ocean, after allowing him to hold it. After accepting that treasure is worthless, Brock laughs at his stupidity. Rose then goes back to her cabin to sleep, whereupon the film ends in the same way as the final version. In the editing room, Cameron decided that by this point, the audience would no longer be interested in Brock Lovett and cut the resolution to his story, so that Rose is alone when she drops the diamond. He also did not want to disrupt the audience's melancholy after the "Titanic"‍‍ '​‍s sinking.
The version used for the first test screening featured a fight between Jack and Lovejoy which takes place after Jack and Rose escape into the flooded dining saloon, but the test audiences disliked it. The scene was written to give the film more suspense, and featured Cal (falsely) offering to give Lovejoy, his valet, the "Heart of the Ocean" if he can get it from Jack and Rose. Lovejoy goes after the pair in the sinking first class dining room. Just as they are about to escape him, Lovejoy notices Rose's hand slap the water as it slips off the table behind which she is hiding. In revenge for framing him for the "theft" of the necklace, Jack attacks him and smashes his head against a glass window, which explains the gash on Lovejoy's head that can be seen when he dies in the completed version of the film. In their reactions to the scene, test audiences said it would be unrealistic to risk one's life for wealth, and Cameron cut it for this reason, as well as for timing and pacing reasons. Many other scenes were cut for similar reasons.
Music and soundtrack.
The soundtrack album for "Titanic" was composed by James Horner. For the vocals heard throughout the film, subsequently described by Earle Hitchner of "The Wall Street Journal" as "evocative", Horner chose Norwegian singer Sissel Kyrkjebø, mononymously known as "Sissel". Horner knew Sissel from her album "Innerst i sjelen", and he particularly liked how she sang "Eg veit i himmerik ei borg" ("I Know in Heaven There Is a Castle"). He had tried twenty-five or thirty singers before he finally chose Sissel as the voice to create specific moods within the film.
Horner additionally wrote the song "My Heart Will Go On" in secret with Will Jennings because Cameron did not want any songs with singing in the film. Céline Dion agreed to record a demo with the persuasion of her husband René Angélil. Horner waited until Cameron was in an appropriate mood before presenting him with the song. After playing it several times, Cameron declared his approval, although worried that he would have been criticized for "going commercial at the end of the movie". Cameron also wanted to appease anxious studio executives and "saw that a hit song from his movie could only be a positive factor in guaranteeing its completion".
Release.
Initial screening.
20th Century Fox and Paramount Pictures co-financed "Titanic", with Paramount handling the North American distribution and Fox handling the international release. They expected Cameron to complete the film for a release on July 2, 1997. The film was to be released on this date "in order to exploit the lucrative summer season ticket sales when blockbuster films usually do better". In April, Cameron said the film's special effects were too complicated and that releasing the film for summer would not be possible. With production delays, Paramount pushed back the release date to December 19, 1997. "This fueled speculation that the film itself was a disaster." A preview screening in Minneapolis on July 14 "generated positive reviews" and "[c]hatter on the internet was responsible for more favorable word of mouth about the [film]". This eventually led to more positive media coverage.
The film premiered on November 1, 1997, at the Tokyo International Film Festival, where reaction was described as "tepid" by "The New York Times". Positive reviews started to appear back in the United States; the official Hollywood premiere occurred on December 14, 1997, where "the big movie stars who attended the opening were enthusiastically gushing about the film to the world media".
Box office.
Including revenue from the 2012 reissue, "Titanic" earned $658,672,302 in North America and $1,526,700,000 in other countries, for a worldwide total of $2,185,372,302. It became the highest-grossing film of all time worldwide in 1998, and remained so for twelve years, until "Avatar", also written and directed by Cameron, surpassed it in 2010. On March 1, 1998, it became the first film to earn more than $1 billion worldwide, and on the weekend April 13–15, 2012—a century after the original vessel's foundering—"Titanic" became the second film to cross the $2 billion threshold during its 3D re-release. Box Office Mojo estimates that "Titanic" is the fifth highest-grossing film of all time in North America when adjusting for ticket price inflation.
Initial theatrical run.
The film received steady attendance after opening in North America on Friday, December 19, 1997. By the end of that same weekend, theaters were beginning to sell out. The film earned $8,658,814 on its opening day and $28,638,131 over the opening weekend from 2,674 theaters, averaging to about $10,710 per venue, and ranking number one at the box office, ahead of the eighteenth James Bond film, "Tomorrow Never Dies". By New Year's Day, "Titanic" had made over $120 million, had increased in popularity and theaters continued to sell out. Its highest grossing single day was Saturday, February 14 (Valentine's Day), 1998, on which it earned $13,048,711, more than eight weeks after its North American debut. It stayed at number one for fifteen consecutive weeks in North America, which remains a record for any film. The film stayed in theaters in North America for almost ten months, before finally closing on Thursday, October 1, 1998 with a final domestic gross of $600,788,188. Outside North America, the film made double its North American gross, generating $1,242,413,080 and accumulating a grand total of $1,843,201,268 worldwide from its initial theatrical run.
Commercial analysis.
Analyzing "Titanic"'s popularity, author Alexandra Keller stated that scholars could agree that the film's popularity "appears dependent on contemporary culture, on perceptions of history, on patterns of consumerism and globalization, as well as on those elements experienced filmgoers conventionally expect of juggernaut film events in the 1990s – awe – some screen spectacle, expansive action, and, more rarely seen, engaging characters and epic drama."
Before "Titanic"'s release, various film critics predicted the film would be a significant disappointment at the box office, especially due to it being the most expensive film ever made at the time. When it was shown to the press in autumn of 1997, "it was with massive forebodings" since the "people in charge of the screenings believed they were on the verge of losing their jobs – because of this great albatross of a picture on which, finally, two studios had to combine to share the great load of its making". Cameron also thought he was "headed for disaster" at one point during filming. "We labored the last six months on "Titanic" in the absolute knowledge that the studio would lose $100m. It was a certainty," he stated. As the film neared release, "particular venom was spat at Cameron for what was seen as his hubris and monumental extravagance". A film critic for the "Los Angeles Times" wrote that "Cameron's overweening pride has come close to capsizing this project" and that the film was "a hackneyed, completely derivative copy of old Hollywood romances".
When the film became a success, with an unprecedented box office performance, it was credited as "the love story [that] stole the world's hearts". "The first batch of people to see it [were] gob smacked by the sheer scale and intimacy of the production. They emerged from the cinema, tear stained and emotionally flabbergasted." The film was playing on 3,200 screens ten weeks after it opened, and out of its fifteen straight weeks on top of the charts, jumped 43% in total sales in its ninth week of release. It earned over $20 million a week for ten weeks, and after 14 weeks was still bringing in more than $1m a week. 20th Century Fox estimated that seven percent of American teenage girls had seen "Titanic" twice by its fifth week. Although young women who saw the film several times, and subsequently caused "", were often credited with having primarily propelled the film to its all-time box office record, other reports have attributed the film's success to "[p]ositive word of mouth and repeat viewership" due to the love story combined with the ground-breaking special effects.
The film's impact on men has also been especially credited. Now considered one of the films that "make men cry", MSNBC's Ian Hodder stated that men admire Jack's sense of adventure, stowing away on a steamship bound for America. "We cheer as he courts a girl who was out of his league. We admire how he suggests nude modeling as an excuse to get naked. So when [the tragic ending happens], an uncontrollable flood of tears sinks our composure," he said. "Titanic"‍‍ '​‍s ability to make men cry was briefly parodied in the 2009 film "Zombieland", where character Tallahassee (Woody Harrelson), when recalling the death of his young son, states: "I haven't cried like that since "Titanic"." Also addressing the sentimentality of the film, Benjamin Willcock of DVDActive.com said that, as a fourteen-year-old male, he had wanted to see "Starship Troopers" instead, but was overruled by an uncle and friends. "Little did I know that I would be seeing a film that would become the biggest, most successful motion picture event of all time," he stated. "I was also blissfully unaware that it would turn out to be so much more than 'some epic love story'".
In 2010, the BBC analyzed the stigma over men crying during "Titanic" and films in general. "Middle-aged men are not 'supposed' to cry during movies," stated Finlo Rohrer of the website, citing the ending of "Titanic" as having generated such tears, adding that "men, if they have felt weepy during [this film], have often tried to be surreptitious about it." Professor Mary Beth Oliver, of Penn State University, stated, "For many men, there is a great deal of pressure to avoid expression of 'female' emotions like sadness and fear. From a very young age, males are taught that it is inappropriate to cry, and these lessons are often accompanied by a great deal of ridicule when the lessons aren't followed." She said, "Indeed, some men who might sneer at the idea of crying during "Titanic" will readily admit to becoming choked up during "Saving Private Ryan" or "Platoon."" For men in general, "the idea of sacrifice for a 'brother' is a more suitable source of emotion".
"Titanic"‍‍ '​‍s catchphrase "I'm the king of the world!" became one of the film industry's more popular quotations. According to Richard Harris, a psychology professor at Kansas State University, who studied why people like to cite films in social situations, using film quotations in everyday conversation is similar to telling a joke and a way to form solidarity with others. "People are doing it to feel good about themselves, to make others laugh, to make themselves laugh", he said.
Cameron explained the film's success as having significantly benefited from the experience of sharing. "When people have an experience that's very powerful in the movie theatre, they want to go share it. They want to grab their friend and bring them, so that they can enjoy it," he said. "They want to be the person to bring them the news that this is something worth having in their life. That's how "Titanic" worked." Media Awareness Network stated, "The normal
repeat viewing rate for a blockbuster theatrical film is about 5%. The repeat rate for "Titanic" was over 20%." The box office receipts "were even more impressive" when factoring in "the film's 3 hour and 14 minute length meant that it could only be shown three times a day compared to a normal movie's four showings". In response to this, "[m]any theatres started midnight showings and were rewarded with full houses until almost 3:30 am".
"Titanic" held the record for box office gross for twelve years. Cameron's follow-up film, "Avatar", was considered the first film with a genuine chance at surpassing its worldwide gross, and did so in 2010. Various explanations for why the film was able to successfully challenge "Titanic" were given. For one, "Two-thirds of "Titanic"‍‍ '​‍s haul was earned overseas, and "Avatar" [tracked] similarly... "Avatar" opened in 106 markets globally and was no. 1 in all of them" and the markets "such as Russia, where "Titanic" saw modest receipts in 1997 and 1998, are white-hot today" with "more screens and moviegoers" than ever before. Brandon Gray, president of Box Office Mojo, said that while "Avatar" may beat "Titanic"‍‍ '​‍s revenue record, the film is unlikely to surpass "Titanic" in attendance. "Ticket prices were about $3 cheaper in the late 1990s." In December 2009, Cameron had stated, "I don't think it's realistic to try to topple "Titanic" off its perch. Some pretty good movies have come out in the last few years. "Titanic" just struck some kind of chord." In a January 2010 interview, he gave a different take on the matter once "Avatar"‍‍ '​‍s performance was easier to predict. "It's gonna happen. It's just a matter of time," he said.
Critical reception.
"Titanic" garnered mainly positive reviews from film critics, and was positively reviewed by audiences and scholars, who commented on the film's cultural, historical and political impacts. It holds an overall 88% approval rating on review aggregator website Rotten Tomatoes, based on 178 reviews, with a rating average of 8 out of 10. The site's consensus reads: "A mostly unqualified triumph for Cameron, who offers a dizzying blend of spectacular visuals and old-fashioned melodrama." At Metacritic, which assigns a weighted mean rating out of 0–100 reviews from film critics, the film has a rating score of 74 based on 34 reviews, classified as a generally favorably reviewed film.
With regard to the film's overall design, Roger Ebert stated, "It is flawlessly crafted, intelligently constructed, strongly acted, and spellbinding... Movies like this are not merely difficult to make at all, but almost impossible to make well." He credited the "technical difficulties" with being "so daunting that it's a wonder when the filmmakers are also able to bring the drama and history into proportion" and "found [himself] convinced by both the story and the sad saga". He named it his ninth best film of 1997. On the television program "Siskel & Ebert", the film received "two thumbs up" and was praised for its accuracy in recreating the ship's sinking; Ebert described the film as "a glorious Hollywood epic" and "well worth the wait," and Gene Siskel found Leonardo DiCaprio "captivating". James Berardinelli stated, "Meticulous in detail, yet vast in scope and intent, "Titanic" is the kind of epic motion picture event that has become a rarity. You don't just watch "Titanic", you experience it." It was named his second best film of 1997. Almar Haflidason of the BBC wrote that "[t]he sinking of the great ship is no secret, yet for many exceeded expectations in sheer scale and tragedy" and that "when you consider that [the film] tops a bum-numbing three-hour running time, then you have a truly impressive feat of entertainment achieved by Cameron". Joseph McBride of "Boxoffice Magazine" concluded, "To describe "Titanic" as the greatest disaster movie ever made is to sell it short. James Cameron's recreation of the 1912 sinking of the 'unsinkable' liner is one of the most magnificent pieces of serious popular entertainment ever to emanate from Hollywood."
The romantic and emotionally charged aspects of the film were equally praised. Andrew L. Urban of "Urban Cinefile" said, "You will walk out of "Titanic" not talking about budget or running time, but of its enormous emotive power, big as the engines of the ship itself, determined as its giant propellers to gouge into your heart, and as lasting as the love story that propels it." Owen Gleiberman of "Entertainment Weekly" described the film as, "A lush and terrifying spectacle of romantic doom. Writer-director James Cameron has restaged the defining catastrophe of the early 20th century on a human scale of such purified yearning and dread that he touches the deepest levels of popular moviemaking." Janet Maslin of "The New York Times" commented that "Cameron's magnificent "Titanic" is the first spectacle in decades that honestly invites comparison to "Gone With the Wind"." Richard Corliss of "Time" magazine, on the other hand, wrote a mostly negative review, criticizing the lack of interesting emotional elements.
Some reviewers felt that the story and dialogue were weak, while the visuals were spectacular. Kenneth Turan's review in the "Los Angeles Times" was particularly scathing. Dismissing the emotive elements, he stated, "What really brings on the tears is Cameron's insistence that writing this kind of movie is within his abilities. Not only is it not, it is not even close.", and later claimed that the only reason that the film won Oscars was because of its box office total. Barbara Shulgasser of "The San Francisco Examiner" gave "Titanic" one star out of four, citing a friend as saying, "The number of times in this unbelievably badly written script that the two [lead characters] refer to each other by name was an indication of just how dramatically the script lacked anything more interesting for the actors to say." Also, filmmaker Robert Altman called it "the most dreadful piece of work I've ever seen in my entire life". In his 2012 study of the lives of the passengers on the "Titanic", historian Richard Davenport-Hines said, "Cameron's film diabolized rich Americans and educated English, anathematizing their emotional restraint, good tailoring, punctilious manners and grammatical training, while it made romantic heroes of the poor Irish and the unlettered".
"Titanic" suffered backlash in addition to its success. In 2003, the film topped a poll of "Best Film Endings", and yet it also topped a poll by "Film 2003" as "the worst movie of all time". The British film magazine "Empire" reduced their rating of the film from the maximum five stars and an enthusiastic review, to four stars with a less positive review in a later edition, to accommodate its readers' tastes, who wanted to disassociate themselves from the hype surrounding the film, and the reported activities of its fans, such as those attending multiple screenings. In addition to this, positive and negative parodies and other such spoofs of the film abounded and were circulated on the internet, often inspiring passionate responses from fans of various opinions of the film. Benjamin Willcock of DVDActive.com did not understand the backlash or the passionate hatred for the film. "What really irks me...," he said, "are those who make nasty stabs at those who do love it." Willcock stated, "I obviously don't have anything against those who dislike "Titanic", but those few who make you feel small and pathetic for doing so (and they do exist, trust me) are way beyond my understanding and sympathy."
Cameron responded to the backlash, and Kenneth Turan's review in particular. ""Titanic" is not a film that is sucking people in with flashy hype and spitting them out onto the street feeling let down and ripped off," he stated. "They are returning again and again to repeat an experience that is taking a 3-hour and 14-minute chunk out of their lives, and dragging others with them, so they can share the emotion." Cameron emphasized people from all ages (ranging from 8 to 80) and from all backgrounds were "celebrating their own essential humanity" by seeing it. He described the script as earnest and straightforward, and said it intentionally "incorporates universals of human experience and emotion that are timeless – and familiar because they reflect our basic emotional fabric" and that the film was able to succeed in this way by dealing with archetypes. He did not see it as pandering. "Turan mistakes archetype for cliche," he said. "I don't share his view that the best scripts are only the ones that explore the perimeter of human experience, or flashily pirouette their witty and cynical dialogue for our admiration."
"Empire" eventually reinstated its original five star rating of the film, commenting, "It should be no surprise then that it became fashionable to bash James Cameron's "Titanic" at approximately the same time it became clear that this was the planet's favourite film. Ever."
Accolades.
"Titanic" began its awards sweep starting with the Golden Globes, winning four, namely Best Motion Picture – Drama, Best Director, Best Original Score, and Best Original Song. Kate Winslet and Gloria Stuart were also nominees. It won the ACE "Eddie" Award, ASC Award, Art Directors Guild Award, Cinema Audio Society Awards, Screen Actors Guild Award (Best Supporting Actress for Gloria Stuart), The Directors Guild of America Award, and Broadcast Film Critics Association Award (Best Director for James Cameron), and The Producer Guild of America Award. It was also nominated for ten BAFTA awards, including Best Film and Best Director; it failed to win any.
The film garnered fourteen Academy Awards nominations, tying the record set in 1950 by Joseph L. Mankiewicz's "All About Eve" and won eleven: Best Picture (the second film about the "Titanic" to win that award, after 1933's "Cavalcade"), Best Director, Best Art Direction, Best Cinematography, Best Visual Effects, Best Film Editing, Best Costume Design, Best Sound (Gary Rydstrom, Tom Johnson, Gary Summers, Mark Ulano), Best Sound Effects Editing, Best Original Dramatic Score, Best Original Song. Kate Winslet, Gloria Stuart and the make-up artists were the three nominees that did not win. James Cameron's original screenplay and Leonardo DiCaprio were not nominees. It was the second film to win eleven Academy Awards, after "Ben-Hur". "" would also match this record in 2004.
"Titanic" won the 1997 Academy Award for Best Original Song, as well as three Grammy Awards for Record of the Year, Song of the Year, and Best Song Written Specifically for a Motion Picture or Television. The film's soundtrack became the best-selling primarily orchestral soundtrack of all time, and became a worldwide success, spending sixteen weeks at number-one in the United States, and was certified diamond for over eleven million copies sold in the United States alone. The soundtrack also became the best-selling album of 1998 in the U.S. "My Heart Will Go On" won the Grammy Awards for Best Song Written Specifically for a Motion Picture or for Television. The film also won Best Male Performance for Leonardo DiCaprio and Best Movie at the MTV Movie Awards, Best Film at the People's Choice Awards, and Favorite Movie at the 1998 Kids' Choice Awards. It won various awards outside the United States, including the Awards of the Japanese Academy as the Best Foreign Film of the Year. "Titanic" eventually won nearly ninety awards and had an additional forty-seven nominations from various award-giving bodies around the world. Additionally, the book about the making of the film was at the top of "The New York Times"' bestseller list for several weeks, "the first time that such
a tie-in book had achieved this status".
Since its release, "Titanic" has appeared on the American Film Institute's award-winning 100 Years… series. So far, it has ranked on the following six lists:
Home media.
"Titanic" was released worldwide in widescreen and pan and scan formats on VHS and laserdisc on September 1, 1998. The VHS was also made available in a deluxe boxed gift set with a mounted filmstrip and six lithograph prints from the movie. A DVD version was released on August 31, 1999 in a widescreen-only (non-anamorphic) single-disc edition with no special features other than a theatrical trailer. Cameron stated at the time that he intended to release a special edition with extra features later. This release became the best-selling DVD of 1999 and early 2000, becoming the first DVD ever to sell one million copies. At the time, fewer than 5% of all U.S. homes had a DVD player. "When we released the original "Titanic" DVD, the industry was much smaller, and bonus features were not the standard they are now," said Meagan Burrows, Paramount's president of domestic home entertainment, which made the film's DVD performance even more impressive.
"Titanic" was re-released to DVD on October 25, 2005 when a three-disc "Special Collector's Edition" was made available in the United States and Canada. This edition contained a newly restored transfer of the film, as well as various special features. An international two and four-disc set followed on November 7, 2005. The two-disc edition was marketed as the "Special Edition", and featured the first two discs of the three-disc set, only PAL-enabled. A four-disc edition, marketed as the "Deluxe Collector's Edition", was also released on November 7, 2005.
Also, available only in the United Kingdom, a limited 5-disc set of the film, under the title "Deluxe Limited Edition", was released with only 10,000 copies manufactured. The fifth disc contains Cameron's documentary "Ghosts of the Abyss", which was distributed by Walt Disney Pictures. Unlike the individual release of "Ghosts of the Abyss", which contained two discs, only the first disc was included in the set.
As regards to television broadcasts, the film airs occasionally across the United States on networks such as TNT. To permit the scene where Jack draws the nude portrait of Rose to be shown on network and specialty cable channels, in addition to minor cuts, the sheer, see-through robe worn by Winslet was digitally painted black. Turner Classic Movies also began to show the film, specifically during the days leading up to the 82nd Academy Awards.
3D conversion.
A 2012 re-release, also known as "Titanic in 3D," was created by re-mastering the original to 4K resolution and post-converting to stereoscopic 3D format. The "Titanic" 3D version took 60 weeks and $18 million to produce, including the 4K restoration. The 3D conversion was performed by Stereo D and Sony with Slam Content's Panther Records remastering the soundtrack. Digital 2D and in 2D IMAX versions were also struck from the new 4K master created in the process. For the 3D release, Cameron opened up the Super 35 film and expanded the image of the film into a new aspect ratio, from 2:35:1 to 1:78:1, allowing the viewer to see more image on the top and bottom of the screen. The only scene entirely redone for the re-release was Rose's view of the night sky at sea, on the morning of April 15, 1912. The scene was replaced with an accurate view of the night-sky star pattern, including the Milky Way, adjusted for the location in the North Atlantic Ocean in April 1912. The change was prompted by astrophysicist Neil deGrasse Tyson, who had criticized the scene for showing an unrealistic star pattern. He agreed to send film director Cameron a corrected view of the sky, which was the basis of the new scene.
The 3D version of "Titanic" premiered at the Royal Albert Hall in London on March 27, 2012, with James Cameron and Kate Winslet in attendance, and entered general release on April 4, 2012, six days shy of the centenary of "RMS Titanic" embarking on her maiden voyage.
"Rolling Stone" film critic Peter Travers rated the reissue 3.5 stars out of 4, explaining he found it "pretty damn dazzling". He said, "The 3D intensifies "Titanic". You are there. Caught up like never before in an intimate epic that earns its place in the movie time capsule." Writing for "Entertainment Weekly", Owen Gleiberman gave the film an A grade. He wrote, "For once, the visuals in a 3-D movie don't look darkened or distracting. They look sensationally crisp and alive." Richard Corliss of "Time" who was very critical in 1997 remained in the same mood, "I had pretty much the same reaction: fitfully awed, mostly water-logged." In regards to the 3D effects, he noted the "careful conversion to 3D lends volume and impact to certain moments ... [but] in separating the foreground and background of each scene, the converters have carved the visual field into discrete, not organic, levels." Ann Hornaday for "The Washington Post" found herself asking "whether the film's twin values of humanism and spectacle are enhanced by Cameron's 3-D conversion, and the answer to that is: They aren't." She further added that the "3-D conversion creates distance where there should be intimacy, not to mention odd moments in framing and composition."
The film grossed an estimated $4.7 million on the first day of its re-release in North America (including midnight preview showings) and went on to make $17.3 million over the weekend, finishing in third place. Outside North America it earned $35.2 million finishing second, and improved on its performance the following weekend by topping the box office with $98.9 million. China has proven to be its most successful territory where it earned $11.6 million on its opening day, going on to earn a record-breaking $67 million in its opening week and taking more money in the process than it did in the entirety of its original theatrical run. The reissue ultimately earned $343.4 million worldwide, with $145 million coming from China and $57.8 million from Canada and United States.
The 3D conversion of the film was also released in the 4DX format in selected international territories, which allows the audience to experience the film's environment using motion, wind, fog, lighting and scent-based special effects.
Further reading.
</dl>

</doc>
<doc id="52373" url="http://en.wikipedia.org/wiki?curid=52373" title="Center pivot irrigation">
Center pivot irrigation

Center-pivot irrigation (sometimes called central pivot irrigation), also called waterwheel and circle irrigation, is a method of crop irrigation in which equipment rotates around a pivot and crops are watered with sprinklers. A circular area centered on the pivot is irrigated, often creating a circular pattern in crops when viewed from above (sometimes referred to as "crop circles"). Most center pivots were initially water-powered, and today most are propelled by electric motors.
History.
Center-pivot irrigation was invented in 1940 by farmer Frank Zybach, who lived in Strasburg, Colorado. It was recognized as a method to improve water distribution to fields.
Overview.
Center pivot irrigation is a form of overhead sprinkler irrigation consisting of several segments of pipe (usually galvanized steel or aluminum) joined together and supported by trusses, mounted on wheeled towers with sprinklers positioned along its length. The machine moves in a circular pattern and is fed with water from the pivot point at the center of the circle. The outside set of wheels sets the master pace for the rotation (typically once every three days). The inner sets of wheels are mounted at hubs between two segments and use angle sensors to detect when the bend at the joint exceeds a certain threshold, and thus, the wheels should be rotated to keep the segments aligned. Center pivots are typically less than 1600 feet (500 meters) in length (circle radius) with the most common size being the standard 1/4 mile (400 m) machine. To achieve uniform application, center pivots require an even emitter flow rate across the radius of the machine. Since the outer-most spans (or towers) travel farther in a given time period than the innermost spans, nozzle sizes are smallest at the inner spans and increase with distance from the pivot point. Aerial views show fields of circles created by the watery tracings of "quarter- or half-mile of the center-pivot irrigation pipe," created by center pivot irrigators which use "hundreds and sometimes thousands of gallons a minute." 
Most center pivot systems now have drops hanging from a u-shaped pipe called a "gooseneck" attached at the top of the pipe with sprinkler heads that are positioned a few feet (at most) above the crop, thus limiting evaporative losses and wind drift. There are many different nozzle configurations available including static plate, moving plate and part circle. Pressure regulators are typically installed upstream of each nozzle to ensure each is operating at the correct design pressure. Drops can also be used with drag hoses or bubblers that deposit the water directly on the ground between crops. This type of system is known as LEPA (Low Energy Precision Application) and is often associated with the construction of small dams along the furrow length (termed furrow diking/dyking). Crops may be planted in straight rows or are sometimes planted in circles to conform to the travel of the irrigation system
Originally, most center pivots were water-powered. These were replaced by hydraulic systems and electric motor-driven systems. Most systems today are driven by an electric motor mounted at each tower.
For a center pivot to be used, the terrain needs to be reasonably flat; but one major advantage of center pivots over alternative systems is the ability to function in undulating country. This advantage has resulted in increased irrigated acreage and water use in some areas. The system is in use, for example, in parts of the United States, Australia, New Zealand, Brazil and also in desert areas such as the Sahara and the Middle East.
Benefits.
The center-pivot irrigation system is considered to be a highly efficient system which helps conserve water.
Center pivot irrigation typically uses less water compared to many surface irrigation and furrow irrigation techniques, which reduces the expenditure of and conserves water. It also helps to reduce labor costs compared to some ground irrigation techniques, which are often more labor-intensive. Some ground irrigation techniques involve the digging of channels on the land for the water to flow, whereas the use of center-pivot irrigation can reduce the amount of soil tillage that occurs and helps to reduce water runoff and soil erosion that can occur with ground irrigation. Less tillage encourages more organic materials and crop residue to decompose back into the soil, and reduces soil compaction.
Early settlers of the semiarid High Plains were plagued by crop failures due to cycles of drought, culminating in the disastrous Dust Bowl of the 1930s. It was only after World War II when center pivot irrigation became available that the land mass of the High Plains aquifer system was transformed into one of the most agriculturally productive regions in the world.
Risks: Shrinking Irreplaceable Aquifers.
It is now understood that groundwater level elevation decreases when the rate of extraction by irrigation exceeds the rate of recharge. At some places, the water table was measured to drop more than five feet (1.5 m) per year at the time of maximum extraction. In extreme cases, the deepening of wells was required to reach the steadily falling water table. In the 21st century, recognition of the significance of the aquifer has led to increased coverage from regional and international journalists.
Writer Emily Woodson characterized the increased use of the center pivot irrigation system as part of a profound attitude shift towards modernism (expensive tractors, center-pivot irrigation, dangerous new pesticides) and away from traditional farming that took place in the mid-1970s and 1980s in the United States. A new generation chose high-risk, high-reward crops such as irrigated corn or peanuts, which require large quantities of ground water, fertilizer and chemicals. The new family farm corporations turned many pastures into new cropland and were more interested in rising land prices than water conservation.
A May 2013 "New York Times" article "Wells dry, fertile plains turn to dust" recounts the relentless decline of parts of the High Plains Aquifer System. One of the world's largest aquifers, it covers an area of approximately 174,000 mi² (450,000 km²) in portions of the eight states of South Dakota, Nebraska, Wyoming, Colorado, Kansas, Oklahoma, New Mexico, and Texas, beneath the Great Plains in the United States.
In parts of the United States, sixty years of the profitable business of intensive farming using huge center-pivot irrigators has emptied parts of the High Plains Aquifer. It would take hundreds to thousands of years of rainfall to replace the groundwater in the dried up aquifer. In 1950, irrigated cropland covered 250,000 acres. With the use of center-pivot irrigation, nearly three million acres of land were irrigated in Kansas alone. In some places in the Texas Panhandle, the water table has been drained (dewatered). "Vast stretches of Texas farmland lying over the aquifer no longer support irrigation. In west-central Kansas, up to a fifth of the irrigated farmland along a 100 mi swath of the aquifer has already gone dry."
Center pivot manufacturers.
Over the 30 years after World War II, there were at least 60 center pivot irrigation manufacturers created in the United States. As of 2010, it has been reported that there are fewer than 12 U.S. manufacturers, of which five are major: Valmont Industries and their "Valley" products, Lindsay Corporation and their "Zimmatic" brand, Reinke Irrigation with their "Electrogator" machines, T-L Irrigation who make a hydrostatically powered system and Pierce Corporation and their "CircleMaster" products. Valmont, Lindsay, Reinke, and Pierce Corporation all manufacture systems powered by 480 V electricity. T-L's variable-displacement hydraulic pump which is typically driven by an electric, natural gas or diesel motor on standard quarter section pivots. Water application typically consist of brass impact sprinklers, drip tubes, rotating nozzles, or stationary sprays. These sprinklers are manufactured by Nelson Irrigation, Senninger Irrigation but also European as Komet irrigation. Varying applications, soils, and crops require different volumes of water and application rates. Pivots often have a large bore impact sprinkler (called "big guns") located on the very end of the machine to aid in irrigating most number of acres possible. While these "end guns" may dramatically increase the irrigated area they suffer from poor uniformity and may have negative impacts on the entire pivot if not designed properly.
The largest maker of mechanized irrigation market is Valmont Industries. Valmont claims a market share between 35–45% of all new center pivots sold in the United States. Reinke is a privately held company which limits the ability for market researches to determine the exact number of center pivot sold. Reinke and Zimmatic compete to share between 30–35% of the irrigation market. Valley, Zimmatic, and Reinke manufacture modern irrigation equipment and consume about 75% and support networks of professional dealers. T-L Irrigation, also privately held, manufactures a hydraulically driven irrigator and markets throughout the United States and in more than 55 countries through independent agriculturally oriented equipment dealers (market share 25%).
Today, many countries use center pivot irrigation. By the mid-1970s Valmont began manufacturing a significant amount of irrigation systems in Europe, the Middle East, Africa, Australia, China, Thailand, Latin America and Switzerland.
Linear/lateral move irrigation machines.
The above-mentioned equipment can also be configured to move in a straight line where it is termed a "linear move", "lateral move", "wheelmove" or "side-roll" irrigation system. In this case the water is supplied by an irrigation channel running the length of the field and positioned either at one side or midway across the field width. The motor and pump equipment is mounted on a cart adjacent to the supply channel that travels with the machine. Farmers may opt for linear moves to conform to existing rectangular field designs such as those converting from furrow irrigation. Lateral moves are far less common, rely on more complex guidance systems, and require additional management compared to center pivot systems. Lateral moves are common in Australia and typically range between 500 and 1,000 meters in length.

</doc>
<doc id="52376" url="http://en.wikipedia.org/wiki?curid=52376" title="Axiom of extensionality">
Axiom of extensionality

In axiomatic set theory and the branches of logic, mathematics, and computer science that use it, the axiom of extensionality, or axiom of extension, is one of the axioms of Zermelo–Fraenkel set theory.
Formal statement.
In the formal language of the Zermelo–Fraenkel axioms, the axiom reads:
or in words:
The converse, formula_2, of this axiom follows from the substitution property of equality.
Interpretation.
To understand this axiom, note that the clause in parentheses in the symbolic statement above simply states that "A" and "B" have precisely the same members.
Thus, what the axiom is really saying is that two sets are equal if and only if they have precisely the same members.
The essence of this is:
The axiom of extensionality can be used with any statement of the form
formula_3,
where "P" is any unary predicate that does not mention "A", to define a unique set formula_4 whose members are precisely the sets satisfying the predicate formula_5.
We can then introduce a new symbol for formula_4; it's in this way that definitions in ordinary mathematics ultimately work when their statements are reduced to purely set-theoretic terms.
The axiom of extensionality is generally uncontroversial in set-theoretical foundations of mathematics, and it or an equivalent appears in just about any alternative axiomatisation of set theory.
However, it may require modifications for some purposes, as below.
In predicate logic without equality.
The axiom given above assumes that equality is a primitive symbol in predicate logic.
Some treatments of axiomatic set theory prefer to do without this, and instead treat the above statement not as an axiom but as a "definition" of equality.
Then it is necessary to include the usual axioms of equality from predicate logic as axioms about this defined symbol. Most of the axioms of equality still follow from the definition; the remaining one is
and it becomes "this" axiom that is referred to as the axiom of extensionality in this context.
In set theory with ur-elements.
An ur-element is a member of a set that is not itself a set.
In the Zermelo–Fraenkel axioms, there are no ur-elements, but they are included in some alternative axiomatisations of set theory.
Ur-elements can be treated as a different logical type from sets; in this case, formula_8 makes no sense if formula_4 is an ur-element, so the axiom of extensionality simply applies only to sets.
Alternatively, in untyped logic, we can require formula_8 to be false whenever formula_4 is an ur-element.
In this case, the usual axiom of extensionality would then imply that every ur-element is equal to the empty set.
To avoid this consequence, we can modify the axiom of extensionality to apply only to nonempty sets, so that it reads:
That is:
Yet another alternative in untyped logic is to define formula_4 itself to be the only element of formula_4
whenever formula_4 is an ur-element. While this approach can serve to preserve the axiom of extensionality, the axiom of regularity will need an adjustment instead.

</doc>
<doc id="52381" url="http://en.wikipedia.org/wiki?curid=52381" title="Thermite">
Thermite

Thermite is a pyrotechnic composition of metal powder, fuel and metal oxide. When ignited by heat, thermite undergoes an exothermic reduction-oxidation reaction. Most varieties are not explosive but can create brief bursts of high temperature in a small area. Its form of action is similar to that of other fuel-oxidizer mixtures, such as black powder.
Thermites have diverse compositions. Fuels include aluminium, magnesium, titanium, zinc, silicon, and boron. Aluminium is common because of its high boiling point and low cost. Oxidizers include bismuth(III) oxide, boron(III) oxide, silicon(IV) oxide, chromium(III) oxide, manganese(IV) oxide, iron(III) oxide, iron(II,III) oxide, copper(II) oxide, and lead(II,IV) oxide.
The reaction is used for thermite welding, often used to join rail tracks. Thermites have also been used in metal refining, demolition of munitions, and in incendiary weapons. Some thermite-like mixtures are used as pyrotechnic initiators in fireworks.
Chemical reactions.
In the following example, elemental aluminium reduces the oxide of another metal, in this common example iron oxide, because aluminium forms stronger, more stable, bonds with oxygen than iron:
The products are aluminium oxide, elemental iron, and a large amount of heat. The reactants are commonly powdered and mixed with a binder to keep the material solid and prevent separation.
Other metal oxides can be used, such as chromium oxide, to generate the given metal in its elemental form. For example, a copper thermite reaction using copper oxide and elemental aluminium can be used for creating electric joints in a process called cadwelding that produces elemental copper (it may react violently):
Thermites with nanosized particles are described by a variety of terms, such as metastable intermolecular composites, super-thermite, nano-thermite, and nanocomposite energetic materials.
History.
The thermite ("thermit") reaction was discovered in 1893 and patented in 1895 by German chemist Hans Goldschmidt. Consequently, the reaction is sometimes called the "Goldschmidt reaction" or "Goldschmidt process". Goldschmidt was originally interested in producing very pure metals by avoiding the use of carbon in smelting, but he soon discovered the value of thermite in welding.
The first commercial application of thermite was the welding of tram tracks in Essen in 1899.
Types.
Red iron(III) oxide (Fe2O3, commonly known as rust) is the most common iron oxide used in thermite. Magnetite also works. Other oxides are occasionally used, such as MnO2 in manganese thermite, Cr2O3 in chromium thermite, quartz in silicon thermite, or copper(II) oxide in copper thermite, but only for specialized purposes. All of these examples use aluminium as the reactive metal. Fluoropolymers can be used in special formulations, Teflon with magnesium or aluminium being a relatively common example. Magnesium/teflon/viton is another pyrolant of this type.
In principle, any reactive metal could be used instead of aluminium. This is rarely done, because the properties of aluminium are nearly ideal for this reaction:
Although the reactants are stable at room temperature, they burn with an extremely intense exothermic reaction when they are heated to ignition temperature. The products emerge as liquids due to the high temperatures reached (up to 2500 °C with iron(III) oxide)—although the actual temperature reached depends on how quickly heat can escape to the surrounding environment. Thermite contains its own supply of oxygen and does not require any external source of air. Consequently, it cannot be smothered and may ignite in any environment, given sufficient initial heat. It will burn well while wet and cannot be easily extinguished with water, although enough water will remove heat and may stop the reaction. Small amounts of water will boil before reaching the reaction. Even so, thermite is used for welding underwater.
The thermites are characterized by almost complete absence of gas production during burning, high reaction temperature, and production of molten slag. The fuel should have high heat of combustion and produce oxides with low melting point and high boiling point. The oxidizer should contain at least 25% oxygen, have high density, low heat of formation, and produce metal with low melting and high boiling point (so the energy released is not consumed in evaporation of reaction products). Organic binders can be added to the composition to improve its mechanical properties, however they tend to produce endothermic decomposition products, causing some loss of reaction heat and production of gases.
The temperature achieved during the reaction determines the outcome. In an ideal case, the reaction produces a well-separated melt of metal and slag. For this, the temperature has to be high enough to melt both the reaction products, the resulting metal and the fuel oxide. Too low temperature will result in a mixture of sintered metal and slag, too high temperature – above boiling point of any reactant or product – will lead to rapid production of gas, dispersing the burning reaction mixture, sometimes with effects similar to a low-yield explosion. In compositions intended for production of metal by aluminothermic reaction, these effects can be counteracted. Too low reaction temperature (e.g. when producing silicon from sand) can be boosted with addition of a suitable oxidizer (e.g. sulfur in aluminium-sulfur-sand compositions), too high temperatures can be reduced by using a suitable coolant and/or slag flux. The flux often used in amateur compositions is calcium fluoride, as it reacts only minimally, has relatively low melting point, low melt viscosity at high temperatures (therefore increasing fluidity of the slag) and forms a eutectic with alumina. Too much of flux however dilutes the reactants to the point of not being able to sustain combustion. The type of metal oxide also has dramatic influence to the amount of energy produced; the higher the oxide, the higher the amount of energy produced. A good example is the difference between manganese(IV) oxide and manganese(II) oxide, where the former produces too high temperature and the latter is barely able to sustain combustion; to achieve good results a mixture with proper ratio of both oxides should be used.
The reaction rate can be also tuned with particle sizes; coarser particles burn slower than finer particles. The effect is more pronounced with the particles requiring being heated to higher temperature to start reacting. This effect is pushed to the extreme with nano-thermites.
The temperature achieved in the reaction in adiabatic conditions, when no heat is lost to the environment, can be estimated using the Hess's law – by calculating the energy produced by the reaction itself (subtracting the enthalpy of the reactants from the enthalpy of the products) and subtracting the energy consumed to heating the products (from their specific heat, when the materials only change their temperature, and their enthalpy of fusion and eventually enthalpy of vaporization, when the materials melt or boil). In real conditions, the reaction loses heat to the environment, the achieved temperature is therefore somewhat lower. The heat transfer rate is finite, so the faster the reaction is, the closer to adiabatic condition it runs and the higher is the achieved temperature.
Iron thermite.
The most common composition is the iron thermite. The oxidizer used is usually either iron(III) oxide or iron(II,III) oxide. The former produces more heat. The latter is easier to ignite, likely due to the crystal structure of the oxide. Addition of copper or manganese oxides can significantly improve the ease of ignition.
The original mixture, as invented, used iron oxide in the form of mill scale. The composition was very difficult to ignite.
Copper thermite.
Copper thermite can be prepared using either copper(I) oxide (Cu2O, red) or copper(II) oxide (CuO, black). The burn rate tends to be very fast and the melting point of copper is relatively low so the reaction produces a significant amount of molten copper in a very short time. Copper(II) thermite reactions can be so fast that copper thermite can be considered a type of flash powder. An explosion can occur and send a spray of copper drops to considerable distance.
Copper(I) thermite has industrial uses in e.g. welding of thick copper conductors ("cadwelding"). This kind of welding is being evaluated also for cable splicing on the US Navy fleet, for use in high-current systems, e.g. electric propulsion.
Thermates.
Thermate composition is a thermite one enriched with a salt-based oxidizer (usually nitrates, e.g. barium nitrate, or peroxides). In contrast with thermites, thermates burn with evolution of flame and gases. The presence of the oxidizer makes the mixture easier to ignite and improves penetration of target by the burning composition, as the evolved gas is projecting the molten slag and providing mechanical agitation. This mechanism makes thermate more suitable than thermite for incendiary purposes and for emergency destruction of sensitive equipment (e.g. cryptographic devices), as thermite's effect is more localized.
Ignition.
Metals are capable of burning under the right conditions, similar to the combustion process of wood or gasoline. In fact, rust is the result of oxidation of steel or iron at very slow rates. A thermite reaction is a process in which the correct mixture of metallic fuels are combined and ignited. Ignition itself requires extremely high temperatures.
Ignition of a thermite reaction normally requires a sparkler or easily obtainable magnesium ribbon, but may require persistent efforts, as ignition can be unreliable and unpredictable. These temperatures cannot be reached with conventional black powder fuses, nitrocellulose rods, detonators, pyrotechnic initiators, or other common igniting substances. Even when the thermite is hot enough to glow bright red, it will not ignite as it must be at or near white-hot to initiate the reaction. It is possible to start the reaction using a propane torch if done correctly.
Often, strips of magnesium metal are used as fuses. Because metals burn without releasing cooling gases, they can potentially burn at extremely high temperatures. Reactive metals such as magnesium can easily reach temperatures sufficiently high for thermite ignition. Magnesium ignition remains popular among amateur thermite users, mainly because it can be easily obtained.
The reaction between potassium permanganate and glycerol or ethylene glycol is used as an alternative to the magnesium method. When these two substances mix, a spontaneous reaction will begin, slowly increasing the temperature of the mixture until flames are produced. The heat released by the oxidation of glycerine is sufficient to initiate a thermite reaction.
Apart from magnesium ignition, some amateurs also choose to use sparklers to ignite the thermite mixture. These reach the necessary temperatures and provide enough time before the burning point reaches the sample. This can be a dangerous method, as the iron sparks, like the magnesium strips, burn at thousands of degrees and can ignite the thermite even though the sparkler itself is not in contact with it. This is especially dangerous with finely powdered thermite.
Similarly, finely powdered thermite can be ignited by a flint spark lighter, as the sparks are burning metal (in this case, the highly reactive rare-earth metals lanthanum and cerium). Therefore it is unsafe to strike a lighter close to thermite.
Civilian uses.
Thermite reactions have many uses. Thermite is not an explosive; instead it operates by exposing a very small area of metal to extremely high temperatures. Intense heat focused on a small spot can be used to cut through metal or weld metal components together both by melting metal from the components, and by injecting molten metal from the thermite reaction itself.
Thermite may be used for repair by the welding in-place of thick steel sections such as locomotive axle-frames where the repair can take place without removing the part from its installed location.
Thermite can be used for quickly cutting or welding steel such as rail tracks, without requiring complex or heavy equipment. However, defects such as slag inclusions and voids (holes) are often present in such welded junctions and great care is needed to operate the process successfully. Care must also be taken to ensure that the rails remain straight, without resulting in dipped joints, which can cause wear on high speed and heavy axle load lines.
A thermite reaction, when used to purify the ores of some metals, is called the thermite process, or aluminothermic reaction. An adaptation of the reaction, used to obtain pure uranium, was developed as part of the Manhattan Project at Ames Laboratory under the direction of Frank Spedding. It is sometimes called the Ames process.
Copper thermite is used for welding together thick copper wires for the purpose of electrical connections. It is used extensively by the electrical utilities and telecommunications industries (exothermic welded connections).
Military uses.
Thermite hand grenades and charges are typically used by armed forces in both an anti-materiel role and in the partial destruction of equipment; the latter being common when time is not available for safer or more thorough methods. For example, thermite can be used for the emergency destruction of cryptographic equipment when there is a danger that it might be captured by enemy troops. Because standard iron-thermite is difficult to ignite, burns with practically no flame and has a small radius of action, standard thermite is rarely used on its own as an incendiary composition. It is more usually employed with other ingredients added to increase its incendiary effects. Thermate-TH3 is a mixture of thermite and pyrotechnic additives which have been found to be superior to standard thermite for incendiary purposes. Its composition by weight is generally about 68.7% thermite, 29.0% barium nitrate, 2.0% sulfur, and 0.3% of a binder (such as PBAN). The addition of barium nitrate to thermite increases its thermal effect, produces a larger flame, and significantly reduces the ignition temperature. Although the primary purpose of Thermate-TH3 by the armed forces is as an incendiary anti-material weapon, it also has uses in welding together metal components.
A classic military use for thermite is disabling artillery pieces, and it has been used for this purpose since World War II; such as at Pointe du Hoc, Normandy. Thermite can permanently disable artillery pieces without the use of explosive charges, and therefore thermite can be used when silence is necessary to an operation. This can be done by inserting one or more armed thermite grenades into the breech and then quickly closing it; this welds the breech shut and makes loading the weapon impossible. Alternatively, a thermite grenade discharged inside the barrel of the gun will foul the barrel, making the weapon very dangerous to fire; thermite can also be used to weld the traversing and elevation mechanism of the weapon, making it impossible to aim properly.
Thermite was used in both German and Allied incendiary bombs during World War II. Incendiary bombs usually consisted of dozens of thin thermite-filled canisters (bomblets) ignited by a magnesium fuse. Incendiary bombs created massive damage in many cities due to fires started by the thermite. Cities that primarily consisted of wooden buildings were especially susceptible. These incendiary bombs were utilized primarily during nighttime air raids. Bombsights could not be used at night, creating the need to use munitions that could destroy targets without the need for precision placement.
Hazards.
Thermite usage is hazardous due to the extremely high temperatures produced and the extreme difficulty in smothering a reaction once initiated. Small streams of molten iron released in the reaction can travel considerable distances and may melt through metal containers, igniting their contents (see images). Additionally, flammable metals with relatively low boiling points such as zinc (with a boiling point of 907 °C, which is about 1,370 °C below the temperature at which thermite burns) could potentially spray superheated boiling metal violently into the air if near a thermite reaction.
If, for some reason, thermite is contaminated with organics, hydrated oxides and other compounds able to produce gases upon heating or reaction with thermite components, the reaction products may be sprayed. Moreover, if the thermite mixture contains enough empty spaces with air and burns fast enough, the super-heated air also may cause the mixture to spray. For this reason it is preferable to use relatively crude powders, so the reaction rate is moderate and hot gases could escape the reaction zone.
Preheating of thermite before ignition can easily be done accidentally, for example by pouring a new pile of thermite over a hot, recently ignited pile of thermite slag. When ignited, preheated thermite can burn almost instantaneously, releasing light and heat energy at a much higher rate than normal and causing burns and eye damage at what would normally be a reasonably safe distance.
The thermite reaction can take place accidentally in industrial locations where abrasive grinding and cutting wheels are used with ferrous metals. Using aluminium in this situation produces a mixture of oxides which is capable of a violent explosive reaction.
Mixing water with thermite or pouring water onto burning thermite can cause a steam explosion, spraying hot fragments in all directions.
Thermite's main ingredients were also utilized for their individual qualities, specifically reflectivity and heat insulation, in a paint coating or dope for the German zeppelin "Hindenburg", possibly contributing to its fiery destruction. This was a theory put forward by the former NASA scientist Addison Bain, and later tested in small scale by the scientific reality-TV show "MythBusters" with semi-inconclusive results (it was proven not to be the fault of the thermite reaction alone, but instead conjectured to be a combination of that and the burning of hydrogen gas that filled the body of the "Hindenburg"). The "MythBusters" program also tested the veracity of a video found on the Internet, whereby a quantity of thermite was allowed to drop onto a block of ice of similar mass, causing a sudden explosion. They were able to confirm the results, finding huge chunks of ice as far as 50m from the point of explosion. Co-host Jamie Hyneman conjectured that this was due to the thermite mixture aerosolizing, perhaps in a cloud of steam, causing it to burn even faster. Hyneman also voiced skepticism about another theory explaining the phenomenon: that the reaction somehow separated the hydrogen and oxygen in the ice and then ignited them. A far more likely explanation is that the explosion is due to the reaction of high temperature molten aluminium with water. Aluminium reacts violently with water or steam at high temperatures, releasing hydrogen and oxidizing in the process. The speed of that reaction and the ignition of the resulting hydrogen can easily account for the explosion verified.<ref name="Aluminium Times Vol 11 n 3 Jul/Aug 2009"></ref> This process is akin to the explosive reaction caused by dropping metallic potassium into water.
Further reading.
</dl>

</doc>
<doc id="52382" url="http://en.wikipedia.org/wiki?curid=52382" title="Watergate scandal">
Watergate scandal

The Watergate scandal was a major political scandal that occurred in the United States in the 1970s as a result of the June 17, 1972, break-in at the Democratic National Committee (DNC) headquarters at the Watergate office complex in Washington, D.C., and the Nixon administration's attempted cover-up of its involvement. When the conspiracy was discovered and investigated by the U.S. Congress, the Nixon administration's resistance to its probes led to a constitutional crisis. The term "Watergate" has come to encompass an array of clandestine and often illegal activities undertaken by members of the Nixon administration. Those activities included such "dirty tricks" as bugging the offices of political opponents and people of whom Nixon or his officials were suspicious. Nixon and his close aides ordered harassment of activist groups and political figures, using the Federal Bureau of Investigation (FBI), the Central Intelligence Agency (CIA), and the Internal Revenue Service (IRS). The scandal led to the discovery of multiple abuses of power by the Nixon administration, articles of impeachment, and the resignation of Richard Nixon, the President of the United States. The scandal also resulted in the indictment of 69 people, with trials or pleas resulting in 48 being found guilty and incarcerated, many of whom were Nixon's top administration officials.
The affair began with the arrest of five men for breaking and entering into the DNC headquarters at the Watergate complex on June 17, 1972. The FBI connected cash found on the burglars to a slush fund used by the Committee for the Re-Election of the President (CRP), the official organization of Nixon's campaign. In July 1973, as evidence mounted against the President's staff, including testimony provided by former staff members in an investigation conducted by the Senate Watergate Committee, it was revealed that President Nixon had a tape-recording system in his offices and that he had recorded many conversations. After a protracted series of bitter court battles, the U.S. Supreme Court unanimously ruled that the president had to hand over the tapes to government investigators; he eventually complied. Recordings from these tapes implicated the president, revealing he had attempted to cover up the questionable goings-on that had taken place after the break-in.
Facing near-certain impeachment in the House of Representatives and equally certain conviction by the Senate, Nixon resigned the presidency on August 9, 1974. On September 8, 1974, his successor, Gerald Ford, pardoned him.
The name "Watergate" and the suffix "-gate" have since become synonymous with political scandals in the United States and in other English- and non-English-speaking nations.
Wiretapping of the Democratic Party's headquarters.
In January 1972, G. Gordon Liddy, general counsel to the Committee for the Re-Election of the President (CRP), presented a campaign intelligence plan to CRP's Acting Chairman Jeb Stuart Magruder, Attorney General John Mitchell, and Presidential Counsel John Dean, that involved extensive illegal activities against the Democratic Party. According to Dean, this marked "the opening scene of the worst political scandal of the twentieth century and the beginning of the end of the Nixon presidency".
Mitchell viewed the plan as unrealistic, but two months later is alleged to have approved a reduced version of the plan which involved burgling the Democratic National Committee's (DNC) headquarters at the Watergate Complex in Washington, D.C., ostensibly to photograph documents and install listening devices. Liddy was nominally in charge of the operation, but has since insisted that he was duped by Dean and at least two of his subordinates. These included former CIA officers E. Howard Hunt and James McCord, then-CRP Security Coordinator (John Mitchell had by then resigned as Attorney General to become chairman of the CRP).
Two phones inside the offices of the DNC headquarters were wiretapped. One of those phones was the phone of Robert Spencer Oliver who at the time was working as the executive director of the Association of State Democratic Chairmen and the other was the phone of DNC secretary Larry O'Brien.
After two attempts to break into the Watergate Complex failed to yield information of value, the order for yet another break-in was given to Liddy by Jeb Magruder, either acting on his own or on orders from Dean.
Shortly after midnight on June 17, 1972, Frank Wills, a security guard at the Watergate Complex, noticed tape covering the latches on some of the doors in the complex leading from the underground parking garage to several offices (allowing the doors to close but remain unlocked). He removed the tape, and thought nothing of it. He returned an hour later, and having discovered that someone had retaped the locks, Wills called the police. Five men were discovered and arrested inside the DNC office. They were Virgilio González, Bernard Barker, James McCord, Eugenio Martínez, and Frank Sturgis, who were charged with attempted burglary and attempted interception of telephone and other communications. On September 15, a grand jury indicted them, as well as Hunt and Liddy, for conspiracy, burglary, and violation of federal wiretapping laws. The five burglars who broke into the office were tried by Judge John Sirica and were convicted on January 30, 1973.
Coverup and its unraveling.
Initial coverup.
Within hours of the burglars' arrest, the FBI discovered the name of E. Howard Hunt in the address books of Barker and Martínez. Nixon administration officials were concerned because Hunt and Liddy were also involved in another secret activity, known as the White House Plumbers, which was set up to stop security "leaks" and to investigate other sensitive security matters. Dean would later testify he was ordered by top Nixon aide John Ehrlichman to "deep six" the contents of Howard Hunt's White House safe. Ehrlichman subsequently denied that and, in the end, the evidence from Hunt's safe was destroyed (in separate operations) by Dean and the FBI's Acting Director, L. Patrick Gray.
Nixon's own reaction to the break-in, at least initially, was one of skepticism. Watergate prosecutor James Neal was sure Nixon had not known in advance of the break-in. As evidence, he cited a June 23 taped conversation between the President and his Chief of Staff, H. R. Haldeman, in which Nixon asked, "Who was the ******* who ordered it?" Nonetheless, Nixon subsequently ordered Haldeman to have the CIA block the FBI's investigation into the source of the funding for the burglary.
A few days later, Nixon's Press Secretary, Ron Ziegler, described the event as "a third-rate burglary attempt". On August 29, at a news conference, President Nixon stated Dean had conducted a thorough investigation of the matter, when in fact Dean had not conducted any investigation at all. Nixon also said, "I can say categorically that... no one in the White House staff, no one in this Administration, presently employed, was involved in this very bizarre incident". On September 15, Nixon congratulated Dean, saying, "The way you've handled it, it seems to me, has been very skillful, because you—putting your fingers in the dikes every time that leaks have sprung here and sprung there."
Money trail.
On June 19, 1972, it was publicly revealed that one of the Watergate burglars was a Republican Party security aide. Former Attorney General John Mitchell, who at the time was the head of the Nixon re-election campaign (CRP), denied any involvement with the Watergate break-in or knowledge of the five burglars. On August 1, a $25,000 cashier's check earmarked for the Nixon re-election campaign was found in the bank account of one of the Watergate burglars. Further investigation by the FBI would reveal still more thousands had supported their travel and expenses in the months leading up to their arrests. Examination showed links to the finance committee of CRP.
Several donations (totaling $86,000) were made by individuals who thought they were making private donations by certified and cashier's checks for the President's re-election. Investigators' examination of the bank records of a Miami company run by Watergate burglar Barker revealed an account controlled by him personally had deposited a check and then transferred it (through the Federal Reserve Check Clearing System).
The banks that had originated the checks were keen to ensure the depository institution used by Barker had acted properly in ensuring the checks had been received and endorsed by the check's payee, before its acceptance for deposit in Bernard Barker's account. Only in this way would the issuing banks not be held liable for the unauthorized and improper release of funds from their customer's accounts.
The investigation by the FBI, which cleared Barker's bank of fiduciary malfeasance, led to the direct implication of members of the CRP, to whom the checks had been delivered. Those individuals were the Committee bookkeeper and its treasurer, Hugh Sloan.
The Committee, as a private organization, followed normal business practice in allowing only duly authorized individual(s) to accept and endorse on behalf of the Committee. Therefore, no financial institution could accept or process a check on behalf of the Committee unless it had been endorsed and by a duly authorized individual. On the checks deposited into Barker's bank account was the endorsement of Committee treasurer Hugh Sloan, who was authorized by the Finance Committee. But once Sloan had endorsed a check made payable to the Committee, he had a legal and fiduciary responsibility to see that the check was deposited into the accounts which were named on the check and only the accounts so named. Sloan failed to do that. When he was confronted with the potential charge of federal bank fraud, he revealed he had been directed by Committee deputy director Jeb Magruder and finance director Maurice Stans to give the money to G. Gordon Liddy.
Liddy gave the money to Barker and attempted to hide its origin. Barker had attempted to disguise the funds by depositing them into bank accounts which were located in banks outside of the United States. What Barker, Liddy, and Sloan did not know was that the complete record of all such transactions were held for roughly six months. Barker's use of foreign banks to deposit checks and withdraw the funds via cashier's checks and money orders in April and May 1972 guaranteed the banks would keep the entire transaction records until October and November 1972.
All five of the Watergate burglars were directly or indirectly tied to the 1972 CRP, thus causing Judge Sirica to suspect a conspiracy involving higher-echelon government officials.
On September 29, 1972, it was revealed that John Mitchell, while serving as Attorney General, controlled a secret Republican fund used to finance intelligence-gathering against the Democrats. On October 10, the FBI reported the Watergate break-in was only part of a massive campaign of political spying and sabotage on behalf of the Nixon re-election committee. Despite these revelations, Nixon's campaign was never seriously jeopardized, and on November 7, the President was re-elected in one of the biggest landslides in American political history.
Role of the media.
The connection between the break-in and the re-election committee was highlighted by media coverage—in particular, investigative coverage by "The Washington Post", "Time", and "The New York Times". The coverage dramatically increased publicity and consequent political repercussions. Relying heavily upon anonymous sources, "Post" reporters Bob Woodward and Carl Bernstein uncovered information suggesting that knowledge of the break-in, and attempts to cover it up, led deeply into the Justice Department, the FBI, the CIA, and the White House.
Chief among the "Post's" anonymous sources was an individual whom Woodward and Bernstein had nicknamed Deep Throat; 33 years later, in 2005, the informant was confirmed to be William Mark Felt, Sr., who was then the deputy director of the FBI. Felt met secretly with Woodward several times, telling him of Howard Hunt's involvement with the Watergate break-in, and that the White House staff regarded the stakes in Watergate extremely high. Felt warned Woodward that the FBI wanted to know where he and other reporters were getting their information, as they were uncovering a wider web of crimes than first disclosed. In one of their last meetings, all of which took place at an underground parking garage somewhere in Rosslyn, Virginia, at 2 a.m., Felt cautioned Woodward that he might be followed and not to trust their phone conversations to be secure. Prior to his resignation from the FBI on June 22, 1973, Felt also planted leaks about Watergate to "Time" magazine, the "Washington Daily News" and other publications.
During this early period, most of the media failed to grasp the full implications of the scandal, and concentrated reporting on other topics related to the 1972 presidential election. After the revelation that one of the convicted burglars wrote to Judge Sirica alleging a high-level coverup, the media shifted its focus. "Time" magazine described Nixon as undergoing "daily hell and very little trust". The distrust between the press and the Nixon administration was mutual and greater than usual due to lingering dissatisfaction with events from the Vietnam War. Public distrust of the media reached over 40%.
Nixon and top administration officials discussed using government agencies to "get" what they perceived as hostile media organizations. The discussions had precedent. At the request of Nixon's White House in 1969, the FBI tapped the phones of five reporters. In 1971, the White House requested an audit of the tax return of the editor of "Newsday", after he wrote a series of articles about the financial dealings of Charles Rebozo, a friend of Nixon's.
The administration and its supporters accused the media of making "wild accusations", putting too much emphasis on the story, and of having a liberal bias against the Administration. Nixon said in a May 1974 interview with supporter Baruch Korff that if he had followed the liberal policies that he thought the media preferred, "Watergate would have been a blip". The media noted that most of the reporting turned out to be accurate and the competitive nature of the media guaranteed massive coverage of the political scandal. Applications to journalism schools reached an all-time high in 1974.
Scandal blows wide open.
Nixon's conversations in late March 1973 and all of April reveal that not only did he know he needed to remove Haldeman, Ehrlichman, and Dean, but he had to do so in a way that he was least likely to incriminate himself and his presidency. Nixon created a new conspiracy—to effect a cover-up of the cover-up—which began in late March 1973 and became fully formed in May and June 1973, operating until his presidency ended in August 9, 1974. On March 23, 1973, Judge Sirica read the court a letter from Watergate burglar James McCord alleging perjury had been committed in the Watergate trial, and defendants had been pressured to remain silent. Trying to make them talk, Sirica gave Hunt and two burglars provisional sentences of up to 40 years. On March 28, on Nixon's orders, aide John Ehrlichman told attorney general Richard Kleindienst that nobody in the White House had prior knowledge of the burglary. On April 13, Magruder told U.S. attorneys that he had perjured himself during the burglars' trial, and implicated John Dean and John Mitchell.
John Dean's belief was that he, Mitchell, Ehrlichman and Haldeman could go to the prosecutors, tell the truth and save the presidency. Dean wanted to protect the presidency and have his four closest men take the fall for telling the truth. During the critical meeting with Dean and Nixon on April 15, 1973, Dean was totally unaware of the president's depth of knowledge and involvement in the Watergate cover-up calamity. It was during this meeting that John Dean got the feeling he was being recorded. He wondered this due to the way Nixon was speaking and trying to prod recollections of earlier troublesome conversations about fundraising. Dean's mention of this curiosity while before the Senate Committee on Watergate, would expose the thread that would unravel the fabric of Watergate.
Two days later, Dean told Nixon that he had been cooperating with the U.S. attorneys. On that same day, U.S. attorneys told Nixon that Haldeman, Ehrlichman, Dean and other White House officials were implicated in the coverup.
On April 30, Nixon asked for the resignation of H. R. Haldeman and John Ehrlichman, two of his most influential aides, both of whom were indicted, convicted, and ultimately sentenced to prison. He asked for the resignation of Attorney General Kleindienst, to ensure no one could claim that his innocent friendship with Haldeman and Ehrlichman could be construed as a conflict. He fired White House Counsel John Dean, who went on to testify before the Senate Watergate Committee and clearly stated that he believed and suspected the conversations in the Oval Office were being taped. This information became the bombshell that helped force Richard Nixon to eventually resign rather than be impeached.
Writing from prison for "New West" and "New York" magazines in 1977, Ehrlichman claimed Nixon had offered him a large sum of money, which he declined.
The President announced the resignations in an address to the American people:
In one of the most difficult decisions of my Presidency, I accepted the resignations of two of my closest associates in the White House, Bob Haldeman, John Ehrlichman, two of the finest public servants it has been my privilege to know. Because Attorney General Kleindienst, though a distinguished public servant, my personal friend for 20 years, with no personal involvement whatsoever in this matter has been a close personal and professional associate of some of those who are involved in this case, he and I both felt that it was also necessary to name a new Attorney General. The Counsel to the President, John Dean, has also resigned.
On the same day, Nixon appointed a new attorney general, Elliot Richardson, and gave him authority to designate a special counsel for the Watergate investigation who would be independent of the regular Justice Department hierarchy. In May 1973, Richardson named Archibald Cox to the position.
Senate Watergate hearings and revelation of the Watergate tapes.
On February 7, 1973, the United States Senate voted 77–0 to approve Senate Resolution and establish a select committee to investigate Watergate, with Sam Ervin named chairman the next day. The hearings held by the Senate committee, in which Dean and other former administration officials testified, were broadcast from May 17 to August 7, 1973. The three major networks of the time agreed to take turns covering the hearings live, each network thus maintaining coverage of the hearings every third day, starting with ABC on May 17 and ending with NBC on August 7. An estimated 85% of Americans with television sets tuned into at least one portion of the hearings.
On Friday, July 13, 1973, during a preliminary interview, deputy minority counsel Donald Sanders asked White House assistant Alexander Butterfield if there was any type of recording system in the White House.
Butterfield said he was reluctant to answer, but finally stated there was a new system in the White House that automatically recorded everything in the Oval Office, the Cabinet Room and others, as well as Nixon's private office in the Old Executive Office Building.
On Monday, July 16, 1973, in front of a live, televised audience, chief minority counsel Fred Thompson asked Butterfield whether he was "aware of the installation of any listening devices in the Oval Office of the President". Butterfield's revelation of the taping system transformed the Watergate investigation yet again. Cox immediately subpoenaed the tapes, as did the Senate, but Nixon refused to release them, citing his executive privilege as president, and ordered Cox to drop his subpoena. Cox refused.
"Saturday Night Massacre".
On October 20, 1973, after Cox refused to drop the subpoena, Nixon demanded the resignations of attorney general Richardson and his deputy William Ruckelshaus for refusing to fire the special prosecutor. Nixon's search for someone in the Justice Department willing to fire Cox ended with the Solicitor General Robert Bork. Though Bork claims to believe Nixon's order to be valid and appropriate, he considered resigning to avoid being "perceived as a man who did the President's bidding to save my job". Bork carried out the presidential order and dismissed the special prosecutor.
These actions met considerable public criticism. Responding to the allegations of possible wrongdoing, in front of 400 Associated Press managing editors on November 17, 1973, Nixon stated emphatically, "I'm not a crook". He needed to allow Bork to appoint a new special prosecutor; Bork chose Leon Jaworski to continue the investigation.
Legal action against Nixon Administration members.
On March 1, 1974, a grand jury in Washington, D.C., indicted several former aides of President Nixon, who became known as the "Watergate Seven": Haldeman, Ehrlichman, Mitchell, Charles Colson, Gordon C. Strachan, Robert Mardian and Kenneth Parkinson, for conspiring to hinder the Watergate investigation. The grand jury secretly named President Nixon as an unindicted co-conspirator. The special prosecutor dissuaded them from an indictment of Nixon, arguing that a President can only be indicted after he leaves office. John Dean, Jeb Stuart Magruder, and other figures already had pleaded guilty. On April 5, 1974, Dwight Chapin, the former Nixon appointments secretary, was convicted of lying to the grand jury. Two days later, the same grand jury indicted Ed Reinecke, the Republican lieutenant governor of California, on three charges of perjury before the Senate committee.
Release of the transcripts.
The Nixon administration struggled to decide what materials to release. All parties involved agreed that all pertinent information should be released. Whether to release profanity and vulgarity unedited divided his advisers. His legal team favored releasing the tapes unedited, while Press Secretary Ron Ziegler preferred using an edited version where "expletive deleted" would replace the raw material. After several weeks of debate, they decided to release an edited version. Nixon announced the release of the transcripts in a speech to the nation on April 29, 1974. Nixon noted that any audio pertinent to national security information could be redacted from the released tapes.
Initially, Nixon was given a positive reaction for his speech. As people read the transcripts over the next couple of weeks, however, former supporters among the public, media and political community called for Nixon's resignation or impeachment. Vice President Gerald Ford said, "While it may be easy to delete characterization from the printed page, we cannot delete characterization from people's minds with a wave of the hand." The Senate Republican Leader Hugh Scott said the transcripts revealed a "deplorable, disgusting, shabby, and immoral" performance on the part of the President and his former aides. The House Republican Leader John Jacob Rhodes agreed with Scott, and Rhodes recommended that if Nixon's position continued to deteriorate, he "ought to consider resigning as a possible option." The editors of the newspaper "The Chicago Tribune", a publication that had supported Nixon, wrote, "He is humorless to the point of being inhumane. He is devious. He is vacillating. He is profane. He is willing to be led. He displays dismaying gaps in knowledge. He is suspicious of his staff. His loyalty is minimal". The "Providence Journal" wrote, "Reading the transcripts is an emetic experience; one comes away feeling unclean." This newspaper continued, that, while the transcripts may not have revealed an indictable offense, they showed Nixon contemptuous of the United States, its institutions, and its people. According to "Time" magazine, the Republican Party leaders in the Western United States felt that while there remained a significant number of Nixon loyalists in the party, the majority believed that Nixon should step down as quickly as possible. They were disturbed by the bad language and the coarse, vindictive tone of the conversations in the transcripts.
Supreme Court.
The issue of access to the tapes went to the Supreme Court. On July 24, 1974, in "United States v. Nixon", the Court, which did not include the recused Justice William Rehnquist (who had recently been appointed by Nixon and had served as Assistant Attorney General of the Office of Legal Counsel in the Nixon Justice Department), ruled unanimously that claims of executive privilege over the tapes were void. They ordered the president to release them to the special prosecutor. On July 30, 1974, President Nixon complied with the order and released the subpoenaed tapes for the public.
Release of the tapes.
The tapes revealed several crucial conversations that took place between the President and his counsel, John Dean, on March 21, 1973. In this conversation, Dean summarized many aspects of the Watergate case, and focused on the subsequent coverup, describing it as a "cancer on the presidency". The burglary team was being paid hush money for their silence and Dean stated: "That's the most troublesome post-thing, because Bob [Haldeman] is involved in that; John [Ehrlichman] is involved in that; I am involved in that; Mitchell is involved in that. And that's an obstruction of justice." Dean continued and stated that Howard Hunt is blackmailing the White House, demanding money immediately; President Nixon replied that the blackmail money should be paid: "…just looking at the immediate problem, don't you have to have – handle Hunt's financial situation damn soon? […] you've got to keep the cap on the bottle that much, in order to have any options."
At the time of the initial congressional impeachment, it was not known if Nixon had known and approved of the payments to the Watergate defendants earlier than this conversation. Nixon's conversation with Haldeman on August 1, 1972, is one of several that establishes this. Nixon states: "Well…they have to be paid. That's all there is to that. They have to be paid." During the congressional debate on impeachment, some believed that impeachment required a criminally indictable offense. President Nixon's agreement to make the blackmail payments was regarded as an affirmative act to obstruct justice.
On December 7, 1973, it was found that an 18½ minute portion of one recorded tape had been erased. Nixon's longtime personal secretary, Rose Mary Woods, said she had accidentally erased the tape by pushing the wrong pedal on her tape player when answering the phone. The press ran photos showing that it was unlikely for Woods to answer the phone and keep her foot on the pedal. Later forensic analysis determined that the tape had been erased in several segments – at least five, and perhaps as many as nine.
Final investigations and resignation.
Nixon's position was becoming increasingly precarious. On February 6, 1974, the House of Representatives approved giving the Judiciary Committee authority to investigate impeachment of the President. On July 27, 1974, the House Judiciary Committee voted 27–11 to recommend the first article of impeachment against the president: obstruction of justice. The House then recommended the second article, abuse of power, on July 29, 1974. The next day, on July 30, 1974, the House recommended the third article: contempt of Congress. On August 20, 1974, the House authorized the printing of the Committee report H. Rept. 93-1305 which included the text of the resolution impeaching President Nixon and set forth articles of impeachment against him.
"Smoking Gun" tape.
On August 5, 1974, the White House released a previously unknown audio tape from June 23, 1972. Recorded only a few days after the break-in, it documented the initial stages of the coverup: it revealed Nixon and Haldeman meeting in the Oval Office and formulating a plan to block investigations by having the CIA falsely claim to the FBI that national security was involved. Haldeman introduced the topic as follows:
…the Democratic break-in thing, we're back to the–in the, the problem area because the FBI is not under control, because Gray doesn't exactly know how to control them, and they have… their investigation is now leading into some productive areas […] and it goes in some directions we don't want it to go.
After explaining how the money from CRP was traced to the burglars, Haldeman explained to Nixon the coverup plan: "the way to handle this now is for us to have Walters [CIA] call Pat Gray [FBI] and just say, 'Stay the hell out of this …this is ah, business here we don't want you to go any further on it.'" President Nixon approved the plan, and after he was given more information about the involvement of his campaign in the break-in, he told Haldeman: "All right, fine, I understand it all. We won't second-guess Mitchell and the rest." Returning to the use of the CIA to obstruct the FBI, he instructed Haldeman: "You call them in. Good. Good deal. Play it tough. That's the way they play it and that's the way we are going to play it."
Nixon denied that this constituted an obstruction of justice, as his instructions ultimately resulted in the CIA truthfully reporting to the FBI that there were no national security issues. Nixon urged the FBI to press forward with the investigation when they expressed concern about interference.
Before the release of this tape, President Nixon had denied any involvement in the scandal. He claimed that there were no political motivations in his instructions to the CIA, and claimed he had no knowledge before March 21, 1973, of involvement by senior campaign officials such as John Mitchell. The contents of this tape persuaded Nixon's own lawyers, Fred Buzhardt and James St. Clair, that "The tape proved that the President had lied to the nation, to his closest aides, and to his own lawyers – for more than two years". The tape, which was referred to as a "smoking gun" by Barber Conable, proved that Nixon had been involved in the coverup from the beginning.
In the week before Nixon's resignation, Ehrlichman and Haldeman unsuccessfully tried to get Nixon to grant them the pardons which Nixon had promised them before their April 1973 resignations.
Resignation.
The release of the "smoking gun" tape destroyed Nixon politically. The ten congressmen who voted against all three articles of impeachment in the House Judiciary Committee announced they would all support impeachment when the vote was taken in the full House.
On the night of August 7, 1974, Senators Barry Goldwater and Hugh Scott and Congressman John Jacob Rhodes met with Nixon in the Oval Office and told him that his support in Congress had all but disappeared. Rhodes told Nixon that he would face certain impeachment when the articles came up for vote in the full House. Goldwater and Scott told the president that there were not only enough votes in the Senate to convict him, but that no more than 15 Senators were willing to vote for acquittal. Realizing that he had no chance of staying in office, Nixon decided to resign. In a from the Oval Office on the evening of August 8, 1974, the president said, in part:
In all the decisions I have made in my public life, I have always tried to do what was best for the Nation. Throughout the long and difficult period of Watergate, I have felt it was my duty to persevere, to make every possible effort to complete the term of office to which you elected me. In the past few days, however, it has become evident to me that I no longer have a strong enough political base in the Congress to justify continuing that effort. As long as there was such a base, I felt strongly that it was necessary to see the constitutional process through to its conclusion, that to do otherwise would be unfaithful to the spirit of that deliberately difficult process and a dangerously destabilizing precedent for the future….
I would have preferred to carry through to the finish whatever the personal agony it would have involved, and my family unanimously urged me to do so. But the interest of the Nation must always come before any personal considerations. From the discussions I have had with Congressional and other leaders, I have concluded that because of the Watergate matter I might not have the support of the Congress that I would consider necessary to back the very difficult decisions and carry out the duties of this office in the way the interests of the Nation would require.
I have never been a quitter. To leave office before my term is completed is abhorrent to every instinct in my body. But as President, I must put the interest of America first. America needs a full-time President and a full-time Congress, particularly at this time with problems we face at home and abroad. To continue to fight through the months ahead for my personal vindication would almost totally absorb the time and attention of both the President and the Congress in a period when our entire focus should be on the great issues of peace abroad and prosperity without inflation at home. Therefore, I shall resign the Presidency effective at noon tomorrow. Vice President Ford will be sworn in as President at that hour in this office.
The morning that his resignation was to take effect, the President, with Mrs. Nixon and their family, said farewell to the White House staff in the East Room. A helicopter carried them from the White House to Andrews Air Force Base in Maryland. Nixon later wrote that he thought, "As the helicopter moved on to Andrews, I found myself thinking not of the past, but of the future. What could I do now?" At Andrews, he and his family boarded Air Force One to El Toro Marine Corps Air Station in California, and then were transported to his home in San Clemente.
President Ford's pardon of Nixon.
With President Nixon's resignation, Congress dropped its impeachment proceedings. Criminal prosecution was still a possibility both on the federal and state level. Nixon was succeeded by Vice President Gerald Ford as President, who on September 8, 1974, issued a full and unconditional pardon of Nixon, immunizing him from prosecution for any crimes he had "committed or may have committed or taken part in" as president. In a televised broadcast to the nation, Ford explained that he felt the pardon was in the best interest of the country. He said that the Nixon family's situation "is an American tragedy in which we all have played a part. It could go on and on and on, or someone must write the end to it. I have concluded that only I can do that, and if I can, I must."
Nixon proclaimed his innocence until his death in 1994. In his official response to the pardon, he said that he "was wrong in not acting more decisively and more forthrightly in dealing with Watergate, particularly when it reached the stage of judicial proceedings and grew from a political scandal into a national tragedy".
Some commentators have argued that pardoning Nixon contributed to President Ford's loss of the presidential election of 1976. Allegations of a secret deal made with Ford, promising a pardon in return for Nixon's resignation, led Ford to testify before the House Judiciary Committee on October 17, 1974.
In his autobiography "A Time to Heal", Ford wrote about a meeting he had with Nixon's Chief of Staff, Alexander Haig. Haig was explaining what he and Nixon's staff thought were Nixon's only options. He could try to ride out the impeachment and fight against conviction in the Senate all the way, or he could resign. His options for resigning were to delay his resignation until further along in the impeachment process to try and settle for a censure vote in Congress, or to pardon himself and then resign. Haig told Ford that some of Nixon's staff suggested that Nixon could agree to resign in return for an agreement that Ford would pardon him.
Haig emphasized that these weren't "his" suggestions. He didn't identify the staff members and he made it very clear that he wasn't recommending any one option over another. What he wanted to know was whether or not my overall assessment of the situation agreed with his.[emphasis in original]... Next he asked if I had any suggestions as to courses of actions for the President. I didn't think it would be proper for me to make any recommendations at all, and I told him so.—Gerald Ford, "A Time to Heal"
Aftermath.
Final legal actions and effect on the law profession.
Charles Colson pleaded guilty to charges concerning the Daniel Ellsberg case; in exchange, the indictment against him for covering up the activities of the Committee to Re-elect the President was dropped, as it was against Strachan. The remaining five members of the Watergate Seven indicted in March went on trial in October 1974. On January 1, 1975, all but Parkinson were found guilty. In 1976, the U.S. Court of Appeals ordered a new trial for Mardian; subsequently, all charges against him were dropped. Haldeman, Ehrlichman, and Mitchell exhausted their appeals in 1977. Ehrlichman entered prison in 1976, followed by the other two in 1977. Since Nixon and many senior officials involved in Watergate were lawyers, the scandal severely tarnished the public image of the legal profession.
The Watergate scandal resulted in 69 government officials being charged and 48 being found guilty, including:
To defuse public demand for direct federal regulation of lawyers (as opposed to leaving it in the hands of state bar associations or courts), the American Bar Association (ABA) launched two major reforms. First, the ABA decided that its existing Model Code of Professional Responsibility (promulgated 1969) was a failure. In 1983 it replaced it with the Model Rules of Professional Conduct. The MRPC have been adopted in part or in whole by 49 states (and is being considered by the last one, California). Its preamble contains an emphatic reminder that the legal profession can remain self-governing only if lawyers behave properly. Second, the ABA promulgated a requirement that law students at ABA-approved law schools take a course in professional responsibility (which means they must study the MRPC). The requirement remains in effect.
On June 24 and 25, 1975, Nixon gave secret testimony to a grand jury. According to news reports at the time, Nixon answered questions about the 18½-minute tape gap, altering White House tape transcripts turned over to the House Judiciary Committee, using the Internal Revenue Service to harass political enemies, and a $100,000 contribution from billionaire Howard Hughes. Aided by the Public Citizen Litigation Group, the historian Stanley Kutler, who has written several books about Nixon and Watergate and had successfully sued for the 1996 public release of the Nixon White House tapes, sued for release of the transcripts of the Nixon grand jury testimony. President Obama's justice department opposed the release of the transcripts on privacy grounds. On July 29, 2011, U.S. District Judge Royce Lamberth granted Kutler's request as historical interests trumped privacy, especially considering that Nixon and other key figures were deceased and most of the surviving figures had testified under oath, have been written about, or were interviewed. The transcripts were not immediately released pending the government's decision on whether to appeal. They were released in their entirety on November 10, 2011, although the names of people still alive were redacted.
Texas A&M University–Central Texas professor Luke Nichter wrote the chief judge of the federal court in Washington to release hundreds of pages of sealed records of the Watergate Seven. In June 2012 The U.S. Department of Justice wrote the court that it would not object to their release with some exceptions.
On November 2, 2012, Watergate trial records for G. Gordon Liddy and James McCord were ordered unsealed by Federal Judge Royce Lamberth.
Political and cultural reverberations.
According to Thomas J. Johnson, a professor of journalism at University of Texas at Austin, Secretary of State Henry Kissinger predicted during Nixon's final days that history would remember Nixon as a great president and that Watergate would be relegated to a "minor footnote".
When Congress investigated the scope of the president's legal powers, it belatedly found that the United States had been declared by presidential administrations to be in a continuous open-ended state of emergency since 1950. Congress enacted the National Emergencies Act in 1976 to regulate such declarations.
The Watergate scandal left such an impression on the national and international consciousness that many scandals since then have been labeled with the suffix "-gate".
The ramifications for the November 1974 Senate and House elections, which took place three months after Nixon's resignation, were significant, with the Democrats gaining five seats in the Senate and forty-nine in the House (the newcomers were nicknamed "Watergate Babies"). Watergate led Congress to pass legislation that changed campaign financing, to amend the Freedom of Information Act, as well as to require financial disclosures by key government officials (via the Ethics in Government Act). Other types of disclosures, such as releasing recent income tax forms, became expected albeit not legally required. Presidents since Franklin D. Roosevelt had recorded many of their conversations but the practice purportedly ended after Watergate.
Ford's pardon of Nixon played a major role in his defeat in the 1976 presidential election against Jimmy Carter.
In 1977, Nixon arranged an interview with British journalist David Frost in the hopes of improving his legacy. Based on a previous interview in 1968, he had assumed Frost would be an easy interviewer and was taken aback by Frost's incisive questions. The interview displayed the entire scandal to the American people, even getting an apology from Nixon, but his legacy remained tarnished.
In 2010, Congressman Ron Paul questioned whether the Federal Reserve Bank had been used to funnel illegal money during Watergate and other scandals. This led House Financial Services Committee Chairman Barney Frank to ask the agency to investigate the charges. In April 2012, the Federal Reserve bank inspector general released a report stating "we did not find any evidence of undue political interference with or improper actions by Federal Reserve officials related to the cash found on the Watergate burglars".
In the aftermath of Watergate, "follow the money" became part of the American lexicon and is widely believed to have been uttered by Mark Felt to Woodward and Bernstein. The phrase was never used in the 1974 book "All The President's Men" and did not become associated with it until the movie of the same name was released in 1976.
Purpose of the break-in.
Despite the enormous impact of the Watergate scandal, the actual purpose of the break-in of the DNC offices has never been conclusively established, though records from the "United States v. Liddy" trial which were made public in 2013 showed that four of the five burglars testified that they were told to take part in the operation with the hopes of finding evidence linking Cuban funding to Democratic campaigns. The likeliest hypothesis suggests that the target of the break-in was the offices of Larry O'Brien, the DNC Chairman. However, O'Brien's name was not on Alfred C. Baldwin III's list of targets that was released in 2013. Among those listed were senior DNC official R. Spencer Oliver, Oliver's secretary Ida "Maxine" Wells, co-worker Robert Allen and secretary Barbara Kennedy. Based on these revelations, Texas A&M history professor Luke Nichter, who had successfully petitioned for the release of the information, argued that Woodward and Bernstein were incorrect in concluding, based largely on Watergate burglar James McCord's word, that the purpose of the break-in was to bug O'Brien's phone to gather political and financial intelligence on the Democrats. Instead, Nichter sided with late "New York Times" journalist J. Anthony Lukas' conclusion that its purpose was to find evidence linking the Democrats to prostitution, as Oliver's office had frequently been used to arrange such meetings. However, he acknowledged that Woodward and Bernstein's theory of O'Brien as the target could not be debunked unless information about what Baldwin heard was released.
In 1968, O'Brien was appointed by Vice President Hubert Humphrey to serve as the national director of Humphrey's presidential campaign and, separately, by Howard Hughes to serve as Hughes' public-policy lobbyist in Washington. O'Brien was elected national chairman of the DNC in 1968 and 1970. In late 1971, the president's brother, Donald Nixon, was collecting intelligence for his brother at the time and asked John H. Meier, an advisor to Howard Hughes, about O'Brien. In 1956, Donald Nixon had borrowed $205,000 from Howard Hughes and had never repaid the loan. The loan's existence surfaced during the 1960 presidential election campaign, embarrassing Richard Nixon and becoming a political liability. According to author Donald M. Bartlett, Richard Nixon would do whatever was necessary to prevent another family embarrassment. From 1968 to 1970, Hughes withdrew nearly half a million dollars from the Texas National Bank of Commerce for contributions to both Democrats and Republicans, including presidential candidates Humphrey and Nixon. Hughes wanted Donald Nixon and Meier involved but Nixon opposed this.
Meier told Donald that he was sure the Democrats would win the election because they had considerable information on Richard Nixon's illicit dealings with Hughes that had never been released, and that it resided with Larry O'Brien. O'Brien, who had received $25,000 from Hughes, did not actually have any documents but Meier claims to have wanted Richard Nixon to think that he did. It is only a question of conjecture then that Donald told his brother that Meier had given the Democrats all the damaging Hughes information and that O'Brien had the proof. As a matter of fact O'Brien had also been a lobbyist for Hughes in a Democrat-controlled Congress and the possibility of his finding out about Hughes' illegal contributions to the Nixon campaign was too much of a danger for Nixon to ignore.
Numerous hypotheses have persisted in claiming deeper significance to the Watergate scandal than that acknowledged by media and historians:
James F. Neal, who prosecuted the Watergate 7, did not believe Nixon had ordered the break in because of Nixon's surprised reaction when he was told about it. He cited the June 23, 1972 conversation when Nixon asked Haldeman, "Who was the asshole that did it?"
Reactions.
 China – In July 1975, according to then-Prime Minister Kukrit Pramoj of Thailand, Chairman Mao Zedong called the Watergate scandal "the result of 'too much freedom of political expression in the U.S.'" On November 1976, Mao called it "an indication of [American] isolationism[,] which he saw as 'disastrous' for Europe." He further said, "Do Americans really want to go isolationist? ... In the two world wars, the Americans came [in] very late, but all the same, they did come in. They haven't been isolationist in practice."
 Cuba – Then-leader Fidel Castro said in his December 1974 interview that, of the crimes committed by the Cuban exiles, like killings, attacks on Cuban ports, and spying, the Watergate burglaries and wiretappings were "probably the least of [them]".
 Iran – Then-Shah of Iran Mohammad Reza Pahlavi told the press, "I want to say quite emphatically that everything that would weaken or jeopardize the President's power to make decisions in split seconds would represent grave danger for the whole world."
 Japan – In August 1973, then-Prime Minister Kakuei Tanaka said "Watergate had 'no cancelling influence on U.S. leadership in the world.'" Furthermore, Tanaka said, "The pivotal role of the United States has not changed, so this internal affair will not be permitted to have an effect." In March 1975, Tanaka's successor, Takeo Miki, said at a convention of the Liberal Democratic Party, "At the time of the Watergate issue in America, I was deeply moved by the scene in the House Judiciary Committee, where each member of the committee expressed his own or her own heart based upon the spirit of the American Constitution. It was this attitude, I think, that rescued American democracy."
 Singapore – Then-Prime Minister Lee Kuan Yew said in August 1973, "As one surprising revelation follows another at the Senate hearings on Watergate, it becomes increasingly clear that Washington, [D.C.], today is in no position to offer the moral or strong political and economic leadership for which its friends and allies are yearning." Moreover, Lee said that the scandal may have led the United States to lessen its interests and commitments in world affairs, to weaken its ability to enforce the Paris Peace Accords on Vietnam, and to not react to violations of the Accords. Lee said further that the United States "makes the future of this peace in Indonesia an extremely bleak one with grave consequence for the contiguous states". Lee then blamed the scandal for economic inflation in Singapore because the Singapore dollar was pegged to the United States dollar at the time, assuming the U.S. dollar was stronger than the British pound sterling.
 Soviet Union – In the press conference of May 1973, General Secretary Leonid Brezhnev told Secretary of State Henry Kissinger how the United States handled the scandal was different from how the Communist Party of the Soviet Union had operated. There, without any opposition party back then, members of the Party had been wiretapped for any possible wrongdoing. In June 1973, when Brezhnev arrives to the United States to have a one-week meeting with President Nixon, Brezhnev told the press, "I do not intent to refer to that matter—[the Watergate]. It would be completely indecent for me to refer to it. [...] My attitude toward Mr. Nixon is of very great respect." When one reporter suggested that President Nixon and his position with Brezhnev were "weakened" by the scandal, Brezhnev replied, "It does not enter my mind to think whether Mr. Nixon has lost or gained any influence because of the affair." Then he said further that he had respected Nixon because of Nixon's "realistic and constructive approach to Soviet Union–United States relations [...] passing from an era of confrontation to an era of negotiations between nations."
 United Kingdom – Talks between Nixon and Prime Minister Edward Heath may have been bugged. Heath did not publicly display his anger, with aides saying that he was unconcerned about having been bugged at the White House. According to officials, Heath commonly had notes taken of his public discussions with Nixon so a recording would not have bothered him. However, officials privately said that if private talks with Nixon were bugged, then Heath would be outraged. Even so, Heath privately was outraged over being taped without his prior knowledge.
 United States – In May 1975, after the fall of Saigon, which ended the Vietnam War, Secretary of State Henry Kissinger said if the scandal did not affect Richard Nixon and the Congress did not override Nixon's veto of the War Powers Resolution, North Vietnam would not have captured South Vietnam.
Further reading.
</dl>

</doc>
<doc id="52385" url="http://en.wikipedia.org/wiki?curid=52385" title="Axiom of pairing">
Axiom of pairing

In axiomatic set theory and the branches of logic, mathematics, and computer science that use it, the axiom of pairing is one of the axioms of Zermelo–Fraenkel set theory.
Formal statement.
In the formal language of the Zermelo–Fraenkel axioms, the axiom reads:
or in words:
or in simpler words:
Interpretation.
What the axiom is really saying is that, given two sets "A" and "B", we can find a set "C" whose members are precisely "A" and "B".
We can use the axiom of extensionality to show that this set "C" is unique.
We call the set "C" the "pair" of "A" and "B", and denote it {"A","B"}.
Thus the essence of the axiom is:
Note that a singleton is a special case of a pair.
The axiom of pairing also allows for the definition of ordered pairs. For any sets formula_2 and formula_3, the ordered pair is defined by the following: 
Note that this definition satisfies the condition 
Ordered "n"-tuples can be defined recursively as follows: 
Non-independence.
The axiom of pairing is generally considered uncontroversial, and it or an equivalent appears in just about any alternative axiomatization of set theory. Nevertheless, in the standard formulation of the Zermelo–Fraenkel set theory, the axiom of pairing follows from the axiom schema of replacement applied to any given set with two or more elements, and thus it is sometimes omitted. The existence of such a set with two elements, such as { {}, { {} } }, can be deduced either from the axiom of empty set and the axiom of power set or from the axiom of infinity.
Generalisation.
Together with the axiom of empty set, the axiom of pairing can be generalised to the following schema:
that is:
This set "C" is again unique by the axiom of extension, and is denoted {"A"1...,"A""n"}.
Of course, we can't refer to a "finite" number of sets rigorously without already having in our hands a (finite) set to which the sets in question belong.
Thus, this is not a single statement but instead a schema, with a separate statement for each natural number "n".
For example, to prove the case "n" = 3, use the axiom of pairing three times, to produce the pair {"A"1,"A"2}, the singleton {"A"3}, and then the pair .
The axiom of union then produces the desired result, {"A"1,"A"2,"A"3}. We can extend this schema to include "n"=0 if we interpret that case as the axiom of empty set.
Thus, one may use this as an axiom schema in the place of the axioms of empty set and pairing. Normally, however, one uses the axioms of empty set and pairing separately, and then proves this as a theorem schema. Note that adopting this as an axiom schema will not replace the axiom of union, which is still needed for other situations.
Another alternative.
Another axiom which implies the axiom of pairing in the presence of the axiom of empty set is 
Using {} for "A" and "x" for B, we get {"x"} for C. Then use {"x"} for "A" and "y" for "B", getting {"x,y"} for C. One may continue in this fashion to build up any finite set. And this could be used to generate all hereditarily finite sets without using the axiom of union.

</doc>
<doc id="52386" url="http://en.wikipedia.org/wiki?curid=52386" title="Axiom schema of specification">
Axiom schema of specification

In many popular versions of axiomatic set theory the axiom schema of specification, also known as the axiom schema of separation, subset axiom scheme or axiom schema of restricted comprehension is an axiom schema. Essentially, it says that any definable subclass of a set is a set.
Some mathematicians call it the axiom schema of comprehension, although others use that term for "unrestricted" comprehension, discussed below.
Because restricted comprehension solved Russell's paradox, several mathematicians including Zermelo, Fraenkel, and Gödel considered it the most important axiom of set theory.
Statement.
One instance of the schema is included for each formula φ in the language of set theory with free variables among "x", "w"1, ..., "w""n", "A". So "B" is not free in φ. In the formal language of set theory, the axiom schema is:
or in words:
Note that there is one axiom for every such predicate φ; thus, this is an axiom schema.
To understand this axiom schema, note that the set "B" must be a subset of "A". Thus, what the axiom schema is really saying is that, given a set "A" and a predicate "P", we can find a subset "B" of "A" whose members are precisely the members of "A" that satisfy "P". By the axiom of extensionality this set is unique. We usually denote this set using set-builder notation as {"C" ∈ "A" : "P"("C")}. Thus the essence of the axiom is:
The axiom schema of specification is characteristic of systems of axiomatic set theory related to the usual set theory ZFC, but does not usually appear in radically different systems of alternative set theory. For example, New Foundations and positive set theory use different restrictions of the axiom of comprehension of naive set theory. The Alternative Set Theory of Vopenka makes a specific point of allowing proper subclasses of sets, called semisets. Even in systems related to ZFC, this scheme is sometimes restricted to formulas with bounded quantifiers, as in Kripke–Platek set theory with urelements.
Relation to the axiom schema of replacement.
The axiom schema of separation can almost be derived from the axiom schema of replacement.
First, recall this axiom schema:
for any functional predicate "F" in one variable that doesn't use the symbols "A", "B", "C" or "D".
Given a suitable predicate "P" for the axiom of specification, define the mapping "F" by "F"("D") = "D" if "P"("D") is true and "F"("D") = "E" if "P"("D") is false, where "E" is any member of "A" such that "P"("E") is true.
Then the set "B" guaranteed by the axiom of replacement is precisely the set "B" required for the axiom of specification. The only problem is if no such "E" exists. But in this case, the set "B" required for the axiom of separation is the empty set, so the axiom of separation follows from the axiom of replacement together with the axiom of empty set.
For this reason, the axiom schema of separation is often left out of modern lists of the Zermelo–Fraenkel axioms. However, it's still important for historical considerations, and for comparison with alternative axiomatizations of set theory, as can be seen for example in the following sections.
Unrestricted comprehension.
The "axiom schema of comprehension" (unrestricted) reads:
that is:
This set "B" is again unique, and is usually denoted as {"x" : φ("x", w1, ... wn)}.
This axiom schema was tacitly used in the early days of naive set theory, before a strict axiomatization was adopted. Unfortunately, it leads directly to Russell's paradox by taking φ("x") to be ¬("x"∈"x") (i.e., the property that set "x" is not a member of itself). Therefore, no useful axiomatization of set theory can use unrestricted comprehension, at least not with classical logic.
Accepting only the axiom schema of specification was the beginning of axiomatic set theory. Most of the other Zermelo–Fraenkel axioms (but not the axiom of extensionality or the axiom of regularity) then became necessary to make up for some of what was lost by changing the axiom schema of comprehension to the axiom schema of specification – each of these axioms states that a certain set exists, and defines that set by giving a predicate for its members to satisfy, i.e. it is a special case of the axiom schema of comprehension.
In NBG class theory.
In von Neumann–Bernays–Gödel set theory, a distinction is made between sets and classes. A class "C" is a set if and only if it belongs to some class "E". In this theory, there is a theorem schema that reads
that is,
provided that the quantifiers in the predicate "P" are restricted to sets.
This theorem schema is itself a restricted form of comprehension, which avoids Russell's paradox because of the requirement that "C" be a set. Then specification for sets themselves can be written as a single axiom
that is,
or even more simply
In this axiom, the predicate "P" is replaced by the class "D", which can be quantified over. Another simpler axiom which achieves the same effect is
that is,
In higher-order settings.
In a typed language where we can quantify over predicates, the axiom schema of specification becomes a simple axiom. This is much the same trick as was used in the NBG axioms of the previous section, where the predicate was replaced by a class that was then quantified over.
In second-order logic and higher-order logic with higher-order semantics, the axiom of specification is a logical validity and does not need to be explicitly included in a theory.
In Quine's New Foundations.
In the New Foundations approach to set theory pioneered by W.V.O. Quine, the axiom of comprehension for a given predicate takes the unrestricted form, but the predicates that may be used in the schema are themselves restricted.
The predicate ("C" is not in "C") is forbidden, because the same symbol "C" appears on both sides of the membership symbol (and so at different "relative types"); thus, Russell's paradox is avoided.
However, by taking "P"("C") to be ("C" = "C"), which is allowed, we can form a set of all sets. For details, see stratification.
References.
</dl>

</doc>
<doc id="52387" url="http://en.wikipedia.org/wiki?curid=52387" title="Axiom schema of replacement">
Axiom schema of replacement

In set theory, the axiom schema of replacement is a schema of axioms in Zermelo–Fraenkel set theory (ZFC) that asserts that the image of any set under any definable mapping is also a set. It is necessary for the construction of certain infinite sets in ZFC.
The axiom schema is motivated by the idea that whether a class is a set depends only on the cardinality of the class, not on the rank of its elements. Thus, if one class is "small enough" to be a set, and there is a surjection from that class to a second class, the axiom states that the second class is also a set. However, because ZFC only speaks of sets, not proper classes, the schema is stated only for definable surjections, which are identified with their defining formulas.
Statement.
Suppose "P" is a definable binary relation (which may be a proper class) such that for every set "x" there is a unique set "y" such that "P"("x","y") holds. There is a corresponding definable function "F""P", where "F""P"("X") = "Y" if and only if "P"("X","Y"); "F" will also be a proper class if "P" is. Consider the (possibly proper) class "B" defined such for every set "y", "y" is in "B" if and only if there is an "x" in "A" with "F""P"("x") = "y". "B" is called the image of "A" under "F""P", and denoted "F""P"["A"] or (using set-builder notation) {"F""P"("x") : "x" ∈ "A"}.
The axiom schema of replacement states that if "F" is a definable class function, as above, and "A" is any set, then the image "F"["A"] is also a set. This can be seen as a principle of smallness: the axiom states that if "A" is small enough to be a set, then "F"["A"] is also small enough to be a set. It is implied by the stronger axiom of limitation of size.
Because it is impossible to quantify over definable functions in first-order logic, one instance of the schema is included for each formula φ in the language of set theory with free variables among "w"1, ..., "w""n", "A", "x", "y"; but "B" is not free in φ. In the formal language of set theory, the axiom schema is:
Axiom schema of collection.
The axiom schema of collection is closely related to and frequently confused with the axiom schema of replacement. While replacement says that the image itself is a set, collection merely says that some superclass of the image is a set. In other words, the resulting set, "B", is not required to be minimal.
This version of collection also lacks the uniqueness requirement on φ. Suppose that the free variables of φ are among "w"1, ..., "w""n", "x", "y"; but neither "A" nor "B" is free in φ. Then the axiom schema is:
That is, the relation defined by φ is not required to be a function — some "x" in "A" may correspond to multiple "y" in "B". In this case, the image set "B" whose existence is asserted must contain at least one such "y" for each "x" of the original set, with no guarantee that it will contain only one.
The axiom schema is sometimes stated without any restrictions on the predicate, φ:
In this case, there may be elements "x" in "A" that are not associated to any other sets by φ. However, the axiom schema as stated requires that, if an element "x" of "A" is associated with at least one set "y", then the image set "B" will contain at least one such "y". The resulting axiom schema is also called the axiom schema of boundedness.
The axiom schema of collection is equivalent to the axiom schema of replacement over the remainder of the ZF axioms. However, this is not so in the absence of the Power Set Axiom or constructive counterpart of ZF, where Collection is stronger.
Example applications.
The ordinal number ω·2 = ω + ω (using the modern definition due to von Neumann) is the first ordinal that cannot be constructed without replacement. The axiom of infinity asserts the existence of the infinite sequence ω = {0, 1, 2, ...}, and only this sequence. One would like to define ω·2 to be the union of the sequence {ω, ω + 1, ω + 2...}. However, arbitrary classes of ordinals need not be sets (the class of all ordinals is not a set, for example). Replacement allows one to replace each finite number "n" in ω with the corresponding ω + "n", and guarantees that this class is a set. Note that one can easily construct a well-ordered set that is isomorphic to ω·2 without resorting to replacement – simply take the disjoint union of two copies of ω, with the second copy greater than the first – but that this is not an ordinal since it is not totally ordered by inclusion.
Clearly then, the existence of an assignment of an ordinal to every well-ordered set requires replacement as well. Similarly the von Neumann cardinal assignment which assigns a cardinal number to each set requires replacement, as well as axiom of choice.
Every countable limit ordinal requires replacement for its construction analogously to ω·2. Larger ordinals rely on replacement less directly. For example ω1, the first uncountable ordinal, can be constructed as follows – the set of countable well orders exists as a subset of "P"(N×N) by separation and powerset (a relation on "A" is a subset of "A"×"A", and so an element of the power set "P"("A"×"A"). A set of relations is thus a subset of "P"("A"×"A")). Replace each well-ordered set with its ordinal. This is the set of countable ordinals ω1, which can itself be shown to be uncountable. The construction uses replacement twice; once to ensure an ordinal assignment for each well ordered set and again to replace well ordered sets by their ordinals. This is a special case of the result of Hartogs number, and the general case can be proved similarly.
The axiom of choice without replacement (ZC set theory) is not strong enough to show that Borel sets are determined; for this, replacement is required.
History.
The axiom schema of replacement was not part of Ernst Zermelo's 1908 axiomatisation of set theory (Z). Some informal approximation to it existed in Cantor's unpublished works, and it appeared again informally in Mirimanoff (1917).
Its publication by Adolf Fraenkel in 1922 is what makes modern set theory Zermelo-"Fraenkel" set theory (ZF). The axiom was independently discovered and announced by Thoralf Skolem later in the same year (and published in 1923). Zermelo himself incorporated Fraenkel's axiom in his revised system he published in 1930, which also included as a new axiom von Neumann's axiom of foundation. Although it is Skolem's first order version of the axiom list that we use today, he usually gets no credit since each individual axiom was developed earlier by either Zermelo or Fraenkel. The phrase “Zermelo-Fraenkel set theory” was first used in print by von Neumann in 1928.
Zermelo and Fraenkel had corresponded heavily in 1921; the axiom of replacement was a major topic of this exchange. Fraenkel initiated correspondence with Zermelo sometime in March 1921. His letters before the one dated 6 May 1921 are lost though. Zermelo first admitted to a gap in his system in a reply to Fraenkel dated 9 May 1921. On 10 July 1921, Fraenkel completed and submitted for publication a paper (published in 1922) that described his axiom as allowing arbitrary replacements: "If "M" is a set and each element of "M" is replaced by [a set or an urelement] then "M" turns into a set again" (parenthetical completion and translation by Ebbinghaus). Fraenkel's 1922 publication thanked Zermelo for helpful arguments. Prior to this publication, Fraenkel publicly announced his new axiom at a meeting of the German Mathematical Society held in Jena on 22 September 1921. Zermelo was present at this meeting; in the discussion following Fraenkel's talk he accepted the axiom of replacement in general terms, but expressed reservations regarding its extent.
Thoralf Skolem made public his discovery of the gap in Zermelo’s system (the same gap that Fraenkel had found) in a talk he gave on 6 July 1922 at the 5th Congress of Scandinavian Mathematicians, which was held in Helsinki; the proceedings of this congress were published in 1923. Skolem presented a resolution in terms of first-order definable replacements: "Let "U" be a definite proposition that holds for certain pairs ("a", "b") in the domain "B"; assume further, that for every "a" there exists at most one "b" such that "U" is true. Then, as "a" ranges over the elements of a set "Ma", "b" ranges over all elements of a set "Mb"." In the same year, Fraenkel wrote a review of Skolem's paper, in which Fraenkel simply stated that Skolem’s considerations correspond to his own.
Zermelo himself never accepted Skolem's formulation of the axiom schema of replacement. At one point he called Skolem's approach “set theory of the impoverished”. Zermelo envisaged a system that would allow for large cardinals. He also objected strongly to the philosophical implications of countable models of set theory, which followed from Skolem's first-order axiomatization. According to the biography of Zermelo by Heinz-Dieter Ebbinghaus, Zermelo's disapproval of Skolem's approach marked the end of Zermelo's influence on the developments of set theory and logic.
Impact.
The axiom schema of replacement drastically increases the strength of ZF, both in terms of the theorems it can prove and in terms of its proof-theoretic consistency strength, compared to Z. In particular, ZF proves the consistency of Z, as the set Vω·2 is a model of Z constructible in ZF. (Gödel's second incompleteness theorem shows that each of these theories contains a sentence, "expressing" the theory's own consistency, that is unprovable in that theory, if that theory is consistent (this result is often loosely expressed as the claim that neither of these theories can prove its own consistency, if it is consistent.)) The cardinal number formula_4 is the first one which can be shown to exist in ZF but not in Z.
The axiom schema of replacement is not necessary for the proofs of most theorems of ordinary mathematics. Indeed, Zermelo set theory already can interpret second-order arithmetic and much of type theory in finite types, which in turn are sufficient to formalize the bulk of mathematics. A notable mathematical theorem that requires the axiom of replacement to be proved in ZF is the Borel determinacy theorem.
The axiom of replacement does have an important role in the study of set theory itself. For example, the replacement schema is needed to construct the von Neumann ordinals from ω·2 onwards; without replacement, it would be necessary to find some other representation for ordinal numbers.
Although the axiom schema of replacement is a standard axiom in set theory today, it is often omitted from systems of type theory and foundation systems in topos theory.
Relation to the axiom schema of separation.
The axiom schema of separation, the other axiom schema in ZFC, is implied by the axiom schema of replacement and the axiom of empty set. Recall that the axiom schema of separation includes 
for each formula θ in the language of set theory in which "B" is not free.
The proof is as follows. Begin with a formula θ(C) that does not mention "B", and a set "A". If no element "E" of "A" satisfies θ then the set "B" desired by the relevant instance of the axiom schema of separation is the empty set. Otherwise, choose a fixed "E" in "A" such that θ("E") holds. Define a class function "F" such that "F"("D") = "D" if θ("D") holds and "F"("D") = "E" if "θ"("D") is false. Then the set "B" = "F" ""A" = "A"∩{"x"|θ("x")} exists, by the axiom of replacement, and is precisely the set "B" required for the axiom of separation.
This result shows that it is possible to axiomatize ZFC with a single infinite axiom schema. Because at least one such infinite schema is required (ZFC is not finitely axiomatizable), this shows that the axiom schema of replacement can stand as the only infinite axiom schema in ZFC if desired. Because the axiom schema of separation is not independent, it is sometimes omitted from contemporary statements of the Zermelo-Fraenkel axioms.
Separation is still important, however, for use in fragments of ZFC, because of historical considerations, and for comparison with alternative axiomatizations of set theory. A formulation of set theory that does not include the axiom of replacement will likely include some form of the axiom of separation, to ensure that its models contain a robust collection of sets. In the study of models of set theory, it is sometimes useful to consider models of ZFC without replacement.
The proof above uses the law of excluded middle in assuming that if "A" is nonempty then it must contain an element (in intuitionistic logic, a set is "empty" if it does not contain an element, and "nonempty" is the formal negation of this, which is weaker than "does contain an element"). The axiom of separation is included in intuitionistic set theory.

</doc>
<doc id="52389" url="http://en.wikipedia.org/wiki?curid=52389" title="Independence Day (1996 film)">
Independence Day (1996 film)

Independence Day is a 1996 American science fiction disaster film co-written and directed by Roland Emmerich. The film stars Will Smith, Bill Pullman, Jeff Goldblum, Mary McDonnell, Judd Hirsch, Margaret Colin, Randy Quaid, Robert Loggia, James Rebhorn, Vivica A. Fox, and Harry Connick, Jr. The film focuses on a disparate group of people who converge in the Nevada desert in the aftermath of a destructive alien attack and, along with the rest of the human population, participate in a last-chance counterattack on July 4, the same date as the Independence Day holiday in the United States. The screenplay was written by Emmerich and producer Dean Devlin.
While promoting "Stargate" in Europe, Emmerich came up with the idea for the film when fielding a question about his own belief in the existence of alien life. He and Devlin decided to incorporate a large-scale attack when noticing that aliens in most invasion films travel long distances in outer space only to remain hidden when reaching Earth. Principal photography for the film began in July 1995 in New York City, and the film was officially completed on June 20, 1996.
The film was scheduled for release on July 3, 1996, but due to its high level of anticipation, many theaters began showing it on the evening of July 2, 1996, the same day the story of the film begins. The film's combined domestic and international box office gross is $817,400,891, which, at the time, was the second-highest worldwide gross of all time. It is currently the 46th highest-grossing film of all time and was at the forefront of the large-scale disaster film and science fiction resurgences of the mid-to-late-1990s. It won the Academy Award for Best Visual Effects, and was also nominated for Best Sound Mixing.
A sequel to "Independence Day" is scheduled to be released on June 24, 2016.
Plot.
On July 2, a 500 km wide alien mothership enters Earth's orbit and deploys several dozen saucer-shaped "destroyer" spacecraft, each 15 mi wide. As they take position over some of Earth's major cities, David Levinson (Jeff Goldblum), an MIT graduate working for a cable company in New York City, discovers hidden transmissions in Earth's satellites which he realizes is a timer counting down to a coordinated attack by the aliens. With the support of his estranged wife Constance Spano (Margaret Colin), the White House Press Secretary, he and his father Julius (Judd Hirsch) gain entrance into the Oval Office to notify President Thomas J. Whitmore (Bill Pullman) about the attack. Whitmore orders large-scale evacuations of the targeted cities, but the aliens attack with advanced directed-energy weapons before these can be carried out. Whitmore, portions of his staff, and the Levinsons narrowly escape aboard Air Force One as Washington, D.C. is destroyed.
On July 3, the Black Knights, a squadron of Marine Corps F/A-18 Hornets, participate in an assault on a destroyer near the ruins of Los Angeles. Their weapons fail to penetrate the craft's force field. Dozens of "attacker" ships are launched by the aliens in defense, and a one-sided dogfight ensues in which nearly all the Hornets are destroyed. Afterwards, many American military installations, including NORAD, are destroyed, killing the Vice President and most of the Cabinet who had been hiding there. Captain Steven Hiller (Will Smith) is the only pilot to survive the Los Angeles assault by luring a single attacker to the Grand Canyon and causing their aircraft to crash into the desert. He subdues the injured alien and is rescued by Russell Casse (Randy Quaid), who is traveling across the desert with a group of refugees. They take the alien to nearby Area 51, where Whitmore and his remaining staff have also landed. Area 51 conceals a top-secret facility housing a repaired attacker and three alien bodies recovered from Roswell in 1947.
When scientist Dr. Brackish Okun (Brent Spiner) attempts to autopsy the alien, it regains consciousness and attempts to escape. When questioned by Whitmore, the alien attempts a psychic attack against him, but is killed by Whitemore's security detail. Whitmore then mentions that while he was being attacked, he saw the alien's thoughts; what its species were planning to do. They were like locusts; their entire species travel from planet to planet, destroying all life and harvesting the natural resources. Whitmore orders a nuclear attack on the destroyers, but the first attempt in Houston fails to penetrate the force field of the destroyer and the remaining strikes are aborted.
On July 4, Levinson devises a plan to use the repaired attacker to introduce a computer virus and plant a nuclear missile on board the mothership, theorizing that this will disrupt the force fields of the destroyers. Hiller volunteers to pilot the attacker, with Levinson accompanying him. With not enough military pilots to man all available aircraft, volunteers including Whitmore and Casse are enlisted for the counterstrike.
With the successful implantation of the virus, Whitmore leads the attack against an alien destroyer approaching Area 51. Although the force field is deactivated and the fighters are able to inflict damage, their supply of missiles quickly becomes exhausted. As the destroyer prepares to fire on the base, Casse has one missile left, but it jams, and he decides to sacrifice his own life. He flies his aircraft into the alien weapon with a kamikaze attack, destroying the craft. The Americans inform resistance forces around the world about how to destroy the other craft, while the nuclear device destroys the alien mothership as Hiller and Levinson escape. They return unharmed and reunite with their families. The whole world then celebrates its heroes' victory as well as its true 'Independence Day'.
Production.
The idea for the film came when Emmerich and Devlin were in Europe promoting their film "Stargate". A reporter asked Emmerich why he made a film with content like "Stargate" if he did not believe in aliens. Emmerich stated he was still fascinated by the idea of an alien arrival, and further explained his response by asking the reporter to imagine what it would be like to wake up one morning and discover 15-mile-wide spaceships were hovering over the world's largest cities. Emmerich then turned to Devlin and said, "I think I have an idea for our next film."
Emmerich and Devlin decided to expand on the idea by incorporating a large-scale attack, with Devlin saying he was bothered by the fact that "for the most part, in alien invasion movies, they come down to Earth and they're hidden in some back field ...[o]r they arrive in little spores and inject themselves into the back of someone's head." Emmerich agreed by asking Devlin if arriving from across the galaxy, "would you hide on a farm or would you make a big entrance?" The two wrote the script during a month-long vacation in Mexico, and just one day after they sent it out for consideration, 20th Century Fox chairman Peter Chernin greenlit the screenplay. Pre-production began just three days later in February 1995. The U.S. military originally intended to provide personnel, vehicles, and costumes for the film; however, they backed out when the producers refused to remove the script's Area 51 references.
A then-record 3,000-plus special effects shots would ultimately be required for the film. The shoot utilized on-set, in-camera special effects more often than computer-generated effects in an effort to save money and get more authentic pyrotechnic results. Many of these shots were accomplished at Hughes Aircraft in Culver City, California, where the film's art department, motion control photography teams, pyrotechnics team, and model shop were headquartered. The production's model-making department built more than twice as many miniatures for the production than had ever been built for any film before by creating miniatures for buildings, city streets, aircraft, landmarks, and monuments. The crew also built miniatures for several of the spaceships featured in the film, including a 30-foot (9.1 m) destroyer model and a version of the mother ship spanning 12 ft. City streets were recreated, then tilted upright beneath a high-speed camera mounted on a scaffolding filming downwards. An explosion would be ignited below the model, and flames would rise towards the camera, engulfing the tilted model and creating the rolling "wall of destruction" look seen in the film. A model of the White House was also created, covering 10 ft by 5 ft, and was used in forced-perspective shots before being destroyed in a similar fashion for its own destruction scene. The detonation took a week to plan and required 40 explosive charges.
The film's aliens were designed by production designer Patrick Tatopoulos. The actual aliens of the film are diminutive and based on a design Tatopoulos drew when tasked by Emmerich to create an alien that was "both familiar and completely original". These creatures wear "bio-mechanical" suits that are based on another design Tatopoulos pitched to Emmerich. These suits were 8 ft tall, equipped with 25 tentacles, and purposely designed to show it could not sustain a person inside so it would not appear to be a "man in a suit".
Principal photography began in July 1995 in New York City. A second unit gathered plate shots and establishing shots of Manhattan, Washington D.C., an RV community in Flagstaff, Arizona, and the Very Large Array on the Plains of San Agustin, New Mexico. The main crew also filmed in nearby Cliffside Park, New Jersey before moving to the former Kaiser Steel mill in Fontana, California to film the post-attack Los Angeles sequences. The production then moved to Wendover, Utah and West Wendover, Nevada, where the deserts doubled for Imperial Valley and the Wendover Airport doubled for the El Toro and Area 51 exteriors. It was here where Pullman filmed his pre-battle speech. Immediately before filming the scene, Devlin and Pullman decided to add "Today, we celebrate our Independence Day!" to the end of the speech. At the time, the production was nicknamed "ID4" because Warner Bros. owned the rights to the title "Independence Day", and Devlin had hoped if Fox executives noticed the addition in dailies, the impact of the new dialogue would help them win the rights to the title. The right to use the title was eventually won two weeks later.
The production team moved to the Bonneville Salt Flats to film three scenes, then returned to California to film in various places around Los Angeles, including Hughes Aircraft where sets for the cable company and Area 51 interiors were constructed at a former aircraft plant. Sets for the latter included corridors containing windows that were covered with blue material. The filmmakers originally intended to use the chroma key technique to make it appear as if activity was happening on the other side of the glass; but the composited images were not added to the final print because production designers decided the blue panels gave the sets a "clinical look". The attacker hangar set contained an attacker mock-up 65 ft wide that took four months to build. The White House interior sets used had already been built for "The American President" and had previously been used for "Nixon". Principal photography completed on November 3, 1995.
The film originally depicted Russell Casse being rejected as a volunteer for the July 4 aerial counteroffensive because of his alcoholism. He then uses a stolen missile tied to his red biplane to carry out his suicide mission. According to Dean Devlin, test audiences responded well to the scene's irony and comedic value. However, the scene was re-shot to include Russell's acceptance as a volunteer, his crash course in modern fighter aircraft, and him flying an F/A-18 instead of the biplane. Devlin preferred the alteration because the viewer now witnesses Russell ultimately making the decision to sacrifice his life, and seeing the biplane keeping pace and flying amongst F/A-18s was "just not believable". The film was officially completed on June 20, 1996.
Music.
The score was composed by David Arnold and has received two official CD releases. RCA released a 50-minute album at the time of the film's release. Then in 2010, La-La Land Records released a limited edition 2-CD set that comprised the complete score plus 12 alternate cues.
Release.
While the film was still in post-production, 20th Century Fox began a massive marketing campaign to help promote the film, beginning with the airing of a dramatic commercial during Super Bowl XXX, for which Fox paid $1.3 million. The film's subsequent success at the box office resulted in the trend of using Super Bowl air time to kick off the advertising campaign for potential blockbusters.
Fox's Licensing and Merchandising division also entered into co-promotional deals with Apple Inc. The co-marketing project was dubbed "The Power to Save the World" campaign, in which the company used footage of David using his PowerBook laptop in their print and television advertisements. Trendmasters entered a merchandising deal with the film's producers to create a line of tie-in toys. In exchange for product placement, Fox also entered into co-promotional deals with Molson Coors Brewing Company and Coca-Cola.
The film was marketed with several taglines, including: "We've always believed we weren't alone. On July 4, we'll wish we were", "Earth. Take a good look. It could be your last", and "Don't make plans for August". The weekend before the film's release, the Fox Network aired a half-hour special on the film, the first third of which was a spoof news report on the events that happen in the film. Roger Ebert attributed most of the film's early success to its teaser trailers and marketing campaigns, acknowledging them as "truly brilliant".
The film had its official premiere held at Los Angeles' now-defunct Mann Plaza Theater on June 25, 1996. It was then screened privately at the White House for President Bill Clinton and his family before receiving a nationwide release in the United States on July 2, 1996, a day earlier than its previously scheduled opening.
After a six-week, $30 million marketing campaign, "Independence Day" was released on VHS on November 22, 1996. It became available on DVD on June 27, 2000, and has been re-released on DVD under several different versions with varying supplemental material ever since, including one instance where it was packaged with a lenticular cover. Often accessible on these versions is a special edition of the film, which features nine minutes of additional footage not seen in the original theatrical release. "Independence Day" became available on Blu-ray discs in the United Kingdom on December 24, 2007, and in North America on March 11, 2008. The Blu-ray edition does not include the deleted scenes.
Censorship.
In Lebanon, certain Jewish and Israel-related content of the film was censored. One cut scene involved Judd Hirsch's character donning a kippah and leading soldiers and White House officials in a Jewish prayer. Other removed footage showed Israeli and Arab troops working together in preparation for countering the alien invasion. The Lebanese Shi'a Islamist militant group Hezbollah called for Muslims to boycott the film, describing it as "propaganda for the so-called genius of the Jews and their concern for humanity." In response, Jewish actor Jeff Goldblum said, "I think Hezbollah has missed the point: the film is not about American Jews saving the world; it's about teamwork among people of different religions and nationalities to defeat a common enemy."
Reception.
Box office.
"Independence Day" was the highest-grossing film of 1996. In the United States, "Independence Day" earned $104.3 million in its first full week, including $96.1 million during its five-day holiday opening, and $50.2 million during its opening weekend. All three figures broke records set by "Jurassic Park" three years earlier. That film's sequel, "", claimed all three records when it was released the following year. "Independence Day" stayed in the number-one spot for three weeks, and grossed $306,169,268 in the domestic market and $510,800,000 in foreign markets during its theatrical run. The combined total of $817,400,891 once trailed only the worldwide earnings of "Jurassic Park" as the highest of all-time. It has been surpassed by multiple 21st century films since, and currently holds the 43rd highest worldwide gross of all-time for a film. Hoping to capitalize in the wake of the film's success, several studios released more large-scale disaster films, and the already rising interest in science fiction-related media was further increased by the film's popularity.
A month after the film's release, jewelry designers and marketing consultants reported an increased interest in dolphin-themed jewelry, since the character of Jasmine in the film wears dolphin earrings and is presented with a wedding ring featuring a gold dolphin.
Critical response.
"Independence Day" is ranked as "fresh" on Rotten Tomatoes with a 60% positive approval rating (34 out of 57 critics gave positive reviews). It has a score of 59 out of 100 (based on 19 reviews) on Metacritic, which indicates the high end of "mixed or average reviews". Critics acknowledged the film had "cardboard" and "stereotypical" characters, and weak dialogue. Yet the shot of the White House's destruction has been declared a milestone in visual effects and one of the most memorable scenes of the 1990s. In a 2010 poll, the readers of "Entertainment Weekly" rated it the second-greatest summer film of the previous 20 years, ranking only behind "Jurassic Park".
Mick LaSalle of the "San Francisco Chronicle" gave the film his highest rating, declaring it the "apotheosis" of "". Lisa Schwarzbaum of "Entertainment Weekly" gave it a B+ for living up to its massive hype, adding "charm is the foremost of this epic's contemporary characteristics. The script is witty, knowing, cool." Eight years later, "Entertainment Weekly" would rate the film as one of the best disaster films of all-time. Kenneth Turan of the "Los Angeles Times" felt that the film did an "excellent job conveying the boggling immensity of [the] extraterrestrial vehicles [...] and panic in the streets" and the scenes of the alien attack were "disturbing, unsettling and completely convincing".
However, the film's nationalistic overtones were widely criticized by reviewers outside the U.S. "Movie Review UK" described the film as "A mish-mash of elements from a wide variety of alien invasion movies and gung-ho American jingoism." The speech in which Whitmore states that victory in the coming war would see the entire world henceforth describe July 4 as its Independence Day, was described as "the most jaw-droppingly pompous soliloquy ever delivered in a mainstream Hollywood movie" in a BBC review. In 2003, readers of "Empire", voted the scene that contained the speech as the "Cheesiest Movie Moment of All-Time". Conversely, "Empire" critic Kim Newman gave the film a five-star rating in the magazine's original review of the film.
Several prominent critics expressed disappointment with the quality of the film's special effects. "Newsweeks David Ansen claimed the special effects were of no better caliber than those seen nineteen years earlier in '. Todd McCarthy of "Variety" felt the production's budget-conscious approach resulted in "cheesy" shots that lacked in quality relative to the effects present in films directed by James Cameron and Steven Spielberg. In his review, Roger Ebert took note of a lack of imagination in the spaceship and creature designs. Gene Siskel expressed the same sentiments in their on-air review of the film.
American Film Institute lists
In other media.
Books.
Author Stephen Molstad wrote a tie-in novel to help promote the film shortly before its release. The novel goes into further detail on the characters, situations, and overall concept not explored in the film. The novel presents the film's finale as originally scripted, with the character played by Randy Quaid stealing a missile and roping it to his crop duster biplane.
Following the film's success, a prequel novel entitled "Independence Day: Silent Zone" was written by Molstad in February 1998. The novel is set in the late 1960s and early 1970s, and details the early career of Dr. Brackish Okun.
Molstad wrote a third novel, "Independence Day: War in the Desert" in July 1999. Set in Saudi Arabia on July 3, it centers around Captain Cummins and Colonel Thompson, the two Royal Air Force officers seen receiving the Morse code message in the film.
A Marvel comic book was also written based on the first two novelizations.
Radio.
On August 4, 1996, BBC Radio 1 broadcast the one-hour play "Independence Day UK", written, produced, and directed by Dirk Maggs, a spin-off depicting the alien invasion from a British perspective. None of the original cast was present. Dean Devlin gave Maggs permission to produce an original version, on the condition he did not reveal certain details of the movie's plot and the British were not depicted as saving the day. "Independence Day UK" was set up to be similar to the 1938 radio broadcast of "The War of the Worlds"; the first 20 minutes were set as being live.
Computer games.
An "Independence Day" video game was released in February 1997 for the PlayStation, Sega Saturn, and PC, each version receiving mostly tepid reviews. The multi-view shooter game contains various missions to perform, with the ultimate goal of destroying the aliens' primary weapon. A wireless mobile version was released in 2005. A computer game entitled "ID4 Online" was released in 2000.
Toys.
Trendmasters released a toy line for the film in 1996. Each action figure, vehicle or playset came with a 3 1⁄2" floppy disk that contained an interactive computer game.
Sequel.
The possibility of a sequel had long been discussed, and Devlin once stated the world's reaction to the September 11 attacks influenced him to strongly consider making a sequel to the film. Devlin began writing an outline for a script with Emmerich, but in May 2004, Emmerich said he and Devlin had attempted to "figure out a way how to continue the story", but that this ultimately did not work, and the pair abandoned the idea. In October 2009, Emmerich said he once again had plans for a sequel, and has since considered the idea of making two sequels to form a trilogy. On June 24, 2011, Devlin confirmed that he and Emmerich have found an idea for the sequels and have written a treatment for it, with both Emmerich and Devlin having the desire for Will Smith to return for the sequels. In October 2011, however, discussions for Smith returning were halted, due to Fox's refusal to provide the $50 million salary demanded by Smith for the two sequels. Emmerich, however, made assurances that the films would be shot back-to-back, regardless of Smith's involvement. In July 2012, Devlin reiterated that the "Independence Day" sequel is still in development, and the script currently takes place in 2012, 16 years after the original film's events.
In March 2013, Emmerich stated that the titles of the new films would be "ID Forever Part I" and "ID Forever Part II". The films will take place twenty years after the original, when reinforcements of the original alien race arrive at Earth after finally receiving a distress call. Bill Pullman has confirmed his participation, though Will Smith has not. The new films will focus on the next generation of heroes, including the stepson of Smith's character in the original film. In May 2013, Roland Emmerich and Dean Devlin mentioned that wormholes would be used as a plot device in "ID Forever" and added that they would like Jeff Goldblum to reprise his role from the original. The film was originally going to be released on July 3, 2015. In June 2013, Emmerich confirmed to the "New York Daily News" that Will Smith is not returning to the sequel because "he’s too expensive". Later in June, it was officially confirmed that both Goldblum and Pullman would return in the sequel, and that a gay character would be prominently featured. On September 26, 2013, actor Michael B. Jordan was said to have been considered for a role in the film. On November 12, 2013, it was announced that the first sequel had been rescheduled for a July 1, 2016 release. On May 29, 2014, it was announced that the script for the first sequel written by Emmerich and Devlin, would be rewritten by Carter Blanchard. On October 14, 2014, Fox moved up the release date to June 24, 2016. On November 26, 2014, the sequel was given an official green light by 20th Century Fox with a release date of June 24, 2016, noting that this will be a stand-alone sequel that will not split into two parts as originally planned, with filming beginning in May 2015 and casting being done after the studio locks down Emmerich as the director on the film. On December 4, 2014, Devlin confirmed that Emmerich would indeed be directing the sequel. On January 27, 2015, "The Wrap" reports that Liam Hemsworth is being eyed for a role in the sequel. In March 2015, Jessie Usher was cast as Steve's son. Emmerich announced on Twitter that Hemsworth joined the cast along with Charlotte Gainsbourg while Jeff Goldblum and Vivica A. Fox were returning as David Levinson and Jasmine Dubrow in the film. In an interview with "The Hollywood Reporter", Emmerich said that film sequel will cost about $150-$160 million. On March 24, 2015 it was revealed that Emmerich is in talks with Travis Tope for an undisclosed role.
It was later announced that Brent Spiner would return for the sequel and Joey King will be a new addition. On April 17, 2015, it was announced that Bill Pullman and Judd Hirsch had officially signed on to return. On April 27, 2015, Emmerich announced on his Twitter page that Maika Monroe was cast in the sequel.
Later on 4th May 2015 it was declared that Sela Ward will play the President Of The United States and on 8th May 2015 DeObia Oparei will play the role Dikembe, the Oxford-educated son of an African warlord.

</doc>
<doc id="52390" url="http://en.wikipedia.org/wiki?curid=52390" title="Armageddon (1998 film)">
Armageddon (1998 film)

Armageddon is a 1998 American science fiction disaster thriller film directed by Michael Bay, produced by Jerry Bruckheimer, and released by Touchstone Pictures. The film follows a group of blue-collar deep-core drillers sent by NASA to stop a gigantic asteroid on a collision course with Earth. It features an ensemble cast including Bruce Willis, Ben Affleck, Billy Bob Thornton, Liv Tyler, Owen Wilson, Will Patton, Peter Stormare, William Fichtner, Michael Clarke Duncan, Keith David, and Steve Buscemi.
"Armageddon" opened in theaters only two and a half months after a similar impact-based film, "Deep Impact", which starred Robert Duvall and Morgan Freeman. "Armageddon" fared better at the box office, while astronomers described "Deep Impact" as being more scientifically accurate. Both films were equally received by film critics. "Armageddon" was an international box-office success, despite generally mixed reviews from critics. It became the highest-grossing film of 1998 worldwide, surpassing the Steven Spielberg war epic "Saving Private Ryan".
Plot.
A massive meteor shower destroys the orbiting Space Shuttle Atlantis and bombards a swath of land from America's East Coast from South Carolina through Finland. NASA discovers that a rogue asteroid the size of Texas passed through the asteroid belt and pushed forward a large amount of space debris. The asteroid will collide with Earth in 18 days, causing an extinction event. NASA scientists, led by Dan Truman, plan to trigger a nuclear detonation 800 ft inside the asteroid to split it in two, driving the pieces apart so both will fly past the Earth. NASA contacts Harry Stamper, considered the best deep-sea oil driller in the world, for assistance. Harry travels to NASA with his daughter Grace, to keep her away from her new boyfriend and one of Harry's drillers, A. J. Frost. Harry explains he will need his team, including A. J., to carry out the mission. They agree to help, but only after their list of unusual rewards and demands are met.
NASA plans to launch two shuttles, "Freedom" and "Independence", to increase the chances of success; the shuttles will refill with liquid oxygen from the Russian space station Mir before making a slingshot maneuver around the Moon to approach the asteroid from behind. NASA puts Harry and his crew through a short and rigorous astronaut training program, while Harry and his team re-outfit the mobile drillers, "Armadillos", for the job.
The destruction of Shanghai by an asteroid fragment forces NASA to reveal the asteroid's existence, as well as their plan. The shuttles are launched and arrive at Mir, where its sole cosmonaut Lev helps with refueling. A major fire breaks out during the fueling process, forcing the crews, including Lev, to evacuate to the shuttles before Mir explodes. The shuttles perform the slingshot around the moon, but approaching the asteroid, the "Independence"'s engines are destroyed by trailing debris, and it crashes on the asteroid. Grace, aware A.J. was aboard the "Independence", is traumatized by this news. Unknown to the others, A.J., Lev, and "Bear" (another of Harry's crew) survive the impact and head towards the target site in their Armadillo.
Meanwhile, "Freedom" safely lands on the asteroid, but overshoots the target zone, landing on a much harder metallic field than planned. Their drilling quickly falls behind schedule. The military initiates "Secondary Protocol" to remotely detonate the nuclear weapon on the asteroid's surface, despite Truman's insistence that it would be ineffective. Truman alerts Harry, and with the shuttle commander's help, they disarm the remote trigger. Harry's crew continues to work, but in their haste, they accidentally hit a gas pocket, blowing their Armadillo into space. As the world learns of the mission's apparent failure, another asteroid fragment devastates Paris. 
All seems lost until the arrival of the "Independence"'s Armadillo. With A.J. at the controls, they reach the required depth for the bomb. However, flying debris from the asteroid damages the triggering device, requiring someone to stay behind to manually detonate the bomb. The crew draw straws, and A.J. is selected. As he and Harry exit the airlock, Harry rips off A.J.'s air hose and shoves him back inside, telling him he is the son Harry never had, and he would be proud to have A.J. marry Grace. Harry prepares to detonate the bomb and contacts Grace to bid his final farewell. After the "Freedom" moves to a safe distance, Harry presses the button at the last moment, and the bomb successfully splits the asteroid, avoiding a collision with Earth. "Freedom" lands, and the surviving crew are treated as heroes. A.J. and Grace get married, with photos of Harry and the other lost crew members present.
Production.
In May 1998, Walt Disney Studios chairman Joe Roth expanded the film's budget by $3 million to include additional special effects scenes. This additional footage, incorporated two months prior to the film's release, was specifically added for the television advertising campaign to differentiate the film from "Deep Impact" which was released a few months before.
Nine writers worked on the script, five of whom are credited. In addition to Robert Roy Pool, Jonathan Hensleigh, Tony Gilroy, Shane Salerno and J.J. Abrams, the writers involved included Paul Attanasio, Ann Biderman, Scott Rosenberg and Robert Towne. Originally, it was Hensleigh’s script, based on Pool’s original, that had been greenlighted by Touchstone. Then producer Jerry Bruckheimer hired the succession of scribes for rewrites and polishes.
Music.
Two soundtrack albums were released for the film. The first, "", was released by Columbia Records on June 23, 1998; it consists mainly of songs from the film, with one score suite.
A more complete album of the film score by composers Trevor Rabin and Harry Gregson-Williams was released as "Armageddon: Original Motion Picture Score" by Sony Classical on November 10, 1998.
Release.
Prior to "Armageddon"‍ '​s release, the film was advertised in Super Bowl XXXII at a cost of $2.6 million.
Home media.
Despite a mixed critical reception, a DVD edition of "Armageddon" was released by The Criterion Collection, a specialist film distributor of primarily arthouse films that markets what it considers to be "important classic and contemporary films" and "cinema at its finest". In an essay supporting the selection of "Armageddon", film scholar Jeanine Basinger, who taught Michael Bay at Wesleyan University, states that the film is "a work of art by a cutting-edge artist who is a master of movement, light, color, and shape—and also of chaos, razzle-dazzle, and explosion". She sees it as a celebration of working men: "This film makes these ordinary men noble, lifting their efforts up into an epic event." Further, she states that in the first few moments of the film all the main characters are well established, saying, "If that isn't screenwriting, I don't know what is". The film was also released by Touchstone Home Entertainment on standard edition Blu-ray disc in 2010 with only a few special features.
Space Shuttle "Columbia" disaster.
Following the 2003 "Columbia" disaster, some screen captures from the opening scene where "Atlantis" is destroyed were passed off as satellite images of the disaster in a hoax. Also, in response to the disaster, FX pulled "Armageddon" from the night's schedule and replaced it with "Aliens".
Reception.
Box office.
"Armageddon" was released on July 1, 1998 in 3,127 theaters in the United States and Canada. It ranked first at the box office with an opening weekend gross of $36 million. It grossed $201.6 million in the United States and Canada and $352.1 million in other territories for a worldwide total of $553.7 million.
Critical response.
"Armageddon" received mostly negative reviews from film critics, who mainly took issue with "the furious pace of its editing". The film is on the list of Roger Ebert's most hated films. In his original review, Ebert stated, "The movie is an assault on the eyes, the ears, the brain, common sense and the human desire to be entertained". Ebert went on to name "Armageddon" as the worst film of 1998. Todd McCarthy of "Variety" also gave the film a negative review, noting Michael Bay's rapid cutting style: "Much of the confusion, as well as the lack of dramatic rhythm or character development, results directly from Bay's cutting style, which resembles a machine gun stuck in the firing position for 2½ hours." The film has a cumulative 39% "Rotten" rating on Rotten Tomatoes, while achieving a 42% aggregate score on "Metacritic".
According to Bruce Joel Rubin, writer of "Deep Impact", a production president at Disney took notes on everything the writer said during lunch about his script and initiated "Armageddon" as a counter film at Disney.
In April 2013, in a "Miami Herald" interview to promote "Pain & Gain", Bay was quoted as having said:
...We had to do the whole movie in 16 weeks. It was a massive undertaking. That was not fair to the movie. I would redo the entire third act if I could. But the studio literally took the movie away from us. It was terrible. My visual effects supervisor had a nervous breakdown, so I had to be in charge of that. I called James Cameron and asked ‘What do you do when you’re doing all the effects yourself?’ But the movie did fine.
Some time after the article was published, Bay corrected his stance, claiming that his apology only related to the editing of the film, not the whole film, and accused the writer of the article for taking his words out of context. The author of the article, "Miami Herald" writer Rene Rodriguez claimed: "NBC asked me for a response, and I played them the tape. I didn’t misquote anyone. All the sites that picked up the story did."
Scientific accuracy.
In an interview with "Entertainment Weekly", Bay admitted that the film's central premise "that [NASA] could actually do something in a situation like this" was unrealistic. Robert Roy Pool, a contributing screenwriter, stated that his script, in which an anti-gravity device is used to deflect a comet from a collision course with Earth, was "much more in line with top-secret research." Additionally, near the end of the credits, there is a disclaimer stating, "The National Aeronautics and Space Administration's cooperation and assistance does not reflect an endorsement of the contents of the film or the treatment of the characters depicted therein."
In 2012, an article titled "Could Bruce Willis Save the World?" was published in the "Journal of Physics Special Topics", an undergraduate journal used as a teaching tool at the University of Leicester. It found that for Willis' approach to be effective, he would need to be in possession of an H-bomb a billion times stronger than the Soviet Union's "Big Ivan", the biggest ever detonated on Earth. Using estimates of the asteroid's size, density, speed and distance from Earth based on information in the film, students found that to split the asteroid in two with both pieces clearing Earth, would require 800 zettajoules of energy. In contrast, the total energy output of "Big Ivan", which was tested by the Soviet Union in 1961, was only 418 petajoules.
Accolades.
The film received four Academy Award nominations at the 71st Academy Awards, including; Best Sound (Kevin O'Connell, Greg P. Russell and Keith A. Wester), Best Visual Effects, Best Sound Effects Editing, and Best Original Song ("I Don't Want to Miss a Thing" performed by Aerosmith). The film received the Saturn Awards for Best Direction and Best Science Fiction Film (where it tied with "Dark City"). It was also nominated for seven Razzie Awards including: Worst Actor (Bruce Willis), Worst Picture, Worst Director, Worst Screenplay, Worst Supporting Actress (Liv Tyler), Worst Screen Couple (Tyler and Ben Affleck) and Worst Original Song. Only one Razzie was awarded: Bruce Willis received the Worst Actor award for "Armageddon", in addition to his appearances in "Mercury Rising" and "The Siege", both released in the same year as this film.
Theme park attraction.
Armageddon – Les Effets Speciaux is an attraction based on "Armageddon" at Walt Disney Studios Park located at Disneyland Paris. The attraction simulates the scene in the movie in which the Russian Space Station is destroyed. Michael Clarke Duncan ("Bear" in the film) is featured in the pre-show.

</doc>
<doc id="52392" url="http://en.wikipedia.org/wiki?curid=52392" title="Prince Maximilian of Baden">
Prince Maximilian of Baden

Maximilian Alexander Friedrich Wilhelm, Prince of Baden (10 July 1867 – 6 November 1929), also known as "Max von Baden", was a German prince and politician. He was heir to the Grand Duchy of Baden and in October and November 1918 briefly served as Chancellor of the German Empire, overseeing the transformation into a parliamentary system during the "October reforms" at the end of World War I.
Early life.
Born in Baden-Baden on 10 July 1867, Maximilian was a member of the House of Baden, the son of Prince Wilhelm Max (1829–1897), third son of Grand Duke Leopold (1790–1852) and Princess Maria Maximilianovna of Leuchtenberg (1841-1914), a granddaughter of Eugène de Beauharnais and niece of Tsar Alexander II of Russia. He was named after his maternal grandfather, Maximilian de Beauharnais, and bore a resemblance to his cousin, Emperor Napoleon III.
Max received a humanistic education at a "Gymnasium" secondary school and studied law and cameralism at the Leipzig University. In 1900, he married Princess Marie Louise of Hanover (1879-1948) at Gmunden. Upon the order of Queen Victoria, Prince Max was brought to Darmstadt in the kingdom of Hesse as a suitor for Victoria's granddaughter, Alix of Hesse-Darmstadt. Alix was the daughter of Victoria's late daughter, Princess Alice of the United Kingdom and Louis IV, Grand Duke of Hesse. Alix quickly rejected Prince Max. She was in love with Nicholas II, the future Tsar of Russia.
Early military and political career.
After finishing his studies, he trained as an officer of the Prussian Army. Following the death of his uncle Grand Duke Frederick I of Baden in 1907, he became designated heir to the grand-ducal throne of his cousin Frederick II, whose marriage remained childless. He also became president of the "Erste Badische Kammer" (the upper house of the parliament of Baden). In 1911, Max applied for a military discharge with the rank of a "Generalmajor" (Major general). 
World War I.
Upon the outbreak of World War I in 1914, he served as a general staff officer at the XIV Corps of the German Army as the representative of the Grand Duke (XIV Corps inlcluded the troops from Baden). Shortly afterwards, however, he retired from his position ("General der Kavallerie à la suite") as he was dissatisfied with his role in the military and was suffering from ill health.
In October 1914, he became honorary president of the Baden section of the German Red Cross, thus beginning his work for prisoners-of-war in- and outside of Germany in which he made use of his family connections to the Russian and Swedish courts as well as his connections to Switzerland. In 1916, he became honorary president of the German-American support union for prisoners-of-war within the YMCA world alliance. 
Due to his liberal stance he came into conflict with the policies of the "Oberste Heeresleitung" (OHL - Supreme Army Command) supreme command under Paul von Hindenburg and Erich Ludendorff. He openly spoke against the resumption of the unrestricted submarine warfare in 1917, which provoked the declaration of war by the United States Congress on 6 April. 
His activity in the interests of prisoners-of-war, as well as his tolerant, easy-going character gave him a reputation as an urbane personality who kept his distance from the extremes of nationalism and official war enthusiasm in evidence elsewhere at the time. Since he was almost unknown in public, it was mainly due to Kurt Hahn, since spring 1917 in the military office of the Foreign Ministry, that he was later considered for the position of Chancellor. Hahn maintained close links with Secretary of State Wilhelm Solf and several Reichstag deputies like Eduard David (SPD) and Conrad Haußmann (FVP). David pushed for Max to be appointed Chancellor in July 1917, after the fall of Chancellor Bethmann-Hollweg. Max then put himself forward for the position in early September 1918, pointing out his links to the social democrats, but Emperor Wilhelm II turned him down.
Chancellor.
Appointment.
After the "Oberste Heeresleitung" told the government in late September 1918 that the German front was about to collapse and asked for immediate negotiation of an armistice, the cabinet of Chancellor Georg von Hertling resigned on 30 September 1918. Hertling, after consulting Vice-Chancellor Friedrich von Payer (FVP), suggested Max von Baden as his successor to the Emperor. However, it took the additional support of Haußmann, Oberst Hans von Haeften (the liaison between OHL and Foreign Office) and Ludendorff himself, to have Wilhelm II appoint Max as Chancellor of Germany and Minister President of Prussia. 
Max was to head a new government based on the majority parties of the Reichstag (SDP, Centre Party and FVP). When Max arrived in Berlin on 1 October he had no idea that he would be asked to approach the Allies about an armistice. Max was horrified and fought against the plan. Moreover, he also admitted openly that he was no politician and that he did not think additional steps towards "parliamentarisation" and democratisation feasible as long as the war continued. Consequently, he did not favour a liberal reform of the constitution. However, Emperor Wilhelm II convinced him to take the post and appointed him on 3 October 1918. The message asking for an armistice went out only on 4 October, not as originally planned on 1 October, hopefully to be accepted by US President Woodrow Wilson.:44
In office.
Although Max had serious reservations about the conditions under which the OHL was willing to conduct negotiations and tried to interpret Wilson's Fourteen Points in a way most favourable to the German position, he accepted the charge. He appointed a government that for the first time included representatives of the largest party in the Reichstag, the Social Democratic Party of Germany, as state secretaries: Philipp Scheidemann and Gustav Bauer. This was following up on an idea of Ludendorff's and former Foreign Secretary Paul von Hintze's (as the representative of the Hertling cabinet) who had agreed on 29 September that the request for an armistice must not come from the old regime, but from one based on the majority parties.:36–37 The official reason for appointing a government that was based on a parliamentary majority was to make it harder for the American president to refuse a peace offer. The need to convince Wilson was also the driving factor behind the move towards "parliamentarisation" that was to make the Chancellor and his government answerable to the Reichstag, as they had not been under the Empire so far. Ludendorff, however, was interested in shifting the blame for the lost war to the politicians and to the Reichstag parties.:33–34
The Allies were cautious, distrusting Max as a member of a ruling family of Germany. These doubts were intensified by the publication of a personal letter Max had written to Prince Alexander zu Hohenlohe-Schillingsfürst in early 1918, in which he had expressed criticism of "parliamentarisation" and his opposition to the "Friedensresolution" of the Reichstag of July 1917, when a majority had demanded a negotiated peace rather than a peace by victory. President Wilson reacted with reserve to the German initiative and took his time to agree to the request for an armistice, sending three diplomatic notes between 8 October and 23 October. When Ludendorff changed his mind about the armistice and suddenly advocated continued fighting, Max opposed him in a cabinet meeting on 17 October.:50 On 24 October, Ludendorff issued an army order that called Wilson's third note "unacceptable" and called on the troops to fight on. On 25 October, Hindenburg and Ludendorff then ignored explicit instructions by the Chancellor and travelled to Berlin. Max asked for Ludendorff to be dismissed and Wilhelm II agreed. On 26 October, the Emperor told Ludendorff that he had lost his trust. Ludendorff offered his resignation and Wilhelm II accepted.:51
Whilst trying to move towards an armistice, Max von Baden, advised closely by Hahn (who also wrote his speeches), Haußmann and Walter Simons worked with the representatives of the majority parties in his cabinet (Scheidemann and Bauer for the SPD, Matthias Erzberger, Karl Trimborn and Adolf Gröberfor the Centre Party, von Payer and, after 14 October, Haußmann for the FVP). Although some of the initiatives were a result of the notes sent by Wilson, they were also in line with the parties' manifestoes: making the Chancellor, his government and the Prussian Minister of War answerable to parliament (Reichstag and "Preußischer Landtag"), introducing a more democratic voting system in the place of the "Dreiklassenwahlrecht" (Three-class franchise) in Prussia, the replacement of the Governor of Alsace-Lorraine with the Mayor of Straßburg, appointing a local deputy from the Centre Party as Secretary of State for Alsace-Lorraine and some other adjustments in government personnel. 
Pushed by the social democrats, the government passed a widespread amnesty, under which political prisoners like Karl Liebknecht were released. Under Max von Baden, the bureaucracy, military and political leadership of the old Reich began a cooperation with the leaders of the majority parties and with the individual States of the Reich. This cooperation would have a significant impact on later events during the revolution. 
In late October, the Imperial constitution was changed, turning the German Empire into a parliamentary system. However, Wilson's third note seemed to imply that negotiations of an armistice would be dependent on the abdication of Wilhelm II. The government of Chancellor Max von Baden now feared that a military collapse and a socialist revolution at home were becoming likelier with every day that went by. In fact, the government's efforts to secure an armistice were interrupted by the Kiel mutiny which began with events at Wilhelmshaven on 30 October and the outbreak of revolution in Germany in early November. On 1 November, Max wrote to all the ruling Princes of Germany, asking them whether they would approve of an abdication by the Emperor. On 6 November, the Chancellor sent Erzberger to conduct the negotiations with the Allies. Maximilian, seriously ill with Spanish influenza, urged Wilhelm II to abdicate. The "Kaiser", who had fled from revolutionary Berlin to the Spa headquarters of the OHL, despite similar advice by Hindenburg and Ludendorff's successor Wilhelm Groener of the OHL was willing to consider abdication only as Emperor, not as King of Prussia.
Revolution and resignation.
On 7 November, Max met with Friedrich Ebert, leader of the SPD and discussed his plan to go to Spa and convince Wilhelm II to abdicate. He was thinking about setting up Wilhelm's second son as regent.:76 However, the outbreak of the revolution in Berlin prevented Max from implementing his plan. Ebert decided that to keep control of the socialist uprising the Emperor must resign quickly and a new government was required.:77 As the masses gathered in Berlin, at noon on 9 November 1918, Maximilian went ahead and unilaterally announced the abdication, as well as the renunciation of Crown Prince Wilhelm.:86 Shortly thereafter, Ebert appeared in the Reichskanzlei and demanded that the office of government be handed over to him and the SPD, as that was the only way to keep up law and order. In an unconstitutional move, Max resigned and appointed Ebert as his successor.:87 On the same day, Philipp Scheidemann proclaimed Germany a republic. When Maximilian later visited Ebert to say goodbye before leaving Berlin, Ebert asked him to stay on as regent ("Reichsverweser"). Maximilian refused and, turning his back on politics for good, departed for Baden.:90 Although events had overtaken him during his tenure at the Reichskanzlei and he was not considered a strong Chancellor, Max is seen today as having played a vital role in enabling the transition from the old regime to a democratic government based on the majority parties and the Reichstag. This made the government of Ebert that emerged from the November revolution acceptable to some conservative forces in the bureaucracy and military. They were thus willing to ally themselves with him against the more radical demands by the revolutionaries on the far-left.
Later life and death.
Prince Maximilian, Margrave of Baden, spent the rest of his life in retirement. He rejected a mandate to the 1919 Weimar National Assembly, offered to him by the German Democratic politician Max Weber. In 1920, together with Kurt Hahn, he established the Schule Schloss Salem boarding school, which was intended to help educate a new German intellectual elite.
Max also published a number of books, assisted by Hahn: "Völkerbund und Rechtsfriede" (1919), "Die moralische Offensive" (1921) and "Erinnerungen und Dokumente" (1927). 
In 1928, following the death of Grand Duke Frederick II, Maximilian became head of the House of Baden. He died at Salem on 6 November the following year.
Children.
Maximilian was married to Princess Marie Louise of Hanover and Cumberland, eldest daughter of Ernest Augustus II of Hanover and Thyra of Denmark. The couple had two children:

</doc>
<doc id="52395" url="http://en.wikipedia.org/wiki?curid=52395" title="Martha Argerich">
Martha Argerich

Martha Argerich (born June 5, 1941) is an Argentine pianist generally considered to be one of the greatest of the second half of the 20th century.
Early life.
Argerich was born in Buenos Aires, Argentina. Her paternal ancestors were Catalonians based in Buenos Aires since the 18th century. Her maternal grandparents were Jewish immigrants from the Russian Empire, who settled in Colonia Villa Clara in the Entre Ríos province—one of the colonies established by Baron de Hirsch and the Jewish Colonization Association. The provenance of the name Argerich is from Catalonia, Spain. She started playing the piano at age three. At the age of five, she moved to teacher Vincenzo Scaramuzza, who stressed to her the importance of lyricism and feeling. Argerich gave her debut concert in 1949 at the age of eight.
The family moved to Europe in 1955, where Argerich studied with Friedrich Gulda in Austria. Juan Perón, then the president of Argentina, made their decision possible by appointing her parents to diplomatic posts in the Argentine Embassy in Vienna. She later studied with Stefan Askenase and Maria Curcio. Argerich also seized opportunities for brief periods of coaching with Madeleine Lipatti (widow of Dinu Lipatti), Abbey Simon, and Nikita Magaloff. In 1957, at sixteen, she won both the Geneva International Music Competition and the Ferruccio Busoni International Competition, within three weeks of each other. It was at the latter that she met Arturo Benedetti Michelangeli, whom she would later seek out for lessons during a personal artistic crisis at the age of twenty, though she only had four lessons with him in a year and a half. Her greatest influence was Gulda, with whom she studied for 18 months.
Professional career.
Argerich rose to international prominence when she won the seventh International Chopin Piano Competition in Warsaw in 1965, at age 24. In 1965, she debuted in the United States in the Lincoln Center's Great Performers Series. In the same year, she also made her first recording, which included works by Chopin, Brahms, Ravel, Prokofiev, and Liszt, which received critical acclaim. In 1965, she recorded Chopin's Scherzo No. 3, Polonaise, Op. 53.
Argerich has often remarked in interviews of feeling "lonely" on stage during solo performances. Since the 1980s, she has staged few solo performances, concentrating instead on concertos and, in particular, chamber music, and accompanying instrumentalists in sonatas. She is noted especially for her recordings of 20th-century works by composers such as Rachmaninoff, Messiaen and Prokofiev. One notable compilation pairs Rachmaninoff's Piano Concerto No. 3 (recorded in December 1982 with the Radio Symphonie-Orchester Berlin under the direction of Riccardo Chailly) with Tchaikovsky's Piano Concerto No. 1 (February 1980, Symphonieorchester des Bayerischen Rundfunks, Kirill Kondrashin).
Argerich is also famous for her interpretation of Prokofiev's Piano Concerto No. 3, Ravel's Piano Concerto in G, and Bach's Partita No. 2 in C minor, which she has recorded several times and continues to perform.
Argerich has also promoted younger pianists, both through her annual festival and through her appearances as a member of the jury at international competitions. The pianist Ivo Pogorelić was thrust into the musical spotlight partly as a result of Argerich's actions: after he was eliminated in the third round of the 1980 International Chopin Piano Competition in Warsaw, Argerich proclaimed him a "genius" and left the jury in protest. She has supported several artists including Gabriela Montero, Mauricio Vallina, Sergio Tiempo, Gabriele Baldocci, Christopher Falzone and others.
Argerich is president of the International Piano Academy Lake Como and performs each year at the Lugano Festival. She also created and has been General Director of the Argerich Music Festival and Encounter in Beppu, Japan, since 1996.
Her aversion to the press and publicity has resulted in her remaining out of the limelight for most of her career. Nevertheless she is widely recognized as one of the greatest pianists of her time.
Personal life.
Argerich has been married twice. Her first marriage, to composer-conductor Robert Chen (), and with whom she had a daughter, violinist Lyda Chen-Argerich, ended in 1964. From 1969 to 1973, Argerich was married to conductor Charles Dutoit, with whom she had a daughter, Annie Dutoit. Argerich continues to record and perform with Dutoit. She was also in a relationship with pianist Stephen Kovacevich, with whom she has a daughter, Stephanie.
In 1990, Argerich was diagnosed with malignant melanoma. After treatment, the cancer went into remission, but there was a recurrence in 1995, eventually metastasizing to her lungs and lymph nodes. Following aggressive treatment at the John Wayne Cancer Institute, which included the removal of part of her lung and use of an experimental vaccine, Argerich's cancer went into remission again. In gratitude, Argerich performed a Carnegie Hall recital benefiting the Institute. s of 2015[ [update]], Argerich remains cancer-free. An intimate film of her life, "Bloody Daughter", directed by her daughter Stephanie Argerich Blagojevic, was shown at festivals in 2013.

</doc>
<doc id="52396" url="http://en.wikipedia.org/wiki?curid=52396" title="Public holiday">
Public holiday

A public holiday, national holiday or legal holiday is a holiday generally established by law and is usually a non-working day during the year.
Sovereign nations and territories observe holidays based on events of significance to their history. For example, Australians celebrate Australia Day.
They vary by country and may vary by year. India leads the list with 21 National Holidays in the year 2015. Cambodia has over 20 days of official public holidays per year. Hong Kong and Egypt have 16 days of holidays per year. The public holidays are generally days of celebration, like the anniversary of a significant historical event, or can be a religious celebration like Christmas. Holidays can land on a specific day of the year, be tied to a certain day of the week in a certain month or follow other calendar systems like the Lunar Calendar.
Solemn ceremonies and children’s festivals take place throughout Turkey on National Sovereignty and Children’s Day, held on April 23 each year. Children take seats in the Turkish Parliament and symbolically govern the country for one day. 
French "Journée de solidarité envers les personnes âgées" ("Day of solidarity with the elderly") is a notable exception. This holiday became a mandatory working day although the French Council of State confirmed it remains a holiday.

</doc>
<doc id="52400" url="http://en.wikipedia.org/wiki?curid=52400" title="Day of Atonement (disambiguation)">
Day of Atonement (disambiguation)

Day of Atonement may refer to:

</doc>
<doc id="52401" url="http://en.wikipedia.org/wiki?curid=52401" title="Hairstyle">
Hairstyle

A hairstyle, hairdo, or haircut refers to the styling of hair, usually on the human scalp. The fashioning of hair can be considered an aspect of personal grooming, fashion, and cosmetics, although practical, cultural, and popular considerations also influence some hairstyles. The oldest known depiction of hair braiding dates back about 30,000 years. In ancient civilizations, women's hair was often elaborately and carefully dressed in special ways. In Imperial Rome, women wore their hair in complicated styles. From the time of the Roman Empire until the Middle Ages, most women grew their hair as long as it would naturally grow. During the Roman Empire as well as in the 16th century in the western world, women began to wear their hair in extremely ornate styles. In the later half of the 15th century and on into the 16th century a very high hairline on the forehead was considered attractive. During the 15th and 16th centuries, European men wore their hair cropped no longer than shoulder-length. In the early 17th century male hairstyles grew longer, with waves or curls being considered desirable.
The male wig was pioneered by King Louis XIII of France (1601–1643) in 1624. Perukes or periwigs for men were introduced into the English-speaking world with other French styles in 1660. Late 17th-century wigs were very long and wavy, but became shorter in the mid-18th century, by which time they were normally white. Short hair for fashionable men was a product of the Neoclassical movement. In the early 19th century the male beard, and also moustaches and sideburns, made a strong reappearance. From the 16th to the 19th century, European women's hair became more visible while their hair coverings grew smaller. In the middle of the 18th century the pouf style developed. During the First World War, women around the world started to shift to shorter hairstyles that were easier to manage. In the early 1950s women's hair was generally curled and worn in a variety of styles and lengths. In the 1960s, many women began to wear their hair in short modern cuts such as the pixie cut, while in the 1970s, hair tended to be longer and looser. In both the 1960s and 1970s many men and women wore their hair very long and straight. In the 1980s, women pulled back their hair with scrunchies. During the 1980s, punk hairstyles were adopted by some people.
Prehistory and history.
Throughout times, people have worn their hair in a wide variety of styles, largely determined by the fashions of the culture they live in. Hairstyles are markers and signifiers of social class, age, marital status, racial identification, political beliefs, and attitudes about gender.
In many cultures, often for religious reasons, women's hair is covered while in public, and in some, such as Haredi Judaism or European Orthodox communities, women's hair is shaved or cut very short, and covered with wigs. Only since the end of World War I have women begun to wear their hair short and in fairly natural styles.
Paleolithic.
The oldest known reproduction of hair braiding lies back about 30,000 years: the Venus of Willendorf, now known in academia as the Woman of Willendorf, of a female figurine from the Paleolithic, estimated to have been made between about 28,000 and 25,000 BCE.
The Venus of Brassempouy counts about 25,000 years old and undisputably shows hairstyling.
Bronze Age.
In Bronze Age razors were known and in use by some men, but not on a daily basis since the procedure was rather unpleasant and required resharpening of the tool which reduced its endurance.
Ancient history.
In ancient civilizations, women's hair was often elaborately and carefully dressed in special ways. Women coloured their hair, curled it, and pinned it up(ponytail) in a variety of ways. They set their hair in waves and curls using wet clay, which they dried in the sun and then combed out, or else by using a jelly made of quince seeds soaked in water, or curling tongs and curling irons of various kinds.
Roman Empire and Middle Ages.
Between 27 BC and 102 AD, in Imperial Rome, women wore their hair in complicated styles: a mass of curls on top, or in rows of waves, drawn back into ringlets or braids. Eventually noblewomen's hairstyles grew so complex that they required daily attention from several slaves and a stylist in order to be maintained. The hair was often lightened using wood ash, unslaked lime and sodium bicarbonate, or darkened with copper filings, oak-apples or leeches marinated in wine and vinegar. It was augmented by wigs, hairpieces and pads, and held in place by nets, pins, combs and pomade. Under the Byzantine Empire, noblewomen covered most of their hair with silk caps and pearl nets.
From the time of the Roman Empire until the Middle Ages, most women grew their hair as long as it would naturally grow. It was normally little styled by cutting, as women's hair was tied up on the head and covered on most occasions when outside the home with a snood, kerchief or veil; for an adult woman to wear uncovered and loose hair in the street was often restricted to prostitutes. Braiding and tying the hair was common. In the 16th century, women began to wear their hair in extremely ornate styles, often decorated with pearls, precious stones, ribbons and veils. Women used a technique called "lacing" or "taping," in which cords or ribbons were used to bind the hair around their heads. During this period, most of the hair was braided and hidden under wimples, veils or couvrechefs. In the later half of the 15th century and on into the 16th century a very high hairline on the forehead was considered attractive, and wealthy women frequently plucked out hair at their temples and the napes of their necks, or used depilatory cream to remove it, if it would otherwise be visible at the edges of their hair coverings. Working-class women in this period wore their hair in simple styles.
Early modern history.
Male styles.
During the 15th and 16th centuries, European men wore their hair cropped no longer than shoulder-length, with very fashionable men wearing bangs or fringes. In Italy it was common for men to dye their hair. In the early 17th century male hairstyles grew longer, with waves or curls being considered desirable.
The male wig was supposedly pioneered by King Louis XIII of France (1601–1643) in 1624 when he had prematurely begun to bald. This fashion was largely promoted by his son and successor Louis XIV of France (1638–1715) that contributed to its spread in European and European-influenced countries. The beard had been in a long decline and now disappeared among the upper classes.
Perukes or periwigs for men were introduced into the English-speaking world with other French styles when Charles II was restored to the throne in 1660, following a lengthy exile in France. These wigs were shoulder-length or longer, imitating the long hair that had become fashionable among men since the 1620s. Their use soon became popular in the English court. The London diarist Samuel Pepys recorded the day in 1665 that a barber had shaved his head and that he tried on his new periwig for the first time, but in a year of plague he was uneasy about wearing it:"3rd September 1665: Up, and put on my coloured silk suit, very fine, and my new periwig, bought a good while since, but darst not wear it because the plague was in Westminster when I bought it. And it is a wonder what will be the fashion after the plague is done as to periwigs, for nobody will dare to buy any haire for fear of the infection? That it had been cut off the heads of people dead of the plague."
Late 17th-century wigs were very long and wavy (see George I below), but became shorter in the mid-18th century, by which time they were normally white (George II). A very common style had a single stiff curl running round the head at the end of the hair. By the late 18th-century the natural hair was often powdered to achieve the impression of a short wig, tied into a small tail or "queue" behind (George III).
Short hair for fashionable men was a product of the Neoclassical movement. Classically inspired male hair styles included the Bedford Crop, arguably the precursor of most plain modern male styles, which was invented by the radical politician Francis Russell, 5th Duke of Bedford as a protest against a tax on hair powder; he encouraged his frends to adopt it by betting them they would not. Another influential style (or group of styles) was named by the French after the Roman Emperor Titus, from his busts, with hair short and layered but somewhat piled up on the crown, often with restrained quiffs or locks hanging down; variants are familiar from the hair of both Napoleon and George IV of England. The style was supposed to have been introduced by the actor François-Joseph Talma, who upstaged his wigged co-actors when appearing in productions of works such as Voltaire's "Brutus". In 1799 a Parisian fashion magazine reported that even bald men were adopting Titus wigs, and the style was also worn by women, the "Journal de Paris" reporting in 1802 that "more than half of elegant women were wearing their hair or wig "à la Titus"."
In the early 19th century the male beard, and also moustaches and sideburns, made a strong reappearance, associated with the Romantic movement, and all remained very common until the 1890s, after which younger men ceased to wear them, with World War I, when the majority of men in many countries saw military service, finally despatching the full beard except for older men retaining the styles of their youth, and those affecting a bohemian look. The short military-style moustache remained popular.
Female styles.
From the 16th to the 19th century, European women's hair became more visible while their hair coverings grew smaller, with both becoming more elaborate, and with hairstyles beginning to include ornamentation such as flowers, ostrich plumes, ropes of pearls, jewels, ribbons and small crafted objects such as replicas of ships and windmills. Bound hair was felt to be symbolic of propriety: loosening one's hair was considered immodest and sexual, and sometimes was felt to have supernatural connotations. Red hair was popular, particularly in England during the reign of the red-haired Elizabeth I, and women and aristocratic men used borax, saltpeter, saffron and sulfur powder to dye their hair red, making themselves nauseated and giving themselves headaches and nosebleeds. During this period in Spain and Latin cultures, women wore lace mantillas, often worn over a high comb, and in Buenos Aires, there developed a fashion for extremely large tortoise-shell hair combs called peinetón, which could measure up to three feet in height and width, and which are said by historians to have reflected the growing influence of France, rather than Spain, upon Argentinians.
In the middle of the 18th century the pouf style developed, with women creating volume in the hair at the front of the head, usually with a pad underneath to lift it higher, and ornamented the back with seashells, pearls or gemstones. In 1750, women began dressing their hair with perfumed pomade and powdering it white. Just before World War I, some women began wearing silk turbans over their hair.
Japan.
In the early 1870s, in a shift that historians attribute to the influence of the West, Japanese men began cutting their hair into styles known as jangiri or zangiri (which roughly means "random cropping"). During this period, Asian women were still wearing traditional hairstyles held up with combs, pins and sticks crafted from tortoise, metal, wood and other materials, but in the middle 1880s, upper-class Japanese women began pushing back their hair in the Western style (known as sokuhatsu), or adopting Westernized versions of traditional Japanese hairstyles (these were called yakaimaki, or literally, soirée chignon).
Inter-war years.
During the First World War, women around the world started to shift to shorter hairstyles that were easier to manage. In the 1920s women started for the first time to bob, shingle and crop their hair, often covering it with small head-hugging cloche hats. In Korea, the bob was called "tanbal". Women began marcelling their hair, creating deep waves in it using heated scissor irons. Durable permanent waving became popular also in this period: it was an expensive, uncomfortable and time-consuming process, in which the hair was put in curlers and inserted into a steam or dry heat machine. During the 1930s women began to wear their hair slightly longer, in pageboys, bobs or waves and curls.
During this period, Western men began to wear their hair in ways popularized by movie stars such as Douglas Fairbanks, Jr. and Rudolph Valentino. Men wore their hair short, and either parted on the side or in the middle, or combed straight back, and used pomade, creams and tonics to keep their hair in place. At the beginning of the Second World War and for some time afterwards, men's haircuts grew shorter, mimicking the military crewcut.
During the 1920s and 1930s, Japanese women began wearing their hair in a style called "mimi-kakushi" (literally, "ear hiding"), in which hair was pulled back to cover the ears and tied into a bun at the nape of the neck. Waved or curled hair became increasingly popular for Japanese women throughout this period, and permanent waves, though controversial, were extremely popular. Bobbed hair also became more popular for Japanese women, mainly among actresses and moga, or "cut-hair girls," young Japanese women who followed Westernized fashions and lifestyles in the 1920s.
Post-war years.
After the war, women started to wear their hair in softer, more natural styles. In the early 1950s women's hair was generally curled and worn in a variety of styles and lengths. In the later 1950s, high bouffant and beehive styles, sometimes nicknamed B-52s for their similarity to the bulbous noses of the B-52 Stratofortress bomber, became popular. During this period many women washed and set their hair only once a week, and kept it in place by wearing curlers every night and reteasing and respraying it every morning. In the 1960s, many women began to wear their hair in short modern cuts such as the pixie cut, while in the 1970s, hair tended to be longer and looser. In both the 1960s and 1970s many men and women wore their hair very long and straight. Women straightened their hair through chemical straightening processes, by ironing their hair at home with a clothes iron, or by rolling it up with large empty cans while wet. African-American men and women began wearing their hair naturally (unprocessed) in large Afros, sometimes ornamented with Afro picks made from wood or plastic. By the end of the 1970s the Afro had fallen out of favour among African-Americans, and was being replaced by other natural hairstyles such as corn rows and dreadlocks.
Contemporary hairstyles.
Since the 1970s, women have worn their hair in a wide variety of fairly natural styles. In the 1980s, women pulled back their hair with scrunchies, stretchy ponytail holders made from cloth over fabric bands. Women also often wear glittery ornaments today, as well as claw-style barrettes used to secure ponytails and other upswept or partially upswept hairstyles. Today, women and men can choose from a broad range of hairstyles, but they are still expected to wear their hair in ways that conform to gender norms: in much of the world, men with long hair and women whose hair doesn't appear carefully groomed may face various forms of discrimination, including harassment, social shaming or workplace discrimination. This is somewhat less true of African-American men, who wear their hair in a variety of styles that overlap with those of African-American women, including braids and cornrows fastened with rubber bands and dreadlocks.
Defining factors.
A hairstyle's aesthetic considerations may be determined by many factors, such as the subject's physical attributes and desired self-image or the stylist's artistic instincts.
Physical factors include natural hair type and growth patterns, face and head shape from various angles, and overall body proportions; medical considerations may also apply. Self-image may be directed toward conforming to mainstream values (military-style crew cuts or current "fad" hairstyles such as the Dido flip), identifying with distinctively groomed subgroups (e.g., punk hair), or obeying religious dictates (e.g., Orthodox Jewish have payot, Rastafari have Dreadlocks, North India jatas, or the Sikh practice of Kesh), though this is highly contextual and a "mainstream" look in one setting may be limited to a "subgroup" in another.
A hairstyle is achieved by arranging hair in a certain way, occasionally using combs, a blow-dryer, gel, or other products. The practice of styling hair is often called "hairdressing", especially when done as an occupation.
Hairstyling may also include adding accessories (such as headbands or barrettes) to the hair to hold it in place, enhance its ornamental appearance, or partially or fully conceal it with coverings such as a kippa, hijab, tam or turban.
Hairstyle process.
Hair dressing may include cuts, weaves, coloring, extensions, perms, permanent relaxers, curling, and any other form of styling or texturing.
Length and trimming.
Hair cutting or hair trimming is intended to create or maintain a specific shape and form. Its extent may range from merely trimming the uneven ends of the hair to a uniform length to completely shaving the head.
The overall shape of the hairstyle is usually maintained by trimming it at regular intervals. There are ways to trim one's own hair but usually another person is enlisted to perform the process, as it is difficult to maintain symmetry while cutting hair at the back of one's head. Although trimming enhances the hair's appearance by removing damaged or split ends, it does not promote faster growth or remove all damage along the length of the hair.
Stylists often wash a subject's hair first, so that the hair is cut while still slightly damp. Compared to dry hair, wet hair can be easier to manage in a cut/style situation because the added weight and surface tension of the water cause the strands to stretch downward and cling together along the hair's length, holding a line and making it easier for the stylist to create a form.
Brushing and combing.
Brushes and combs are used to organize and untangle the hair, encouraging all of the strands to lie in the same direction and removing debris such as lint, dandruff, or hairs that have already shed from their follicles but continue to cling to the other hairs.
There are all manner of detangling tools available in a wide variety of price ranges. Combs come in all shapes and sizes and all manner of materials including plastics, wood, and horn. Similarly, brushes also come in all sizes and shapes, including various paddle shapes. Most benefit from using some form of a wide tooth comb for detangling. Most physicians advise against sharing hair care instruments like combs and clips, to prevent spreading hair conditions like dandruff and head lice.
The historical dictum to brush hair with one hundred strokes every day is somewhat archaic, dating from a time when hair was washed less frequently; the brushstrokes would spread the scalp's natural oils down through the hair, creating a protective effect. Now, however, this does not apply when the natural oils have been washed off by frequent shampoos. Also, hairbrushes are now usually made with rigid plastic bristles instead of the natural boar's bristles that were once standard; the plastic bristles increase the likelihood of actually injuring the scalp and hair with excessively vigorous brushing.
Drying.
Hair dryers speed the drying process of hair by blowing air, which is usually heated, over the wet hair shaft to accelerate the rate of water evaporation.
Excessive heat may increase the rate of shaft-splitting or other damage to the hair. Hair dryer diffusers can be used to widen the stream of air flow so it is weaker but covers a larger area of the hair.
Hair dryers can also be used as a tool to sculpt the hair to a very slight degree. Proper technique involves aiming the dryer such that the air does not blow onto the face or scalp, which can cause burns.
Braiding and updos.
Tight or frequent braiding may pull at the hair roots and cause traction alopecia. Rubber bands with metal clasps or tight clips, which bend the hair shaft at extreme angles, can have the same effect.
If hair is pinned too tightly, or the whole updo slips causing pulling on the hair in the follicle at the hair root are other scenarios that can cause aggravation to the hair follicle and result in headaches. Although many African- Americans use braiding extensions as a form of convenience, it is important not to keep the braids up longer than needed to avoid hair breakage or hair loss.
Curling and Straightening.
Curling and straightening hair requires the stylist to use a curling rod or a flat iron to get a desired look. These irons use heat to manipulate the hair into a variety of waves, curls and reversing natural curls and temporarily straightening the hair. Straightening or even curling hair can damage it due to direct heat from the iron and applying chemicals afterwards to keep its shape.
There are irons that have a function to straighten or curl hair even when its damp (from showering or wetting the hair), but this requires more heat than the average iron(temperatures can range from 300-400 degrees). Heat protection sprays, and hair repairing shampoos and conditioners can protect the hair from damage caused by the direct heat from the irons.
Industry.
Hair styling is a major world industry, from the salon itself to products, advertising, and even magazines on the subject. In the United States, most hairstylists are licensed after obtaining training at a cosmetology or beauty school.
In recent years, competitive events for professional stylists have grown in popularity. Stylists compete on deadline to create the most elaborate hairstyle using props, lights and other accessories.
Tools.
Styling tools may include hair irons (including flat, curling, and crimping irons), hair dryers, and hair rollers. Hair dressing might also include the use of hair product to add texture, shine, curl, volume or hold to a particular style. Hairpins are also used when creating particular hairstyles. Their uses and designs vary over different cultural backgrounds.
Products.
Styling products aside from shampoo and conditioner are many and varied. Leave-in conditioner, conditioning treatments, mousse, gels, lotions, waxes, creams, clays, serums, oils, and sprays are used to change the texture or shape of the hair, or to hold it in place in a certain style. Applied properly, most styling products will not damage the hair apart from drying it out; most styling products contain alcohols, which can dissolve oils. Many hair products contain chemicals which can cause build-up, resulting in dull hair or a change in perceived texture.
Wigs.
Care of human or other natural hair wigs is similar to care of a normal head of hair in that the wig can be brushed, styled, and kept clean using haircare products.
Synthetic wigs are usually made from a fine fiber that mimics human hair. This fiber can be made in almost any color and hairstyle, and is often glossier than human hair. However, this fiber is sensitive to heat and cannot be styled with flat irons or curling irons. There is a newer synthetic fiber that can take heat up to a certain temperature.
Human hair wigs can be styled with heat, and they must be brushed only when dry. "Synthetic" and human hair wigs should be brushed dry before shampooing to remove tangles. To clean the wig, the wig should be dipped into a container with water and mild shampoo, then dipped in clear water and moved up and down to remove excess water. The wig must then be air dried naturally into its own hairstyle.Proper maintenance can make a human hair wig last for many years.
Functional and decorative ornaments.
There are many options to adorn and arrange the hair. Hairpins, clasps, barrettes, headbands, ribbons, rubber bands, scrunchies, and combs can be used to achieve a variety of styles. There are also many decorative ornaments that, while they may have clasps to affix them to the hair, are used solely for appearance and do not aid in keeping the hair in place. In India for example, the Gajra (flower garland) is common there are heaps on hairstyles.
Social and cultural implications.
Gender.
At most times in most cultures, men have worn their hair in styles that are different from women's. American sociologist Rose Weitz once wrote that the most widespread cultural rule about hair is that women's hair must differ from men's hair. An exception is the men and women living in the Orinoco-Amazon Basin, where traditionally both genders have worn their hair cut into a bowl shape. In Western countries in the 1960s, both young men and young women wore their hair long and natural, and since then it has become more common for men to grow their hair. During most periods in human history when men and women wore similar hairstyles, as in the 1920s and 1960s, it has generated significant social concern and approbation.
Religion.
Cutting off one's hair is often associated with religious faith: Catholic nuns often cut their hair very short, and men who joined Catholic monastic orders in the eighth century adopted what was known as the tonsure, which involved shaving the tops of their heads and leaving a ring of hair around the bald crown. Many Buddhists, Hajj pilgrims and Vaisnavas, especially members of the Hare Krishna movement who are "brahmacharis" or "sannyasis", shave their heads. Some Hindu and most Buddhist monks and nuns shave their heads upon entering their order, and Korean Buddhist monks and nuns have their heads shaved every 15 days. Adherents of Sikhism are required to wear their hair unshorn. Women usually wear it in a braid or a bun and men cover it with a turban.
Marital status.
In the 1800s, American women started wearing their hair up when they became ready to get married. Among the Fulani people of west Africa, unmarried women wear their hair ornamented with small amber beads and coins, while married women wear large amber ornaments. Marriage is signified among the Toposa women of South Sudan by wearing the hair in many small pigtails. Unmarried Hopi women have traditionally worn a "butterfly" hairstyle characterized by a twist or whorl of hair at each side of the face.
Life transitions.
In many cultures, including Hindu culture and among the Wayana people of the Guiana highlands, young people have historically shaved off their hair to denote coming-of-age. Women in India historically have signified adulthood by switching from wearing two braids to one. Among the Rendille of north-eastern Kenya and the Tchikrin people of the Brazilian rainforest, both men and women shave their heads after the death of a close family member. When a man died in ancient Greece, his wife cut off her hair and buried it with him, and in Hindu families, the chief mourner is expected to shave his or her head 10 days after a death.
Social class.
Throughout history, hair has been a signifier of social class.
Upper-class people have always used their hairstyles to signal wealth and status. Wealthy Roman women wore complex hairstyles that needed the labours of several people to maintain them, and rich people have also often chosen hairstyles that restricted or burdened their movement, making it obvious that they did not need to work. Wealthy people's hairstyles used to be at the cutting edge of fashion, setting the styles for the less wealthy. But today, the wealthy are generally observed to wear their hair in conservative styles that date back decades prior.
Middle-class hairstyles tend to be understated and professional. Middle-class people aspire to have their hair look healthy and natural, implying that they have the resources to live a healthy lifestyle and take good care of themselves.
Historically, working-class people's haircuts have tended to be practical and simple. Working-class men have often shaved their heads or worn their hair close-cropped, and working-class women have typically pulled their hair up and off their faces in simple styles. However, today, working-class people often have more elaborate and fashion-conscious hairstyles than other social classes. Many working-class Mexican men in American cities wear their hair in styles like the Mongolian (shaved except for a tuft of hair at the nape of the neck) or the rat tail (crewcut on top, tuft at the nape), and African-Americans often wear their hair in complex patterns of braids and cornrows, fastened with barrettes and beads, and sometimes including shaved sections or bright colour. Sociologists say these styles are an attempt to express individuality and presence in the face of social denigration and invisibility.
Haircut in space.
Haircuts also occur in space at the International Space Station. During the various Expeditions astronauts use hair clippers attached to vacuum devices for grooming their colleagues so that the cut hair will not drift inside the weightless environment of the space station and become a nuisance to the astronauts or a hazard to the sensitive equipment installations inside the station.
Haircutting in space was also used for charitable purposes in the case of astronaut Sunita Williams who obtained such a haircut by fellow astronaut Joan Higginbotham inside the International Space Station. Sunita's ponytail was brought back to earth with the STS-116 crew and was donated to Locks of Love.

</doc>
<doc id="52404" url="http://en.wikipedia.org/wiki?curid=52404" title="24th century BC">
24th century BC

The 24th century BC is a century which lasted from the year 2400 BC to 2301 BC.
In popular culture.
In modern Korean national mythology, the character Dangun, whose mother was originally a bear, founded the state Gojoseon in 2333 BC and ruled it for about 2000 years. Some Koreans think of it as the earliest Korean state and of Dangun as the ancestor of Koreans, and from 1948 until December 1961, the Republic of Korea officially reckoned years by adding 2333 to the Common Era year. The year 2333 BC and the related myth are sometimes presented matter-of-factly as "history" rather than "mythology" in Korea.

</doc>
<doc id="52412" url="http://en.wikipedia.org/wiki?curid=52412" title="2022">
2022

2022 ()
will be .

</doc>
<doc id="52413" url="http://en.wikipedia.org/wiki?curid=52413" title="2023">
2023

2023 ()
will be .

</doc>
<doc id="52414" url="http://en.wikipedia.org/wiki?curid=52414" title="42 BC">
42 BC

Year 42 BC was either a common year starting on Monday, Tuesday or Wednesday or a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar (the sources differ, see leap year error for further information) and a common year starting on Tuesday of the Proleptic Julian calendar. At the time, it was known as the Year of the Consulship of Lepidus and Plancus (or, less frequently, year 712 "Ab urbe condita"). The denomination 42 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52415" url="http://en.wikipedia.org/wiki?curid=52415" title="50 BC">
50 BC

Year 50 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Paullus and Marcellus (or, less frequently, year 704 "Ab urbe condita"). The denomination 50 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52416" url="http://en.wikipedia.org/wiki?curid=52416" title="2024">
2024

2024 ()
will be .

</doc>
<doc id="52417" url="http://en.wikipedia.org/wiki?curid=52417" title="2025">
2025

2025 ()
will be .

</doc>
<doc id="52418" url="http://en.wikipedia.org/wiki?curid=52418" title="2070s">
2070s

The 2070s decade will begin on January 1, 2070 and will end on December 31, 2079.

</doc>
<doc id="52419" url="http://en.wikipedia.org/wiki?curid=52419" title="52 BC">
52 BC

 
Year 52 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Pompeius and Scipio (or, less frequently, year 702 "Ab urbe condita"). The denomination 52 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52420" url="http://en.wikipedia.org/wiki?curid=52420" title="280s BC">
280s BC


</doc>
<doc id="52421" url="http://en.wikipedia.org/wiki?curid=52421" title="270s BC">
270s BC


</doc>
<doc id="52422" url="http://en.wikipedia.org/wiki?curid=52422" title="260s BC">
260s BC

Deaths.
Euclid

</doc>
<doc id="52423" url="http://en.wikipedia.org/wiki?curid=52423" title="250s BC">
250s BC


</doc>
<doc id="52424" url="http://en.wikipedia.org/wiki?curid=52424" title="240s BC">
240s BC


</doc>
<doc id="52425" url="http://en.wikipedia.org/wiki?curid=52425" title="230s BC">
230s BC


</doc>
<doc id="52426" url="http://en.wikipedia.org/wiki?curid=52426" title="210s BC">
210s BC

Deaths.
210 BC

</doc>
<doc id="52428" url="http://en.wikipedia.org/wiki?curid=52428" title="160s BC">
160s BC


</doc>
<doc id="52429" url="http://en.wikipedia.org/wiki?curid=52429" title="170s BC">
170s BC


</doc>
<doc id="52430" url="http://en.wikipedia.org/wiki?curid=52430" title="180s BC">
180s BC


</doc>
<doc id="52431" url="http://en.wikipedia.org/wiki?curid=52431" title="190s BC">
190s BC


</doc>
<doc id="52432" url="http://en.wikipedia.org/wiki?curid=52432" title="Xanthine">
Xanthine

Xanthine ( or ; archaically xanthic acid) (3,7-dihydro-purine-2,6-dione), is a purine base found in most human body tissues and fluids and in other organisms. A number of stimulants are derived from xanthine, including caffeine and theobromine.
Xanthine is a product on the pathway of purine degradation.
Xanthine is subsequently converted to uric acid by the action of the xanthine oxidase enzyme.
Studies reported in 2008, based on 12C/13C isotopic ratios of organic compounds found in the Murchison meteorite, suggested that xanthine and related chemicals, including the RNA component uracil, were formed extraterrestrially. In August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting xanthine and related organic molecules, including the DNA and RNA components adenine and guanine, were made in outer space.
Pathology.
People with the rare genetic disorder xanthinuria lack sufficient xanthine oxidase and cannot convert xanthine to uric acid.
Clinical significance of xanthine derivatives.
Derivatives of xanthine (known collectively as xanthines) are a group of alkaloids commonly used for their effects as mild stimulants and as bronchodilators, notably in the treatment of asthma symptoms. In contrast to other, more potent stimulants like sympathomimetic amines, xanthines mainly act to oppose the actions of the sleepiness-inducing adenosine, and increase alertness in the central nervous system. They also stimulate the respiratory centre, and are used for treatment of infantile apnea. Due to widespread effects, the therapeutic range of xanthines is narrow, making them merely a second-line asthma treatment. The therapeutic level is 10-20 micrograms/mL blood; signs of toxicity include tremor, nausea, nervousness, and tachycardia/arrhythmia.
Methylated xanthines (methylxanthines), which include caffeine, aminophylline, IBMX, paraxanthine, pentoxifylline, theobromine, and theophylline, affect not only the airways but stimulate heart rate, force of contraction, and cardiac arrhythmias at high concentrations. In high doses they can lead to convulsions that are resistant to anticonvulsants. Methylxanthines induce acid and pepsin secretions in the GI tract. Methylxanthines are metabolized by cytochrome P450 in the liver.
These drugs act as both:
But different analogues show varying potency at the numerous subtypes, and a wide range of synthetic xanthines (some nonmethylated) have been developed searching for compounds with greater selectivity for phosphodiesterase enzyme or adenosine receptor subtypes. Xanthines are also found very rarely as constituents of nucleic acids.

</doc>
<doc id="52433" url="http://en.wikipedia.org/wiki?curid=52433" title="2026">
2026

2026 ()
will be .

</doc>
<doc id="52434" url="http://en.wikipedia.org/wiki?curid=52434" title="2027">
2027

2027 ()
will be .

</doc>
<doc id="52435" url="http://en.wikipedia.org/wiki?curid=52435" title="Tait's conjecture">
Tait's conjecture

In mathematics, Tait's conjecture states that "Every 3-connected planar cubic graph has a Hamiltonian cycle (along the edges) through all its vertices". It was proposed by P. G. Tait (1884) and disproved by W. T. Tutte (1946), who constructed a counterexample with 25 faces, 69 edges and 46 vertices. Several smaller counterexamples, with 21 faces, 57 edges and 38 vertices, were later proved minimal by .
The condition that the graph be 3-regular is necessary due to polyhedra such as the rhombic dodecahedron, which forms a bipartite graph with six degree-four vertices on one side and eight degree-three vertices on the other side; because any Hamiltonian cycle would have to alternate between the two sides of the bipartition, but they have unequal numbers of vertices, the rhombic dodecahedron is not Hamiltonian. 
The conjecture was significant, because if true, it would have implied the four color theorem: as Tait described, the four-color problem is equivalent to the problem of finding 3-edge-colorings of bridgeless cubic planar graphs. In a Hamiltonian cubic planar graph, such an edge coloring is easy to find: use two colors alternately on the cycle, and a third color for all remaining edges. Alternatively, a 4-coloring of the faces of a Hamiltonian cubic planar graph may be constructed directly, using two colors for the faces inside the cycle and two more colors for the faces outside.
Tutte's counterexample.
Tutte's fragment.
The key to this counter-example is what is now known as Tutte's fragment, not shown to the right.
If this fragment is part of a larger graph, then any Hamiltonian cycle through the graph must go in or out of the top vertex (and either one of the lower ones). It cannot go in one lower vertex and out the other.
The counterexample.
The fragment can then be used to construct the non-Hamiltonian Tutte graph, by putting
together three such fragments as shown on the picture. The "compulsory" edges of the fragments, that must be part of any Hamiltonian path through the fragment, are connected at the central vertex; because any cycle can use only two of these three edges, there can be no Hamiltonian cycle.
The resulting Tutte graph is 3-connected and planar, so by Steinitz' theorem it is the graph of a polyhedron. In total it has 25 faces, 69 edges and 46 vertices.
It can be realized geometrically from a tetrahedron (the faces of which correspond to the four large faces in the drawing, three of which are between pairs of fragments and the fourth of which forms the exterior) by multiply truncating three of its vertices.
Smaller counterexamples.
As show, there are exactly six 38-vertex non-Hamiltonian polyhedra that have nontrivial three-edge cuts. They are formed by replacing two of the vertices of a pentagonal prism by the same fragment used in Tutte's example.
References.
Partly based on , used by permission."

</doc>
<doc id="52436" url="http://en.wikipedia.org/wiki?curid=52436" title="51 BC">
51 BC

Year 51 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Marcellus and Sulpicius (or, less frequently, year 703 "Ab urbe condita"). The denomination 51 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="52437" url="http://en.wikipedia.org/wiki?curid=52437" title="99 BC">
99 BC

Year 99 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Antonius and Albinus (or, less frequently, year 655 "Ab urbe condita"). The denomination 99 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52438" url="http://en.wikipedia.org/wiki?curid=52438" title="53 BC">
53 BC

Year 53 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Messalla and Calvinus (or, less frequently, year 701 "Ab urbe condita"). The denomination 53 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Armenia.
</onlyinclude>

</doc>
<doc id="52439" url="http://en.wikipedia.org/wiki?curid=52439" title="433">
433

Year 433 (CDXXXIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Maximus (or, less frequently, year 1186 "Ab urbe condita"). The denomination 433 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52440" url="http://en.wikipedia.org/wiki?curid=52440" title="435">
435

Year 435 (CDXXXV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Valentinianus (or, less frequently, year 1188 "Ab urbe condita"). The denomination 435 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52442" url="http://en.wikipedia.org/wiki?curid=52442" title="447">
447

Year 447 (CDXLVII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Calepius and Ardabur (or, less frequently, year 1200 "Ab urbe condita"). The denomination 447 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52443" url="http://en.wikipedia.org/wiki?curid=52443" title="449">
449

Year 449 (CDXLIX) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Astyrius and Romanus (or, less frequently, year 1202 "Ab urbe condita"). The denomination 449 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52444" url="http://en.wikipedia.org/wiki?curid=52444" title="460">
460

Year 460 (CDLX) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Magnus and Apollonius (or, less frequently, year 1213 "Ab urbe condita"). The denomination 460 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52445" url="http://en.wikipedia.org/wiki?curid=52445" title="469">
469

Year 469 (CDLXIX) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Marcianus and Zeno (or, less frequently, year 1222 "Ab urbe condita"). The denomination 469 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52446" url="http://en.wikipedia.org/wiki?curid=52446" title="882">
882

Year 882 (DCCCLXXXII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
