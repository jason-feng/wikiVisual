<doc id="43465" url="http://en.wikipedia.org/wiki?curid=43465" title="Shang dynasty">
Shang dynasty

The Shang dynasty () or Yin dynasty (), according to traditional historiography, ruled in the Yellow River valley in the second millennium BC, succeeding the Xia dynasty and followed by the Zhou dynasty. The classic account of the Shang comes from texts such as the "Book of Documents", "Bamboo Annals" and "Records of the Grand Historian". According to the traditional chronology based upon calculations made approximately 2,000 years ago by Liu Xin, the Shang ruled from 1766 to 1122 BC, but according to the chronology based upon the "current text" of "Bamboo Annals", they ruled from 1556 to 1046 BC. The Xia–Shang–Zhou Chronology Project dated them from c. 1600 to 1046 BC.
Archaeological work at the Ruins of Yin (near modern-day Anyang), which has been identified as the last Shang capital, uncovered eleven major Yin royal tombs and the foundations of palaces and ritual sites, containing weapons of war and remains from both animal and human sacrifices. Tens of thousands of bronze, jade, stone, bone, and ceramic artifacts have been obtained.
The Anyang site has yielded the earliest known body of Chinese writing, mostly divinations inscribed on oracle bones – turtle shells, ox scapulae, or other bones.
More than 20,000 were discovered in the initial scientific excavations during the 1920s and 1930s, and over four times as many have been found since. The inscriptions provide critical insight into many topics from the politics, economy, and religious practices to the art and medicine of this early stage of Chinese civilization.
Traditional accounts.
Many events concerning the Shang dynasty are mentioned in various Chinese classics, including the "Book of Documents", the "Mencius" and the "Zuo Zhuan". Working from all the available documents, the Han dynasty historian Sima Qian assembled a sequential account of the Shang dynasty as part of his "Records of the Grand Historian". His history describes some events in detail, while in other cases only the name of a king is given. A closely related, but slightly different, account is given by the "Bamboo Annals". The "Annals" were interred in 296 BC, but the text has a complex history and the authenticity of the surviving versions is controversial.
The name "Yīn" (殷) is used by Sima Qian for the dynasty, and in the "Bamboo Annals" for both the dynasty and its final capital. It has been a popular name for the Shang throughout history, and is often used specifically to describe the later half of the Shang dynasty. In Japan and Korea, the Shang are still referred to almost exclusively as the Yin ("In") dynasty. However it seems to have been the Zhou name for the earlier dynasty.
The word does not appear in the oracle bones, which refer to the state as "Shāng" (商), and the capital as "Dàyì Shāng" (大邑商 "Great Settlement Shang").
Course of the dynasty.
Sima Qian's "Annals of the Yin" begins by describing the predynastic founder of the Shang lineage, Xie (偰) — also appearing as Qi (契) — as having been miraculously conceived when Jiandi, a wife of Emperor Ku, swallowed an egg dropped by a black bird. Xie is said to have helped Yu the Great to control the Great Flood and for his service to have been granted a place called Shang as a fief.
Sima Qian relates that the dynasty itself was founded 13 generations later, when Xie's descendant Tang overthrew the impious and cruel final Xia ruler in the Battle of Mingtiao. The "Records" recount events from the reigns of Tang, Tai Jia, Tai Wu, Pan Geng, Wu Ding, Wu Yi and the depraved final king Di Xin, but the rest of the Shang rulers are merely mentioned by name. According to the "Records", the Shang moved their capital five times, with the final move to Yin in the reign of Pan Geng inaugurating the golden age of the dynasty.
Di Xin, the last Shang king, is said to have committed suicide after his army was defeated by Wu of Zhou. Legends say that his army and his equipped slaves betrayed him by joining the Zhou rebels in the decisive Battle of Muye. According to the "Yi Zhou Shu" and Mencius the battle was very bloody. The classic, Ming-era novel "Fengshen Yanyi" retells the story of the war between Shang and Zhou as a conflict where rival factions of gods supported different sides in the war.
After the Shang were defeated, King Wu allowed Di Xin's son Wu Geng to rule the Shang as a vassal kingdom. However, Zhou Wu sent three of his brothers and an army to ensure that Wu Geng would not rebel. After Zhou Wu's death, the Shang joined the Rebellion of the Three Guards against the Duke of Zhou, but the rebellion collapsed after three years, leaving Zhou in control of Shang territory.
Descendants.
After Shang's collapse, Zhou's rulers forcibly relocated "Yin diehards" (殷頑) and scattered them throughout Zhou territory. Some surviving members of the Shang royal family collectively changed their surname from the ancestral name Zi (子) to the name of their fallen dynasty, Yin. The family retained an aristocratic standing and often provided needed administrative services to the succeeding Zhou dynasty. The "Records of the Grand Historian" states that King Cheng of Zhou, with the support of his regent and uncle, the Duke of Zhou, enfeoffed Weiziqi (微子啟), a brother of Di Xin, as the Duke of Song, with its capital at Shangqiu. The Dukes of Song would maintain rites honoring the Shang kings until Song was conquered by Qi in 286 BC. Confucius was a descendant of the Shang Kings through the Dukes of Song. The Dukes of Yansheng are in turn the descendants of Confucius.
The vassal state of Guzhu, located in what is now Tangshan, was formed by another remnant of the Shang, and was destroyed by Duke Huan of Qi. Many Shang clans that migrated northeast after the dynasty's collapse were integrated into Yan culture during the Western Zhou period. These clans maintained an elite status and continued practicing the sacrificial and burial traditions of the Shang.
Both Korean and Chinese legends, including reports in the "Book of Documents" and the "Bamboo Annals", state that a disgruntled Shang prince named Jizi, who had refused to cede power to the Zhou, left China with a small army. According to these legends, he founded a state known as Gija Joseon in northwest Korea during the Gojoseon period of ancient Korean history. However, the historical accuracy of these legends is widely debated by scholars.
Early Bronze Age archaeology.
Before the 20th century, the Zhou dynasty (1046–256 BC) was the earliest Chinese dynasty that could be verified from its own records. However during the Song dynasty (960–1279 AD), antiquarians collected bronze ritual vessels attributed to the Shang era, some of which bore inscriptions.
Yellow River valley.
In 1899, it was found that Chinese pharmacists were selling "dragon bones" marked with curious and archaic characters. These were finally traced back in 1928 to a site (now called Yinxu) near Anyang, north of the Yellow River in modern Henan province, where the Academia Sinica undertook archeological excavation until the Japanese invasion in 1937.
Archaeologists focused on the Yellow River valley in Henan as the most likely site of the states described in the traditional histories.
After 1950, remnants of an earlier walled city were discovered near Zhengzhou.
It has been determined that the earth walls at Zhengzhou, erected in the 15th century BC, would have been 20 m wide at the base, rising to a height of 8 m, and formed a roughly rectangular wall 7 km around the ancient city. The rammed earth construction of these walls was an inherited tradition, since much older fortifications of this type have been found at Chinese Neolithic sites of the Longshan culture (c. 3000–2000 BC).
In 1959, the site of the Erlitou culture was found in Yanshi, south of the Yellow River near Luoyang. Radiocarbon dating suggests that the Erlitou culture flourished ca. 2100 BC to 1800 BC. They built large palaces, suggesting the existence of an organized state.
The remains of a walled city of about 470 ha were discovered in 1999 across the Huan River from the Yinxu site. The city, now known as Huanbei, was apparently occupied for less than a century and destroyed shortly before the construction of the Yinxu complex.
Chinese historians living in later periods were accustomed to the notion of one dynasty succeeding another, and readily identified the Zhengzhou and Erlitou sites with the early Shang and Xia dynasty of traditional histories.
The actual political situation in early China may have been more complicated, with the Xia and Shang being political entities that existed concurrently, just as the early Zhou, who established the successor state of the Shang, are known to have existed at the same time as the Shang.
Other sites.
The Erligang culture represented by the Zhengzhou site is found across a wide area of China, even as far northeast as the area of modern Beijing, where at least one burial in this region during this period contained both Erligang-style bronzes and local-style gold jewelry. The discovery of a Chenggu-style "ge" dagger-axe at Xiaohenan demonstrates that even at this early stage of Chinese history, there were some ties between the distant areas of north China.
The Panlongcheng site in the middle Yangtze valley was an important regional center of the Erligang culture.
Accidental finds elsewhere in China have revealed advanced civilizations contemporaneous with but culturally unlike the settlement at Anyang, such as the walled city of Sanxingdui in Sichuan.
Western scholars are hesitant to designate such settlements as belonging to the Shang dynasty.
Also unlike the Shang, there is no known evidence that the Sanxingdui culture had a system of writing. The late Shang state at Anyang is thus generally considered the first verifiable civilization in Chinese history.
In contrast, the earliest layers of the Wucheng site, pre-dating Anyang, have yielded pottery fragments containing short sequences of symbols, suggesting that they may be a form of writing quite different in form from oracle bone characters, but the sample is too small for decipherment.
Genetic studies.
A study of mitochondrial DNA (inherited in the maternal line) from Yinxu graves showed similarity with modern northern Han Chinese, but significant differences from southern Han Chinese.
Late Shang at Anyang.
The oldest extant direct records date from around 1200 BC at Anyang, covering the reigns of the last nine Shang kings.
The Shang had a fully developed system of writing, preserved on bronze inscriptions and a small number of other writings on pottery, jade and other stones, horn, etc., but most prolifically on oracle bones. The complexity and sophistication of this writing system indicates an earlier period of development, but direct evidence of that development is still lacking.
Other advances included the invention of many musical instruments and observations of Mars and various comets by Shang astronomers.
Their civilization was based on agriculture and augmented by hunting and animal husbandry.
In addition to war, the Shang also practiced human sacrifice.
Cowry shells were also excavated at Anyang, suggesting trade with coast-dwellers, but there was very limited sea trade in ancient China since China was isolated from other large civilizations during the Shang period. Trade relations and diplomatic ties with other formidable powers via the Silk Road and Chinese voyages to the Indian Ocean did not exist until the reign of Emperor Wu during the Han dynasty (206 BC–221 AD).
Court life.
At the excavated royal palace of Yinxu, large stone pillar bases were found along with rammed earth foundations and platforms, which according to Fairbank, were "as hard as cement." These foundations in turn originally supported 53 buildings of wooden post-and-beam construction. In close proximity to the main palatial complex, there were underground pits used for storage, servants' quarters, and housing quarters.
Many Shang royal tombs had been tunneled into and ravaged by grave robbers in ancient times, but in the spring of 1976, the discovery of Tomb 5 at Yinxu revealed a tomb that was not only undisturbed, but one of the most richly furnished Shang tombs that archaeologists had yet come across. With over 200 bronze ritual vessels and 109 inscriptions of Lady Fu Hao's name, archaeologists realized they had stumbled across the tomb of the militant consort to King Wu Ding, as described in 170 to 180 Shang oracle bones. Along with bronze vessels, stoneware and pottery vessels, bronze weapons, jade figures and hair combs, and bone hairpins were found. Historian Robert L. Thorp states that the large assortment of weapons and ritual vessels in her tomb correlate with the oracle bone accounts of her military career and involvement in Wu Ding's ritual ancestral sacrifices.
The capital was the center of court life. Over time, court rituals to appease spirits developed, and in addition to his secular duties, the king would serve as the head of the ancestor worship cult. Often, the king would even perform oracle bone divinations himself, especially near the end of the dynasty. Evidence from excavations of the royal tombs indicates that royalty were buried with articles of value, presumably for use in the afterlife. Perhaps for the same reason, hundreds of commoners, who may have been slaves, were buried alive with the royal corpse.
A line of hereditary Shang kings ruled over much of northern China, and Shang troops fought frequent wars with neighboring settlements and nomadic herdsmen from the inner Asian steppes. The Shang king, in his oracular divinations, repeatedly shows concern about the "fang" groups, the barbarians living outside of the civilized "tu" regions, which made up the center of Shang territory. In particular, the "tufang" group of the Yanshan region were regularly mentioned as hostile to the Shang.
Apart from their role as the head military commanders, Shang kings also asserted their social supremacy by acting as the high priests of society and leading the divination ceremonies. As the oracle bone texts reveal, the Shang kings were viewed as the best qualified members of society to offer sacrifices to their royal ancestors and to the high god Di, who in their beliefs was responsible for the rain, wind, and thunder.
Religion.
Shang religion consisted of a mixture of shamanism, divination and sacrifice. There were six main recipients of sacrifice: (1) Di, the High God, (2) nature powers like the sun and mountain powers, (3) former lords, deceased humans who had been added to the dynastic pantheon, (4) predynastic ancestors, (5) dynastic ancestors, and (6) dynastic ancestresses such as the concubines of a past emperor. The Shang rulers subscribed to the notion that these ancestors held power over them and performed rituals to ascertain their intentions.
One of the most common rituals was divination, which often was performed to determine whether ancestors desired specific sacrifices or rituals. Divination involved cracking a turtle carapace or ox scapula to answer a question, and to then record the response to that question on the bone itself. It is unknown what criteria the diviners used to determine the response, but it is believed to be the sound or pattern of the cracks on the bone. 
The Shang also seem to have believed in an afterlife, as evidenced by the elaborate burial tombs built for deceased rulers. Often "carriages, utensils, sacrificial vessels, [and] weapons" would be included in the tomb. A king's burial involved the burial of up to several hundred humans and horses as well to accompany the king into the afterlife, in some cases even numbering four hundred. Finally, tombs included ornaments such as jade, which the Shang may have believed to protect against decay or confer immortality.
The degree to which shamanism was a central aspect of Shang religion is a subject of debate.
The Shang religion was highly bureaucratic and meticulously ordered. Oracle bones contained descriptions of the date, ritual, person, ancestor, and questions associated with the divination. Tombs displayed highly ordered arrangements of bones, with groups of skeletons laid out facing the same direction.
Bronze working.
Chinese bronze casting and pottery advanced during the Shang dynasty, with bronze typically being used for ritually significant, rather than primarily utilitarian, items.
As far back as c. 1500 BC, the early Shang dynasty engaged in large-scale production of bronze-ware vessels and weapons. This production required a large labor force that could handle the mining, refining, and transportation of the necessary copper, tin, and lead ores. This in turn created a need for official managers that could oversee both hard-laborers and skilled artisans and craftsmen. The Shang royal court and aristocrats required a vast amount of different bronze vessels for various ceremonial purposes and events of religious divination. Ceremonial rules even decreed how many bronze containers of each type a nobleman or noblewoman of a certain rank could own. With the increased amount of bronze available, the army could also better equip itself with an assortment of bronze weaponry. Bronze was also used for the fittings of spoke-wheeled chariots, which appeared in China around 1200 BC.
Military.
Bronze weapons were an integral part of Shang society. 
Shang infantry were armed with a variety of stone and bronze weaponry, including "máo" spears, "yuè" pole-axes, "gē" pole-based dagger-axes, composite bows, and bronze or leather helmets. The chariot first appeared in China during the reign of Wu Ding. Oracle bone inscriptions suggest that the western enemies of the Shang used limited numbers of chariots in battle, but the Shang themselves used them only as mobile command vehicles and in royal hunts. It is little doubt that the chariot entered China through the Central Asia and the Northern Steppe, possibly indicating some form of contact with the Indo-Europeans. Recent archaeological finds have shown that the late Shang used horses, chariots, bows and practiced horse burials that are similar to the steppe peoples to the west. Other possible cultural influences resulting from Indo-European contact may include fighting styles, head-and-hoof rituals, art motifs and myths. These influences have led one scholar, Christopher I. Beckwith, to speculate that Indo-Europeans "may even have been responsible for the foundation of the Shang Dynasty," though he admits there is no direct evidence. A crucial factor in the Zhou conquest of the Shang may have been their more effective use of chariots.
Although the Shang depended upon the military skills of their nobility, Shang rulers could mobilize the masses of town-dwelling and rural commoners as conscript laborers and soldiers for both campaigns of defense and conquest. Aristocrats and other state rulers were obligated to furnish their local garrisons with all necessary equipment, armor, and armaments. The Shang king maintained a force of about a thousand troops at his capital and would personally lead this force into battle. A rudimentary military bureaucracy was also needed in order to muster forces ranging from three to five thousand troops for border campaigns to thirteen thousand troops for suppressing rebellions against Shang dynasty.
Kings.
The earliest records are the oracle bones inscribed during the reigns of the Shang kings from Wu Ding.
The oracle bones do not contain king lists, but they do record the sacrifices to previous kings and the ancestors of the current king, which follow a standard schedule that scholars have reconstructed. From this evidence, scholars have assembled the implied king list and genealogy, finding that it is in substantial agreement with the later accounts, especially for later kings.
The Shang kings were referred to in the oracle bones by posthumous names.
The last character of each name is one of the 10 celestial stems, which also denoted the day of the 10-day Shang week on which sacrifices would be offered to that ancestor within the ritual schedule.
There were more kings than stems, so the names have distinguishing prefixes such as 大 "Dà" (greater), 中 "Zhōng" (middle), 小 "Xiǎo" (lesser), 卜 "Bǔ" (outer), 祖 "Zǔ" (ancestor) and a few more obscure names.
The kings, in the order of succession derived from the oracle bones, are here grouped by generation. Later reigns were assigned to oracle bone diviner groups by Dong Zuobin:
References.
</dl>

</doc>
<doc id="43467" url="http://en.wikipedia.org/wiki?curid=43467" title="Xia dynasty">
Xia dynasty

The Xia dynasty (; ]; c. 2070 – c. 1600 BC) is the first dynasty in China to be described in ancient historical chronicles such as "Bamboo Annals", "Classic of History" and "Records of the Grand Historian". The dynasty was established by the legendary Yu the Great after Shun, the last of the Five Emperors, gave his throne to him. The Xia was later succeeded by the Shang dynasty (1600–1046 BC).
According to the traditional chronology based upon calculations by Liu Xin, the Xia ruled between 2205 and 1766 BC; according to the chronology based upon the "Bamboo Annals", it ruled between 1989 and 1558 BC. The Xia–Shang–Zhou Chronology Project concluded that the Xia existed between 2070 and 1600 BC. The tradition of tracing Chinese political history from heroic early emperors to the Xia to succeeding dynasties comes from the idea of the Mandate of Heaven, in which only one legitimate dynasty can exist at any given time, and was promoted by the Confucian school in the Eastern Zhou period, later becoming the basic position of imperial historiography and ideology. Although the Xia is an important element in early Chinese history, reliable information on the history of China before 13th century BC can only come from archaeological evidence since China's first established written system on a durable medium, the oracle bone script, did not exist until then. Thus, the concrete existence of the Xia is yet to be proven, despite efforts by Chinese archaeologists to link the Xia with Bronze Age Erlitou archaeological sites.
Traditional history.
The Xia dynasty was described in classic texts such as the "Classic of History" ("Shujing"), the "Bamboo Annals", and the "Records of the Grand Historian" ("Shiji") by Sima Qian. It has been documented that the tribe that founded the dynasty was the Huaxia, who were the ancestral people of the Han Chinese.
Origins and early development.
According to ancient Chinese texts, before the Xia dynasty was established, battles were frequent between the Xia tribe and Chi You's tribe. The Xia tribe slowly developed around the time of Zhuanxu, one of the legendary Five Emperors. The Records of the Grand Historian and the Classic of Rites say that Yu the Great is the grandson of Zhuanxu, but there are also other records, like Ban Gu, that say Yu is the fifth generation of Zhuanxu. Based on this, it is possible that the people of the Xia clan are descendants of Zhuanxu.
Gun's attempt to stop the flood.
Gun, the father of Yu the Great, is the earliest recorded member of the Xia clan. When the Yellow River flooded, many tribes united together to control and stop the flooding. Gun was appointed by Yao to stop the flooding. He ordered the construction of large blockades to block the path of the water. The attempt of Gun to stop the flooding lasted for nine years, but it was a failure because the floods became stronger. After nine years, Yao had already given his throne to Shun. Gun was ordered to be executed by Shun at Yushan (Chinese: 羽山), a mountain located between modern Donghai County in Jiangsu Province and Linshu County in Shandong Province.
Yu the Great's attempt to stop the floods.
Yu was highly trusted by Shun, so Shun appointed him to finish his father’s work, which was to stop the flooding. Yu’s method was different from his father’s: he organized people from different tribes and ordered them to help him build canals in all the major rivers that were flooding and lead the water out to the sea. Yu was dedicated to his work. People praised his perseverance and were inspired, so much so that other tribes joined in the work. Legend says that in the 13 years it took him to successfully complete the work to stop the floods, he never went back to his home village to stop and rest, even though he passed by his house three times.
Establishment.
Yu’s success in stopping the flooding increased agricultural production (since the floods were destructive). The Xia tribe’s power increased and Yu became the leader of the surrounding tribes. Soon afterwards Shun sent Yu to lead an army to suppress the Sanmiao tribe, which continuously abused the border tribes. After defeating them, he exiled them south to the Han River area. This victory strengthened the Xia tribe’s power even more. As Shun aged, he thought of a successor and relinquished the throne to Yu, whom he deemed worthy. Yu’s succession marks the start of the Xia dynasty. As Yu neared death he passed the throne to his son, Qi, instead of passing it to the most capable candidate, thus setting the precedent for dynastic rule or the Hereditary System. The Xia dynasty began a period of family or clan control.
Jie, the last king, was said to be corrupt. He was overthrown by Tang, the first king of the Shang dynasty.
Qi state.
After the defeat of Xia by Shang, the imperial descendants scattered and were absorbed by the nearby clans, and some members of the royal family of the Xia Dynasty survived as the Qi (Henan) state until 445 BC. The Qi state was well recorded in the Oracle script as the one major supporter of the Xia dynasty.
Modern skepticism.
The Skeptical School of early Chinese history, started by Gu Jiegang in the 1920s, was the first group of scholars within China to seriously question the traditional story of its early history: "the later the time, the longer the legendary period of earlier history... early Chinese history is a tale told and retold for generations, during which new elements were added to the front end". Yun Kuen Lee's criticism of nationalist sentiment in developing an explanation of Three Dynasties chronology focuses on the dichotomy of evidence provided by archaeological versus historical research, in particular the claim that the archaeological Erlitou Culture is also the historical Xia dynasty. "How to fuse the archaeological dates with historical dates is a challenge to all chronological studies of early civilization."
In "The Shape of the Turtle: Myth, Art, and Cosmos in Early China", Sarah Allan noted that many aspects of the Xia are simply the opposite of traits held to be emblematic of the Shang dynasty. The implied dualism between the Shang and Xia, Allan argues, is that while the Shang represent fire or the sun, birds and the east, the Xia represent the west and water. The development of this mythical Xia, Allan argues, is a necessary act on the part of the Zhou dynasty, who justify their conquest of the Shang by noting that the Shang had supplanted the Xia.
Archaeological discoveries.
Archaeologists have uncovered urban sites, bronze implements, and tombs that point to the possible existence of the Xia dynasty at locations cited in ancient Chinese historical texts. There exists a debate as to whether or not the Erlitou culture was the site of the Xia dynasty. Radiocarbon dating places the site at c. 2100 to 1800 BC, providing physical evidence of the existence of a state contemporaneous with and possibly equivalent to the Xia dynasty as described in Chinese historical works. In 1959, a site located in the city of Yanshi was excavated containing large palaces that some archaeologists have attributed to capital of the Xia dynasty. Through the 1960s and 1970s, archaeologists have uncovered urban sites, bronze implements, and tombs in the same locations cited in ancient Chinese historical texts regarding Xia; at a minimum, the Xia dynasty marked an evolutionary stage between the late neolithic cultures and the typical Chinese urban civilization of the Shang dynasty.
In 2011, Chinese archaeologists uncovered the remains of an imperial sized palace—dated to about 1700 BC—at Erlitou in Henan, further fueling the discussions about the existence of the dynasty.
Sovereigns of the Xia dynasty.
The following table lists the rulers of Xia according to Sima Qian's "Shiji". Unlike Sima's list of Shang dynasty kings, which is closely matched by inscriptions on oracle bones from late in that period, records of Xia rulers have not yet been found in archeological excavations.

</doc>
<doc id="43468" url="http://en.wikipedia.org/wiki?curid=43468" title="T. E. Lawrence">
T. E. Lawrence

Thomas Edward Lawrence CB DSO (16 August 1888 – 19 May 1935) was an archaeologist and British Army officer renowned especially for his liaison role during the Sinai and Palestine Campaign, and the Arab Revolt against Ottoman Turkish rule of 1916–18. The breadth and variety of his activities and associations, and his ability to describe them vividly in writing, earned him international fame as Lawrence of Arabia, a title which was later used for the 1962 film based on his First World War activities.
Lawrence was born out of wedlock in Tremadog, Wales, in August 1888 to Sir Thomas Chapman and Sarah Junner, a governess who was herself illegitimate. Chapman had left his wife and first family in Ireland to live with Junner, and they called themselves Mr and Mrs Lawrence. In the summer of 1896 the Lawrences moved to Oxford, where in 1907–10 young Lawrence studied History at Jesus College and graduated with First Class Honours. He became a practising archaeologist in the Middle East, working at various excavations with David George Hogarth and Leonard Woolley. In 1908, he joined the Oxford University Officers' Training Corps and underwent a two-year training course. In January 1914, before the outbreak of the Great War, Lawrence was commissioned by the British Army to undertake a military survey of the Negev Desert while doing archaeological research.
Lawrence's public image resulted in part from the sensationalized reportage of the Arab revolt by an American journalist, Lowell Thomas, as well as from Lawrence's autobiographical account "Seven Pillars of Wisdom" (1922). In 1935, Lawrence was fatally injured in a motorcycle accident in Dorset.
Early life.
Lawrence was born on 16 August 1888 in Tremadog, Caernarfonshire (now Gwynedd), Wales, in a house named Gorphwysfa, now known as Snowdon Lodge. His Anglo-Irish father, Thomas Robert Tighe Chapman, who in 1914 inherited the title of Westmeath in Ireland as seventh Baronet, had left his wife Edith for his daughters' governess Sarah Junner. Junner's mother, Elizabeth Junner, had named as Sarah's father a "John Junner – shipwright journeyman", though she had been living as an unmarried servant in the household of a John Lawrence, ship's carpenter, just four months earlier.
Thomas Chapman and Sarah Junner did not marry but were known as Mr and Mrs Lawrence. They had five sons; Thomas Edward was the second eldest. From Wales the family moved to Kirkcudbright, Galloway, in southwestern Scotland, then Dinard in Brittany, then to Jersey. In 1894–96 the family lived at Langley Lodge (now demolished), set in private woods between the eastern borders of the New Forest and Southampton Water in Hampshire. Mr Lawrence sailed and took the boys to watch yacht racing in the Solent. By the time they left, the eight-year-old Ned (as Lawrence became known) had developed a taste for the countryside and outdoor activities.
In the summer of 1896 the Lawrences moved to 2, Polstead Road in Oxford, where, until 1921, they lived under the names of Mr and Mrs Lawrence. Lawrence attended the City of Oxford High School for Boys, where one of the four houses was later named "Lawrence" in his honour; the school closed in 1966. Lawrence and one of his brothers became commissioned officers in the Church Lads' Brigade at St Aldate's Church.
Lawrence claimed that circa 1905, he ran away from home and served for a few weeks as a boy soldier with the Royal Garrison Artillery at St Mawes Castle in Cornwall, from which he was bought out. No evidence of this can be found in army records.
Middle East archaeology.
At the age of 15, Lawrence and his schoolfriend Cyril Beeson bicycled around Berkshire, Buckinghamshire and Oxfordshire, visited almost every village's parish church, studied their monuments and antiquities, and made rubbings of their monumental brasses. Lawrence and Beeson monitored building sites in Oxford and presented their finds to the Ashmolean Museum. The Ashmolean's "Annual Report" for 1906 said the two teenage boys "by incessant watchfulness secured everything of antiquarian value which has been found". In the summers of 1906 and 1907, Lawrence and Beeson toured France by bicycle, collecting photographs, drawings, and measurements of medieval castles.
From 1907 to 1910, Lawrence studied History at Jesus College, Oxford. In the summer of 1909, he set out alone on a three-month walking tour of crusader castles in Ottoman Syria, during which he travelled 1000 mi on foot. Lawrence graduated with First Class Honours after submitting a thesis entitled "The influence of the Crusades on European Military Architecture—to the end of the 12th century", based on his field research with Beeson in France, notably in Châlus, and his solo research in the Middle East.
On completing his degree in 1910, Lawrence commenced postgraduate research in medieval pottery with a Senior Demy, a form of scholarship, at Magdalen College, Oxford, which he abandoned after he was offered the opportunity to become a practising archaeologist in the Middle East, at Carchemish in the expedition which D. G. Hogarth was setting up on behalf of the British Museum. Lawrence was a polyglot whose published work demonstrates competence in Ancient Greek, Arabic, and French.
In December 1910, he sailed for Beirut and on his arrival went to Jbail (Byblos), where he studied Arabic. He then went to work on the excavations at Carchemish, near Jerablus in northern Syria, where he worked under Hogarth and R. Campbell Thompson of the British Museum. He would later state that everything he had accomplished, he owed to Hogarth. As the site lay near an important crossing on the Baghdad Railway, knowledge he gathered there subsequently proved to be of considerable importance to the military. While excavating at Carchemish, Lawrence met Gertrude Bell, who was to work with him later on in setting up the state of Iraq.
In late 1911, Lawrence returned to England for a brief sojourn. By November he was en route to Beirut for a second season at Carchemish, where he was to work with Leonard Woolley. Before resuming work there, however, he briefly worked with Flinders Petrie at Kafr Ammar in Egypt. Between the spring of 1912 and the autumn of 1913, Lawrence stayed at Carchemish for four excavation seasons, residing in a spacious excavation house, newly built inside the site by himself and Woolley on behalf of the British Museum.
In January 1914, Woolley and Lawrence were co-opted by the British military as an archaeological smokescreen for a British military survey of the Negev Desert. They were funded by the Palestine Exploration Fund to search for an area referred to in the Bible as the "Wilderness of Zin"; along the way, they undertook an archaeological survey of the Negev Desert. The Negev was of strategic importance, as it would have to be crossed by any Ottoman army attacking Egypt in the event of war. Woolley and Lawrence subsequently published a report of the expedition's archaeological findings, but a more important result was an updated mapping of the area, with special attention to features of military relevance such as water sources. Lawrence also visited Aqaba and Petra.
From March to May 1914, Lawrence worked again at Carchemish. Following the outbreak of hostilities in August 1914, Lawrence did not immediately enlist in the British Army; on the advice of S. F. Newcombe he held back until October, when he was commissioned on the General List and posted to the intelligence staff in Cairo before the end of the year.
Arab Revolt.
At the outbreak of the war Lawrence was a university post-graduate researcher who had for years travelled extensively within the Ottoman Empire provinces of the Levant (Transjordan and Palestine) and Mesopotamia (Syria and Iraq) under his own name. As such he had become known to the Ottoman Interior Ministry authorities and their German technical advisers, travelling on the German-designed, built, and financed railways during the course of his research.
The Arab Bureau of Britain's Foreign Office conceived a campaign of internal insurgency against the Ottoman Empire in the Middle East. The Arab Bureau had long felt it likely that a campaign instigated and financed by outside powers, supporting the breakaway-minded tribes and regional challengers to the Turkish government's centralised rule of their empire, would pay great dividends in the diversion of effort that would be needed to meet such a challenge. The Arab Bureau had recognised the strategic value of what is today called the "asymmetry" of such conflict. The Ottoman authorities would have to devote from a hundred to a thousand times the resources to contain the threat of such an internal rebellion compared to the Allies' cost of sponsoring it.
With his first-hand knowledge of Syria, the Levant, and Mesopotamia (not to mention having already worked as a part-time civilian army intelligence officer), on his formal enlistment in 1914 Lawrence was posted to Cairo on the Intelligence Staff of the GOC Middle East. The British government in Egypt sent Lawrence to work with the Hashemite forces in the Arabian Hejaz in October 1916. There he met and worked with Herbert Garland.
During the war, Lawrence fought alongside Arab irregular troops under the command of Emir Faisal, a son of Sharif Hussein of Mecca, in extended guerrilla operations against the armed forces of the Ottoman Empire. Lawrence obtained assistance from the Royal Navy to turn back an Ottoman attack on Yanbu in December 1916. Lawrence's major contribution to the revolt was convincing the Arab leaders (Faisal and Abdullah) to co-ordinate their actions in support of British strategy. He persuaded the Arabs not to make a frontal assault on the Ottoman stronghold in Medina but to allow the Turkish army to tie up troops in the city garrison. The Arabs were then free to direct most of their attention to the Turks' weak point, the Hejaz railway that supplied the garrison. This vastly expanded the battlefield and tied up even more Ottoman troops, who were then forced to protect the railway and repair the constant damage. Lawrence developed a close relationship with Faisal, whose Arab Northern Army was to become the main beneficiary of British aid.
On January 3, 1917, Lawrence went off on his first desert raid with 35 armed tribesmen. Under cover of darkness, they rode their camels out of camp, dismounted and scrambled up a steep hill overlooking a Turkish encampment, which they peppered with rifle fire until driven off. Returning, they came across two Turks relieving themselves, and took them back to camp for questioning. That minor triumph was later counterbalanced by a small tragedy when, to prevent a crippling blood feud from breaking out, Lawrence had to personally execute a member of his own band, a deed that would haunt him for the rest of his life. At the end of March, Lawrence set off on his first raid against the railway, a Turkish station at Abu el-Naam. After carefully reconnoitering it, Lawrence crept down to the lines at nightfall and laid a Garland mine under the tracks, cutting the telegraph wires as he left. The next morning, the Bedouins overran the station with the aid of a mountain gun and a howitzer, setting several wagons of a nearby train on fire. As it steamed out of the station, Lawrence blew the mine under the front bogies, knocking it off the rails. Although the Turks got the train rolling again, the operation was a success. The attacks on the railway continued throughout 1917. During one, Lawrence blew up a locomotive with an electric mine. ‘We had a Lewis [machine gun],’ he wrote in a letter to a friend, ‘and flung bullets through the sides. So they hopped out and took cover behind the embankment, and shot at us between the wheels at 50 yards.’ The Arabs brought up a Stokes mortar, and the Turks fled across open ground. ‘Unfortunately for them,’ Lawrence continued, ‘the Lewis covered the open stretch. The whole job took ten minutes, and they lost 70 killed, 30 wounded and 80 prisoners,’ for the loss of only one Arab. While the Arabs looted the train, another Turkish force arrived, nearly cutting off the Bedouins. ‘I lost some baggage, and nearly myself,’ Lawrence added nonchalantly. In another letter about that same’show,’ Lawrence confided, ‘I’m not going to last out this game much longer: nerves going and temper wearing thin….This killing and killing of Turks is horrible.
Capture of Aqaba.
In 1917, Lawrence arranged a joint action with the Arab irregulars and forces including Auda Abu Tayi (until then in the employ of the Ottomans) against the strategically located but lightly defended town of Aqaba. On 6 July, after a surprise overland attack, Aqaba fell to Lawrence and the Arab forces. After Aqaba, Lawrence was promoted to major, and the new commander-in-chief of the Egyptian Expeditionary Force, General Sir Edmund Allenby, agreed to his strategy for the revolt, stating after the war:
I gave him a free hand. His cooperation was marked by the utmost loyalty, and I never had anything but praise for his work, which, indeed, was invaluable throughout the campaign. He was the mainspring of the Arab movement and knew their language, their manners and their mentality."
Lawrence now held a powerful position, as an adviser to Faisal and a person who had Allenby's confidence.
Battle of Tafileh.
In January 1918, Lawrence fought in the battle of Tafileh, an important region southeast of the Dead Sea, together with Arab regulars under the command of Jafar Pasha al-Askari. The battle was a defensive engagement that turned into an offensive rout and was described in the official history of the war as a "brilliant feat of arms". Lawrence was awarded the Distinguished Service Order for his leadership at Tafileh and was promoted to Lieutenant Colonel. The battle took the lives of 400 Turks and captured more than 200 prisoners.
By the summer of 1918, the Turks were offering a substantial reward for Lawrence's capture, with one officer writing in his notes: "Though a price of £15,000 has been put on his head by the Turks, no Arab has, as yet, attempted to betray him. The Sharif of Mecca [King of the Hedjaz] has given him the status of one of his sons, and he is just the finely tempered steel that supports the whole structure of our influence in Arabia. He is a very inspiring gentleman adventurer."
Fall of Damascus.
Lawrence was involved in the build-up to the capture of Damascus in the final weeks of the war. Much to his disappointment, and contrary to instructions he had issued, he was not present at the city's formal surrender, having arrived several hours after the city had fallen. Lawrence entered Damascus around 9am on 1 October 1918 but was only the third arrival of the day; the first was the 10th Australian Light Horse Brigade, led by Major A.C.N. 'Harry' Olden, who formally accepted the surrender of the city from acting Governor Emir Said. In newly liberated Damascus —which he had envisaged as the capital of an Arab state—Lawrence was instrumental in establishing a provisional Arab government under Faisal. The latter's rule as king, however, came to an abrupt end in 1920, after the battle of Maysaloun, when the French Forces of General Gouraud, under the command of General Mariano Goybet, entered Damascus, destroying Lawrence's dream of an independent Arabia.
During the closing years of the war Lawrence sought, with mixed success, to convince his superiors in the British government that Arab independence was in their interests. The secret Sykes-Picot Agreement between France and Britain contradicted the promises of independence he had made to the Arabs and frustrated his work.
In 1918, he cooperated with war correspondent Lowell Thomas for a short period. During this time Thomas and his cameraman Harry Chase shot a great deal of film and many photographs, which Thomas used in a highly lucrative film that toured the world after the war.
[Lowell Thomas] went to Jerusalem where he met Lawrence, whose enigmatic figure in Arab uniform fired his imagination. With Allenby's permission he linked up with Lawrence for a brief couple of weeks ... Returning to America, Thomas, early in 1919, started his lectures, supported by moving pictures of veiled women, Arabs in their picturesque robes, camels and dashing Bedouin cavalry, which took the nation by storm, after running at Madison Square Garden in New York. On being asked to come to England, he made the condition he would do so if asked by the King and given Drury Lane or Covent Garden ... He opened at Covent Garden on 14 August 1919 ... And so followed a series of some hundreds of lectures—film shows, attended by the highest in the land ..."
Postwar years.
Lawrence returned to the United Kingdom a full colonel. Immediately after the war, Lawrence worked for the Foreign Office, attending the Paris Peace Conference between January and May as a member of Faisal's delegation. 
On 17 May 1919 the Handley Page Type O carrying Lawrence on a flight to Egypt crashed at the airport of Roma-Centocelle. The pilot and co-pilot were killed; Lawrence survived with a broken shoulder blade and two broken ribs. During his brief hospitalisation, he was visited by King Victor Emmanuel III.
In August 1919 Lowell Thomas launched a colourful photo show in London entitled "With Allenby in Palestine", which included a lecture, dancing, and music. Initially, Lawrence played only a supporting role in the show, but when Thomas realised that it was the photos of Lawrence dressed as a Bedouin that had captured the public's imagination, he photographed him again, in London, in Arab dress. With the new photos, Thomas re-launched his show as "With Allenby in Palestine and Lawrence in Arabia" in early 1920; it was extremely popular. Thomas' shows made the previously-obscure Lawrence into a household name.
He served for much of 1921 as an advisor to Winston Churchill at the Colonial Office.
In August 1922, Lawrence enlisted in the Royal Air Force as an aircraftman under the name John Hume Ross. At the RAF recruiting centre in Covent Garden, London, he was interviewed by a recruiting officer—Flying Officer W. E. Johns, later to be well known as the author of the Biggles series of novels. Johns rejected Lawrence's application as he correctly believed "Ross" was a false name. Lawrence admitted this was so, and that the documents he had provided were false, and left. But he returned some time later with an RAF Messenger, carrying a written order for Johns to accept Lawrence.
However, Lawrence was forced out of the RAF in February 1923 after being exposed. He changed his name to T. E. Shaw and joined the Royal Tank Corps in 1923. He was unhappy there and repeatedly petitioned to rejoin the RAF, which finally readmitted him in August 1925. A fresh burst of publicity after the publication of "Revolt in the Desert" resulted in his assignment to a remote base in British India in late 1926, where he remained until the end of 1928. At that time he was forced to return to Britain after rumours began to circulate that he was involved in espionage activities.
He purchased several small plots of land in Chingford, built a hut and swimming pool there, and visited frequently. The hut was removed in 1930 when the Chingford Urban District Council acquired the land and passed it to the City of London Corporation, which re-erected the hut in the grounds of The Warren, Loughton, where it remains, neglected, today. Lawrence's tenure of the Chingford land has now been commemorated by a plaque fixed on the sighting obelisk on Pole Hill.
He continued serving in the RAF based at Bridlington, East Riding of Yorkshire, specialising in high-speed boats and professing happiness, and it was with considerable regret that he left the service at the end of his enlistment in March 1935.
Lawrence was a keen motorcyclist, and, at different times, had owned eight Brough Superior motorcycles. His last SS100 (Registration GW 2275) is privately owned but has been on loan to the National Motor Museum, Beaulieu and the Imperial War Museum in London.
Among the books Lawrence is known to have carried with him on his military campaigns is Thomas Malory's "Morte D'Arthur". Accounts of the 1934 discovery of the Winchester Manuscript of the "Morte" include a report that Lawrence followed Eugene Vinaver—a Malory scholar—by motorcycle from Manchester to Winchester upon reading of the discovery in "The Times".
Death.
At the age of 46, two months after leaving military service, Lawrence was fatally injured in an accident on his Brough Superior SS100 motorcycle in Dorset, close to his cottage, Clouds Hill, near Wareham. A dip in the road obstructed his view of two boys on their bicycles; he swerved to avoid them, lost control, and was thrown over the handlebars. He died six days later on 19 May 1935. The spot is marked by a small memorial at the side of the road.
One of the doctors attending him was the neurosurgeon Hugh Cairns, who consequently began a long study of the unnecessary loss of life by motorcycle dispatch riders through head injuries. His research led to the use of crash helmets by both military and civilian motorcyclists.
The Moreton estate, which borders Bovington Camp, was owned by Lawrence's cousins, the Frampton family. Lawrence had rented and later bought Clouds Hill from the Framptons. He had been a frequent visitor to their home, Okers Wood House, and had for years corresponded with Louisa Frampton. With his body wrapped in the Union Flag, Lawrence's mother arranged with the Framptons for him to be buried in their family plot at Moreton. His coffin was transported on the Frampton estate's bier. Mourners included Winston and Clementine Churchill, E. M. Forster and Lawrence's youngest brother, Arnold.
A bust of Lawrence was placed in the crypt at St Paul's Cathedral, London, and a stone effigy by Eric Kennington remains in the Anglo-Saxon church of St Martin, Wareham in Dorset.
Writings.
Throughout his life, Lawrence was a prolific writer. A large portion of his output was epistolary; he often sent several letters a day. Several collections of his letters have been published. He corresponded with many notable figures, including George Bernard Shaw, Edward Elgar, Winston Churchill, Robert Graves, Noël Coward, E. M. Forster, Siegfried Sassoon, John Buchan, Augustus John and Henry Williamson. He met Joseph Conrad and commented perceptively on his works. The many letters that he sent to Shaw's wife, Charlotte, are revealing as to his character.
In his lifetime, Lawrence published three major texts. The most significant was his account of the Arab Revolt, "Seven Pillars of Wisdom". Two were translations: Homer's "Odyssey", and "The Forest Giant"—the latter an otherwise forgotten work of French fiction. He received a flat fee for the second translation, and negotiated a generous fee plus royalties for the first.
"Seven Pillars of Wisdom".
Lawrence's major work is "Seven Pillars of Wisdom", an account of his war experiences. In 1919 he had been elected to a seven-year research fellowship at All Souls College, Oxford, providing him with support while he worked on the book. In addition to being a memoir of his experiences during the war, certain parts also serve as essays on military strategy, Arabian culture and geography, and other topics. Lawrence re-wrote "Seven Pillars of Wisdom" three times; once "blind" after he lost the manuscript while changing trains at Reading railway station.
The list of his alleged "embellishments" in "Seven Pillars" is long, though many such allegations have been disproved with time, most definitively in Jeremy Wilson's . However, Lawrence's own notebooks refute his claim to have crossed the Sinai Peninsula from Aqaba to the Suez Canal in just 49 hours without any sleep. In reality, this famous camel ride lasted for more than 70 hours and was interrupted by two long breaks for sleeping, which Lawrence omitted when he wrote his book.
Lawrence acknowledged having been helped in the editing of the book by George Bernard Shaw. In the preface to "Seven Pillars", Lawrence offered his "thanks to Mr. and Mrs. Bernard Shaw for countless suggestions of great value and diversity: and for all the present semicolons".
The first public edition was published in 1926 as a high-priced private subscription edition, printed in London by Herbert John Hodgson and Roy Manning Pike, with illustrations by Eric Kennington, Augustus John, Paul Nash, Blair Hughes-Stanton and his wife Gertrude Hermes. Lawrence was afraid that the public would think that he would make a substantial income from the book, and he stated that it was written as a result of his war service. He vowed not to take any money from it, and indeed he did not, as the sale price was one third of the production costs. This, along with his "saintlike" generosity, left Lawrence in substantial debt.
"Revolt in the Desert".
 "Revolt in the Desert" was an abridged version of "Seven Pillars" that he began in 1926 and that was published in March 1927 in both limited and trade editions. He undertook a needed but reluctant publicity exercise, which resulted in a best-seller. Again he vowed not to take any fees from the publication, partly to appease the subscribers to "Seven Pillars" who had paid dearly for their editions. By the fourth reprint in 1927, the debt from "Seven Pillars" was paid off. As Lawrence left for military service in India at the end of 1926, he set up the "Seven Pillars Trust" with his friend D. G. Hogarth as a trustee, in which he made over the copyright and any surplus income of "Revolt in the Desert". He later told Hogarth that he had "made the Trust final, to save myself the temptation of reviewing it, if "Revolt" turned out a best seller."
The resultant trust paid off the debt, and Lawrence then invoked a clause in his publishing contract to halt publication of the abridgment in the United Kingdom. However, he allowed both American editions and translations, which resulted in a substantial flow of income. The trust paid income either into an educational fund for children of RAF officers who lost their lives or were invalided as a result of service, or more substantially into the RAF Benevolent Fund.
Posthumous.
Lawrence left unpublished "The Mint", a memoir of his experiences as an enlisted man in the Royal Air Force (RAF). For this, he worked from a notebook that he kept while enlisted, writing of the daily lives of enlisted men and his desire to be a part of something larger than himself: the Royal Air Force. The book is stylistically very different from "Seven Pillars of Wisdom", using sparse prose as opposed to the complicated syntax found in "Seven Pillars". It was published posthumously, edited by his brother, Professor A. W. Lawrence.
After Lawrence's death, A. W. Lawrence inherited Lawrence's estate and his copyrights as the sole beneficiary. To pay the inheritance tax, he sold the U.S. copyright of "Seven Pillars of Wisdom" (subscribers' text) outright to Doubleday Doran in 1935. Doubleday still controls publication rights of this version of the text of "Seven Pillars of Wisdom" in the USA. In 1936 Prof. Lawrence split the remaining assets of the estate, giving Clouds Hill and many copies of less substantial or historical letters to the nation via the National Trust, and then set up two trusts to control interests in T. E. Lawrence's residual copyrights. To the original Seven Pillars Trust, Prof. Lawrence assigned the copyright in "Seven Pillars of Wisdom", as a result of which it was given its first general publication. To the Letters and Symposium Trust, he assigned the copyright in "The Mint" and all Lawrence's letters, which were subsequently edited and published in the book "T. E. Lawrence by his Friends" (edited by A. W. Lawrence, London, Jonathan Cape, 1937).
A substantial amount of income went directly to the RAF Benevolent Fund or for archaeological, environmental, or academic projects. The two trusts were amalgamated in 1986 and, on the death of Prof. A. W. Lawrence in 1991, the unified trust also acquired all the remaining rights to Lawrence's works that it had not owned, plus rights to all of Prof. Lawrence's works.
Sexuality.
Lawrence's biographers have discussed his sexuality at considerable length, and this discussion has spilled into the popular press.
There is no reliable evidence for consensual sexual intimacy between Lawrence and any person. His friends have expressed the opinion that he was asexual, and Lawrence himself specifically denied, in multiple private letters, any personal experience of sex. While there were suggestions that Lawrence had been intimate with Dahoum, who worked with Lawrence at a pre-war archaeological dig in Carchemish, and fellow-serviceman R.A.M. Guy, his biographers and contemporaries have found them unconvincing.
The dedication to his book "Seven Pillars" is a poem titled "To S.A." which opens:
Lawrence was never specific about the identity of "S.A." There are many theories which argue in favour of individual men, women, and the Arab nation. The most popular is that S.A. represents (at least in part) his companion Selim Ahmed, "Dahoum", who apparently died of typhus before 1918.
Although Lawrence lived in a period during which official opposition to homosexuality was strong, his writing on the subject was tolerant. In "Seven Pillars", when discussing relationships between young male fighters in the war, he refers on one occasion to "the openness and honesty of perfect love" and on another to "friends quivering together in the yielding sand with intimate hot limbs in supreme embrace". In a letter to Charlotte Shaw he wrote, "I've seen lots of man-and-man loves: very lovely and fortunate some of them were."
In both "Seven Pillars" and a 1919 letter to a military colleague, Lawrence describes an episode on 20 November 1917 in which, while reconnoitring Dera'a in disguise, he was captured by the Ottoman military, heavily beaten, and sexually abused by the local Bey and his guardsmen. The precise nature of the sexual contact is not specified. There have been allegations that the episode was an invention of Lawrence’s and (with some evidence) that the injuries Lawrence claims to have suffered were exaggerated. Although there is no independent testimony, the multiple consistent reports, and the absence of evidence for outright invention in Lawrence's works, make the account believable to his biographers. At least three of Lawrence's biographers (Malcolm Brown, John E. Mack, and Jeremy Wilson) have argued this episode had strong psychological effects on Lawrence which may explain some of his unconventional behaviour in later life.
There is considerable evidence that Lawrence was a masochist. In his description of the Dera'a beating, Lawrence wrote "a delicious warmth, probably sexual, was swelling through me", and also included a detailed description of the guards' whip in a style typical of masochists' writing. In later life, Lawrence arranged to pay a military colleague to administer beatings to him, and to be subjected to severe formal tests of fitness and stamina. While John Bruce, who first wrote on this topic, included some other claims which were not credible, Lawrence's biographers regard the beatings as established fact.
John E. Mack sees a possible connection between T. E.'s masochism and the childhood beatings he had received from his mother for routine misbehaviours. His brother Arnold thought the beatings had been given for the purpose of breaking T. E.'s will. Writing in 1997, Angus Calder noted that it is "astonishing" that earlier commentators discussing Lawrence's apparent masochism and self-loathing failed to consider the impact on Lawrence of having lost his brothers Frank and Will on the Western front, along with many other school friends.
Awards and commemorations.
Lawrence was made a Companion of the Order of the Bath and awarded the Distinguished Service Order and the French Légion d'Honneur, though in October 1918 he refused to be made a Knight Commander of the Most Excellent Order of the British Empire. A bronze bust of Lawrence was placed in the crypt of St Paul's Cathedral alongside the tombs of Britain's greatest military leaders. An English Heritage blue plaque marks Lawrence's childhood home at 2 Polstead Road, Oxford, OX2, and another appears on his London home at 14 Barton Street Westminster, SW1. In 2002, Lawrence was named 53rd in the BBC's list of the 100 Greatest Britons following a UK-wide vote.

</doc>
<doc id="43469" url="http://en.wikipedia.org/wiki?curid=43469" title="Karel Čapek">
Karel Čapek

Karel Čapek (]) (January 9, 1890 – December 25, 1938) was a Czech writer of the early 20th century. He had multiple roles throughout his career such as playwright, dramatist, essayist, publisher, literary reviewer, and art critic. Nonetheless, he's best known for his science fiction, including his novel "War with the Newts" and play "R.U.R.", The latter work popularized the word "robot" and changed history (the second "R" of the title standing for "Robots").
Arthur Miller wrote in 1990: "I read Karel Čapek for the first time when I was a college student long
ago in the Thirties. There was no writer like him...prophetic assurance mixed with surrealistic humour and hard-edged social satire: a unique combination...he is a joy to read."
Although primarily known for his work in science fiction, Čapek also wrote several politically charged works dealing with the social turmoil of his time. Having help create the Czechoslovak PEN Club as a key part of the International PEN Club, he campaigned in favor of free expression and utterly despised the rise of fascism in Europe. Were it not for his untimely death (of natural causes) taking place as Nazi Germany began its takeover of Czechoslovakia, he would likely have been found and executed by the Gestapo. In the aftermath of World War II, his legacy as a literary figure has been well established.
Biography.
Born in 1890 in the Bohemian mountain village of Malé Svatoňovice to an overbearing, emotional mother and a distant yet adored father, Čapek was the youngest of three siblings. Čapek would maintain a close relationship with his brother Josef, living and writing with him throughout his adult life.
Čapek became enamored with the visual arts in his teenage years, especially Cubism. He studied in Prague at Charles University and at the Sorbonne in Paris. Exempted from military service due to the spinal problems that would haunt him his whole life, Čapek observed World War I from Prague. His political views were strongly affected by the war, and as a budding journalist he began to write on topics like nationalism, totalitarianism and consumerism. Through social circles, the young writer developed close relationships with many of the political leaders of the nascent Czechoslovak state. This included Tomáš Garrigue Masaryk and his son Jan, who would later become foreign secretary.
His early attempts at fiction were mostly plays written with brother Josef. Čapek's first international success was "Rossum's Universal Robots", a dystopian work about a bad day at a factory populated with sentient androids. The play was translated into English in 1922, and was being performed in the UK and America by 1923. Throughout the 1920s, Čapek worked in many writing genres, producing both fiction and non-fiction, but worked primarily as a journalist. .
In the 1930s, Čapek's work focused on the threat of brutal national socialist and fascist dictatorships; by
the mid-1930s, Čapek had become "an outspoken anti-fascist". His most productive years were during the The First Republic of Czechoslovakia (1918–1938). He wrote "Talks with T. G. Masaryk" – Masaryk was a Czechoslovak patriot, the first President of Czechoslovakia, and a regular guest at Čapek's "Friday Men" garden parties for leading Czech intellectuals. Čapek was also a member of Masaryk's "Hrad" political network. This extraordinary relationship between the writer and the political leader may be unique. He also became a member of International PEN and established, and was first president of, the Czechoslovak Pen Club.
Soon after 1938 it became clear that the Western allies (France, Great Britain) had failed to fulfill the agreements (see Western betrayal), and failed to defend Czechoslovakia against Adolf Hitler. Karel Čapek refused to leave his country – despite the fact that the Nazi Gestapo had named him Czechoslovakia's "public enemy number two". Although he suffered all his life from the condition spondyloarthritis, Karel Čapek died of double pneumonia, on 25 December 1938, shortly after part of Czechoslovakia was annexed by Nazi Germany following the so-called Munich Agreement. Čapek is buried at the Vyšehrad cemetery in Prague. His brother Josef Čapek, a painter and writer, died in the Nazi Bergen-Belsen concentration camp.
Writing.
Karel Čapek wrote with intelligence and humor on a wide variety of subjects. His works are known for their interesting and precise description of reality. Čapek is renowned for his excellent work with the Czech language. He is known as a science fiction author, who wrote before science fiction became widely recognized as a separate genre.
Čapek began his writing career as a journalist. With his brother Josef, he worked as an editor for the Czech paper Narodni Listy (The National Newspaper) from October 1917 to April 1921. Upon leaving, he and Josef joined the staff of Lidove noviny (The People's Paper) in April 1921.
Many of his works discuss ethical aspects of industrial inventions and processes already anticipated in the first half of the 20th century. These include mass production, nuclear weapons, and post-human intelligent beings such as robots or intelligent salamanders.
Čapek also expressed fear of social disasters, dictatorship, violence, human stupidity, the unlimited power of corporations, and greed. Čapek tried to find hope, and the way out.
From the 1930s onward, Čapek's work became increasingly anti-fascist, anti-militarist, and critical
of what he saw as "irrationalism".
Ivan Klíma, in his biography of Čapek, notes his influence on modern Czech literature, as well as on the development of Czech as a written language. Čapek, along with contemporaries like Jaroslav Hašek, spawned part of the early 20th century revival in written Czech thanks to their decision to use the vernacular. Klíma writes, "It is thanks to Čapek that the written Czech language grew closer to the language people actually spoke". Čapek was also a translator, and his translations of French poetry into the language inspired a new generation of Czech poets.
His other books and plays include detective stories, novels, fairy tales and theatre plays, and even a book on gardening. His most important works attempt to resolve problems of epistemology, to answer the question: "What is knowledge?" Examples include "Tales from Two Pockets", and the first book of the trilogy of novels "Hordubal," "Meteor," and "An Ordinary Life."
After World War II, Čapek's work was only reluctantly accepted by the communist government of Czechoslovakia, because during his life he had refused to accept communism as a viable alternative. He was the first in a series of influential non-Marxist intellectuals who wrote a newspaper essay in a series called "Why I am not a Communist".
In 2009 (70 years after his death), a book was published containing extensive correspondence by Karel Čapek, in which the writer discusses the subjects of pacifism and his conscientious objection to military service with lawyer Jindřich Groag from Brno. Until then, only a portion of these letters were known.
Etymology of "robot".
Karel Čapek introduced and made popular the frequently used international word "robot", which first appeared in his play "R.U.R." in 1920. While it is frequently thought that he was the originator of the word, he wrote a short letter in reference to an article in the "Oxford English Dictionary" etymology in which he named his brother, painter and writer Josef Čapek, as its actual inventor. In an article in the Czech journal "Lidové noviny" in 1933, he also explained that he had originally wanted to call the creatures "laboři" (from Latin "labor", work). However, he did not like the word, seeing it as too artificial, and sought advice from his brother Josef, who suggested "roboti" ("robots" in English).
The word "robot" comes from the word "robota". The word robota means literally "corvée", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in Slovak, archaic Czech, and many other Slavic languages (e.g.: Bulgarian, Russian, Serbian, Polish, Macedonian, Ukrainian, etc.). Another cognate is the German word for "work", "arbeit".

</doc>
<doc id="43472" url="http://en.wikipedia.org/wiki?curid=43472" title="Nine Men's Morris">
Nine Men's Morris

Nine Men's Morris is a strategy board game for two players that emerged from the Roman Empire. The game is also known as Nine Man Morris, Mill, Mills, The Mill Game, Merels, Merrills, Merelles, Marelles, Morelles and Ninepenny Marl in English. The game has also been called Cowboy Checkers and was once printed on the back of checkerboards. Nine Men's Morris is a solved game in which either player can force the game into a draw.
Three main variants of Nine Men's Morris are Three-, Six- and Twelve-Men's Morris.
Game rules.
The board consists of a grid with twenty-four intersections or "points". Each player has nine pieces, or "men", usually coloured black and white. Players try to form 'mills'— three of their own men lined horizontally or vertically—allowing a player to remove an opponent's man from the game. A player wins by reducing the opponent to two pieces (where he could no longer form mills and thus be unable to win), or by leaving him without a legal move.
The game proceeds in three phases:
Phase 1: Placing pieces.
The game begins with an empty board. The players determine who plays first, then take turns placing their men one per play on empty points. If a player is able to place three of his pieces in a straight line, vertically or horizontally, he has formed a "mill" and may remove one of his opponent's pieces from the board and the game. Any piece can be chosen for the removal, but a piece not in an opponent's mill must be selected, if possible.
Phase 2: Moving pieces.
Players continue to alternate moves, this time moving a man to an adjacent point. A piece may not "jump" another piece. Players continue to try to form mills and remove their opponent's pieces in the same manner as in phase one. A player may "break" a mill by moving one of his pieces out of an existing mill, then moving the piece back to form the same mill a second time (or any number of times), each time removing one of his opponent's men. The act of removing an opponent's man is sometimes called "pounding" the opponent. When one player has been reduced to three men, phase three begins.
Phase 3: "Flying".
When a player is reduced to three pieces, there is no longer a limitation on that player of moving to only adjacent points: The player's men may "fly", "hop", or "jump" from any point to any vacant point.
Some rules sources say this is the way the game is played, some treat it as a variation, and some don't mention it at all. A 19th-century games manual calls this the "truly rustic mode of playing the game". Flying was introduced to compensate when the weaker side is one man away from losing the game.
Strategy.
At the beginning of the game, it is more important to place pieces in versatile locations rather than to try to form mills immediately and make the mistake of concentrating one's pieces in one area of the board.
An ideal position, which typically results in a win, is to be able to shuttle one piece back and forth between two mills, removing a piece every turn.
Variants.
Three Men's Morris.
Three Men's Morris, also called "Nine Holes", is played on the points of a grid of 2×2 squares, or in the squares of a grid of 3×3 squares as in tic-tac-toe. The game is for two players; each player has three men. The players put one man on the board in each of their first three plays, winning if a mill is formed (as in tic-tac-toe). After that, each player moves one of his men, according to one of the following rules versions:
A player wins by forming a mill.
Harold James Ruthven Murray calls version No. 1 "Nine Holes", and version No. 2 "Three Men's Morris" or "The Smaller Merels".
Six Men's Morris.
Six Men's Morris gives each player six pieces and is played without the outer square found on the board of Nine Men's Morris. Flying is not permitted. The game was popular in Italy, France and England during the Middle Ages but was obsolete by 1600.
This board is also used for Five Men's Morris (also called "Smaller Merels"). Seven Men's Morris uses this board with a cross in the center.
Twelve Men's Morris.
Twelve Men's Morris adds four diagonal lines to the board and gives each player twelve pieces. This means the board can be filled in the placement stage; if this happens the game is a draw. This variation on the game is popular amongst rural youth in South Africa where it is known as Morabaraba and is now recognized as a sport in that country. Harold James Ruthven Murray also calls the game "the larger merels".
This board is also used for Eleven Men's Morris.
Lasker Morris.
This variant (also called Ten Men's Morris) was invented by Emanuel Lasker, chess world champion from 1894 to 1921. It is based on the
rules of Nine Men’s Morris, but there are two changes: 1. Both players get ten pieces each, 2. Pieces can be moved in the first phase already. This means each player can choose to either place a new piece or to move one of his pieces already on the board. This variant is more complex than Nine Men's Morris and draws are less likely.
History.
According to R. C. Bell, the earliest known board for the game includes diagonal lines and was "cut into the roofing slabs of the temple at Kurna in Egypt" c. 1400 BCE. However, Friedrich Berger writes that some of the diagrams at Kurna include Coptic crosses, making it "doubtful" that the diagrams date to 1400 BCE. Berger concludes, "certainly they cannot be dated."
One of the earliest mentions of the game may be in Ovid's "Ars Amatoria". In book III (c. 8 CE), after discussing "Latrones", a popular board game, Ovid wrote:
Berger believes the game was "probably well known by the Romans", as there are many boards on Roman buildings, even though dating them is impossible because the buildings "have been easily accessible" since they were built. It is possible that the Romans were introduced to the game via trade routes, but this cannot be proven.
The game peaked in popularity in medieval England. Boards have been found carved into the cloister seats at the English cathedrals at Canterbury, Gloucester, Norwich, Salisbury and Westminster Abbey. These boards used holes, not lines, to represent the nine spaces on the board — hence the name "nine holes" — and forming a diagonal row did not win the game. Another board is carved into the base of a pillar in Chester Cathedral in Chester. Giant outdoor boards were sometimes cut into village greens. In Shakespeare's 16th century work "A Midsummer Night's Dream", Titania refers to such a board: "The nine men's morris is filled up with mud" ("A Midsummer Night's Dream", Act II, Scene I).
Some authors say the game's origin is uncertain. It has been speculated that its name may be related to Morris dances, and hence to Moorish, but according to Daniel King, "the word 'morris' has nothing to do with the old English dance of the same name. It comes from the Latin word "merellus", which means a counter or gaming piece." King also notes that the game was popular among Roman soldiers.
In some European countries, the design of the board was given special significance as a symbol of protection from evil, and "to the ancient Celts, the Morris Square was sacred: at the center lay the holy Mill or Cauldron, a symbol of regeneration; and emanating out from it, the four cardinal directions, the four elements and the four winds."
External links.
Variants

</doc>
<doc id="43473" url="http://en.wikipedia.org/wiki?curid=43473" title="Wars of Scottish Independence">
Wars of Scottish Independence

The Wars of Scottish Independence were a series of military campaigns fought between the Kingdom of Scotland and the Kingdom of England in the late 13th and early 14th centuries.
The First War (1296–1328) began with the English invasion of Scotland in 1296, and ended with the signing of the Treaty of Edinburgh-Northampton in 1328. The Second War (1332–1357) began with the English-supported invasion by Edward Balliol and the "Disinherited" in 1332, and ended in 1357 with the signing of the Treaty of Berwick. The wars were part of a great national crisis for Scotland and the period became one of the most defining times in the nation's history. At the end of both wars, Scotland retained its status as an independent nation. The wars were important for other reasons, such as the emergence of the longbow as a key weapon in medieval warfare.
The First War of Independence: 1296–1328.
Background.
King Alexander III of Scotland died in 1286, leaving his 3-year-old granddaughter Margaret (called "the Maid of Norway") as his heir. In 1290, the Guardians of Scotland signed the Treaty of Birgham agreeing to the marriage of the Maid of Norway and Edward of Caernarvon, the son of Edward I, who was Margaret's great-uncle. This marriage would not create a union between Scotland and England because the Scots insisted that the Treaty declare that Scotland was separate and divided from England and that its rights, laws, liberties and customs were wholly and inviolably preserved for all time.
However, Margaret, travelling to her new kingdom, died shortly after landing on the Orkney Islands around 26 September 1290. With her death, there were 13 rivals for succession. The two leading competitors for the Scottish crown were Robert Bruce, 5th Lord of Annandale (grandfather of the future King Robert the Bruce) and John Balliol, Lord of Galloway. Fearing civil war between the Bruce and Balliol families and supporters, the Guardians of Scotland wrote to Edward I of England, asking him to come north and arbitrate between the claimants in order to avoid civil war.
Edward agreed to meet the guardians at Norham in 1291. Before the process got underway Edward insisted that he be recognised as Lord Paramount of Scotland. When they refused, he gave the claimants three weeks to agree to his terms, knowing that by then his armies would have arrived and the Scots would have no choice. Edward's ploy worked, and the claimants to the crown were forced to acknowledge Edward as their Lord Paramount and accept his arbitration. Their decision was influenced in part by the fact that most of the claimants had large estates in England and, therefore, would have lost them if they had defied the English king. However, many involved were churchmen such as Bishop Wishart for whom such mitigation cannot be claimed.
On 11 June, acting as the Lord Paramount of Scotland, Edward I ordered that every Scottish royal castle be placed temporarily under his control and every Scottish official resign his office and be re-appointed by him. Two days later, in Upsettlington, the Guardians of the Realm and the leading Scottish nobles gathered to swear allegiance to King Edward I as Lord Paramount. All Scots were also required to pay homage to Edward I, either in person or at one of the designated centres by 27 July 1291.
There were thirteen meetings from May to August 1291 at Berwick, where the claimants to the crown pleaded their cases before Edward, in what came to be known as the "Great Cause". The claims of most of the competitors were rejected, leaving Balliol, Bruce, Floris V, Count of Holland and John de Hastings of Abergavenny, 2nd Baron Hastings, as the only men who could prove direct descent from David I.
On 3 August, Edward asked Balliol and Bruce to choose 40 arbiters each, while he chose 24, to decide the case. On 12 August, he signed a writ that required the collection of all documents that might concern the competitors' rights or his own title to the superiority of Scotland, which was accordingly executed. Balliol was named king by a majority on 17 November 1292 and on 30 November he was crowned King of Scots at Scone Abbey. On 26 December, at Newcastle upon Tyne, King John swore homage to Edward I for the Kingdom of Scotland. Edward soon made it clear that he regarded the country as a vassal state. Balliol, undermined by members of the Bruce faction, struggled to resist, and the Scots resented Edward's demands. In 1294, Edward summoned John Balliol to appear before him, and then ordered that he had until 1 September 1294 to provide Scottish troops and funds for his invasion of France.
On his return to Scotland, John held a meeting with his council and after a few days of heated debate, plans were made to defy the orders of Edward I. A few weeks later a Scottish parliament was hastily convened and 12 members of a war council (four Earls, Barons, and Bishops, respectively) were selected to advise King John.
Emissaries were immediately dispatched to inform King Philip IV of France of the intentions of the English. They also negotiated a treaty by which the Scots would invade England if the English invaded France, and in return the French would support the Scots. The treaty would be sealed by the arranged marriage of John's son Edward and Philip's niece Joan. Another treaty with King Eric II of Norway was hammered out, in which for the sum of 50,000 groats he would supply 100 ships for four months of the year, so long as hostilities between France and England continued. Although Norway never acted, the Franco-Scottish alliance, later known as the Auld Alliance, was renewed frequently until 1560.
It was not until 1295 that Edward I became aware of the secret Franco-Scottish negotiations. In early October, he began to strengthen his northern defences against a possible invasion. It was at this point that Robert Bruce, 6th Lord of Annandale (father of the future King Robert the Bruce) was appointed by Edward as the governor of Carlisle Castle. Edward also ordered John Balliol to relinquish control of the castles and burghs of Berwick, Jedburgh and Roxburgh. In December, more than 200 of Edward's tenants in Newcastle were summoned to form a militia by March 1296 and in February, a fleet sailed north to meet with his land forces in Newcastle.
The movement of English forces along the Anglo-Scottish border did not go unnoticed. In response, King John Balliol summoned all able-bodied Scotsmen to bear arms and gather at Caddonlee by 11 March. Several Scottish nobles chose to ignore the summons, including Robert Bruce, Earl of Carrick, whose Carrick estates had been seized by John Balliol and reassigned to John 'The Red' Comyn. Robert Bruce, Earl of Carrick had become Earl of Carrick at the resignation of his father earlier that year.
Beginning of the war: 1296–1306.
The First War of Scottish Independence can be loosely divided into four phases: the initial English invasion and success in 1296; the campaigns led by William Wallace, Andrew de Moray and various Scottish Guardians from 1297 until John Comyn negotiated for the general Scottish submission in February 1304; the renewed campaigns led by Robert the Bruce following his killing of The Red Comyn in Dumfries in 1306 to his and the Scottish victory at Bannockburn in 1314; and a final phase of Scottish diplomatic initiatives and military campaigns in Scotland, Ireland and Northern England from 1314 until the Treaty of Edinburgh-Northampton in 1328.
The war began in earnest with Edward I's brutal sacking of Berwick in March 1296, followed by the Scottish defeat at the Battle of Dunbar and the abdication of John Balliol in July. The English invasion campaign had subdued most of the country by August and, after removing the Stone of Destiny from Scone Abbey and transporting it to Westminster Abbey, Edward convened a parliament at Berwick, where the Scottish nobles paid homage to him as King of England. Scotland had been all but conquered.
The revolts which broke out in early 1297, led by William Wallace, Andrew de Moray and other Scottish nobles, forced Edward to send more forces to deal with the Scots, and although they managed to force the nobles to capitulate at Irvine, Wallace and de Moray's continuing campaigns eventually led to the first key Scottish victory, at Stirling Bridge. Moray was fatally wounded in the fighting at Stirling, and died soon after the battle. This was followed by Scottish raids into northern England and the appointment of Wallace as Guardian of Scotland in March 1298. But in July, Edward invaded again, intending to crush Wallace and his followers, and defeated the Scots at Falkirk. Edward failed to subdue Scotland completely before returning to England.
There have been, however, several stories regarding Wallace and what he did after the Battle of Falkirk. It is said by some sources that Wallace travelled to France and fought for the French King against the English during their own ongoing war while Bishop Lamberton of St Andrews, who gave much support to the Scottish cause, went and spoke to the pope.
Wallace was succeeded by Robert Bruce and John Comyn as joint guardians, with William de Lamberton, Bishop of St Andrews being appointed in 1299 as a third, neutral Guardian to try and maintain order between them. During that year, diplomatic pressure from France and Rome persuaded Edward to release the imprisoned King John into the custody of the pope, and Wallace was sent to France to seek the aid of Philip IV; he possibly also travelled to Rome.
Further campaigns by Edward in 1300 and 1301 led to a truce between the Scots and the English in 1302. After another campaign in 1303/1304, Stirling Castle, the last major Scottish held stronghold, fell to the English, and in February 1304, negotiations led to most of the remaining nobles paying homage to Edward and to the Scots all but surrendering. At this point, Robert Bruce and William Lamberton may have made a secret bond of alliance, aiming to place Bruce on the Scottish throne and continue the struggle. However, Lamberton came from a family associated with the Balliol-Comyn faction and his ultimate allegiances are unknown.
After the capture and execution of Wallace in 1305, Scotland seemed to have been finally conquered and the revolt calmed for a period.
King Robert the Bruce: 1306–1328.
On 10 February 1306, during a meeting between Bruce and Comyn, the two surviving claimants for the Scottish throne, Bruce quarrelled with and killed John Comyn at Greyfriars Kirk in Dumfries. At this moment the rebellion was sparked again.
Comyn, it seems, had broken an agreement between the two, and informed King Edward of Bruce's plans to be king. The agreement was that one of the two claimants would renounce his claim on the throne of Scotland, but receive lands from the other and support his claim. Comyn appears to have thought to get both the lands and the throne by betraying Bruce to the English. A messenger carrying documents from Comyn to Edward was captured by Bruce and his party, plainly implicating Comyn. Bruce then rallied the Scottish prelates and nobles behind him and had himself crowned King of Scots at Scone less than five weeks after the killing in Dumfries. He then began a new campaign to free his kingdom. After being defeated in battle he was driven from the Scottish mainland as an outlaw. Bruce later came out of hiding in 1307. The Scots thronged to him, and he defeated the English in a number of battles. His forces continued to grow in strength, encouraged in part by the death of Edward I in July 1307. The Battle of Bannockburn in 1314 was an especially important Scottish victory.
In 1320, the Declaration of Arbroath was sent by a group of Scottish nobles to the Pope affirming Scottish independence from England. Two similar declarations were also sent by the clergy and Robert I. In 1327, Edward II of England was deposed and killed. The invasion of the North of England by Robert the Bruce forced Edward III of England to sign the Treaty of Edinburgh-Northampton on 1 May 1328, which recognised the independence of Scotland with Bruce as King. To further seal the peace, Robert's son and heir David married the sister of Edward III.
The Second War of Independence: 1332–1357.
After Robert the Bruce's death, King David II was too young to rule, so the guardianship was assumed by Thomas Randolph, Earl of Moray. But Edward III, despite having given his name to the Treaty of Edinburgh-Northampton, was determined to avenge the humiliation by the Scots and he could count on the assistance of Edward Balliol, the son of John Balliol and a claimant to the Scottish throne.
Edward III also had the support of a group of Scottish nobles, led by Balliol and Henry Beaumont, known as the 'Disinherited.' This group of nobles had supported the English in the First War and, after Bannockburn, Robert the Bruce had given them a year to return to his peace. When they refused he deprived them of their titles and lands, granting them to his allies. When peace was concluded, they received no war reparations. These 'Disinherited' were hungry for their old lands and would prove to be the undoing of the peace.
The Earl of Moray died on 20 July 1332. The Scots nobility gathered at Perth where they elected Domhnall II, Earl of Mar as the new Guardian. Meanwhile a small band led by Balliol had set sail from the Humber. Consisting of the disinherited noblemen and mercenaries, they were probably no more than a few hundred men strong.
Edward III was still formally at peace with David II and his dealings with Balliol were therefore deliberately obscured. He of course knew what was happening and Balliol probably did homage in secret before leaving, but Balliol's desperate scheme must have seemed doomed to failure. Edward therefore refused to allow Balliol to invade Scotland from across the River Tweed. This would have been too open a breach of the treaty. He agreed to turn a blind eye to an invasion by sea, but made it clear that he would disavow them and confiscate all their English lands should Balliol and his friends fail.
The 'Disinherited' landed at Kinghorn in Fife on 6 August. The news of their advance had preceded them, and, as they marched towards Perth, they found their route barred by a large Scottish army, mostly of infantry, under the new Guardian.
At the Battle of Dupplin Moor, Balliol's army, commanded by Henry Beaumont, defeated the larger Scottish force. Beaumont made use of the same tactics that the English would make famous during the Hundred Years' War, with dismounted knights in the centre and archers on the flanks. Caught in the murderous rain of arrows, most of the Scots didn't reach the enemy's line. When the slaughter was finally over, the Earl of Mar, Sir Robert Bruce (an illegitimate son of Robert the Bruce), many nobles and around 2,000 Scots had been slain. Edward Balliol then had himself crowned King of Scots, first at Perth, and then again in September at Scone Abbey. Balliol's success surprised Edward III, and fearing that Balliol's invasion would eventually fail leading to a Scots invasion of England, he moved north with his army.
In October, Sir Archibald Douglas, now Guardian of Scotland, made a truce with Balliol, supposedly to let the Scottish Parliament assemble and decide who their true king was. Emboldened by the truce, Balliol dismissed most of his English troops and moved to Annan, on the north shore of the Solway Firth. He issued two public letters, saying that with the help of England he had reclaimed his kingdom, and acknowledged that Scotland had always been a fief of England. He also promised land for Edward III on the border, including Berwick-on-Tweed, and that he would serve Edward for the rest of his life. But in December, Douglas attacked Balliol at Annan in the early hours of the morning. Most of Balliol's men were killed, though he himself managed to escape through a hole in the wall, and fled, naked and on horse, to Carlisle.
In April 1333, Edward III and Balliol, with a large English army, laid siege to Berwick. Archibald Douglas attempted to relieve the town in July, but was defeated and killed at the Battle of Halidon Hill. David II and his Queen were moved to the safety of Dumbarton Castle, while Berwick surrendered and was annexed by Edward. By now, much of Scotland was under English occupation, with eight of the Scottish lowland counties being ceded to England by Edward Balliol.
At the beginning of 1334, Philip VI of France offered to bring David II and his court to France for asylum, and in May they arrived in France, setting up a court-in-exile at Château Gaillard in Normandy. Philip also decided to derail the Anglo-French peace negotiations then taking place (at the time England and France were engaged in disputes that would lead to the Hundred Years' War), declaring to Edward III that any treaty between France and England must include the exiled King of Scots.
In David's absence, a series of Guardians kept up the struggle. In November, Edward III invaded again, but he accomplished little and retreated in February 1335 due primarily to his failure to bring the Scots to battle. He and Edward Balliol returned again in July with an army of 13,000, and advanced through Scotland, first to Glasgow and then to Perth, where Edward III installed himself while his army looted and destroyed the surrounding countryside. At this time, the Scots followed a plan of avoiding pitched battles, depending instead on minor actions of heavy cavalry – the normal practice of the day. Some Scottish leaders, including the Earl of Atholl, who had returned to Scotland with Edward Balliol in 1332 and 1333, defected to the Bruce party.
Following Edward's return to England, the remaining leaders of the Scots resistance chose Sir Andrew Murray as Guardian. He soon negotiated a truce with Edward until April 1336, during which, various French and Papal emissaries attempted to negotiate a peace between the two countries. In January, the Scots drew up a draft treaty agreeing to recognise the elderly and childless Edward Balliol as King, so long as David II would be his heir and David would leave France to live in England. However, David II rejected the peace proposal and any further truces. In May, an English army under Henry of Lancaster invaded, followed in July by another army under King Edward. Together, they ravaged much of the north-east and sacked Elgin and Aberdeen, while a third army ravaged the south-west and the Clyde valley. Prompted by this invasion, Philip VI of France announced that he intended to aid the Scots by every means in his power, and that he had a large fleet and army preparing to invade both England and Scotland. Edward soon returned to England, while the Scots, under Murray, captured and destroyed English strongholds and ravaged the countryside, making it uninhabitable for the English.
Although Edward III invaded again, he was becoming more anxious over the possible French invasion, and by late 1336, the Scots had regained control over virtually all of Scotland and by 1338 the tide had turned. While "Black Agnes," Countess-consort Dunbar and March, continued to resist the English laying siege to Dunbar Castle, hurling defiance and abuse from the walls, Scotland received some breathing space when Edward III claimed the French throne and took his army to Flanders, beginning the Hundred Years' War with France.
In the late autumn of 1335, Strathbogie, dispossessed Earl of Atholl, and Edward III set out to destroy Scottish resistance by dispossessing and killing the Scottish freeholders. Following this, Strathbogie moved to lay siege to Kildrummy Castle, held by Lady Christian Bruce, sister of the late King Robert and wife of the Guardian, Andrew de Moray. Her husband moved his small army quickly to her relief although outnumbered by some five to one. However, many of Strathbogie's men had been impressed and had no loyalty to the English or the usurper, Balliol. Pinned by a flank attack while making a downhill charge, Strathbogie's army broke and Strathbogie refused to surrender and was killed. The Battle of Culblean was the effective end of Balliol's attempt to overthrow the King of Scots.
So, in just nine years, the kingdom so hard won by Robert the Bruce had been shattered and had recovered. Many of her experienced nobles were dead and the economy which had barely begun to recover from the earlier wars was once again in tatters. It was to an impoverished country in need of peace and good government that David II was finally able to return in June 1341.
When David returned, he was determined to live up to the memory of his illustrious father. He ignored truces with England and was determined to stand by his ally Philip VI during the early years of the Hundred Years' War. In 1341 he led a raid into England, forcing Edward III to lead an army north to reinforce the border. In 1346, after more Scottish raids, Philip VI appealed for a counter invasion of England in order to relieve the English stranglehold on Calais. David gladly accepted and personally led a Scots army southwards with intention of capturing Durham. In reply, an English army moved northwards from Yorkshire to confront the Scots. On 14 October, at the Battle of Neville's Cross, the Scots were defeated. They suffered heavy casualties and David was wounded in the face by two arrows before being captured. He was sufficiently strong however to knock out two teeth from the mouth of his captor. After a period of convalescence, he was imprisoned in the Tower of London, where he was held prisoner for eleven years, during which time Scotland was ruled by his nephew, Robert Stewart, 7th High Steward. Edward Balliol returned to Scotland soon afterwards with a small force, in a final attempt to recover Scotland. He only succeeded in gaining control of some of Galloway, with his power diminishing there until 1355. He finally resigned his claim to the Scottish throne in January 1356 and died childless in 1364.
Finally, on 3 October 1357, David was released under the Treaty of Berwick, under which the Scots agreed to pay an enormous ransom of 100,000 merks for him (1 merk was ⅔ of an English pound) payable in 10 years. Heavy taxation was needed to provide funds for the ransom, which was to be paid in instalments, and David alienated his subjects by using the money for his own purposes. The country was in a sorry state then; she had been ravaged by war and also the Black Death. The first instalment of the ransom was paid punctually. The second was late and after that no more could be paid for.
In 1363, David went to London and agreed that should he die childless, the crown would pass to Edward (his brother-in-law) or one of his sons, with the Stone of Destiny being returned for their coronation as King of Scots. However, this seems to have been no more than a rather dishonest attempt to re-negotiate the ransom since David knew perfectly well that Parliament would reject such an arrangement out of hand. The Scots did reject this arrangement, and offered to continue paying the ransom (now increased to 100,000 pounds). A 25-year truce was agreed and in 1369, the treaty of 1365 was cancelled and a new one set up to the Scots benefit, due to the influence of the war with France. The new terms saw the 44,000 merks already paid deducted from the original 100,000 with the balance due in instalments of 4,000 for the next 14 years.
When Edward died in 1377, there were still 24,000 merks owed which were never paid. David himself had lost his popularity and the respect of his nobles when he married the widow of a minor laird after the death of his English wife. He himself died in February 1371.
By the end of the campaign, Scotland was independent and remained thus, until the unification of the Kingdom of England and the Kingdom of Scotland to create the single Kingdom of Great Britain was completed in the Treaty of Union of 1707.
Major battles and events.
</dl>
Important figures.
Scotland.
</dl>
England.
</dl>
Other important figures.
</dl>
References.
Notes
Citations

</doc>
<doc id="43474" url="http://en.wikipedia.org/wiki?curid=43474" title="William Lamb, 2nd Viscount Melbourne">
William Lamb, 2nd Viscount Melbourne

William Lamb, 2nd Viscount Melbourne, usually referred to as Lord Melbourne, PC, FRS (15 March 1779 – 24 November 1848) was a British Whig statesman who served as Home Secretary (1830–1834) and Prime Minister (1834 and 1835–1841). He is best known for his intense and successful mentoring of Queen Victoria, at ages 18–21, in the ways of politics. Historians conclude that Melbourne does not rank high as a prime minister, for there were no great foreign wars or domestic issues to handle, he lacked major achievements, and he enunciated no grand principles. "But he was kind, honest, and not self-seeking." Melbourne was dismissed by the King in 1834, the last Prime Minister to have been dismissed by a Monarch.
Early life.
Born in London to an aristocratic Whig family, son of Sir Penniston Lamb and Elizabeth Milbanke Lamb, Viscountess Melbourne (1751–1818), though his paternity was questioned. He was educated at Eton and Trinity College, Cambridge, he fell in with a group of Romantic Radicals that included Percy Bysshe Shelley and Lord Byron. In 1805 he succeeded his elder brother as heir to his father's title and he married Lady Caroline Ponsonby. The next year he was elected to the British House of Commons as the Whig MP for Leominster. For the election in 1806 he was moved to the seat of Haddington burghs and for the 1807 election successfully stood for Portarlington (a seat he held until 1812).
He first came to general notice for reasons he would rather have avoided: his wife had a public affair with Lord Byron—she coined the famous characterisation of him as "mad, bad, and dangerous to know". The resulting scandal was the talk of Britain in 1812. In 1816 Lady Caroline published a Gothic novel "Glenarvon", which portrayed both the marriage and her affair with Byron in a lurid fashion which caused William even greater embarrassment, while the spiteful caricatures of leading society figures made them several influential enemies. Eventually the two reconciled and though they separated in 1825, her death in 1828 affected him considerably.
In 1816 Lamb was returned for Peterborough by Whig grandee Lord Fitzwilliam. He told Lord Holland that he was committed to the Whig principles of the Glorious Revolution but not to "a heap of modern additions, interpolations, facts and fictions". He therefore spoke against parliamentary reform and voted for the suspension of "habeas corpus" in 1817 when sedition was rife.
Lamb's hallmark was finding the middle ground. Though a Whig, he accepted (29 April 1827) the post of Chief Secretary for Ireland in the moderate Tory governments of George Canning and Lord Goderich. Upon the death of his father in 1828 and his becoming Viscount Melbourne, he moved to the House of Lords. He had spent 25 years in Commons as a backbencher and politically was not well known.
Home Secretary: 1830–1834.
When the Whigs came to power under Lord Grey in November 1830 he became Home Secretary in the new government. During the disturbances of 1830–32 Melbourne "acted both vigorously and sensitively, and it was for this function that his reforming brethren thanked him heartily". In the aftermath of the Swing Riots of 1830–31 he countered the Tory magistrates' alarmism by refusing to resort to military force and instead he advocated magistrates' usual powers be fully enforced along with special constables and financial rewards for the arrest of rioters and rabble-rousers. He appointed a special commission to try approximately one thousand of those arrested and ensured that justice was strictly adhered to: one third were acquitted; and most of the one-fifth sentenced to death were instead transported. The disturbances over reform in 1831–32 were countered with the enforcement of the usual laws and again Melbourne refused to pass emergency legislation against sedition.
Prime Minister: 1834, 1835–1841.
After Lord Grey resigned as Prime Minister in July 1834, the King was forced to appoint another Whig to replace him, as the Tories were not strong enough to support a government. Melbourne was the man most likely to be both acceptable to the King and hold the Whig party together. Melbourne hesitated after receiving from Grey the letter from the King requesting him to visit him to discuss the formation of a government. Melbourne thought he would not enjoy the extra work that accompanied the office of Premier but he did not want to let his friends and party down. According to Charles Greville, Melbourne said to his secretary, Tom Young: "I think it's a damned bore. I am in many minds as to what to do". Young replied: "Why, damn it all, such a position was never held by any Greek or Roman: and if it only lasts three months, it will be worth while to have been Prime Minister of England ["sic"]. "By God, that's true," Melbourne said, "I'll go!"
Compromise was the key to many of Melbourne's actions. As an aristocrat, he had a vested interest in the status quo. He was opposed to the Reform Act 1832 proposed by the Whigs, arguing that Catholic emancipation had not ended in the tranquility expected of it, but reluctantly agreed that they were necessary to forestall the threat of revolution. Later he opposed the repeal of the Corn Laws arguing not only had Catholic emancipation failed, but also that the reform bill had not improved the condition of the people. While he was less radical than many, when Lord Grey resigned (July 1834), Melbourne was widely seen as the most acceptable replacement among the Whig leaders, and became Prime Minister.
King William IV's opposition to the Whigs' reforming ways led him to dismiss Melbourne in November. He then gave the Tories under Sir Robert Peel an opportunity to form a government. Peel's failure to win a House of Commons majority in the resulting general election (January 1835) made it impossible for him to govern, and the Whigs returned to power under Melbourne in April 1835. This was the last time a British monarch attempted to appoint a government against parliamentary majority.
Blackmailed.
The next year, Melbourne was once again involved in a sex scandal. This time he was the victim of attempted blackmail from the husband of a close friend, society beauty and author Caroline Norton. The husband demanded £1400, and when he was turned down he accused Melbourne of having an affair with his wife. At this time such a scandal would be enough to derail a major politician, so it is a measure of the respect contemporaries had for his integrity that Melbourne's government did not fall. The king and the Duke of Wellington urged him to stay on as prime minister. After Norton failed in court, Melbourne was vindicated, but he did stop seeing Mrs Norton.
Nonetheless, as historian Boyd Hilton concludes, "it is irrefutable that Melbourne's personal life was problematic. Spanking sessions with aristocratic ladies were harmless, not so the whippings administered to orphan girls taken into his household as objects of charity."
Queen Victoria.
Melbourne was Prime Minister when Queen Victoria came to the throne (June 1837). Barely eighteen, she was only just breaking free from the domineering influence of her mother, the Duchess of Kent, and her mother's advisor, Sir John Conroy. Over the next four years Melbourne trained her in the art of politics and the two became friends: Victoria was quoted as saying she considered him like a father (her own had died when she was only eight months old), and Melbourne's daughter had died at a young age. Melbourne was given a private apartment at Windsor Castle, and unfounded rumours circulated for a time that Victoria would marry Melbourne, forty years her senior. Tutoring Victoria was the climax of Melbourne's career—the prime minister spent four to five hours a day visiting and writing to her, and she responded with enthusiasm, and grew in wisdom.
Continued rule.
In May 1839, Melbourne's resignation led to the Bedchamber Crisis. Prospective prime minister Robert Peel requested that Victoria dismiss some of the wives and daughters of Whig MPs who made up her personal entourage, arguing that the monarch should avoid any hint of favouritism to a party out of power. As the Queen refused to comply, supported by Melbourne although unaware that Peel had not requested the resignation of all the Queen's ladies as she had led him to believe, Peel refused to form a new government and Melbourne was persuaded to stay on as Prime Minister.
Melbourne left a considerable list of reforming legislation—not as long as that of Lord Grey, but worthy nonetheless. Among his administration's acts were a reduction in the number of capital offences, reforms of local government, and the reform of the Poor laws. This restricted the terms on which the poor were allowed relief and established compulsory admission to workhouses for the impoverished.
On 25 February 1841, he was admitted a Fellow of the Royal Society.
Later life (1841–1848).
Even after Melbourne resigned permanently in August 1841, Victoria continued writing to him but eventually the correspondence ceased as it was seen as inappropriate. Melbourne's role faded away as Victoria came to rely on her new husband Prince Albert as well as on herself.
On his death his titles passed to his brother Frederick, as both of his children—a son, George Augustus Frederick (b. 1807) and a premature daughter (b. 1809)—had predeceased him.
Legacy.
The city of Melbourne, Victoria, Australia, was named in his honour in March 1837, as he was the Prime Minister of Great Britain at the time.
Another lasting memorial is his favourite, and most famous, dictum in politics: "Why not leave it alone?", quoted by those who object to change for change's sake.

</doc>
<doc id="43475" url="http://en.wikipedia.org/wiki?curid=43475" title="Mutt">
Mutt

A mutt is a mongrel (a dog of unknown ancestry).
Mutt may also refer to:

</doc>
<doc id="43476" url="http://en.wikipedia.org/wiki?curid=43476" title="Operations research">
Operations research

Operations research, or operational research in British usage, is a discipline that deals with the application of advanced analytical methods to help make better decisions. It is often considered to be a sub-field of mathematics. The terms management science and decision science are sometimes used as synonyms.
Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost) of some real-world objective. Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.
Overview.
Operational research (OR) encompasses a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system. Because of the computational and statistical nature of most of these fields, OR also has strong ties to computer science and analytics. Operational researchers faced with a new problem must determine which of these techniques are most appropriate given the nature of the system, the goals for improvement, and constraints on time and computing power.
The major subdisciplines in modern operational research, as identified by the journal "Operations Research", are:
History.
As a formal discipline, operational research originated in the efforts of military planners during World War II. In the decades after the war, the techniques were more widely applied to problems in business, industry and society. Since that time, operational research has expanded into a field widely used in industries ranging from petrochemicals to airlines, finance, logistics, and government, moving to a focus on the development of mathematical models that can be used to analyse and optimize complex systems, and has become an area of active academic and industrial research.
Historical origins.
Early work in operational research was carried out by individuals such as Charles Babbage. His research into the cost of transportation and sorting of mail led to England's universal "Penny Post" in 1840, and studies into the dynamical behaviour of railway vehicles in defence of the GWR's broad gauge. Percy Bridgman brought operational research to bear on problems in physics in the 1920s and would later attempt to extend these to the social sciences.
Modern operational research originated at the Bawdsey Research Station in the UK in 1937 and was the result of an initiative of the station's superintendent, A. P. Rowe. Rowe conceived the idea as a means to analyse and improve the working of the UK's early warning radar system, Chain Home (CH). Initially, he analysed the operating of the radar equipment and its communication networks, expanding later to include the operating personnel's behaviour. This revealed unappreciated limitations of the CH network and allowed remedial action to be taken.
Scientists in the United Kingdom including Patrick Blackett (later Lord Blackett OM PRS), Cecil Gordon, Solly Zuckerman, (later Baron Zuckerman OM, KCB, FRS), C. H. Waddington, Owen Wansbrough-Jones, Frank Yates, Jacob Bronowski and Freeman Dyson, and in the United States with George Dantzig looked for ways to make better decisions in such areas as logistics and training schedules.
Second World War.
The modern field of operational research arose during World War II. In the World War II era, operational research was defined as "a scientific method of providing executive departments with a quantitative basis for decisions regarding the operations under their control." Other names for it included operational analysis (UK Ministry of Defence from 1962) and quantitative management.
During the Second World War close to 1,000 men and women in Britain were engaged in operational research. About 200 operational research scientists worked for the British Army.
Patrick Blackett worked for several different organizations during the war. Early in the war while working for the Royal Aircraft Establishment (RAE) he set up a team known as the "Circus" which helped to reduce the number of anti-aircraft artillery rounds needed to shoot down an enemy aircraft from an average of over 20,000 at the start of the Battle of Britain to 4,000 in 1941.
In 1941 Blackett moved from the RAE to the Navy, after first working with RAF Coastal Command, in 1941 and then early in 1942 to the Admiralty. Blackett's team at Coastal Command's Operational Research Section (CC-ORS) included two future Nobel prize winners and many other people who went on to be pre-eminent in their fields. They undertook a number of crucial analyses that aided the war effort. Britain introduced the convoy system to reduce shipping losses, but while the principle of using warships to accompany merchant ships was generally accepted, it was unclear whether it was better for convoys to be small or large. Convoys travel at the speed of the slowest member, so small convoys can travel faster. It was also argued that small convoys would be harder for German U-boats to detect. On the other hand, large convoys could deploy more warships against an attacker. Blackett's staff showed that the losses suffered by convoys depended largely on the number of escort vessels present, rather than the size of the convoy. Their conclusion was that a few large convoys are more defensible than many small ones.
While performing an analysis of the methods used by RAF Coastal Command to hunt and destroy submarines, one of the analysts asked what colour the aircraft were. As most of them were from Bomber Command they were painted black for night-time operations. At the suggestion of CC-ORS a test was run to see if that was the best colour to camouflage the aircraft for daytime operations in the grey North Atlantic skies. Tests showed that aircraft painted white were on average not spotted until they were 20% closer than those painted black. This change indicated that 30% more submarines would be attacked and sunk for the same number of sightings. As a result of these findings Coastal Command changed their aircraft to using white undersurfaces.
Other work by the CC-ORS indicated that on average if the trigger depth of aerial-delivered depth charges (DCs) were changed from 100 feet to 25 feet, the kill ratios would go up. The reason was that if a U-boat saw an aircraft only shortly before it arrived over the target then at 100 feet the charges would do no damage (because the U-boat wouldn't have had time to descend as far as 100 feet), and if it saw the aircraft a long way from the target it had time to alter course under water so the chances of it being within the 20-foot kill zone of the charges was small. It was more efficient to attack those submarines close to the surface when the targets' locations were better known than to attempt their destruction at greater depths when their positions could only be guessed. Before the change of settings from 100 feet to 25 feet, 1% of submerged U-boats were sunk and 14% damaged. After the change, 7% were sunk and 11% damaged. (If submarines were caught on the surface, even if attacked shortly after submerging, the numbers rose to 11% sunk and 15% damaged). Blackett observed "there can be few cases where such a great operational gain had been obtained by such a small and simple change of tactics".
Bomber Command's Operational Research Section (BC-ORS), analysed a report of a survey carried out by RAF Bomber Command. For the survey, Bomber Command inspected all bombers returning from bombing raids over Germany over a particular period. All damage inflicted by German air defences was noted and the recommendation was given that armour be added in the most heavily damaged areas. This recommendation was not adopted because the fact that the aircraft returned with these areas damaged indicated these areas were NOT vital, and adding armour to non-vital areas where damage is acceptable negatively affects aircraft performance. Their suggestion to remove some of the crew so that an aircraft loss would result in fewer personnel losses, was also rejected by RAF command. Blackett's team made the logical recommendation that the armour be placed in the areas which were completely untouched by damage in the bombers which returned. They reasoned that the survey was biased, since it only included aircraft that returned to Britain. The untouched areas of returning aircraft were probably vital areas, which, if hit, would result in the loss of the aircraft.
When Germany organised its air defences into the Kammhuber Line, it was realised by the British that if the RAF bombers were to fly in a bomber stream they could overwhelm the night fighters who flew in individual cells directed to their targets by ground controllers. It was then a matter of calculating the statistical loss from collisions against the statistical loss from night fighters to calculate how close the bombers should fly to minimise RAF losses.
The "exchange rate" ratio of output to input was a characteristic feature of operational research. By comparing the number of flying hours put in by Allied aircraft to the number of U-boat sightings in a given area, it was possible to redistribute aircraft to more productive patrol areas. Comparison of exchange rates established "effectiveness ratios" useful in planning. The ratio of 60 mines laid per ship sunk was common to several campaigns: German mines in British ports, British mines on German routes, and United States mines in Japanese routes.
Operational research doubled the on-target bomb rate of B-29s bombing Japan from the Marianas Islands by increasing the training ratio from 4 to 10 percent of flying hours; revealed that wolf-packs of three United States submarines were the most effective number to enable all members of the pack to engage targets discovered on their individual patrol stations; revealed that glossy enamel paint was more effective camouflage for night fighters than traditional dull camouflage paint finish, and the smooth paint finish increased airspeed by reducing skin friction.
On land, the operational research sections of the Army Operational Research Group (AORG) of the Ministry of Supply (MoS) were landed in Normandy in 1944, and they followed British forces in the advance across Europe. They analysed, among other topics, the effectiveness of artillery, aerial bombing and anti-tank shooting.
After World War II.
With expanded techniques and growing awareness of the field at the close of the war, operational research was no longer limited to only operational, but was extended to encompass equipment procurement, training, logistics and infrastructure. Operations Research also grew in many areas other than the military once scientists learned to apply its principles to the civilian sector. With the development of the simplex algorithm for Linear Programming in 1947 and the development of computers over the next three decades, Operations Research can now “solve problems with hundreds of thousands of variables and constraints. Moreover, the large volumes of data required for such problems can be stored and manipulated very efficiently.” 
Problems addressed.
Operational research is also used extensively in government where evidence-based policy is used.
Management science.
In 1967 Stafford Beer characterized the field of management science as "the business use of operations research". However, in modern times the term management science may also be used to refer to the separate fields of organizational studies or corporate strategy. Like operational research itself, management science (MS) is an interdisciplinary branch of applied mathematics devoted to optimal decision planning, with strong links with economics, business, engineering, and other sciences. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and meaningful management decisions by arriving at optimal or near optimal solutions to complex decision problems. In short, management sciences help businesses to achieve their goals using the scientific methods of operational research.
The management scientist's mandate is to use rational, systematic, science-based techniques to inform and improve decisions of all kinds. Of course, the techniques of management science are not restricted to business applications but may be applied to military, medical, public administration, charitable groups, political groups or community groups.
Management science is concerned with developing and applying models and concepts that may prove useful in helping to illuminate management issues and solve managerial problems, as well as designing and developing new and better models of organizational excellence.
The application of these models within the corporate sector became known as management science.
Related fields.
Some of the fields that have considerable overlap with Operations Research and Management Science include:
Applications.
Applications of management science is abundant in industry as airlines, manufacturing companies, service organizations, military branches, and in government. The range of problems and issues to which management science has contributed insights and solutions is vast. It includes:
Management science is also concerned with so-called ”soft-operational analysis”, which concerns methods for strategic planning, strategic decision support, and Problem Structuring Methods (PSM). In dealing with these sorts of challenges mathematical modeling and simulation are not appropriate or will not suffice. Therefore, during the past 30 years, a number of non-quantified modeling methods have been developed. These include:
Societies and journals.
The International Federation of Operational Research Societies (IFORS) is an umbrella organization for operational research societies worldwide, representing approximately 50 national societies including those in the US, UK, France, Germany, Canada, Australia, New Zealand, Philippines, India, Japan and South Africa (ORSSA). The constituent members of IFORS form regional groups, such as that in Europe. Other important operational research organizations are Simulation Interoperability Standards Organization (SISO) and Interservice/Industry Training, Simulation and Education Conference (I/ITSEC)
In 2004 the US-based organization INFORMS began an initiative to market the OR profession better, including a website entitled "The Science of Better" which provides an introduction to OR and examples of successful applications of OR to industrial problems. This initiative has been adopted by the Operational Research Society in the UK, including a website entitled "Learn about OR".
The Institute for Operations Research and the Management Sciences (INFORMS) publishes thirteen scholarly journals about operations research, including the top two journals in their class, according to 2005 Journal Citation Reports. They are:

</doc>
<doc id="43478" url="http://en.wikipedia.org/wiki?curid=43478" title="Email client">
Email client

An email client, email reader or more formally mail user agent (MUA) is a computer program used to access and manage a user's email.
Popular locally installed email clients include Microsoft Outlook, Windows Live Mail, IBM Lotus Notes, Pegasus Mail, Mozilla's Thunderbird, The Bat!, Eudora, KMail in the Kontact suite, Evolution and Apple Mail.
A web application that provides message management, composition, and reception functions is sometimes also considered an email client, but more commonly referred to as webmail. Popular web-based email clients include Gmail, Lycos Mail, Mail.com, Outlook.com and Yahoo! Mail.
Retrieving messages from a mailbox.
Like most client programs, an email client is only active when a user runs it. The most common arrangement is for an email user (the client) to make an arrangement with a remote Mail Transfer Agent (MTA) server for the receipt and storage of the client's emails. The MTA, using a suitable mail delivery agent (MDA), adds email messages to a client's storage as they arrive. The remote mail storage is referred to as the user's mailbox. The default setting on many Unix systems is for the mail server to store formatted messages in mbox, within the user's HOME directory. Of course, users of the system can log-in and run a mail client on the same computer that hosts their mailboxes; in which case, the server is not actually "remote", other than in a generic sense.
Emails are stored in the user's mailbox on the remote server until the user's email client requests them to be downloaded to the user's computer, or can otherwise access the user's mailbox on the possibly remote server. The email client can be set up to connect to multiple mailboxes at the same time and to request the download of emails either automatically, such as at pre-set intervals, or the request can be manually initiated by the user.
A user's mailbox can be accessed in two dedicated ways. The Post Office Protocol (POP) allows the user to download messages one at a time and only deletes them from the server after they have been successfully saved on local storage. It is possible to leave messages on the server to permit another client to access them. However, there is no provision for flagging a specific message as "seen", "answered", or "forwarded", thus POP is not convenient for users who access the same mail from different machines.
Alternatively, the Internet Message Access Protocol (IMAP) allows users to keep messages on the server, flagging them as appropriate. IMAP provides folders and sub-folders, which can be shared among different users with possibly different access rights. Typically, the "Sent", "Drafts", and "Trash" folders are created by default. IMAP features an "idle" extension for real time updates, providing faster notification than polling, where long lasting connections are feasible. See also the remote messages section below.
In addition, the mailbox storage can be accessed directly by programs running on the server or via shared disks. Direct access can be more efficient but is less portable as it depends on the mailbox format; it is used by some email clients, including some webmail applications.
Message composition.
Email clients usually contain user interfaces to display and edit text. Some applications permit the use of a program-external editor.
The email clients will perform formatting according to RFC 5322 for headers and body, and MIME for non-textual content and attachments. Headers include the destination fields, "To", "Cc", and "Bcc", and the originator fields "From" which is the message's author(s), "Sender" in case there are more authors, and "Reply-To" in case responses should be addressed to a different mailbox. To better assist the user with destination fields, many clients maintain one or more address books and/or are able to connect to an LDAP directory server. For originator fields, clients may support different identities.
Client settings require the user's "real name" and "email address" for each user's identity, and possibly a list of LDAP servers.
Submitting messages to a server.
When a user wishes to create and send an email, the email client will handle the task. The email client is usually set up automatically to connect to the user's mail server, which is typically either an MSA or an MTA, two variations of the SMTP protocol. The email client which uses the SMTP protocol creates an authentication extension, which the mail server uses to authenticate the sender. This method eases modularity and nomadic computing. The older method was for the mail server to recognize the client's IP address, e.g. because the client is on the same machine and uses internal address 127.0.0.1, or because the client's IP address is controlled by the same internet service provider that provides both internet access and mail services.
Client settings require the name or IP address of the preferred "outgoing mail server", the "port number" (25 for MTA, 587 for MSA), and the "user name" and "password" for the authentication, if any. There is a non-standard port 465 for SSL encrypted SMTP sessions, that many clients and servers support for backward compatibility.
Encryption.
With no encryption, much like for postcards, email activity is plainly visible by any occasional eavesdropper. Email encryption enables privacy to be safeguarded by encrypting the mail sessions, the body of the message, or both. Without it, anyone with network access and the right tools can monitor email and obtain login passwords. Examples of concern include the government censorship and surveillance and fellow wireless network users such as at an Internet cafe.
Encryption of mail sessions.
All relevant email protocols have an option to encrypt the whole session, to prevent a user's "name" and "password" from being sniffed. They are strongly suggested for nomadic users and whenever the internet access provider is not trusted. When sending mail, users can only control encryption at the first hop from a client to its configured "outgoing mail server". At any further hop, messages may be transmitted with or without encryption, depending solely on the general configuration of the transmitting server and the capabilities of the receiving one.
Encrypted mail sessions deliver messages in their original format, i.e. plain text or encrypted body, on a user's local mailbox and on the destination server's. The latter server is operated by an email hosting service provider, possibly a different entity than the internet "access" provider currently at hand.
Encryption of the message body.
There are two models for managing cryptographic keys. S/MIME employs a model based on a trusted certificate authority (CA) that signs users' public keys. OpenPGP employs a somewhat more flexible "web of trust" mechanism that allows users to sign one another's public keys. OpenPGP is also more flexible in the format of the messages, in that it still supports plain message encryption and signing as they used to work before MIME standardization.
In both cases, only the message body is encrypted. Header fields, including originator, recipients, and subject, remain in plain text.
Webmail.
In addition to the fat client email clients and small email clients, there are also Web-based email applications called webmail. Webmail has several advantages, including an ability to send and receive email away from the user's normal base using a web browser, thus eliminating the need for an email client.
Some websites are dedicated to providing email services, including AOL, Gmail, Outlook.com and Yahoo; but there are many internet service providers which provide webmail services as part of their internet service package. The main limitations of webmail are that user interactions are subject to the website's operating system and the general inability to download email messages and compose or work on the messages offline, although Gmail does offer Offline Gmail through the installation of a Google Chrome extension and there are also other tools to integrate parts of the webmail functionality into the OS (e.g. creating messages directly from third party applications via MAPI).
Like MAPI, webmail provides for email messages to remain on the mail server. See next section.
Remote messages.
POP3 has an option to leave messages on the server, but it doesn't work very well. By contrast, both IMAP and webmail provide for keeping messages on the server as their method of operating, albeit users can make local copies as they like. Keeping messages on the server has advantages and disadvantages.
Protocols.
While popular protocols for retrieving mail include POP3 and IMAP4, sending mail is usually done using the SMTP protocol.
Another important standard supported by most email clients is MIME, which is used to send binary file email attachments. Attachments are files that are not part of the email proper, but are sent with the email.
Most email clients use a "User-Agent" header field to identify the software used to send the message. According to RFC 2076, this is a common but non-standard header field.
RFC 6409, "Message Submission for Mail", details the role of the Mail submission agent.
RFC 5068, "Email Submission Operations: Access and Accountability Requirements", provides a survey of the concepts of MTA, MSA, MDA, and MUA. It mentions that ""Access Providers MUST NOT block users from accessing the external Internet using the SUBMISSION port 587" and that "MUAs SHOULD use the SUBMISSION port for message submission.""
Port numbers.
Email servers and clients by convention use the TCP port numbers in the following table. For MSA, IMAP and POP3, the table reports also the labels that a client can use to query the SRV records and discover both the host name and the port number of the corresponding service.
Note that while webmail obeys the earlier HTTP disposition of having separate ports for encrypt and plain text sessions, mail protocols use the STARTTLS technique, thereby allowing encryption to start on an already established TCP connection. RFC 2595 discourages the use of the previously established ports 995 and 993 and promotes the use of a single port for POP, IMAP and SMTP while using TLS when available.
Proprietary client protocols.
Microsoft mail systems define the proprietary Messaging Application Programming Interface (MAPI) that is used in client applications, such as Microsoft Outlook, to access Microsoft Exchange electronic mail servers.
History.
See Partridge for early history of email clients. Email clients pre-date networks, with early email providing only local delivery on mainframe computers in the 1960s. In 1975, Sixth Edition Unix for minicomputers was released, featuring the mbox format for storing mail.

</doc>
<doc id="43480" url="http://en.wikipedia.org/wiki?curid=43480" title="Montgomery County">
Montgomery County

Montgomery County may refer to:

</doc>
<doc id="43481" url="http://en.wikipedia.org/wiki?curid=43481" title="Pacifist (disambiguation)">
Pacifist (disambiguation)

Pacifist may mean:

</doc>
<doc id="43482" url="http://en.wikipedia.org/wiki?curid=43482" title="Montgomery County, Maryland">
Montgomery County, Maryland

Montgomery County is a county in the U.S. state of Maryland. As of the 2010 census, the population was 971,777, and a 2013 estimate put the population at 1,016,677. It is the most populous county in Maryland. The county seat and largest municipality is Rockville, although the census-designated place of Germantown is the most populous place.
Montgomery County is included in the Washington-Arlington-Alexandria, DC-VA-MD-WV Metropolitan Statistical Area. Most of the county's residents live in unincorporated locales, of which the most built up are Silver Spring, and Bethesda, though the incorporated cities of Rockville and Gaithersburg are also large population centers as are many smaller but significant places. 
As one of the most affluent counties in the United States, it also has the highest percentage (29.2%) of residents over 25 years of age who hold post-graduate degrees. In 2011, it was ranked by Forbes as the 10th richest in the United States, with a median household income of $92,213.
Montgomery County, like other inter Washington D.C. suburban counties, contain many major U.S. government offices, scientific research and learning centers, and business campuses.
Etymology.
The Maryland state legislature named Montgomery County after Richard Montgomery; the county was created from lands that had at one point or another been part of Frederick County. On September 6, 1776, Thomas Sprigg Wootton from Rockville, Maryland, introduced legislation, while serving at the Maryland Constitutional Convention, to create lower Frederick County as Montgomery County. The name, Montgomery County, along with the founding of Washington County, Maryland, after George Washington, was the first time in American history that names identifying counties and provinces in the thirteen colonies would be relegated to British reference. The name use of Montgomery and Washington County were seen as further defiance to Great Britain during the American Revolutionary War. The county's nickname of "MoCo" is derived from a portmanteau of "Montgomery" and "County".
The county's motto, adopted in 1976, is "Gardez Bien", a phrase meaning "Watch Well". The county's motto is also the motto of its namesake's family.
History.
Early history.
Before European immigration, the land now known as Montgomery County was covered in a vast swath of forest crossed by the creeks and small streams that feed the Potomac and Patuxent rivers. A few small villages of the Piscataway, members of the Algonquian people, were scattered across the southern portions of the county. North of the Great Falls of the Potomac, there were few permanent settlements, and the Piscataway shared hunting camps and foot paths with members of rival peoples like the Susquehannocks and the Senecas.
Captain John Smith of the English settlement at Jamestown was probably the first European to explore the area, during his travels along the Potomac River and throughout the Chesapeake region.
These lands were claimed by Europeans for the first time when George Calvert, 1st Baron Baltimore was granted the charter for the colony of Maryland by Charles I of England. However, it was not until 1688 that the first tract of land in what is now Montgomery County was granted by the Calvert family to an individual colonist, a wealthy and prominent early Marylander named Henry Darnall. He and other early claimants had no intention of settling their families. They were little more than speculators, securing grants from the colonial leadership and then selling their lands in pieces to settlers. Thus, it was not until approximately 1715 that the first British settlers began building farms and plantations in the area.
These earliest settlers were English or Scottish immigrants from other portions of Maryland, German settlers moving down from Pennsylvania, or Quakers who came to settle on land granted to a convert named James Brooke in what is now Brookeville. Most of these early settlers were small farmers, growing wheat and a variety of other subsistence crops in addition to the region's main cash crop, tobacco. Many of the farmers owned slaves. They transported the tobacco they grew to market through the Potomac River port of Georgetown. Sparsely settled, the area's farms and taverns were nonetheless of strategic importance as access to the interior. General Edward Braddock's army traveled through the county on the way to its disastrous defeat at Fort Duquesne during the French and Indian War.
Like other regions of the American colonies, the region that is now Montgomery County saw protests against British taxation in the years before the American Revolution. In 1774, local residents met at Hungerford's Tavern and agreed to break off commerce with Great Britain. Following the signing of the Declaration of Independence, representatives of the area helped to draft the new state constitution and began to build a Maryland free of proprietary control. In 1776, the Maryland Constitutional Convention formed Montgomery County from lands that had at one point or another been part of Charles, Prince George's and Frederick Counties, naming it after General Richard Montgomery. The leaders of the new county chose as their county seat an area adjacent to Hungerford's Tavern near the center of the county, which later became Rockville. The newly formed Montgomery County supplied arms, food and forage for the Continental Army during the Revolution, in addition to soldiers.
In 1791, portions of Montgomery County, including Georgetown, were ceded to form the new District of Columbia, along with portions of Prince George's County, Maryland, as well as parts of Virginia that were later returned to Virginia.
19th century.
In 1828, construction on the C&O Canal commenced and was completed in 1850, built primarily by Irish immigrants Throughout the 19th century, agriculture dominated the economy in Montgomery County, with slaves playing a significant role, though the vast majority of farmers owned 10 slaves or less rather than large plantations. In the first half of the 19th century, low tobacco prices and worn-out soil caused many tobacco farms to be abandoned. Crop production gradually shifted away from tobacco and toward wheat and corn. Prior to the Civil War, Montgomery County allied itself with other slaveholding counties in southern Maryland and the Eastern Shore. Montgomery County was important in the abolitionist movement, especially among the Quakers in the northern part of the county near Sandy Spring. Josiah Henson grew up as a slave on the Riley farm south of Rockville. He wrote about his experiences in a memoir which became the basis for Harriet Beecher Stowe's "Uncle Tom's Cabin" (1852). A slave cabin where he is believed to have spent time still stands at the end of a driveway off Old Georgetown Road.
Until 1860, only private schools existed in Montgomery County. Initially, schools for European American students were built. A school in Rockville for free African-Americans existed until 1866. Another school for African-Americans was opened by 1877 in Rockville.
Montgomery County's proximity to the nation's capital and split sympathies to North and South resulted in it being occupied by Union forces during the Civil War. The county was "invaded" on multiple occasions by Confederate and Union forces.
In 1855, work on the Metropolitan Branch of the Baltimore & Ohio Railroad began, in order to provide a route between Washington, D.C., and Point of Rocks, Maryland. In 1873, the railroad opened. The railroad spurred development at Takoma Park, Kensington, Garrett Park and Chevy Chase. By providing a much-needed transportation link, it also greatly increased the value of farmland and spurred the development of a dairy industry in the county.
20th and 21st centuries.
On July 1, 1922, the Montgomery County Police Department was established. Prior to that time, law enforcement duties rested in the Montgomery County Sheriff and designated constables. In 1922, the police department consisted of three to six officers who were appointed to two-year terms by the Board of County Commissioners, one of whom would be appointed as Chief. In 1927, the police department was enlarged to twenty officers.
Home rule.
The county began with most legislative power in the hands of the Maryland state government, with a five-person Montgomery County Board of Commissioners who oversaw the administration of the county but could neither pass county laws nor enact policies.
In 1915, Maryland amended its constitution. According to the constitutional amendment, a county's residents may propose a petition to create a nonpartisan Charter Board. If the petition receives signatures from at least twenty percent of the county's registered voters, all county residents would vote to select the members of the Charter Board during the next general election. The Charter Board would be authorized to draft a new system of county government, which could include a county council with the power to enact legislation and policies. Charter Board's proposal would then be voted upon by registered voters during the following general election, and it would be enacted if approved by the voters.
The county commissioners authorized the Brookings Institution to study the existing county government structure and suggest recommendations in 1938. The 720-page report opined that the county had outgrown its form of government. The study recommended the creation of a nonpartisan county council consisting of nine members representing defined districts of the county who could pass legislation, determine policy, and control over the administration of the county. The study also recommended the hiring of a county administrator, the creation of an independent comptroller in charge of county finances, and a civil service commission. The Montgomery County Civic Federation praised the study for its comprehensiveness. The county commissioners appointed a citizen's committee to determine how to enact the study's recommendations. The county commissioners strongly criticized the recommendation to create a county council, both because the council would be nonpartisan and because each county resident would be able to vote for only one representative rather than for all nine.
In 1942, Montgomery County Charter Committee was organized, which was formed primarily to circulate petitions to form a Charter Board. The Charter Board petitioned to form a county council with the power to pass laws without the consent of the Maryland General Assembly and with authority over the administration the county. While the Democratic Party did not explicitly denounce the charter, it issued a statement calling out the ostensibly nonpartisan movement for hidden partisan goals and claimed that backers of the charter petition for their alleged personal and political attacks on the Democratic Party and its officials. The newly formed Independent Party endorsed the charter, praising county residents' goal of improving their form of government. In the November 1942 election, county residents voted in favor of forming a Charter Board.
In 1943, the Charter Board released its draft charter. The council would have nine unpaid members, of which five would represent each of five single-member districts and four would represent the county at-large. Council seats would be nonpartisan, each seat would be held for four years, and elections would occur every two years. The council would have the power to enact legislation and hire a county manager, to whom all governmental department heads would report. All sessions of the council would be open to the public. The office of county treasurer would be abolished and replaced by a director of finance, who would be responsible for assessment and collection of taxes, assessments, and licenses, custody and disbursement of public funds and property, and preparation of monthly financial statements. A department of public works, department of education, department of safety, department of welfare, and department of health would also be created.
The Montgomery County Charter Board opened its campaign headquarters in Bethesda, to serve as an information center regarding the draft charter. The Charter Board emphasized that the draft charter would allow for county affairs to be decided by local representatives rather than by a vote of members of the Maryland General Assembly who represent citizens of all parts of the state. Another group opposing the draft charter opened its headquarters in Bethesda. The group said there was no good reason to abolish the functional state-level system already in place, and that the draft charter would increase taxes and establish heads of governmental agencies with indefinite terms who are not directly accountable to the public. The group also said that making the council seats nonpartisan would go against the country's political history. The editorial board of "The Washington Post" supported the draft charter. The Montgomery County League of Women Voters also endorsed the draft charter.
In a near-record turnout, a 1946 vote to enact a home-rule charter failed by a vote of 14,471 to 13,270. Following the vote, proponents of the charter said they would not give up the fight.
Several months later, Montgomery County Democratic Party leader Col. E. Brooke Lee said he would support Montgomery County home rule by way of an act of the Maryland state legislature. Lee proposed a bill to create a position of county supervisor, who would be in charge of routine county business, would appoint county employees subject to civil service rules and regulations, would supervise expenditures, and would prepare the operating and capital budgets. The bill did not disband the county commissioners or create a county council. The bill passed the state legislature and was signed by Governor Herbert R. O'Conor in 1945. The county commissioners appointed Willard F. Day to the position.
In 1948, by a vote of 17,809 in favor and 13,752 against, voters approved a charter for a "Council-Manager" form of government, making Montgomery County the first home-rule county in Maryland. The charter created an elected seven-member County Council with the power to pass local laws. All seven Council Members would be elected at-large by all county residents. Five would have to live in five different residence districts, while the other two could live anywhere within the county. The Council would serve as both the legislative and executive functions of the county. Council members would elect one of their own to serve as president of the Council. The charter also authorized the hiring of a county manager, the top administrative official, who could be dismissed by the Council after a public hearing. The charter created a department of public works, department of finance, and office of the county attorney, while it abolished the positions of county treasurer and police commissioner. The first County County was elected in 1949.
County Executive.
In 1962, a county civic group advocated for the election of a county executive. The group's report said that the new position would give citizens another place to go if their concerns were refused by the Montgomery County Council. The group said that the fact that the county had four appointed county managers in thirteen years demonstrates that establishing the position in 1948 had not been a stunning success. The group also advocated for nonpartisan elections for council members. Among the reasons for the suggested changes in governance is the fact that the county's population had more than doubled since the governmental system had been established in 1948.
In 1966, the Montgomery County Council adopted a proposed charter amendment to create a new elected position of County Executive. Elected to a four-year term, the County Executive could veto legislation passed by the County Council, although five members of the County Council could vote to overrule the veto. All Council Members would continue to be at-large, but they would need to live in seven different residence districts. Republicans favored a referendum on the proposed charter amendment, while Democrats favored it in principle but urged the specific amendment's defeat because the duties of the County Executive were not specific enough. In the referendum held in September 1966, the referendum was defeated, with 57 percent of voters opposed.
In February 1967, the Montgomery County Council formed a commission to draft a charter amendment to elect a county executive. The commission's plan was to separate the legislative and executive functions of government. The county council would continue to be the legislative branch of county government, and an elected full-time County Executive who could veto legislation passed by the Council; it would take five votes by the Council to override a veto. The County Executive would hire a chief administrative officer to supervise the daily operations of the government. All Council Members would be elected at-large by all county residents, but five of the seven would need to live in each of five different residential districts of substantially equal population. In November 1968, the charter amendment was approved, with 53 percent of votes in favor.
In the first election for County Executive, held in 1970, Republican James P. Gleason defeated Democrat William W. Greenhalgh by a margin of 420 votes.
In November 1986, voters amended the Charter to increase the number of Council seats in the 1990 election from seven to nine. Now five members are elected by the voters of their council district and four are elected at-large. Each voter may vote for five council members; four at-large and one from the district in which they reside.
Annexation.
In November 1995, the City of Takoma Park held a state-sponsored referendum asking whether the portions of the city in Prince George's County should be annexed to Montgomery County or vice versa. The majority of votes in the referendum were in favor of unification of the entire city in Montgomery County. Following subsequent approval by both counties' councils and the Maryland General Assembly, the county line was moved to include the entire city into Montgomery County (including territory in Prince George's County newly annexed by the city) on July 1, 1997. This added about 800 residents to Montgomery County's population.
In October 2002, the county was the site of several of the Beltway sniper attacks.
The county has a number of sites on the National Register of Historic Places.
Geography.
According to the U.S. Census Bureau, the county has a total area of 507 sqmi, of which 491 sqmi is land and 16 sqmi (3.1%) is water. Montgomery County lies entirely inside the Piedmont plateau. The topography is generally rolling. Elevations range from a low of near sea level along the Potomac River to about 875 feet in the northernmost portion of the county north of Damascus. Relief between valley bottoms and hilltops is several hundred feet. The northern boundary of the county was set (from the northeast to the northwest) as westerly along the course of the Patuxent River, then from the source of this river NE to the source of the South Branch of the Patapsco River at Parr's Spring, then a long straight line roughly southwest to the junction of the Monocacy and Potomac Rivers. This forms a geographic oddity of a sliver of land at the far northern tip of the county that is several miles long and averages less than 200 yards wide. In fact, a single house on Lakeview Drive and its yard is sectioned by this sliver into 3 portions, each separately contained within Montgomery, Frederick and Howard Counties. These jurisdictions and Carroll County meet at a single point at Parr's Spring on Parr's Ridge.
Climate.
Montgomery County lies within the northern portions of the humid subtropical climate. It has four distinct seasons, including hot, humid summers and chilly winters. Precipitation is fairly evenly distributed throughout the year, with an inter-quartile range of 37–45 inches. Thunderstorms are common during the summer months, and account for the majority of the average 35 days with thunder per year. Heavy precipitation is most common in summer thunderstorms, but droughty periods are more likely during these months since summer precipitation is more variable than winter.
The mean annual temperature is 55 °F. The average summer (June–July–August) afternoon maximum is about 85 °F while the morning minimums average 66 °F. In winter (December–January–February), these averages are 44 °F and 28 °F. Extreme heat waves can raise readings to around and slightly above 100 °F, and arctic blasts can drop lows to -10 °F to 0 °F.
The average yearly snowfall for the county ranges from 32" in the southernmost extents to 89" across the northern portions where elevations are highest (850 ft. asl) in the Damascus area. Infrequent, large snowstorms, called nor'easters (named for their formation along the Northeast coast, and more importantly, their strong winds from the northeast) can sometimes paralyze the area. The greatest seasonal snowfall total on record was 98.5" in Damascus during the 2009–2010 season.
Demographics.
At the 2010 census, there were 971,777 people residing in the county. The population density was 1762 /sqmi. In 2000, there were 334,632 housing units at an average density of 675 /sqmi.
2010.
The ethnic makeup of the county, according to the 2010 U.S. Census, was the following:
2000.
In 2000, significant national ethnic groups included people of Irish (8.5%), German (8.1%), English (6.8%) and American (5.0%) ancestry according to Census 2000. The county also has a sizable Jewish population, and is home to an increasing number of affluent Indian Americans, Armenian Americans and Iranian Americans. There is also a large population of African immigrants to the United States.
There were 324,565 households of which 35% had children under the age of 18 living with them, 55.2% were married couples living together, 10.5% had a female householder with no husband present, and 30.9% were non-families. 24.4% of all households were made up of individuals, and 7.7% had someone living alone who was 65 years of age or older. The average household size was 2.66 and the average family size was 3.19.
25.4% of the population was under the age of 18, 6.9% from 18 to 24, 32.3% from 25 to 44, 24.2% from 45 to 64, and 11.2% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 92.1 males. For every 100 females age 18 and over, there were 88.1 males.
Montgomery County has the tenth highest median household income in the United States, and the second highest in the state after Howard County as of 2011. The median household income in 2007 was $89,284 and the median family income was $106,093. Males had a median income of $66,415 versus $52,134 for females. The per capita income for the county was $43,073. About 3.3% of families and 4.6% of the population were below the poverty line, including 4.6% of those under age 18 and 4.6% of those age 65 or over.
Since the 1970s, the county has had in place a Moderately Priced Dwelling Unit (MPDU) zoning plan that requires developers to include affordable housing in any new residential developments that they construct in the county. The goal is to create socioeconomically mixed neighborhoods and schools so the rich and poor are not isolated in separate parts of the county. Developers who provide for more than the minimum amount of MPDUs are rewarded with permission to increase the density of their developments, which allows them to build more housing and generate more revenue. Montgomery County was one of the first counties in the U.S. to adopt such a plan, but many other areas have since followed suit.
Economy.
Montgomery County is an important business and research center. It is the epicenter for biotechnology in the Mid-Atlantic region. Montgomery County, as third largest biotechnology cluster in the U.S., holds a large cluster and companies of large corporate size within the state. Biomedical research is carried out by institutions including Johns Hopkins University's Montgomery County Campus (JHU MCC), and the Howard Hughes Medical Institute (HHMI). Federal government agencies engaged in related work include the Food and Drug Administration (FDA), the National Institutes of Health (NIH), the Uniformed Services University of the Health Sciences (USUHS), and the Walter Reed Army Institute of Research.
Many large firms are based in the county, including Discovery Communications, Coventry Health Care, Lockheed Martin, Marriott International, Host Hotels & Resorts, Travel Channel, Ritz-Carlton, Robert Louis Johnson Companies (RLJ Companies), Choice Hotels, MedImmune, TV One, BAE Systems Inc, Hughes Network Systems and GEICO.
Other U.S. federal government agencies based in the county include the National Oceanic and Atmospheric Administration (NOAA), Nuclear Regulatory Commission (NRC), U.S. Department of Energy (DOE), the National Institute of Standards and Technology (NIST), the Walter Reed National Military Medical Center (WRNMMC), and the U.S. Consumer Product Safety Commission (CPSC).
Downtown Bethesda and Silver Spring are the largest urban business hubs in the county; combined, they rival many major city cores.
Top employers.
According to the County's 2011 Comprehensive Annual Financial Report, the top employers by number of employees in the county are:
Government.
Montgomery County was granted a charter form of government in 1948.
The present County Executive/County Council form of government of Montgomery County dates to November 1968 when the voters changed the form of government from a County Commission/County Manager system, as provided in the original 1948 home rule Charter.
The Montgomery County Police Department provides county-wide law enforcement services.
County executives.
The office of the county executive was established in 1970. The first executive was James P. Gleason. The current executive is Isiah "Ike" Leggett, who was sworn in for his first term on December 4, 2006. He was reelected on November 2, 2010 and on November 4, 2014 
Legislative body.
The last Republican serving on the Montgomery County Council, Howard A. Denis of District 1 (Potomac/Bethesda), was defeated in 2006. The board has since been all-Democratic. The members of the County Council for the 2014–2018 term are:
Budget.
Montgomery County has a budget of $13.8 Billion, of which $3.41 Billion are invested in Montgomery County Public Schools, $3 Billion in Washington Suburban Sanitary Commission and $2.8 Billion in transportation. 
Bi-county agencies.
Montgomery and Prince George's Counties share a bi-county planning and parks agency in the Maryland-National Capital Park and Planning Commission (often referred to as "Park and Planning" or its initials M-NCPPC by county residents) and a public bi-county water and sewer utility in the Washington Suburban Sanitary Commission (WSSC).
Liquor control.
Montgomery County is an alcoholic beverage control county. Beer and wine may still be sold in individual stores and at a single location within the county for a store chain.
Federal representation.
Since 2013, Montgomery has been represented in the U.S House of Representatives by John Sarbanes of the 3rd district, John Delaney of the 6th district, and Chris Van Hollen of the 8th district.
Transportation.
Roads.
Montgomery County is approximately bisected northwest-southeast by Interstate 270, a connector linking Interstate 70 with Washington. I-270 divides in North Bethesda with its primary roadway connecting to the eastbound Capital Beltway (Interstate 495), and a spur connecting to southbound I-495 as it approaches Northern Virginia. Another spur highway, Interstate 370, connects Interstate 270 with the Shady Grove Metro station.
An east-west toll freeway, the Intercounty Connector (Maryland Route 200), also known as the ICC, links Interstate 370 and I-270 with U.S. 29; Interstate 95, and terminates at U.S. 1 in Laurel, Prince George's County. The first portion of the freeway, from I-370 to Georgia Avenue, opened in February 2011. The second portion, from Georgia Avenue to I-95, opened in November 2011. The final, 1.5 mile section of highway, from I-95 to Route 1, opened in November 2014.
Roughly paralleling 270 is Maryland Route 355, a surface street known for much of its length as Rockville Pike or simply "The Pike". In its southern reaches it is known as Wisconsin Avenue, while in the north it is known as Frederick Road, or Frederick Ave in Gaithersburg; in the northern half of Rockville (from Town Center north), it is named Hungerford Drive.
Other major routes include Maryland Route 190 (River Road); Maryland Route 97 (Georgia Avenue); Maryland Route 650 (New Hampshire Avenue), Maryland Route 185 (Connecticut Avenue), Randolph Road/Montrose Road, Maryland Route 28 (Darnestown Road, Montgomery Avenue and Norbeck Road), and Maryland Route 27 (Father Hurley Blvd., Ridge Road). U.S. Route 29 parallels the eastern border of the county; first as Georgia Avenue in Silver Spring, then Colesville Road, and thence as Columbia Pike through Burtonsville and into Howard County.
The Montgomery County government has strongly supported the use of automated traffic enforcement on county roads. In 2007 this county became the first jurisdiction in Maryland to introduce automated speed cameras on roads with speed limits up to 35 mph, issuing fines of $40 by mail. Red light cameras with fines of $75 are also in use.
A computer system coordinates the setting of 750 traffic lights. In 2009, the computer system failed for a brief period, causing considerable problems.
Bus.
Montgomery County operates its own bus public transit system, known as Ride On. Major routes are also covered by WMATA's Metrobus service.
Rail.
Montgomery County is served by three passenger rail systems.
Amtrak, the U.S. national passenger rail system, operates its "Capitol Limited" to Rockville, between Washington Union Station and Chicago Union Station.
The Brunswick line of the MARC commuter rail system makes stops at Silver Spring, Kensington, Garrett Park, Rockville, Washington Grove, Gaithersburg, Metropolitan Grove, Germantown, Boyds, Barnesville, and Dickerson, where the line splits into its Frederick and Martinsburg branches.
Both suburban arms of the Red Line of the Washington Metro serve Montgomery County. It follows the CSX right of way to the west, roughly paralleling Route 355 from Friendship Heights to Shady Grove. The eastern side runs between the two tracks of the CSX right of way from Washington Union Station to Silver Spring, and roughly parallels Georgia Avenue, from Silver Spring to Glenmont.
There has been much debate on the construction of two new transitways, both of which are still in the early stages of design. The Purple Line would run "cross-town" connecting nodes in Montgomery and Prince George's Counties near the Beltway; and the Corridor Cities Transitway would provide an extension of the Red Line corridor from Gaithersburg to Germantown and beyond.
Air.
The Montgomery County Airpark (FAA GAI, ICAO KGAI), a general aviation facility in Gaithersburg, is the major airport in the county. Davis Airport (FAA Identifier W50), a privately owned airstrip, is located in Laytonsville on Hawkins Creamery Road. Commercial air service is provided at the nearby Ronald Reagan Washington National, Washington Dulles International, and BWI Airports.
Education.
Educative system is conformed by Montgomery County Public Schools, Montgomery College and other institutions in the area.
Montgomery County Public Schools.
Elementary and secondary public schools are operated by the Montgomery County Public Schools and has a budget of 3.41 US$Billions for 2015.
Montgomery College.
The county is also served by Montgomery College, a public, open access community college that has a budget of 950 US$ Millions for 2015. The county has no public university of its own, but the state university system does operate a facility called Universities at Shady Grove in Rockville that provides access to baccalaureate and Master's level programs from several of the state's public universities.
MCPS operates under the jurisdiction of an elected Board of Education. Its current members are:
Montgomery County Public Libraries.
Montgomery County Public Libraries (MCPL) is the public library system for residents of Montgomery County, Maryland The system includes 23 individual libraries. And has a budget 38 US$ Millions for 2015.
The "Smondrowski Amendment".
On November 11, 2014, the Board approved an amendment introduced by Rebecca Smondrowksi to modify the school calendar to delete all references to religious holidays such as Christmas, Easter, Yom Kippur and Rosh Hashanah. The amendment was in response to requests by an interfaith organization called Equality for Eid which asked that the listing for the Islamic holiday, Eid Al-Adha be listed alongside Yom Kippur which occurred on the same day.
The Smondrowski amendment received both national and international attention. Criticism of the Smondrowski amendment came from a variety of sources including the Montgomery County Executive, Isiah Leggett, and congressman John Delaney.
Culture.
Religion.
Montgomery County is filled with diversity. Of Montgomery County's population, 13% is Catholic, 5% is Baptist, 4% is Evangelical Protestant, 3% is Jewish, 3% is Methodist/Pietist, 2% is Adventist, 2% is Presbyterian, 1% is Episcopalian/Anglican, 1% is Mormon, 1% is Muslim, 1% is Lutheran, 1% is Eastern Orthodox, 1% is Pentecostal, 1% is Buddhist, and 1% is Hindu.
The Seventh-day Adventist Church maintains its General Conference headquarters in Silver Spring Maryland. 
Sports.
Bethesda's Congressional Country Club has hosted four Major Championships, including three playings of the U.S. Open, most recently in 2011 which was won by Rory McIlroy. The Club also hosts the AT&T National, an annual event on the PGA Tour which benefits the Tiger Woods Foundation. Previously, neighboring TPC at Avenel hosted the Booz Allen Classic.
The Bethesda Big Train, Rockville Express, and Silver Spring-Takoma Thunderbolts all play college level wooden bat baseball in the Cal Ripken Collegiate Baseball League.
Montgomery County is home of the Montgomery County Swim League, a youth (ages 4–18) competitive swimming league composed of ninety teams based at community pools throughout the county.
There are future possibilities of a minor league baseball team forming to play for the Atlantic League of Professional Baseball to represent Montgomery County.
Montgomery County State Agricultural Fair.
Since 1949 the Montgomery County Agricultural Fair, the largest in the state, showcases farm life in the county. The week long event offers family events, carnival rides, live animals, entertainment and food. Visitors can also purchase canned and baked goods, clothing, quilts and produce from local county farmers.
Communities.
Special Tax Districts.
Occupying a middle ground between incorporated and unincorporated areas are Special Tax Districts, quasi-municipal unincorporated areas created by legislation passed by either the Maryland General Assembly or the county. The Special Tax Districts generally have limited purposes, such as providing some municipal services or improvements to drainage or street lighting. Special Tax Districts lack home rule authority and must petition their cognizant governmental entity for changes affecting the authority of the district. The four incorporated villages of Montgomery County and the town of Chevy Chase View were originally established as Special Tax Districts. Four Special Tax Districts remain in the county:
Census-designated places.
Unincorporated areas are also considered as towns by many people and listed in many collections of towns, but they lack local government. Various organizations, such as the United States Census Bureau, the United States Postal Service, and local chambers of commerce, define the communities they wish to recognize differently, and since they are not incorporated, their boundaries have no official status outside the organizations in question. The Census Bureau recognizes the following census-designated places in the county:

</doc>
<doc id="43483" url="http://en.wikipedia.org/wiki?curid=43483" title="Rockville, Maryland">
Rockville, Maryland

Rockville is a city located in the central region of Montgomery County, Maryland. It is the county seat and is a major incorporated city of Montgomery County and forms part of the Baltimore–Washington metropolitan area. The 2010 census tabulated the Rockville's population at 61,209, making it the third largest incorporated city in Maryland, behind Baltimore and Frederick. Rockville is the largest incorporated city in Montgomery County, Maryland, although the nearby census-designated place of Germantown is more populous.
Rockville, along with neighboring Gaithersburg and Bethesda, is at the core of the Interstate 270 Technology Corridor which is home to numerous software and biotechnology companies as well as several federal government institutions. The city also has several upscale regional shopping centers and is one of the major retail hubs in Montgomery County.
History.
Early history.
Situated in the Piedmont region and crossed by three creeks (Rock Creek, Cabin John Creek, and Watts Branch), Rockville provided an excellent refuge for semi-nomadic Native Americans as early as 8000 BC. By the first millennium BC, a few of these groups had settled down into year-round agricultural communities that exploited the native flora, including sunflowers and marsh elder. By AD 1200, these early groups (dubbed "Montgomery Indians" by later archaeologists) were increasingly drawn into conflict with the Senecas and Susquehannocks who had migrated south from Pennsylvania and New York. Within the present-day boundaries of the city, six prehistoric sites have been uncovered and documented, and borne artifacts several thousand years old. By the year 1700, under pressure from European colonists, the majority of these original inhabitants had been driven away.
The indigenous population carved a path on the high ground, known as Sinequa Trail, which is now downtown Rockville. Later, the Maryland Assembly set the standard of 20 feet for main thoroughfares and designated the Rock Creek Main Road or Great Road to be built to this standard. In the mid-18th century, Lawrence Owen opened a small inn on the road. The place, known as Owen's Ordinary, took on greater prominence when, on April 14, 1755, Major General Edward Braddock stopped at Owen's Ordinary on a start of a mission from George Town (now Washington, D.C.) to press British claims of the western frontier. The location of the road, near the present Rockville Pike, was strategically located on higher ground making it dry year-round.:6–9
18th century.
The first land patents in the Rockville area were obtained by Arthur Nelson between 1717 and 1735. Within three decades, the first permanent buildings in what would become the center of Rockville were established on this land. Still a part of Prince George's County at this time, the growth of Daniel Dulaney's Frederick Town prompted the separation of the western portion of the county, including Rockville, into Frederick County in 1748.
Being a small, unincorporated town, early Rockville was known by a variety of names, including Owen's Ordinary, Hungerford's Tavern, and Daley's Tavern. The first recorded mention of the settlement which would later become known as Rockville dates to the Braddock Expedition in 1755. On April 14, one of the approximately two thousand men who were accompanying General Braddock through wrote the following: "we marched to larance Owings or Owings Oardianary, a Single House, it being 18 miles and very dirty." Owen's Ordinary was a small rest stop on Rock Creek Main Road (later the Rockville Pike), which stretched from George Town to Frederick Town, and was then one of the largest thoroughfares in the colony of Maryland.
On September 6, 1776, the Maryland Constitutional Convention agreed to a proposal introduced by Thomas Sprigg Wootton wherein Frederick County, the largest and most populous county in Maryland, would be divided into three smaller units. The southern portion of the county, of which Rockville was a part, was named Montgomery County. The most populous and prosperous urban center in this new county was George Town, but its location at the far southern edge rendered it worthless as a seat of local government. Rockville, a small, but centrally located and well-traveled town, was chosen as the seat of the county's government. At the time, Rockville did not have a name; it was generally called Hungerford's Tavern, after the well-known tavern in it. After being named the county seat, the village was referred to by all as Montgomery Court House. The tavern served as the county courthouse, and it held its first such proceedings on May 20, 1777.
In 1784, William Prather Williams, a local landowner, hired a surveyor to lay out much of the town. In his honor, many took to calling the town Williamsburg. In practice, however, Williamsburg and Montgomery Court House were used interchangeably. Rockville came to greater prominence when Montgomery county was created and later when George Town was ceded to the federal government to create the District of Columbia.
19th century.
It was first considered to officially name the town Wattsville, after the nearby Watts Branch, but the stream was later considered too small to give its name to the town. On July 16, 1803, when the area was officially entered into the county land records with the name "Rockville," derived from Rock Creek. Nevertheless, the name Montgomery Court House continued to appear on maps and other documents through the 1820s.
By petition of Rockville's citizens, the Maryland General Assembly incorporated the village on March 10, 1860. During the American Civil War, General George B. McClellan stayed at the Beall Dawson house in 1862. In addition, General J.E.B. Stuart and an army of 8,000 Confederate cavalrymen marched through and occupied Rockville on June 28, 1863, while on their way to Gettysburg and stayed at the Prettyman house. Jubal Anderson Early had also crossed through Maryland on his way to and from his attack on Washington. A monument to the Confederate soldier is hidden behind the old courthouse building The monument was dedicated on June 3, 1913 at a cost of $3,600.
In 1873, the Baltimore and Ohio Railroad arrived, making Rockville easily accessible from Washington, D.C. ("See Metropolitan Branch.") In July 1891, the Tennallytown and Rockville Railway inaugurated Rockville's first trolley service connecting to the Georgetown and Tennallytown Railway terminus at Western Avenue and Wisconsin Avenue.
Twentieth century through today.
The newly opened railroad provided service from Georgetown to Rockville, connecting Rockville to Washington, D.C. by trolley. Trolley service operated for four decades, until, eclipsed by the growing popularity of the automobile, service was halted in August 1935. The Blue Ridge Transportation Company provided bus service for Rockville and Montgomery County from 1924 through 1955. After 1955, Rockville would not see a concerted effort to develop a public transportation infrastructure until the 1970s, when the Washington Metropolitan Area Transit Authority (WMATA) began work to extend the Washington Metro into Rockville and extended Metrobus service into Montgomery County. The Rockville station of Washington Metro began service on July 25, 1984, and the Twinbrook station began service on December 15, 1984. Metrobus service was supplemented by Montgomery County's own Ride On bus service starting in 1979. MARC, Maryland's Rail Commuter service, serves Rockville with its Brunswick line. From Rockville MARC provides service to Union Station in Washington D.C. (southbound) and, Frederick and Martinsburg, West Virginia (northbound), as well as intermediate points. Amtrak, the national passenger rail system, provides service from Rockville to Chicago and Washington D.C.
The mid-20th century saw substantial growth in Rockville, especially with the annexation of the Twinbrook subdivision in 1949, which added hundreds of new homes and thousands of new residents to the city. In 1954, Congressional Airport closed, and its land was sold to developers to build residences and a commercial shopping center. The shopping center, named Congressional Plaza, opened in 1958. These new areas provided affordable housing and grew quickly with young families eager to start their lives following World War II.
During the Cold War, it was considered safer to remain in Rockville than to evacuate during a hypothetical nuclear attack on Washington, D.C. Bomb shelters were built, including the largest one at Glenview Mansion and 15 other locations. The I-270 highway was designated as an emergency aircraft landing strip. Two Nike missile launcher sites were located on Muddy Branch and Snouffer School Roads until the mid-1970s.:163
From the 1960s, Rockville's town center, formerly one of the area's commercial centers, suffered from a period of decline. Rockville soon became the first city in Maryland to enter into a government funded urban renewal program. This resulted in the demolition of most of the original business district. Included in the plan was the unsuccessful Rockville Mall, which failed to attract either major retailers or customers and was demolished in 1994, various government buildings such as the new Montgomery County Judicial Center, and a reorganization of the road plan near the Courthouse. Unfortunately, the once-promising plan was for the most part a disappointment. Although efforts to restore the town center continue, the majority of the city's economic activity has since relocated along Rockville Pike (MD Route 355/Wisconsin Avenue). In 2004, Rockville Mayor Larry Giammo announced plans to renovate the , including building new stores and housing and relocating the city's library. In the past year, the new Rockville Town Center has been transformed and includes a number of boutique-like stores, restaurants, condominiums and apartments, as well as stages, fountains and the Rockville Library. The headquarters of the U.S. Public Health Service is on Montrose Road while the U.S. Nuclear Regulatory Commission's headquarters is just south of the City's corporate limits.
The city is closely associated with the neighboring towns of Kensington and the unincorporated census-designated place, North Bethesda. The Music Center at Strathmore, an arts and theater center, opened in February 2005 in the latter of these two areas and is presently the second home of the Baltimore Symphony Orchestra, and the Fitzgerald Theatre in Rockville Civic Center Park has provided diverse entertainment since 1960. In 1998, Regal Cinemas opened in Town Center.:217
The city also has a in the British style.
The REM song "(Don't Go Back To) Rockville", released in 1984, was written by Mike Mills about not wanting his girlfriend to return to Rockville, Maryland.
Geography.
According to the United States Census Bureau, the city has a total area of 13.57 sqmi, of which, 13.51 sqmi is land and 0.06 sqmi is water.
Climate.
The climate in this area is characterized by hot, humid summers and generally mild to cool winters. According to the Köppen Climate Classification system, Rockville has a humid subtropical climate, abbreviated "Cfa" on climate maps. According to the United States Department of Agriculture, Rockville is in hardiness zone 7a, meaning that the average annual minimum winter temperature is 0 to. The average first frost occurs on October 21, and the average final frost occurs on April 16.
Demographics.
In addition to North Potomac with a 27.59% Asian population and Potomac with close to 15%, according to the U.S. Census Bureau, Rockville is home to one of the largest Chinese communities in Maryland. According to the U.S. Census conducted in 2000, 14.5% of North Potomac's residents identified themselves as being of Chinese ancestry, making North Potomac the area with the highest percentage of Chinese ancestry in any place besides California and Hawaii. According to the Montgomery County Public Schools (MCPS) enrollment demographic statistics, the two high schools in Montgomery County with the highest reported Asian ancestry are Thomas S. Wootton High School in Rockville, MD with a 32.1% Asian population and caters to the communities in North Potomac, Rockville, and Potomac, MD, and Winston Churchill High School in Potomac, MD with a 23.0% Asian population. Although North Potomac and Potomac have the highest concentration of Asian population in Maryland, the areas are largely residential and consist of suburban subdivisions. Thus, the more commercially favorable Rockville has become the center for Chinese/Taiwanese businesses since it is the county seat of Montgomery County and has large economic activity along Rockville Pike/Wisconsin Avenue (MD Route 355) in addition to having its own middle class and upscale residential areas. Rockville is widely considered to be a "Little Taipei" due to the area's high concentration of Taiwanese immigrants.
Rockville is also the center of the Washington, D.C., Metropolitan Area's Jewish population, containing several synagogues, kosher restaurants, and the largest of the Washington area's three Jewish community centers, part of a complex which includes a Jewish nursing home, day school, theater, and educational facility. There are also high percentages of Jewish population in the surrounding areas of North Potomac and Potomac, which are largely residential and not as commercially suitable as Rockville. The city also has large Korean and Indian populations.
The median income for a household in the city as of 2007 is $86,085. As of 2007, the median income for a family was $98,257. Males have a median income of $53,764 versus $38,788 for females. The per capita income for the city is $30,518. 7.8% of the population and 5.6% of families are below the poverty line. Out of the total population, 8.9% of those under the age of 18 and 7.9% of those 65 and older are living below the poverty line.
2010 census.
As of the census of 2010, there were 61,209 people, 23,686 households, and 15,524 families residing in the city. The population density was 4530.6 PD/sqmi. There were 25,199 housing units at an average density of 1865.2 /sqmi. The racial makeup of the city was 60.4% White (52.8% non-Hispanic white), 9.6% African American, 0.3% Native American, 20.6% Asian, 5.3% from other races, and 3.8% from two or more races. Hispanic or Latino of any race were 14.3% of the population.
There were 23,686 households of which 31.8% had children under the age of 18 living with them, 52.3% were married couples living together, 9.9% had a female householder with no husband present, 3.4% had a male householder with no wife present, and 34.5% were non-families. 27.0% of all households were made up of individuals and 9.5% had someone living alone who was 65 years of age or older. The average household size was 2.54 and the average family size was 3.08.
The median age in the city was 38.7 years. 21.5% of residents were under the age of 18; 7.2% were between the ages of 18 and 24; 31.1% were from 25 to 44; 26.3% were from 45 to 64; and 14% were 65 years of age or older. The gender makeup of the city was 47.9% male and 52.1% female.
Economy.
Adventist HealthCare, the American Kidney Fund, Choice Hotels, Emergent BioSolutions, Human Genome Sciences, Westat, United States Pharmacopeia (USP), Bethesda Softworks and Goodwill Industries are headquartered in Rockville.
Largest employers.
According to the city's 2012 Comprehensive Annual Financial Report, the top employers in the city are:
Government.
Rockville has a council-manager form of government.
Mayor.
The current Mayor of Rockville is Bridget Donnell Newton.
Rockville was incorporated in 1860, but its early records were destroyed by Confederate soldiers in July 1864.
Rockville's mayors include:
Representative body.
Rockville has a four-member City Council, whose members, along with the Mayor, serve as the governing body of the city. The Council members for the 2013 to 2015 session are Beryl L. Feinberg, Virginia Onley, Tom Moore, and Julie Palakovich Carr.
Departments and offices.
The city manager oversees the following departments:
Law enforcement.
The city is serviced by the Rockville City Police Department and is aided by the Montgomery County Police Department as directed by authority.
Education.
Rockville is served by Montgomery County Public Schools. Public high schools in Rockville include Thomas S. Wootton High School, Richard Montgomery High School, and Rockville High School.
The Charles E. Smith Jewish Day School, the Melvin J. Berman Hebrew Academy and the Montrose Christian School, among other private schools, are located in Rockville.
Institutions of higher education in Rockville include Montgomery College (Rockville Campus), The University of Maryland University College (main campus is in Adelphi, Maryland), The Johns Hopkins University Montgomery County Campus (main campus is in Baltimore, Maryland), and the Universities at Shady Grove, a collaboration of nine Maryland public degree-granting institutions, all have Rockville addresses, but are outside the city limits.
Infrastructure.
Transportation.
The Red Line of the Washington Metro rail system services the Rockville station and the Twinbrook station. The Rockville station is located at Hungerford Drive near Park Road. The Twinbrook station is located near Rockville Pike and Halpine Road with entrances on Chapman Avenue.
At the same location as the Rockville metro station is Rockville Station on the Brunswick Line of the MARC commuter rail system, which runs to and from Washington, DC.
Amtrak, the national passenger rail system, provides intercity train service to Rockville. The city's passenger rail station is located at 251 Hungerford Drive (at Park Road), ZIP code 20850; this is also the location of the MARC station described above.
Sister cities.
Rockville has one sister city:
It has a "friendship relationship" (a step preliminary to a sister-city relationship) with another city: 

</doc>
<doc id="43484" url="http://en.wikipedia.org/wiki?curid=43484" title="Bethesda, Maryland">
Bethesda, Maryland

Bethesda is a census-designated place in southern Montgomery County, Maryland, just northwest of the United States capital of Washington, D.C. It takes its name from a local church, the Bethesda Meeting House (1820, rebuilt 1849), which in turn took its name from Jerusalem's Pool of Bethesda. (In Aramaic, ܒܝܬ ܚܣܕܐ "beth ḥesda" means "House of Mercy" and in Hebrew, בית חסד "beit ḥesed" means "House of Kindness".) The National Institutes of Health main campus and the Walter Reed National Military Medical Center are in Bethesda, as are a number of corporate and government headquarters.
Bethesda is one of the most affluent and highly educated communities in the United States. In 2014 it placed first in "Forbes" list of America's most educated small towns and first on CNNMoney.com's list of top-earning American towns in 2012. In April 2009, "Forbes" ranked Bethesda second on its list of "America's Most Livable Cities." In October 2009, based on education, income, health, and fitness, "Total Beauty" ranked Bethesda first on its list of the U.S.'s "Top 10 Hottest-Guy Cities."
As an unincorporated area, Bethesda has no official boundaries. The United States Census Bureau defines a census-designated place named Bethesda whose center is located at . The United States Geological Survey has defined Bethesda as an area whose center is at , slightly different from the Census Bureau's definition. Other definitions are used by the Bethesda Urban Planning District, the United States Postal Service (which defines Bethesda to comprise the zip codes 20810, 20811, 20813, 20814, 20815, 20816, and 20817), and other organizations. According to estimates released by the U.S. Census Bureau in 2013, the community had a total population of 63,374. Most of Bethesda's residents are in Maryland Legislative District 16.
History.
Bethesda is situated along a major thoroughfare that was originally the route of an ancient Native American trail. Between 1805 and 1821, it was developed into a toll road called the Washington and Rockville Turnpike, which carried tobacco and other products between Georgetown and Rockville, and north to Frederick. A small settlement grew around a store and tollhouse along the turnpike. By 1862, the community was known as "Darcy's Store" after the owner of a local establishment, William E. Darcy. The settlement was renamed in 1871 by the new postmaster, Robert Franck, after the Bethesda Meeting House, a Presbyterian church built in 1820 on the present site of the Cemetery of the Bethesda Meeting House. The church burned in 1849 and was rebuilt the same year about 100 yards south at its present site.
Throughout most of the 19th century, Bethesda was a small crossroads village, consisting of a post office, a blacksmith shop, a church and school, and a few houses and stores. It was not until the installation of a streetcar line in 1890 and the beginnings of suburbanization in the early 1900s that Bethesda began to grow in population. Subdivisions began to appear on old farmland, becoming the neighborhoods of Drummond, Woodmont, Edgemoor, and Battery Park. Further north, several wealthy men made Rockville Pike famous for its mansions. These included Brainard W. Parker ("Cedarcroft", 1892), James Oyster ("Strathmore", 1899), George E. Hamilton ("Hamilton House", 1904; now the Stone Ridge School), Luke I. Wilson ("Tree Tops", 1926), Gilbert Grosvenor ("Wild Acres", 1928–29), and George Freeland Peter ("Stone House", 1930). In 1930, Dr Armistead Peter's pioneering manor house "Winona" (1873) became the clubhouse of the original Woodmont Country Club (on land that is now part of the National Institutes of Health (NIH) campus). Merle Thorpe's mansion, "Pook's Hill" (1927, razed 1948) — on the site of the current neighborhood of the same name — became the home-in-exile of the Norwegian Royal Family during World War II.
That war, and the expansion of government that it created, further fed the rapid expansion of Bethesda. Both the National Naval Medical Center (1940–42) and the NIH complex (1948) were built just to the north of the developing downtown. This, in turn, drew further government contractors, medical professionals, and other businesses to the area. In recent years, Bethesda has consolidated as the major urban core and employment center of southwestern Montgomery County. This recent growth has been significantly vigorous following the expansion of Metrorail with a station in Bethesda in 1984. Alan Kay built the Bethesda Metro Center over the Red line metro rail which opened up further commercial and residential development around the surrounding vicinity.
Geography.
According to the United States Census Bureau, the CDP has a total area of 13.2 sqmi. 13.1 sqmi of it is land and 0.1 sqmi of it (0.38%) is water.
The main commercial corridor that runs through Bethesda is Maryland Route 355 (known as Wisconsin Avenue in Bethesda and as Rockville Pike and Hungerford Drive in more northern communities), which, to the north, connects Bethesda with the communities of North Bethesda and Rockville, ending, after several name changes, in Frederick, Maryland. Toward the South, Rockville Pike becomes Wisconsin Avenue near the NIH Campus and continues beyond Bethesda through Chevy Chase, Friendship Heights and into Washington, D.C., ending in Georgetown.
The area commonly known as "Downtown Bethesda" (or to natives "B-town") 
is centered at the intersection of Wisconsin Avenue, Old Georgetown Road and East-West Highway. Other focal points of downtown Bethesda include the Woodmont Triangle, bordered by Old Georgetown Road (Maryland Route 187), Woodmont and Rugby Avenues, and the Bethesda Row, centered at the intersection of Woodmont Avenue and Bethesda Avenue. Much of the dense construction in that area followed the opening of the Bethesda station on the Red Line of the Washington Metro rapid transit system, also located at this intersection and the centerpiece of the Bethesda Metro Center development. The Medical Center Metro stop lies about 0.7 miles north of the Bethesda stop, Medical Center, which serves the NIH Campus, the Walter Reed National Military Medical Center, and the Uniformed Services University of the Health Sciences.
Demographics.
2000.
As of the census of 2000, there were 55,277 people, 23,659 households, and 14,455 families residing in the CDP. The population density was 4,205.8 inhabitants per square mile (1,624.2/km²). There were 24,368 housing units at an average density of 1,854.1 per square mile (716.0/km²). The racial makeup of the CDP was 85.86% White, 2.67% Black or African American, 0.17% Native American, 7.92% Asian, 0.05% Pacific Islander, 1.23% from other races, and 2.11% from two or more races. 5.43% of the population were Hispanic or Latino of any race.
There were 23,659 households out of which 28.0% had children under the age of 18 living with them, 53.4% were married couples living together, 6.0% had a female householder with no husband present, and 38.9% were non-families. 32.2% of all households were made up of individuals and 11.6% had someone living alone who was 65 years of age or older. The average household size was 2.30 and the average family size was 2.92.
In the CDP the population was spread out with 21.8% under the age of 18, 4.6% from 18 to 24, 29.2% from 25 to 44, 27.1% from 45 to 64, and 17.2% who were 65 years of age or older. The median age was 41 years. For every 100 females there were 87.7 males. For every 100 females age 18 and over, there were 84.0 males.
Bethesda is a very wealthy and well-educated area. According to the 2000 Census, Bethesda was the best-educated city in the United States of America with a population of 50,000 or more. 79% of residents 25 or older have bachelor's degrees and 49% have graduate or professional degrees. According to a 2007 estimate, the median income for a household in the CDP was $117,723, and the median income for a family was $168,385. Males had a median income of $84,797 versus $57,569 for females. The per capita income for the CDP was $58,479. About 1.7% of families and 3.3% of the population were below the poverty line, including 1.8% of those under age 18 and 4.1% of those age 65 or over. Many commute to Washington D.C. for work. The average price of a four bedroom, two bath home in Bethesda in 2010 was $806,817 (which ranks it as the twentieth most expensive community in America).
Bethesda is often associated with its neighboring communities, Potomac, Maryland, Chevy Chase, Maryland, Great Falls, Virginia, and McLean, Virginia for their similar demographics. In 2009, "Self" magazine ranked Bethesda as the second healthiest place for women in the country, a year after being ranked number one. As of 2009, 8 Pulitzer Prize winners live in Bethesda, as do several well-known political commentators (including George Will, David Brooks, and Tom Friedman).
Landmarks.
Important institutions located in Bethesda include the National Institutes of Health campus, the Consumer Product Safety Commission, and the Naval Surface Warfare Center Carderock Division. Bethesda is also home to the Walter Reed National Military Medical Center, formerly referred to as Bethesda Naval Hospital (National Naval Medical Center, NNMC). The Medical Center is also the place where the President goes to get his yearly check-up. Adjoining the hospital to the east is the Uniformed Services University of the Health Sciences, as well as a number of other military medical and research institutions.
The headquarters of defense conglomerate Lockheed Martin, managed health care company Coventry Health Care and hotel and resort chains Marriott International and Host Hotels & Resorts, Inc. are located in Bethesda. Software company Bethesda Softworks were originally located in Bethesda, but moved to Rockville, Maryland, in 1990. The Discovery Channel also had its headquarters in Bethesda before relocating to Silver Spring in 2004. On the professional services side, numerous banks (PNC, Capital One Bank) brokerage firms (SmithBarney, Merrill Lynch, Charles Schwab, Fidelity) and law firms (Ballard Spahr, JDKatz, Paley Rothman, Lerch Early & Brewer) maintain offices in Bethesda. Bethesda has two farmers markets, the Montgomery Farm Woman's Cooperative Market and the Bethesda Farmer's Market
Federal Realty Investment Trust (FRIT) has developed much of the west side of downtown Bethesda into an area called Bethesda Row. The vibrant district includes Barnes & Noble, an Apple Store, a cinema, and dozens of shops and restaurants. Also located in downtown Bethesda is one of the Madonna of the Trail monuments, erected by the National Old Trails Association working in concert with the Daughters of the American Revolution. Judge Harry S Truman, presided over the dedication of the Bethesda monument, on April 19, 1929. Nearby is the Bethesda Post Office. Also starting in the heart of downtown Bethesda, is the Capital Crescent Trail which follows the old tracks of the B&O Railroad stretching from Georgetown, Washington, D.C., to Silver Spring, MD. Walter Reed Medical Center and the Bethesda Theater are two important Art Deco architectural structures in the suburbs surrounding Washington, D.C.
The Writer's Center in Bethesda publishes "Poet Lore", the longest continuously running poetry journal in the United States.
Bethesda Lane opened in 2008. This extension of the Bethesda Row development, located near the corner of Bethesda Avenue and Arlington Road, includes many new retail shops and restaurants underneath luxury apartments.
Bethesda is the home of Congressional Country Club, which is recognized as one of the most prestigious country clubs in the world. Congressional has hosted four major golf championships, including the 2011 U.S. Open, won by Rory McIlroy. The AT&T National, hosted by Tiger Woods, has been played at Congressional four times.
Bethesda is also home of the exclusive Burning Tree Club, the Bethesda Country Club, and the Bethesda Community Baseball Club, which operates the Bethesda Big Train, a summer collegiate baseball team.
Education.
Bethesda is also home to a federally funded and operated health science university, the Uniformed Services University of the Health Sciences (USU). The primary mission of USU is to prepare graduates for service in the Medical Corps of the U.S. Army, Navy, Air Force, and Public Health Service. The university consists of the F. Edward Hebert School of Medicine, a medical school, and the Graduate School of Nursing, a nursing school.
The Washington Japanese Language School (WJLS, ワシントン日本語学校 "Washington Nihongo Gakkō"), a supplementary weekend Japanese school, holds its classes at the Stone Ridge School of the Sacred Heart in Bethesda. The WJLS maintains its school office in North Bethesda, adjacent to Garrett Park. The institution, giving supplemental education to Japanese-speaking children in the Washington DC area, was founded in 1958, making it the oldest Japanese government-sponsored supplementary school in the U.S.
Economy.
Notable companies based in Bethesda include:
Transportation.
Washington Metro's Red Line services two primary locations in Bethesda: the downtown area at the Bethesda, and the area near the National Institutes of Health and the Walter Reed Medical Center at the Medical Center Washington Metro stations.
Local buses include:
Long-distance buses include:

</doc>
<doc id="43485" url="http://en.wikipedia.org/wiki?curid=43485" title="Silver Spring, Maryland">
Silver Spring, Maryland

Silver Spring is an unincorporated area and census-designated place (CDP) in Montgomery County, Maryland, United States. It had a population of 76,716 according to 2013 estimates by the United States Census Bureau, making it the fourth most populous place in Maryland, after Baltimore, Columbia, and Germantown.
The urbanized, oldest, and southernmost part of Silver Spring is a major business hub that lies at the north apex of Washington, D.C. As of 2004, the Central Business District (CBD) held 7254729 sqft of office space, 5216 dwelling units and 17.6 acre of parkland. The population density of this CBD area of Silver Spring was 15,600 per square mile all within 360 acre and approximately 2.5 sqmi in the CBD/downtown area.
The community has recently undergone a significant renaissance, with the addition of major retail, residential, and office developments.
Silver Spring takes its name from a mica-flecked spring discovered there in 1840 by Francis Preston Blair, who subsequently bought much of the surrounding land. Acorn Park, tucked away in an area of south Silver Spring away from the main downtown area, is believed to be the site of the original spring.
Geography.
As an unincorporated area, Silver Spring's boundaries are not officially defined. As of the 2010 Census the United States Census Bureau defines Silver Spring as a census-designated place with a total area of 7.92 sqmi, all land; however, it does contain numerous creeks and small lakes. This definition is a 15% reduction from the 9.4 sq. mi. used in previous years. The United States Geological Survey locates the center of Silver Spring at , notably some distance from the Census Bureau's datum. By another definition, Silver Spring is located at (39.004242, -77.019004). The definitions used by the Silver Spring Urban Planning District, the United States Postal Service, the Greater Silver Spring Chamber of Commerce, etc., are all different, each defining it for its own purposes.
Residents of a large swath of South-Eastern Montgomery County have Silver Spring mailing addresses. This area extends roughly from the Washington, D.C., Prince George's County, Maryland and Howard County, Maryland lines to the south, east and north, and Rock Creek Park and Plyers Mill Road to the west and north-west. These boundaries make Silver Spring larger in area than any city in Maryland except for Baltimore. Some notable landmarks are the world headquarters of Discovery Communications, the AFI Silver Theatre, the NOAA headquarters, the headquarters of the Seventh-day Adventist Church and the national headquarters of the Ahmadiyya Muslim Community.
Parks and recreation.
Rock Creek Park passes along the west side of Silver Spring, and offers hiking trails, picnic grounds, and bicycling on weekends, when its main road, Beach Drive, is mostly closed to motor vehicles.
Sligo Creek Park follows Sligo Creek through Silver Spring; it offers hiking trails, tennis courts, playgrounds and bicycling. The latter is facilitated on weekends, when parts of Sligo Creek Parkway are closed to autos. The bike trails are winding and slower than most in the region. Recently, rocks have been spread along either side of the road, providing a hazardous bike ride, or skating leisure.
Acorn Park in the downtown area of Silver Spring is believed to be the site of the eponymous "silver spring".
The 14.5 acre Jesup Blair Park was recently renovated and has a soccer field, tennis courts, basketball courts, and picnic area.
Brookside Gardens is a 50 acre park within Wheaton Regional Park, in "greater" Silver Spring. It is located on the original site of Stadler Nursery (now in Laytonsville, Maryland).
Demographics.
2000.
For the 2000 census, there were 76,540 people, 30,374 households, and 17,616 families residing in the census area (if all areas with the "Silver Spring" address are included, the population swells to around 250,000). The population density was 8,123.6 people per square mile (3,137.2/km²). There were 31,208 housing units at an average density of 3,312.3 per square mile (1,279.1/km²). The racial makeup of the community was 46.61% Caucasian, 28.07% Black American, 0.44% Indian American, 8.22% Asian, 0.06% Pacific Islander, 11.55% from other races, and 5.04% from two or more races. Hispanic or Latino people of any race consist of 22.22% of the population.
There were 30,374 households out of which 29.4% had children under the age of 18 living with them, 40.8% were married couples living together, 12.5% had a female householder with no husband present, and 42.0% were non-families. Thirty-two point six percent (32.6%) of all households are made up of individuals and 7.8% had someone living alone who was 65 years of age or older. The average household size was 2.50 and the average family size was 3.21.
In the census area, the population was spread out with 23.0% under the age of 18, 9.3% from 18 to 24, 37.0% from 25 to 44, 21.2% from 45 to 64, and 9.6% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 93.4 males. For every 100 females age 18 and over, there were 89.5 males.
The median income for a household in the census area was $51,653, and the median income for a family was $60,631. Males had a median income of $38,124 versus $36,096 for females. The per capita income for the area was $26,357. 9.3% of the population and 6.4% of families were below the poverty line. 11.7% of those under the age of 18 and 9.7% of those 65 and older were living below the poverty line.
2010.
"Note: For the 2010 Census the boundaries of the Silver Spring CDP were changed reducing the land area by approx. 15%. As a result, the population count for 2010 shows a 6.6% decrease, while the population density increases 11%."
As of the 2010 census, there were 71,452 people, 28,837 households, and 15,684 families residing in the Silver Spring CDP. The population density was 9,021.7 people per square mile (3,485.5/km²). There were 30,522 housing units at an average density of 3,853.8 per square mile (1,488.9/km²). The racial makeup of the community was 45.7% Caucasian, 27.8% Black American, 0.6% Indian American, 7.9% Asian, 0.1% Pacific Islander, 13.2% from other races, and 4.8% from two or more races. Hispanic or Latino people of any race consist of 26.3% of the population. Like much of the Washington, D.C. metropolitan area, Silver Spring's Hispanic population is mostly made up of Salvadorans and Guatemalans with other immigrants from Central America.
There were 28,603 households out of which 27.1% had children under the age of 18 living with them, 37.6% were married couples living together, 11.9% had a female householder with no husband present, and 45.2% were non-families. Thirty-three point six percent (33.6%) of all households are made up of individuals living alone and 16.5% had someone living alone who was 65 years of age or older. The average household size was 2.49 and the average family size was 3.21.
In the census area, the population was spread out with 21.4% under the age of 18, 9.4% from 18 to 24, 37.1% from 25 to 44, 23.8% from 45 to 64, and 8.4% who were 65 years of age or older. The median age was 33.8 years. For every 100 females there were 94.9 males. For every 100 females age 18 and over, there were 92.2 males.
The median income for a household in the census area was $71,986, and the median income for a family was $84,136. Males had a median income of $46,407 versus $49,979 for females. The per capita income for the area was $32,181. 15.0% (+/- 4.9%) of the population and 13.3% (+/-4.3%) of families were below the poverty line. 21% (+/- 9.1%) of those under the age of 18 and 23.6% (+/-10.6%) of those 65 and older were living below the poverty line.
History.
Nineteenth century.
The Blair, Lee, and Jalloh and Barrie families, three politically active families of the time, are irrefutably tied to Silver Spring's history. In 1840, Francis Preston Blair, who later helped organize the modern American Republican Party, along with his daughter, Elizabeth, discovered a spring flowing with chips of mica (the now-dry spring is still visible at Acorn Park). Two years later, he completed a twenty-room mansion he dubbed Silver Spring on a 250-acre (one-square-kilometer) country homestead situated just outside of Washington, D.C. (The house stood until 1954.) By 1854, Blair's son, Montgomery Blair, who became Postmaster General under Abraham Lincoln and represented Dred Scott before the United States Supreme Court, built the Falkland house in the area. By the end of the decade, Elizabeth Blair married Samuel Phillips Lee, third cousin of future Confederate leader Robert E. Lee, and gave birth to a boy, Francis Preston Blair Lee. The child would eventually become the first popularly elected Senator in United States history.
During the American Civil War, Abraham Lincoln visited the Silver Spring mansion multiple times. During some of the visits he relaxed by playing town ball with Francis P. Blair's grandchildren. In 1864, Confederate Army General Jubal Early occupied Silver Spring prior to the Battle of Fort Stevens. After the engagement, fleeing Confederate soldiers razed Montgomery Blair's Falkland residence. By the end of the 19th century, the region began to develop into a town of decent size and importance. The Baltimore & Ohio Railroad's Metropolitan Branch opened on April 30, 1873, and ran from Washington, D.C. to Point of Rocks, Maryland, through Silver Spring. The first suburban development appeared in 1887 when Selina Wilson divided part of her farm on current-day Colesville Road (U.S. Route 29) and Brookeville Road into five- and ten-acre (20,000- and 40,000 m²) plots. In 1893, Francis Preston Blair Lee and his wife, Anne Brooke Lee, gave birth to E. Brooke Lee, who is known as the father of modern Silver Spring for his visionary attitude toward developing the region.
Twentieth century.
The early 20th century set the pace for downtown Silver Spring's growth. E. Brooke Lee and his brother, Blair Lee I, founded the Lee Development Company, whose Colesville Road office building remains a downtown fixture. Dale Drive, a winding roadway, was built to provide vehicular access to much of the family's substantial real estate holdings. Suburban development continued in 1922 when Woodside Development Corporation created Woodside Park, a neighborhood of 1 acre plot home sites built on the former Noyes estate in 1923. In 1924, Washington trolley service on Georgia Avenue (present-day Maryland Route 97) across B&O's Metropolitan Branch was temporarily suspended so that an underpass could be built. The underpass was completed two years later, but trolley service never resumed. It would be rebuilt again in 1948 with additional lanes for automobile traffic, opening the areas to the north for readily accessible suburban development.
Takoma-Silver Spring High School, built in 1924, was the first high school for Silver Spring. The community's rapid growth led to the need for a larger school. In 1935, when a new high school was built at Wayne Avenue and Sligo Creek Parkway, it was renamed Montgomery Blair High School. (The school remained at that location for over six decades, until 1998, when it was moved to a new, larger facility at the corner of Colesville Road (U.S. Route 29) and University Boulevard (Maryland Route 193). The former high school building became a combined middle school and elementary school, housing Silver Spring International Middle School and Sligo Creek Elementary School.) The Silver Spring Shopping Center (built by developer Albert Small) and the Silver Theatre (designed by noted theater architect John Eberson) were completed in 1938, at the request of developer William Alexander Julian. The Silver Spring Shopping Center was unique because it was one of the nation's first retail spaces that featured a street-front parking lot. Conventional wisdom held that merchandise should be in windows closest to the street so that people could see it; the shopping center broke those rules (the shopping center was purchased by real estate developer Sam Eig in 1944 who was instrumental in attracting large retailers to the city).
By the 1950s, Silver Spring was the second busiest retail market between Baltimore and Richmond, with the Hecht Company, J.C. Penney, Sears, Roebuck and Company, and a number of other retailers. In 1954, after standing for over a century, the Blair mansion "Silver Spring" was razed and replaced with the Blair Station Post office. In 1960, Wheaton Plaza (later known as Westfield Wheaton), a shopping center several miles north of downtown Silver Spring opened, and captured much of the town's business. The downtown area soon started a long period of decline.
On December 19, 1961, a 2 mi segment of the Capital Beltway (I-495) was opened to traffic between Georgia Avenue (MD 97) and University Boulevard East (MD 193). On Monday, August 17, 1964, the final segment of the 64 mi Beltway was opened to traffic, and a ribbon-cutting ceremony was held near the New Hampshire Avenue interchange, with a speech by then-Gov. J. Millard Tawes, who called it a "road of opportunity" for Maryland and the nation.
Washington Metro rail service into Washington, D.C. helped breathe life into the region starting in 1978 with the opening of the Silver Spring station. The Metro Red Line was built following the alignment of the B&O Metropolitan Branch, with the Metro tracks centered between the B&O's eastbound and westbound mains. The Red Line heads south to downtown DC from Silver Spring, running at grade before descending into Union Station. By the mid-1990s, the Red Line continued north from the downtown Silver Spring core, entering a tunnel just past the Silver Spring station and running underground to three more stations, Forest Glen, Wheaton and Glenmont.
Nevertheless, the downtown decline continued in the 1980s, as the Hecht Company closed in 1987 and opened a new store at Wheaton Plaza. Furthermore, Hecht's added a covenant forbidding another department store from renting its old spot. City Place, a multi-level mall, was established in the old Hecht Company building in 1992, but it had trouble attracting quality anchor stores and gained a reputation as a budget mall, anchored by Burlington Coat Factory and Marshalls, as well as now-closed anchors AMC Theaters, Gold's Gym, Steve and Barry's, and Nordstrom Rack. JC Penney closed its downtown store—downtown's last remaining department store—in 1989, opening several years later at Wheaton Plaza. In the late 1980s and early 1990s, developers considered a shopping mall and office project called Silver Triangle, with possible anchor stores Nordstrom, Macy's, and JC Penney, but no final agreement was reached. Shortly thereafter, in the mid-1990s, developers considered building a mega-mall and entertainment complex called the American Dream (similar to the Mall of America) in downtown Silver Spring, but the revitalization plan fell through before any construction began because the developers were unable to secure funding. However, one bright spot for downtown was that the National Oceanographic and Atmospheric Administration (NOAA) consolidated its headquarters in a series of 4 new high-rise office buildings near the Silver Spring Metro station in the late 1980s and early 1990s.
Another notable occurrence in Silver Spring during the 1990s was a 1996 train collision on the Silver Spring section of the Metropolitan line. On February 16 of that year, during the Friday-evening rush hour, a MARC commuter train bound for Washington Union Station collided with the Amtrak "Capitol Limited" train and erupted in flames on a snow-swept stretch of track in Silver Spring, leaving eleven people dead.
The Maryland State Highway Administration started studies of improvements to the Capital Beltway in 1993, and have continued, off and on, examining a number of alternatives (including HOV lanes and high-occupancy toll lanes) since then.
Twenty-first century.
At the beginning of the 21st century, downtown Silver Spring began to see the results of redevelopment. Several city blocks near City Place Mall were completely reconstructed to accommodate a new outdoor shopping plaza called "Downtown Silver Spring." New shops included national retail chains such as Whole Foods Market, a 20-screen Regal Theatres, Men's Wearhouse, Ann Taylor Loft, DSW Shoe Warehouse, Office Depot, and the now-closed Pier 1 Imports, as well as many restaurants, including Romano's Macaroni Grill, Panera Bread, Red Lobster, Cold Stone Creamery, Fuddruckers, Potbelly Sandwich Works, Baja Fresh, Nando's, and Chick-fil-a. A Borders book store was a popular spot until it closed when the chain went out of business. In addition to these chains, Downtown Silver Spring is home to a wide variety of family-owned restaurants representing its vast ethnic diversity. As downtown Silver Spring revived, its 160-year history was celebrated in a PBS documentary entitled "Silver Spring: Story of an American Suburb", released in 2002. In 2003, Discovery Communications completed the construction of its headquarters and relocated to downtown Silver Spring from nearby Bethesda. The same year also brought the reopening of the Silver Theatre, as AFI Silver, under the auspices of the American Film Institute. Development continues with the opening of new office buildings, condos, stores, and restaurants, although City Place Mall continues to struggle to fill its vacancies despite the explosive growth around it. The restoration of the old B&O Passenger Station was undertaken between 2000 and 2002, as recorded in the documentary film "". In 2005 Downtown Silver Spring was awarded the Rudy Bruner Award for Urban Excellence silver medal.
Beginning in 2004, the downtown redevelopment was marketed locally with the "silver sprung" advertising campaign, which declared on buses and in print ads that Silver Spring had "sprung" and was ready for business. In June 2007, "The New York Times" noted that downtown was "enjoying a renaissance, a result of public involvement and private investment that is turning it into an arts and entertainment center".
In 2007, the downtown Silver Spring area gained attention when an amateur photographer was prohibited from taking photographs in what appeared to be a public street. The land, leased to the Peterson Cos., a developer, for $1, was technically private property. The citizens argued that the Downtown Silver Spring development, partially built with public money, was still public property. After a protest on July 4, 2007, Peterson relented and allowed photography on their property under limited conditions. Peterson also claimed that it could revoke these rights at any time. The company further stated that other activities permitted in public spaces, such as organizing protests or distributing campaign literature, were still prohibited. In response, Montgomery County Attorney Leon Rodriguez said that the street in question, Ellsworth Drive, "constitutes a public forum" and that the First Amendment's protection of free speech applies there. In an eight-page letter, Rodriguez wrote, "Although the courts have not definitively resolved the issue of whether the taking, as opposed to the display, of photographs is a protected expressive act, we think it is likely that a court would consider the taking of the photograph to be part of the continuum of action that leads to the display of the photograph and thus also protected by the First Amendment." The incident was part of a trend in the United States regarding the blurring of public and private spaces in developments built with both public and private funds.
In 2008, construction of the long-planned Intercounty Connector (ICC), which crosses the upper reaches of Silver Spring, got under way. The highway's first section opened on February 21, 2011; the entire route was completed by 2012.
In July 2010, the Silver Spring Civic Building and Veterans Plaza opened in downtown Silver Spring.
Culture.
Downtown Silver Spring hosts several entertainment, musical, and ethnic festivals, the most notable of which are the Silverdocs documentary film festival held each June and hosted by Discovery Communications and the American Film Institute, as well as the annual Thanksgiving Day Parade (Saturday before Thanksgiving) for Montgomery County. The Silver Spring Jazz Festival has become the biggest event of the year drawing 20,000 people to the free festival held on the second Saturday in September. Featuring local jazz artists and a battle of high school bands, the Silver Spring Jazz Festival has featured such jazz greats as Wynton Marsalis, Arturo Sandoval, Sérgio Mendes, Aaron Neville and such bands as the Mingus Big Band and the Fred Wesley Group.
Dining in Silver Spring is also extremely varied, including American, African, Burmese, Ethiopian, Moroccan, Italian, Mexican, Salvadoran, Jamaican, Vietnamese, Lebanese, Thai, Chinese, Indian, and fusion restaurants, as well as many national and regional chains.
The Fillmore is a live entertainment and music venue with a capacity of 2,000 people. It opened in 2011 in the former JC Penney building on Colesville Road. The venue joins the American Film Institute and Discovery Communications as cornerstones of the downtown Silver Spring's arts and entertainment district. It has featured performances by artists Prince Royce, Minus the Bear, Tyga and many other hip hop acts. In August 2012 R&B singer Reesa Renee launched her album "Reelease" at the Fillmore.
Silver Spring has many churches, synagogues, temples, and other religious institutions, including the World Headquarters of the Seventh-day Adventist Church. Silver Spring serves as the primary urban area in Montgomery County and its revitalization has ushered in an eclectic mix of people and ideas, evident in the fact that the flagship high school (Montgomery Blair High School) has no majority group with each major racial and ethnic group claiming a significant percentage.
Silver Spring hosts the American Film Institute Silver Theatre and Culture Center, on Colesville Road. The theatre showcases American and foreign films. Discovery Communications, a cable TV and satellite programming company, has its headquarters in downtown, as well. Gandhi Brigade, a youth development media project, began in Silver Spring out of the Long Branch neighborhood. Docs in Progress, a non-profit media arts center devoted to the promotion of documentary filmmaking is located at the "Documentary House" in downtown Silver Spring. Silver Spring Stage , an all-volunteer community theater, performs in Woodmoor, approximately 3 mi north up Colesville Road from the downtown area. Downtown Silver Spring is also home to the National Oceanic and Atmospheric Administration (NOAA), an agency of the United States Department of Commerce that includes the National Weather Service; the American Nurses Association; and numerous real estate development, biotechnology, and media and communications companies.
Transportation.
The major roads in Silver Spring are mostly 3 to 5-lane highways. The Capital Beltway can be accessed from Georgia Avenue (MD 97), Colesville Road (US 29), and New Hampshire Avenue (MD 650).
The long-planned Intercounty Connector (ICC) (designated Maryland Route 200) toll road was completed in early 2012. It has interchanges at Georgia Avenue, Layhill Road (MD 182), New Hampshire Avenue, Columbia Pike (as US 29 is known north of Lockwood Drive) and a half-interchange at Briggs Chaney Road.
Silver Spring is serviced by the Brunswick Line of the MARC Train, Metrorail Red Line, Metrobus, Ride On, and the free The bus terminal at the Silver Spring Rail Station is the busiest in the entire Washington Metro Area, and provides connections between several transit services, including those mentioned above. This transit facility serves nearly 60,000 passengers daily. 
Construction commenced in October 2008 on the new $91 million Paul S Sarbanes Transit Center which will further expand the station to facilitate the growing demand for public transportation, due to the increase in population in the Silver Spring area. The new center will be a multilevel, multimodal facility which will incorporate Metrobus, Ride On, Metrorail, MARC train, intercity Greyhound bus, and local taxi services under one roof. The project is over four years behind schedule and $50M over budget, with estimated completion in spring 2015. 
The Purple Line light rail, being studied by the Maryland Transit Administration (MTA) is planned to service this station, connecting Silver Spring with Bethesda to the west and then running east to the University of Maryland-College Park and then southeast to the New Carrollton Metro station. Construction was planned to begin in 2015, although as of January 2015 the project had not been fully approved.
In addition to the Silver Spring station, the Washington Metrorail's Forest Glen station is also located in Silver Spring and the MARC train also stops at the nearby Kensington station.
Education.
Montgomery County Public Schools.
Silver Spring is served by a county-wide public school system, Montgomery County Public Schools. Public high schools that serve the region include:
Of the public high schools in the region, Montgomery Blair High School is the only one within the Census Designation Place of Silver Spring. It is nationally recognized for its Communication Arts Program and its Science, Mathematics, and Computer Science Magnet Program, the latter of which perennially produces a large number of finalists and semi-finalists in such academic competitions as the Intel Science Talent Search. 
Notable private schools in the region include , , , Nora School, and The Barrie School.
Montgomery College.
A portion of the Montgomery College Takoma Park/Silver Spring campus is located within the Silver Spring boundary, with the rest of the campus located in Takoma Park. The community college is Montgomery County's main institute of higher education. (The main campus is in the county seat of Rockville.) Adjacent to the White Oak neighborhood in the outer reaches of Silver Spring is the campus of the National Labor College. 
Howard University.
Howard University also has its School of Continuing Education in Silver Spring (its main campus is located nearby in Washington, D.C.). Formerly Sanz School, Medtech Institute is located in Silver Spring and offers training in healthcare, but most notably English as a Second Language. Sanz School started offering English language education in 1939.
Libraries.
Silver Spring is served by at least five public libraries of which one – 
Silver Spring library start operation in 1931 and is one of the most heavily used in the Montgomery County system, is being renovated, enlarged, and relocated to Wayne Ave. and Fenton St. as part of the Silver Spring redevelopment plan.
Economy.
The following companies/agencies/organizations have their headquarters based in the Silver Spring CBD:
Sports.
The Silver Spring Saints Youth Football Organization has been a mainstay of youth sports in the town since 1951. Located in Silver Spring, Maryland, the Silver Spring Saints play home games at St. Bernadette's Church near Blair High School. The club was formed when two local Catholic parishes, St. John the Baptist and St. Andrews, merged their football programs to compete in the Capital Beltway League after the CYO (Catholic Youth Organization) for the Archdiocese of Washington D.C. discontinued its youth football program at the end of the 1994 season. The name "Saints" is derived from the merging of the two Catholic parishes. In 2009, the Saints moved from the Capital Beltway League (CBL) to the Mid-Maryland Youth Football & Cheer League (MMYFCL).
Silver Spring is also home to several MCSL swim teams, including Parkland, Robin Hood, Calverton, Franklin Knolls, Daleview, Oakview, Forest Knolls, Kemp Mill, Long Branch, Stonegate, Glenwood, Rock Creek, and Northwest Branch, Stonegate, Hillandale, and West Hillandale.
Silver Spring and Takoma Park together host Silver Spring-Takoma Thunderbolts a college wooden-bat baseball team playing in the Cal Ripken, Sr. Collegiate Baseball League. Home games are played at Montgomery Blair Stadium.
The Potomac Athletic Club Rugby team has a youth rugby organization based in Silver Spring. Established in 2005, PAC Youth Rugby has tag rugby for ages 5 thru 15, girls and boys and also offer introduction to tackle rugby for U13 and U15 players. In addition to introducing numerous young athletes to the sport of rugby, PAC has also won Maryland state championships across the age groups.
Media.
The area newspapers are the "The Gazette", "Washington Post," and the "Washington Times". Several online outlets also cover local Silver Spring news, including "The Voice" and Silver Spring Patch.
The "Washington Hispanic" has its offices in Silver Spring.

</doc>
<doc id="43486" url="http://en.wikipedia.org/wiki?curid=43486" title="Moulin Rouge">
Moulin Rouge

Moulin Rouge (], French for "Red Mill") is a cabaret in Paris, France.
The house was co-founded in 1889 by Charles Zidler and Joseph Oller, who also owned the Paris Olympia. Close to Montmartre in the Paris district of Pigalle on Boulevard de Clichy in the 18th "arrondissement", it is marked by the red windmill on its roof. The closest métro station is Blanche.
Moulin Rouge is best known as the spiritual birthplace of the modern form of the can-can dance. Originally introduced as a seductive dance by the courtesans who operated from the site, the can-can dance revue evolved into a form of entertainment of its own and led to the introduction of cabarets across Europe. Today, the Moulin Rouge is a tourist attraction, offering musical dance entertainment for visitors from around the world. The club's decor still contains much of the romance of "fin de siècle" France.
History.
Birth.
The Belle Époque was a period of peace and optimism marked by industrial progress, and a particularly rich cultural exuberance was about at the opening of the Moulin Rouge. The Expositions Universelles of 1889 and 1900 are symbols of this period. The Eiffel Tower was also constructed in 1889, epitomising the spirit of progress along with the culturally transgressive cabaret. Japonism, an artistic movement inspired by the Orient, with Henri de Toulouse-Lautrec as its most brilliant disciple, was also at its height. Montmartre, which, at the heart of an increasingly vast and impersonal Paris, managed to retain a bucolic village atmosphere; festivities and artists mixed, with pleasure and beauty as their values. On 6 October 1889, the Moulin Rouge opened in the Jardin de Paris, at the foot of the Montmartre hill. Its creator Joseph Oller and his Manager Charles Zidler were formidable businessmen who understood the public's tastes. The aim was to allow the very rich to come and 'slum it' in a fashionable district, Montmartre. The extravagant setting – the garden was adorned with a gigantic elephant – allowed people from all walks of life to mix. Workers, residents of the Place Blanche, artists, the middle classes, businessmen, elegant women and foreigners passing through Paris rubbed shoulders. Nicknamed "The First Palace of Women" by Oller and Zidler, the cabaret quickly became a great success.
The ingredients for its success:

</doc>
<doc id="43487" url="http://en.wikipedia.org/wiki?curid=43487" title="Probability density function">
Probability density function

In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function that describes the relative likelihood for this random variable to take on a given value. The probability of the random variable falling within a particular range of values is given by the integral of this variable’s density over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to one.
The terms ""probability distribution function" and "probability function"" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, "probability distribution function" may be used when the probability distribution is defined as a function over general sets of values, or it may refer to the cumulative distribution function, or it may be a probability mass function rather than the density. Further confusion of terminology exists because "density function" has also been used for what is here called the "probability mass function".
Example.
Suppose a species of bacteria typically lives 4 to 6 hours. What is the probability that a bacterium lives "exactly" 5 hours? The answer is actually 0%. Lots of bacteria live for "approximately" 5 hours, but there is negligible chance that any given bacterium dies at "exactly" 5.0000000000... hours.
Instead we might ask: What is the probability that the bacterium dies between 5 hours and 5.01 hours? Let's say the answer is 0.02 (i.e., 2%). Next: What is the probability that the bacterium dies between 5 hours and 5.001 hours? The answer is probably around 0.002, since this is 1/10th of the previous interval. The probability that the bacterium dies between 5 hours and 5.0001 hours is probably about 0.0002, and so on. 
In these three examples, the ratio (probability of dying during an interval) / (duration of the interval) is approximately constant, and equal to 2 per hour (or 2 hour−1). For example, there is 0.02 probability of dying in the 0.01-hour interval between 5 and 5.01 hours, and (0.02 probability / 0.01 hours) = 2 hour−1. This quantity 2 hour−1 is called the "probability density" for dying at around 5 hours.
Therefore, in response to the question "What is the probability that the bacterium dies at 5 hours?", a literally correct but unhelpful answer is "0", but a better answer can be written as (2 hour−1) "dt". This is the probability that the bacterium dies within a small (infinitesimal) window of time around 5 hours, where "dt" is the duration of this window. 
For example, the probability that it lives longer than 5 hours, but shorter than (5 hours + 1 nanosecond), is (2 hour−1)×(1 nanosecond) ≃ 6×10−13 (using the unit conversion 3.6×1012 nanoseconds = 1 hour).
There is a "probability density function" "f" with "f"(5 hours) = 2 hour−1. The integral of "f" over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window.
Absolutely continuous univariate distributions.
A probability density function is most commonly associated with absolutely continuous univariate distributions. A random variable "X" has density "fX", where "fX" is a non-negative Lebesgue-integrable function, if:
Hence, if "FX" is the cumulative distribution function of "X", then:
and (if "fX" is continuous at "x")
Intuitively, one can think of "fX"("x") d"x" as being the probability of "X" falling within the infinitesimal interval ["x", "x" + d"x"].
Formal definition.
A random variable "X" with values in a measurable space formula_4 
(usually Rn with the Borel sets as measurable subsets) has as probability distribution the measure "X"∗"P" on formula_4: the density of "X" with respect to a reference measure "μ" on formula_4 is the Radon–Nikodym derivative:
That is, "f" is any measurable function with the property that:
for any measurable set formula_9.
Discussion.
In the continuous univariate case above, the reference measure is the Lebesgue measure. The probability mass function of a discrete random variable is the density with respect to the counting measure over the sample space (usually the set of integers, or some subset thereof).
Note that it is not possible to define a density with reference to an arbitrary measure (e.g. one can't choose the counting measure as a reference for a continuous random variable). Furthermore, when it does exist, the density is almost everywhere unique.
Further details.
Unlike a probability, a probability density function can take on values greater than one; for example, the uniform distribution on the interval [0, ½] has probability density "f"("x") = 2 for 0 ≤ "x" ≤ ½ and "f"("x") = 0 elsewhere. 
The standard normal distribution has probability density
If a random variable "X" is given and its distribution admits a probability density function "f", then the expected value of "X" (if the expected value exists) can be calculated as
Not every probability distribution has a density function: the distributions of discrete random variables do not; nor does the Cantor distribution, even though it has no discrete component, i.e., does not assign positive probability to any individual point.
A distribution has a density function if and only if its cumulative distribution function "F"("x") is absolutely continuous. In this case: "F" is almost everywhere differentiable, and its derivative can be used as probability density:
If a probability distribution admits a density, then the probability of every one-point set {"a"} is zero; the same holds for finite and countable sets. 
Two probability densities "f" and "g" represent the same probability distribution precisely if they differ only on a set of Lebesgue measure zero.
In the field of statistical physics, a non-formal reformulation of the relation above between the derivative of the cumulative distribution function and the probability density function is generally used as the definition of the probability density function. This alternate definition is the following:
If "dt" is an infinitely small number, the probability that "X" is included within the interval ("t", "t" + "dt") is equal to "f"("t") "dt", or:
Link between discrete and continuous distributions.
It is possible to represent certain discrete random variables as well as random variables involving both a continuous and a discrete part with a generalized probability density function, by using the Dirac delta function. For example, let us consider a binary discrete random variable having the Rademacher distribution—that is, taking −1 or 1 for values, with probability ½ each. The density of probability associated with this variable is: 
More generally, if a discrete variable can take "n" different values among real numbers, then the associated probability density function is: 
where "x"1, …, "xn" are the discrete values accessible to the variable and "p"1, …, "pn" are the probabilities associated with these values.
This substantially unifies the treatment of discrete and continuous probability distributions. For instance, the above expression allows for determining statistical characteristics of such a discrete variable (such as its mean, its variance and its kurtosis), starting from the formulas given for a continuous distribution of the probability.
Families of densities.
It is common for probability density functions (and probability mass functions) to 
be parametrized—that is, to be characterized by unspecified parameters. For example, the normal distribution is parametrized in terms of the mean and the variance, denoted by formula_16 and formula_17 respectively, giving the family of densities
It is important to keep in mind the difference between the domain of a family of densities and the parameters of the family. Different values of the parameters describe different distributions of different random variables on the same sample space (the same set of all possible values of the variable); this sample space is the domain of the family of random variables that this family of distributions describes. A given set of parameters describes a single distribution within the family sharing the functional form of the density. From the perspective of a given distribution, the parameters are constants, and terms in a density function that contain only parameters, but not variables, are part of the normalization factor of a distribution (the multiplicative factor that ensures that the area under the density—the probability of "something" in the domain occurring— equals 1). This normalization factor is outside the kernel of the distribution. 
Since the parameters are constants, reparametrizing a density in terms of different parameters, to give a characterization of a different random variable in the family, means simply substituting the new parameter values into the formula in place of the old ones. Changing the domain of a probability density, however, is trickier and requires more work: see the section below on change of variables.
Densities associated with multiple variables.
For continuous random variables "X"1, …, "Xn", it is also possible to define a probability density function associated to the set as a whole, often called joint probability density function. This density function is defined as a function of the "n" variables, such that, for any domain "D" in the "n"-dimensional space of the values of the variables "X"1, …, "Xn", the probability that a realisation of the set variables falls inside the domain "D" is
If "F"("x"1, …, "x""n") = Pr("X"1 ≤ "x"1, …, "X""n" ≤ "x""n") is the cumulative distribution function of the vector ("X"1, …, "X""n"), then the joint probability density function can be computed as a partial derivative
Marginal densities.
For "i"=1, 2, …,"n", let "fXi"("xi") be the probability density function associated with variable "Xi" alone. This is called the “marginal” density function, and can be deduced from the probability density associated with the random variables "X"1, …, "Xn" by integrating on all values of the "n" − 1 other variables:
Independence.
Continuous random variables "X"1, …, "Xn" admitting a joint density are all independent from each other if and only if
Corollary.
If the joint probability density function of a vector of "n" random variables can be factored into a product of "n" functions of one variable
(where each "fi" is not necessarily a density) then the "n" variables in the set are all independent from each other, and the marginal probability density function of each of them is given by
Example.
This elementary example illustrates the above definition of multidimensional probability density functions in the simple case of a function of a set of two variables. Let us call formula_25 a 2-dimensional random vector of coordinates ("X", "Y"): the probability to obtain formula_25 in the quarter plane of positive "x" and "y" is
Dependent variables and change of variables.
If the probability density function of a random variable "X" is given as "fX"("x"), it is possible (but often not necessary; see below) to calculate the probability density function of some variable . This is also called a “change of variable” and is in practice used to generate a random variable of arbitrary shape using a known (for instance uniform) random number generator. 
If the function "g" is monotonic, then the resulting density function is
Here "g"−1 denotes the inverse function.
This follows from the fact that the probability contained in a differential area must be invariant under change of variables. That is,
or
For functions which are not monotonic the probability density function for "y" is
where "n"("y") is the number of solutions in "x" for the equation , and "g"−1"k"("y") are these solutions.
It is tempting to think that in order to find the expected value "E"("g"("X")) one must first find the probability density "f""g"("X") of the new random variable . However, rather than computing
one may find instead
The values of the two integrals are the same in all cases in which both "X" and "g"("X") actually have probability density functions. It is not necessary that "g" be a one-to-one function. In some cases the latter integral is computed much more easily than the former. See Law of the unconscious statistician.
Multiple variables.
The above formulas can be generalized to variables (which we will again call "y") depending on more than one other variable. "f"("x"1, …, "x""n") shall denote the probability density function of the variables that "y" depends on, and the dependence shall be . Then, the resulting density function is
where the integral is over the entire ("n"-1)-dimensional solution of the subscripted equation and the symbolic "dV" must be replaced by a parametrization of this solution for a particular calculation; the variables "x"1, …, "x""n" are then of course functions of this parametrization.
This derives from the following, perhaps more intuitive representation: Suppose x is an "n"-dimensional random variable with joint density "f". If , where "H" is a bijective, differentiable function, then y has density "g":
with the differential regarded as the Jacobian of the inverse of "H", evaluated at y.
Using the delta-function (and assuming independence) the same result is formulated as follows.
If the probability density function of independent random variables "Xi", are given as "fXi"("xi"), it is possible to calculate the probability density function of some variable . The following formula establishes a connection between the probability density function of "Y" denoted by "fY"("y") and "fXi"("xi") using the Dirac delta function:
Sums of independent random variables.
The probability density function of the sum of two independent random variables "U" and "V", each of which has a probability density function, is the convolution of their separate density functions:
It is possible to generalize the previous relation to a sum of N independent random variables, with densities "U"1, …, "UN":
This can be derived from a two-way change of variables involving "Y=U+V" and "Z=V", similarly to the example below for the quotient of independent random variables.
Products and quotients of independent random variables.
Given two independent random variables "U" and "V", each of which has a probability density function, the density of the product "Y"="UV" and quotient "Y"="U"/"V" can be computed by a change of variables.
Example: Quotient distribution.
To compute the quotient "Y"="U"/"V" of two independent random variables "U" and "V", define the following transformation:
Then, the joint density "p(Y,Z)" can be computed by a change of variables from "U,V" to "Y,Z", and "Y" can be derived by marginalizing out "Z" from the joint density.
The inverse transformation is
The Jacobian matrix formula_43 of this transformation is
Thus:
And the distribution of "Y" can be computed by marginalizing out "Z":
Note that this method crucially requires that the transformation from "U,V" to "Y,Z" be bijective. The above transformation meets this because "Z" can be mapped directly back to "V", and for a given "V" the quotient "U/V" is monotonic. This is similarly the case for the sum "U+V", difference "U-V" and product "UV".
Exactly the same method can be used to compute the distribution of other functions of multiple independent random variables.
Example: Quotient of two standard normals.
Given two standard normal variables "U" and "V", the quotient can be computed as follows. First, the variables have the following density functions:
We transform as described above:
This leads to:
This is a standard Cauchy distribution.

</doc>
<doc id="43488" url="http://en.wikipedia.org/wiki?curid=43488" title="Rave">
Rave

A rave (from the verb: "to rave") is a large party featuring performances by disc jockeys (DJs) and occasionally live performers playing electronic music, particularly electronic dance music (EDM). Music played at raves may include house, trance, acid house, electro house, progressive house, progressive trance, goa trance, breakbeat, acid breaks, breakbeat hardcore, drum and bass, hardcore techno, gabber, big beat, moombahcore, dancehall, industrial, jumpstyle, hardstyle, tropical house, ghetto house, jungle and other forms of electronic dance music. In rare cases, the term is used to refer to less electronic related genres such as glam rock, new wave, new rave, power pop, trip hop, psychedelic rock and dub. The music is amplified with a large sound reinforcement system, typically with large subwoofers to produce a deep bass sound. The music is accompanied by laser light shows, projected images, visual effects and smoke machines. While some raves may be small affairs held at clubs or private residences, some raves have grown to immense size, such as the Castlemorton Common Festival in 1992. Some electronic dance music festivals have features of raves, but on a large, often commercial scale.
History.
Origin of 'rave' (1950s-1970s).
In the late 1950s in London the term "Rave" was used to describe the "wild bohemian parties" of the Soho beatnik set. In 1958 Buddy Holly recorded the hit "Rave On," citing the madness and frenzy of a feeling and the desire for it to never end. The word "rave" was later used in the burgeoning mod youth culture of the early 1960s as the way to describe any wild party in general. People who were gregarious party animals were described as "ravers". Pop musicians such as Steve Marriott of The Small Faces and Keith Moon of The Who were self-described "ravers".
Presaging the word's subsequent 1980s association with electronic music, the word "rave" was a common term used regarding the music of mid-1960s garage rock and psychedelia bands (most notably The Yardbirds, who released an album in the US called "Having a Rave Up"). Along with being an alternative term for partying at such garage events in general, the "rave-up" referred to a specific crescendo moment near the end of a song where the music was played faster, heavier and with intense soloing or elements of controlled feedback. It was later part of the title of an electronic music performance event held on 28 January 1967 at London's Roundhouse titled the "Million Volt Light and Sound Rave". The event featured the only known public airing of an experimental sound collage created for the occasion by Paul McCartney of The Beatles – the legendary "Carnival of Light" recording.
With the rapid change of British pop culture from the mod era of 1963–1966 to the hippie era of 1967 and beyond, the term fell out of popular usage. During the 1970s and early 1980s until its resurrection, the term was not in vogue, one notable exception being in the lyrics of the song "Drive-In Saturday" by David Bowie (from his 1973 album "Aladdin Sane") which includes the line "It's a crash course for the ravers." Its use during that era would have been perceived as a quaint or ironic use of bygone slang: part of the dated 1960s lexicon along with words such as "groovy". The perception of the word changed again in the late 1980s when the term was revived and adopted by a new youth culture, possibly inspired by the use of the term in Jamaica.
Birth of techno and acid house (1980s).
In the mid to late 1980s a wave of psychedelic and other electronic dance music, most notably acid house-music and Techno, emerged and caught on in the clubs, warehouses, and free-parties first in Manchester in the mid 1980s and then later London. In many ways what would become known as the Rave scene, was influenced by the Northern Soul scene which throughout the late 1960s and through the 1970s and 1980s had involved large groups of mainly working class kids dancing all night to rare US soul records. With the end of the UK's textile industry in the northwest, suddenly large mills and warehouses became empty and unauthorized parties were held in them. The first warehouse parties in Manchester were organized by the group The Stone Roses back in 1985, when to get around the licensing laws they would play a gig and book a line up of DJs under the disused arches of Piccadilly train station.
These parties were then advertised as an all-night video shoot, and the kids who bought tickets for £5 would have a 1p piece sellotaped to the back as their fee for being extras in a video shoot, thus for several months the forces of law were kept at bay.
Dance music was always prominent with big electro, jazz funk and early house tunes being played in a somewhat balearic mix alongside New Order, The Clash and The Smiths. House music caught on very quickly in the north and midlands from 1986 onwards, even being played in mainstream night clubs. In 1988 London suddenly adopted this scene, and rebranded it, so records which a week earlier had been House Records, were suddenly Acid House-music and smiley badges and other marketing paraphernalia became involved. These early raves were called "Acid Music Parties".
In the late 1980s, the word "rave" was adopted to describe the subculture that grew out of the acid house movement. Activities were related to the party atmosphere of Ibiza, a Mediterranean island in Spain, frequented by British, Italian, Greek, Irish and German youth on vacation.
Growth of the scene (1990s-present).
By the 1990s, genres such as acid house, house music, oldschool jungle, techno, and electronica were all being featured at raves, both large and small. There were mainstream events that attracted thousands of people (up to 25,000 instead of the 4,000 that came to earlier warehouse parties). Acid House Music parties were first re-branded "rave parties" in the media, during the summer of 1989 by Neil Andrew Megson during a television interview, however, the ambience of the rave was not fully formed until 28 May 1991. In the UK, in 1988–89, raves were similar to football matches in that they provided a setting for working-class unification, in a time with a union movement in decline and few jobs, and many of the attendees of raves were die-hard football fans. In 1990 Rave came also underground in several cities as Berlin, Milan, Patras in basements, warehouses and forests.
British politicians responded with hostility to the emerging rave party trend. Politicians spoke out against raves and began to fine anyone who held unauthorized parties. Police crackdowns on these often-unauthorized parties drove the scene into the countryside. The word "rave" somehow caught on in the UK to describe common semi-spontaneous weekend parties occurring at various locations linked by the brand new M25 London Orbital motorway that ringed London and the Home Counties. (It was this that gave the band Orbital their name.) These ranged from former warehouses and industrial sites, in London, to fields and country clubs in the countryside. 
Characteristics.
Location.
Prior to the commercialization of the rave scene, where large legal venues became the norm for these events, the location of the rave was kept secret until the night of the event, usually being communicated through mobile messaging, secret flyers, and websites. This level of secrecy, necessary for avoiding any interference by the police, on account of the illicit drug use, enabled the ravers a location they could stay in for ten hours at a time. It promoted the sense of deviance and removal from social control. Today, this level of secrecy still exists in the underground rave scene. However "after-hours" clubs, as well as large outdoor events, create a similar type of alternate atmosphere, but focus much more on vibrant visual effects, such as props and décor.
Some raves make use of pagan symbolism. Modern raving venues attempt to immerse the raver in a fantasy-like world. Indigenous imagery and spirituality can be characteristic in the Raving ethos. In both the New Moon and Gateway collectives, "pagan altars are set up, sacred images from primitive cultures decorate the walls, and rituals of cleansing are performed over the turntables and the dance floor" This type of spatial strategy is an integral part of the raving experience because it sets the initial "vibe" in which the ravers will immerse themselves. This said "vibe" is a concept in the raver ethos that represents the allure and receptiveness of an environment's portrayed and or innate energy. The geographical landscape is an integral feature in the composition of rave, much like it is in pagan rituals. For example, The Numic Ghost Dancers rituals would be held on specific geographical landscapes, considered to hold powerful natural flows of energy. These sites were later represented in the rhythmic dances, in order to achieve a greater level of connectivity.
Notable venues.
The following is an incomplete list of venues associated with the rave subculture:
Dancing.
A sense of participation in a group event is among the chief appeals of rave music and dancing to pulsating beats is its immediate outlet. Raving in itself is a syllabus-free dance, whereby the movements are not predefined and the dance is performed randomly, dancers take immediate inspiration from the music and their mood. Rave dance refers to the street dance styles that evolved alongside rave culture. Such dances are street dances since they evolved alongside the underground rave movements, thus without the intervention of dance studios. Sometimes club-oriented dances would be danced to rave music, too, for example, tecktonik is sometimes danced to fast-paced electro house.
Such dances are usually freestyle in nature, since they are very rarely choreographed in preparation for such events (although some ravers may create personal dance routines). Dances like Jumpstyle, Tecktonik, Liquid and digits, Melbourne Shuffle and Industrial dance may be sometimes highly dependent on pre-planned choreography for performances at raves, therefore such dance styles may be practised professionally. Nonetheless, rave dance styles can be completely freeform due to their simple footwork and arm movements.
Attire.
The loose, casual and sports clothing was originally adopted by the acid house set earlier on in Ibiza, utilizing easy-to-dance-in attire from hip hop and football/soccer culture. As well as clothing there developed a range of accessories carried by many ravers including: Vicks VapoRub, which ravers find pleasant under the influence of MDMA, pacifiers to satiate the need to grind one's teeth (bruxism) caused by taking MDMA, and glow sticks which adjunct the mild psychedelia of MDMA's effect. This led some clubs and event organizers to search participants on entry and confiscate such items due to it being evidence of drug use inside the venue.
Recent global rave events such as Sensation have a strict minimalistic dress policy, either all white or black attire. This ties in with the initial PLUR approach upheld from earlier rave culture. In the United States, rave fashion is characterized by colorful clothing and accessories, most notably "kandi" jewellery, that fluoresce under ultraviolet light. In European countries, this kandi culture is much less common. Most raves are illegal and take place outside or in poorly heated warehouses, so keeping warm is a priority. Dreadlocks, dyed hair and mohawks are popular, as are tattoos and piercings. Clothing is vibrant and alternative, often taking inspiration from new-age punk and grunge style. However, there is no set dress code for the illegal rave scene.
Since rave culture has seen such an explosion in the US since 2010 as the rave scene is no longer illegal or underground, raves in the US are now so popular that there are many brands, retailers, and websites selling apparel, costumes, and accessories just for those who go to dress up at raves. This style of attire, along with the entire rave culture, is now spilling out into the mainstream, especially in the US. Sometimes called "rave fashion" or "festival fashion," it now includes all kinds of accessories to create unique looks depending on the person and event. Items such as jewelry, body chains, temporary tattoos, furry leg warmers, sunglasses, fanny packs, pasties, light up items, spirit hoods, and much more can be seen at any major rave event around the US and globe. There is also been a recent trend in the use of Diffraction and Kaleidoscope Glasses in the festival industry.
Light shows.
Some ravers participate in one of four light-oriented dances, called "glowsticking", "glowstringing", "gloving", and "lightshows". Of the four types of light-orientated dances, gloving in particular has evolved beyond and outside of the rave culture. Other types of light-related dancing include LED lights, flash-lights and blinking strobe lights. LEDs come in various colours with different settings.
Gloving has evolved into a separate dance form that has grown exponentially in the last couple of years while still keeping its rave roots. The origins of gloving is often credited to Hermes who put together 10 Rav'n lights into a pair of white gloves in 2006. Since then the culture has extended to all ages, ranging from kids in their early teens to college students and more. The traditional Rav'n lights are limited now, but many stores have developed newer, brighter, and more advanced version of lights with a plethora of colors and modes—modes include solid, stribbon, strobe, dops, hyper flash, and other variations.
What was once an extension of the rave culture has now blossomed into a hobby or a form of dance. Annual competitions such as the International Gloving Competition and monthly B.O.S.S competitions hosted by EmazingLights exemplifies its growing popularity as thousands flock to those events. Past winners of International Gloving Competitions includes Munch and Thumper. Even though gloving originated in Southern California, it can now be seen in Northern California, Florida, New York, and many other states in the US. In college, you can see a gloving club called Ambience, which has spread into University of California Irvine, University of California Davis, University of California Berkeley, University of California Santa Barbara, University of California San Diego.
Drug use.
In the U.S., law enforcement agencies have branded the subculture as a purely drug-centric culture, usually drugs such as Marijuana, MDMA, 2CB, LSD, DMT, Amphetamine and Ketamine, similar to the hippie movement of the 1960s.
Groups that have addressed alleged drug use at raves include the Electronic Music Defense and Education Fund (EMDEF), The Toronto Raver Info Project, and DanceSafe, all of which advocate harm reduction approaches.
In 2005, Antonio Maria Costa, Executive Director of the United Nations Office on Drugs and Crime, advocated drug testing on highways as a countermeasure against drug use at raves.
Rave history by region.
Continental Europe.
By 1987, a German party scene, started by Tauseef Alam, based on the Chicago House sound was well established. The following year (1988) saw acid house making as significant an impact on popular consciousness in Germany and Central Europe as it had in England. In 1989 German DJs Westbam and Dr. Motte established the Ufo Club, an illegal party venue, and co-founded the Love Parade. On 9 November 1989 the Berlin Wall fell, free underground Techno parties mushroomed in East Berlin, and a rave scene comparable to that in the UK was established. East German DJ Paul van Dyk has remarked that the Techno-based rave scene was a major force in re-establishing social connections between East and West Germany during the unification period.
In 1991 a number of party venues closed, including Ufo, and the Berlin Techno scene centred itself around three locations close to the foundations of the Berlin Wall: the "E-Werk", "Der Bunker" and the now legendary "Tresor". In the same period, German DJs began intensifying the speed and abrasiveness of the sound, as an acid-infused techno began transmuting into hardcore. This emerging sound was influenced by Dutch gabber and Belgian hardcore. Other influences on the development of this style were European Electronic Body Music groups of the mid-1980s such as DAF, Front 242, and Nitzer Ebb.
Across Europe, rave culture was becoming part of a new youth movement. DJs and electronic-music producers such as Westbam proclaimed the existence of a "raving society" and promoted electronic music as legitimate competition for rock and roll. Indeed, electronic dance music and rave subculture became mass movements. Raves had tens of thousands of attendees, youth magazines featured styling tips, and television networks launched music magazines on House and Techno music. The annual Love Parade festivals in Berlin (in the Metropolitan Ruhr area onwards) attracted more than one million party-goers between 1997 and 2000. Meanwhile, the more commercial sound of happy hardcore topped the music charts across Europe. Nowadays there are only a few popular raving acts on the case in Germany, but many underground acts in Berlin and Frankfurt (Main). That is why Berlin (especially the east side) is still called the capital city of electro music and rave. Although electro composer Paul Kalkbrenner from Friedrichshain, Berlin made "Berlin Techno" world popular again, he is touring on his Berlin Calling (named after the movie he acted the main character and the soundtrack he produced for) tour through Europe and America.
United Kingdom.
Birth of UK rave scene (1980s-1990s).
The UK was finally recognized for its rave culture around the late 1980s early 1990s. EXODUS collective which was founded in Luton famously known for London Luton airport and hat manufacturing. Exodus played a big part in the UK's rave scene today. 
By 1991, organisations such as Fantazia, Universe, ", Raindance and Amnesia House were holding massive legal raves in fields and warehouses around the country. One Fantazia party, called One Step Beyond, was an open-air, all-night affair that attracted 30,000 people. Other notable events included Vision at Pophams airfield in August 1992, with 40,000 in attendance and Universe's Tribal Gathering in 1993.
In the early 1990s, the scene was slowly changing, with local councils passing by-laws and increasing fees in an effort to prevent or discourage rave organisations from acquiring necessary licenses. This meant that the days of legal one-off parties were numbered. By the mid-1990s, the scene had fragmented into many different styles of dance music, making large parties more expensive to set up and more difficult to promote. The happy old skool style was replaced by the darker jungle and the faster happy hardcore. Although many ravers left the scene due to the split, promoters such as ESP Dreamscape and Helter Skelter still enjoyed widespread popularity and capacity attendances with multi-arena events catering to the various genres. Particularly notable events of this period included ESP's Dreamscape 20 on 9 September 1995 at Brafield aerodrome fields, Northants and Helter Skelter's Energy 97 event on 9 Aug 1997 at Turweston Aerodrome, Northants.
Free parties and outlawing of raves (1992-1994).
The illegal free party scene also reached its zenith for that time after a particularly large festival, when many individual sound systems such as Bedlam, Circus Warp, DIY, and Spiral Tribe set up near Castlemorton Common. In May 1992, the government acted. Under the "Criminal Justice and Public Order Act 1994", the definition of music played at a rave was given as:
"Music" includes sounds wholly or predominantly characterised by the emission of a succession of repetitive beats.<br>          –Criminal Justice and Public Order Act 1994
Sections 63, 64 & 65 of the Act targeted electronic dance music played at raves. The Criminal Justice and Public Order Act empowered police to stop a rave in the open air when a hundred or more people are attending, or where two or more are making preparations for a rave. Section 65 allows any uniformed constable who believes a person is on their way to a rave within a five-mile radius to stop them and direct them away from the area; non-compliant citizens may be subject to a maximum fine not exceeding level 3 on the standard scale (£1000). The Act was officially introduced because of the noise and disruption caused by all night parties to nearby residents, and to protect the countryside. However, some participants in the scene claimed it was an attempt to lure youth culture away from MDMA and back to taxable alcohol. In November 1994, the Zippies staged an act of electronic civil disobedience to protest against the CJB (i.e., Criminal Justice Bill).
Legal and underground raves (1994-present).
After 1993, the main outlet for raves in the UK were a number of licensed venues, amongst them Helter Skelter, Life at Bowlers (Trafford Park, Manchester), The Edge (formerly the Eclipse [Coventry]), The Sanctuary (Milton Keynes) and Club Kinetic. In London, itself, there were a few large clubs that staged raves on a regular basis, most notably "The Laser Dome", "The Fridge", "The Hippodrome", "Club U.K.", and "Trade." "The Laser Dome" featured two separate dance areas, "Hardcore" and "Garage", as well as over 20 video game machines, a silent-movie screening lounge, replicas of the "Statue of Liberty", "San Francisco Bridge", and a large glass maze. At capacity "The Laser Dome" held in excess of 6,000 people. Events proved to be one of the main forces in rave, holding legendary events across the north-east and Scotland. Initially playing Techno, Breakbeat, Rave and drum and bass, it later embraced hardcore techno including happy hardcore and bouncy techno. Judgement Day, History of Dance, and now REGENeration continued the Rezerection legacy. Scotland's clubs, such as the FUBAR in Stirling, Hangar 13 in Ayr, and Nosebleed in Rosyth played important roles in the development of these dance music styles.
These were nearly all pay-to-enter events; however, it could be argued that rave organisers saw the writing on the wall and moved towards more organised and "legitimate" venues, enabling a continuation of large-scale indoor raves well into the mid-nineties. One might remember that the earliest house and acid house clubs were themselves effectively "nightclubs". Public perception of raves was also overshadowed in the press by the 1995 death of Leah Betts, a teenager who died after taking MDMA; journalists and billboard campaigns focused on drug use, despite Betts cause of death being water intoxication in her home, not an MDMA overdose at a rave.
In London, the warehouse party scene has made a revival, with many large clubs closing, popular DJs are playing in abandoned car parks, warehouses, factories etc. Many put this down to the recession, nightclubs and bars being less affordable than in the past few years, a similar situation to the late 1980s and early 1990s when House Music and rave took off.
Genuine illegal raves have continued throughout the UK to this day and unlicensed parties have been organised in venues including disused quarries, warehouses, and condemned night clubs. The rise of the Internet has both helped and hindered the cause, with much wider and more accessible communication resulting in bigger parties, but consequently increasing the risk of police involvement.
North America.
Origins in disco and psychedelia (1970s).
American ravers, following their early UK & European counterparts, have been compared to the hippies of the 1960s due to their shared interest in non-violence and psychedelia.
Rave culture incorporated disco culture's same love of dance music, drug exploration, sexual promiscuity, and hedonism. Although disco culture had thrived in the mainstream, the rave culture would make an effort to stay underground to avoid the animosity that was still surrounding disco and dance music.
New York raves and party promoters (1980s).
In the late 80s, rave culture began to filter through from English ex-pats and DJs who would visit Europe. However, rave culture's major expansion in North America is often credited to Frankie Bones, who after spinning a party in an aircraft hangar in England helped organize some of the earliest known American raves in the 1990s in New York City called "Storm Raves" which maintained a consistent core audience, fostered also by zines like fellow Storm Dj (and co-founder, with Adam X and Frankie Bones, of the US' first techno record store Groove Records) Heather Heart's Under One Sky. Simultaneously in NYC, events called NASA were introducing electronic dance music to New York. In 1993, P.A.W.N. Lasers from Pennsylvania produced the 1st electronic Dance Festival "IMPACT" with Josh Wink, Superstar DJ Keoki in PA, and then later became the most well known laser company at raves in East Coast by cross-promoting these rave events State to State as far south as Florida and Louisiana. After this, hundreds of smaller promotional groups sprung up across the east coast such as Ultraworld (MD,DC), Park Rave Madness (NYC), G.O. Guaranteed Overdose (NYC), Local 13 (NJ), Caffeine (NYC), Liquid Grooove aka Liquified (GA), Columns of Knowledge (CT), Special K aka Circle Management (PA), Zen Festivals (FL), Disco Donnie (LA), Ultra Music Festival (FL), and later the west coast, causing a true "scene" to develop.
San Diego and Latin America (1990s).
In the 1990s, one of the most influential Rave organisers / promoters in America was San Diego's, Global Underworld Network. They were made famous for organizing and throwing the OPIUM and NARNIA raves that reached in size of 60,000 plus people in attendance, a feat unheard of at that time. Narnia which would become famous for a morning hand holding circle of unity was featured on MTV and twice in LIFE magazine being honored with Event of the Year in 1995. Narnia became known as the "Woodstock of Generation X" & Nicholas Luckinbill and Branden Powers of G.U.N. have been called the Merry Pranksters of the Rave scene. These festivals were mostly held on Indian Reservations and Ski Resorts during the Summer months and were headlined by well-known DJs such as Doc Martin, Dimitri of Deee-lite, Afrika Islam and the Hardkiss brothers from San Francisco. They were instrumental in creating the RIGHT TO DANCE movement—a non violent protest held in San Diego and later in Los Angeles on the steps of City Hall which aimed to demonstrate that rave culture was about community, peace and love.
Featuring local San Diego DJ's Jon Bishop, Steve Pagan, Alien Tom, Jeff Skot and Mark E. Quark, Global Underworld's events were the first prop-heavy, themed parties in America. They were also the first production company to throw Raves within Mexico, thus launching the entire rave culture movement within South America. The iconic fairy and pixie craze with ravers getting fairy tattoos and wearing wings to parties likely started from an image of a winged fairy on the first Narnia flyer. The Crystal Method played their first out of town show for G.U.N.'s Universary event. Fearing reprisals from the police the event was advertised as "A thousand Points of Light" referring to the power of healing crystals of the Crystal Methods name. This tickled the upcoming artists so much they would refer to it years later in their biography.
The communal space hosting the G.U.N. office amongst many others—something of a Waco meets Warhol in the MIT media lab—was a crossroads of the scene. This vibrant, weird, & chaotic top 3 stories of a building in downtown San Diego, unceremoniously known as "The Loft", grew out of an unlikely collaboration between Alabama yoga guru Murshid Van Merlin, hackers Jerry Lugert & Bill Huey, & Sin Magazine editor Chris Howland. The mythology goes that Howland met Lugert at a Denny's, jumped on the back of a Harley, and was blindfolded for a windy ride deep into Rancho Santa Fe. Awestruck by weirdness, Sin Magazine's warehouse office was soon thereafter offered to & annexed by the cult which was being driven from the upscale estate by neighbors whom were not fans of the late night electronic music arts. In contrast to the commercial oriented mega raves, the Loft hosted intimate parties over the years & provided an art & technology incubator for thousands in the SD underground scene of the 1990s. The percussive group Crash Worship in particular, sometimes working out of the Loft, mechanistically generated the essence of techno tribal dionysian abandon of which the raves scene is rooted. This scene marks the post-industrial, pre-rave period of tranced out dance parties in the U.S.. Symbolic & predictive of the changes to come, they were known to march on & raid the early Burning Man rave camps, analog & un-amplified, to take the DJ hostage.
Adults are often active members of the U.S. scene and are well represented at events. Certain facets of dance music culture in the UK, Europe and globally, are also welcoming to the older generation (especially the free party/squat party/gay scenes). However, rave and club culture remains on the whole very much a youth-driven movement in terms of its core fan base. Although rave parties are commonly associated with warehouse break-ins & such, raves themselves are more often considered to be legal, & often commercial gatherings in recent times.
Growth in California.
In late 80s and early 90s, there was a boom in rave culture in the Bay Area. At first, small underground parties sprung up all over the SOMA district in vacant warehouses, loft spaces, and clubs like DV8 and 1015 Folsom, and basement of Jessie Street that had permits to run to 6am as long as no alcohol was served. The no alcohol rule fueled the ecstasy-driven parties to a much larger crowd, and soon followed were the first large scale raves. Every weekend a few hundred would show up at venues like the Townsend warehouse, the King Street garage, and other mid-size warehouse's located in the SOMA and south San Francisco area.
Rave crews started to become famous not only for their quality of music and the smoothness of the parties thrown but also for the 'vibe'. Crews grew to legendary status at this time: 'The Gathering', 'Toontown', 'Wiked', 'Rave Called Sharon', 'The Church', and 'Osmosis'. Small underground raves were just starting out and expanding beyond SF to include the east bay, the south bay area including San Jose, Santa Clara, and Santa Cruz beaches (where the notorious 'full moon raves' took place at Bonny Dune beach every month).
In late 1991 raves started to explode across northern California, and cities like Sacramento, Oakland, Silicon Valley (Palo Alto, San Jose) were taking off every weekend. This proved to be the turning point in northern California's rave history. No longer were raves a secret, where one had to know the right people to gain access to map points. Now rave flyers were to be found up and down Haight Street, at stores like Anubis Warpus, Amoeba, Behind The Post Office, and newly opened Housewares. Raves were exploding at an enormous rate, and now there were thousands of ravers living for every weekend. The second generation of raves were just starting to be realized.
Toontown's NYE 91 rave, which took place in the basement of the Fashion Center in SF was the first massive in the Bay Area. Over 8,000 people helped welcome in the new year and at the same time put SF as a must-visit city for the burgeoning world-wide rave scene. Similarly, a year later, 'The Gathering' held New Year's Eve of 1992 in Vallejo had over 12,000 people in attendance. Security measures could not stop the influx of people as they were hopping gates and climbing over walls. The massive parties were taking place every weekend now from such disparate locations as outdoor fields to airplane hangars and hilltops that surround the valley.
San Francisco has long been a Mecca for ravers from all over the world and true to form a lot of the early promoters and DJs were from the UK and Europe. For almost ten years after the initial raves took place, one could find up to 2 to 4 parties happening a weekend and sometimes on the same night. There was no curfew in place, which allowed the SF scene to explode by the late 1990s when venues would have up to 20,000 people every weekend; 'Homebase', and '85 & Baldwin' were the largest venues to be used in the Bay Area. Many amazing venues were used by crews that held clout or members that were tied to the city or knew the appropriate ways to navigate the permit maze. Thus, in the late 1990s some of the most memorable raves took place in locations such as the SOMA art museum, 'Where the wild things are' museum on top of the Sony Metreon, and in the venerable Maritime hall that was used for many parties from 1998-2002. Some old locations appeared again brand new, such as the concourse that saw thousands of ravers in 1992, now saw the same amount in late 1999. The galleria that once held a 'concert' in 92 with artists such as Moby, Aphex twin, Prodigy, Space time continuum, was now used for a few one-off events that utilized all 5 floors of the building with a different music style on each floor.
The mid part of the 1990s saw a general loss of the first generation of ravers, causing the scene to take a short dive. In this time, however, a new West coast sound was formed and developed by DJs such as Jeno, Tony, Spun, Galen, Solar, Harry Who?, Rick Preston to name but a few. Venues and parties such as Stompy, Harmony, CloudFactory, Cyborganic lounge, Acme warehouse among many others started to fuse the Breakbeat sound from hardcore trax with the more melodic pace of house. West coast funky break-beat was born from this and stormed the dance scene. By the end of '94 all the people that had left a gap in the rave scene in '93 were long forgotten as twice as many people now found the new sounds completely and utterly funky. The LA Scene had promoters such as Vince Bannon and Phil Blaine throw gigs for Electronic acts like 808 State, Aphex Twin, Prodigy, and Massive Attack to name a few.
This time period saw the rise of the many facets of EDM. It came to be that many genres of electronic dance music could be enjoyed by anyone willing to go out to any of these parties. Raves could be found in many different kinds of venues, as opposed to just basements and warehouses. Promoters started to take notice and put together the massives of the late 1990s with many music forms under one roof for 12 hour events. Parties were known to attract tens of thousands in venues like Homebase or 85th/Baldwin for a night of continuous dancing. San Francisco became a notorious destination for raves in the United States, and to a lesser extent, the world at large. DJs from all corners of the globe began performing in San Francisco.
The year 2000 saw the demise of massive raves as curfews were placed on permits handed out to promoters throwing parties. Instead of all night and into the next day, parties now had to end at 2 a.m. Two of the largest venues closed down soon after, and there wasn't enough momentum to sustain parties that catered to tens of thousands of people. As if a nail was drove into the coffin of the SF rave scene, the Homebase warehouse that held parties from 1996-2000 burned down to the ground in a spectacular six alarm fire in 2004. Smaller, intimate venues continued just like they had from the start and underground raves became the norm in the years after the tech boom of the 1990s.
While San Francisco's crowd attendance and variety of DJs might have peaked, it still maintains a much smaller but dedicated cadre of various crews, DJs, promoters and producers. Every weekend, many events are still dedicated to the various forms of electronic music across the greater Bay Area.
Seattle raves.
Through the mid 1990s and into the 2000s the city of Seattle also shared in the tradition of West Coast rave culture. Though a smaller scene compared to San Francisco, Seattle also had many different rave crews, promoters, Djs, and fans. Candy Raver style, friendship and culture became particularly popular in the West Coast rave scene, both in Seattle and San Francisco. At the peak of West Coast rave, Candy Raver, and massive rave popularity (1996-1999,) it was common to meet groups of ravers, promoters, and Djs who frequently travelled between Seattle and San Francisco, which spread the overall sense of West Coast rave culture and the phenomenon of West Coast "massives".
Recent years (2000s).
By 2010, raves were becoming the equivalent of large-scale rock music festivals, but many times even bigger and more profitable. The Electric Daisy Carnival in Las Vegas drew more than 300,000 fans over three days in the summer of 2012, making it the largest EDM music festival in North America. Ultra Music Festival in Miami drew 150,000 fans over three days in 2012 while other raves like Electric Zoo in New York, Beyond Wonderland in LA, Movement in Detroit, Electric Forest in Michigan, Spring Awakening in Chicago, and dozens more now attract hundreds of thousands of “ravers” every year.
Almost all of these new EDM-based rave events (now simply referred generically to as “music festivals”) sell out, proving that the market is there and willing to pay money to be a part of this new culture. Festival attendance at the Electric Daisy Carnival (EDC) increased by 39.1%, or 90,000 attendees from 2011 to 2012. In 2013, EDC had attendance of approximately 345,000 people, a record for the festival. The average ticket for EDC cost over $300 and the event contributed $278 million to the Clark County economy in 2013. This festival takes place at a 1,000-acre complex featuring a half dozen custom built stages, enormous interactive art installations, and hundreds of EDM artists.
Insomniac, one of the largest EDM event promoters in the US, has been throwing yearly EDC and other EDM events around the country for over 20 years now, and is a good example of how far “raves” have come in the US.
Australia.
Rave parties began in Australia as early as the 1980s and continued well into the late 1990s. They were mobilised versions of the 'warehouse parties', across Britain. Similar to the United States and Britain, raves in Australia were unlicensed and held in spaces normally used for industrial and manufacturing purposes, such as warehouses, factories and carpet showrooms. In addition, suburban locations were also used: basketball gymnasiums, train stations and even circus tents were all common venues. In Sydney, common areas used for outdoor events included Sydney Park, a reclaimed garbage dump in the inner south west of the city, Cataract Park and various other natural, unused locations and bush lands. The raves placed a heavy emphasis on the connection between humans and the natural environment, thus many raves in Sydney were held outdoors, notably the 'Happy Valley' parties (1991-1994), 'Ecology' (1992) and 'Field of Dreams 4' (July 6, 1996). The tradition continued in Melbourne, with 'Earthcore' parties staged in the cities hinterland. The mid-late 1990s saw a slight decline in rave attendance, attributed to the death of Anna Wood at an inner-city Sydney venue, which was hosting a rave party known as "Apache". Wood had taken ecstasy and died in hospital a few days later, leading to extensive media exposure on the correlation of drug culture and its links to the rave scene. Nonetheless, the rave scene in Australia experienced a brief resurgence in the early 2000s to late 2005.
During this period the resurfacing of the Melbourne Shuffle became a huge YouTube sensation and many videos have been uploaded. 
The Sub coulter or Raves with in Melbourne was taken to new heights with the opening of clubs such as Bass Station and Hard candy. Melbourne had a huge influence on the world stage for keeping Raving culture in with young people and set a new type of music designed for the Shuffle.
Notable raves.
The following is an incomplete list of notable raves, particularly smaller raves that may not fit the profile of being an electronic dance music festival:
Notable soundsystems.
The following is an incomplete list of notable sound systems:

</doc>
<doc id="43490" url="http://en.wikipedia.org/wiki?curid=43490" title="Windmill">
Windmill

A windmill is a mill that converts the energy of wind into rotational energy by means of vanes called sails or blades. Centuries ago, windmills usually were used to mill grain, pump water, or both. Thus they often were gristmills, windpumps, or both. The majority of modern windmills take the form of wind turbines used to generate electricity, or windpumps used to pump water, either for land drainage or to extract groundwater.
Windmills in antiquity.
The windwheel of the Greek engineer Heron of Alexandria in the first century AD is the earliest known instance of using a wind-driven wheel to power a machine. Another early example of a wind-driven wheel was the prayer wheel, which was used in ancient Tibet and China since the fourth century.
It has been claimed that the Babylonian emperor Hammurabi planned to use wind power for his ambitious irrigation project in the seventeenth century BC.
Horizontal windmills.
The first practical windmills had sails that rotated in a horizontal plane, around a vertical axis. According to Ahmad Y. al-Hassan, these panemone windmills were invented in eastern Persia as recorded by the Persian geographer Estakhri in the ninth century. The authenticity of an earlier anecdote of a windmill involving the second caliph Umar (AD 634–644) is questioned on the grounds that it appears in a tenth-century document. Made of six to 12 sails covered in reed matting or cloth material, these windmills were used to grind grain or draw up water, and were quite different from the later European vertical windmills. Windmills were in widespread use across the Middle East and Central Asia, and later spread to China and India from there.
A similar type of horizontal windmill with rectangular blades, used for irrigation, can also be found in thirteenth-century China (during the Jurchen Jin Dynasty in the north), introduced by the travels of Yelü Chucai to Turkestan in 1219.
Horizontal windmills were built, in small numbers, in Europe during the 18th and nineteenth centuries, for example Fowler's Mill at Battersea in London, and Hooper's Mill at Margate in Kent. These early modern examples seem not to have been directly influenced by the horizontal windmills of the Middle and Far East, but to have been independent inventions by engineers influenced by the Industrial Revolution.
Vertical windmills.
Due to a lack of evidence, debate occurs among historians as to whether or not Middle Eastern horizontal windmills triggered the original development of European windmills. In northwestern Europe, the horizontal-axis or vertical windmill (so called due to the plane of the movement of its sails) is believed to date from the last quarter of the twelfth century in the triangle of northern France, eastern England and Flanders.
The earliest certain reference to a windmill in Europe (assumed to have been of the vertical type) dates from 1185, in the former village of Weedley in Yorkshire which was located at the southern tip of the Wold overlooking the Humber estuary. A number of earlier, but less certainly dated, twelfth-century European sources referring to windmills have also been found.
These earliest mills were used to grind cereals.
Post mill.
The evidence at present is that the earliest type of European windmill was the post mill, so named because of the large upright post on which the mill's main structure (the "body" or "buck") is balanced. By mounting the body this way, the mill is able to rotate to face the wind direction; an essential requirement for windmills to operate economically in north-western Europe, where wind directions are variable. The body contains all the milling machinery. The first post mills were of the sunken type, where the post was buried in an earth mound to support it. Later, a wooden support was developed called the trestle. This was often covered over or surrounded by a roundhouse to protect the trestle from the weather and to provide storage space. This type of windmill was the most common in Europe until the nineteenth century, when more powerful tower and smock mills replaced them.
Hollow-post mill.
In a hollow-post mill, the post on which the body is mounted is hollowed out, to accommodate the drive shaft.
This makes it possible to drive machinery below or outside the body while still being able to rotate the body into the wind. Hollow-post mills driving scoop wheels were used in the Netherlands to drain wetlands from the fourteenth century onwards.
Tower mill.
By the end of the thirteenth century, the masonry tower mill, on which only the cap is rotated rather than the whole body of the mill, had been introduced. The spread of tower mills came with a growing economy that called for larger and more stable sources of power, though they were more expensive to build. In contrast to the post mill, only the cap of the tower mill needs to be turned into the wind, so the main structure can be made much taller, allowing the sails to be made longer, which enables them to provide useful work even in low winds. The cap can be turned into the wind either by winches or gearing inside the cap or from a winch on the tail pole outside the mill. A method of keeping the cap and sails into the wind automatically is by using a fantail, a small windmill mounted at right angles to the sails, at the rear of the windmill. These are also fitted to tail poles of post mills and are common in Great Britain and English-speaking countries of the former British Empire, Denmark, and Germany but rare in other places. Around some parts of the Mediterranean Sea, tower mills with fixed caps were built because the wind's direction varied little most of the time.
Smock mill.
The smock mill is a later development of the tower mill, where the tower is replaced by a wooden framework, called the "smock." The smock is commonly of octagonal plan, though examples with more, or fewer, sides exist. The smock is thatched, boarded or covered by other materials, such as slate, sheet metal, or tar paper. The lighter construction in comparison to tower mills make smock mills practical as drainage mills as these often had to be built in areas with unstable subsoil. Having originated as a drainage mill, smock mills are also used for a variety of purposes. When used in a built-up area it is often placed on a masonry base to raise it above the surrounding buildings.
Mechanics.
Sails.
Common sails consist of a lattice framework on which a sailcloth is spread. The miller can adjust the amount of cloth spread according to the amount of wind available and power needed. In medieval mills, the sailcloth was wound in and out of a ladder type arrangement of sails. Postmedieval mill sails had a lattice framework over which the sailcloth was spread, while in colder climates, the cloth was replaced by wooden slats, which were easier to handle in freezing conditions. The jib sail is commonly found in Mediterranean countries, and consists of a simple triangle of cloth wound round a spar.
In all cases, the mill needs to be stopped to adjust the sails. Inventions in Great Britain in the late eighteenth and nineteenth centuries led to sails that automatically adjust to the wind speed without the need for the miller to intervene, culminating in patent sails invented by William Cubitt in 1813. In these sails, the cloth is replaced by a mechanism of connected shutters.
In France, Pierre-Théophile Berton invented a system consisting of longitudinal wooden slats connected by a mechanism that lets the miller open them while the mill is turning. In the twentieth century, increased knowledge of aerodynamics from the development of the airplane led to further improvements in efficiency by German engineer Bilau and several Dutch millwrights.
The majority of windmills have four sails. Multiple-sailed mills, with five, six or eight sails, were built in Great Britain (especially in and around the counties of Lincolnshire and Yorkshire), Germany, and less commonly elsewhere. Earlier multiple-sailed mills are found in Spain, Portugal, Greece, parts of Romania, Bulgaria, and Russia. A mill with an even number of sails has the advantage of being able to run with a damaged sail and the one opposite removed without resulting in an unbalanced mill.
In the Netherlands the stationary position of the sails, i.e. when the mill is not working, has long been used to give signals. Most notably the positions with the sails just before or just after the main building signal joy or mourning. Across the Netherlands windmills with the sails tilted to the right were used to mourn the victims of the 2014 flight MH17. 
Machinery.
Gears inside a windmill convey power from the rotary motion of the sails to a mechanical device. The sails are carried on the horizontal windshaft. Windshafts can be wholly made of wood, or wood with a cast iron poll end (where the sails are mounted) or entirely of cast iron. The brake wheel is fitted onto the windshaft between the front and rear bearing. It has the brake around the outside of the rim and teeth in the side of the rim which drive the horizontal gearwheel called wallower on the top end of the vertical upright shaft. In grist mills, the great spur wheel, lower down the upright shaft, drives one or more stone nuts on the shafts driving each millstone. Post mills sometimes have a head and/or tail wheel driving the stone nuts directly, instead of the spur gear arrangement. Additional gear wheels drive a sack hoist or other machinery.
The machinery differs if the windmill is used for other applications than milling grain. A drainage mill uses another set of gear wheels on the bottom end of the upright shaft to drive a scoop wheel or Archimedes' screw. Sawmills use a crankshaft to provide a reciprocating motion to the saws. Windmills have been used to power many other industrial processes, including papermills, threshing mills, and to process oil seeds, wool, paints and stone products.
Spread and decline.
The total number of wind-powered mills in Europe is estimated to have been around 200,000 at its peak, which is modest compared to some 500,000 waterwheels. Windmills were applied in regions where there was too little water, where rivers freeze in winter and in flat lands where the flow of the river was too slow to provide the required power. With the coming of the industrial revolution, the importance of wind and water as primary industrial energy sources declined and were eventually replaced by steam (in steam mills) and internal combustion engines, although windmills continued to be built in large numbers until late in the nineteenth century.
More recently, windmills have been preserved for their historic value, in some cases as static exhibits when the antique machinery is too fragile to put in motion, and in other cases as fully working mills.
Of the 10,000 windmills in use in the Netherlands around 1850, about 1000 are still standing. Most of these are being run by volunteers, though some grist mills are still operating commercially. Many of the drainage mills have been appointed as backup to the modern pumping stations. The Zaan district has been said to have been the first industrialized region of the world with around 600 operating wind-powered industries by the end of the eighteenth century. Economic fluctuations and the industrial revolution had a much greater impact on these industries than on grain and drainage mills so only very few are left.
Construction of mills spread to the Cape Colony in the seventeenth century. The early tower mills did not survive the gales of the Cape Peninsula, so in 1717, the Heeren XVII sent carpenters, masons, and materials to construct a durable mill. The mill, completed in 1718, became known as the "Oude Molen" and was located between Pinelands Station and the Black River. Long since demolished, its name lives on as that of a Technical school in Pinelands. By 1863, Cape Town could boast 11 mills stretching from Paarden Eiland to Mowbray.
Wind turbines.
A wind turbine is a windmill-like structure specifically developed to generate electricity. They can be seen as the next step in the development of the windmill. The first wind turbines were built by the end of the nineteenth century by Prof James Blyth in Scotland (1887), Charles F. Brush in Cleveland, Ohio (1887–1888) and Poul la Cour in Denmark (1890s). La Cour's mill from 1896 later became the local powerplant of the village Askov. By 1908 there were 72 wind-driven electric generators in Denmark, ranging from 5 to 25 kW. By the 1930s, windmills were widely used to generate electricity on farms in the United States where distribution systems had not yet been installed, built by companies such as Jacobs Wind, Wincharger, Miller Airlite, Universal Aeroelectric, Paris-Dunn, Airline, and Winpower. The Dunlite Corporation produced turbines for similar locations in Australia.
Forerunners of modern horizontal-axis utility-scale wind generators were the WIME-3D in service in Balaklava USSR from 1931 until 1942, a 100-kW generator on a 30-m (100-ft) tower, the Smith-Putnam wind turbine built in 1941 on the mountain known as Grandpa's Knob in Castleton, Vermont, USA of 1.25 MW and the NASA wind turbines developed from 1974 through the mid-1980s. The development of these 13 experimental wind turbines pioneered many of the wind turbine design technologies in use today, including: steel tube towers, variable-speed generators, composite blade materials, and partial-span pitch control, as well as aerodynamic, structural, and acoustic engineering design capabilities. The modern wind power industry began in 1979 with the serial production of wind turbines by Danish manufacturers Kuriant, Vestas, Nordtank, and Bonus. These early turbines were small by today's standards, with capacities of 20–30 kW each. Since then, commercial turbines have increased greatly in size, with the Enercon E-126 capable of delivering up to 7 MW, while wind turbine production has expanded to many countries.
As the 21st century began, rising concerns over energy security, global warming, and eventual fossil fuel depletion led to an expansion of interest in all available forms of renewable energy. Worldwide, many thousands of wind turbines are now operating, with a total nameplate capacity of 194,400 MW. Europe accounted for 48% of the total in 2009.
Windpumps.
Windpumps were used to pump water since at least the 9th century in what is now Afghanistan, Iran and Pakistan. The use of wind pumps became widespread across the Muslim world and later spread to China and India. Windmills were later used extensively in Europe, particularly in the Netherlands and the East Anglia area of Great Britain, from the late Middle Ages onwards, to drain land for agricultural or building purposes.
The American windmill, or wind engine, was invented by Daniel Halladay in 1854 and was used mostly for lifting water from wells. Larger versions were also used for tasks such as sawing wood, chopping hay, and shelling and grinding grain. In early California and some other states, the windmill was part of a self-contained domestic water system which included a hand-dug well and a wooden water tower supporting a redwood tank enclosed by wooden siding known as a tankhouse. During the late 19th century steel blades and steel towers replaced wooden construction. At their peak in 1930, an estimated 600,000 units were in use. Firms such as U.S. Wind Engine and Pump Company, Challenge Wind Mill and Feed Mill Company, Appleton Manufacturing Company, Star, Eclipse, Fairbanks-Morse, and Aermotor became the main suppliers in North and South America. These windpumps are used extensively on farms and ranches in the United States, Canada, Southern Africa, and Australia. They feature a large number of blades, so they turn slowly with considerable torque in low winds and are self-regulating in high winds. A tower-top gearbox and crankshaft convert the rotary motion into reciprocating strokes carried downward through a rod to the pump cylinder below. Such mills pumped water and powered feed mills, saw mills, and agricultural machinery.
In Australia, the Griffiths Brothers at Toowoomba manufactured windmills from 1876, with the trade name Southern Cross Windmills in use from 1903. These became an icon of the Australian rural sector by utilizing the water of the Great Artesian Basin.

</doc>
<doc id="43491" url="http://en.wikipedia.org/wiki?curid=43491" title="Heuristic argument">
Heuristic argument

A heuristic argument is an argument that reasons from the value of a method or principle that has been shown by experimental (especially trial-and-error) investigation to be a useful aid in learning, discovery and problem-solving. A widely used and important example of a heuristic argument is Occam's Razor.
It is a speculative, non-rigorous argument, that relies on an analogy or on intuition, that allows one to achieve a result or approximation to be checked later with more rigor; otherwise the results are of doubt. It is used as a hypothesis or conjecture in an investigation. It can also be used as a mnemonic.

</doc>
<doc id="43492" url="http://en.wikipedia.org/wiki?curid=43492" title="Ian Dury">
Ian Dury

Ian Robins Dury (12 May 1942 – 27 March 2000) was an English rock and roll singer-songwriter, bandleader, artist, and actor who first rose to fame during the late 1970s, during the punk and new wave era of rock music. He was the lead singer of Ian Dury and the Blockheads and before that of Kilburn and the High Roads.
Biography.
Early life.
Dury was born in northwest London at his parents' home at 43 Weald Rise, Harrow Weald, Harrow (though he often pretended that he had been born in Upminster, Havering, and all but one of his obituaries in the UK national press stated this as fact). His father, William George Dury (born 23 September 1905, Southborough, Kent; died 25 February 1968), was a bus driver and former boxer, while his mother Margaret (known as "Peggy", born Margaret Cuthbertson Walker, 17 April 1910, Rochdale, Lancashire) was a health visitor, the daughter of a Cornish doctor and the granddaughter of an Irish landowner.
William Dury trained with Rolls-Royce to be a chauffeur, and was then absent for long periods, so Peggy Dury took Ian to stay with her parents in Cornwall. After the Second World War, the family moved to Switzerland, where his father chauffeured for a millionaire and the Western European Union. In 1946 Peggy brought Ian back to England and they stayed with her sister, Mary, a physician in Cranham, a small village in Essex. Although he saw his father on visits, they never lived together again.
At the age of seven, he contracted polio; most likely, he believed, from a swimming pool at Southend on Sea during the 1949 polio epidemic. After six weeks in a full plaster cast in Truro hospital, he was moved to Black Notley Hospital, Braintree, Essex, where he spent a year and a half before going to Chailey Heritage Craft School, East Sussex, in 1951.
Chailey was a school and hospital for disabled children, and believed in toughening them up, contributing to the observant and determined person Dury became. Chailey taught trades such as cobbling and printing, but Dury's mother wanted him to be more academic, so his aunt Moll arranged for him to enter the Royal Grammar School, High Wycombe, where he recounted being punished for misdemeanours by being made to learn long tracts of poetry until a housemaster found him sobbing and put a stop to it:
I had to go into a box room where the suitcases were stored and learn 80 lines of "Ode to Autumn" by yer man Keats. If I got a word wrong I had to go back, they added that to the end of the sentence and after five nights of this my head had definitely gone. He left the school at the age of 16 to study painting at the Walthamstow College of Art, having gained GCE 'O' Levels in English Language, English Literature and Art.
From 1964 he studied art at the Royal College of Art under British artist Peter Blake, and in 1967 took part in a group exhibition, "Fantasy and Figuration", alongside Pat Douthwaite, Herbert Kitchen and Stass Paraskos at the Institute of Contemporary Arts in London. From 1967 he taught art at various colleges in the south of England. He also painted commercial illustrations for "The Sunday Times" in the early 1970s.
Dury married Elizabeth "Betty" Rathmell (born 12 August 1942, Leamington Spa, Warwickshire), on 3 June 1967 and they had two children, Jemima (born 4 January 1969, Hounslow, Middlesex) and the recording artist Baxter Dury (born 18 December 1971, Wingrave, Buckinghamshire, England). Dury divorced Rathmell in 1985, but remained on good terms. He also cohabited with a teenage fan, Denise Roudette, for six years after he moved to London. Later, he squatted at Oval Mansions, Kennington Oval.
Kilburn and the High Roads.
Dury formed Kilburn and the High Roads (a reference to the road in North West London) in 1971, and they played their first gig at Croydon School of Art on 5 December 1971. Dury was vocalist and lyricist, co-writing with pianist Russell Hardy and later enrolling into the group a number of the students he was teaching at Canterbury College of Art (now the University for the Creative Arts), including guitarist Keith Lucas (who later became the guitarist for 999 under the name Nick Cash) and bassist Humphrey Ocean.
Managed first by Charlie Gillett and Gordon Nelki and latterly by fashion entrepreneur Tommy Roberts, the Kilburns found favour on London's pub rock circuit and signed to Dawn Records in 1974, but despite favourable press coverage and a tour opening for English rock band The Who, the group failed to rise above cult status and disbanded in 1975.
The group produced two albums: "Handsome" and "Wotabunch" (plus a 5-track "Best Of" EP).
The Blockheads.
Under the management of Andrew King and Peter Jenner, the original managers of Pink Floyd, Ian Dury and the Blockheads quickly gained a reputation as one of the top live acts of new wave music.
Dury's lyrics are a combination of lyrical poetry, word play, observation of British everyday life, character sketches, and sexual humour: "This is what we find ... Home improvement expert Harold Hill of Harold Hill, Of do-it-yourself dexterity and double-glazing skill, Came home to find another gentleman's kippers in the grill, So he sanded off his winkle with his Black & Decker drill." The song "Billericay Dickie" rhymes "I had a love affair with Nina, In the back of my Cortina" with "A seasoned-up hyena Could not have been more obscener".
The Blockheads' sound drew from its members' diverse musical influences, which included jazz, rock and roll, funk, and reggae, and Dury's love of music hall. The band was formed after Dury began writing songs with pianist and guitarist Chaz Jankel (the brother of noted music video, TV, commercial and film director Annabel Jankel). Jankel took Dury's lyrics, fashioned a number of songs, and they began recording with members of Radio Caroline's Loving Awareness Band—drummer Charley Charles (born Hugh Glenn Mortimer Charles, Guyana 1945), bassist Norman Watt-Roy, keyboard player Mick Gallagher, guitarist John Turnbull and former Kilburns saxophonist Davey Payne. An album was completed, but major record labels passed on the band. Next door to Dury's manager's office was the newly formed Stiff Records, a perfect home for Dury's maverick style.
The single "Sex & Drugs & Rock & Roll", released 26 August 1977, marked Dury's Stiff debut. Although it was banned by the BBC it was named Single of the Week by "NME" on its release. The single issue was soon followed at the end of September, by the album "New Boots and Panties!!" which, although it did not include the single, achieved platinum status.
In October 1977 Dury and his band started performing as Ian Dury & the Blockheads, when the band signed on for the Stiff "Live Stiffs Tour" alongside Elvis Costello & the Attractions, Nick Lowe, Wreckless Eric, and Larry Wallis. The tour was a success, and Stiff launched a concerted Ian Dury marketing campaign, resulting in the Top Ten hit "What a Waste", and the hit single "Hit Me with Your Rhythm Stick", which reached No. 1 in the UK at the beginning of 1979, selling just short of a million copies. Again "Hit Me" was not included on the original release of the subsequent album "Do It Yourself". Both the single and its accompanying music video featured Davey Payne playing two saxophones simultaneously during his solo, in evident homage to jazz saxophonist Rahsaan Roland Kirk, whose 'trademark' technique this was. With their hit singles, the band built up a dedicated following in the UK and other countries and their next single "Reasons to be Cheerful, Part 3" made number three in the UK.
The band's second album "Do It Yourself" was released in June 1979 in a Barney Bubbles-designed sleeve of which there were over a dozen variations, all based on samples from the Crown wallpaper catalogue. Bubbles also designed the Blockhead logo.
Jankel left the band temporarily and relocated to the U.S. after the release of "What a Waste" (his organ part on that single was overdubbed later) but he subsequently returned to the UK and began touring sporadically with the Blockheads, eventually returning to the group full-time for the recording of "Hit Me with Your Rhythm Stick"; according to Mickey Gallagher, the band recorded 28 takes of the song but eventually settled on the second take for the single release. Partly due to personality clashes with Dury, Jankel left the group again in 1980, after the recording of the "Do It Yourself" LP, and he returned to the USA to concentrate on his solo career.
The group worked solidly over the eighteen months between the release of "Rhythm Stick" and their next single, "Reasons to Be Cheerful", which returned them to the charts, making the UK Top 10. Jankel was replaced by former Dr. Feelgood guitarist Wilko Johnson, who also contributed to the next album "Laughter" (1980) and its two hit singles, although Gallagher recalls that the recording of the "Laughter" album was difficult and that Dury was drinking heavily in this period.
In 1980–81 Dury and Jankel teamed up again with Sly and Robbie and the Compass Point All Stars to record "Lord Upminster" (1981). The Blockheads toured the UK and Europe throughout 1981, sometimes augmented by jazz trumpeter Don Cherry, ending the year with their only tour of Australia.
The Blockheads disbanded in early 1982 after Dury secured a new recording deal with Polydor Records through A&R man Frank Neilson. Choosing to work with a group of young musicians which he named the Music Students, he recorded the album "Four Thousand Weeks' Holiday". This album marked a departure from his usual style and was not as well received by fans for its American jazz influence.
The Blockheads briefly reformed in June 1987 to play a short tour of Japan, and then disbanded again. In September 1990, following the death from cancer of drummer Charley Charles, they reunited for two benefit concerts in aid of Charles' family, held at The Forum, Camden Town, with Steven Monti on drums. In December 1990, augmented by Merlin Rhys-Jones on guitar and Will Parnell on percussion, they recorded the live album "Warts & Audience" at the Brixton Academy.
The Blockheads (minus Jankel, who returned to California) toured Spain in January 1991, then disbanded again until August 1994 when, following Jankel's return to England, they were invited to reform for the Madstock! Festival in Finsbury Park; this was followed by sporadic gigs in Europe, Ireland, the UK and Japan through late 1994 and 1995. In the early 1990s, Dury appeared with English band Curve on the benefit compilation album "Peace Together". Dury and Curve singer Toni Halliday shared vocals on a cover of the Blockheads' track "What a Waste".
In March 1996 Dury was diagnosed with cancer and, after recovering from an operation, he set about writing another album. In early 1998 he reunited with the Blockheads to record the album "Mr Love-Pants". In May, Ian Dury & the Blockheads hit the road again, with Dylan Howe replacing Steven Monti on drums. Davey Payne left the group permanently in August and was replaced by Gilad Atzmon; this line-up gigged throughout 1999, culminating in their last performance with Ian Dury on 6 February 2000 at the London Palladium. Dury died six weeks later on 27 March 2000.
The Blockheads have continued after Dury's death, contributing to the tribute album "Brand New Boots And Panties", then "Where's The Party". The Blockheads still tour, and are currently recording a new album. They currently comprise Jankel, Watt-Roy, Gallagher, Turnbull, John Roberts on drums, Gilad Atzmon and Dave Lewis on saxes. Derek The Draw (who was Dury's friend and minder) is now writing songs with Jankel as well as singing. They are aided and abetted by Lee Harris, who is their 'aide de camp'.
Roger Daltrey.
In 1984, Dury worked on Roger Daltrey's solo album "Parting Should Be Painless" singing backing vocals. He is also featured on the music video for Daltrey's minor hit single "Walking in My Sleep".
Spasticus Autisticus.
Dury's 1981 song "Spasticus Autisticus"—written to show his disdain for that year's International Year of Disabled Persons, which he saw as patronising and counter-productive—was banned by the BBC. Dury was a disabled person himself, having been left crippled by childhood polio. The lyrics were uncompromising:
The song's refrain, "I'm spasticus, autisticus", was inspired by the response of the rebellious Roman gladiators in the film "Spartacus", who, when instructed to identify their leader, all answered, "I am Spartacus", to protect him. According to Professor George McKay, in a 2009 article in "Popular Music" called 'Crippled with nerves' (an early Dury song title):
Dury described the song as "a war cry" on "Desert Island Discs". Although the song was banned from being broadcast by the BBC before 6 p.m. when it first came out, it was used at the opening of the London 2012 Paralympics.
Acting and other activities.
Dury's confident and unusual demeanour caught the eyes of producers and directors of drama. His first important and extensive role was in Farrukh Dhondy's mini-series for the BBC "King of the Ghetto" (1987), a drama set in London's multi-racial Brick Lane area with a cast led by a young Tim Roth.
Dury had small parts in several films, probably the best known of which was Peter Greenaway's
"The Cook, the Thief, His Wife & Her Lover", as well as cameo appearances in Roman Polanski's "Pirates", in the Eduardo Guedes film " Rocinante", and the Sylvester Stallone science fiction film "Judge Dredd". He also made an appearance in the film "Split Second" starring Rutger Hauer and Kim Cattrall.
Dury also wrote a musical, "Apples", staged in London's Royal Court Theatre. He had a small supporting role in "", directed by Tim Pope, who had directed a few of Dury's music videos. He also appeared alongside fellow lyricists Bob Dylan and Tom Waits, respectively, in the movies "Hearts of Fire" (1987) and "Bearskin: An Urban Fairytale" (1989).
In 1987 he appeared as the narrator (Scullery) in "Road" at the Royal Court Theatre. Among the cast was actress and singer Jane Horrocks, who cohabited with Dury until late in 1988, although the relationship was kept discreet.
Dury wrote and performed the theme song "Profoundly in Love with Pandora" for the television series "The Secret Diary of Adrian Mole" (1985), based on the book of the same name by Sue Townsend, as well as its follow-up, "The Growing Pains of Adrian Mole" (1987). Dury turned down an offer from Andrew Lloyd Webber to write the libretto for "Cats" (from which Richard Stilgoe reportedly earned millions). The reason, said Dury, "I can't stand his music." "... I said no straight off. I hate Andrew Lloyd Webber. He's a wanker, isn't he? ... [E]very time I hear 'Don't Cry for Me Argentina' I feel sick, it's so bad. He got Richard Stilgoe to do the lyrics in the end, who's not as good as me. He made millions out of it. He's crap, but he did ask the top man first!"
When AIDS first came to prominence in the mid-1980s, Dury was among celebrities who appeared on UK television to promote safe sex, demonstrating how to put on a condom using a model of an erect penis. In the 1990s, he became an ambassador for UNICEF, recruiting stars such as Robbie Williams to publicise the cause. The two visited Sri Lanka in this capacity to promote polio vaccination. Dury appeared with Curve on the "Peace Together" concert and CD (1993), performing "What a Waste", with benefits to the Youth of Northern Ireland. He also supported the charity Cancer BACUP.
Dury appeared in the "Classic Albums" episode that focused on Steely Dan's album "Aja". Dury commented that the album was one of the most "hopeful" he'd ever heard, and that the album "lifted [his] spirits up" whenever he played it. He also felt that it showed Steely Dan's love for jazz musicians and that it had "California in its blood ... [even though it was recorded by] boys from New York."
Dury also appeared at the end of the Carter USM track "Skywest & Crooked" narrating from the book "Don Quixote". The track appeared on "1992 – The Love Album".
Illness.
It was known for some time before his death that Dury had cancer. He was diagnosed with colorectal cancer in 1996 and underwent surgery, but tumours were later found in his liver, and he was told that his condition was terminal. Upon learning of his illness, Dury took the opportunity to marry his girlfriend, sculptor Sophy Tilson, with whom he had had two children, Bill and Albert.
In 1998, his death was incorrectly announced on XFM radio by Bob Geldof, possibly due to hoax information from a listener.
In 1999, Dury collaborated with Madness on their first original album in fourteen years on the track "Drip Fed Fred". Suggs and the band cite him as a great influence. It was to be one of his last recordings.
Ian Dury & the Blockheads' last performance was a charity concert in aid of Cancer BACUP on 6 February 2000 at the London Palladium, supported by Kirsty MacColl and Phill Jupitus. Dury was noticeably ill and had to be helped on and off stage.
Death.
Dury died of metastatic colorectal cancer on 27 March 2000, aged 57. An obituary in "The Guardian" read: "one of few true originals of the English music scene". Meanwhile, he was described by Suggs, the singer of Madness, as "possibly the finest lyricist we've seen." The Ian Dury website opened an online book of condolence shortly after his death, which was signed by hundreds of fans. He was cremated following a humanist funeral at Golders Green Crematorium with 250 mourners at the service, including fellow musicians Suggs and Jools Holland as well as other "celebrity fans" such as MP Mo Mowlam.
Legacy.
Dury's son, Baxter Dury, is also a singer. He sang a few of his father's songs at the wake after the funeral, and has released his own albums – "Len Parrot's Memorial Lift", "Floor Show" and "Happy Soup".
In 2002 a "musical bench" designed by Mil Stricevic was placed in a favoured viewing spot of Dury's near Poets' Corner, in the gardens of Pembroke Lodge, in Richmond Park, south-west London. The back of the bench is inscribed with the words "Reasons to be cheerful", the title of one of Dury's songs. This solar powered seat was intended to allow visitors to plug in and listen to eight of his songs as well as an interview, but unfortunately has been subjected to repeated vandalism.
Between 6 January and 14 February 2009 a musical about his life, entitled "Hit Me! The Life & Rhymes of Ian Dury", was premiered and ran at the Leicester Square Theatre in London.
A biopic entitled "Sex & Drugs & Rock & Roll" starring Andy Serkis as Dury was released on 8 January 2010, and was nominated for several awards. Ray Winstone and Naomie Harris also appeared. The title of the film is derived from Dury's 1977 7" single "Sex & Drugs & Rock & Roll".
A musical, "Reasons to be Cheerful", was produced by the Graeae Theatre Company in association with Theatre Royal Stratford East and New Wolsey Theatre. Set in 1979 the musical featured Dury classics in a "riotous coming-of-age tale". The 2010 production was supported by the Blockheads, while Sir Peter Blake donated a limited edition print of the "Reasons to be Cheerful" artwork.
Quotations.
"NME" – December 1977
"NME" – June 1979

</doc>
<doc id="43494" url="http://en.wikipedia.org/wiki?curid=43494" title="Fairy tale">
Fairy tale

A fairy tale (pronounced /ˈfeəriˌteɪl/) is a type of short story that typically features European folkloric fantasy characters, such as dwarves, elves, fairies, giants, gnomes, goblins, mermaids, trolls, or witches, and usually magic or enchantments. Fairy tales may be distinguished from other folk narratives such as legends (which generally involve belief in the veracity of the events described) and explicitly moral tales, including beast fables.
In less technical contexts, the term is also used to describe something blessed with unusual happiness, as in "fairy tale ending" (a happy ending) or "fairy tale romance" (though not all fairy tales end happily). Colloquially, a "fairy tale" or "fairy story" can also mean any farfetched story or tall tale; it is used especially of any story that not only is not true, but could not possibly be true. Legends are perceived as real; fairy tales may merge into legends, where the narrative is perceived both by teller and hearers as being grounded in historical truth. However, unlike legends and epics, they usually do not contain more than superficial references to religion and actual places, people, and events; they take place once upon a time rather than in actual times.
Fairy tales are found in oral and in literary form. The history of the fairy tale is particularly difficult to trace because only the literary forms can survive. Still, the evidence of literary works at least indicates that fairy tales have existed for thousands of years, although not perhaps recognized as a genre; the name "fairy tale" was first ascribed to them by Madame d'Aulnoy in the late 17th century. Many of today's fairy tales have evolved from centuries-old stories that have appeared, with variations, in multiple cultures around the world. Fairy tales, and works derived from fairy tales, are still written today.
The older fairy tales were intended for an audience of adults, as well as children, but they were associated with children as early as the writings of the "précieuses"; the Brothers Grimm titled their collection "Children's and Household Tales", and the link with children has only grown stronger with time.
Folklorists have classified fairy tales in various ways. The Aarne-Thompson classification system and the morphological analysis of Vladimir Propp are among the most notable. Other folklorists have interpreted the tales' significance, but no school has been definitively established for the meaning of the tales.
Terminology.
Some folklorists prefer to use the German term "Märchen" or "wonder tale" to refer to the genre over "fairy tale", a practice given weight by the definition of Thompson in his 1977 [1946] edition of "The Folktale": "a tale of some length involving a succession of motifs or episodes. It moves in an unreal world without definite locality or definite creatures and is filled with the marvelous. In this never-never land, humble heroes kill adversaries, succeed to kingdoms and marry princesses." The characters and motifs of fairy tales are simple and archetypal: princesses and goose-girls; youngest sons and gallant princes; ogres, giants, dragons, and trolls; wicked stepmothers and false heroes; fairy godmothers and other magical helpers, often talking horses, or foxes, or birds; glass mountains; and prohibitions and breaking of prohibitions.
Definition.
Although the fairy tale is a distinct genre within the larger category of folktale, the definition that marks a work as a fairy tale is a source of considerable dispute. One universally agreed-upon matter is that fairy tales do not require fairies. (The term itself comes from the translation of Madame D'Aulnoy's "conte de fées", first used in her collection in 1697.) Common parlance conflates fairy tales with beast fables and other folktales, and scholars differ on the degree to which the presence of fairies and/or similarly mythical beings (e.g., elves, goblins, trolls, giants, huge monsters) should be taken as a differentiator. Vladimir Propp, in his "Morphology of the Folktale", criticized the common distinction between "fairy tales" and "animal tales" on the grounds that many tales contained both fantastic elements and animals. Nevertheless, to select works for his analysis, Propp used all Russian folktales classified as a folk lore Aarne-Thompson 300-749 – in a cataloguing system that made such a distinction – to gain a clear set of tales. His own analysis identified fairy tales by their plot elements, but that in itself has been criticized, as the analysis does not lend itself easily to tales that do not involve a quest, and furthermore, the same plot elements are found in non-fairy tale works.
As Stith Thompson points out, talking animals and the presence of magic seem to be more common to the fairy tale than fairies themselves. However, the mere presence of animals that talk does not make a tale a fairy tale, especially when the animal is clearly a mask on a human face, as in fables.
In his essay "On Fairy-Stories", J. R. R. Tolkien agreed with the exclusion of "fairies" from the definition, defining fairy tales as stories about the adventures of men in "Faërie", the land of fairies, fairytale princes and princesses, dwarves, elves, and not only other magical species but many other marvels. However, the same essay excludes tales that are often considered fairy tales, citing as an example "The Monkey's Heart", which Andrew Lang included in "The Lilac Fairy Book".
Steven Swann Jones identified the presence of magic as the feature by which fairy tales can be distinguished from other sorts of folktales. Davidson and Chaudri identify "transformation" as the key feature of the genre. From a psychological point of view, Jean Chiriac argued for the necessity of the fantastic in these narratives.
In terms of aesthetic values, Italo Calvino cited the fairy tale as a prime example of "quickness" in literature, because of the economy and concision of the tales.
History of the genre.
Originally, stories that we would now call fairy tales were not marked out as a separate genre. The German term "Märchen" stems from the old German word "Mär", which means story or tale. The word "Märchen" is the diminutive of the word "Mär", therefore it means a "little story". Together with the common beginning "once upon a time" it means a fairy tale or a märchen was originally a little story from a long time ago, when the world was still magic. (Indeed one less regular German opening is "In the old times when wishing was still effective".)
The English term "fairy tale" stems from the fact that the French "contes" often included fairies.
Roots of the genre come from different oral stories passed down in European cultures. The genre was first marked out by writers of the Renaissance, such as Giovanni Francesco Straparola and Giambattista Basile, and stabilized through the works of later collectors such as Charles Perrault and the Brothers Grimm. In this evolution, the name was coined when the "précieuses" took up writing literary stories; Madame d'Aulnoy invented the term "conte de fée", or fairy tale, in the late 17th century.
Before the definition of the genre of fantasy, many works that would now be classified as fantasy were termed "fairy tales", including Tolkien's "The Hobbit", George Orwell's "Animal Farm", and L. Frank Baum's "The Wonderful Wizard of Oz". Indeed, Tolkien's "On Fairy-Stories" includes discussions of world-building and is considered a vital part of fantasy criticism. Although fantasy, particularly the subgenre of fairytale fantasy, draws heavily on fairy tale motifs, the genres are now regarded as distinct.
Folk and literary.
The fairy tale, told orally, is a sub-class of the folktale. Many writers have written in the form of the fairy tale. These are the literary fairy tales, or "Kunstmärchen". The oldest forms, from "Panchatantra" to the "Pentamerone", show considerable reworking from the oral form. The Brothers Grimm were among the first to try to preserve the features of oral tales. Yet the stories printed under the Grimm name have been considerably reworked to fit the written form.
Literary fairy tales and oral fairy tales freely exchanged plots, motifs, and elements with one another and with the tales of foreign lands. Many 18th-century folklorists attempted to recover the "pure" folktale, uncontaminated by literary versions. Yet while oral fairy tales likely existed for thousands of years before the literary forms, there is no pure folktale, and each literary fairy tale draws on folk traditions, if only in parody. This makes it impossible to trace forms of transmission of a fairy tale. Oral story-tellers have been known to read literary fairy tales to increase their own stock of stories and treatments.
History.
The oral tradition of the fairy tale came long before the written page. Tales were told or enacted dramatically, rather than written down, and handed down from generation to generation. Because of this, the history of their development is necessarily obscure and blurred. Fairy tales appear, now and again, in written literature throughout literate cultures, as in "The Golden Ass", which includes "Cupid and Psyche" (Roman, 100–200 AD), or the "Panchatantra" (India 3rd century BCE), but it is unknown to what extent these reflect the actual folk tales even of their own time. The stylistic evidence indicates that these, and many later collections, reworked folk tales into literary forms. What they do show is that the fairy tale has ancient roots, older than the "Arabian Nights" collection of magical tales (compiled "circa" 1500 AD), such as "Vikram and the Vampire", and "Bel and the Dragon". Besides such collections and individual tales, in China, Taoist philosophers such as Liezi and Zhuangzi recounted fairy tales in their philosophical works. In the broader definition of the genre, the first famous Western fairy tales are those of Aesop (6th century BC) in ancient Greece.
Jack Zipes writes in "When Dreams Came True", "There are fairy tale elements in Chaucer's "The Canterbury Tales", Edmund Spenser's "The Faerie Queene", and ... in many of William Shakespeare plays". "King Lear" can be considered a literary variant of fairy tales such as "Water and Salt" and "Cap O' Rushes". The tale itself resurfaced in Western literature in the 16th and 17th centuries, with "The Facetious Nights of Straparola" by Giovanni Francesco Straparola (Italy, 1550 and 1553), which contains many fairy tales in its inset tales, and the Neapolitan tales of Giambattista Basile (Naples, 1634–6), which are all fairy tales. Carlo Gozzi made use of many fairy tale motifs among his Commedia dell'Arte scenarios, including among them one based on "The Love For Three Oranges" (1761). Simultaneously, Pu Songling, in China, included many fairy tales in his collection, "Strange Stories from a Chinese Studio" (published posthumously, 1766). The fairy tale itself became popular among the "précieuses" of upper-class France (1690–1710), and among the tales told in that time were the ones of La Fontaine and the "Contes" of Charles Perrault (1697), who fixed the forms of "Sleeping Beauty" and "Cinderella". Although Straparola's, Basile's and Perrault's collections contain the oldest known forms of various fairy tales, on the stylistic evidence, all the writers rewrote the tales for literary effect.
The Salon Era.
In the mid-17th century, a vogue for magical tales emerged among the intellectuals who frequented the salons of Paris. These salons were regular gatherings hosted by prominent aristocratic women, where women and men could gather together to discuss the issues of the day.
In the 1630s, aristocratic women began to gather in their own living rooms, salons, in order to discuss the topics of their choice: arts and letters, politics, and social matters of immediate concern to the women of their class: marriage, love, financial and physical independence, and access to education. This was a time when women were barred from receiving a formal education. Some of the most gifted women writers of the period came out of these early salons (such as Madeleine de Scudéry and Madame de Lafayette), which encouraged women's independence and pushed against the gender barriers that defined their lives. The salonnières argued particularly for love and intellectual compatibility between the sexes, opposing the system of arranged marriages.
Sometime in the middle of the 17th century, a passion for the conversational parlour game based on the plots of old folk tales swept through the salons. Each salonnière was called upon to retell an old tale or rework an old theme, spinning clever new stories that not only showcased verbal agility and imagination, but also slyly commented on the conditions of aristocratic life. Great emphasis was placed on a mode of delivery that seemed natural and spontaneous. The decorative language of the fairy tales served an important function: disguising the rebellious subtext of the stories and sliding them past the court censors. Critiques of court life (and even of the king) were embedded in extravagant tales and in dark, sharply dystopian ones. Not surprisingly, the tales by women often featured young (but clever) aristocratic girls whose lives were controlled by the arbitrary whims of fathers, kings, and elderly wicked fairies, as well as tales in which groups of wise fairies (i.e., intelligent, independent women) stepped in and put all to rights.
The salon tales as they were originally written and published have been preserved in a monumental work called "Le Cabinet des Fées", an enormous collection of stories from the 17th and 18th centuries.
Later works.
The first collectors to attempt to preserve not only the plot and characters of the tale, but also the style in which they were told, were the Brothers Grimm, collecting German fairy tales; ironically, this meant although their first edition (1812 & 1815) remains a treasure for folklorists, they rewrote the tales in later editions to make them more acceptable, which ensured their sales and the later popularity of their work.
Such literary forms did not merely draw from the folktale, but also influenced folktales in turn. The Brothers Grimm rejected several tales for their collection, though told orally to them by Germans, because the tales derived from Perrault, and they concluded they were thereby French and not German tales; an oral version of "Bluebeard" was thus rejected, and the tale of "Little Briar Rose", clearly related to Perrault's "The Sleeping Beauty", was included only because Jacob Grimm convinced his brother that the figure of Brynhildr, from much earlier Norse mythology, proved that the sleeping princess was authentically Germanic folklore.
This consideration of whether to keep "Sleeping Beauty" reflected a belief common among folklorists of the 19th century: that the folk tradition preserved fairy tales in forms from pre-history except when "contaminated" by such literary forms, leading people to tell inauthentic tales. The rural, illiterate, and uneducated peasants, if suitably isolated, were the "folk" and would tell pure "folk" tales. Sometimes they regarded fairy tales as a form of fossil, the remnants of a once-perfect tale. However, further research has concluded that fairy tales never had a fixed form, and regardless of literary influence, the tellers constantly altered them for their own purposes.
The work of the Brothers Grimm influenced other collectors, both inspiring them to collect tales and leading them to similarly believe, in a spirit of romantic nationalism, that the fairy tales of a country were particularly representative of it, to the neglect of cross-cultural influence. Among those influenced were the Russian Alexander Afanasyev (first published in 1866), the Norwegians Peter Christen Asbjørnsen and Jørgen Moe (first published in 1845), the Romanian Petre Ispirescu (first published in 1874), the English Joseph Jacobs (first published in 1890), and Jeremiah Curtin, an American who collected Irish tales (first published in 1890). Ethnographers collected fairy tales throughout the world, finding similar tales in Africa, the Americas, and Australia; Andrew Lang was able to draw on not only the written tales of Europe and Asia, but those collected by ethnographers, to fill his "coloured" fairy books series. They also encouraged other collectors of fairy tales, as when Yei Theodora Ozaki created a collection, "Japanese Fairy Tales" (1908), after encouragement from Lang. Simultaneously, writers such as Hans Christian Andersen and George MacDonald continued the tradition of literary fairy tales. Andersen's work sometimes drew on old folktales, but more often deployed fairytale motifs and plots in new tales. MacDonald incorporated fairytale motifs both in new literary fairy tales, such as "The Light Princess", and in works of the genre that would become fantasy, as in "The Princess and the Goblin" or "Lilith".
Although many mainstream biologists contend that fairies may well be evidenced by the existence of the stick mantis ("brunneria borealis"), the existence of fairies could also be evidenced by fireflies. Looking beyond the mundane, examples of folklore permeate the history of fairy tales, as exemplified in Sir Walter Scott's novel "Rob Roy", with the Clachan of Aberfoil being just one example of such entities.
Cross-cultural transmission.
Two theories of origins have attempted to explain the common elements in fairy tales found spread over continents. One is that a single point of origin generated any given tale, which then spread over the centuries; the other is that such fairy tales stem from common human experience and therefore can appear separately in many different origins.
Fairy tales with very similar plots, characters, and motifs are found spread across many different cultures. Many researchers hold this to be caused by the spread of such tales, as people repeat tales they have heard in foreign lands, although the oral nature makes it impossible to trace the route except by inference. Folklorists have attempted to determine the origin by internal evidence, which can not always be clear; Joseph Jacobs, comparing the Scottish tale "The Ridere of Riddles" with the version collected by the Brothers Grimm, "The Riddle", noted that in "The Ridere of Riddles" one hero ends up polygamously married, which might point to an ancient custom, but in "The Riddle", the simpler riddle might argue greater antiquity.
Folklorists of the "Finnish" (or historical-geographical) school attempted to place fairy tales to their origin, with inconclusive results. Sometimes influence, especially within a limited area and time, is clearer, as when considering the influence of Perrault's tales on those collected by the Brothers Grimm. "Little Briar-Rose" appears to stem from Perrault's "The Sleeping Beauty", as the Grimms' tale appears to be the only independent German variant. Similarly, the close agreement between the opening of the Grimms' version of "Little Red Riding Hood" and Perrault's tale points to an influence, although the Grimms' version adds a different ending (perhaps derived from "The Wolf and the Seven Young Kids").
Fairy tales also tend to take on the color of their location, through the choice of motifs, the style in which they are told, and the depiction of character and local color.
Association with children.
Originally, adults were the audience of a fairy tale just as often as children. Literary fairy tales appeared in works intended for adults, but in the 19th and 20th centuries the fairy tale became associated with children's literature.
The "précieuses", including Madame d'Aulnoy, intended their works for adults, but regarded their source as the tales that servants, or other women of lower class, would tell to children. Indeed, a novel of that time, depicting a countess's suitor offering to tell such a tale, has the countess exclaim that she loves fairy tales as if she were still a child. Among the late "précieuses", Jeanne-Marie Le Prince de Beaumont redacted a version of "Beauty and the Beast" for children, and it is her tale that is best known today. The Brothers Grimm titled their collection "Children's and Household Tales" and rewrote their tales after complaints that they were not suitable for children.
In the modern era, fairy tales were altered so that they could be read to children. The Brothers Grimm concentrated mostly on sexual references; Rapunzel, in the first edition, revealed the prince's visits by asking why her clothing had grown tight, thus letting the witch deduce that she was pregnant, but in subsequent editions carelessly revealed that it was easier to pull up the prince than the witch. On the other hand, in many respects, violence – particularly when punishing villains – was increased. Other, later, revisions cut out violence; J. R. R. Tolkien noted that "The Juniper Tree" often had its cannibalistic stew cut out in a version intended for children. The moralizing strain in the Victorian era altered the classical tales to teach lessons, as when George Cruikshank rewrote "Cinderella" in 1854 to contain temperance themes. His acquaintance Charles Dickens protested, "In an utilitarian age, of all other times, it is a matter of grave importance that fairy tales should be respected."
Psychoanalysts such as Bruno Bettelheim, who regarded the cruelty of older fairy tales as indicative of psychological conflicts, strongly criticized this expurgation, because it weakened their usefulness to both children and adults as ways of symbolically resolving issues.
The adaptation of fairy tales for children continues. Walt Disney's influential "Snow White and the Seven Dwarfs" was largely (although certainly not solely) intended for the children's market. The anime "Magical Princess Minky Momo" draws on the fairy tale "Momotarō". Jack Zipes has spent many years working to make the older traditional stories accessible to modern readers and their children.
Contemporary tales.
Literary.
In contemporary literature, many authors have used the form of fairy tales for various reasons, such as examining the human condition from the simple framework a fairytale provides. Some authors seek to recreate a sense of the fantastic in a contemporary discourse. Some writers use fairy tale forms for modern issues; this can include using the psychological dramas implicit in the story, as when Robin McKinley retold "Donkeyskin" as the novel "Deerskin", with emphasis on the abusive treatment the father of the tale dealt to his daughter. Sometimes, especially in children's literature, fairy tales are retold with a twist simply for comic effect, such as "The Stinky Cheese Man" by Jon Scieszka and "The ASBO Fairy Tales" by Chris Pilbeam. A common comic motif is a world where all the fairy tales take place, and the characters are aware of their role in the story, such as in the film series Shrek.
Other authors may have specific motives, such as multicultural or feminist reevaluations of predominantly Eurocentric masculine-dominated fairy tales, implying critique of older narratives. The figure of the damsel in distress has been particularly attacked by many feminist critics. Examples of narrative reversal rejecting this figure include "The Paperbag Princess" by Robert Munsch, a picture book aimed at children in which a princess rescues a prince, and Angela Carter's "The Bloody Chamber", which retells a number of fairy tales from a female point of view.
There are also many contemporary erotic retellings of fairy tales, which explicitly draw upon the original spirit of the tales, and are specifically for adults. Modern retellings focus on exploring the tale through use of the erotic, explicit sexuality, dark and/or comic themes, female empowerment, fetish and BDSM, multicultural, heterosexual and LGBT characters. Cleis Press, independent publisher of books in the areas of sexuality, erotica, feminism, gay and lesbian studies, gender studies, fiction, and human rights, has released several fairy tale themed erotic anthologies, including "Fairy Tale Lust", "Lustfully Ever After", and "A Princess Bound".
It may be hard to lay down the rule between fairy tales and fantasies that use fairy tale motifs, or even whole plots, but the distinction is commonly made, even within the works of a single author: George MacDonald's "Lilith" and "Phantastes" are regarded as fantasies, while his "The Light Princess", "The Golden Key", and "The Wise Woman" are commonly called fairy tales. The most notable distinction is that fairytale fantasies, like other fantasies, make use of novelistic writing conventions of prose, characterization, or setting.
Film.
Fairy tales have been enacted dramatically; records exist of this in commedia dell'arte, and later in pantomime. The advent of cinema has meant that such stories could be presented in a more plausible manner, with the use of special effects and animation; the Disney movie "Snow White and the Seven Dwarfs" in 1937 was a ground-breaking film for fairy tales and, indeed, fantasy in general. Disney's influence helped establish this genre as a children's genre, and has been blamed for simplification of fairy tales ending in situations where everything goes right, as opposed to the pain and suffering – and sometimes unhappy endings – of many folk fairy tales.
Many filmed fairy tales have been made primarily for children, from Disney's later works to Aleksandr Rou's retelling of "Vasilissa the Beautiful", the first Soviet film to use Russian folk tales in a big-budget feature. Others have used the conventions of fairy tales to create new stories with sentiments more relevant to contemporary life, as in "Labyrinth", "My Neighbor Totoro", the films of Michel Ocelot, and "Happily N'Ever After".
Other works have retold familiar fairy tales in a darker, more horrific or psychological variant aimed primarily at adults. Notable examples are Jean Cocteau's "Beauty and the Beast" and "The Company of Wolves", based on Angela Carter's retelling of "Little Red Riding Hood". Likewise, "Princess Mononoke", "Pan's Labyrinth", "Suspiria", and "Spike" create new stories in this genre from fairy tale and folklore motifs.
In comics and animated TV series, "The Sandman", "Revolutionary Girl Utena", "Princess Tutu", "Fables" and "MÄR" all make use of standard fairy tale elements to various extents but are more accurately categorised as fairytale fantasy due to the definite locations and characters which a longer narrative requires.
A more modern cinematic fairy tale would be Luchino Visconti's "Le Notti Bianche", starring Marcello Mastroianni before he became a superstar. It involves many of the romantic conventions of fairy tales, yet it takes place in post-World War II Italy, and it ends realistically.
Motifs.
Any comparison of fairy tales quickly discovers that many fairy tales have features in common with each other. Two of the most influential classifications are those of Antti Aarne, as revised by Stith Thompson into the Aarne-Thompson classification system, and Vladimir Propp's "Morphology of the Folk Tale".
Aarne-Thompson.
This system groups fairy and folk tales according to their overall plot. Common, identifying features are picked out to decide which tales are grouped together. Much therefore depends on what features are regarded as decisive.
For instance, tales like "Cinderella" – in which a persecuted heroine, with the help of the fairy godmother or similar magical helper, attends an event (or three) in which she wins the love of a prince and is identified as his true bride – are classified as type 510, the persecuted heroine. Some such tales are "The Wonderful Birch"; "Aschenputtel"; "Katie Woodencloak"; "The Story of Tam and Cam"; "Ye Xian"; "Cap O' Rushes"; "Catskin"; "Fair, Brown and Trembling"; "Finette Cendron"; "Allerleirauh".
Further analysis of the tales shows that in "Cinderella", "The Wonderful Birch", "The Story of Tam and Cam", "Ye Xian", and "Aschenputtel", the heroine is persecuted by her stepmother and refused permission to go to the ball or other event, and in "Fair, Brown and Trembling" and "Finette Cendron" by her sisters and other female figures, and these are grouped as 510A; while in "Cap O' Rushes", "Catskin", and "Allerleirauh", the heroine is driven from home by her father's persecutions, and must take work in a kitchen elsewhere, and these are grouped as 510B. But in "Katie Woodencloak", she is driven from home by her stepmother's persecutions and must take service in a kitchen elsewhere, and in "Tattercoats", she is refused permission to go to the ball by her grandfather. Given these features common with both types of 510, "Katie Woodencloak" is classified as 510A because the villain is the stepmother, and "Tattercoats" as 510B because the grandfather fills the father's role.
This system has its weaknesses in the difficulty of having no way to classify subportions of a tale as motifs. "Rapunzel" is type 310 (The Maiden in the Tower), but it opens with a child being demanded in return for stolen food, as does "Puddocky"; but "Puddocky" is not a Maiden in the Tower tale, while "The Canary Prince", which opens with a jealous stepmother, is.
It also lends itself to emphasis on the common elements, to the extent that the folklorist describes "The Black Bull of Norroway" as the same story as "Beauty and the Beast". This can be useful as a shorthand but can also erase the coloring and details of a story.
Morphology.
Vladimir Propp specifically studied a collection of Russian fairy tales, but his analysis has been found useful for the tales of other countries.
Having criticized Aarne-Thompson type analysis for ignoring what motifs "did" in stories, and because the motifs used were not clearly distinct, he analyzed the tales for the "function" each character and action fulfilled and concluded that a tale was composed of thirty-one elements ('functions') and seven characters or 'spheres of action' ('the princess and her father' are a single sphere). While the elements were not all required for all tales, when they appeared they did so in an invariant order – except that each individual element might be negated twice, so that it would appear three times, as when, in "Brother and Sister", the brother resists drinking from enchanted streams twice, so that it is the third that enchants him. Propp's 31 functions also fall within six 'stages' (preparation, complication, transference, struggle, return, recognition), and a stage can also be repeated, which can affect the perceived order of elements.
One such element is the "donor" who gives the hero magical assistance, often after testing him. In "The Golden Bird", the talking fox tests the hero by warning him against entering an inn and, after he succeeds, helps him find the object of his quest; in "The Boy Who Drew Cats", the priest advised the hero to stay in small places at night, which protects him from an evil spirit; in "Cinderella", the fairy godmother gives Cinderella the dresses she needs to attend the ball, as their mothers' spirits do in "Bawang Putih Bawang Merah" and "The Wonderful Birch"; in "The Fox Sister", a Buddhist monk gives the brothers magical bottles to protect against the fox spirit. The roles can be more complicated. In "The Red Ettin", the role is split into the mother – who offers the hero the whole of a journey cake with her curse or half with her blessing – and when he takes the half, a fairy who gives him advice; in "Mr Simigdáli", the sun, the moon, and the stars all give the heroine a magical gift. Characters who are not always the donor can act like the donor. In "Kallo and the Goblins", the villain goblins also give the heroine gifts, because they are tricked; in "Schippeitaro", the evil cats betray their secret to the hero, giving him the means to defeat them. Other fairy tales, such as "The Story of the Youth Who Went Forth to Learn What Fear Was", do not feature the donor.
Analogies have been drawn between this and the analysis of myths into the Hero's journey.
Interpretations.
Many fairy tales have been interpreted for their (purported) significance. One mythological interpretation claimed that many fairy tales, including "Hansel and Gretel", "Sleeping Beauty", and "The Frog King", all were solar myths; this mode of interpretation is rather less popular now. Many have also been subjected to Freudian, Jungian, and other psychological analyses, but no mode of interpretation has ever established itself definitively.
Specific analyses have often been criticized for lending great importance to motifs that are not, in fact, integral to the tale; this has often stemmed from treating one instance of a fairy tale as the definitive text, where the tale has been told and retold in many variations. In variants of "Bluebeard", the wife's curiosity is betrayed by a blood-stained key, by an egg's breaking, or by the singing of a rose she wore, without affecting the tale, but interpretations of specific variants have claimed that the precise object is integral to the tale.
Other folklorists have interpreted tales as historical documents. Many German folklorists, believing the tales to have been preserved from ancient times, used Grimms' tales to explain ancient customs. Other folklorists have explained the figure of the wicked stepmother historically: many women did die in childbirth, their husbands remarried, and the new stepmothers competed with the children of the first marriage for resources.
In a 2012 lecture, Jack Zipes reads fairy tales as examples of what he calls "childism". He suggests that there are terrible aspects to the tales, which (among other things) have conditioned children to accept mistreatment and even abuse.
Compilations.
Authors and works:

</doc>
<doc id="43496" url="http://en.wikipedia.org/wiki?curid=43496" title="Frank Robinson">
Frank Robinson

Frank Robinson (born August 31, 1935) is an American former Major League Baseball outfielder and manager. He played for five teams from 1956 to 1976, and became the only player to win league MVP honors in both the National and American Leagues. He won the Triple crown, was a member of two teams that won the World Series (the 1966 and 1970 Baltimore Orioles), and amassed the fourth-most career home runs at the time of his retirement (he is currently ninth). Robinson was elected to the Baseball Hall of Fame in 1982.
Robinson was the first African-American hired to serve as manager in Major League history. He managed the Cleveland Indians during the last two years of his playing career, compiling a 186–189 record. He went on to manage the San Francisco Giants, the Baltimore Orioles, and the Montreal Expos/Washington Nationals. Currently he is the honorary President of the American League.
Early life.
Robinson attended McClymonds High School in Oakland, California, where he was a basketball teammate of Bill Russell. He was a baseball teammate of Vada Pinson and Curt Flood. While playing for the Reds in the late 1950s, he attended Xavier University in Cincinnati during the off-season.
Playing career.
Robinson had a long and successful playing career. Unusual for a star in the era before free agency, he split his best years between two teams: the Cincinnati Reds (1956–65) and the Baltimore Orioles (1966–71). The later years of his career were spent with the Los Angeles Dodgers (1972), California Angels (1973–74) and Cleveland Indians (1974–76). He is the only player to be named Most Valuable Player in both leagues, in 1961 with the Reds and again in 1966 with the Orioles.
In his rookie year, 1956, he tied the then-record of 38 home runs by a rookie, as a member of the Cincinnati Reds, and was named Rookie of the Year. Although the Reds won the NL pennant in 1961 and Robinson won his first MVP that year (in July he batted .409, hit 13 home runs, and drove in 34 RBI to win NL Player of the Month) his best offensive year arguably came in 1962, when he hit .342 with 39 home runs, 51 doubles, 208 hits (his only 200+ hit season), 136 RBI and 134 runs. The Reds lost the 1961 World Series to the Yankees.
Robinson practiced a gutsy batting style, crowding the plate perhaps more than any other player of his time, and experienced many knockdowns. Asked by an announcer what his solution to the problem was, he answered simply, "Just stand up and lambast the next pitch", which he often did.
Prior to the 1966 season, Reds owner Bill DeWitt sent Robinson to Baltimore in exchange for pitcher Milt Pappas, pitcher Jack Baldschun and outfielder Dick Simpson. The trade is now considered among the most lopsided deals in baseball history, especially as Robinson was only 30 years old and appeared to have many productive years ahead of him. DeWitt attempted to downplay this fact and defend the deal to skeptical Reds fans by famously referring to Robinson as "not a young 30." It forever tarnished Dewitt's legacy, and outrage over the deal made it difficult for Pappas to adjust to pitching in Cincinnati (he was traded out of town after only three seasons). There were also rumors that Robinson did not get along well with teammate Vada Pinson. In Robinson's first year in Baltimore, he won the Triple Crown, leading the American League with a .316 batting average (the lowest ever by a Triple crown winner), 49 home runs (the most ever by a right-handed Triple crown winner) and 122 runs batted in. On May 8, 1966, Robinson became the only player ever to hit a home run completely out of Memorial Stadium. The shot came off Luis Tiant in the second game of a doubleheader against the Cleveland Indians. Until the Orioles' move to Camden Yards in 1991, a flag labeled "HERE" was flown at the spot where the ball left the stadium.
The Orioles won the 1966 World Series and Robinson was named the Series MVP. In the Orioles' four-game sweep of the defending champion Los Angeles Dodgers, Robinson hit two home runs—in Game One, which Baltimore won 5–2, and in Game Four, the only run of the game in a 1–0 series-clinching victory. Both home runs were hit off Don Drysdale.
It was in Baltimore that he first became active in the civil rights movement. He originally declined membership in the NAACP unless the organization promised not to make him do public appearances. However, after witnessing Baltimore's segregated housing and discriminatory real estate practices, he changed his mind and became an enthusiastic speaker on racial issues.
On June 26, 1970, Robinson hit back-to-back grand slams (in the fifth and sixth innings) in the Orioles' 12–2 victory over the Washington Senators at RFK Stadium. The same runners were on base on both home runs—Dave McNally on third, Don Buford on second and Paul Blair on first.
The Orioles won three consecutive pennants between 1969 and 1971, and won the 1970 World Series over his old club Cincinnati.
During a 21-year baseball career, he batted .294 with 586 home runs, 1,812 runs batted in, and 2,943 hits. At his retirement, his 586 career home runs were the fourth-best in history (behind only Hank Aaron, Babe Ruth, and Willie Mays). He is second on Cincinnati's all-time home run leaders list (324) behind Johnny Bench and is the Reds' all-time leader in slugging percentage (.554).
Robinson finished his playing career with brief appearances for the Los Angeles Dodgers, California Angels (where he became their first DH and was again team mates with Vada Pinson) and Cleveland Indians.
Managing career.
Robinson managed in the winter leagues late in his playing career. By the early 1970s, he had his heart set on becoming the first black manager in the majors. In fact, the Angels traded him to the Cleveland Indians midway through the 1974 season due to his open campaigning for the manager's job.
In 1975, the Indians named him player-manager, giving him distinction of being the first black manager in the Majors. In his first at bat as player/manager of the Indians, he homered at Cleveland Stadium off Yankees pitcher Doc Medich.
His managing career would go on to include Cleveland (1975–77); the San Francisco Giants (1981–84); the Baltimore Orioles (1988–91); and the Montreal Expos/Washington Nationals franchise (2002–06).
In addition to being the first black manager in the major leagues with the American League's Indians, upon joining the Giants, he also became the first black manager in the National League.
He was awarded the American League Manager of the Year Award in 1989 for leading the Orioles to an 87–75 record, a turnaround from their previous season in which they went 54–107. After Robinson had spent some years known in baseball as the "Director of Discipline", he was chosen by Major League Baseball in 2002 to manage the Expos, which MLB owned at that time. The 2002 Expos performed surprisingly well, finishing 2nd in the NL East, and posting a 19-game improvement over 2001.
In 2005, the Montreal Gazette's Stephanie Myles reported that Robinson had devoted much time playing golf during his years in Montreal, sometimes spending 16-hour days between the course and the games at night. Some journalists have questioned his disregard of statistics to determine pitching match-ups with his hitting line-ups. Robinson defended his style of managing by saying that he goes by his "gut feeling".
In a June 2005 "Sports Illustrated" poll of 450 MLB players, Robinson was selected the worst manager in baseball, along with Buck Showalter, then manager of the Texas Rangers. In the August 2006 poll, he again was voted worst manager with 17% of the vote and 37.7% of the NL East vote.
In 2005, one of Robinson's Nationals players asked him if he had ever played in the majors. This was reported on "Real Sports with Bryant Gumbel" as an illustration of how little some current players are aware of the history of the game.
On Thursday, April 20, 2006, with the Nationals winning 10–4 against the Philadelphia Phillies, Robinson got his 1000th win, becoming the 53rd manager to reach that milestone. He had earned his 1000th loss two seasons earlier.
During a game against the Houston Astros on May 25, 2006, Robinson pulled Nationals catcher Matt LeCroy during the middle of the 7th inning, violating an unwritten rule that managers do not remove position players in the middle of an inning. Instead, managers are supposed to discreetly switch position players in between innings. However, LeCroy, the third-string catcher, had allowed Houston Astros baserunners to steal seven bases over seven innings and had committed two throwing errors. Although the Nationals won the game 8–5, Robinson found the decision so difficult to make on a player he respected so much, he broke down crying during the post-game interviews.
On September 30, 2006, the Nationals' management declined to renew Robinson's contract for the 2007 season, though they stated he was welcome to come to spring training in an unspecified role. Robinson, who wanted either a front office job or a consultancy, declined. On October 1, 2006, he managed his final game, a 6–2 loss to the Mets, and prior to the game addressed the fans at RFK Stadium.
Robinson's record as a manager stands as (1065–1176).
Honors.
In addition to his two Most Valuable Player awards (1961 and 1966) and his World Series Most Valuable Player award (1966), Robinson was honored in 1966 with the Hickok Belt as the top professional athlete of the year in any sport.
In 1982, Robinson was inducted into the National Baseball Hall of Fame as a Baltimore Oriole. Robinson is also a charter member of the Baltimore Orioles Hall of Fame (along with Brooks Robinson), and a member of the Cincinnati Reds Hall of Fame, being inducted into both in 1978. Both the Reds and the Orioles have retired his uniform number 20.
In 1999, he ranked Number 22 on "The Sporting News" list of the 100 Greatest Baseball Players, and was nominated as a finalist for the Major League Baseball All-Century Team.
In 2003, the Reds dedicated a bronze statue of Robinson at Great American Ball Park. On April 28, 2012, the Orioles unveiled a bronze statue of Robinson at Oriole Park at Camden Yards as part of the Orioles Legends Celebration Series.
He was awarded the Presidential Medal of Freedom on November 9, 2005, by President George W. Bush. On April 13, 2007 Robinson was awarded the first Jackie Robinson Society Community Recognition Award at George Washington University.
In his career, he held several major league records. In his rookie season, he tied Wally Berger's record for home runs by a rookie (38). (The current record would be set by Mark McGwire in 1987.) Robinson still holds the record for home runs on opening day (8), which includes a home run in his first at bat as a player-manager. Robinson won the American League Triple Crown (.316 BA, 49 HR, 122 RBI) – only two players (Carl Yastrzemski and Miguel Cabrera) have since won the award in either league – and the two MVP awards, which made him the first player in baseball history to earn the title in both leagues.
Post-managerial career.
Robinson first served in the MLB front office as Vice President of On-Field Operations from 1999 to 2002, responsible for player discipline, uniform policy, stadium configuration, and other on-field issues.
Robinson served as an analyst for ESPN during 2007 Spring Training. The Nationals offered to honor Robinson during a May 20 game against his former club the Baltimore Orioles but he refused.
In 2007 Robinson rejoined the MLB front office, serving as a Special Advisor for Baseball Operations from 2007 to 2009. He then served as Special Assistant to Bud Selig from 2009 to 2010, and then was named Senior Vice President for Major League Operations from 2010 to 2011. In June 2012, he became Executive Vice President of Baseball Development. In February 2015, Robinson left his position as Executive Vice President of Baseball Development and was named senior advisor to the Commissioner of Baseball and Honorary American League President.

</doc>
<doc id="43498" url="http://en.wikipedia.org/wiki?curid=43498" title="Dord">
Dord

Dord is a notable error in lexicography, an accidental creation, or ghost word, of the G. and C. Merriam Company's staff in the second (1934) edition of its "New International Dictionary", in which the term is defined as "density".
Philip Babcock Gove, an editor at Merriam-Webster who became editor-in-chief of "Webster's Third New International Dictionary", wrote a letter to the journal "American Speech", fifteen years after the error was caught, in which he explained why "dord" was included in that dictionary.
On July 31, 1931, Austin M. Patterson, Webster's chemistry editor, sent in a slip reading "D or d, cont./density." This was intended to add "density" to the existing list of words that the letter "D" can abbreviate. The slip somehow went astray, and the phrase "D or d" was misinterpreted as a single, run-together word: "Dord". (This was a plausible mistake because headwords on slips were typed with spaces between the letters, making "D or d" look very much like "D o r d".) A new slip was prepared for the printer and a part of speech assigned along with a pronunciation. The would-be word got past proofreaders and appeared on page 771 of the dictionary around 1934.
On February 28, 1939, an editor noticed "dord" lacked an etymology and investigated. Soon an order was sent to the printer marked "plate change/imperative/urgent". In 1940, bound books began appearing without the ghost word but with a new abbreviation (although inspection of printed copies well into the 1940s show "dord" still present). The non-word "dord" was excised, and the definition of the adjacent entry "Doré furnace" was expanded from "A furnace for refining dore bullion" to "a furnace in which dore bullion is refined" to close up the space. Gove wrote that this was "probably too bad, for why shouldn't "dord" mean 'density'?" The entry "dord" was not removed until 1947.

</doc>
<doc id="43500" url="http://en.wikipedia.org/wiki?curid=43500" title="Ganja">
Ganja

Ganja may refer to:

</doc>
<doc id="43501" url="http://en.wikipedia.org/wiki?curid=43501" title="Edith Stein">
Edith Stein

Edith Stein, also known as St. Teresa Benedicta of the Cross, OCD, (German: "Teresia Benedicta vom Kreuz", Latin: "Teresia Benedicta a Cruce") (12 October 1891 – 9 August 1942), was a German Jewish philosopher who converted to the Roman Catholic Church and became a Discalced Carmelite nun. She is a martyr and saint of the Catholic Church.
She was born into an observant Jewish family, but was an atheist by her teenage years. Moved by the tragedies of World War I, in 1915 she took lessons to become a nursing assistant and worked in a hospital for the prevention of disease outbreaks. After completing her doctoral thesis in 1916 from the University of Göttingen, she obtained an assistantship at the University of Freiburg.
From reading the works of the reformer of the Carmelite Order, St. Teresa of Jesus, OCD, she was drawn to the Catholic Faith. She was baptized on 1 January 1922 into the Roman Catholic Church. At that point she wanted to become a Discalced Carmelite nun, but was dissuaded by her spiritual mentors. She then taught at a Catholic school of education in Speyer.
As a result of the requirement of an "Aryan certificate" for civil servants promulgated by the Nazi government in April 1933 as part of its Law for the Restoration of the Professional Civil Service, she had to quit her teaching position. She was admitted to the Discalced Carmelite monastery in Cologne the following October. She received the religious habit of the Order as a novice in April 1934, taking the religious name Teresa Benedicta of the Cross ("Teresa blessed by the Cross"). In 1938 she and her sister Rosa, by then also a convert and an extern Sister of the monastery, were sent to the Carmelite monastery in Echt, Netherlands for their safety. Despite the Nazi invasion of that state in 1940, they remained undisturbed until they were arrested by the Nazis on 2 August 1942 and sent to the Auschwitz concentration camp, where they died in the gas chamber on 9 August 1942.
She was canonized by Pope Saint John Paul II in 1998. She is one of the six patron saints of Europe, together with St. Benedict of Nursia, Sts. Cyril and Methodius, St. Bridget of Sweden, and St. Catherine of Siena.
Life.
Early life.
Edith Stein was born in Breslau, in the Prussian Province of Silesia, into an observant Jewish family. She was the youngest of 11 children and was born on Yom Kippur, the holiest day of the Hebrew calendar, which combined to make her a favorite of her mother. She was a very gifted child who enjoyed learning, in a home where her mother encouraged critical thinking, and she greatly admired her mother's strong religious faith. By her teenage years, however, Edith had become an atheist.
Though her father died while she was young, her widowed mother was determined to give her children a thorough education and consequently sent Edith to study at the University of Breslau.
Academic career.
In 1916 Edith Stein received a doctorate of philosophy from the University of Freiburg with a dissertation under the philosopher Edmund Husserl, "Zum Problem der Einfühlung" ("On the Problem of Empathy"). She then became a member of the faculty at the University of Freiburg, where she worked as a teaching assistant to Husserl, who had transferred to that institution. In the previous year she had worked with Martin Heidegger in editing Husserl's papers for publication, and Heidegger succeeded her as a teaching assistant to Husserl in 1919. Because she was a woman, Husserl did not support her submitting her habilitational thesis (a prerequisite for an academic chair) to the University of Freiburg in 1918. Her other thesis, "Psychische Kausalität" ("Sentient Causality"), submitted at the University of Göttingen the following year, was likewise rejected.
While Stein had earlier contacts with Roman Catholicism, it was her reading of the autobiography of the mystic St. Teresa of Ávila during summer holidays in Bad Bergzabern in 1921 that caused her conversion. Baptized on 1 January 1922, and dissuaded by her her spiritual advisers from immediately seeking entry to the religious life, she obtained a position to teach at the Dominican nuns' school in Speyer from 1923 to 1931. While there, she translated Thomas Aquinas' "De Veritate" ("Of Truth") into German, familiarized herself with Roman Catholic philosophy in general, and tried to bridge the phenomenology of her former teacher, Husserl, to Thomism. She visited Husserl and Heidegger at Freiburg in April 1929, the same month that Heidegger gave a speech to Husserl on his 70th birthday. In 1932 she became a lecturer at the Catholic Church-affiliated Institute for Scientific Pedagogy in Münster, but antisemitic legislation passed by the Nazi government forced her to resign the post in 1933. In a letter to Pope Pius XI, she denounced the Nazi regime and asked the Pope to openly denounce the regime "to put a stop to this abuse of Christ's name."
 As a child of the Jewish people who, by the grace of God, for the past eleven years has also been a child of the Catholic Church, I dare to speak to the Father of Christianity about that which oppresses millions of Germans. For weeks we have seen deeds perpetrated in Germany which mock any sense of justice and humanity, not to mention love of neighbor. For years the leaders of National Socialism have been preaching hatred of the Jews... But the responsibility must fall, after all, on those who brought them to this point and it also falls on those who keep silent in the face of such happenings.
Everything that happened and continues to happen on a daily basis originates with a government that calls itself 'Christian.' For weeks not only Jews but also thousands of faithful Catholics in Germany, and, I believe, all over the world, have been waiting and hoping for the Church of Christ to raise its voice to put a stop to this abuse of Christ’s name.
— "Edith Stein, Letter to Pope Pius XI
Her letter received no answer, and it is not known for certain whether the Pope ever read it. However, in 1937 the Pope issued an encyclical written in German, "Mit brennender Sorge" ("With Burning Anxiety"), in which he criticized Nazism, listed violations of the Concordat between Germany and the Church of 1933, and condemned antisemitism.
Discalced Carmelite nun and martyr.
Edith Stein entered the Discalced Carmelite monastery St. Maria vom Frieden (Our Lady of Peace) in Cologne in 1933 and took the religious name of Teresa Benedicta of the Cross. There she wrote her metaphysical book "Endliches und ewiges Sein" ("Finite and Eternal Being"), which attempted to combine the philosophies of St. Thomas Aquinas and Husserl.
To avoid the growing Nazi threat, her Order transferred her and her sister, Rosa, who was also a convert and an extern sister of the Carmel, to the Discalced Carmelite monastery in Echt, Netherlands. There she wrote "Studie über Joannes a Cruce: Kreuzeswissenschaft" ("Studies on John of the Cross: The Science of the Cross"). In her testament of 6 June 1939 she wrote: "I beg the Lord to take my life and my death … for all concerns of the sacred hearts of Jesus and Mary and the holy [C]hurch, especially for the preservation of our holy [O]rder, in particular the Carmelite monasteries of Cologne and Echt, as atonement for the unbelief of the Jewish People, and that the Lord will be received by [H]is own people and [H]is kingdom shall come in glory, for the salvation of Germany and the peace of the world, at last for my loved ones, living or dead, and for all God gave to me: that none of them shall go astray."
Stein’s move to Echt prompted her to be more devout and an even greater subscriber to the Carmelite lifestyle. After having her teaching position revoked by the implementation of the Law for the Restoration of the Professional Civil Service, Stein quickly eased back into the role of instructor at the convent in Echt, teaching both fellow sisters and students within the community Latin and philosophy.
Even prior to the Nazi occupation of the Netherlands, Stein believed she would not survive the war, going as far to write the Prioress to request her permission to “allow [Stein] to offer [her]self to the heart of Jesus as a sacrifice of atonement for true peace” and created a will. Her fellow sisters would later recount how Stein begin “quietly training herself for life in a concentration camp, by enduring cold and hunger” after the Nazi invasion of the Netherlands in May 1940.
Ultimately, she was not safe in the Netherlands. The Dutch Bishops' Conference had a public statement read in all the churches of the nation on 20 July 1942 condemning Nazi racism. In a retaliatory response on 26 July 1942 the Reichskommissar of the Netherlands, Arthur Seyss-Inquart ordered the arrest of all Jewish converts who had previously been spared. Along with two hundred and forty-three baptized Jews living in the Netherlands, Stein was arrested by the SS on 2 August 1942. Stein and her sister, Rosa, were imprisoned at the concentration camps of Amersfoort and Westerbork before being deported to Auschwitz. A Dutch official at Westerbork was so impressed by her sense of faith and calm, he offered her an escape plan. Stein vehemently denied his assistance, stating, “If somebody intervened at this point and took away her chance to share in the fate of her brothers and sisters, that would be utter annihilation.”
On 7 August 1942, early in the morning, 987 Jews were deported to the Auschwitz concentration camp. It was probably on 9 August that Sr. Teresa Benedicta of the Cross, her sister, and many more of her people were killed in a mass gas chamber.
Legacy and veneration.
Edith Stein was beatified as a martyr on 1 May 1987 in Cologne, Germany by Pope St. John Paul II and then canonized by him 11 years later on 11 October 1998 in Vatican City. The miracle which was the basis for her canonization was the cure of Teresa Benedicta McCarthy, a little girl who had swallowed a large amount of paracetamol (acetaminophen), which causes hepatic necrosis. Her father, Rev. Emmanuel Charles McCarthy, a priest of the Melkite Greek Catholic Church, immediately rounded up relatives and prayed for St. Teresa's intercession. Shortly thereafter the nurses in the intensive care unit saw her sit up completely healthy. Dr. Ronald Kleinman, a pediatric specialist at Massachusetts General Hospital in Boston who treated Teresa Benedicta, testified about her recovery to Church tribunals, stating: "I was willing to say that it was miraculous." McCarthy would later attend St. Teresa's canonization.
Today there are many schools named in tribute to her, for example in Darmstadt, Germany, Hengelo, Netherlands, and Mississauga, Ontario, Canada. Also named for her are a women's dormitory at the University of Tübingen and a classroom building at The College of the Holy Cross in Worcester, Massachusetts.
The philosopher Alasdair MacIntyre published a book in 2006 titled "Edith Stein: A Philosophical Prologue, 1913-1922", in which he contrasted her living of her own personal philosophy with Martin Heidegger, whose actions during the Nazi era, according to MacIntyre, suggested a "bifurcation of personality."
In 2009 her bust was installed at the Walhalla Memorial near Regensburg, Germany. In June 2009 the International Association for the Study of the Philosophy of Edith Stein (IASPES) was founded, and held its first international conference at Maynooth University, Ireland in order to advance the philosophical writings of Edith Stein.
On 6 June 2014, the 70th anniversary of D-Day, a bell dedicated to her was named by Prince Charles at Bayeux Cathedral.
Also in 2014, the book "Edith Stein and Regina Jonas: Religious Visionaries in the Time of the Death Camps", by Emily Leah Silverman, was published.
Controversy as to the cause of her murder.
The beatification of St. Teresa as a martyr generated criticism. Critics argued that she was murdered because she was Jewish by birth, rather than for her Catholic Faith, and that, in the words of Daniel Polish, the beatification seemed to "carry the tacit message encouraging conversionary activities" because "official discussion of the beatification seemed to make a point of conjoining Stein's Catholic faith with her death with 'fellow Jews' in Auschwitz". The position of the Catholic Church is that St. Teresa also died because of the Dutch episcopacy's public condemnation of Nazi racism in 1942; in other words, that she died because of the moral teaching of the Church and is thus a true martyr.

</doc>
<doc id="43505" url="http://en.wikipedia.org/wiki?curid=43505" title="Ross Perot">
Ross Perot

Henry Ross Perot (; born June 27, 1930) is an American businessman best known for being an independent presidential candidate in 1992 and the Reform party presidential candidate in 1996. Perot founded Electronic Data Systems (EDS) in 1962, sold the company to General Motors in 1984, and founded Perot Systems in 1988. Perot Systems was bought by Dell for $3.9 billion in 2009.
With an estimated net worth of about US$3.5 billion in 2012, he is ranked by "Forbes" as the 134th-richest person in the United States.
Early life.
Perot was born in Texarkana, Texas, to Lula May Perot (née Ray) and Gabriel Ross Perot, a commodity broker specializing in cotton contracts. His patrilineal line traces back to a French-Canadian immigrant to Louisiana, in the 1740s. He attended a private school called Patty Hill. He graduated from Texas High School in Texarkana in 1947. One of Perot's boyhood friends was Hayes McClerkin, later Speaker of the Arkansas House of Representatives and a prominent Texarkana, Arkansas, lawyer.
Perot joined the Boy Scouts of America and made Eagle Scout in 1942, after thirteen months in the program. He is a recipient of the Distinguished Eagle Scout Award.
From 1947 to 1949, he attended Texarkana Junior College, then entered the U.S. Naval Academy in 1949 and helped establish its honor system. Perot said his appointment notice to the academy—sent by telegram—was sent by W. Lee "Pappy" O'Daniel, Texas' 34th governor and former senator.
Perot married Margot Birmingham of Greensburg, Pennsylvania, in 1956.
Business.
After he left the Navy in 1957, Perot became a salesman for International Business Machines. He quickly became a top employee (one year, he fulfilled his annual sales quota in a mere two weeks) and tried to pitch his ideas to supervisors who largely ignored him. He left IBM in 1962 to found Electronic Data Systems (EDS) in Dallas, Texas, and courted large corporations for his data processing services. Perot was refused seventy-seven times before he was given his first contract. EDS received lucrative contracts from the U.S. government in the 1960s, computerizing Medicare records. EDS went public in 1968 and the stock price rose from $16 a share to $160 within days. "Fortune" called Perot the "fastest, richest Texan" in a 1968 cover story. In 1984 General Motors bought controlling interest in EDS for $2.4 billion.
In 1974 Perot gained some press attention for being "the biggest individual loser ever on the New York Stock Exchange" when his EDS shares dropped $450 million in value in a single day in April 1970.
Just prior to the 1979 Iranian Revolution, the government of Iran imprisoned two EDS employees in a contract dispute. Perot organized and sponsored their rescue. The rescue team was led by retired U.S. Army Special Forces Colonel Arthur D. "Bull" Simons. When the team was unable to find a way to extract their two prisoners, they decided to wait for a mob of pro-Ayatollah revolutionaries to storm the jail and free all 10,000 inmates, many of whom were political prisoners. The two prisoners then connected with the rescue team, and the team spirited them out of Iran via a risky border crossing into Turkey. The exploit was recounted in a book, "On Wings of Eagles" by Ken Follett, which became a best-seller. In the 1986 miniseries, Perot was portrayed by Richard Crenna.
In 1984 Perot bought a very early copy of the Magna Carta, one of only a few to leave the United Kingdom. It was lent to the National Archives in Washington, D.C., where it was displayed alongside the Declaration of Independence and the United States Constitution. In 2007, it was sold by the Perot Foundation, in order to provide "for medical research, for improving public education and for assisting wounded soldiers and their families." The document sold for US$21.3 million on December 18, 2007, to David Rubenstein, managing director of the Carlyle Group and kept on display at the National Archives.
As Steve Jobs lost the original power struggle at Apple and left to found NeXT, his angel investor was Perot who invested over 20 million dollars. Perot believed in Jobs and did not want to miss out, as he had with his chance to invest in Bill Gates' fledgling Microsoft.
In 1988 he founded Perot Systems Corporation, Inc. in Plano, Texas. His son, Ross Perot, Jr., eventually succeeded him as CEO. In September 2009, Perot Systems was acquired by Dell for $3.9 billion.
Early political activities.
Perot became heavily involved in the Vietnam War POW/MIA issue. He believed that hundreds of American servicemen were left behind in Southeast Asia at the end of the U.S. involvement in the war, and that government officials were covering up POW/MIA investigations in order to avoid revealing a drug smuggling operation used to finance a secret war in Laos. Perot engaged in unauthorized back-channel discussions with Vietnamese officials in the late 1980s, which led to fractured relations between Perot and the Reagan and George H.W. Bush administrations. In 1990, Perot reached agreement with Vietnam's Foreign Ministry to become its business agent in the event that diplomatic relations were normalized. Perot also launched private investigations of, and attacks upon, U.S. Department of Defense official Richard Armitage.
In Florida in 1990, retired financial planner Jack Gargan funded a series of "I'm mad as hell and I'm not going to take it anymore" (a reference to a famous quotation from the 1976 political and mass media satire movie, "Network") newspaper advertisements denouncing the U.S. Congress for voting for legislative pay raises at a time when average wages nationwide were not increasing. Gargan later founded "Throw the Hypocritical Rascals Out" (THRO), which Ross Perot supported.
Maya Lin, architect of the Vietnam Veterans Memorial received harassment from Perot after her race was revealed; he was known to have called her an "egg roll" after it was revealed that she was Asian.
Perot did not support President George H. W. Bush and vigorously opposed the United States involvement in the 1990–1991 Persian Gulf War. He unsuccessfully urged Senators to vote against the war resolution, and began to consider his own presidential run.
1992 presidential candidacy.
On February 20, 1992, he appeared on CNN's "Larry King Live" and announced his intention to run as an independent if his supporters could get his name on the ballot in all fifty states. With such declared policies as balancing the federal budget, opposition to gun control, ending the outsourcing of jobs and enacting electronic direct democracy via "electronic town halls", he became a potential candidate and soon polled roughly even with the two major party candidates.
Perot's candidacy received increasing media attention when the competitive phase of the primary season ended for the two major parties. With the insurgent candidacies of Republican Pat Buchanan and Democrat Jerry Brown winding down, Perot was the natural beneficiary of populist resentment toward establishment politicians. On May 25, 1992, he was featured on the cover of "Time" with the title "Waiting for Perot", an allusion to Samuel Beckett's play "Waiting for Godot".
Several months before the Democratic and Republican conventions, Perot filled the vacuum of election news, as his supporters began petition drives to get him on the ballot in all fifty states. This sense of momentum was reinforced when Perot employed two savvy campaign managers in Democrat Hamilton Jordan and Republican Ed Rollins. In July, while Perot was pondering whether to run for office, his supporters established a campaign organization United We Stand America. Perot was late in making formal policy proposals, but most of what he did call for were intended to reduce the deficit. He wanted a gasoline tax increase and some cutbacks of Social Security.
In June, Perot led in the Gallop poll with a 39 percent rating. By mid-July, the "Washington Post" reported that Perot's campaign managers were becoming increasingly disillusioned by his unwillingness to follow their advice to be more specific on issues, and his need to be in full control of operations with such tactics as forcing volunteers to sign loyalty oaths. Perot's poll numbers began to slip to 25%, and his advisers warned that if he continued to ignore them, he would fall into single digits. Co-manager Hamilton Jordan threatened to quit, and on July 15, Ed Rollins resigned after Perot fired advertisement specialist Hal Riney, who worked with Rollins on the Reagan campaign. Rollins would later claim that a member of the campaign accused him of being a Bush plant with ties to the CIA. Amidst the chaos, Perot's support fell to 20%. The next day, Perot announced on "Larry King Live" that he would not seek the presidency. He explained that he did not want the House of Representatives to decide the election if the result caused the electoral college to be split. Perot eventually stated the reason was that he received threats that digitally altered photographs would be released by the Bush campaign to sabotage his daughter's wedding. Regardless of the reasons for withdrawing, his reputation was badly damaged. Many of his supporters felt betrayed and public opinion polls would subsequently show a large negative view of Perot that was absent prior to his decision to end the campaign.
In September, he qualified for all fifty state ballots. On October 1, he announced his intention to reenter the presidential race. He said that Republican operatives had wanted to reveal compromising photographs of his daughter, which would disrupt her wedding, and he wanted to spare her from embarrassment. He campaigned in 16 states and spent an estimated $12.3 million of his own money. Perot employed the innovative strategy of purchasing half-hour blocks of time on major networks for infomercial-type campaign advertisements; this advertising garnered more viewership than many sitcoms, with one Friday night program in October attracting 10.5 million viewers.
Perot's running mate was retired Vice Admiral James Stockdale, a highly decorated former Vietnam prisoner of war (POW). In December 1969 he organized and flew to North Vietnam in an attempt to deliver thirty tons of supplies to beleaguered American POWs in North Vietnam. Although North Vietnam blocked the flights, the effort was instrumental in bringing the plight of those POWs to the world's attention and their captors soon began treating them better.
At one point in June, Perot led the polls with 39% (versus 31% for Bush and 25% for Clinton). Just prior to the debates, Perot received 7–9% support in nationwide polls. It is likely that the debates played a significant role in his ultimate receipt of 19% of the popular vote. Although his answers during the debates were often general, many Democrats and Republicans conceded that Perot won at least the first debate. In the debate he remarked: "Keep in mind our Constitution predates the Industrial Revolution. Our founders did not know about electricity, the train, telephones, radio, television, automobiles, airplanes, rockets, nuclear weapons, satellites, or space exploration. There's a lot they didn't know about. It would be interesting to see what kind of document they'd draft today. Just keeping it frozen in time won't hack it."
Perot denounced Congress for its inaction in his speech at the National Press Club in Washington, D.C., on March 18, 1992. Perot said:This city has become a town filled with sound bites, shell games, handlers, media stuntmen who posture, create images, talk, shoot off Roman candles, but don't ever accomplish anything. We need deeds, not words, in this city.
In the 1992 election, he received 18.9% of the popular vote, approximately 19,741,065 votes (but no electoral college votes), making him the most successful third-party presidential candidate in terms of the popular vote since Theodore Roosevelt in the 1912 election. Unlike Perot, however, some other third party candidates since Roosevelt have won electoral college votes. (Robert La Follette had 13 in 1924, Strom Thurmond had 39 in 1948, George Wallace had 46 in 1968 and John Hospers won one in 1972). Compared with Thurmond and Wallace, who polled very strongly in a small number of states, Perot's vote was more evenly spread across the country. Perot managed to finish second in two states: In Maine, Perot received 30.44% of the vote to Bush's 30.39% (Clinton won Maine with 38.77%); in Utah, Perot received 27.34% of the vote to Clinton's 24.65% (Bush won Utah with 43.36%). Although Perot won no state, he received the most votes in some counties, including Trinity County, California.
A detailed analysis of voting demographics revealed that Perot's support drew heavily from across the political spectrum, with 20% of his votes coming from self-described liberals, 27% from self-described conservatives, and 53% coming from self-described moderates. Economically, however, the majority of Perot voters (57%) were middle class, earning between $15,000 and $49,000 annually, with the bulk of the remainder drawing from the upper middle class (29% earning more than $50,000 annually). Exit polls also showed that Ross Perot drew 38% of his vote from Bush, and 38% of his vote from Clinton, while the rest of his voters would have stayed home had he not been on the ballot.
Based on his performance in the popular vote in 1992, Perot was entitled to receive federal election funding for 1996. Perot remained in the public eye after the election and championed opposition to the North American Free Trade Agreement (NAFTA), urging voters to listen for the "giant sucking sound" of American jobs heading south to Mexico should NAFTA be ratified.
Reform Party and 1996 presidential run.
Perot tried to keep his movement alive through the mid-1990s, continuing to speak about the increasing national debt. He was a prominent campaigner against the North American Free Trade Agreement, and even debated with Al Gore on the issue on "Larry King Live". Perot's behavior during the debate was a source of mirth thereafter, including his repeated pleas to "let me finish" in his southern drawl. The debate was seen by many as effectively ending Perot’s political career. Support for NAFTA went from 34% to 57%.
In 1995, he founded the Reform Party and won their presidential nomination for the 1996 election. His vice presidential running mate was Pat Choate. Because of the ballot access laws, he had to run as an Independent on many state ballots. Perot received eight percent of the popular vote in 1996, much less than in the 1992 race but still an unusually successful third-party showing by U.S. standards. He spent much less of his own money in this race than he had four years before, and also allowed other people to contribute to his campaign, unlike his prior race. One common explanation for the decline was Perot's exclusion from the presidential debates, based on the preferences of the Democratic and Republican party candidates. Jamin B. Raskin of "Open Debates" filed a lawsuit on Perot's behalf on this assumption.
Later activities.
Later in the 1990s, Perot's detractors accused him of not allowing the Reform Party to develop into a genuine national political party, but rather using it as a vehicle to promote himself. They cited as evidence the control of party offices by operatives from his presidential campaigns. Perot did not give an endorsement during Jesse Ventura's run for governor of Minnesota in the 1998 election, and this became suspicious to detractors when he made fun of Ventura at a conference after Ventura had a falling out with the press. The party leadership grew in tighter opposition to groups supporting Ventura and Jack Gargan. Evidence of this was demonstrated when Gargan was officially removed as Reform Party Chairman by the Reform Party National Committee.
In the 2000 presidential election, Perot refused to become openly involved with the internal Reform Party dispute between supporters of Pat Buchanan and of John Hagelin. Perot was reportedly unhappy with what he saw as the disintegration of the party, as well as his own portrayal in the press; thus he chose to remain quiet. He appeared on "Larry King Live" four days before the election and endorsed George W. Bush for president. Despite his earlier opposition to NAFTA, Perot remained largely silent about expanded use of guest worker visas in the United States, with Buchanan supporters attributing this silence to his corporate reliance on foreign workers. Some state parties affiliated with the new (Buchananite) America First Party; others gave Ralph Nader their ballot lines in the 2004 presidential election.
Since then, Perot has been largely silent on political issues, refusing to answer most questions from the press. When interviewed, he usually remains on the subject of his business career and refuses to answer specific questions on politics, candidates, or his past activities.
The one exception to this came in 2005, when he was asked to testify before the Texas Legislature in support of proposals to extend technology to students, including making laptops available to them; additionally, changing the process of buying textbooks, by making electronic books (ebooks) available and by allowing schools to buy books at the local level instead of going through the state. In an April 2005 interview, Perot expressed concern about the state of progress on issues that he had raised in his presidential runs.
In January 2008, Perot publicly came out against Republican candidate John McCain and endorsed Mitt Romney for President. He also announced that he would soon be launching a new website with updated economic graphs and charts. In June 2008, the blog launched, focusing on entitlements (Medicare, Medicaid, Social Security), the U.S. national debt and related issues.
In 2012, Perot endorsed Mitt Romney for President.
Family.
Perot and his wife Margot (née Birmingham) have five children (Ross Jr., Nancy, Suzanne, Carolyn, and Katherine). s of 2012[ [update]], the Perots had 16 grandchildren.
Electoral history.
United States presidential election, 1992
United States presidential election, 1996

</doc>
<doc id="43506" url="http://en.wikipedia.org/wiki?curid=43506" title="Axis of evil">
Axis of evil

U.S. President George W. Bush used the term Axis of evil in his State of the Union Address on January 29, 2002, and often repeated it throughout his presidency, to describe governments that he accused of helping terrorism and seeking weapons of mass destruction. Iran, Iraq, and North Korea were portrayed by George W. Bush during the State of the Union as building nuclear weapons. The Axis of Evil was used to pinpoint these common enemies of the United States and rally the country in support of the War on Terror.
2002 State of the Union.
In his 2002 State of the Union Address, Bush also called North Korea "A regime arming with missiles and weapons of mass destruction, while starving its citizens." He also stated Iran "aggressively pursues these weapons and exports terror, while an unelected few repress the Iranian people's hope for freedom." Of the three nations Bush cited, however, he gave the most criticism to Iraq.
He stated "Iraq continues to flaunt its hostility toward America and to support terror. The Iraqi regime has plotted to develop anthrax and nerve gas and nuclear weapons for over a decade. This is a regime that has already used poison gas to murder thousands of its own citizens, leaving the bodies of mothers huddled over their dead children. This is a regime that agreed to international inspections, then kicked out the inspectors. This is a regime that has something to hide from the civilized world." Afterwards, Bush said, "States like these and their terrorist allies constitute an axis of evil, arming to threaten the peace of the world."
David Frum.
The phrase was attributed to former Bush speechwriter David Frum, originally as the "axis of hatred" and then "evil". Frum explained his rationale for creating the phrase "axis of evil" in his book "The Right Man: The Surprise Presidency of George W. Bush". Essentially, the story begins in late December 2001 when head speechwriter Michael Gerson gave Frum the assignment of articulating the case for dislodging the government of Saddam Hussein in Iraq in only a few sentences for the upcoming State of the Union address. Frum says he began by rereading President Franklin D. Roosevelt's "date which will live in infamy" speech given on December 8, 1941, after the Japanese surprise attack on Pearl Harbor. While Americans needed no convincing about going to war with Japan, Roosevelt saw the greater threat to the United States coming from Nazi Germany, and he had to make the case for fighting a two-ocean war.
Frum points in his book to a now often-overlooked sentence in Roosevelt's speech which reads in part, "...we will not only defend ourselves to the uttermost but will make very certain that this form of treachery shall never endanger us again." Frum interprets Roosevelt's oratory like this: "For FDR, Pearl Harbor was not only an attack—it was a warning of future and worse attacks from another, even more dangerous enemy." Japan, a country with one-tenth of America's industrial capacity, a dependence on imports for its food, and already engaged in a war with China, was extremely reckless to attack the United States, a recklessness "that made the Axis such a menace to world peace", Frum says. Saddam Hussein's two wars, against Iran and Kuwait, were just as reckless, Frum decided, and therefore presented the same threat to world peace.
In his book Frum relates that the more he compared the Axis powers of World War II to modern "terror states", the more similarities he saw. "The Axis powers disliked and distrusted one another", Frum writes. "Had the Axis somehow won the war, its members would quickly have turned on one another." Iran, Iraq, al-Qaeda, and Hezbollah, despite quarreling among themselves, "all resented power of the West and Israel, and they all despised the humane values of democracy." There, Frum saw the connection: "Together, the terror states and the terror organizations formed an axis of hatred against the United States."
Frum tells that he then sent off a memo with the above arguments and also cited some of the atrocities perpetrated by the Iraqi government. He expected his words to be chopped apart and altered beyond recognition, as is the fate of much presidential speechwriting, but his words were ultimately read by Bush nearly verbatim, though Bush changed the term "axis of hatred" to "axis of evil". North Korea was added to the list, he says, because it was attempting to develop nuclear weapons, had a history of reckless aggression, and "needed to feel a stronger hand".
Afterwards, Frum's wife disclosed his authorship to the public.
Yossef Bodansky.
A decade before the 2002 State of the Union address, in August 1992, the political scientist Yossef Bodansky wrote a paper entitled "Tehran, Baghdad & Damascus: The New Axis Pact" while serving as the Director of the Congressional Task Force on Terrorism and Unconventional Warfare of the US House of Representatives. Although he did not explicitly apply the epithet "evil" to his New Axis, Bodansky's axis was otherwise very reminiscent of Frum's axis. Bodansky felt that this new Axis was a very dangerous development. The gist of Bodansky's argument was that Iran, Iraq and Syria had formed a "tripartite alliance" in the wake of the First Gulf War, and that this alliance posed an imminent threat that could only be dealt with by invading Iraq a second time and overthrowing Saddam Hussein.
Development.
Bolton: "Beyond the Axis of Evil".
On May 6, 2002, then-Undersecretary of State John R. Bolton gave a speech entitled "Beyond the Axis of Evil". In it he added three more nations to be grouped with the already mentioned rogue states: Cuba, Libya, and Syria. The criteria for inclusion in this grouping were: "state sponsors of terrorism that are pursuing or who have the potential to pursue weapons of mass destruction (WMD) or have the capability to do so in violation of their treaty obligations".
Rice: Outposts of Tyranny.
In January 2005, at the beginning of Bush's second term as President, the incoming Secretary of State, Condoleezza Rice, made a speech regarding the newly termed "outposts of tyranny", a list of six countries deemed most repressive. This included the two remaining "Axis" members, as well as Cuba, Belarus, Burma and Zimbabwe.
Criticism.
No coordination.
One criticism is that unlike the Axis powers, the three nations mentioned in Bush's speech did not coordinate public policy, and therefore the term "axis" is incorrect. Also, while the Axis Powers of the Second World War signed diplomatic treaties with one another, such as the Pact of Steel and the Tripartite Pact, that created a military alliance between them, none of the nations that make up the "axis of evil" have taken similar steps publicly, nor are they known to have done so secretly according to present intelligence records.
No category.
In addition, Iran and Iraq fought the long Iran–Iraq War in the 1980s, under basically the same leadership as that which existed at the time of Bush's speech, leading some to believe that the linking of the nations under the same banner was misguided. Others argued that each of the three nations in the "axis of evil" had some special characteristics which were obscured by grouping them together. Anne Applebaum wrote about the debate over North Korea's inclusion in the group.
Effect on U.S.–Iran relations.
In the days after the 9/11 attacks, Crocker and other senior U.S. State Department officials flew to Geneva to meet secretly with representatives of the government of Iran. For several months, Crocker and his Iranian counterparts cooperated on capturing Al Qaeda operatives in the region and fighting the Taliban government in Afghanistan. These meetings stopped after the "Axis of Evil" speech hardened Iranian attitudes toward cooperating with the U.S.
Other axes.
Axis of terror.
In January 2006, Israeli Defense Minister Shaul Mofaz implicated "the axis of terror that operates between Iran and Syria" following a suicide bomb in Tel Aviv.
In April 2006 the phrase "axis of terror" earned more publicity. Israel's UN Ambassador, Dan Gillerman, cautioned of a new "axis of terror"—Iran, Syria and the Hamas-run Palestinian government; Gillerman repeated the term before the UN over the crisis in Lebanon. Some three months later Israeli senior foreign ministry official Gideon Meir branded the alleged alliance an "axis of terror and hate".
Axis of belligerence.
In 2006, Isaias Afewerki, the president of Eritrea, had declared in response to the deteriorating relations with the neighboring countries of Ethiopia, Sudan and Yemen by accusing them of being an "Axis of Belligerence."
Axis of good.
The former president of Venezuela, Hugo Chávez, described the so-called New Latin Left as an "axis of good" comprising Bolivia, Chile, Cuba, Ecuador, Nicaragua, Uruguay and Venezuela (all countries now governed by leftist political leaders) and instead "Washington and its allies" as an "axis of evil".
Axis of diesel.
"The Economist" referred to an article about the term apparently used more often in reference to a burgeoning alliance of Iran, Russia, and Venezuela. They cite the billions of dollars in arms sales to Venezuela and the construction of Iranian nuclear facilities as well as the rejection of added sanctions on Iran. They did conclude that the benefits of the arrangement were exaggerated, however.
Axis of environmental evil.
Several environmental non-governmental organizations, including Friends of the Earth International and Greenpeace, as well as the Green Party of Canada, have dubbed Australia, Canada and United States, the "Axis of Environmental Evil" because of their lack of support for international environmental agreements, particularly those related to climate change.
Media use.
Parodies.
Various related pun phrases include:
The term has also lent itself to various parodies, including the following:
Bill Bailey.
British Comedian Bill Bailey also referred to the Axis of Evil in his Part Troll tour. He queried whether it was possible to assume a non-evil role within a terrorist organisation, possibly in the laundry or catering department. He then went on to pretend that he was the receptionist for the Axis of Evil. Imagining he was answering the phone, Bill Bailey says to the audience, "Hello, Axis of Evil. Oh no, they're all out at the moment. Oh, I don't know. Doing something evil I suppose". Placing the "caller" on hold, he then played a short jingle for the "Axis of Evil Pension Scheme".
Comedy Tour.
In response to the problems Americans of Middle-Eastern descent have in the current climate, a group of comedians have banded together to form the "Axis of Evil Comedy Tour". The comedians, Ahmed Ahmed (from Egypt), Maz Jobrani (from Iran), and Aron Kader (whose father is Palestinian), have created a show which currently plays on Comedy Central. They have also included half-Palestinian, half-Italian Dean Obeidallah in some of their acts.
The group recently took the comedy tour around the Middle East (November - December 2007), performing in the U.A.E., Egypt, Kuwait, Jordan, and Lebanon to sell-out crowds.
Lullabies.
In 2003 the Norwegian record label Kirkelig Kulturverksted published the CD "Lullabies from the Axis of Evil" containing 14 lullabies from Iran, Iraq, North Korea, Palestine, Syria, Afghanistan and Cuba. Every lullaby is presented in its original form sung by women from these countries, and then a western version with interpretations in English.
Words Without Borders.
In 2007 the online magazine "Words Without Borders" published its first anthology titled "Literature From The "Axis of Evil"". The anthology contains works from Syria, Cuba, Libya, Sudan, Iran, Iraq and North Korea. The works included are typically non-political in nature, and are intended to further a human understanding of life inside the countries designated as part of the "Axis". Sudan (included in the anthology) has never been given an "Axis" or "Beyond the Axis" designation by proponents of the terms.
Cosmology.
In cosmology, the "axis of evil" is the pattern that is left imprinted on the radiation left behind by the Big Bang. The pattern itself is an alignment of hot and cold spots in the Cosmic Microwave Background that seemingly defies the standard isotropic model of the Universe. Discovered and named in 2005 by Kate Land and João Magueijo of Imperial College, London, the pattern is controversial and disputed amongst scientists, though two independent studies have confirmed its existence.

</doc>
<doc id="43507" url="http://en.wikipedia.org/wiki?curid=43507" title="Axis powers">
Axis powers

The Axis powers (German: "Achsenmächte", Japanese: 枢軸国 "Sūjikukoku", Italian: "Potenze dell'Asse"), also known as the Axis, were the nations that fought in the Second World War against the Allied forces. The Axis powers agreed on their opposition to the Allies, but did not coordinate their wars.
The Axis grew out of the diplomatic efforts of Germany, Italy and Japan to secure their own specific expansionist interests in the mid-1930s. The first step was the treaty signed by Germany and Italy in October 1936. Mussolini declared on November 1 that all other European countries would from then on rotate on the Rome-Berlin axis, thus creating the term "Axis". The almost simultaneous second step was the signing in November 1936 of the Anti-Comintern Pact, an anti-communist treaty between Germany and Japan. Italy joined the Pact in 1937. The "Rome–Berlin Axis" became a military alliance in 1939 under the so-called "Pact of Steel", with the Tripartite Pact of 1940 leading to the integration of the military aims of Germany and its two treaty-bound allies.
At its zenith during World War II, the Axis presided over territories that occupied large parts of Europe, North Africa, and East Asia. There were no three-way summit meetings and cooperation and coordination was minimal, with a bit more between Germany and Italy. The war ended in 1945 with the defeat of the Axis powers and the dissolution of their alliance. As in the case of the Allies, membership of the Axis was fluid, with some nations switching sides or changing their degree of military involvement over the course of the war.
Origins and creation.
The term "axis" was first applied to the Italo-German relationship by the Italian prime minister Benito Mussolini in September 1923, when he wrote in the preface to Roberto Suster's "Germania Repubblica" that "there is no doubt that in this moment the axis of European history passes through Berlin" ("non v'ha dubbio che in questo momento l'asse della storia europea passa per Berlino"). At the time he was seeking an alliance with the Weimar Republic against Yugoslavia and France in the dispute over the Free State of Fiume.
The term was used by Hungary's prime minister Gyula Gömbös when advocating an alliance of Hungary with Germany and Italy in the early 1930s. Gömbös' efforts did effect the Italo-Hungarian Rome Protocols, but his sudden death in 1936 while negotiating with Germany in Munich and the arrival of Kálmán Darányi, his successor, ended Hungary's involvement in pursuing a trilateral axis. Contentious negotiations between the Italian foreign minister, Galeazzo Ciano, and the German ambassador, Ulrich von Hassell, resulted in a Nineteen-Point Protocol, signed by Ciano and his German counterpart, Konstantin von Neurath, in 1936. When Mussolini publicly announced the signing on 1 November, he proclaimed the creation of a Rome–Berlin axis.
Initial proposals of a German-Italian alliance.
Italy under "Duce" Benito Mussolini had pursued a strategic alliance of Italy with Germany against France since the early 1920s. Prior to becoming head of government in Italy as leader of the Italian Fascist movement, Mussolini had advocated alliance with recently defeated Germany after the Paris Peace Conference of 1919 settled World War I. He believed that Italy could expand its influence in Europe by allying with Germany against France. In early 1923, as a goodwill gesture to Germany, Italy secretly delivered weapons for the German Army, which had faced major disarmament under the provisions of the Treaty of Versailles.
In September 1923, Mussolini offered German Chancellor Gustav Stresemann a "common policy": he sought German military support against potential French military intervention over Italy's diplomatic dispute with Yugoslavia over Fiume, should an Italian seizure of Fiume result in war between Italy and Yugoslavia. The German ambassador to Italy in 1924 reported that Mussolini saw a nationalist Germany as an essential ally for Italy against France, and hoped to tap into the desire within the German army and the German political right for a war of revenge against France.
During the Weimar Republic, the German government did not respect the Treaty of Versailles that it had been pressured to sign, and various government figures at the time rejected Germany's post-Versailles borders. General Hans von Seeckt (head of the "Reichswehr" command from 1920 to 1926) supported an alliance between Germany and the Soviet Union to invade and partition Poland between them and restore the German-Russian border of 1914. Gustav Streseman as German foreign minister in 1925 declared that the reincorporation of territories lost to Poland and Danzig in the Treaty of Versailles was a major task of German foreign policy The "Reichswehr" Ministry memorandum of 1926 declared its intention to seek the reincorporation of German territory lost to Poland as its first priority, to be followed by the return of the Saar territory, the annexation of Austria, and remilitarization of the Rhineland.
Since the 1920s Italy had identified the year 1935 as a crucial date for preparing for a war against France, as 1935 was the year when Germany's obligations under the Treaty of Versailles were scheduled to expire.
Meetings took place in Berlin in 1924 between Italian General Luigi Capello and prominent figures in the German military, such as von Seeckt and Erich von Ludendorff, over military collaboration between Germany and Italy. The discussions concluded that Germans still wanted a war of revenge against France but were short on weapons and hoped that Italy could assist Germany.
However at this time Mussolini stressed one important condition that Italy must pursue in an alliance with Germany: that Italy "must ... tow them, not be towed by them". Italian foreign minister Dino Grandi in the early 1930s stressed the importance of "decisive weight", involving Italy's relations between France and Germany, in which he recognized that Italy was not yet a major power, but perceived that Italy did have strong enough influence to alter the political situation in Europe by placing the weight of its support onto one side or another. However Grandi stressed that Italy must seek to avoid becoming a "slave of the rule of three" in order to pursue its interests, arguing that although substantial Italo-French tensions existed, Italy would not unconditionally commit itself to an alliance with Germany, just as it would neither unconditionally commit itself to an alliance with France over conceivable Italo-German tensions. Grandi's attempts to maintain a diplomatic balance between France and Germany were challenged in 1932 by pressure from the French, who had begun to prepare an alliance with Britain and the United States against the threat of a revanchist Germany. The French government warned Italy that it had to choose whether to be on the side of the pro-Versailles powers or that of the anti-Versailles revanchists. Grandi responded that Italy would be willing to offer France support against Germany if France gave Italy its mandate over Cameroon and allowed Italy a free hand in Ethiopia. France refused Italy's proposed exchange for support, as it believed Italy's demands were unacceptable and the threat from Germany was not yet immediate.
On 23 October 1932, Mussolini declared support for a Four Power Directorate that included Britain, France, Germany, and Italy, to bring about an orderly treaty revision outside of what he considered the outmoded League of Nations. The proposed Directorate was pragmatically designed to reduce French hegemony in continental Europe, in order to reduce tensions between the great powers in the short term to buy Italy relief from being pressured into a specific war alliance while at the same time allowing them to benefit from diplomatic deals on treaty revisions.
Danube alliance, dispute over Austria.
In 1932, Gyula Gömbös and the Party of National Unity rose to power in Hungary, and immediately sought an alliance with Italy. Gömbös sought to alter Hungary's post-Treaty of Trianon borders, but knew that Hungary alone was not capable of challenging the Little Entente powers by forming an alliance with Austria and Italy. Mussolini was elated by Gömbös' offer of alliance with Italy, and they co-operated in seeking to persuade Austrian Chancellor Engelbert Dollfuss to join a tripartite economic agreement with Italy and Hungary. At the meeting between Gömbös and Mussolini in Rome on 10 November 1932, the question came up of the sovereignty of Austria in relation to the predicted rise to power in Germany of the Nazi Party. Mussolini was worried about Nazi ambitions towards Austria, and indicated that at least in the short term he was committed to maintaining Austria as a sovereign state. Italy had concerns over a Germany which included Austria laying land claims to German-populated territories of the South Tyrol (also known as Alto-Adige) within Italy, which bordered Austria on the Brenner Pass. Gömbös responded to Mussolini that as the Austrians primarily identified as Germans, the Anschluss of Austria to Germany was inevitable, and advised that it would be better for Italy to have a friendly Germany across the Brenner Pass than a hostile Germany bent on entering the Adriatic. Mussolini said he hoped the Anschluss could be postponed as long as possible until the breakout of a European war that he estimated would begin in 1938.
In 1933, Adolf Hitler and the Nazi Party came to power in Germany. His first diplomatic visitor was Gömbös. In a letter to Hitler within a day of his being appointed Chancellor, Gömbös told the Hungarian ambassador to Germany to remind Hitler "that ten years ago, on the basis of our common principles and ideology, we were in contact via Dr. Scheubner-Richter". Gömbös told the Hungarian ambassador to inform Hitler of Hungary's intentions "for the two countries to cooperate in foreign and economic policy".
Hitler had advocated an alliance between Germany and Italy since the 1920s. Shortly after being appointed Chancellor, Hitler sent a personal message to Mussolini, declaring "admiration and homage" and declaring his anticipation of the prospects of German-Italian friendship and even alliance. Hitler was aware that Italy held concerns over potential German land claims on South Tyrol, and assured Mussolini that Germany was not interested in South Tyrol. Hitler in "Mein Kampf" had declared that South Tyrol was a non-issue considering the advantages that would be gained from a German-Italian alliance. After Hitler's rise to power, the Four Power Directorate proposal by Italy had been looked at with interest by Britain, but Hitler was not committed to it, resulting in Mussolini urging Hitler to consider the diplomatic advantages Germany would gain by breaking out of isolation by entering the Directorate and avoiding an immediate armed conflict. The Four Power Directorate proposal stipulated that Germany would no longer be required to have limited arms and would be granted the right to re-armament under foreign supervision in stages. Hitler completely rejected the idea of controlled rearmament under foreign supervision.
Mussolini did not trust Hitler's intentions regarding Anschluss nor Hitler's promise of no territorial claims on South Tyrol. Mussolini informed Hitler that he was satisfied with the presence of the anti-Marxist government of Dollfuss in Austria, and warned Hitler that he was adamantly opposed to Anschluss. Hitler responded in contempt to Mussolini that he intended "to throw Dollfuss into the sea". With this disagreement over Austria, relations between Hitler and Mussolini steadily became more distant.
Hitler attempted to break the impasse with Italy over Austria by sending Hermann Göring to negotiate with Mussolini in 1933 to convince Mussolini to press the Austrian government to appoint members of Austria's Nazis to the government. Göring claimed that Nazi domination of Austria was inevitable and that Italy should accept this, as well as repeating to Mussolini of Hitler's promise to "regard the question of the South Tyrol frontier as finally liquidated by the peace treaties". In response to Göring's visit with Mussolini, Dollfuss immediately went to Italy to counter any German diplomatic headway. Dollfuss claimed that his government was actively challenging Marxists in Austria and claimed that once the Marxists were defeated in Austria, that support for Austria's Nazis would decline.
In 1934, Hitler and Mussolini met for the first time, in Venice. The meeting did not proceed amicably. Hitler demanded that Mussolini compromise on Austria by pressuring Dollfuss to appoint Austrian Nazis his cabinet, in which Mussolini flatly refused the demand. In response, Hitler promised that he would accept Austria's independence for the time being, saying that due to the internal tensions in Germany (referring to sections of the Nazi SA that Hitler would soon kill in the Night of the Long Knives) that Germany could not afford to provoke Italy. Galeazzo Ciano told the press that the two leaders had made a "gentleman's agreement" to avoid interfering in Austria.
Several weeks after the Venice meeting, on 25 June 1934, Austrian Nazis assassinated Dollfuss. Mussolini was outraged as he held Hitler directly responsible for the assassination that violated Hitler's promise made only weeks ago to respect Austrian independence. Mussolini rapidly deployed several army divisions and air squadrons to the Brenner Pass, and warned that a German move against Austria would result in war between Germany and Italy. Hitler responded by both denying Nazi responsibility for the assassination and issuing orders to dissolve all ties between the German Nazi Party and its Austrian branch, which Germany claimed was responsible for the political crisis.
Italy effectively abandoned diplomatic relations with Germany while turning to France in order to challenge Germany's intransigence by signing a Franco-Italian accord to protect Austrian independence. French and Italian military staff discussed possible military cooperation involving a war with Germany should Hitler dare to attack Austria. As late as May 1935, Mussolini spoke of his desire to destroy Hitler.
Relations between Germany and Italy recovered due to Hitler's support of Italy's invasion of Ethiopia in 1935, while other countries condemned the invasion and advocated sanctions against Italy.
Development of German-Japanese-Italian alliance.
Interest in Germany and Japan in forming an alliance began when Japanese diplomat Oshima Hiroshi visited Joachim von Ribbentrop in Berlin in 1935. Oshima informed von Ribbentrop of Japan's interest in forming a German-Japanese alliance against the Soviet Union. Von Ribbentrop expanded on Oshima's proposal by advocating that the alliance be based in a political context of a pact to oppose the Comintern. The proposed pact was met with mixed reviews in Japan, with a faction of ultra-nationalists within the government supporting the pact while the Japanese Navy and the Japanese Foreign Ministry were staunchly opposed to the pact. There was great concern in the Japanese government that such a pact with Germany could alienate Japan's relations with Britain, endangering years of a beneficial Anglo-Japanese accord, that had allowed Japan to ascend in the international community in the first place. The response to the pact was met with similar division in Germany; while the proposed pact was popular amongst the upper echelons of the Nazi Party, it was opposed by many in the Foreign Ministry, the Army, and the business community who held financial interests in China to which Japan was hostile.
On learning of German-Japanese negotiations, Italy also began to take an interest in forming an alliance with Japan. Italy had hoped that due to Japan's long-term close relations with Britain, that an Italo-Japanese alliance could pressure Britain into adopting a more accommodating stance towards Italy in the Mediterranean. In the summer of 1936, Italian Foreign Minister Ciano informed Japanese Ambassador to Italy, Sugimura Yotaro, "I have heard that a Japanese-German agreement concerning the Soviet Union has been reached, and I think it would be natural for a similar agreement to be made between Italy and Japan". Initially Japan's attitude towards Italy's proposal was generally dismissive, viewing a German-Japanese alliance against the Soviet Union as imperative while regarding an Italo-Japanese alliance as secondary, as Japan anticipated that an Italo-Japanese alliance would antagonize Britain that had condemned Italy's invasion of Ethiopia. This attitude by Japan towards Italy altered in 1937 after the League of Nations condemned Japan for aggression in China and faced international isolation, while Italy remained favourable to Japan. As a result of Italy's support for Japan against international condemnation, Japan took a more positive attitude towards Italy and offered proposals for a non-aggression or neutrality pact with Italy.
The "Axis powers" formally took the name after the Tripartite Pact was signed by Germany, Italy, and Japan on 27 September 1940, in Berlin. The pact was subsequently joined by Hungary (20 November 1940), Romania (23 November 1940), Slovakia (24 November 1940), and Bulgaria (1 March 1941). 
Ideology.
In ideological terms the Axis powers described their goals as breaking the hegemony of plutocratic-capitalist Western powers and defending civilization from communism.
Economic resources.
The Axis population in 1938 was 258.9 million, while the Allied population (excluding the Soviet Union and the United States, who later joined the Allies) was 689.7 million. Thus the Allied powers outnumbered the Axis powers by 2.7 to 1. The leading Axis states had the following domestic populations: Germany 75.5 million (including 6.8 million from recently annexed Austria), Japan 71.9 million (excluding its colonies), and Italy 43.4 million (excluding its colonies). The United Kingdom (excluding its colonies) had a population of 47.5 million and France (excluding its colonies) 42 million.
The wartime gross domestic product (GDP) of the Axis was $911 billion at its highest in 1941 in international dollars by 1990 prices. The GDP of the Allied powers was $1,798 billion. The United States stood at $1,094 billion, more than the Axis combined.
The burden of the war upon participating countries has been measured through the percentage of gross national product (GNP) devoted to military expenditures. Nearly one-quarter of Germany's GNP was committed to the war effort in 1939, and this rose to three-quarters of GNP in 1944, prior to the collapse of the economy. In 1939, Japan committed 22 percent of its GNP to its war effort in China; this rose to three-quarters of GNP in 1944. Italy did not mobilize its economy; its GNP committed to the war effort remained at prewar levels. 
Italy and Japan lacked industrial capacity; their economies were small, dependent on international trade, external sources of fuel and other industrial resources. As a result, Italian and Japanese mobilization remained low, even by 1943.
Among the three major Axis powers, Japan had the lowest per capita income, while Germany and Italy had an income level comparable to the United Kingdom. 
Major affiliated state combatants.
Germany.
War justifications.
Hitler in 1941 described the outbreak of World War II as the fault of the intervention of Western powers against Germany during its war with Poland, describing it as the result of "the European and American warmongers". Hitler denied accusations by the Allies that he wanted a world war, and invoked anti-Semitic claims that the war was wanted and provoked by politicians of Jewish origin or associated with Jewish interests. However Hitler clearly had designs for Germany to become the dominant and leading state in the world, such as his intention for Germany's capital of Berlin to become the "Welthauptstadt" ("World Capital"), renamed Germania. The German government also justified its actions by claiming that Germany inevitably needed to territorially expand because it was facing an overpopulation crisis that Hitler described: "We are overpopulated and cannot feed ourselves from our own resources". Thus expansion was justified as an inevitable necessity to provide "lebensraum" ("living space") for the German nation and end the country's overpopulation within existing confined territory, and provide resources necessary to its people's well-being. Since the 1920s, the Nazi Party publicly promoted the expansion of Germany into territories held by the Soviet Union. However from 1939 to 1941, the Nazi regime claimed to have discarded those plans in light of improved relations with the Soviet Union via the Molotov-Ribbentrop Pact, and claimed that central Africa was where Germany sought to achieve "lebensraum". Hitler publicly claimed that Germany wanted to settle the "lebensraum" issue peacefully through diplomatic negotiations that would require other powers to make concessions to Germany. At the same time however Germany did prepare for war in the cause of "lebensraum", and in the late 1930s Hitler emphasized the need for a military build-up to prepare for a potential clash between the peoples of Germany and the Soviet Union.
Germany justified its war against Poland on the issues of German minority within Poland and Polish opposition to the incorporation of the German-majority Free City of Danzig into Germany. While Hitler and the Nazi party before taking power openly talked about destroying Poland and were hostile to Poles, after gaining power until February 1939 Hitler tried to conceal his true intentions towards Poland and revealed them only to his closest associates. Relations between Germany and Poland altered from the early to the late 1930s, as Germany sought rapprochement with Poland to avoid the risk of Poland entering the Soviet sphere of influence, and appealed to anti-Soviet sentiment in Poland. The Soviet Union in turn at this time competed with Germany for influence in Poland. At the same time Germany was preparing for a war with Poland and was secretly preparing German minority in Poland for a war. and since 1935 weapons were being smuggled and gathered in frontier Polish regions by German intelligence. In November 1938, Germany organized German paramilitary units in the Polish region of Pomerania that were trained to engage in diversion, sabotage as well as murder and ethnic cleansing upon a German invasion of Poland. At the end of 1938 one of the first editions of Sonderfahndungsbuch Polen was printed by the Nazis, containing several thousand names of Poles targeted for execution and imprisonment after an invasion of Poland From late 1938 to early 1939, Germany in talks with Poland suggested that as reward for Poland transferring territories in Pomerania to Germany that Poland could annex Ukrainian territories from the Soviet Union after a war with Soviet Union. In January 1939, Ribbentrop held negotiations with Józef Beck, the Polish minister of foreign affairs; and Edward Rydz-Śmigły, the commander-in-chief of the Polish Army; in which Ribbentrop urged them to have Poland enter the Anti-Comintern Pact and work together with Germany for mutual war in the East, whereby Poland would take Slovakia and the Ukraine. Ribbentrop in private discussion with German officials stated that he hoped that by offering Poland large new territories in the Soviet Union, that Germany would gain not only from Polish cooperation in a war with the Soviet Union, but also that Poland would cooperate by transferring the Polish Corridor to Germany in exchange for these gains, because though it would lose access to the Baltic Sea, it would gain access to the Black Sea via Ukraine. However Beck refused to discuss German demands for the Corridor and was recalcitrant to the idea of a war with the Soviet Union. The Polish government distrusted Hitler and saw the plan as a threat to Polish sovereignty, practically subordinating Poland to the Axis and the Anti-Comintern Bloc while reducing the country to a state of near-servitude as its entire trade with Western Europe through the Baltic Sea would become dependent on Germany.
A diplomatic crisis erupted following Hitler demanding that the Free City of Danzig be annexed to Germany, as it was led by a Nazi government seeking annexation to Germany. Germany used legal precedents to justify its intervention against Poland and annexation of the Free City of Danzig (led by a local Nazi government that sought incorporation into Germany) in 1939. Germany noted one such violation as being in 1933 when Poland sent additional troops into the city in violation of the limit of Polish troops admissible to Danzig as agreed to by treaty. Hitler believed that Poland could be pressured to cede claimed territory through diplomatic means combined with the threat of military force, and believed that Germany could gain such concessions from Poland without provoking a war with Britain or France. Hitler believed that Britain's guarantee of military support to Poland was a bluff, and with a German-Soviet agreement on both countries recognizing their mutual interests involving Poland. The Soviet Union had diplomatic grievances with Poland since the Soviet-Polish War of 1919–1921 in which the Soviets agreed that Northeastern Poland, Western Belarus and Western Ukraine will become part of restored Polish state after intense fighting in those years over the territories, and the Soviet Union sought to gain those territories.
Poland rejected Germany's demands and Germany in response prepared a general mobilization on the morning of 30 August 1939. Hitler believed that one of two outcomes would occur. The first was that the British would accept Germany's demands and pressure Poland to agree to them. The second was that a conflict with Poland would be an isolated conflict, as Britain would not engage in a war with both Germany and the Soviet Union. At midnight 30 August 1939, German foreign minister Joachim Ribbentrop was expecting the arrival of the British ambassador Nevile Henderson as well as a Polish plenipotentiary to negotiate terms with Germany. Only Henderson arrived, and Henderson informed Ribbentrop that no Polish plenipotentiary was arriving. Ribbentrop became extremely upset and demanded the immediate arrival of a Polish diplomat, informing Henderson that the situation was "damned serious!", and read out to Henderson Germany's demands that Poland accept Germany annexing Danzig as well as Poland granting Germany the right to increase the connection of the infrastructure of East Prussia to mainland Germany by building an extraterritorial highway and railway that passed through the Polish Gdansk Pomerania, and a plebiscite to determine whether the Polish Corridor, that had a mixed composition of ethnic Poles and ethnic Germans, should remain within Poland or be transferred to Germany.
Germany justified its invasion of the Low Countries of Belgium, Luxembourg, and the Netherlands in May 1940 by claiming that it suspected that Britain and France were preparing to use the Low Countries to launch an invasion of the industrial Ruhr region of Germany. When war between Germany versus Britain and France appeared likely in May 1939, Hitler declared that the Netherlands and Belgium would need to be occupied, saying: "Dutch and Belgian air bases must be occupied ... Declarations of neutrality must be ignored". In a conference with Germany's military leaders on 23 November 1939, Hitler declared to the military leaders that "We have an Achilles heel, the Ruhr", and said that "If England and France push through Belgium and Holland into the Ruhr, we shall be in the greatest danger", and thus claimed that Belgium and the Netherlands had to be occupied by Germany to protect Germany from a British-French offensive against the Ruhr, irrespective of their claims to neutrality.
In April 1941, shortly after Germany and Yugoslavia completed negotiations for Yugoslavia to join the Axis, a coup d'état occurred in Yugoslavia that led to the Axis invasion of Yugoslavia. Germany needed access to the territory held by Yugoslavia to allow German forces to have a direct route to travel through, to reach and rescue Italian military forces that were faltering in their campaign in Greece. There was substantial animosity towards the alliance amongst Serbs, Yugoslavia's largest ethnic group, who had fought German Austrians and Germany on the side of the Allies in World War I, and three Serb cabinet ministers resigned their positions in protest after the alliance was signed. Hitler initially attempted to be conciliatory to the Serbs who held animosity to the agreement, saying that he "understood the feelings" of those Serbs who opposed the alliance. Amidst the negotiations, Hitler expressed concern to Italian foreign minister Ciano that he sensed trouble coming in Belgrade. A coup d'état occurred in Yugoslavia in which a government rose to power and abandoned its association with the Axis. Hitler accused the coup of being engineered by the British. The coup was at least partly supported by the British though there was substantial patriotic enthusiasm against the Pact with rallies in Belgrade. At the rallies in Belgrade immediately after the coup, people were heard to be shouting "Better war than pact!" and waving British, American, and French flags. Days after the coup d'état, Hitler ordered the invasion of Yugoslavia.
Germany's invasion of the Soviet Union in 1941 involved issues of "lebensraum", anti-communism, and Soviet foreign policy. Hitler in his early years as Nazi leader had claimed that he would be willing to accept friendly relations with Russia on the tactical condition that Russia agree to return to the borders established by the German-Russian peace agreement of the Treaty of Brest-Litovsk signed by Vladimir Lenin of the Russian Soviet Federated Socialist Republic in 1918 which gave large territories held by Russia to German control in exchange for peace. Hitler in 1921 had commended the Treaty of Brest Litovsk as opening the possibility for restoration of relations between Germany and Russia, saying:
"Through the peace with Russia the sustenance of Germany as well as the provision of work were to have been secured by the acquisition of land and soil, by access to raw materials, and by friendly relations between the two lands."—Adolf Hitler, 1921
From 1921 to 1922 Hitler evoked rhetoric of both the achievement of lebensraum involving the acceptance of a territorially reduced Russia as well as supporting Russian nationals in overthrowing the Bolshevik government and establishing a new Russian government. However Hitler's attitudes changed by the end of 1922, in which he then supported an alliance of Germany with Britain to destroy Russia. Later Hitler declared how far into Russia he intended to expand Germany to:
"Asia, what a disquieting reservoir of men! The safety of Europe will not be assured until we have driven Asia back behind the Urals. No organized Russian state must be allowed to exist west of that line."—Adolf Hitler.
Policy for "lebensraum" planned mass expansion of Germany eastwards to the Ural Mountains. Hitler planned for the "surplus" Russian population living west of the Urals to be deported to the east of the Urals. After Germany invaded the Soviet Union in 1941, the Nazi regime's stance towards an independent, territorially-reduced Russia was affected by pressure beginning in 1942 from the German Army on Hitler to endorse a Russian national liberation army led by Andrey Vlasov that officially sought to overthrow Joseph Stalin and the communist regime and establish a new Russian state. Initially the proposal to support an anti-communist Russian army was met with outright rejection by Hitler, however by 1944 as Germany faced mounting losses on the Eastern Front, Vlasov's forces were recognized by Germany as an ally, particularly by Reichsführer-SS Heinrich Himmler.
After the Molotov-Ribbentrop Pact was signed, in 1940 when Molotov arrived in Berlin on a diplomatic visit during which Ribbentrop stated that Germany was directing its "lebensraum" southward. Ribbentrop described to Molotov that further extension of Germany's "lebensraum" was now going to be founded in Central Africa, and suggested that Germany would accept the Soviet Union taking part in the partitioning of the British Empire upon a British defeat in the war.
Germany and the Soviet Union in 1940 were in dispute over their respective influences in the Balkans and the Turkish Straits. The Soviet seizure of Bessarabia from Romania in June 1940 placed the Soviet-Romanian frontier dangerously close to Romania's oil fields that Germany needed oil trade from to support its war effort. When negotiations with Molotov let to no resolution, Hitler determined that Britain was only continuing to fight in hope of Soviet intervention and therefore the defeat of the Soviet Union would result in the defeat of Britain and in July 1940 began planning for a possible invasion of the Soviet Union.
After the Japanese attack on Pearl Harbor and the outbreak of war between Japan and the United States, Germany supported Japan by declaring war on the US. During the war Germany denounced the Atlantic Charter and the Lend-Lease Act that the US adopted to support the Allied powers prior to entry into the alliance, as imperialism directed at dominating and exploit countries outside of the continental Americas. Hitler denounced American President Roosevelt's invoking of the term "freedom" to describe US actions in the war, and accused the American meaning of "freedom" to be the freedom for democracy to exploit the world and the freedom for plutocrats within such democracy to exploit the masses.
History.
At the end of World War I, German citizens felt that their country had been humiliated as a result of the Treaty of Versailles, which forced Germany to pay enormous reparations payments and forfeit territories formerly controlled by German Empire and all its colonies. The pressure of the reparations on the German economy led to hyperinflation during the early 1920s. In 1923 the French occupied the Ruhr region when Germany defaulted on its reparations payments. Although Germany began to improve economically in the mid-1920s, the Great Depression created more economic hardship and a rise in political forces that advocated radical solutions to Germany's woes. The Nazis, under Hitler, promoted the nationalist stab-in-the-back legend stating that Germany had been betrayed by Jews and Communists. The party promised to rebuild Germany as a major power and create a Greater Germany that would include Alsace-Lorraine, Austria, Sudetenland, and other German-populated territories in Europe. The Nazis also aimed to occupy and colonize non-German territories in Poland, the Baltic states, and the Soviet Union, as part of the Nazi policy of seeking "Lebensraum" ("living space") in eastern Europe.
Germany renounced the Versailles treaty and remilitarized the Rhineland in March 1936. Germany had already resumed conscription and announced the existence of a German air force in 1935. Germany annexed Austria in 1938, the Sudetenland from Czechoslovakia, and the Memel territory from Lithuania in 1939. Germany then invaded the rest of Czechoslovakia in 1939, creating the Protectorate of Bohemia and Moravia and the country of Slovakia.
On 23 August 1939, Germany and the Soviet Union signed the Molotov-Ribbentrop Pact, which contained a secret protocol dividing eastern Europe into spheres of influence. Germany's invasion of its part of Poland under the Pact eight days later triggered the beginning of World War II. By the end of 1941, Germany occupied a large part of Europe and its military forces were fighting the Soviet Union, nearly capturing Moscow. However, crushing defeats at the Battle of Stalingrad and the Battle of Kursk devastated the German armed forces. This, combined with Western Allied landings in France and Italy, led to a three-front war that depleted Germany's armed forces and resulted in Germany's defeat in 1945.
There was substantial internal opposition within the German military to the Nazi regime's aggressive strategy of rearmament and foreign policy in the 1930s. From 1936 to 1938, Germany's top four military leaders, Ludwig Beck, Werner von Blomberg, Werner von Fritsch, Walther von Reichenau, were all in opposition to the rearmament strategy and foreign policy. They criticized the hurried nature of rearmament, the lack of planning, Germany's insufficient resources to carry out a war, the dangerous implications of Hitler's foreign policy, and the increasing subordination of the army to the Nazi Party's rules. These four military leaders were outspoken and public in their opposition to these tendencies. The Nazi regime responded with contempt to the four military leaders' opposition, and Nazi members brewed a false crass scandal that alleged that the two top army leaders von Blomberg and von Fritsch were homosexual lovers, in order to pressure them to resign. Though started by lower-ranking Nazi members, Hitler took advantage of the scandal by forcing von Blomberg and von Fritsch to resign and replaced them with opportunists who were subservient to him. Shortly afterwards Hitler announced on 4 February 1938 that he was taking personal command over Germany's military with the new High Command of the Armed Forces with the "Führer" as its head.
The opposition to the Nazi regime's aggressive foreign policy in the military became so strong from 1936 to 1938, that considerations of overthrowing the Nazi regime were discussed within the upper echelons of the military and remaining non-Nazi members of the German government. Minister of Economics, Hjalmar Schacht met with Beck in 1936 in which Schacht declared to Beck that he was considering an overthrow of the Nazi regime and was inquiring what the stance was by the German military on support of an overthrow of the Nazi regime. Beck was lukewarm to the idea, and responded that if a coup against the Nazi regime began with support at the civilian level, the military would not oppose it. Schacht considered this promise by Beck to be inadequate because he knew that without the support of the army, any coup attempt would be crushed by the Gestapo and the SS. However by 1938, Beck became a firm opponent of the Nazi regime out of his opposition to Hitler's military plans of 1937–38 that told the military to prepare for the possibility of a world war as a result of German annexation plans for Austria and Czechoslovakia.
Colonies and dependencies.
The Protectorate of Bohemia and Moravia was created from the dismemberment of Czechoslovakia. Shortly after Germany annexed the Sudetenland region of Czechoslovakia, Slovakia declared its independence. The new Slovak State allied itself with Germany. The remainder of the country was occupied by German military forces and organized into the Protectorate. Czech civil institutions were preserved but the Protectorate was considered within the sovereign territory of Germany.
The General Government was the name given to the territories of occupied Poland that were not directly annexed into German provinces, but like Bohemia and Moravia was consideree within the sovereign territory of Germany.
Belgium quickly surrendered to the Germany, and the Belgian King remained in the country during the German military occupation from 1940 to 1944. The Belgian King cooperated closely with Germany and repeatedly sought assurances that Belgian rights would be retained once Germany achieved total victory. However, Hitler intended to annex Belgium and its Germanic population into the Greater Germanic Reich, initiated by the creation of Reichskommissariat Belgien, an authority run directly by the German government that sought the incorporation of the territory into the planned Germanic Reich. However Belgium was soon occupied by Allied forces in 1944.
Reichskommissariat Niederlande was an occupation authority and territory established in the Netherlands in 1940 designated as a colony to be incorporated into the planned Greater Germanic Reich.
Reichskommissariat Norwegen was established in Norway in 1940. Like the Reichskommissariats in Belgium and the Netherlands, its Germanic peoples were to be incorporated into the Greater Germanic Reich. In Norway, the Quisling regime, headed by Vidkun Quisling, was installed by the Germans as a client regime during the occupation, while king Haakon VII and the legal government were in exile. Quisling encouraged Norwegians to serve as volunteers in the Waffen-SS, collaborated in the deportation of Jews, and was responsible for the executions of members of the Norwegian resistance movement.
About 45,000 Norwegian collaborators joined the pro-Nazi party "Nasjonal Samling" (National Union), and some police units helped arrest many Jews. However, Norway was one of the first countries where resistance during World War II was widespread before the turning point of the war in 1943. After the war, Quisling and other collaborators were executed. Quisling's name has become an international eponym for traitor.
Reichskommissariat Ostland was established in the Baltic region in 1941. Unlike the western Reichskommissariats that sought the incorporation of their majority Germanic peoples, Ostland were designed for settlement by Germans who would displace the majority non-Germanic peoples living there, as part of "lebensraum".
Reichskommissariat Ukraine was established in Ukraine in 1941. Like Ostland it was slated for settlement by Germans.
The Military Administration in Serbia was established on occupied Yugoslav territory in April 1941, following the invasion of the country. On 30 April a pro-German Serbian administration was formed under Milan Aćimović to serve as a civil administration in the military occupation zone. A joint Partisan and Chetnik uprising in late 1941 became a serious concern for the Germans, as most of their forces were deployed to Russia; only three divisions were in the country. On 13 August 546 Serbs, including some of the country's prominent and influential leaders, issued an appeal to the Serbian nation that condemned the Partisan and royalist resistance as unpatriotic. Two weeks after the appeal, with the Partisan and royalist insurgency beginning to gain momentum, 75 prominent Serbs convened a meeting in Belgrade and formed a Government of National Salvation under Serbian General Milan Nedić to replace the existing Serbian administration. The Germans were short of police and military forces in Serbia, and came to rely on poorly armed Serbian formations, the Serbian State Guard and Serbian Volunteer Corps, to maintain order. These forces, however, were not able to contain the resistance, and for the most of the war large parts of Serbia were under control of the Partisans or Chetniks (the two resistance movements soon became mutually-hostile). The Government of National Salvation, imbued with few powers upon formation, saw its functions further decreased and taken over by the Wehrmacht occupation authorities as the war progressed. After the initial mass revolts, the German authorities instituted an extreme regime of reprisals: proclaiming that 100 civilians will be executed for every German soldier killed, and 50 for each one wounded. These measures were actually implemented on more than one occasion, large scale shootings took place in the Serbian towns of Kraljevo and Kragujevac during October 1941.
Italy.
War justifications.
"Duce" Benito Mussolini described Italy's declaration of war against the Western Allies of Britain and France in June 1940 as the following: "We are going to war against the plutocratic and reactionary democracies of the West who have invariably hindered the progress and often threatened the very existence of the Italian people". Italy condemned the Western powers for enacting sanctions on Italy in 1935 for its actions in the Second Italo-Ethiopian War that Italy claimed was a response to an act of Ethiopian aggression against tribesmen in Italian Eritrea in the Walwal incident of 1934. Italy, like Germany, also justified its actions by claiming that Italy needed to territorially expand to provide "spazio vitale" ("vital space") for the Italian nation.
In October 1938 in the aftermath of the Munich Agreement, Italy demanded concessions from France to yield to Italy: a free port at Djibouti, control of the Addis Ababa-Djibouti railroad, Italian participation in the management of Suez Canal Company, some form of French-Italian condominium over Tunisia, and the preservation of Italian culture in French-held Corsica with no French assimilation of the people. Italy opposed the French monopoly over the Suez Canal because under the French-dominated Suez Canal Company all Italian merchant traffic to its colony of Italian East Africa was forced to pay tolls upon entering the canal. Mussolini hoped that in light of Italy's role in settling the Munich Agreement that prevented the outbreak of war, that Britain would react by putting pressure on France to yield to Italy's demands to preserve the peace. France refused to accept Italy's demands as it was widely suspected that Italy's true intentions were territorial acquisition of Nice, Corsica, Tunisia, and Djibouti and not the milder official demands put forth. Relations between Italy and France deteriorated with France's refusal to accept Italy's demands. France responded to Italy's demands with threatening naval maneuvers as a warning to Italy. As tensions between Italy and France grew, Hitler made a major speech on 30 January 1939 in which he promised German military support in the case of an unprovoked war against Italy.
Italy justified its intervention against Greece in October 1940 on the allegation that Greece was being used by Britain against Italy, Mussolini informed this to Hitler, saying: "Greece is one of the main points of English maritime strategy in the Mediterranean".
Italy justified its intervention against Yugoslavia in 1941 by appealing to both Italian irredentist claims and the fact of Albanian, Croatian, and Macedonian separatists not wishing to be part of Yugoslavia. Croatian separatism soared after the assassination of Croatian political leaders in the Yugoslav parliament in 1928 including the death of Stjepan Radić, and Italy endorsed Croatian separatist Ante Pavelić and his fascist Ustaše movement that was based and trained in Italy with the Fascist regime's support prior to intervention against Yugoslavia.
History.
In the late 19th century, after Italian unification, a nationalist movement had grown around the concept of "Italia irredenta", which advocated the incorporation into Italy of Italian-populated areas still under foreign rule. There was a desire to annex Dalmatian territories, which had formerly been ruled by the Venetians, and which consequently had Italian-speaking elites. The intention of the Fascist regime was to create a "New Roman Empire" in which Italy would dominate the Mediterranean. In 1935–1936 Italy invaded and annexed Ethiopia and the Fascist government proclaimed the creation of the "Italian Empire". Protests by the League of Nations, especially the British, who had interests in that area, led to no serious action, although The League did try to enforce economic sanctions upon Italy, but to no avail. The incident highlighted French and British weakness, exemplified by their reluctance to alienate Italy and lose her as their ally. The limited actions taken by the Western powers pushed Mussolini's Italy towards alliance with Hitler's Germany anyway. In 1937 Italy left the League of Nations and joined the Anti-Comintern Pact, which had been signed by Germany and Japan the preceding year. In March/April 1939 Italian troops invaded and annexed Albania. Germany and Italy signed the Pact of Steel on May 22.
Italy entered World War II on 10 June 1940. In September 1940 Germany, Italy, and Japan signed the Tripartite Pact.
Italy was ill-prepared for war, in spite of the fact that it had continuously been involved in conflict since 1935, first with Ethiopia in 1935–1936 and then in the Spanish Civil War on the side of Francisco Franco's Nationalists. 
Mussolini refused to heed warnings from his minister of exchange and currency, Felice Guarneri, who said that Italy's actions in Ethiopia and Spain meant that Italy was on the verge of bankruptcy. By 1939 military expenditures by Britain and France far exceeded what Italy could afford. As a result of Italy's economic difficulties its soldiers were poorly paid, often being poorly equipped and poorly supplied, and animosity arose between soldiers and class-conscious officers; these contributed to low morale amongst Italian soldiers. Military planning was deficient, as the Italian government had not decided on which theatre would be the most important. Power over the military was overcentralized to Mussolini's direct control; he personally undertook to direct the ministry of war, the navy, and the air force. The navy did not have any aircraft carriers to provide air cover for amphibious assaults in the Mediterranean, as the Fascist regime believed that the air bases on the Italian Peninsula would be able to do this task. Italy's army had outmoded artillery and the armoured units used outdated formations not suited to modern warfare. Diversion of funds to the air force and navy to prepare for overseas operations meant less money was available for the army; the standard rifle was a design that dated back to 1891. The Fascist government failed to learn from mistakes made in Ethiopia and Spain; it ignored the implications of the Italian Fascist volunteer soldiers being routed at the Battle of Guadalajara in the Spanish Civil War. Military exercises by the army in the Po Valley in August 1939 disappointed onlookers, including King Victor Emmanuel III. Mussolini who was angered by Italy's military unpreparedness, dismissed Alberto Pariani as Chief of Staff of the Italian military in 1939.
Italy's only strategic natural resource was an abundance of aluminum. Petroleum, iron, copper, nickel, chrome, and rubber all had to be imported. The Fascist government's economic policy of autarky and a recourse to synthetic materials was not able to meet the demand. Prior to entering the war, the Fascist government sought to gain control over resources in the Balkans, particularly oil from Romania. The agreement between Germany and the Soviet Union to invade and partition Poland between them resulted in Hungary that bordered the Soviet Union after Poland's partition, and Romania viewing Soviet invasion as an immediate threat, resulting in both countries appealing to Italy for support, beginning in September 1939. Italy - then still officially neutral - responded to appeals by the Hungarian and Romanian governments for protection from the Soviet Union, by proposing a Danube-Balkan neutrals bloc. The proposed bloc was designed to increase Italian influence in the Balkans: it met resistance from France, Germany, and the Soviet Union that did not want to lose their influence in the Balkans; however Britain, that still hoped that Italy would not enter the war on Germany's side, supported the neutral bloc. The efforts to form the bloc failed by November 1939 after Turkey made an agreement that it would protect Allied Mediterranean territory, along with Greece and Romania.
Initially upon the outbreak of war between Germany and the Allies, Mussolini pursued a non-belligerent role for Italy out of concerns that Germany may not win its war with the Allies. However Mussolini in private grew anxious that Italy not intervening in support of Germany in September 1939 upon Britain and France waging war on Germany, would eventually result in retribution by Germany if Italy did not get involved in the war on Germany's side.
By early 1940, Italy was still a non-belligerent, and Mussolini communicated to Hitler that Italy was not prepared to intervene soon. By March 1940, Mussolini decided that Italy would intervene, but the date was not yet chosen. His senior military leadership unanimously opposed the action because Italy was unprepared. No raw materials had been stockpiled and the reserves it did have would soon be exhausted, Italy's industrial base was only one-tenth of Germany's, and even with supplies the Italian military was not organized to provide the equipment needed to fight a modern war of a long duration. An ambitious rearmament program was impossible because of Italy's limited reserves in gold and foreign currencies and lack of raw materials. Mussolini ignored the negative advice.
An April 1938 report by German Naval High Command (OKM) warned that Italy as a combatant ally would be a serious "burden" to Germany in a war between Germany and Britain occurred, and recommended that it would be preferable for Germany to seek for Italy to be a "benevolent neutral" during the war. On 18 March 1940, Hitler told Mussolini in person that the war would be over by the summer and that Italy's military involvement was not required.
Mussolini on 29 May 1940 discussed the situation of the Italian Army in which he acknowledged that it was not ideal but believed that it was satisfactory, and discussed the timeline for a declaration of war on Britain and France. He said: "a delay of two weeks or a month would not be an improvement, and Germany could think we entered the war when the risk was very small ... And this could be a burden on us when peace comes."
After entering the war in 1940, Italy had been slated to be granted a series of territorial concessions from France that Hitler had agreed to with Italian foreign minister Ciano, that included Italian annexation of claimed territories in southeastern France, a military occupation of southeastern France up to the river Rhone, and receiving the French colonies of Tunisia and Djibouti. However on 22 June 1940, Mussolini suddenly informed Hitler that Italy was abandoning its claims "in the Rhone, Corsica, Tunisia, and Djibouti", instead requesting a demilitarized zone along the French border, and on 24 June Italy agreed to an armistice with the Vichy regime to that effect. Later on 7 July 1940, the Italian government changed its decision, and Ciano attempted to make an agreement with Hitler to have Nice, Corsica, Tunisia, and Djibouti be transferred to Italy; Hitler adamantly rejected any new settlement or separate French-Italian peace agreement for the time being prior to the defeat of Britain in the war. However Italy continued to press Germany for the incorporation of Nice, Corsica, and Tunisia into Italy, with Mussolini sending a letter to Hitler in October 1940, informing him that as the 850,000 Italians living under France's current borders formed the largest minority community, that ceding these territories to Italy would be beneficial to both Germany and Italy as it would reduce France's population from 35 million to 34 and forestall any possibility of resumed French ambitions for expansion or hegemony in Europe. Germany had considered the possibility of invading and occupying the non-occupied territories of Vichy France including occupying Corsica; Germany capturing the Vichy French fleet for use by Germany, in December 1940 with the proposed Operation Attila. An invasion of Vichy France by Germany and Italy took place with Case Anton in November 1942.
In mid-1940, in response to an agreement by Romanian "Conductator" Ion Antonescu to accept German "training troops" to be sent to Romania, both Mussolini and Stalin in the Soviet Union were angered by Germany's expanding sphere of influence into Romania, and especially because neither was informed in advance of the action in spite of German agreements with Italy and the Soviet Union at that time. Mussolini in a conversation with Ciano responded to Hitler's deployment of troops into Romania, saying: "Hitler always faces me with accomplished facts. Now I'll pay him back by his same currency. He'll learn from the papers that I have occupied Greece. So the balance will be re-established.". However Mussolini later decided to inform Hitler in advance of Italy's designs on Greece. Upon hearing of Italy's intervention against Greece, Hitler was deeply concerned as he said that the Greeks were not bad soldiers that Italy might not win in its war with Greece, as he did not want Germany to become embroiled in a Balkan conflict.
By 1941, Italy's attempts to run an autonomous campaign from Germany's, collapsed as a result of multiple defeats in Greece, North Africa, and Eastern Africa; and the country became dependent and effectively subordinate to Germany. After the German-led invasion and occupation of Yugoslavia and Greece, that had both been targets of Italy's war aims, Italy was forced to accept German dominance in the two occupied countries. Furthermore, by 1941, German forces in North Africa under Erwin Rommel effectively took charge of the military effort ousting Allied forces from the Italian colony of Libya, and German forces were stationed in Sicily in that year. The German government in response to Italian military failures and dependence on German military assistance, viewed Italy with contempt as an unreliable ally, and no longer took any serious consideration of Italian interests. Germany's contempt for Italy as an ally was demonstrated that year when Italy was pressured to send 350,000 "guest workers" to Germany who were used as forced labour. While Hitler was deeply disappointed with the Italian military's performance, he maintained overall favourable relations with Italy because of his personal friendship and admiration of Mussolini.
Mussolini by mid-1941 was left bewildered and recognized both that Italy's war objectives had failed and that Italy was completely subordinate and dependent to Germany. Mussolini henceforth believed that Italy was left with no choice in such a subordinate status other than to follow Germany in its war and hope for a German victory. However Germany supported Italian propaganda of the creation of a "Latin Bloc" of Italy, Vichy France, Spain, and Portugal to ally with Germany against the threat of communism, and after the German invasion of the Soviet Union, the prospect of a Latin Bloc seemed plausible. From 1940 to 1941, Francisco Franco of Spain had endorsed a Latin Bloc of Italy, Vichy France, Spain and Portugal, in order to balance the countries' powers to that of Germany; however, the discussions failed to yield an agreement.
After the invasion and occupation of Yugoslavia, Italy annexed numerous Adriatic islands and a portion of Dalmatia that was formed into the Italian Governorship of Dalmatia including territory from the provinces of Spalato, Zara, and Cattaro. Though Italy had initially larger territorial aims that extended from the Velebit mountains to the Albanian Alps, Mussolini decided against annexing further territories due to a number of factors, including that Italy held the economically valuable portion of that territory within its possession while the northern Adriatic coast had no important railways or roads and because a larger annexation would have included hundreds of thousands of Slavs who were hostile to Italy, within its national borders. Mussolini and foreign minister Ciano demanded that the Yugoslav region of Slovenia to be directly annexed into Italy, however in negotiations with German foreign minister Ribbentrop in April 1941, Ribbentrop insisted on Hitler's demands that Germany be allocated the eastern Slovenia while Italy would be allocated western Slovenia, Italy conceded to this German demand and Slovenia was partitioned between Germany and Italy.
With the commencing of the Allies' Operation Torch against Vichy French-held Morocco and Algeria, Germany and Italy intervened in Vichy France and in Vichy French-held Tunisia. Italy seized military control over a significant portion of southern France and Corsica, while a joint German-Italian force seized control over most of Tunisia. When the issue of sovereign control over Tunisia arose from seizure of control by the German-Italian force from Vichy French control, Ribbentrop proclaimed Italian predominance in Tunisia. However in spite of Germany's claim to respect Italian predominance, Germans supervised public services and local government in Tunisia, and the German presence was more popular in Tunisia with both the local Arab population and Vichy French collaborators since Germany had no imperial aspirations in Tunisia while Italy did.
Internal opposition by Italians to the war and the Fascist regime accelerated by 1942, though significant opposition to the war had existed at the outset in 1940, as police reports indicated that many Italians were secretly listening to the BBC rather than Italian media in 1940. Underground Catholic, Communist, and socialist newspapers began to become prominent by 1942.
In spring 1941, Victor Emmanuel III visited Italian soldiers on the front in Yugoslavia and Albania, he was dismayed by the Fascist regime's brutal imperialism in Dalmatia, Slovenia, and Montenegro because he suspected it would impose impossible burdens on Italy by creating new enemies amongst the occupied peoples that Italy would be forced to fight. Victor Emmanuel was highly disappointed with the Italian military's performance in the war, as he noted the army, navy, and air force could not drop their mutual jealousies and competition to work together. Furthermore he feared that overly ambitious generals attempting to win promotion were attempting to persuade Mussolini to divert military resources in an ever-widening field of action. In June 1941, Mussolini's decision to follow Germany by waging war on the Soviet Union in which Victor Emmanuel was informed at the last moment giving him time only to advice to Mussolini against sending anything more than a token force to fight against the Soviet Union; his advice was not taken. A few weeks after Italy's declaration of war against the Soviet Union, a senior general of the "Carabinieri" informed the royal palace that the military police were awaiting a royal order to act against the Fascist regime. In September 1941, Victor Emmanuel held a private discussion with Ciano, in which Ciano said to the King that Fascism was doomed. In 1942, opposition to Italy's involvement in the war expanded amongst the Fascist regime's senior officials, with Giuseppe Bottai in private stating that he and other Fascist officials should have resigned from office when Mussolini declared war on Britain and France in June 1940, while Dino Grandi approached the King urging him to dismantle Mussolini's dictatorship in order to withdraw Italy from the war as he saw Italy facing ruin. By January 1943, King Victor Emmanuel III was persuaded by the Minister of the Royal Household, the Duke of Acquarone that Mussolini had to be removed from office.
In March 1943, the first sign of serious rebellion by Italians against the Fascist regime and the war began with a strike by factory workers who were joined by soldiers singing communist songs and even rank-in-file Fascist party members. The Fascist regime also faced passive resistance by civil servants who had begun to refuse to obey orders or pretend to obey orders.
On 25 July 1943, King Victor Emmanuel III dismissed Mussolini, placed him under arrest, and began secret negotiations with the Allies. An armistice was signed on 8 September 1943, and Italy joined the Allies as a co-belligerent. On 12 September 1943, Mussolini was rescued by the Germans in Operation Oak and placed in charge of a puppet state called the Italian Social Republic ("Repubblica Sociale Italiana"/RSI, or "Repubblica di Salò") in northern Italy. The war went on for months as the Allies, the Italian Co-Belligerent Army and the partisans contended the Social Republic's forces and its German allies. Some areas in Northern Italy were liberated from the Germans as late as May, 1945. Mussolini was killed by Communist partisans on 28 April 1945 while trying to escape to Switzerland.
Colonies and dependencies.
In Europe.
The Dodecanese Islands were an Italian dependency from 1912 to 1943.
Montenegro was an Italian dependency from 1941 to 1943 known as the Governorate of Montenegro that was under the control of an Italian military governor. Initially, the Italians intended that Montenegro would become an "independent" state closely allied with Italy, reinforced through the strong dynastic links between Italy and Montenegro, as Queen Elena of Italy was a daughter of the last Montenegrin king Nicholas I. The Italian-backed Montenegrin nationalist Sekula Drljević and his followers attempted to create a Montenegrin state. On 12 July 1941, they proclaimed the "Kingdom of Montenegro" under the protection of Italy. In less than 24 hours, that triggered a general uprising against the Italians. Within three weeks, the insurgents managed to capture almost all the territory of Montenegro. Over 70,000 Italian troops and 20,000 of Albanian and Muslim irregulars were deployed to suppress the rebellion. Drljevic was expelled from Montenegro in October 1941. Montenegro then came under full direct Italian control. With the Italian capitulation of 1943, Montenegro came directly under the control of Germany.
Albania was an Italian protectorate and dependency from 1939 to 1943. In spite of Albania's long-standing protection and alliance with Italy, on 7 April 1939 Italian troops invaded Albania, five months before the start of the Second World War. Following the invasion, Albania became a protectorate under Italy, with King Victor Emmanuel III of Italy being awarded the crown of Albania. An Italian governor controlled Albania. Albanian troops under Italian control were sent to participate in the Italian invasion of Greece and the Axis occupation of Yugoslavia. Following Yugoslavia's defeat, Kosovo was annexed to Albania by the Italians.
Politically and economically dominated by Italy from its creation in 1913, Albania was occupied by Italian military forces in 1939 as the Albanian king [Zog] fled the country with his family. The Albanian parliament voted to offer the Albanian throne to the King of Italy, resulting in a personal union between the two countries.
The Albanian army, having been trained by Italian advisors, was reinforced by 100,000 Italian troops. A Fascist militia was organized, drawing its strength principally from Albanians of Italian descent.
Albania served as the staging area for the Italian invasions of Greece and Yugoslavia. Albania annexed Kosovo in 1941 when Yugoslavia was dissolved, creating a Greater Albania.
Albanian troops were dispatched to the Eastern Front to fight the Soviets as part of the Italian Eighth Army.
Albania declared war on the United States in 1941.
When the Fascist regime of Italy fell, in September 1943 Albania fell under German occupation.
In Africa and Asia.
Italian East Africa was an Italian colony existing from 1936 to 1943. Prior to the invasion and annexation of Ethiopia into this united colony in 1936, Italy had two colonies, Eritrea and Somalia since the 1880s.
Libya was an Italian colony existing from 1912 to 1943. The northern portion of Libya was incorporated directly into Italy in 1939, however the region remained united as a colony under a colonial governor.
There was also a minor Italian concession territory in Tientsin, Republic of China.
Japan.
War justifications.
The Japanese government justified its actions by claiming that it was seeking to unite East Asia under Japanese leadership in a Greater East Asia Co-Prosperity Sphere that would free East Asians from domination and rule by clients of Western powers and particularly the United States. Japan invoked themes of Pan-Asianism and said that the Asian people needed to be free from Western influence.
The United States opposed the Japanese war in China, and recognized Chiang Kai-Shek's Nationalist Government as the legitimate government of China. As a result, the United States sought to bring the Japanese war effort to a halt by imposing an embargo on all trade between the United States and Japan. Japan was dependent on the United States for 80 percent of its petroleum, and as a consequence the embargo resulted in an economic and military crisis for Japan, as Japan could not continue its war effort against China without access to petroleum.
In order to maintain its military campaign in China with the major loss of petroleum trade with the United States, Japan saw the best means to secure an alternative source of petroleum in the petroleum-rich and natural-resources-rich Southeast Asia. This threat of retaliation by Japan to the total trade embargo by the United States was known by the American government, including American Secretary of State Cordell Hull who was negotiating with the Japanese to avoid a war, fearing that the total embargo would pre-empt a Japanese attack on the Dutch East Indies.
Japan identified the American Pacific fleet based in Pearl Harbor as the principal threat to its designs to invade and capture Southeast Asia. Thus Japan initiated the attack on Pearl Harbor on 7 December 1941 as a means to inhibit an American response to the invasion of Southeast Asia, and buy time to allow Japan to consolidate itself with these resources to engage in a total war against the United States, and force the United States to accept Japan's acquisitions.
History.
The Empire of Japan, a constitutional monarchy ruled by Hirohito, was the principal Axis power in Asia and the Pacific. Under the emperor were a political cabinet and the Imperial General Headquarters, with two chiefs of staff. By 1945 the Emperor of Japan was more than a symbolic leader; he played a major role in devising a strategy to keep himself on the throne.
At its height, Japan's Greater East Asia Co-Prosperity Sphere included Manchuria, Inner Mongolia, large parts of China, Malaysia, French Indochina, Dutch East Indies, The Philippines, Burma, a small part of India, and various Pacific Islands in the central Pacific.
As a result of the internal discord and economic downturn of the 1920s, militaristic elements set Japan on a path of expansionism. As the Japanese home islands lacked natural resources needed for growth, Japan planned to establish hegemony in Asia and become self-sufficient by acquiring territories with abundant natural resources. Japan's expansionist policies alienated it from other countries in the League of Nations and by the mid-1930s brought it closer to Germany and Italy, who had both pursued similar expansionist policies. Cooperation between Japan and Germany began with the Anti-Comintern Pact, in which the two countries agreed to ally to challenge any attack by the Soviet Union.
Japan entered into conflict against the Chinese in 1937. The Japanese invasion and occupation of parts of China resulted in numerous atrocities against civilians, such as the Nanking massacre and the Three Alls Policy. The Japanese also fought skirmishes with Soviet–Mongolian forces in Manchukuo in 1938 and 1939. Japan sought to avoid war with the Soviet Union by signing a non-aggression pact with it in 1941.
Japan's military leaders were divided on diplomatic relationships with Germany and Italy and the attitude towards the United States. The Imperial Japanese Army was in favour of war with the United States, but the Imperial Japanese Navy was generally strongly opposed. When Prime Minister of Japan General Hideki Tojo refused American demands that Japan withdraw its military forces from China, a confrontation became more likely. War with the United States was being discussed within the Japanese government by 1940. Commander of the Combined Fleet Admiral Isoroku Yamamoto was outspoken in his opposition, especially after the signing of the Tripartite Pact, saying on 14 October 1940: "To fight the United States is like fighting the whole world. But it has been decided. So I will fight the best I can. Doubtless I shall die on board "Nagato" [his flagship]. Meanwhile Tokyo will be burnt to the ground three times. Konoe and others will be torn to pieces by the revengeful people, I [shouldn't] wonder. " In October and November 1940, Yamamoto communicated with Navy Minister Oikawa, and stated, "Unlike the pre-Tripartite days, great determination is required to make certain that we avoid the danger of going to war. "
With the European powers focused on the war in Europe, Japan sought to acquire their colonies. In 1940 Japan responded to the German invasion of France by occupying French Indochina. The Vichy France regime, a "de facto" ally of Germany, accepted the takeover. The allied forces did not respond with war. However, the United States instituted an embargo against Japan in 1941 because of the continuing war in China. This cut off Japan's supply of scrap metal and oil needed for industry, trade, and the war effort.
To isolate the American forces stationed in the Philippines and to reduce American naval power, the Imperial General Headquarters ordered an attack on the U. S. naval base at Pearl Harbor, Hawaii, on 7 December 1941. They also invaded Malaya and Hong Kong. Initially achieving a series of victories, by 1943 the Japanese forces were driven back towards the home islands. The Pacific War lasted until the atomic bombings of Hiroshima and Nagasaki in 1945. The Soviets formally declared war in August 1945 and engaged Japanese forces in Manchuria and northeast China.
Colonies and dependencies.
Korea was a Japanese protectorate and dependency formally established by the Japan–Korea Treaty of 1910.
The South Pacific Mandate were territories granted to Japan in 1919 in the peace agreements of World War I, that designated to Japan the German South Pacific islands. Japan received these as a reward by the Allies of World War I, when Japan was then allied against Germany.
Taiwan, then known as Formosa, was a Japanese dependency established in 1895.
Japan occupied the Dutch East Indies during the war. Japan planned to transform these territories into a client state of Indonesia and sought alliance with Indonesian nationalists including future Indonesian President Sukarno, however these efforts did not deliver the creation of an Indonesian state until after Japan's surrender.
Minor affiliated state combatants.
Bulgaria.
The Kingdom of Bulgaria was ruled by Тsar Boris III when it signed the Tripartite Pact on 1 March 1941. Bulgaria had been on the losing side in the First World War and sought a return of lost ethnically and historically Bulgarian territories, specifically in Macedonia and Thrace. During the 1930s, because of traditional right-wing elements, Bulgaria drew closer to Nazi Germany. In 1940 Germany pressured Romania to sign the Treaty of Craiova, returning to Bulgaria the region of Southern Dobrudja, which it had lost in 1913. The Germans also promised Bulgaria — in case it joined the Axis — an enlargement of its territory to the borders specified in the Treaty of San Stefano.
Bulgaria participated in the Axis invasion of Yugoslavia and Greece by letting German troops attack from its territory and sent troops to Greece on April 20. As a reward, the Axis powers allowed Bulgaria to occupy parts of both countries—southern and south-eastern Yugoslavia (Vardar Banovina) and north-eastern Greece (parts of Greek Macedonia and Greek Thrace). The Bulgarian forces in these areas spent the following years fighting various nationalist groups and resistance movements. Despite German pressure, Bulgaria did not take part in the Axis invasion of the Soviet Union and actually never declared war on the Soviet Union. The Bulgarian Navy was nonetheless involved in a number of skirmishes with the Soviet Black Sea Fleet, which attacked Bulgarian shipping.
Following the Japanese attack on Pearl Harbor in December 1941, the Bulgarian government declared war on the Western Allies. This action remained largely symbolic (at least from the Bulgarian perspective), until August 1943, when Bulgarian air defense and air force attacked Allied bombers, returning (heavily damaged) from a mission over the Romanian oil refineries. This turned into a disaster for the citizens of Sofia and other major Bulgarian cities, which were heavily bombed by the Allies in the winter of 1943–1944.
On 2 September 1944, as the Red Army approached the Bulgarian border, a new Bulgarian government came to power and sought peace with the Allies, expelled the few remaining German troops, and declared neutrality. These measures however did not prevent the Soviet Union from declaring war on Bulgaria on 5 September, and on 8 September the Red Army marched into the country, meeting no resistance. This was followed by the coup d'état of 9 September 1944, which brought a government of the pro-Soviet Fatherland Front to power. After this, the Bulgarian army (as part of the Red Army's 3rd Ukrainian Front) fought the Germans in Yugoslavia and Hungary, sustaining numerous casualties. Despite this, the Paris Peace Treaty treated Bulgaria as one of the defeated countries. Bulgaria was allowed to keep Southern Dobruja, but had to give up all claims to Greek and Yugoslav territory.
Hungary.
Hungary, ruled by Regent Admiral Miklós Horthy, was the first country apart from Germany, Italy, and Japan to adhere to the Tripartite Pact, signing the agreement on 20 November 1940. Hungary had been a client state of Germany since 1938.
Political instability plagued the country until Miklós Horthy, a Hungarian nobleman and Austro-Hungarian naval officer, became regent in 1920. Hungarian nationalists desired to recover territories lost through the Trianon Treaty. The country drew closer to Germany and Italy largely because of a shared desire to revise the peace settlements made after World War I. Many people sympathized with the anti-Semitic policy of the Nazi regime. Due to its pro-German stance, Hungary received favourable territorial settlements when Germany annexed Czechoslovakia in 1938–1939 and received Northern Transylvania from Romania via the Vienna Awards of 1940. Hungarians permitted German troops to transit through their territory during the invasion of Yugoslavia, and Hungarian forces took part in the invasion. Parts of Yugoslavia were annexed to Hungary; the United Kingdom immediately broke off diplomatic relations in response.
Although Hungary did not initially participate in the German invasion of the Soviet Union, Hungary declared war on the Soviet Union on 27 June 1941. Over 500,000 soldiers served on the Eastern Front. All five of Hungary's field armies ultimately participated in the war against the Soviet Union; a significant contribution was made by the Hungarian Second Army.
On 25 November 1941, Hungary was one of thirteen signatories to the revived Anti-Comintern Pact. Hungarian troops, like their Axis counterparts, were involved in numerous actions against the Soviets. By the end of 1943, the Soviets had gained the upper hand and the Germans were retreating. The Hungarian Second Army was destroyed in fighting on the Voronezh Front, on the banks of the Don River. In 1944, with Soviet troops advancing toward Hungary, Horthy attempted to reach an armistice with the Allies. However, the Germans replaced the existing regime with a new one. After fierce fighting, Budapest was taken by the Soviets. A number of pro-German Hungarians retreated to Italy and Germany, where they fought until the end of the war.
Relations between Germany and the regency of Miklós Horthy collapsed in Hungary in 1944. Horthy was forced to abdicate after German armed forces held his son hostage as part of Operation Panzerfaust. Hungary was reorganized following Horthy's abdication in December 1944 into a totalitarian fascist regime called the Government of National Unity, led by Ferenc Szálasi. He had been Prime Minister of Hungary since October 1944 and was leader of the anti-Semitic fascist Arrow Cross Party. In power, his government was a puppet regime with little authority, and the country was effectively under German control. Days after the Szálasi government took power, the capital of Budapest was surrounded by the Soviet Red Army. German and Hungarian fascist forces tried to hold off the Soviet advance but failed. In March 1945, Szálasi fled to Germany as the leader of a government in exile, until the surrender of Germany in May 1945.
Romania.
When war erupted in Europe in 1939, the Kingdom of Romania was pro-British and allied to the Poles. Following the invasion of Poland by Germany and the Soviet Union, and the German conquest of France and the Low Countries, Romania found itself increasingly isolated; meanwhile, pro-German and pro-Fascist elements began to grow.
The August 1939 Molotov–Ribbentrop Pact between Germany and the Soviet Union contained a secret protocol ceding Bessarabia, and Northern Bukovina to the Soviet Union. On June 28, 1940, the Soviet Union occupied and annexed Bessarabia, as well as part of northern Romania and the Hertza region. On 30 August 1940, Germany forced Romania to cede Northern Transylvania to Hungary as a result of the second Vienna Award. Southern Dobruja was ceded to Bulgaria in September 1940. In an effort to appease the Fascist elements within the country and obtain German protection, King Carol II appointed the General Ion Antonescu as Prime Minister on September 6, 1940.
Two days later, Antonescu forced the king to abdicate and installed the king's young son Michael (Mihai) on the throne, then declared himself "Conducător" ("Leader") with dictatorial powers. Under King Michael I and the military government of Antonescu, Romania signed the Tripartite Pact on November 23, 1940. German troops entered the country in 1941 and used the country as platform for invasions of Yugoslavia and the Soviet Union. Romania was a key supplier of resources, especially oil and grain.
Romania joined the German-led invasion of the Soviet Union on June 22, 1941; nearly 800,000 Romanian soldiers fought on the Eastern front. Areas that were annexed by the Soviets were reincorporated into Romania, along with the newly established Transnistria Governorate. After suffering devastating losses at Stalingrad, Romanian officials began secretly negotiating peace conditions with the Allies. By 1943, the tide began to turn. The Soviets pushed further west, retaking Ukraine and eventually launching an unsuccessful invasion of eastern Romania in the spring of 1944. Foreseeing the fall of Nazi Germany, Romania switched sides during King Michael's Coup on August 23, 1944. Romanian troops then fought alongside the Soviet Army until the end of the war, reaching as far as Czechoslovakia and Austria.
Thailand.
Thailand waged the Franco-Thai War in October 1940 to May 1941 to reclaim territory from French Indochina. It became a formal ally of Japan from 25 January 1942.
Japanese forces invaded Thailand's territory an hour and a half before the attack on Pearl Harbor, (because of the International Dateline, the local time was on the morning of 8 December 1941). Only hours after the invasion, prime minister Field Marshal Phibunsongkhram ordered the cessation of resistance against the Japanese. On 21 December 1941, a military alliance with Japan was signed and on 25 January 1942, Sang Phathanothai read over the radio Thailand's formal declaration of war on the United Kingdom and the United States. The Thai ambassador to the United States, Mom Rajawongse Seni Pramoj, did not deliver his copy of the declaration of war. Therefore, although the British reciprocated by declaring war on Thailand and considered it a hostile country, the United States did not.
When Thailand signed the Tripartite Pact on 15 February 1942, the Thais and Japanese also agreed that Shan State and Kayah State were to be under Thai control. The rest of Burma was to be under Japanese control, On 10 May 1942, the Thai Phayap Army entered Burma's eastern Shan State, which had been claimed by Siamese kingdoms. Three Thai infantry and one cavalry division, spearheaded by armoured reconnaissance groups and supported by the air force, engaged the retreating Chinese 93rd Division. Kengtung, the main objective, was captured on 27 May. Renewed offensives in June and November evicted the Chinese into Yunnan. The area containing the Shan States and Kayah State was annexed by Thailand in 1942. The areas were ceded back to Burma in 1945.
The Free Thai Movement ("Seri Thai") was established during these first few months. Parallel Free Thai organizations were also established in the United Kingdom. Queen Ramphaiphanni was the nominal head of the British-based organization, and Pridi Phanomyong, the regent, headed its largest contingent, which was operating within Thailand. Aided by elements of the military, secret airfields and training camps were established, while Office of Strategic Services and Force 136 agents slipped in and out of the country.
As the war dragged on, the Thai population came to resent the Japanese presence. In June 1944, Phibun was overthrown in a coup d'état. The new civilian government under Khuang Aphaiwong attempted to aid the resistance while maintaining cordial relations with the Japanese. After the war, U. S. influence prevented Thailand from being treated as an Axis country, but the British demanded three million tons of rice as reparations and the return of areas annexed from Malaya during the war. Thailand also returned the portions of British Burma and French Indochina that had been annexed. Phibun and a number of his associates were put on trial on charges of having committed war crimes and of collaborating with the Axis powers. However, the charges were dropped due to intense public pressure. Public opinion was favourable to Phibun, since he was thought to have done his best to protect Thai interests.
Minor co-belligerent state combatants.
Various countries fought side by side with the Axis powers for a common cause. These countries were not signatories of the Tripartite Pact and thus not formal members of the Axis.
Finland.
Although Finland never signed the Tripartite Pact and legally "(de jure)" was not a part of the Axis, it was Axis-aligned in its fight against the Soviet Union. Finland signed the revived Anti-Comintern Pact of November 1941.
The August 1939 Molotov-Ribbentrop Pact between Germany and the Soviet Union contained a secret protocol dividing much of eastern Europe and assigning Finland to the Soviet sphere of influence. After unsuccessfully attempting to force territorial and other concessions on the Finns, the Soviet Union tried to invade Finland in November 1939 during the Winter War, intending to establish a communist puppet government in Finland. The conflict threatened Germany's iron-ore supplies and offered the prospect of Allied interference in the region. Despite Finnish resistance, a peace treaty was signed in March 1940, wherein Finland ceded some key territory to the Soviet Union, including the Karelian Isthmus, containing Finland's second-largest city, Viipuri, and the critical defensive structure of the Mannerheim Line. After this war, Finland sought protection and support from the United Kingdom and non-aligned Sweden, but was thwarted by Soviet and German actions. This resulted in Finland being drawn closer to Germany, first with the intent of enlisting German support as a counterweight to thwart continuing Soviet pressure, and later to help regain lost territories.
In the opening days of Operation Barbarossa, Germany's invasion of the Soviet Union, Finland permitted German planes returning from mine dropping runs over Kronstadt and Neva River to refuel at Finnish airfields before returning to bases in East Prussia. In retaliation, the Soviet Union launched a major air offensive against Finnish airfields and towns, which resulted in a Finnish declaration of war against the Soviet Union on 25 June 1941. The Finnish conflict with the Soviet Union is generally referred to as the Continuation War.
Finland's main objective was to regain territory lost to the Soviet Union in the Winter War. However, on 10 July 1941, Field Marshal Carl Gustaf Emil Mannerheim issued an Order of the Day that contained a formulation understood internationally as a Finnish territorial interest in Russian Karelia.
Diplomatic relations between the United Kingdom and Finland were severed on 1 August 1941, after the British bombed German forces in the Finnish village and port of Petsamo. The United Kingdom repeatedly called on Finland to cease its offensive against the Soviet Union, and declared war on Finland on 6 December 1941, although no other military operations followed. War was never declared between Finland and the United States, though relations were severed between the two countries in 1944 as a result of the Ryti-Ribbentrop Agreement.
Finland maintained command of its armed forces and pursued war objectives independently of Germany. Germans and Finns did work closely together during Operation Silverfox, a joint offensive against Murmansk. Finland refused German requests to participate actively in the Siege of Leningrad, and also granted asylum to Jews, while Jewish soldiers continued to serve in its army.
The relationship between Finland and Germany more closely resembled an alliance during the six weeks of the Ryti-Ribbentrop Agreement, which was presented as a German condition for help with munitions and air support, as the Soviet offensive coordinated with D-Day threatened Finland with complete occupation. The agreement, signed by President Risto Ryti but never ratified by the Finnish Parliament, bound Finland not to seek a separate peace.
After Soviet offensives were fought to a standstill, Ryti's successor as president, Marshall Mannerheim, dismissed the agreement and opened secret negotiations with the Soviets, which resulted in a ceasefire on 4 September and the Moscow Armistice on 19 September 1944. Under the terms of the armistice, Finland was obliged to expel German troops from Finnish territory, which resulted in the Lapland War. Finland signed a peace treaty with the Allied powers in 1947.
Iraq.
The Kingdom of Iraq was briefly an ally of the Axis, fighting the United Kingdom in the Anglo-Iraqi War of May 1941.
Anti-British sentiments were widespread in Iraq prior to 1941. Seizing power on 1 April 1941, the nationalist government of Prime Minister Rashid Ali repudiated the Anglo-Iraqi Treaty of 1930 and demanded that the British abandon their military bases and withdraw from the country. Ali sought support from Germany and Italy in expelling British forces from Iraq.
On 9 May 1941, Mohammad Amin al-Husayni, the Mufti of Jerusalem and associate of Ali, declared holy war against the British and called on Arabs throughout the Middle East to rise up against British rule. On 25 May 1941, the Germans stepped up offensive operations in the Middle East.
Hitler issued Order 30: "The Arab Freedom Movement in the Middle East is our natural ally against England. In this connection special importance is attached to the liberation of Iraq ... I have therefore decided to move forward in the Middle East by supporting Iraq. "
Hostilities between the Iraqi and British forces began on 2 May 1941, with heavy fighting at the RAF air base in Habbaniyah. The Germans and Italians dispatched aircraft and aircrew to Iraq utilizing Vichy French bases in Syria, which would later invoke fighting between Allied and Vichy French forces in Syria.
The Germans planned to coordinate a combined German-Italian offensive against the British in Egypt, Palestine, and Iraq. Iraqi military resistance ended by 31 May 1941. Rashid Ali and the Mufti of Jerusalem fled to Iran, then Turkey, Italy, and finally Germany, where Ali was welcomed by Hitler as head of the Iraqi government-in-exile in Berlin. In propaganda broadcasts from Berlin, the Mufti continued to call on Arabs to rise up against the British and aid German and Italian forces. He also helped recruit Muslim volunteers in the Balkans for the "Waffen-SS".
Client states.
Japanese.
The Empire of Japan created a number of client states in the areas occupied by its military, beginning with the creation of Manchukuo in 1932. These puppet states achieved varying degrees of international recognition.
Burma (Ba Maw regime).
The Japanese Army and Burma nationalists, led by Aung San, seized control of Burma from the United Kingdom during 1942. A State of Burma was formed on 1 August under the Burmese nationalist leader Ba Maw. The Ba Maw regime established the Burma Defence Army (later renamed the Burma National Army), which was commanded by Aung San.
Cambodia.
The Kingdom of Cambodia was a short-lived Japanese puppet state that lasted from 9 March 1945 to 15 August 1945.
The Japanese entered Cambodia in mid-1941, but allowed Vichy French officials to remain in administrative posts. The Japanese calls for an "Asia for the Asiatics" won over many Cambodian nationalists.
This policy changed during the last months of the war. The Japanese wanted to gain local support, so they dissolved French colonial rule and pressured Cambodia to declare its independence within the Greater East Asia Co-Prosperity Sphere. Four days later, King Sihanouk declared Kampuchea (the original Khmer pronunciation of Cambodia) independent. Co-editor of the "Nagaravatta", Son Ngoc Thanh, returned from Tokyo in May and was appointed foreign minister.
On the date of Japanese surrender, a new government was proclaimed with Son Ngoc Thanh as prime minister. When the Allies occupied Phnom Penh in October, Son Ngoc Thanh was arrested for collaborating with the Japanese and was exiled to France. Some of his supporters went to northwestern Cambodia, which had been under Thai control since the French-Thai War of 1940, where they banded together as one faction in the Khmer Issarak movement, originally formed with Thai encouragement in the 1940s.
China (Reorganized National Government of China).
During the Second Sino-Japanese War, Japan advanced from its bases in Manchuria to occupy much of East and Central China. Several Japanese puppet states were organized in areas occupied by the Japanese Army, including the Provisional Government of the Republic of China at Beijing, which was formed in 1937, and the Reformed Government of the Republic of China at Nanjing, which was formed in 1938. These governments were merged into the Reorganized National Government of China at Nanjing on 29 March 1940. Wang Jingwei became head of state. The government was to be run along the same lines as the Nationalist regime and adopted its symbols.
The Nanjing Government had no real power; its main role was to act as a propaganda tool for the Japanese. The Nanjing Government concluded agreements with Japan and Manchukuo, authorising Japanese occupation of China and recognising the independence of Manchukuo under Japanese protection. The Nanjing Government signed the Anti-Comintern Pact of 1941 and declared war on the United States and the United Kingdom on 9 January 1943.
The government had a strained relationship with the Japanese from the beginning. Wang's insistence on his regime being the true Nationalist government of China and in replicating all the symbols of the Kuomintang led to frequent conflicts with the Japanese, the most prominent being the issue of the regime's flag, which was identical to that of the Republic of China.
The worsening situation for Japan from 1943 onwards meant that the Nanking Army was given a more substantial role in the defence of occupied China than the Japanese had initially envisaged. The army was almost continuously employed against the communist New Fourth Army.
Wang Jingwei died on 10 November 1944, and was succeeded by his deputy, Chen Gongbo. Chen had little influence; the real power behind the regime was Zhou Fohai, the mayor of Shanghai. Wang's death dispelled what little legitimacy the regime had. The state stuttered on for another year and continued the display and show of a fascist regime.
On 9 September 1945, following the defeat of Japan, the area was surrendered to General He Yingqin, a nationalist general loyal to Chiang Kai-shek. The Nanking Army generals quickly declared their alliance to the Generalissimo, and were subsequently ordered to resist Communist attempts to fill the vacuum left by the Japanese surrender. Chen Gongbo was tried and executed in 1946.
India (Provisional Government of Free India).
The "Arzi Hukumat-e-Azad Hind", the Provisional Government of Free India was a state that was recognized by nine Axis governments. It was led by Subhas Chandra Bose, an Indian nationalist who rejected Mohandas K. Gandhi's nonviolent methods for achieving independence. It was sponsored and effectively controlled by Japan. In 1942 Japan provided Bose with Indian prisoners of war and Indian residents in southeast Asia to allow him to found the Indian National Army. Bose declared India's independence on October 21, 1943. The Japanese Army supervised the Indian National Army through military advisors such as Hideo Iwakuro. It fought in Burma and was largely destroyed by the British and disease. The provisional government was given nominal control of the Andaman and Nicobar Islands from November 1943 to August 1945.
Inner Mongolia (Mengjiang).
Mengjiang was a Japanese puppet state in Inner Mongolia. It was nominally ruled by Prince Demchugdongrub, a Mongol nobleman descended from Genghis Khan, but was in fact controlled by the Japanese military. Mengjiang's independence was proclaimed on 18 February 1936, following the Japanese occupation of the region.
The Inner Mongolians had several grievances against the central Chinese government in Nanking, including their policy of allowing unlimited migration of Han Chinese to the region. Several of the young princes of Inner Mongolia began to agitate for greater freedom from the central government, and it was through these men that Japanese saw their best chance of exploiting Pan-Mongol nationalism and eventually seizing control of Outer Mongolia from the Soviet Union.
Japan created Mengjiang to exploit tensions between ethnic Mongolians and the central government of China, which in theory ruled Inner Mongolia. When the various puppet governments of China were unified under the Wang Jingwei government in March 1940, Mengjiang retained its separate identity as an autonomous federation. Although under the firm control of the Japanese Imperial Army, which occupied its territory, Prince Demchugdongrub had his own independent army.
Mengjiang vanished in 1945 following Japan's defeat in World War II. As Soviet forces advanced into Inner Mongolia, they met limited resistance from small detachments of Mongolian cavalry, which, like the rest of the army, were quickly overwhelmed.
Laos.
Fears of Thai irredentism led to the formation of the first Lao nationalist organization, the Movement for National Renovation, in January 1941. The group was led by Prince Phetxarāt and supported by local French officials, though not by the Vichy authorities in Hanoi. This group wrote the current Lao national anthem and designed the current Lao flag, while paradoxically pledging support for France. The country declared its independence in 1945.
The liberation of France in 1944, bringing Charles de Gaulle to power, meant the end of the alliance between Japan and the Vichy French administration in Indochina. The Japanese had no intention of allowing the Gaullists to take over, and in March 1945 they staged a military coup in Hanoi. Some French units fled over the mountains to Laos, pursued by the Japanese, who occupied Viang Chan in March 1945 and Luang Phrabāng in April. King Sīsavāngvong was detained by the Japanese, but his son Crown Prince Savāngvatthanā called on all Lao to assist the French, and many Lao died fighting against the Japanese occupiers.
Prince Phetxarāt opposed this position. He thought that Lao independence could be gained by siding with the Japanese, who made him Prime Minister of Luang Phrabāng, though not of Laos as a whole. The country was in chaos, and Phetxarāt's government had no real authority. Another Lao group, the Lao Sēri (Free Lao), received unofficial support from the Free Thai movement in the Isan region.
Manchuria (Manchukuo).
Manchukuo, in the northeast region of China, had been a Japanese puppet state in Manchuria since the 1930s. It was nominally ruled by Puyi, the last emperor of the Qing Dynasty, but was in fact controlled by the Japanese military, in particular the Kwantung Army. While Manchukuo ostensibly was a state for ethnic Manchus, the region had a Han Chinese majority.
Following the Japanese invasion of Manchuria in 1931, the independence of Manchukuo was proclaimed on 18 February 1932, with Puyi as head of state. He was proclaimed the Emperor of Manchukuo a year later. The new Manchu nation was recognized by 23 of the League of Nations' 80 members. Germany, Italy, and the Soviet Union were among the major powers who recognised Manchukuo. Other countries who recognized the State were the Dominican Republic, Costa Rica, El Salvador, and Vatican City. Manchukuo was also recognised by the other Japanese allies and puppet states, including Mengjiang, the Burmese government of Ba Maw, Thailand, the Wang Jingwei regime, and the Indian government of Subhas Chandra Bose. The League of Nations later declared in 1934 that Manchuria lawfully remained a part of China. This precipitated Japanese withdrawal from the League. The Manchukuoan state ceased to exist after the Soviet invasion of Manchuria in 1945.
Philippines (Second Republic).
After the surrender of the Filipino and American forces in Bataan Peninsula and Corregidor Island, the Japanese established a puppet state in the Philippines in 1942. The following year, the Philippine National Assembly declared the Philippines an independent Republic and elected José Laurel as its President. There was never widespread civilian support for the state, largely because of the general anti-Japanese sentiment stemming from atrocities committed by the Imperial Japanese Army. The Second Philippine Republic ended with Japanese surrender in 1945, and Laurel was arrested and charged with treason by the US government. He was granted amnesty by President Manuel Roxas, and remained active in politics, ultimately winning a seat in the post-war Senate.
Vietnam (Empire of Vietnam).
The Empire of Vietnam was a short-lived Japanese puppet state that lasted from 11 March to 23 August 1945.
When the Japanese seized control of French Indochina, they allowed Vichy French administrators to remain in nominal control. This French rule ended on 9 March 1945, when the Japanese officially took control of the government. Soon after, Emperor Bảo Đại voided the 1884 treaty with France and Trần Trọng Kim, a historian, became prime minister.
The state suffered through the Vietnamese Famine of 1945 and replaced French-speaking schools with Vietnamese language schools, taught by Vietnamese scholars.
Italian.
Italy occupied several nations and set up clients in those regions to carry out administrative tasks and maintain order.
Monaco.
The Principality of Monaco was officially neutral during the war. The population of the country was largely of Italian descent and sympathized with Italy. Its prince was a close friend of the Vichy French leader, Marshal Philippe Pétain, an Axis collaborator. A fascist regime was established under the nominal rule of the prince when the Italian Fourth Army occupied the country on November 10, 1942 as a part of Case Anton. Monaco's military forces, consisting primarily of police and palace guards, collaborated with the Italians during the occupation. German troops occupied Monaco in 1943, and Monaco was liberated by Allied forces in 1944.
German.
The collaborationist administrations of German-occupied countries in Europe had varying degrees of autonomy, and not all of them qualified as fully recognized sovereign states. The General Government in occupied Poland was a German administration, not a Polish government. In occupied Norway, the National Government headed by Vidkun Quisling – whose name came to symbolize pro-Axis collaboration in several languages – was subordinate to the Reichskommissariat Norwegen. It was never allowed to have any armed forces, be a recognized military partner, or have autonomy of any kind. In the occupied Netherlands, Anton Mussert was given the symbolic title of "Führer of the Netherlands' people". His National Socialist Movement formed a cabinet assisting the German administration, but was never recognized as a real Dutch government. The following list of German client states includes only those entities that were officially considered to be independent countries allied with Germany. They were under varying degrees of German influence and control, but were not ruled directly by Germans.
Albania (under German control).
After the Italian armistice, a vacuum of power opened up in Albania. The Italian occupying forces were rendered largely powerless, as the National Liberation Movement took control of the south and the National Front (Balli Kombëtar) took control of the north. Albanians in the Italian army joined the guerrilla forces. In September 1943 the guerrillas moved to take the capital of Tirana, but German paratroopers dropped into the city. Soon after the battle, the German High Command announced that they would recognize the independence of a greater Albania. They organized an Albanian government, police, and military in collaboration with the Balli Kombëtar. The Germans did not exert heavy control over Albania's administration, but instead attempted to gain popular appeal by giving their political partners what they wanted. Several Balli Kombëtar leaders held positions in the regime. The joint forces incorporated Kosovo, western Macedonia, southern Montenegro, and Presevo into the Albanian state. A High Council of Regency was created to carry out the functions of a head of state, while the government was headed mainly by Albanian conservative politicians. Albania was the only European country occupied by the Axis powers that ended World War II with a larger Jewish population than before the war. The Albanian government had refused to hand over their Jewish population. They provided Jewish families with forged documents and helped them disperse in the Albanian population. Albania was completely liberated on November 29, 1944.
Italy (Italian Social Republic).
 
Italian Fascist leader Benito Mussolini formed the Italian Social Republic ("Repubblica Sociale Italiana" in Italian) on 23 September 1943, succeeding the Kingdom of Italy as a member of the Axis.
Mussolini had been removed from office and arrested by King Victor Emmanuel III on 25 July 1943. After the Italian armistice, in a raid led by German paratrooper Otto Skorzeny, Mussolini was rescued from arrest.
Once restored to power, Mussolini declared that Italy was a republic and that he was the new head of state. He was subject to German control for the duration of the war.
Slovakia (Tiso regime).
The Slovak Republic under President Josef Tiso signed the Tripartite Pact on 24 November 1940.
Slovakia had been closely aligned with Germany almost immediately from its declaration of independence from Czechoslovakia on 14 March 1939. Slovakia entered into a treaty of protection with Germany on 23 March 1939.
Slovak troops joined the German invasion of Poland, having interest in Spiš and Orava. Those two regions, along with Cieszyn Silesia, had been disputed between Poland and Czechoslovakia since 1918. The Poles fully annexed them following the Munich Agreement. After the invasion of Poland, Slovakia reclaimed control of those territories.
Slovakia invaded Poland alongside German forces, contributing 50,000 men at this stage of the war.
Slovakia declared war on the Soviet Union in 1941 and signed the revived Anti-Comintern Pact in 1941. Slovak troops fought on Germany's Eastern Front, furnishing Germany with two divisions totaling 80,000 men. Slovakia declared war on the United Kingdom and the United States in 1942.
Slovakia was spared German military occupation until the Slovak National Uprising, which began on 29 August 1944, and was almost immediately crushed by the Waffen SS and Slovak troops loyal to Josef Tiso.
After the war, Tiso was executed and Slovakia once again became part of Czechoslovakia. The border with Poland was shifted back to the pre-war state. Slovakia and the Czech Republic finally separated into independent states in 1993.
Joint German-Italian client states.
Croatia (Independent State of Croatia).
On 10 April 1941, the Independent State of Croatia ("Nezavisna Država Hrvatska", or NDH) declared itself a member of the Axis, co-signing the Tripartite Pact. The NDH remained a member of the Axis until the end of Second World War, its forces fighting for Germany even after its territory had been overrun by Yugoslav Partisans. On 16 April 1941, Ante Pavelić, a Croatian nationalist and one of the founders of the Ustaše ("Croatian Liberation Movement"), was proclaimed "Poglavnik" (leader) of the new regime.
Initially the Ustaše had been heavily influenced by Italy. They were actively supported by Mussolini's Fascist regime in Italy, which gave the movement training grounds to prepare for war against Yugoslavia, as well as accepting Pavelić as an exile and allowing him to reside in Rome. Italy intended to use the movement to destroy Yugoslavia, which would allow Italy to expand its power through the Adriatic. Hitler did not want to engage in a war in the Balkans until the Soviet Union was defeated. The Italian occupation of Greece was not going well; Mussolini wanted Germany to invade Yugoslavia to save the Italian forces in Greece. Hitler reluctantly agreed; Yugoslavia was invaded and the Independent State of Croatia was created. Pavelić led a delegation to Rome and offered the crown of Croatia to an Italian prince of the House of Savoy, who was crowned "Tomislav II, King of Croatia, Prince of Bosnia and Herzegovina, Voivode of Dalmatia, Tuzla and Knin, Prince of Cisterna and of Belriguardo, Marquess of Voghera, and Count of Ponderano." The next day, Pavelić signed the Contracts of Rome with Mussolini, ceding Dalmatia to Italy and fixing the permanent borders between the NDH and Italy. Italian armed forces were allowed to control all of the coastline of the NDH, effectively giving Italy total control of the Adriatic coastline.
However, strong German influence began to be asserted soon after the NDH was founded. When the King of Italy ousted Mussolini from power and Italy capitulated, the NDH became completely under German influence.
The platform of the Ustaše movement proclaimed that Croatians had been oppressed by the Serb-dominated Kingdom of Yugoslavia, and that Croatians deserved to have an independent nation after years of domination by foreign empires. The Ustaše perceived Serbs to be racially inferior to Croats and saw them as infiltrators who were occupying Croatian lands. They saw the extermination of Serbs as necessary to racially purify Croatia. While part of Yugoslavia, many Croatian nationalists violently opposed the Serb-dominated Yugoslav monarchy, and assassinated Alexander I of Yugoslavia, together with the Internal Macedonian Revolutionary Organization. The regime enjoyed support amongst radical Croatian nationalists. Ustashe forces fought against communist Yugoslav Partisan guerrilla throughout the war.
Upon coming to power, Pavelić formed the Croatian Home Guard ("Hrvatsko domobranstvo") as the official military force of the NDH. Originally authorized at 16,000 men, it grew to a peak fighting force of 130,000. The Croatian Home Guard included an air force and navy, although its navy was restricted in size by the Contracts of Rome. In addition to the Croatian Home Guard, Pavelić was also the supreme commander of the Ustaše militia, although all NDH military units were generally under the command of the German or Italian formations in their area of operations.
The Ustaše government declared war on the Soviet Union, signed the Anti-Comintern Pact of 1941, and sent troops to Germany's Eastern Front. Ustaše militia were garrisoned in the Balkans, battling the communist partisans.
The Ustaše government applied racial laws on Serbs, Jews, and Romani people, and after June 1941 deported them to the Jasenovac concentration camp or to German camps in Poland. The racial laws were enforced by the Ustaše militia. The exact number of victims of the Ustaše regime is uncertain due to the destruction of documents and varying numbers given by historians. According to the United States Holocaust Memorial Museum in Washington, DC, between 320,000 and 340,000 Serbs were killed in the NDH.
The Ustaše never had widespread support among the population of the NDH. Their own estimates put the number of sympathizers, even in the early phase, at around 40,000 out of total population of 7 million. However, they were able to rely on the passive acceptance of much of the Croat population of the NDH.
Greece.
Following the German invasion of Greece and the flight of the Greek government to Crete and then Egypt, the Hellenic State was formed in May 1941 as a puppet state of both Italy and Germany. Initially, Italy had wished to annex Greece, but was pressured by Germany to avoid civil unrest such as had occurred in Bulgarian-annexed areas. The result was Italy accepting the creation of a puppet regime with the support of Germany. Italy had been assured by Hitler of a primary role in Greece. Most of the country was held by Italian forces, but strategic locations (Central Macedonia, the islands of the northeastern Aegean, most of Crete, and parts of Attica) were held by the Germans, who seized most of the country's economic assets and effectively controlled the collaborationist government. The puppet regime never commanded any real authority, and did not gain the allegiance of the people. It was somewhat successful in preventing secessionist movements like the Principality of the Pindus from establishing themselves. By mid-1943, the Greek Resistance had liberated large parts of the mountainous interior ("Free Greece"), setting up a separate administration there. After the Italian armistice, the Italian occupation zone was taken over by the German armed forces, who remained in charge of the country until their withdrawal in autumn 1944. In some Aegean islands, German garrisons were left behind, and surrendered only after the end of the war.
Controversial cases.
States listed in this section were not officially members of the Axis, but at some point during the war engaged in cooperation with one or more Axis members on level that makes their neutrality disputable.
Denmark.
Denmark was occupied by Germany after April 1940 but never joined the Axis. On 31 May 1939, Denmark and Germany signed a treaty of non-aggression, which did not contain any military obligations for either party. On April 9, Germany attacked Scandinavia, and the speed of the German invasion of Denmark prevented King Christian X and the Danish government from going into exile. They had to accept "protection by the Reich" and the stationing of German forces in exchange for nominal independence. Denmark coordinated its foreign policy with Germany, extending diplomatic recognition to Axis collaborator and puppet regimes, and breaking diplomatic relations with the Allied governments-in-exile. Denmark broke diplomatic relations with the Soviet Union and signed the Anti-Comintern Pact in 1941. However the United States and Britain ignored Denmark and worked with Denmark's ambassadors when it came to dealings about using Iceland, Greenland, and the Danish merchant fleet against Germany.
In 1941 Danish Nazis set up the "Frikorps Danmark". Thousands of volunteers fought and many died as part of the German Army on the Eastern Front. Denmark sold agricultural and industrial products to Germany and made loans for armaments and fortifications. The German presence in Denmark, including the construction of the Danish paid for part of the Atlantic Wall fortifications and was never reimbursed.
The Danish protectorate government lasted until 29 August 1943, when the cabinet resigned after the regularly scheduled and largely free election concluding the Folketing's current term. The Germans imposed martial law, and Danish collaboration continued on an administrative level, with the Danish bureaucracy functioning under German command. The Danish navy scuttled 32 of its larger ships; Germany seized 64 ships and later raised and refitted 15 of the sunken vessels. 13 warships escaped to Sweden and formed a Danish naval flotilla in exile. Sweden allowed formation of a Danish military brigade in exile; it did not see combat. The resistance movement was active in sabotage and issuing underground newspapers and blacklists of collaborators.
Soviet Union.
Relations between the Soviet Union and the major Axis powers were generally hostile before 1938. In the Spanish Civil War, the Soviet Union gave military aid to the Second Spanish Republic, against Spanish Nationalist forces, which were assisted by Germany and Italy. However, the Nationalist forces were victorious. The Soviets suffered another political defeat when their ally Czechoslovakia was partitioned and taken over by Germany in 1938-39. In 1938 and 1939, the USSR fought and defeated Japan in two separate border conflicts, at Lake Khasan and Khalkhin Gol. The latter was a major Soviet victory that led the Japanese Army to avoid war with the Soviets and instead call for expansion south.
In 1939 the Soviet Union considered forming an alliance with either Britain and France or with Germany. When negotiations with Britain and France failed, they turned to Germany and signed the Molotov-Ribbentrop Pact in August 1939. Germany was now freed from the risk of war with the Soviets, and was assured a supply of oil. This included a secret protocol whereby the independent countries of Finland, Estonia, Latvia, Lithuania, Poland, and Romania were divided into spheres of interest of the parties. The Soviet Union had been forced to cede Western Belarus and Western Ukraine to Poland after losing the Soviet-Polish War of 1919–1921, and the Soviet Union sought to regain those territories.
On 1 September, barely a week after the pact had been signed, Germany invaded Poland. The Soviet Union invaded Poland from the east on 17 September and on 28 September signed a secret treaty with Nazi Germany to arrange coordination of fighting against Polish resistance. The Soviets targeted intelligence, entrepreneurs, and officers, committing a string of atrocities that culminated in the Katyn massacre and mass relocation to the Gulag in Siberia.
Soon thereafter, the Soviet Union occupied the Baltic countries of Estonia, Latvia, and Lithuania, and annexed Bessarabia and Northern Bukovina from Romania. The Soviet Union attacked Finland on 30 November 1939, which started the Winter War. Finnish defences prevented an all-out invasion, resulting in an interim peace, but Finland was forced to cede strategically important border areas near Leningrad.
The Soviet Union provided material support to Germany in the war effort against Western Europe through a pair of commercial agreements, the first in 1939 and the second in 1940, which involved exports of raw materials (phosphates, chromium and iron ore, mineral oil, grain, cotton, and rubber). These and other export goods transported through Soviet and occupied Polish territories allowed Germany to circumvent the British naval blockade.
In October and November 1940, German-Soviet talks about the potential of joining the Axis took place in Berlin. Joseph Stalin later personally countered with a separate proposal in a letter later in November that contained several secret protocols, including that "the area south of Batum and Baku in the general direction of the Persian Gulf is recognized as the center of aspirations of the Soviet Union", referring to an area approximating present day Iraq and Iran, and a Soviet claim to Bulgaria. Hitler never responded to Stalin's letter. Shortly thereafter, Hitler issued a secret directive on the eventual attempt to invade the Soviet Union. 
Germany ended the Molotov-Ribbentrop Pact by invading the Soviet Union in Operation Barbarossa on 22 June 1941. That resulted in the Soviet Union becoming one of the main members of the Allies.
Germany then revived its Anti-Comintern Pact, enlisting many European and Asian countries in opposition to the Soviet Union. The Soviet Union and Japan remained neutral towards each other for most of the war by the Soviet-Japanese Neutrality Pact. The Soviet Union ended the Soviet-Japanese Neutrality Pact by invading Manchukuo on 8 August 1945, due to agreements reached at the Yalta Conference with Roosevelt and Churchill.
Spain.
"Caudillo" Francisco Franco's Spanish State gave moral, economic, and military assistance to the Axis powers, while nominally maintaining neutrality. Franco described Spain as a member of the Axis and signed the Anti-Comintern Pact in 1941 with Hitler and Mussolini. Members of the ruling Falange party in Spain held irredentist designs on Gibraltar. Falangists also supported Spanish colonial acquisition of Tangier, French Morocco and northwestern French Algeria. In addition, Spain held ambitions on former Spanish colonies in Latin America. In June 1940 the Spanish government approached Germany to propose an alliance in exchange for Germany recognizing Spain's territorial aims: the annexation of the Oran province of Algeria, the incorporation of all Morocco, the extension of Spanish Sahara southward to the twentieth parallel, and the incorporation of French Cameroons into Spanish Guinea. In 1940 Spain invaded and occupied the Tangier International Zone, maintaining its occupation until 1945. The occupation caused a dispute between Britain and Spain in November 1940; Spain conceded to protect British rights in the area and promised not to fortify the area. The Spanish government secretly held expansionist plans towards Portugal that it made known to the German government. In a communiqué with Germany on 26 May 1942, Franco declared that Portugal should be annexed into Spain. 
Franco had previously won the Spanish Civil War with the help of Nazi Germany and Fascist Italy. Both were eager to establish another fascist state in Europe. Spain owed Germany over $212 million for supplies of matériel during the Spanish Civil War, and Italian combat troops had actually fought in Spain on the side of Franco's Nationalists.
From 1940 to 1941, Franco endorsed a Latin Bloc of Italy, Vichy France, Spain, and Portugal, with support from the Vatican in order to balance the countries' powers to that of Germany. Franco discussed the Latin Bloc alliance with Pétain of Vichy France in Montpellier, France in 1940, and with Mussolini in Bordighera, Italy.
When Germany invaded the Soviet Union in 1941, Franco immediately offered to form a unit of military volunteers to join the invasion. This was accepted by Hitler and, within two weeks, there were more than enough volunteers to form a division – the Blue Division ("División Azul") under General Agustín Muñoz Grandes.
The possibility of Spanish intervention in World War II was of concern to the United States, which investigated the activities of Spain's ruling Falange party in Latin America, especially Puerto Rico, where pro-Falange and pro-Franco sentiment was high, even amongst the ruling upper classes. The Falangists promoted the idea of supporting Spain's former colonies in fighting against American domination. Prior to the outbreak of war, support for Franco and the Falange was high in the Philippines. The Falange Exterior, the international department of the Falange, collaborated with Japanese forces against U.S. and Filipino forces in the Philippines through the Philippine Falange. 
Vichy France.
Although officially neutral, Marshal Philippe Pétain's "Vichy regime" collaborated with the Axis from June 1940. It retained full control of the non-occupied part of France until November 1942 - when the whole of France was occupied by Germany - and of a large part of France's colonial empire, until the colonies gradually fell under Free French control.
The German invasion army entered Paris on 14 June 1940, following the battle of France. Pétain became the last Prime Minister of the French Third Republic on 16 June 1940. He sued for peace with Germany and on 22 June 1940, the Vichy government concluded an armistice with Hitler. Under the terms of the agreement, Germany occupied two-thirds of France, including Paris. Pétain was permitted to keep an "armistice army" of 100,000 men within the unoccupied southern zone. This number included neither the army based in the French colonial empire nor the French fleet. In Africa the Vichy regime was permitted to maintain 127,000. The French also maintained substantial garrisons at the French-mandated territory of Syria and Greater Lebanon, the French colony of Madagascar, and in French Somaliland. Some members of the Vichy government pushed for closer cooperation, but they were rebuffed by Pétain. Neither did Hitler accept that France could ever become a full military partner, and constantly prevented the buildup of Vichy's military strength.
After the armistice, relations between the Vichy French and the British quickly worsened. Fearful that the powerful French fleet might fall into German hands, the British launched several naval attacks, the most notable of which was against the Algerian harbour of Mers el-Kebir on 3 July 1940. Though Churchill defended his controversial decision to attack the French Fleet, the action deteriorated greatly the relations between France and Britain. German propaganda trumpeted these attacks as an absolute betrayal of the French people by their former allies.
On 10 July 1940, Pétain was given emergency "full powers" by a majority vote of the French National Assembly. The following day approval of the new constitution by the Assembly effectively created the French State ("l'État Français"), replacing the French Republic with the government unofficially called "Vichy France," after the resort town of Vichy, where Pétain maintained his seat of government. This continued to be recognised as the lawful government of France by the neutral United States until 1942, while the United Kingdom had recognised de Gaulle's government-in-exile in London. Racial laws were introduced in France and its colonies and many French Jews were deported to Germany. Albert Lebrun, last President of the Republic, did not resign from the presidential office when he moved to Vizille on 10 July 1940. By 25 April 1945, during Pétain's trial, Lebrun argued that he thought he would be able to return to power after the fall of Germany, since he had not resigned.
In September 1940, Vichy France was forced to allow Japan occupy French Indochina, a federation of French colonial possessions and protectorates encompassing modern day Vietnam, Laos, and Cambodia. The Vichy regime continued to administer them under Japanese military occupation. French Indochina was the base for the Japanese invasions of Thailand, Malaya, and the Dutch East Indies. In 1945, under Japanese sponsorship, the Empire of Vietnam and the Kingdom of Kampuchea were proclaimed as Japanese puppet states.
On 26 September 1940, de Gaulle led an attack by Allied forces on the Vichy port of Dakar in French West Africa. Forces loyal to Pétain fired on de Gaulle and repulsed the attack after two days of heavy fighting, drawing Vichy France closer to Germany.
During the Anglo–Iraqi War of May 1941, Vichy France allowed Germany and Italy to use air bases in the French mandate of Syria to support the Iraqi revolt. British and Free French forces attacked later Syria and Lebanon in June–July 1941, and in 1942 Allied forces took over French Madagascar. More and more colonies abandoned Vichy, joining the Free French territories of French Equatorial Africa, Polynesia, New Caledonia and others who had sided with de Gaulle from the start.
In November 1942 Vichy French troops briefly resisted the landing of Allied troops in French North Africa for a couple of days, until Admiral François Darlan negotiated a local ceasefire with the Allies. In response to the landings, Axis troops invaded the non-occupied zone in southern France and ended Vichy France as an entity with any kind of autonomy; it then became a puppet government for the occupied territories.
In June 1943, the formerly Vichy-loyal colonial authorities in French North Africa led by Henri Giraud came to an agreement with the Free French to merge with their own interim regime with the French National Committee ("Comité Français National", CFN) to form a provisional government in Algiers, known as the French Committee of National Liberation ("Comité Français de Libération Nationale", CFLN) initially led by Darlan. After his assassination De Gaulle emerged as the uncontested French leader. The CFLN raised more troops and re-organised, re-trained and re-equipped the Free French military, in cooperation with Allied forces in preparation of future operations against Italy and the German Atlantic wall.
In 1943 the Milice, a paramilitary force which had been founded by Vichy, was subordinated to the Germans and assisted them in rounding up opponents and Jews, as well as fighting the French Resistance. the Germans recruited volunteers in units independent of Vichy. Partly as a result of the great animosity of many right-wingers against the pre-war Front Populaire, volunteers joined the German forces in their anti-communist crusade against the URSS. Almost 7,000 joined "Légion des Volontaires Français" (LVF) from 1941 to 1944. The LVF then formed the cadre of the Waffen-SS Division "Charlemagne" in 1944-1945, with a maximum strength of some 7,500. Both the LVF and the "Division Charlemagne" fought on the eastern front.
Deprived of any military assets, territory or resources, the members of the Vichy government continued to fulfil their role as German puppets, being quasi-prisoners in the so-called "Sigmaringen enclave" in a castle in Baden-Württemberg at the end of the war in May 1945.
Yugoslavia.
After the dissolution of the Little Entente, German occupation of Czechoslovakia, and annexation of Austria in 1938, a position of Yugoslavia become very difficult. Italy, Bulgaria, Hungary and Albania had territorial pretensions. France, a traditional Yugoslav ally, capitulated in 1940. In October 1940, Italy unsuccessfully invaded Greece from Albania. British troops landed in Greece. Hitler wanted to intervene in Greece. Hungary, Romania and Bulgaria signed the Tripartite Pact.
In February 1941, Hitler requested Yugoslavia's accession to the Tripartite Pact. In March, German army arrived at the Bulgarian-Yugoslav border. On 25 March 1941, fearing that Yugoslavia would be invaded otherwise, the Yugoslav government signed the Tripartite Pact with significant reservations. Unlike other Axis powers, Yugoslavia was not obligated to provide military assistance, nor to provide its territory for Axis to move military forces during the war.
Less than two days latter, after demonstrations in the streets of Belgrade, Prince Paul and the government were removed from office by a coup d'état. Seventeen-year-old King Peter was declared to be of age. The new Yugoslav government under General Dušan Simović, refused to ratify Yugoslavia's signing of the Tripartite Pact, but did not openly rule it out. At the same time, the government started negotiations with Great Britain and Soviet Union. Winston Churchill, Prime Minister of the United Kingdom, commented that "Yugoslavia has found its soul".
Hitler was angered by the coup and anti-German incidents in Belgrade. On the same day as the coup he issued Führer Directive 25 which called for Yugoslavia to be treated as a hostile state, and ordered that Yugoslavia be crushed without delay. The German invasion began on 6 April 1941. Royal Yugoslav Army was thoroughly defeated in less than two weeks and an unconditional surrender of the all Yugoslav troops was signed on 17 April. King Peter II and much of the Yugoslavian government had left the country.
After the surrender, Yugoslavia was subsequently divided amongst Germany, Hungary, Italy and Bulgaria, with most of Serbia being occupied by Germany. The Italian-backed Croatian fascist leader Ante Pavelić declared the Independent State of Croatia before the invasion was even over. The Yugoslav government-in-exile was still recognized by the Allied powers and some neutral countries. Yugoslavia was one of 26 Allies of World War II that signed the Declaration by the United Nations on 1 January 1942.
Beginning with the uprising in Herzegovina in June 1941, there was continuous resistance to the occupying armies in Yugoslavia until the end of the war. While in the beginning both Partisans and the Chetniks engaged in resistance, the Partisans became the main resistance force after Chetniks started to collaborate with the Axis forces in 1942.
German, Japanese and Italian World War II cooperation.
Germany's and Italy's declaration of war against the United States.
On 7 December 1941, Japan attacked the naval bases in Pearl Harbor, Hawaii. According to the stipulation of the Tripartite Pact, Nazi Germany was required to come to the defense of her allies only if they were attacked. Since Japan had made the first move, Germany and Italy were not obliged to aid her until the United States counterattacked. Nevertheless, Hitler ordered the "Reichstag" to formally declare war on the United States. Italy also declared war.
Historian Ian Kershaw suggests that this declaration of war against the United States was a serious blunder made by Germany, as it allowed the United States to join the war without any limitation. On the other hand, American destroyers escorting convoys had already been de facto at war for months with German U-boats in the Atlantic, and the immediate war declaration made the Second Happy Time possible for U-boats. Americans played a key role in financing and supplying the Allies, in the strategic bombardment of Germany, and in the final invasion of the continent.
References.
Print sources
Online sources

</doc>
<doc id="43508" url="http://en.wikipedia.org/wiki?curid=43508" title="Amen">
Amen

The word amen ( or ; Hebrew: אָמֵן,  "amen",  "ʾāmēn"; Greek: ἀμήν; Arabic: آمين‎, "ʾāmīn" ; "So be it; truly") is a declaration of affirmation found in the Hebrew Bible and the New Testament. Its use in Judaism dates back to its earliest texts. It has been generally adopted in Christian worship as a concluding word for prayers and hymns. In Islam, it is the standard ending to Dua (supplication). Common English translations of the word "amen" include "verily" and "truly". It can also be used colloquially to express strong agreement, as in, for instance, "amen to that".
Pronunciation.
In English, the word "amen" has two primary pronunciations, "ah-men" (/ɑːˈmɛn/) or "ay-men" (/eɪˈmɛn/), with minor additional variation in emphasis (the two syllables may be equally stressed instead of placing primary stress on the second). The Oxford English Dictionary gives "eɪ'mεn, often ɑː'mɛn".
The "ah-men" pronunciation is used in performances of classical music, in churches with more formalized rituals and liturgy and in liberal to mainline Protestant denominations, as well as almost every Jewish congregation, in line with modern Hebrew pronunciation. The "ay-men" pronunciation, a product of the Great Vowel Shift dating to the 15th century, is associated with Irish Protestantism and conservative Evangelical denominations generally, and is the pronunciation typically used in gospel music.
In Islam the pronunciation "ah-meen" (ʾĀmīn) is used upon completing a supplication to God or when concluding recitation of the first surah Al Fatiha in prayer.
Etymology.
The usage of "Amen", meaning "so be it", as found in the early scriptures of the Bible is said to be of Hebrew origin; however, the basic triconsonantal root from which the word was derived is common to a number of Semitic Languages such as Aramaic or Syriac. The word was imported into the Greek of the early Church from Judaism. From Greek, "amen" entered the other Western languages. According to a standard dictionary etymology, "amen" passed from Greek into Late Latin, and thence into English. Rabbinic scholars from medieval France believed the standard Hebrew word for faith "emuna" comes from the root "amen." Although in English transliteration they look different, they are both from the root aleph-mem-nun. That is, the Hebrew word "amen" derives from the same ancient triliteral Hebrew root as does the verb "ʾāmán".
Grammarians frequently list "ʾāmán" under its three consonants (aleph-mem-nun), which are identical to those of "ʾāmēn" (note that the Hebrew letter א "aleph" represents a glottal stop sound, which functions as a consonant in the morphology of Hebrew). This triliteral root means "to be firm, confirmed, reliable, faithful, have faith, believe."
In Arabic, the word is derived from its triliteral common root word ʾĀmana (Arabic: آمن‎), which has the same meanings as the Hebrew root word.
Popular among some theosophists, proponents of Afrocentric theories of history, and adherents of esoteric Christianity is the conjecture that "amen" is a derivative of the name of the Egyptian god Amun (which is sometimes also spelled Amen). Some adherents of Eastern religions believe that "amen" shares roots with the Hindu Sanskrit word, "Aum". Such external etymologies are not included in standard etymological reference works. The Hebrew word, as noted above, starts with aleph, while the Egyptian name begins with a yodh.
The Armenian word "ամեն" means "every"; however it is also used in the same form at the conclusion of prayers, much as in English.
Hebrew Bible.
The word first occurs in the Hebrew Bible in Numbers 5.22 when the Priest addresses a suspected adulteress and she responds “Amen, Amen”. Overall, the word appears in the Hebrew Bible 30 times.
Three distinct Biblical usages of "amen" may be noted:
New Testament.
There are 52 amens in the Synoptic Gospels and 25 in John. The five final amens (Matthew 6:13, 28:20, Mark 16:20, Luke 24:53 and John 21:25), which are wanting in certain manuscripts, simulate the effect of final amen in the Hebrew Psalms. All initial amens occur in the sayings of Jesus. These initial amens are unparalleled in Hebrew literature, according to Friedrich Delitzsch, because they do not refer to the words of a previous speaker but instead introduce a new thought.
The uses of "amen" ("verily" or "I tell you the truth", depending on the translation) in the Gospels form a peculiar class; they are initial, but often lack any backward reference. Jesus used the word to affirm his own utterances, not those of another person, and this usage was adopted by the church. The use of the initial amen, single or double in form, to introduce solemn statements of Jesus in the Gospels had no parallel in Jewish practice.
In the King James Bible, the word "amen" is preserved in a number of contexts. Notable ones include:
Religious use.
Judaism.
Although amen, in Judaism, is commonly stated as a response to a blessing, it is also often used as an affirmation of any declaration.
Jewish rabbinical law requires an individual to say "amen" in a variety of contexts.
With the rise of the synagogue during the Second Temple period, "amen" became a common response, especially to benedictions. It is recited communally to affirm a blessing made by the prayer reader. It is also mandated as a response during the kaddish doxology.
The congregation is sometimes prompted to answer 'amen' by the terms "ve-'imru" (Hebrew: ואמרו‎) = "and [now] say (pl.)," or, "ve-nomar" (ונאמר) = "and let us say." Contemporary usage reflects ancient practice: As early as the 4th century BCE, Jews assembled in the Temple responded 'amen' at the close of a doxology or other prayer uttered by a priest. This Jewish liturgical use of amen was adopted by the Christians. But Jewish law also requires individuals to answer "amen" whenever they hear a blessing recited, even in a non-liturgical setting.
The Talmud teaches homiletically that the word "amen" is an acronym for אל מלך נאמן ("ʾEl melekh neʾeman", "God, trustworthy King"), the phrase recited silently by an individual before reciting the Shma.
Jews usually approximate the Hebrew pronunciation of the word: (Israeli-Ashkenazi and Sephardi) or (non-Israeli Ashkenazi).
Christianity.
The use of "amen" has been generally adopted in Christian worship as a concluding word for prayers and hymns and express strong agreements. The liturgical use of the word in apostolic times is attested by the passage from 1 Corinthians cited above, and Justin Martyr (c. 150) describes the congregation as responding "amen" to the benediction after the celebration of the Eucharist. Its introduction into the baptismal formula (in the Greek Orthodox Church it is pronounced after the name of each person of the Trinity) is probably later. Among certain Gnostic sects "Amen" became the name of an angel.
In Isaiah 65:16, the authorized version has "the God of truth," ("the God of amen," in Hebrew. Jesus often used amen to put emphasis to his own words (translated: "verily"). In John's Gospel, it is repeated, "Verily, verily." Amen is also used in oath (Numbers 5:22; Deuteronomy 27:15–26; Nehemiah 5:13; 8:6; 1 Chronicles 16:36). "Amen" is further found at the end of the prayer of primitive churches (1 Corinthians 14:16).
In some Christian churches, the amen corner or amen section is any subset of the congregation likely to call out "Amen!" in response to points in a preacher's sermon. Metaphorically, the term can refer to any group of heartfelt traditionalists or supporters of an authority figure.
"Amen" is also used in standard, international French, but in Cajun French "Ainsi soit-il" ("so be it") is used instead.
Amen is used at the end of Our Lord's Prayer, which is also called the Our Father or the Pater Noster.
Islam.
"ʾĀmīn" (Arabic: آمين‎) is the Arabic form of "Amen". In Islam, it is used with the same meaning as in Judaism and Christianity; when concluding a prayer, especially after a supplication (du'a) or reciting the first surah Al Fatiha of the Qur'an (salat), and as an assent to the prayers of others.
Further reading.
Schnitker, Thaddeus A. "Amen." In "The Encyclopedia of Christianity", edited by Erwin Fahlbusch and Geoffrey William Bromiley, 43–44. Vol. 1. Grand Rapids: Wm. B. Eerdmans, 1999. ISBN 0802824137

</doc>
<doc id="43512" url="http://en.wikipedia.org/wiki?curid=43512" title="Abendana">
Abendana

Abendana is a surname. Notable people with the surname include:

</doc>
<doc id="43513" url="http://en.wikipedia.org/wiki?curid=43513" title="Rhodes piano">
Rhodes piano

The Rhodes piano (also known as the Fender Rhodes piano or simply Fender Rhodes or Rhodes) is an electric piano invented by Harold Rhodes, which became particularly popular throughout the 1970s. It generates sound using keys and hammers in the same manner as an acoustic piano, but the hammers strike thin metal rods of varied length, connected to tonebars, which are then amplified via an electromagnetic pickup. 
The instrument evolved from Rhodes' attempt to manufacture pianos to teach recovering soldiers during World War II under a strict budget, and development continued throughout the 1940s and 1950s. Fender started marketing the Piano Bass, a cut-down version of the piano, but the full-size instrument did not appear until after the sale to CBS in 1965. CBS oversaw mass production of the Rhodes piano in the 1970s, and it was used extensively through the decade, particularly in jazz, pop and soul music. It fell out of fashion for a while in the mid-1980s, principally due to the emergence of polyphonic and later digital synthesizers, especially the Yamaha DX7, and partly through inconsistent quality control in production due to cost-cutting measures. The company was eventually sold to Roland, who manufactured digital versions of the Rhodes without authorization or approval from its inventor.
In the 1990s, the instrument enjoyed a resurgence in popularity, resulting in Rhodes re-obtaining the rights to the piano in 1997. Although he died in 2000, the instrument has since been reissued, and his teaching methods are still receiving active use.
Features.
The Rhodes piano features a keyboard with a similar layout to an acoustic piano, but some models contain 73 keys instead of 88. The touch and action of the keyboard is designed to be as close to an acoustic piano as possible. Pressing a key results in a hammer striking a thin rod connected to a "tonebar" resembling a tuning fork, known as a tine. The resulting vibrations from the tine sit below a pickup, which induces an electric current in a similar manner to an electric guitar. The basic mechanical act of hitting tines does not need an external power supply and a Rhodes will make a sound even when not plugged into an amplifier, though like an unplugged electric guitar the sound will be weak.
Some models of Rhodes include a built-in power amplifier and a combined tremolo and auto-pan feature that bounces the output signal from the piano in stereo across two speakers. This feature is mistakenly called "vibrato" (which is a variation in "pitch") on some models to be consistent with the labelling on Fender amplifiers.
Although the Rhodes has the same mechanical operation as an acoustic piano, its sound is very different. The sound produced by the tines has a more mellow timbre, but varies depending on the location of the tine to the pickup. Putting the two close together gives a characteristic "bell" sound. The instrument's sound has been frequently compared with the Wurlitzer electric piano, which used a similar technology, but with the hammers striking metal bars instead of tines. The Rhodes has a better sustain, while the Wurlitzer produces significant harmonics when the keys are played hard, giving it a "bite" that the Rhodes does not have.
History.
Early models.
Rhodes started teaching piano when he was 19. He dropped out of studying at the University of Southern California in 1929 to support his family through the Great Depression by full-time teaching. As a teacher, he designed a method that combined classical and jazz music, which became popular across the United States, and resulted in an hour-long nationally syndicated radio show. Rhodes continued to teach the piano through his lifetime, and the piano method continues to be taught today by a team led by Joseph Brandsetter.
By 1942, Rhodes was working for the Army Air Corps, where he was asked to devise a teaching program to provide therapy for soldiers recovering from combat in hospital. He was unable to supply enough acoustic pianos, so decided to develop a miniature electric model that could be made from surplus army parts. Rhodes won a service award for his piano design and subsequently put the model into production for piano teachers during the 1950s. These were retrospectively known as the "Pre-Piano".
In 1959, Rhodes entered a joint venture with Leo Fender to manufacture the instruments. Fender, however, disliked the higher tones of the pre-piano, and decided to manufacture a keyboard bass using the bottom 32 notes, known as the "Piano Bass". The instrument introduced the design that would become common to subsequent Rhodes pianos, with the same tolex body as Fender amplifiers and a fiberglass top. The tops came from a boat manufacturer who supplied whatever color happened to be available; consequently a number of different colored piano basses went into production.
Under CBS.
Fender was bought by CBS in 1965. Rhodes stayed with the company, and released the first Fender Rhodes piano, a 73 note model. The instrument consisted of two components — the piano and a separate enclosure containing the power amplifier and loudspeaker, which was placed underneath the former. Like the piano bass, it was finished in black tolex, and came with a fiberglass top. During the late 1960s, two models of the Fender Rhodes Celeste also became available, which used the top three or four octaves, respectively, of the Fender Rhodes piano. The Celeste didn't sell particularly well and examples are now hard to find.
The Student and Instructor models were also introduced in the late 1960s. They were designed to teach the piano in the classroom. By connecting the output of a network of student models, the teacher could listen to each student in isolation on the instructor model, and send an audio backing track to them. This allowed the teacher to monitor individual students' progress.
In 1970, the 73-note Stage Piano was introduced as a lighter and more portable alternative to the existing two-piece style, featuring four detachable legs (used in Fender steel pedal guitars), a sustain pedal and a single output jack. Although the Stage could be used with any amplifier, catalogs suggested the use of the Fender Twin Reverb. The older style piano continued to be sold alongside the Stage and was renamed the Suitcase Piano, with 88 note models also becoming available.
Later models.
During the 1970s various internal changes and improvements were made to the mechanics. In 1969 the hammer tips were changed to neoprene rubber instead of felt, to avoid the excessive need for regular maintenance, while in 1975 harp supports were changed from wood to aluminum. Although this made production cheaper, it changed the resonance of the instrument slightly. In 1977 the power amplifier design was changed from an 80 to a 100-watt model. The Mk II model was introduced in late 1979, which was simply a set of cosmetic changes over the most recent Mk I models. A new 54-note model was added to the range.
The Rhodes Mk III EK-10 was a combination electric piano and synthesizer instrument, introduced when CBS brought ARP Instruments in 1981. It used analog oscillators and filters alongside the existing electromechanical elements. The overall effect was that of a Rhodes piano and a synthesizer being played simultaneously. The instrument was unreliable with a problematic production, particularly when a shipment of 150 units to Japan caused interference with local television reception. Compared to the new polyphonic synthesizers being marketed at the same time, it was limited in scope and sound, and very few units were sold.
The final Rhodes produced by the original company was the Mk V in 1984. Among other improvements, it had a lighter plastic body and an improved action that varied the dynamics with each note. The Mark V is the easiest of the original Rhodes pianos for touring musicians to transport to gigs.
One of the key problems with production of Rhodes pianos under the original company was the desire to mass-produce the instrument, which caused a variation in quality. Collectors are advised to take care when buying a second-hand instrument.
After CBS.
In 1983, Rhodes was sold to CBS boss William Schultz, who closed down the main factory in 1985 and subsequently sold the business to Roland in 1987. Roland manufactured digital pianos under the Rhodes name, but Harold Rhodes disapproved of the instruments, which were made without his consultation or endorsement.
Rhodes subsequently re-acquired the rights to the instrument in 1997. However, by this time he was in ill health and died in December 2000. In 2007, a re-formed Rhodes Music Corporation introduced a reproduction of the original electric piano, called the Rhodes Mark 7. This was a version of the Rhodes housed in a molded plastic enclosure.
Dyno My Piano.
During the late 1970s and 1980s, Chuck Monte manufactured an after-market modification to the Rhodes, known as "Dyno My Piano". It included a lever that moved the relative position of the tines to the pickups, modifying the sound, and fed the output signal through additional electronics. This sound was emulated by the Yamaha DX7 with a patch (known as the DX7 Rhodes) that was popular during the 1980s, and caused several players to abandon the Rhodes in favor of the DX7.
Notable users.
The Doors' Ray Manzarek began using Rhodes instruments when the band was formed. He played bass parts on a Piano Bass with his left hand, while playing organ with his right.
The Rhodes piano became a popular instrument in jazz in the late 1960s, particularly for several sidemen who played with Miles Davis. Herbie Hancock first encountered the instrument in 1968 while booked for a session with Davis. He immediately became an enthusiast, noting that the amplification allowed him to be heard much more easily in groups when compared to the acoustic piano. Hancock continued to experiment with the Rhodes over the next few years, including playing it through a wah wah. Another former Davis sideman, Chick Corea started using the Rhodes prominently during the 1970s, as did Weather Report founder Joe Zawinul. Zawinul favored the sound of the Rhodes over the Wurlitzer, because it had a fuller and rich tone. From 1969's "In A Silent Way" and "Bitches Brew" onwards, the Rhodes became the most prominent keyboard on Davis' recordings until the mid-1970s. Vince Guaraldi started using a Rhodes in 1968, and toured with it alternating with an acoustic piano. He achieved particular prominence with his soundtrack music for the "Charlie Brown" series of films.
Billy Preston has been described as "The Ruler of the Rhodes" by "Music Radar" magazine, and played one during The Beatles' rooftop concert in 1969, and on the group's hit single "Get Back". Many of Stevie Wonder's recordings from the 1970s feature him playing the Rhodes, often alongside the Hohner Clavinet. Donny Hathaway regularly used the Rhodes. His hit single, This Christmas, which receives seasonal radio play on African American stations, makes a prominent use of the instrument. Although better known for playing the Wurlitzer, Ray Charles played a Rhodes on his performance of "Shake A Tailfeather" in the film "The Blues Brothers".
The French band Air make regular use of the Rhodes piano in their recordings.
References.
</dl>

</doc>
<doc id="43516" url="http://en.wikipedia.org/wiki?curid=43516" title="Sywell Aerodrome">
Sywell Aerodrome

Sywell Aerodrome (IATA: ORM, ICAO: EGBK) is the local aerodrome serving the town of Northampton, Wellingborough, Kettering and Rushden, as well as wider Northamptonshire. The aerodrome is located 5 NM northeast of Northampton and was originally opened in 1928 on the edge of Sywell village.
The aerodrome caters for private flying, flight training and corporate flights. There is one fixed-wing flying school, one microlight school and a helicopter school. The 1930s Art Deco hotel has bar and restaurant facilities. Aviation related industries and businesses are also located at the aerodrome.
Northampton (Sywell) Aerodrome has a CAA Ordinary Licence (Number P496) that allows flights for the public transport of passengers or for flying instruction as authorised by the licensee (Sywell Aerodrome Limited).
A viewing area is provided for aircraft spotters.
Second World War.
The aerodrome opened in 1928 and during the Second World War the aerodrome as RAF Sywell, was used as a training facility (Tiger Moths) and later an important centre for the repair of Wellington bombers and extensive sheds from this time still remain on the site.
Many aerial shots for the film "Battle of Britain" were taken over the airport and nearby area.
Expansion.
Since 1999, the aerodrome sought planning permission for a hard runway, which was intended to allow operations to continue over the winter, when the grass runways often become waterlogged. In February 2010, the final inspection of the newly completed all-weather hard runway was carried out by the CAA who confirmed that it could be licensed for use.
The organizations STARE (Stop The Aerodrome Runway Expansion) and CPRE (Campaign to Protect Rural England) campaigned against this change, arguing that it would lead to more and larger aircraft flying over the area and disturb its "rural tranquility".
Permission was granted for the runway on 22 November 2007 by the Department for Transport, and though campaigners vowed to fight the decision they were unsuccessful and construction of the runway began in 2008. It opened during summer 2009 and enabled safe operations during the winter of 2009/2010 and onwards.
Operations.
Sywell has three all-grass operational runways and a fourth all-weather concrete runway. The aerodrome's operational hours are 0900-1700 during winter and 0800-1700 during summer. The aerodrome offers an Aerodrome Flight Information Service to pilots.
Sywell Aviation Museum.
The aereodrome also houses the Sywell Aviation Museum dedicated to telling the wartime history of the site and the airmen that used it. The museum was celebrated with a visit a flypast by three North American P-51 Mustangs.
Brooklands Flying Club.
Brooklands Flying Club was based at Sywell, with a fleet of four Aero AT-3 aircraft and a Cessna 172. The club offered training for a Private Pilot's License (PPL), night rating and IMC rating.
The club opened in 2005 under the same name of the previous company that had been located on the airfield.
The flying school closed down at the end of 2013. 
Training.
Other flight training organisations on the airfield include:
The Blades.
The Blades aerobatic display team are based at Sywell, where their five Extra EA-300 aircraft are hangared. They perform aerobatic displays at major events around the country, in Europe, and in the Middle East.The team is the only one fully licensed by the CAA to carry paying 'passengers' in aerobatic displays, out of Sywell, or at many other locations.
Sywell Airshow.
The aerodrome now hosts a bi-annual charity airshow in aid of the local Air Ambulance where there are many classic aircraft flying and on display such as the Catalina, Mustang, North American Harvards.
Light Aircraft Association Rally.
The Light Aircraft Association (LAA - formerly the Popular Flying Association), is the UK's body for amateur aircraft construction, and recreational and sport flying. It used to hold its annual rally at Cranfield Airport, and then at Kemble Airport. In 2006, the LAA lost so much money through poor attendances resulting from poor weather that in 2007 and 2008, much smaller (and cheaper) "regional rallies" were held. These were unpopular and in September 2009 a revived (if cut-down) LAA Rally was held at Sywell. This proved successful, and as further well-attended rallies took place at Sywell in 2010, 2011 and 2012, the long-term future of the LAA Rally seems secure.
Music in Flight.
Music in Flight is an airshow held at Sywell, in which an orchestra plays classical music to accompany flying aircraft, hot air balloons, the Red Devils parachute display team and a fireworks display.
Awards.
In 2009, Sywell was awarded the Best General Aviation Airport 2009 airport member award by the Airport Operators Association (AOA). The award was determined by the following bodies; the British Air Transport Association (BATA), the Airport Owners and Pilots Association (AOPA) and the British Business and General Aviation Association (BBGA).
Business park.
An industrial area in the complex accommodates firms, agencies and other commercial businesses.

</doc>
<doc id="43519" url="http://en.wikipedia.org/wiki?curid=43519" title="List of hypothetical Solar System objects">
List of hypothetical Solar System objects

A hypothetical Solar System object is a planet, natural satellite or similar body in our Solar System whose existence is not known, but has been inferred from observational scientific evidence. Over the years a number of hypothetical planets have been proposed, and many have been disproved. However, even today there is scientific speculation about the possibility of planets yet unknown that may exist beyond the range of our current knowledge.

</doc>
<doc id="43521" url="http://en.wikipedia.org/wiki?curid=43521" title="Three Little Pigs">
Three Little Pigs

The Three Little Pigs is a fable/fairy tale featuring anthropomorphic pigs who build three houses of different materials. A big bad wolf is able to blow down the first two pigs' houses, made of straw and sticks respectively, but is unable to destroy the third pig's house, made of bricks. Printed versions date back to the 1840s, but the story itself is thought to be much older. (The story bears an at least superficial resemblance to "The Wolf and the Seven Young Kids," a German folktale collected as the fifth tale in Grimms' Fairy Tales in 1812.) The phrases used in the story, and the various morals that can be drawn from it, have become embedded in Western culture.
It is a type 124 folktale in the Aarne–Thompson classification system.
Traditional versions.
The Three Little Pigs was included in "The Nursery Rhymes of England" (London and New York, c.1886), by James Orchard Halliwell-Phillipps. The story in its arguably best-known form appeared in "English Fairy Tales" by Joseph Jacobs, first published in 1890 and crediting Halliwell as his source.
The story begins with the title characters being sent out into the world by their mother, to "seek out their fortune". The first little pig builds a house of straw, but a wolf blows it down and eats him. The second pig builds a house of furze sticks, which the wolf also blows down and eats him. Each exchange between wolf and pig features ringing proverbial phrases, namely:
"Little pig, little Pig, let me come in.."
"No, no, not by the hair on my chinny chin chin."
"Then I'll huff, and I'll puff, and I'll blow your house in.."
The third pig builds a house of bricks. The wolf fails to blow down the house. He then attempts to trick the pig out of the house by asking to meet him at various places, but he is outwitted each time. Finally, the wolf resolves to come down the chimney, whereupon the pig catches the wolf in a cauldron of boiling water, slams the lid on, then cooks and eats him. In another version the first and second little pigs run to their brother's house and after the wolf goes down the chimney he runs away and never goes back to eat the three little pigs, who all survive.
The story uses the literary rule of three, expressed in this case as a "contrasting three", as the third pig's brick house turns out to be the only one which is adequate to withstand the wolf.
Variations of the tale appeared in "Uncle Remus: His Songs and Sayings" in 1881. The story also made an appearance in "Nights with Uncle Remus" in 1883, both by Joel Chandler Harris, in which the pigs were replaced by Brer Rabbit. Andrew Lang included it in "The Green Fairy Book", published in 1892, but did not cite his source. In contrast to Jacobs's version, which left the pigs nameless, Lang's retelling cast the pigs as Browny, Whitey, and Blacky. It also set itself apart by exploring each pig's character and detailing interaction between them. The antagonist of this version is a fox, not a wolf. The pig's house is made either of mud, cabbage, or brick. There Blacky, the third pig, rescues his brother and sister from the fox's den after killing the fox.
Later adaptations.
Disney cartoon.
The most well-known version of the story is an award-winning 1933 "Silly Symphony" cartoon, which was produced by Walt Disney. The production cast the title characters as "Fifer Pig", "Fiddler Pig", and "Practical Pig". The first two are depicted as both frivolous and arrogant. The story has been somewhat softened. The first two pigs still get their houses blown down, but escape from the wolf. Also, the wolf is not boiled to death but simply burns his behind and runs away. Three sequels soon followed in 1934, 1936 and 1939 respectively.
Fifer Pig, Fiddler Pig, Practical Pig and the Big Bad Wolf appeared in the 2001 series "Disney's House of Mouse" in many episodes, and again in "". The three pigs can be seen in Walt Disney Parks and Resorts as greetable characters.
Warner Brothers Versions.
In 1942, there was a Merrie Melodies version made that was a serious musical treatment, plus the usual Friz Freleng visual humor. It parodies both the Disney version, and Fantasia itself.
Other versions of the tale were also made. One was an MGM Tex Avery cartoon named Blitz Wolf, a 1942 wartime version with the Wolf as a Nazi. Another animated spoof was a 1952 Warner Brothers cartoon called The Turn-Tale Wolf, directed by Robert McKimson. This cartoon tells the story from the wolf's point of view and makes the pigs out to be the villains. Another Warner Brothers spoof was Friz Freleng's The Three Little Bops (1957), which depicts the three little pigs as jazz musicians who refuse to let the wolf join their band.
Subsequent retellings.
In 1953, Tex Avery did a Droopy cartoon, "The Three Little Pups". In it, the wolf is a Southern-speaking dog catcher (voiced by Daws Butler) trying to catch Droopy and his brothers, Snoopy and Loopy, to put in the dog pound. Though first successful in blowing the first two houses down, he meets his match when he fails to blow Droopy's house of bricks. The dog catcher makes several failed attempts to destroy the house and catch the pups. His last attempt ended with him inside the television set where he is a cowboy.
In 1985, Faerie Tale Theatre recreated "The Three Little Pigs", starring Jeff Goldblum as The Wolf, and Billy Crystal, Stephen Furst, and Fred Willard as the pigs.
The 1989 parody, "The True Story of the Three Little Pigs", is presented as a first-person narrative by the wolf, who portrays the entire incident as a misunderstanding; he had gone to the pigs to borrow some sugar, had destroyed their houses in a sneezing fit, ate the first two pigs to not waste food (since they'd died in the house collapse anyway), and was caught attacking the third pig's house after the pig had continually insulted him.
The 1992 Green Jellÿ song, "Three Little Pigs" (and its claymation music video) sets the story in Los Angeles. The wolf drives a Harley Davidson motorcycle, the first little pig is an aspiring guitarist, the second is a cannabis smoking, dumpster diving evangelist and the third holds a Master of Architecture degree from Harvard University. In the end, with all three pigs barricaded in the brick house, the third pig calls 9-1-1. John Rambo is dispatched to the scene, and kills the wolf with a machine gun.
The 1993 children's book "The Three Little Wolves and the Big Bad Pig" inverts the cast and makes a few changes to the plot: the wolves build a brick house, then a concrete house, then a steel house, and finally a house of flowers. The pig is unable to blow the houses down, destroying them by other means, but eventually gives up his wicked ways when he smells the scent of the flower house, and becomes friends with the wolves.
In an advert for the British newspaper "The Guardian", the aftermath of the Three Little Pigs tale is told in the style of modern news media coverage, including social media reaction and the sociopolitical consequences of the story. This retelling eventually reveals the pigs to be attempting insurance fraud by blaming the wolf, who had asthma, for blowing their houses down.
The story is present in a 2014 commercial for Symbicort asthma medicine.
Musical.
In 2003 the Flemish company Studio 100 created a musical called "Three Little Pigs" (Dutch: "De 3 Biggetjes"), which follows the three daughters of the pig with the house of stone with new original songs, introducing a completely new story loosely based on the original story. The musical was specially written for the band K3, who play the three little pigs, Pirky, Parky and Porky (Dutch: Knirri, Knarri and Knorri).
In 2014 Peter Lund let the three little pigs live together in a village in the musical Grimm with Little Red Riding Hood and other fairy tale characters.
In "Shrek".
The three pigs and the wolf appear in the four "Shrek" films, and the specials "Shrek the Halls" and "Scared Shrekless".
In the book and first film, they are among the many fairy tale creatures banished to Shrek's swamp by Lord Farquaad. The pigs have German accents.
They have a larger role in "Shrek 2". They are friends with Shrek before the events of the film. In the beginning, they housesit for Shrek and Fiona while they visit the Kingdom of Far Far Away. Later, they see Shrek, Donkey and Puss in Boots arrested in Far Far Away and free them, then head to the castle to stop Prince Charming. They stop The Fairy Godmother's scheme and celebrate. They sing a song together, "Far Far Away Idol".
They are less prominent in "Shrek the Third". They appear at the start, heckling Prince Charming (now a dinner theatre actor) off the stage. They appear in the castle, pretending to have tea with the others, while the heroes find Artie, the kingdom's heir apparent. Under pressure, one pig reveals Shrek's plan to Prince Charming and the four are imprisoned. They are later freed and head to the castle to stop Prince Charming from killing Shrek in his show.
In "Shrek Forever After", they are at the ogre triplets' birthday party, and eat the occasion cake. In the alternate universe, they are attendants to Fifi in Rumpelstiltskin's castle.

</doc>
<doc id="43523" url="http://en.wikipedia.org/wiki?curid=43523" title="Khmer">
Khmer

Khmer may refer to:
Political terms coined by Norodom Sihanouk based on the word 'Khmer': 

</doc>
<doc id="43527" url="http://en.wikipedia.org/wiki?curid=43527" title="Mon language">
Mon language

The Mon language (Mon: ဘာသာ မန်; Burmese: မွန်ဘာသာ) is an Austroasiatic language spoken by the Mon people, who live in Burma (Myanmar) and Thailand. Mon, like the related Khmer language—but unlike most languages in Mainland Southeast Asia—is not tonal. Mon is spoken by more than a million people today. In recent years, usage of Mon has declined rapidly, especially among the younger generation. Many ethnic Mon are monolingual in Burmese. In Burma, the majority of speakers live in Mon State, followed by Tanintharyi Region and Kayin State.
The Mon script is derived from the Indian Brahmi script and is used to write the Burmese language.
History.
Mon is an important language in Burmese history. Up until the 12th century AD, it was the lingua franca of the Irrawaddy valley—not only in the Mon kingdoms of the lower Irrawaddy valley but also of the upriver Pagan Kingdom (Bagan) of the Bamar people. Mon, especially written Mon, continued to be the primary language even after the fall of the Mon kingdom of Thaton to Pagan in 1057. Pagan king Kyansittha (r. 1084–1113) admired Mon culture and the Mon language was patronized. The Mon script was adopted for Burmese during his reign.
Kyanzittha left many inscriptions in Mon. During this period, the Myazedi inscription, which contains identical inscriptions of a story in Pali, Pyu, Mon and Burmese on the four sides, was carved.
However, after Kyansittha's death, usage of the Mon language declined among the Bamar and the Burmese language began to replace Mon and Pyu as a lingua franca.
Mon inscriptions from Dvaravati's ruins also litter Thailand. However it is not clear if the inhabitants were Mon, a mix of Mon and Malay or Khmer. Later inscriptions and kingdoms like Lavo were subservient to the Khmer Empire.
After the fall of Pagan, the Mon language again became the lingua franca of the Mon Hanthawaddy Kingdom (1287–1539) in present-day Lower Burma. The language long continued to be prevalent in Lower Burma until the mid-19th century because the region was still mainly populated by Mon. This changed after the British captured Lower Burma in 1852, and encouraged immigration to develop Irrawaddy Delta for farming. The ensuing mass migration of peoples into the region from other areas of Burma as well as India and China relegated the Mon language to a tertiary status.
The language languished during British colonial rule, and has experienced a rapid decline in the number of speakers since the Burmese independence in 1948. Currently, according to scholars, the number of Mon speakers is relatively very small when compared to the large numbers who identify themselves as Mon people. With little or no support from successive Burmese governments, the Mon language (especially written Mon) continues to be propagated mostly by Mon monks. The Mon language instruction survives in the Thai-Burmese border inside the Mon rebel controlled areas.
In 2013, it was announced that the Than Lwin Times would begin to carry news in the Mon language, becoming Myanmar's first Mon language publication since 1962.
Dialects.
Mon has three primary dialects in Burma, coming from the various regions the Mon inhabit. They are the Central (areas surrounding Mottama and Mawlamyine), Bago, and Ye dialects. All are mutually intelligible. Thai Mon has some differences from the Burmese dialects of Mon, but they are mutually intelligible.
Alphabet.
The Old Mon script, which has been dated to the 6th century, with the earliest inscriptions found in Nakhon Pathom and Saraburi (in Thailand), is ancestral to the Burmese alphabet, which has been adapted to modern Mon. The modern Mon alphabet, however, utilizes several letters and diacritics that do not exist in Burmese, such as the stacking diacritic for medial 'l', which is placed underneath the letter.
There is a great deal of discrepancy between the written and spoken forms of Mon, with a single pronunciation capable of having several spellings. The Mon script also makes prominent use of consonant stacking, to represent consonant clusters found in the language.
The Mon alphabet contains 35 consonants (including a null consonant), as follows, with consonants belonging to the breathy register indicated in gray:
In the Mon script, consonants belong to one of two registers: clear and breathy, each of which has different inherent vowels and pronunciations for the same set of diacritics. For instance, က, which belongs to the clear register, is pronounced /kaˀ/, while ဂ is pronounced /kɛ̀ˀ/, to accommodate the vowel complexity of the Mon phonology. The addition of diacritics makes this obvious. Whereas in Burmese, spellings with the same diacritics are rhyming, in Mon, this depends on the consonant's inherent register. A few examples are listed below:
Mon uses the same diacritics and diacritic combinations as in Burmese to represent vowels, with the addition of a few diacritics unique to the Mon script, including ဴ (/ɛ̀a/), and ဳ (/i/), since the diacritic ိ represents /ìˀ/. Also, ဨ (/e/) is used instead of ဧ, as in Burmese.
The Mon language has 8 medials, as follows: ္ၚ (/-ŋ-/), ၞ (/-n-/), ၟ (/-m-/), ျ (/-j-/), ြ (/-r-/), ၠ (/-l-/), ွ (/-w-/), and ှ (/-hn-/). Consonantal finals are indicated with a virama (်), as in Burmese. Furthermore, consonant stacking is possible in Mon spellings, particularly for Pali and Sanskrit-derived vocabulary.
Phonology.
Consonants.
1/ç/ is only found in Burmese loans.
Vocalic register.
Unlike the surrounding Burmese and Thai languages, Mon is not a tonal language. As in many Mon–Khmer languages, Mon uses a vowel-phonation or vowel-register system in which the quality of voice in pronouncing the vowel is phonemic. There are two registers in Mon:
One study involving speakers of a Mon dialect in Thailand found that in some syllabic environments, words with a breathy voice vowel are significantly lower in pitch than similar words with a clear vowel counterpart. While difference in pitch in certain environments was found to be significant, there are no minimal pairs that are distinguished solely by pitch. The contrastive mechanism is the vowel phonation.
In the examples below, breathy voice is marked with a grave accent.
Syntax.
Verbs and verb phrases.
Mon verbs do not inflect for person. Tense is shown through particles.
Some verbs have a morphological causative, which is most frequently a /pə-/ prefix (Pan Hla 1989:29):
Nouns and noun phrases.
Singular and Plural.
Mon nouns do not inflect for number. That is, they do not have separate forms for singular and plural:
'one apple'
'two apples'
Adjectives.
Adjectives follow the noun (Pan Hla p. 24):
'beautiful woman'
Demonstratives.
Demonstratives follow the noun:
Classifiers.
Like many other Southeast Asian languages, Mon has classifiers which are used when a noun appears with a numeral. The choice of classifier depends on the semantics of the noun involved.
'one pen'
'one tree'
Prepositions and prepositional phrases.
Mon is a prepositional language.
Sentences.
The ordinary word order for sentences in Mon is subject–verb–object, as in the following examples
'I bought rice.'
'They taught me English.'
Questions.
Yes-no questions are shown with a final particle "ha"
‘Have you eaten rice?’
‘Will father go?’ (Pan Hla, p. 42)
Wh-questions show a different final particle, "rau". The interrogative word does not undergo wh-movement. That is, it does not necessarily move to the front of the sentence:
'What did Tala Ong wash?'
External links.
</dl>

</doc>
<doc id="43530" url="http://en.wikipedia.org/wiki?curid=43530" title="Schist">
Schist

Schist is a medium-grade metamorphic rock with medium to large, flat, sheet-like grains in a preferred orientation (nearby grains are roughly parallel). It is defined by having more than 50% platy and elongated minerals, often finely interleaved with quartz and feldspar. These lamellar (flat, planar) minerals include micas, chlorite, talc, hornblende, graphite, and others. Quartz often occurs in drawn-out grains to such an extent that a particular form called quartz schist is produced. Schist is often garnetiferous. Schist forms at a higher temperature and has larger grains than phyllite. Geological foliation (metamorphic arrangement in layers) with medium to large grained flakes in a preferred sheetlike orientation is called "schistosity". 
The names of various schists are derived from their mineral constituents. For example, schists rich in mica are called mica schists and include biotite or muscovite. Most schists are mica schists, but graphite and chlorite schists are also common. Schists are also named for their prominent or perhaps unusual mineral constituents, as in the case of garnet schist, tourmaline schist, and glaucophane schist.
The individual mineral grains in schist, drawn out into flaky scales by heat and pressure, can be seen with the naked eye. Schist is characteristically "foliated", meaning that the individual mineral grains split off easily into flakes or slabs. The word schist is derived ultimately from the Greek word "σχίζειν schízein" meaning "to split", which is a reference to the ease with which schists can be split along the plane in which the platy minerals lie.
Most schists are derived from clays and muds that have passed through a series of metamorphic processes involving the production of shales, slates and phyllites as intermediate steps. Certain schists are derived from fine-grained igneous rocks such as basalts and tuffs. 
Schists are frequently used as dimension stone, which stone that has been selected and fabricated to specific shapes or sizes.
Historical mining terminology.
Before the mid-18th century, the terms slate, shale and schist were not sharply differentiated by those involved with mining. In the context of underground coal mining, shale was frequently referred to as slate well into the 20th century.
Formation.
During metamorphism, rocks which were originally sedimentary, igneous or metamorphic are converted into schists and gneisses. If the composition of the rocks was originally similar, they may be very difficult to distinguish from one another if the metamorphism has been great. A quartz-porphyry, for example, and a fine grained feldspathic sandstone, may both be converted into a grey or pink mica-schist. Usually, however, it is possible to distinguish between sedimentary and igneous schists and gneisses. If, for example, the whole district occupied by these rocks has traces of bedding, clastic structure, or unconformability, then it may be a sign that the original rock was sedimentary. In other cases intrusive junctions, chilled edges, contact alteration or porphyritic structure may prove that in its original condition a metamorphic gneiss was an igneous rock. The last appeal is often to the chemistry, for there are certain rock types which occur only as sediments, while others are found only among igneous masses, and however advanced the metamorphism may be, it rarely modifies the chemical composition of the mass very greatly. Such rocks as limestones, dolomites, quartzites and aluminous shales have very definite chemical characteristics which distinguish them even when completely recrystallized.
The schists are classified principally according to the minerals they consist of and on their chemical composition. For example, many metamorphic limestones, marbles, and calc-schists, with crystalline dolomites, contain silicate minerals such as mica, tremolite, diopside, scapolite, quartz and feldspar. They are derived from calcareous sediments of different degrees of purity. Another group is rich in quartz (quartzites, quartz schists and quartzose gneisses), with variable amounts of white and black mica, garnet, feldspar, zoisite and hornblende. These were once sandstones and arenaceous rocks. The graphitic schists may readily be believed to represent sediments once containing coal or plant remains; there are also schistose ironstones (hematite-schists), but metamorphic beds of salt or gypsum are exceedingly uncommon. Among schists of igneous origin there are the silky calc-schists, the foliated serpentines (once ultramafic masses rich in olivine), and the white mica-schists, porphyroids and banded halleflintas, which have been derived from rhyolites, quartz-porphyries and felsic tuffs. The majority of mica-schists, however, are altered claystones and shales, and pass into the normal sedimentary rocks through various types of phyllite and mica-slates. They are among the most common metamorphic rocks; some of them are graphitic and others calcareous. The diversity in appearance and composition is very great, but they form a well-defined group not difficult to recognize, from the abundance of black and white micas and their thin, foliated, schistose character. A subgroup is the andalusite-, staurolite-, kyanite- and sillimanite-schists which usually make their appearance in the vicinity of gneissose granites, and have presumably been affected by contact metamorphism.
Engineering considerations.
In geotechnical engineering a schistosity plane often forms a discontinuity that may have a large influence on the mechanical behavior (strength, deformation, etc.) of rock masses in, for example, tunnel, foundation, or slope construction.

</doc>
<doc id="43532" url="http://en.wikipedia.org/wiki?curid=43532" title="Uraninite">
Uraninite

Uraninite is a radioactive, uranium-rich mineral and ore with a chemical composition that is largely UO2, but due to oxidation the mineral typically contains variable proportions of U3O8. Additionally, due to radioactive decay, the ore also contains oxides of lead and trace amounts of helium. It may also contain thorium, and rare earth elements. It used to be known as pitchblende (from "pitch", because of its black color, and "blende", a term used by German miners to denote minerals whose density suggested metal content, but whose exploitation, at the time they were named, was either unknown, impossible or not economically feasible). The mineral has been known at least since the 15th century from silver mines in the Ore Mountains, on the German/Czech border. The type locality is the town of Jáchymov, on the Czech side of the mountains, where F.E. Brückmann described the mineral in 1772. Pitchblende from the Johanngeorgenstadt deposit in Germany was used by M. Klaproth in 1789 to discover the element uranium.
All uraninite minerals contain a small amount of radium as a radioactive decay product of uranium. Uraninite also always contains small amounts of the lead isotopes 206Pb and 207Pb, the end products of the decay series of the uranium isotopes 238U and 235U respectively. Small amounts of helium are also present in uraninite as a result of alpha decay. Helium was first found on Earth in uraninite after having been discovered spectroscopically in the Sun's atmosphere. The extremely rare elements technetium and promethium can be found in uraninite in very small quantities (about 200 pg/kg and 4 fg/kg respectively), produced by the spontaneous fission of uranium-238.
Occurrence.
Uraninite is a major ore of uranium. Some of the highest grade uranium ores in the world were found in the Shinkolobwe mine in the Democratic Republic of the Congo (the initial source for the Manhattan Project) and in the Athabasca Basin in northern Saskatchewan, Canada. Another important source of pitchblende is at Great Bear Lake in the Northwest Territories of Canada, where it is found in large quantities associated with silver. It also occurs in Australia, the Czech Republic, Germany, England, Rwanda and South Africa. In the United States it can be found in the states of Arizona, Colorado, Connecticut, Maine, New Hampshire, New Mexico, North Carolina and Wyoming. The geologist Charles Steen made a fortune on the production of Uraninite in his Mi Vida mine in Moab, Utah.
Uranium ore is generally processed close to the mine into yellowcake, which is an intermediate step in the processing of uranium.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="43533" url="http://en.wikipedia.org/wiki?curid=43533" title="Hornblende">
Hornblende

Hornblende is a complex inosilicate series of minerals (ferrohornblende – magnesiohornblende). It is not a recognized mineral in its own right, but the name is used as a general or field term, to refer to a dark amphibole.
Hornblende is an isomorphous mixture of three molecules; a calcium-iron-magnesium silicate, an aluminium-iron-magnesium silicate, and an iron-magnesium silicate. 
The general formula can be given as (Ca,Na)2–3(Mg,Fe,Al)5(Al,Si)8O22(OH,F)2.
Compositional variances.
Some metals vary in their occurrence and magnitude:
Physical properties.
Hornblende has a hardness of 5–6, a specific gravity of 2.9–3.4 and is typically an opaque green, greenish-brown, brown or black color.
Its cleavage angles are at 56 and 124 degrees. It is most often confused with various pyroxene minerals and biotite mica, which are black and can be found in granite and in charnockite.
Occurrence.
Hornblende is a common constituent of many igneous and metamorphic rocks such as granite, syenite, diorite, gabbro, basalt, andesite, gneiss, and schist.
It is the principal mineral of amphibolites. Very dark brown to black hornblendes that contain titanium are ordinarily called basaltic hornblende, from the fact that they are usually a constituent of basalt and related rocks. Hornblende alters easily to chlorite and epidote.
A rare variety of hornblende contains less than 5% of iron oxide, is gray to white in color, and named edenite, from its locality in Edenville, Orange County, New York.
Other minerals in the hornblende series include:
Etymology.
The word hornblende is derived from the German "horn" and "blenden", to 'deceive' in allusion to its similarity in appearance to metal-bearing ore minerals.

</doc>
<doc id="43534" url="http://en.wikipedia.org/wiki?curid=43534" title="Basalt">
Basalt

Basalt (, , , or )
is a common extrusive igneous (volcanic) rock formed from the rapid cooling of basaltic lava exposed at or very near the surface of a planet or moon. Flood basalt describes the formation in a series of lava basalt flows. 
Definition.
By definition, basalt is an aphanitic igneous rock with less than 20% quartz and less than 10% feldspathoid by volume, and where at least 65% of the feldspar is in the form of plagioclase. Basalt features a glassy matrix interspersed with minerals. The average density is 3.0 gm/cm3.
Basalt is defined by its mineral content and texture, and physical descriptions without mineralogical context may be unreliable in some circumstances. Basalt is usually grey to black in colour, but rapidly weathers to brown or rust-red due to oxidation of its mafic (iron-rich) minerals into rust. Although usually characterized as "dark", basaltic rocks exhibit a wide range of shading due to regional geochemical processes. Due to weathering or high concentrations of plagioclase, some basalts are quite light coloured, superficially resembling rhyolite to untrained eyes. Basalt has a fine-grained mineral texture due to the molten rock cooling too quickly for large mineral crystals to grow, although it is often porphyritic, containing the larger crystals formed prior to the extrusion that brought the lava to the surface, embedded in a finer-grained matrix.
Basalt with a vesicular or frothy texture is called scoria, and forms when dissolved gases are forced out of solution and form vesicles as the lava decompresses as it reaches the surface.
The term basalt is at times applied to shallow intrusive rocks with a composition typical of basalt, but rocks of this composition with a phaneritic (coarse) groundmass are generally referred to as diabase (also called dolerite) or gabbro.
In the Hadean and Archean (and the early Precambrian) eras of Earth's history the chemistry of erupted basalts was significantly different from today's, due to crustal and asthenosphere differentiation issues—so much so that there is an alternate (but less well known) name for this kind of basalt.
Etymology.
The word "basalt" is ultimately derived from Late Latin "basaltes," misspelling of L. "basanites" "very hard stone," which was imported from Ancient Greek βασανίτης (basanites), from βάσανος (basanos, "touchstone") and originated in Egyptian "bauhun" "slate". The modern petrological term "basalt" describing a particular composition of lava-derived rock originates from its use by Georgius Agricola in 1556 in his famous work of mining and mineralogy "De re metallica, libri XII". Agricola applied "basalt" to the volcanic black rock of the Schloßberg (local castle hill) at Stolpen, believing it to be the same as the "very hard stone" described by Pliny the Elder in "Naturalis Historiae".
Occurrence.
On Earth, most basalt magmas have formed by decompression melting of the mantle. Basalt commonly erupts on Io, the third largest moon of Jupiter, and has also formed on Earth's Moon, Mars, Venus, and the asteroid Vesta.
The crustal portions of oceanic tectonic plates are composed predominantly of basalt, produced from upwelling mantle below, the ocean ridges.
Uses.
Basalt is used in construction (e.g. as building blocks or in the groundwork), making cobblestones (from columnar basalt) and in making statues. Heating and extruding basalt yields stone wool, said to be an excellent thermal insulator.
Petrology.
The mineralogy of basalt is characterized by a preponderance of calcic plagioclase feldspar and pyroxene. Olivine can also be a significant constituent. Accessory minerals present in relatively minor amounts include iron oxides and iron-titanium oxides, such as magnetite, ulvospinel, and ilmenite. Because of the presence of such oxide minerals, basalt can acquire strong magnetic signatures as it cools, and paleomagnetic studies have made extensive use of basalt.
In tholeiitic basalt, pyroxene (augite and orthopyroxene or pigeonite) and calcium-rich plagioclase are common phenocryst minerals. Olivine may also be a phenocryst, and when present, may have rims of pigeonite. The groundmass contains interstitial quartz or tridymite or cristobalite. "Olivine tholeiite" has augite and orthopyroxene or pigeonite with abundant olivine, but olivine may have rims of pyroxene and is unlikely to be present in the groundmass.
Alkali basalts typically have mineral assemblages that lack orthopyroxene but contain olivine. Feldspar phenocrysts typically are labradorite to andesine in composition. Augite is rich in titanium compared to augite in tholeiitic basalt. Minerals such as alkali feldspar, leucite, nepheline, sodalite, phlogopite mica, and apatite may be present in the groundmass.
Basalt has high liquidus and solidus temperatures—values at the Earth's surface are near or above 1200 °C (liquidus) and near or below 1000 °C (solidus); these values are higher than those of other common igneous rocks.
The majority of tholeiites are formed at approximately 50–100 km depth within the mantle. Many alkali basalts may be formed at greater depths, perhaps as deep as 150–200 km. The origin of high-alumina basalt continues to be controversial, with interpretations that it is a primary melt and that instead it is derived from other basalt types (e.g., Ozerov, 2000).
Geochemistry.
Relative to most common igneous rocks, basalt compositions are rich in MgO and CaO and low in SiO2 and the alkali oxides, i.e., Na2O + K2O, consistent with the TAS classification.
Basalt generally has a composition of 45–55 wt% SiO2, 2–6 wt% total alkalis, 0.5–2.0 wt% TiO2, 5–14 wt% FeO and 14 wt% or more Al2O3. Contents of CaO are commonly near 10 wt%, those of MgO commonly in the range 5 to 12 wt%.
High-alumina basalts have aluminium contents of 17–19 wt% Al2O3; boninites have magnesium contents of up to 15 percent MgO. Rare feldspathoid-rich mafic rocks, akin to alkali basalts, may have Na2O + K2O contents of 12% or more.
The abundances of the lanthanide or rare-earth elements (REE) can be a useful diagnostic tool to help explain the history of mineral crystallisation as the melt cooled. In particular, the relative abundance of europium compared to the other REE is often markedly higher or lower, and called the europium anomaly. It arises because Eu2+ can substitute for Ca2+ in plagioclase feldspar, unlike any of the other lanthanides, which tend to only form 3+ cations.
MORB basalts and their intrusive equivalents, gabbros, are the characteristic igneous rocks formed at mid-ocean ridges. They are tholeiites particularly low in total alkalis and in incompatible trace elements, and they have relatively flat REE patterns normalized to mantle or chondrite values. In contrast, alkali basalts have normalized patterns highly enriched in the light REE, and with greater abundances of the REE and of other incompatible elements. Because MORB basalt is considered a key to understanding plate tectonics, its compositions have been much studied. Although MORB compositions are distinctive relative to average compositions of basalts erupted in other environments, they are not uniform. For instance, compositions change with position along the Mid-Atlantic ridge, and the compositions also define different ranges in different ocean basins (Hofmann, 2003).
Isotope ratios of elements such as strontium, neodymium, lead, hafnium, and osmium in basalts have been much studied to learn about the evolution of the Earth's mantle. Isotopic ratios of noble gases, such as 3He/4He, are also of great value: for instance, ratios for basalts range from 6 to 10 for mid-ocean ridge tholeiite (normalized to atmospheric values), but to 15-24+ for ocean island basalts thought to be derived from mantle plumes.
Source rocks for the partial melts probably include both peridotite and pyroxenite (e.g., Sobolev et al., 2007).
Morphology and textures.
The shape, structure and texture of a basalt is diagnostic of how and where it erupted—whether into the sea, in an explosive cinder eruption or as creeping pahoehoe lava flows, the classic image of Hawaiian basalt eruptions.
Subaerial eruptions.
Basalt which erupts under open air (that is, subaerially) forms three distinct types of lava or volcanic deposits: scoria; ash or cinder (breccia); and lava flows.
Basalt in the tops of subaerial lava flows and cinder cones will often be highly vesiculated, imparting a lightweight "frothy" texture to the rock. Basaltic cinders are often red, coloured by oxidized iron from weathered iron-rich minerals such as pyroxene.
ʻAʻā types of blocky, cinder and breccia flows of thick, viscous basaltic lava are common in Hawaii. Pāhoehoe is a highly fluid, hot form of basalt which tends to form thin aprons of molten lava which fill up hollows and sometimes forms lava lakes. Lava tubes are common features of pahoehoe eruptions.
Basaltic tuff or pyroclastic rocks are rare but not unknown. Usually basalt is too hot and fluid to build up sufficient pressure to form explosive lava eruptions but occasionally this will happen by trapping of the lava within the volcanic throat and buildup of volcanic gases. Hawaii's Mauna Loa volcano erupted in this way in the 19th century, as did Mount Tarawera, New Zealand in its violent 1886 eruption. Maar volcanoes are typical of small basalt tuffs, formed by explosive eruption of basalt through the crust, forming an apron of mixed basalt and wall rock breccia and a fan of basalt tuff further out from the volcano.
Amygdaloidal structure is common in relict vesicles and beautifully crystallized species of zeolites, quartz or calcite are frequently found.
Columnar basalt.
During the cooling of a thick lava flow, contractional joints or fractures form. If a flow cools relatively rapidly, significant contraction forces build up. While a flow can shrink in the vertical dimension without fracturing, it can't easily accommodate shrinking in the horizontal direction unless cracks form; the extensive fracture network that develops results in the formation of columns. The topology of the lateral shapes of these columns can broadly be classed as a random cellular network. These structures are predominantly hexagonal in cross-section, but polygons with three to twelve or more sides can be observed. The size of the columns depends loosely on the rate of cooling; very rapid cooling may result in very small (<1 cm diameter) columns, while slow cooling is more likely to produce large columns.
Submarine eruptions.
Pillow basalts.
When basalt erupts underwater or flows into the sea, contact with the water quenches the surface and the lava forms a distinctive "pillow" shape, through which the hot lava breaks to form another pillow. This "pillow" texture is very common in underwater basaltic flows and is diagnostic of an underwater eruption environment when found in ancient rocks. Pillows typically consist of a fine-grained core with a glassy crust and have radial jointing. The size of individual pillows varies from 10 cm up to several meters.
When "pahoehoe" lava enters the sea it usually forms pillow basalts. However when "a'a" enters the ocean it forms a littoral cone, a small cone-shaped accumulation of tuffaceous debris formed when the blocky "a'a" lava enters the water and explodes from built-up steam.
The island of Surtsey in the Atlantic Ocean is a basalt volcano which breached the ocean surface in 1963. The initial phase of Surtsey's eruption was highly explosive, as the magma was quite wet, causing the rock to be blown apart by the boiling steam to form a tuff and cinder cone. This has subsequently moved to a typical pahoehoe-type behaviour.
Volcanic glass may be present, particularly as rinds on rapidly chilled surfaces of lava flows, and is commonly (but not exclusively) associated with underwater eruptions.
Life on basaltic rocks.
The common corrosion features of underwater volcanic basalt suggest that microbial activity may play a significant role in the chemical exchange between basaltic rocks and seawater. The significant amounts of reduced iron, Fe(II), and manganese, Mn(II), present in basaltic rocks provide potential energy sources for bacteria. Recent research has shown that some Fe(II)-oxidizing bacteria cultured from iron-sulfide surfaces are also able to grow with basaltic rock as a source of Fe(II). In recent work at Loihi Seamount, Fe- and Mn- oxidizing bacteria have been cultured from weathered basalts. The impact of bacteria on altering the chemical composition of basaltic glass (and thus, the oceanic crust) and seawater suggest that these interactions may lead to an application of hydrothermal vents to the origin of life.
Distribution.
Basalt is one of the most common rock types in the world.
Basalt is the rock most typical of large igneous provinces. The largest occurrences of basalt are in the ocean floor that is almost completely made up by basalt. Above sea level basalt is common in hotspot islands and around volcanic arcs, specially those on thin crust. However, the largest volumes of basalt on land correspond to continental flood basalts. Continental flood basalts are known to exist in the Deccan Traps in India, the Chilcotin Group in British Columbia, Canada, the Paraná Traps in Brazil, the Siberian Traps in Russia, the Karoo flood basalt province in South Africa, the Columbia River Plateau of Washington and Oregon.
Many archipelagoes and island nations have an overwhelming majority of its exposed bedrock made up by basalt due to being above hotspots, for example, Iceland and Hawaii.
Ancient Precambrian basalts are usually only found in fold and thrust belts, and are often heavily metamorphosed. These are known as greenstone belts, because low-grade metamorphism of basalt produces chlorite, actinolite, epidote and other green minerals.
Lunar and Martian basalt.
The dark areas visible on Earth's moon, the lunar maria, are plains of flood basaltic lava flows. These rocks were sampled by the manned American Apollo program, the robotic Russian Luna program, and are represented among the lunar meteorites.
Lunar basalts differ from their terrestrial counterparts principally in their high iron contents, which typically range from about 17 to 22 wt% FeO. They also possess a stunning range of titanium concentrations (present in the mineral ilmenite), ranging from less than 1 wt% TiO2, to about 13 wt.%. Traditionally, lunar basalts have been classified according to their titanium content, with classes being named high-Ti, low-Ti, and very-low-Ti. Nevertheless, global geochemical maps of titanium obtained from the Clementine mission demonstrate that the lunar maria possess a continuum of titanium concentrations, and that the highest concentrations are the least abundant.
Lunar basalts show exotic textures and mineralogy, particularly shock metamorphism, lack of the oxidation typical of terrestrial basalts, and a complete lack of hydration. While most of the Moon's basalts erupted between about 3 and 3.5 billion years ago, the oldest samples are 4.2 billion years old, and the youngest flows, based on the age dating method of crater counting, are estimated to have erupted only 1.2 billion years ago.
Basalt is also a common rock on the surface of Mars, as determined by data sent back from the planet's surface and by Martian meteorites.
Alteration of basalt.
Metamorphism.
Basalts are important rocks within metamorphic belts, as they can provide vital information on the conditions of metamorphism within the belt. Various metamorphic facies are named after the mineral assemblages and rock types formed by subjecting basalts to the temperatures and pressures of the metamorphic event. These are:
Metamorphosed basalts are important hosts for a variety of hydrothermal ore deposits, including gold deposits, copper deposits, volcanogenic massive sulfide ore deposits and others.
Weathering.
Compared to other rocks found on Earth's surface, basalts weather relatively fast. The typically iron-rich minerals oxidise rapidly in water and air, staining the rock a brown to red colour due to iron oxide (rust). Chemical weathering also releases readily water-soluble cations such as calcium, sodium and magnesium, which give basaltic areas a strong buffer capacity against acidification. Calcium released by basalts binds up CO2 from the atmosphere forming CaCO3 acting thus as a CO2 trap. To this it must be added that the eruption of basalt itself is often associated with the release of large quantities of CO2 into the atmosphere from volcanic gases.
Carbon sequestration in basalt has been studied as a means of removing carbon dioxide, produced by human industrialization, from the atmosphere. Underwater basalt deposits, scattered in seas around the globe, have the added benefit of the water serving as a barrier to the re-release of CO2 into the atmosphere.
References.
</dl>

</doc>
<doc id="43536" url="http://en.wikipedia.org/wiki?curid=43536" title="Beeswax">
Beeswax

Beeswax is a natural wax produced by individual honey bees of the genus "Apis". The wax is formed into "scales" by eight wax-producing glands in the abdominal segments 4 through 7 of worker bees, who discard it in or at the hive. The hive workers collect and use it for comb structural stability, to form cells for honey-storage and larval and pupal comfort and protection within the bee hive. Chemically, beeswax consists of mainly esters of fatty acids and various long-chain alcohols.
Small amounts of beeswax have human food and flavoring applications, and are edible in the sense of having similar toxicity to undigestable plant waxes. However, the wax monoesters in beeswax are poorly hydrolysed in the guts of humans and other mammals, so are not considered as having a significant nutritional value. Some birds, such as honeyguides, can digest beeswax.
Production.
The wax is formed by worker bees, which secrete it from eight wax-producing mirror glands on the inner sides of the sternites (the ventral shield or plate of each segment of the body) on abdominal segments 4 to 7. The sizes of these wax glands depend on the age of the worker, and after many daily flights, these glands begin to gradually atrophy.
The new wax is initially glass-clear and colourless, becoming opaque after mastication and adulteration with pollen by the hive worker bees. Also, the wax becomes progressively more yellow or brown by incorporation of pollen oils and propolis. The wax scales are about 3 mm across and 0.1 mm thick, and about 1,100 are required to make a gram of wax.
Honey bees use the beeswax to build honeycomb cells in which their young are raised with honey and pollen cells being capped for storage. For the wax-making bees to secrete wax, the ambient temperature in the hive must be 33 to 36 °C (91 to 97 °F).
The amount of honey sacrificed to wax production is presently disputed. Current thinking suggests a correlation between the amount of honey used to produce its equivalent weight in wax and the amount of wax used to store its equivalent weight in honey. It is believed that by multiplying these figures together, that it should be possible to provide a figure for the amount of honey sacrificed to build storage comb and vice versa.
According to Whitcomb's 1946 experiment, 6.66 to 8.80 pounds of honey yields 1 pound of wax. Les Crowder's study of five Langstroth hives, which re-use comb after honey extraction, and five top bar hives, which extract honey by crushing the comb, concluded 75%-80% as much honey production and 600% as much beeswax production in the top bar hives, which suggest 24-30 pounds of wax per 1 pound of honey. These studies only measured honey production versus comb production; they did not account fully for bees' feeding in a closed environment.
Various sources specify anywhere from 20 to 400 pounds of honey stored per pound of wax. The book, "Beeswax Production, Harvesting, Processing and Products", suggests 1 pound beeswax to store 22 pounds honey.
Processing.
When beekeepers extract the honey, they cut off the wax caps from each honeycomb cell with an uncapping knife or machine. Its color varies from nearly white to brownish, but most often a shade of yellow, depending on purity and the type of flowers gathered by the bees. Wax from the brood comb of the honey bee hive tends to be darker than wax from the honeycomb. Impurities accumulate more quickly in the brood comb. Due to the impurities, the wax must be rendered before further use. The leftovers are called slumgum.
The wax may be clarified further by heating in water. As with petroleum waxes, it may be softened by dilution with mineral oil or vegetable oil to make it more workable at room temperature.
Physical characteristics.
Beeswax is a tough wax formed from a mixture of several compounds.
An approximate chemical formula for beeswax is C15H31COOC30H61. Its main components are palmitate, palmitoleate, and oleate esters of long-chain (30–32 carbons) aliphatic alcohols, with the ratio of triacontanyl palmitate CH3(CH2)29O-CO-(CH2)14CH3 to cerotic acid CH3(CH2)24COOH, the two principal components, being 6:1. Beeswax can be classified generally into European and Oriental types. The saponification value is lower (3–5) for European beeswax, and higher (8–9) for Oriental types.
Beeswax has a relatively low melting point range of 62 to 64 °C (144 to 147 °F). If beeswax is heated above 85 °C (185 °F) discoloration occurs. The flash point of beeswax is 204.4 °C (400 °F). Density at 15 °C is 958 to 970 kg/m³.
Natural beeswax: "When cold it is brittle; at ordinary temperatures it is tenacious; its fracture is dry and granular. The sp. gr. at 15° [59°F] is from 0.958 to 0.975, that of melted wax at 98°- 99° [208.4F° - 210.2F°] compared with water at 15.5° [59.9°F] is 0.822. It softens when held in the hand, and melts at 62°- 66° [143.6°F - 145.4°F]; it solidifies at 60.5° - 63° [140.9°F - 150.8°F]."
Uses.
Beeswax has many and varied uses. Primarily, it is used by the bees in making their honeycomb foundations.
Apart from this use by bees, the use of beeswax has become widespread and varied. Purified and bleached beeswax is used in the production of food, cosmetics, and pharmaceuticals. The three main types of beeswax products are yellow, white, and beeswax absolute. Yellow beeswax is the crude product obtained from the honeycomb, white beeswax is bleached yellow beeswax, and beeswax absolute is yellow beeswax treated with alcohol.
In food preparation, it is used as a coating for cheese; by sealing out the air, protection is given against spoilage (mold growth). Beeswax may also be used as a food additive E901, in small quantities acting as a glazing agent, which serves to prevent water loss, or used to provide surface protection for some fruits. Soft gelatin capsules and tablet coatings may also use E901. Beeswax is also a common ingredient of natural chewing gum.
Use of beeswax in skin care and cosmetics has been increasing. A German study found beeswax to be superior to similar barrier creams (usually mineral oil-based creams such as petroleum jelly), when used according to its protocol.
Beeswax is used in lip balm, lip gloss, hand creams, and moisturizers; and in cosmetics such as eye shadow, blush, and eye liner. Beeswax is an important ingredient in moustache wax and hair pomades, which make hair look sleek and shiny.
Candle-making has long involved the use of beeswax, which is highly flammable, and this material traditionally was prescribed for the making of the Paschal candle or "Easter candle". It is further recommended for the making of other candles used in the liturgy of the Roman Catholic Church. Beeswax is also the candle constituent of choice in the Orthodox Church.
From a relatively small production of about 10,000 tons a year, a number of different niches are served: beeswax is an ingredient in surgical bone wax, which is used during surgery to control bleeding from bone surfaces; shoe polish and furniture polish can both use beeswax as a component, dissolved in turpentine or sometimes blended with linseed oil or tung oil; modeling waxes can also use beeswax as a component; pure beeswax can also be used as an organic surfboard wax. Beeswax blended with pine rosin, can serve as an adhesive to attach reed plates to the structure inside a squeezebox. It can also be used to make Cutler's resin, an adhesive used to glue handles onto cutlery knives. It is used in Eastern Europe in egg decoration; it is used for writing, via resist dyeing, on batik eggs (as in pysanky) and for making beaded eggs.
Beeswax is used by percussionists to make a surface on tambourines for thumb rolls. It can also be used as a metal injection moulding binder component along with other polymeric binder materials.
Beeswax was formerly used in the manufacture of phonograph cylinders. It may still be used to seal formal legal or Royal decree and academic parchments such as placing an awarding stamp imprimatur of the university upon completion of post-graduate degrees.
Historical uses.
Beeswax was among the first plastics to be used, alongside other natural polymers such as gutta-percha, horn, tortoiseshell, and shellac. For thousands of years, beeswax has had a wide variety of applications; it has been found in the tombs of Egypt, in wrecked Viking ships, and in Roman ruins. Beeswax never goes bad and can be heated and reused. Historically, it has been used:

</doc>
<doc id="43537" url="http://en.wikipedia.org/wiki?curid=43537" title="Sacramento County, California">
Sacramento County, California

Sacramento County is a county in the U.S. state of California. As of the 2010 census, the population was 1,418,788. Its county seat is Sacramento, the state capital.
Sacramento County is included in the Sacramento-Roseville-Arden-Arcade, CA Metropolitan Statistical Area. The county covers about 994 sqmi in the northern portion of the Central Valley, on into Gold Country. Sacramento County extends from the low delta lands between the Sacramento River and San Joaquin River north to about ten miles (16 km) beyond the State Capitol and east into the foothills of the Sierra Nevada Mountains. The southernmost portion of Sacramento County has direct access to San Francisco Bay.
History.
Sacramento County was one of the original counties of California, which were created in 1850 at the time of statehood.
The county was named after the Sacramento River, which forms its western border. The river was named by Spanish cavalry officer Gabriel Moraga for the "Santisimo Sacramento" (Most Holy Sacrament), referring to the Catholic Eucharist.
Alexander Hamilton Willard, a member of the Lewis and Clark Expedition, is buried in the old Franklin Cemetery.
Geography.
According to the U.S. Census Bureau, the county has a total area of 994 sqmi, of which 965 sqmi is land and 29 sqmi (3.0%) is water. Most of the county is at an elevation close to sea level, with some areas below sea level. The highest point in the county is Carpenter Hill at 828 feet (252 m), in the southeast part of Folsom. Major watercourses in the county include the American River, Sacramento River and Dry Creek, a tributary of the Sacramento River.
Government and politics.
Government.
The Government of Sacramento County is defined and authorized under the California Constitution, California law, and the Charter of the County of Sacramento. Much of the Government of California is in practice the responsibility of county governments such as the Government of Sacramento County, while municipalities such as the city of Sacramento and Folsom provide additional, often non-essential services.
It is composed of the elected five-member Board of Supervisors, several other elected offices including the Sheriff, District Attorney, and Assessor, and numerous county departments and entities under the supervision of the County Executive Officer. In addition, several entities of the government of California have jurisdiction conterminous with Sacramento County, such as the Sacramento County Superior Court.
Under its foundational Charter, the five-member elected Sacramento County Board of Supervisors (BOS) is the county legislature. The board operates in a legislative, executive, and quasi-judicial capacity. The current members are:
The Sacramento County Code is the codified law of Sacramento County in the form of local ordinances passed by the Board of Supervisors.
Politics.
Overview.
Sacramento County is politically competitive in presidential elections, though marginally Democratic. Candidates from the Democratic Party have carried the county in the past six presidential elections, but have won a majority of the county's votes only twice during that time (in 2008 and 2012). The city of Sacramento is strongly Democratic, while rural areas are strongly Republican; suburban areas are more divided. This pattern is also present in congressional and state legislative elections. The last Republican presidential candidate to win a majority in the county was George H.W. Bush in 1988. Conversely, in gubernatorial elections the county often favors Republican candidates, doing so in 2002, 2003 and 2006 before supporting Democrat Jerry Brown in 2010.
In the House of Representatives, all of California's 7th congressional district and portions of its 3rd, 6th, and 9th districts are in the county.
In the State Assembly, all of the 7th and 8th districts and parts of the 6th, 9th, and 11th districts are in the county.
In the State Senate, parts of the 1st, 3rd, 4th, 6th, and 8th districts are in the county.
According to the California Secretary of State, as of October 22, 2012, Sacramento County has 698,899 registered voters, out of 944,243 eligible (74%). Of those, 306,960 (44%) are registered Democrats, 225,688 (32%) are registered Republicans, and 134,677 (19%) have declined to state a political party.
Transportation.
Public transportation.
Sacramento Regional Transit (RT) provides bus and light rail service in Sacramento and nearby communities like Rancho Cordova, Citrus Heights, and Rosemont. Sacramento hosts 37.4 miles (60.2 km) of light rail. The cities of Elk Grove, Folsom and Galt also operate their own bus lines. In addition, the transit agencies of the adjacent counties have routes operating into downtown Sacramento, or connecting with the light rail system.
Greyhound and Amtrak both serve Sacramento. The port of Sacramento ships 870,000 short tons (790,000 metric tons) of cargo annually.
Airports.
Sacramento International Airport is a major, full-service airport with passenger flights. It is owned by the County of Sacramento. The County also owns Sacramento Mather Airport in Rancho Cordova and Sacramento Executive Airport, both of which are general aviation airports. There are also privately owned public use airports located in Elk Grove and Rio Linda.
Public Roadways.
The maintains approximately 2200 miles of roadway within the unincorporated area. The roads range from six lane thoroughfares to rural roads.
Crime.
The following table includes the number of incidents reported and the rate per 1,000 persons for each type of offense.
Demographics.
2010.
The 2010 United States Census reported that Sacramento County had a population of 1,418,788. The racial makeup of Sacramento County was 815,151 (57.5%) White, 200,228 (15.4%) African American, 14,308 (1.0%) Native American, 203,211 (14.3%) Asian, 13,858 (1.0%) Pacific Islander, 131,691 (9.3%) from other races, and 93,511 (6.6%) from two or more races. Hispanic or Latino of any race were 306,196 persons (21.6%).
2000.
As of the census of 2000, there were 1,223,499 people, 453,602 households, and 297,562 families residing in the county. The population density was 1,267 people per square mile (489/km²). There were 474,814 housing units at an average density of 492/sq mi (190/km²). The racial makeup of the county was 64.0% White, 10.6% Black or African American, 1.09% Native American, 13.5% Asian, 0.6% Pacific Islander, 7.5% from other races, and 5.8% from two or more races. 19.3% of the population were Hispanic or Latino of any race. 10.2% were of German, 7.0% English, 6.7% Irish and 5.1% American ancestry according to Census 2000. 75.7% spoke only English at home; 10.0% spoke Spanish, 1.5% Hmong, 1.4% Chinese or Mandarin, 1.3% Vietnamese, 1.2% Tagalog and 1.2% Russian.
There were 453,602 households out of which 33.7% had children under the age of 18 living with them, 46.4% were married couples living together, 14.1% had a female householder with no husband present, and 34.4% were non-families. 26.7% of all households were made up of individuals and 8.0% had someone living alone who was 65 years of age or older. The average household size was 2.64 and the average family size was 3.24.
In the county the population was spread out with 27.6% under the age of 18, 9.5% from 18 to 24, 31.0% from 25 to 44, 20.9% from 45 to 64, and 11.1% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 95.9 males. For every 100 females age 18 and over, there were 92.5 males.
The median income for a household in the county was $43,816, and the median income for a family was $50,717. Males had a median income of $39,482 versus $31,569 for females. The per capita income for the county was $21,142. About 10.3% of families and 14.1% of the population were below the poverty line, including 20.2% of those under age 18 and 6.6% of those age 65 or over.

</doc>
<doc id="43551" url="http://en.wikipedia.org/wiki?curid=43551" title="Ruby">
Ruby

A ruby is a pink to blood-red colored gemstone, a variety of the mineral corundum (aluminium oxide). The red color is caused mainly by the presence of the element chromium. Its name comes from "ruber", Latin for red. Other varieties of gem-quality corundum are called sapphires. Ruby is considered one of the four precious stones, together with sapphire, emerald and diamond.
Prices of rubies are primarily determined by color. The brightest and most valuable "red" called blood-red, commands a large premium over other rubies of similar quality. After color follows clarity: similar to diamonds, a clear stone will command a premium, but a ruby without any needle-like rutile inclusions may indicate that the stone has been treated. Cut and carat (weight) are also an important factor in determining the price. Ruby is the traditional birthstone for July and is always lighter red or pink than garnet.
Physical properties.
Rubies have a hardness of 9.0 on the Mohs scale of mineral hardness. Among the natural gems only moissanite and diamond are harder, with diamond having a Mohs hardness of 10.0 and moissonite falling somewhere in between corundum (ruby) and diamond in hardness. Ruby is α-alumina (the most stable form of Al2O3) in which a small fraction of the aluminium3+ ions are replaced by chromium3+ ions. Each Cr3+ is surrounded octahedrally by six O2− ions. This crystallographic arrangement strongly affects each Cr3+, resulting in light absorption in the yellow-green region of the spectrum and thus in the red color of the gem. When yellow-green light is absorbed by Cr3+, it is re-emitted as red luminescence. This red emission adds to the red color perceived by the subtraction of green and violet light from white light, and adds luster to the gem's appearance. When the optical arrangement is such that the emission is stimulated by 694-nanometer photons reflecting back and forth between two mirrors, the emission grows strongly in intensity. This effect was used by Theodore Maiman in 1960 to make the first successful laser, based on ruby.
All natural rubies have imperfections in them, including color impurities and inclusions of rutile needles known as "silk". Gemologists use these needle inclusions found in natural rubies to distinguish them from synthetics, simulants, or substitutes. Usually the rough stone is heated before cutting. Almost all rubies today are treated in some form, with heat treatment being the most common practice. However, rubies that are completely untreated but still of excellent quality command a large premium.
Some rubies show a three-point or six-point asterism or "star". These rubies are cut into cabochons to display the effect properly. Asterisms are best visible with a single-light source, and move across the stone as the light moves or the stone is rotated. Such effects occur when light is reflected off the "silk" (the structurally oriented rutile needle inclusions) in a certain way. This is one example where inclusions increase the value of a gemstone. Furthermore, rubies can show color changes—though this occurs very rarely—as well as chatoyancy or the "cat's eye" effect.
Color.
Generally, gemstone-quality corundum in all shades of red, including pink, are called rubies. However, in the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone will be called a pink sapphire. This distinction between rubies and pink sapphires is relatively new, having arisen sometime in the 20th century. If a distinction is made, the line separating a ruby from a pink sapphire is not clear and highly debated. As a result of the difficulty and subjectiveness of such distinctions, trade organizations such as the International Colored Gemstone Association (ICGA) have adopted the broader definition for ruby which encompasses its lighter shades, including pink.
Natural occurrence.
The Mogok Valley in Upper Myanmar (Burma) was for centuries the world's main source for rubies. That region has produced some of the finest rubies ever mined, but in recent years very few good rubies have been found there. The very best color in Myanmar rubies is sometimes described as "pigeon's blood." In central Myanmar, the area of Mong Hsu began producing rubies during the 1990s and rapidly became the world's main ruby mining area. The most recently found ruby deposit in Myanmar is in Namya (Namyazeik) located in the northern state of Kachin.
Rubies have historically been mined in Thailand, the Pailin and Samlout District of Cambodia, Burma, India, Afghanistan, Australia, Namibia, Colombia, Japan, Scotland, Brazil and in Pakistan. In Sri Lanka, lighter shades of rubies (often "pink sapphires") are more commonly found. After the Second World War ruby deposits were found in Tanzania, Madagascar, Vietnam, Nepal, Tajikistan, and Pakistan.
A few rubies have been found in the U.S. states of Montana, North Carolina, South Carolina and Wyoming. While searching for aluminous schists in Wyoming, geologist Dan Hausel noted an association of vermiculite with ruby and sapphire and located six previously undocumented deposits.
More recently, large ruby deposits have been found under the receding ice shelf of Greenland.
Republic of Macedonia is the only country in mainland Europe to have naturally occurring rubies. They can mainly be found around the city of Prilep. Macedonian ruby has a unique raspberry color. The ruby is also included on the Macedonian Coat of Arms. 
In 2002 rubies were found in the Waseges River area of Kenya. There are reports of a large deposit of rubies found in 2009 in Mozambique, in Nanhumbir in the Cabo Delgado district of Montepuez.
Spinel, another red gemstone, is sometimes found along with rubies in the same gem gravel or marble. Red spinel may be mistaken for ruby by those lacking experience with gems. However, the finest red spinels can have a value approaching that of the average ruby.
Factors affecting value.
Diamonds are graded using criteria that have become known as the four Cs, namely color, cut, clarity and carat weight. Similarly natural rubies can be evaluated using the four Cs together with their size and geographic origin.
Color: In the evaluation of colored gemstones, color is the most important factor. Color divides into three components: "hue", "saturation" and "tone". Hue refers to "color" as we normally use the term. Transparent gemstones occur in the following primary hues: red, orange, yellow, green, blue, violet. These are known as "pure spectral hues". In nature, there are rarely pure hues, so when speaking of the hue of a gemstone, we speak of primary and secondary and sometimes tertiary hues. In ruby, the primary hue must be red. All other hues of the gem species corundum are called sapphire. Ruby may exhibit a range of secondary hues. Orange, purple, violet and pink are possible.
The finest ruby is best described as being a vivid medium-dark toned red. Secondary hues add an additional complication. Pink, orange, and purple are the normal secondary hues in ruby. Of the three, purple is preferred because, firstly, the purple reinforces the red, making it appear richer. Secondly, purple occupies a position on the color wheel halfway between red and blue. In Burma where the term 'pigeon blood' originated, rubies are set in pure gold. Pure gold is itself a highly saturated yellow. When a purplish-red ruby is set in yellow, the yellow neutralizes its complement blue, leaving the stone appearing to be pure red in the setting.
Treatments and enhancements.
Improving the quality of gemstones by treating them is common practice. Some treatments are used in almost all cases and are therefore considered acceptable. During the late 1990s, a large supply of low-cost materials caused a sudden surge in supply of heat-treated rubies, leading to a downward pressure on ruby prices.
Improvements used include color alteration, improving transparency by dissolving rutile inclusions, healing of fractures (cracks) or even completely filling them.
The most common treatment is the application of heat. Most, if not all, rubies at the lower end of the market are heat treated on the rough stones to improve color, remove "purple tinge", blue patches and silk. These heat treatments typically occur around temperatures of 1800 °C (3300 °F). Some rubies undergo a process of low tube heat, when the stone is heated over charcoal of a temperature of about 1300 °C (2400 °F) for 20 to 30 minutes. The silk is only partially broken as the color is improved.
Another treatment, which has become more frequent in recent years, is lead glass filling. Filling the fractures inside the ruby with lead glass (or a similar material) dramatically improves the transparency of the stone, making previously unsuitable rubies fit for applications in jewelry. The process is done in four steps:
If a color needs to be added, the glass powder can be "enhanced" with copper or other metal oxides as well as elements such as sodium, calcium, potassium etc.
The second heating process can be repeated three to four times, even applying different mixtures. When jewelry containing rubies is heated (for repairs) it should not be coated with boracic acid or any other substance, as this can etch the surface; it does not have to be "protected" like a diamond.
The treatment can easily be determined using a 10x loupe and determination focuses on finding bubbles either in the cavities or in the fractures that were filled with glass.
Synthetic and imitation rubies.
In 1837 Gaudin made the first synthetic rubies by fusing potash alum at a high temperature with a little chromium as a pigment. In 1847 Ebelmen made white sapphire by fusing alumina in boric acid. In 1877 Frenic and Freil made crystal corundum from which small stones could be cut. Frimy and Auguste Verneuil manufactured artificial ruby by fusing BaF2 and Al2O3 with a little chromium at red heat. In 1903 Verneuil announced he could produce synthetic rubies on a commercial scale using this flame fusion process. By 1910, Verneuil's laboratory had expanded into a 30 furnace production facility, with annual gemstone production having reached 1000 kg in 1907.
Other processes in which synthetic rubies can be produced are through Czochralski's pulling process, flux process, and the hydrothermal process. Most synthetic rubies originate from flame fusion, due to the low costs involved. Synthetic rubies may have no imperfections visible to the naked eye but magnification may reveal curves, striae and gas bubbles. The fewer the number and the less obvious the imperfections, the more valuable the ruby is; unless there are no imperfections (i.e., a "perfect" ruby), in which case it will be suspected of being artificial. Dopants are added to some manufactured rubies so they can be identified as synthetic, but most need gemological testing to determine their origin.
Synthetic rubies have technological uses as well as gemological ones. Rods of synthetic ruby are used to make ruby lasers and masers. The first working laser was made by Theodore H. Maiman in 1960 at Hughes Research Laboratories in Malibu, California, beating several research teams including those of Charles H. Townes at Columbia University, Arthur Schawlow at Bell Labs, and Gould at a company called TRG (Technical Research Group). Maiman used a solid-state light-pumped synthetic ruby to produce red laser light at a wavelength of 694 nanometers (nm). Ruby lasers are still in use. Rubies are also used in applications where high hardness is required such as at wear exposed locations in modern mechanical clockworks, or as scanning probe tips in a coordinate measuring machine.
Imitation rubies are also marketed. Red spinels, red garnets, and colored glass have been falsely claimed to be rubies. Imitations go back to Roman times and already in the 17th century techniques were developed to color foil red—by burning scarlet wool in the bottom part of the furnace—which was then placed under the imitation stone. Trade terms such as balas ruby for red spinel and rubellite for red tourmaline can mislead unsuspecting buyers. Such terms are therefore discouraged from use by many gemological associations such as the Laboratory Manual Harmonisation Committee (LMHC).

</doc>
<doc id="43552" url="http://en.wikipedia.org/wiki?curid=43552" title="Ruby character">
Ruby character

Ruby characters (ルビ, Rubi) are small, annotative glosses that can be placed above or to the right of a Chinese character when writing languages with logographic characters such as Chinese or Japanese to show the pronunciation. Typically called just ruby or rubi, such annotations are used as pronunciation guides for characters that are likely to be unfamiliar to the reader.
Examples.
Here is an example of Japanese ruby characters (called "furigana") for Tokyo ("東京"):
Most "furigana" (Japanese ruby characters) are written with the "hiragana" syllabary, but "katakana" and "romaji" are also occasionally used. Alternatively, sometimes foreign words (usually English) are printed with furigana implying the meaning, and vice versa. Textbooks usually write on-readings with katakana and kun-readings with hiragana.
Here is an example of the Chinese ruby characters for Beijing ("北京"):
In Taiwan, the syllabary used for Chinese ruby characters is "Zhuyin fuhao" (also known as "Bopomofo"); in mainland China "pinyin" is used. Typically, zhuyin is used with a vertical traditional writing and zhuyin is written on the right side of the characters. In mainland China, horizontal script is used and ruby characters (pinyin) are written above the Chinese characters.
Books with phonetic guides are popular with children and foreigners learning Chinese (especially pinyin).
Here is an example of the Korean ruby characters for Korea ("韓國"):
Uses.
Ruby may be used for different reasons:
Also, ruby may be used to show the meaning, rather than pronunciation, of a possibly-unfamiliar (usually foreign) or slang word. This is generally used with spoken dialogue and applies only to Japanese publications. The most common form of ruby is called "furigana" or "yomigana" and is found in Japanese instructional books, newspapers, comics and books for children.
In Japanese, certain characters, such as the sokuon (促音, tsu, 小さいつ literally "little "tsu"") (っ) that indicates a pause before the consonant it precedes, are normally written at about half the size of normal characters. When written as ruby, such characters are usually the same size as other ruby characters. Advancements in technology now allow certain characters to render accurately.
In Chinese, the practice of providing phonetic cues via ruby is rare, but does occur systematically in grade-school level text books or dictionaries. The Chinese have no special name for this practice, as it is not as widespread as in Japan. In Taiwan, it is known as "zhuyin", from the name of the phonetic system employed for this purpose there. It is virtually always used vertically, because publications are normally in a vertical format, and zhuyin is not as easy to read when presented horizontally. Where zhuyin is not used, other Chinese phonetic systems like pinyin are employed.
Sometimes interlinear glosses are visually similar to ruby, appearing above or below the main text in smaller type. However, this is a distinct practice used for helping students of a foreign language by giving glosses for the words in a text, as opposed to the pronunciation of lesser-known characters.
Ruby annotation can also be used in handwriting.
History.
In British typography, "ruby" was originally the name for type with a height of 5.5 points, which printers used for interlinear annotations in printed documents. In Japanese, rather than referring to a font size, the word became the name for typeset "furigana". When transliterated back into English, some texts rendered the word as "rubi", (a typical romanization of the Japanese word ルビ). However, the spelling "ruby" has become more common since the W3C published a recommendation for "ruby markup". In the US, the font size had been called "agate", a term in use since 1831 according to the "Oxford English Dictionary".
HTML markup.
In 2001, the W3C published the Ruby Annotation specification for supplementing XHTML with ruby markup. Ruby markup is not a standard part of HTML 4.01 or any of the XHTML 1.0 specifications (XHTML-1.0-Strict, XHTML-1.0-Transitional, and XHTML-1.0-Frameset), but was incorporated into the XHTML 1.1 specification, and is expected to be a core part of HTML5 once the specification becomes finalised by the W3C.
Support for ruby markup in web browsers is limited, as XHTML 1.1 is not yet widely implemented. Ruby markup is partially supported by Microsoft Internet Explorer (5.0+) for Windows and Macintosh, supported by Chrome, but is not supported by Konqueror or Opera. The WebKit nightly builds added support for Ruby HTML markup in January 2010. Safari has included support in version 5.0.6. It is also supported in Mozilla Firefox as of version 38.
For those browsers that don't support Ruby natively, Ruby support is most easily added by using CSS rules that are easily found on the web.
Ruby markup support can also be added to some browsers that support custom extensions.
Ruby markup is structured such that a fallback rendering, consisting of the ruby characters in parentheses immediately after the main text, appears if the browser does not support ruby.
The W3C is also working on a specific ruby module for CSS level 2, which additionally allows the grouping of ruby and automatic omission of furigana matching their annotated part. This is currently only supported by Firefox 38.
Markup examples.
Below are a few examples of ruby markup. The markup is shown first, and the rendered markup is shown next, followed by the unmarked version. Web browsers either render it with the correct size and positioning as shown in the table-based examples above, or use the fallback rendering with the ruby characters in parentheses:
Note that Chinese ruby text would normally be displayed in vertical columns to the right of each character. This approach is not typically supported in browsers at present.
This is a table-based example of vertical columns:
Complex ruby markup.
Complex ruby markup makes it possible to associate more than one ruby text with a base text, or parts of ruby text with parts of base text. It is currently only supported in Firefox 38.
Unicode.
Unicode and its companion standard, the Universal Character Set, support ruby via these "interlinear annotation" characters:
Few applications implement these characters. Unicode Technical Report #20 clarifies that these characters are not intended to be exposed to users of markup languages and software applications. It suggests that ruby markup be used instead, where appropriate.
ANSI.
ISO/IEC 6429 (also known as ECMA-48) which defines the ANSI escape codes also provided a mechanism for ruby text for use by text terminals, although few terminals and terminal emulators implement it. The PARALLEL TEXTS (PTX) escape code accepted six parameter values giving the following escape sequences for marking ruby text:

</doc>
<doc id="43564" url="http://en.wikipedia.org/wiki?curid=43564" title="Action film">
Action film

Action film is a film genre in which one or more heroes are thrust into a series of challenges that typically include physical feats, extended fight scenes, violence, and frantic chases. Action films tend to feature a resourceful character struggling against incredible odds, which include life-threatening situations, a villain, or a pursuit which generally concludes in victory for the hero.
Advancements in CGI have made it cheaper and easier to create action sequences and other visual effects that required the efforts of professional stunt crews in the past. However, reactions to action films containing significant amounts of CGI have been mixed as films that use computer animations to create unrealistic, highly unbelievable events are often met with criticism. While action has long been a recurring component in films, the "action film" genre began to develop in the 1970s along with the increase of stunts and special effects. The genre is closely associated with the thriller and adventure film genres, and it may also contain elements of spy fiction and espionage.
History.
Early action films.
Some historians consider "The Great Train Robbery"| to be the first action film.During the 1920s and 1930s, action-based films were often "swashbuckling" adventure films in which actors, such as Douglas Fairbanks, wielded swords in period pieces or Westerns.
The 1940s and 1950s saw "action" in a new form through war and cowboy movies. Alfred Hitchcock ushered in the spy-adventure genre while also establishing the use of action-oriented "set pieces" like the famous crop-duster scene and the Mount Rushmore finale in "North by Northwest". The film, along with a war-adventure called "The Guns of Navarone", inspired producers Albert R. Broccoli and Harry Saltzman to invest in their own spy-adventure, based on the novels of Ian Fleming.
The long-running success of the James Bond films or series (which dominated the action films of the 1960s) introduced a staple of the modern-day action film: the resourceful hero. Such larger-than-life characters were a veritable “one-man army”; able to dispatch villainous masterminds after cutting through their disposable henchmen in increasingly creative ways. Such heroes are ready with one-liners, puns, and dry quips. The Bond films also used fast cutting, car chases, fist fights, a variety of weapons and gadgets, and elaborate action sequences.
1970s.
During the 1970s, the Bond films faced competition as gritty detective stories and urban crime dramas began to evolve and fuse themselves with the new "action" style, leading to a string of maverick police officer films, such as "Bullitt" (1968), "The French Connection" (1971) , "Dirty Harry" (1971) and "The Seven-Ups" (1973). Dirty Harry essentially lifted its star, Clint Eastwood, out of his cowboy typecasting, and framed him as the archetypal hero of the urban action film, proving that the modern world offered just as much glamour, excitement, and potential for violence as the Old West. "Dirty Harry" signaled the end of the prolific "cowboys and Indians" era of Western films. Restrictions on language, adult content, and violence had loosened up, and these elements became more widespread. The cross-breeding of genres (such as spy-films and war movies, or westerns and detective dramas) would become the norm in the 1980s. It should also be noted, however, that the 1970s saw the introduction of martial-arts films to western audiences.
Inspired by the success of James Bond, Asian-influenced martial-arts-themed action movies, such as Bruce Lee's "Enter the Dragon" (1973) and "Way of (or Return of) the Dragon" (1972), exploded onto western cinema screens. The latter also introduced action fans to then-rising star Chuck Norris. Though Jackie Chan's "Rush Hour" is often credited as popularizing the martial arts action film in the United States, Chuck Norris had been blending martial arts with cops and robbers since "Good Guys Wear Black" (1977) and "A Force of One" (1979).
From Japan, Sonny Chiba starred in his first martial arts movie in 1973 called the "Karate Kiba". His breakthrough international hit was "The Street Fighter series" (1974), which established him as the reigning Japanese martial arts actor in international cinema. He also played the role of Mas Oyama in "Champion of Death", "Karate Bearfighter," and "Karate for Life" (1975–1977). Chiba's action films were not only bounded by martial arts, but also action thriller ("Doberman Cop" and "" - both of 1977), jidaigeki ("Shogun's Samurai" - 1978, "Samurai Reincarnation" - 1981), and science fiction ("G.I. Samurai" - 1979).
1980s.
The 1980s would see the action film take over Hollywood to become a dominant form of summer blockbuster. "The action era" was popularized by actors such as Sylvester Stallone, Arnold Schwarzenegger, Bruce Willis, and Chuck Norris. Steven Spielberg and George Lucas even paid their homage to the Bond-inspired style with the mega-hit "Raiders of the Lost Ark" (1981). In 1982, veteran actor Nick Nolte and rising comedian Eddie Murphy smashed box office records with the action-comedy "48 Hrs", credited as the first "buddy-cop" movie. That same year, Sylvester Stallone starred in "First Blood", the first installment in the popular Rambo film series. The film proved to be successful and was followed with a sequel in 1985, "", which became the most successful film in the series and made the character John Rambo a pop culture icon.
1987's Lethal Weapon starring Mel Gibson, Danny Glover and Darlene Love was another significant action film hit of the decade, and another "buddy-cop" genre classic, launching a franchise that spawned 3 sequels.
Later, the 1988 film, "Die Hard", was particularly influential on the development of the action genre. In the film, Bruce Willis plays a New York police detective who inadvertently becomes embroiled in a terrorist take-over of a Los Angeles office building high-rise. The film set a pattern for a host of imitators, like "Under Siege" (1992), which used the same formula in a different setting. By the end of the 1980s, the influence of the successful action film could be felt in almost every genre.
1990s.
Like the Western genre, spy-movies, as well as urban-action films, were starting to parody themselves, and with the growing revolution in CGI (computer generated imagery), the "real-world" settings began to give way to increasingly fantastic environments. This new era of action films often had budgets unlike any in the history of motion pictures. The success of the many Dirty Harry and James Bond sequels had proven that a single successful action film could lead to a continuing action franchise. Thus, the 1980s and 1990s saw a rise in both budgets and the number of sequels a film could generally have. This led to an increasing number of filmmakers to create new technologies that would allow them to beat the competition and take audiences to new heights. The success of Tim Burton's "Batman" (1989) led to a string of financially successful sequels. Within a single decade, they proved the viability of a novel subgenre of action film: the comic-book movie.
2000s.
While action films continue to flourish as the medium-budget genre movie, it is remarkable how well it has fused with tent-pole pictures. For example, 2009's "Star Trek" had several science fiction tropes and concepts like time travel through a black hole. However, most of the film was structured around action sequences, many of them quite conventional (hand-to-hand, shooting). While the original "Star Wars" featured some of this kind of fighting, there was just as much emphasis on star-ship chases and dog fights in outer space. The newer films featured more light-saber duels, sometimes more intense and acrobatic than the originals. Some fan films also have similar duel scenes like those the prequel trilogy.
It was action with a science fiction twist. The trend with "Star Trek" and even the grittier "Dark Knight" trilogy, is that hand-to-hand fighting and Asian martial-arts techniques are now widely used in science fiction and superhero movies.
As for the 21st century action star, Jason Statham might be the most obvious Western example, though he still has not led a summer tent-pole. His dedication to being an action star is nonetheless notable. The dearth of new action heroes is a popular topic of conversation, so much so that Sylvester Stallone's "The Expendables" parodies the aging crop of 1980s superstars.
Hong Kong action cinema.
Currently, action films requiring extensive stunt work and special effects tend to be expensive. As such, they are regarded as mostly a large-studio genre in Hollywood, although this is not the case in Hong Kong action cinema, where action films are often modern variations of martial arts films. Because of their roots and lower budgets, Hong Kong action films typically center on physical acrobatics, martial arts fight scenes, stylized gun-play, and dangerous stunt work performed by leading stunt actors. On the other hand, American action films typically feature big explosions, car chases, stunt doubles and CGI special effects.
Hong Kong action cinema was at its peak from the 1970s to 1990s, when its action movies were experimenting with and popularizing various new techniques that would eventually be adopted by Hollywood action movies. This began in the early 1970s with the martial arts movies of Bruce Lee, which led to a wave of Bruceploitation movies that eventually gave way to the comedy kung fu films of Jackie Chan by the end of the decade. During the 1980s, Hong Kong action cinema re-invented itself with various new movies. These included the modern martial arts action movies featuring physical acrobatics and dangerous stunt work of Jackie Chan and his stunt team, as well as Sammo Hung and Yuen Biao; the wire fu and wuxia films of Tsui Hark, Yuen Woo-Ping, Jet Li and Donnie Yen; the gun fu, heroic bloodshed and Triad films of John Woo, Ringo Lam and Chow Yun-Fat; and the girls with guns films of Moon Lee and Michelle Yeoh.
Notable individuals.
Actors.
Actors from the 1950s and 1960s, such as John Wayne, Steve McQueen and Lee Marvin, passed the torch in the 1970s to actors such as Bruce Lee, Tom Laughlin, Charles Bronson, Chuck Norris, Clint Eastwood and Sonny Chiba. In the 1980s, Mel Gibson and Danny Glover had a popular string of "buddy cop" films in the "Lethal Weapon" franchise. Beginning in the mid-1980s, actors such as the burly ex-bodybuilder Arnold Schwarzenegger and Sylvester Stallone wielded automatic weapons in a number of action films. Stern-faced martial artist Steven Seagal made a number of films. Bruce Willis played a Western-inspired hero in the popular "Die Hard" series of action films.
In the 1990s and 2000s, Asian actors Chow Yun-fat, Jet Li, and Jackie Chan appeared in a number of different types of action films, and US actors Wesley Snipes and Vin Diesel both had many roles. As well, several female actors had major roles in action films, such as Michelle Yeoh, Lucy Liu and ex-model Milla Jovovich. While Keanu Reeves and Harrison Ford both had major roles in action science fiction films ("The Matrix" and "Blade Runner", respectively), Ford branched out into a number of other action genres, such as action-adventure films.
US actor Matt Damon, who was nominated for an Academy Award for his sensitive portrayal of a math genius working as a janitor in "Good Will Hunting", later metamorphosed into an action hero with the car-chase and gunfire-filled Jason Bourne franchise. European action actors such as Belgian-born Jean-Claude Van Damme ("Bloodsport", "Hard Target", "Timecop"), Moroccan-born Jean Reno ("Ronin"), Swedish-born Dolph Lundgren ("Showdown in Little Tokyo", "Universal Soldier", "The Expendables"), Irish-born Colin Farrell ("SWAT", "Daredevil", "Miami Vice"), and English-born Jason Statham ("The Transporter", "The Expendables", "Crank"), appeared in a number of action films in the 1990s and 2000s.
Directors.
Notable action film directors from the 1960s and 1970s include Sam Peckinpah, whose 1969 Western "The Wild Bunch" was controversial for its bloody violence and nihilist tone. Influential and popular directors from the 1980s to 2000s include James Cameron (for the first two "Terminator" films, "Aliens", "True Lies"); Andrew Davis ("Code of Silence", "Above the Law", "Under Siege"); John Woo (Hong Kong action films such as "Hard Boiled" and US-made English-language films such as "Hard Target", "Broken Arrow" and "Face/Off"); John McTiernan (the first and third "Die Hard" films, "Predator", "The Last Action Hero"); Ridley Scott ("Black Rain", "Black Hawk Down"); The Wachowski Brothers ("The Matrix" trilogy), Andrzej Bartkowiak ("Romeo Must Die", "Exit Wounds", "Cradle 2 the Grave", ""), Robert Rodriguez ("Mexico" trilogy, "From Dusk till Dawn", "Machete") and Michael Bay (the first two "Bad Boys" films, "The Rock", "Transformers" trilogy); Louis Leterrier (the first two "Transporter" films, "Unleashed"). For a longer list, see the List of action film directors article.

</doc>
<doc id="43566" url="http://en.wikipedia.org/wiki?curid=43566" title="Heat (1995 film)">
Heat (1995 film)

Heat is a 1995 American crime thriller film written, produced and directed by Michael Mann, and starring Robert De Niro, Al Pacino, and Val Kilmer. The film was released in the United States on December 15, 1995. De Niro plays Neil McCauley, a professional thief, while Pacino plays Lt. Vincent Hanna, a veteran L.A.P.D. robbery-homicide detective tracking down McCauley's crew. The central conflict is based on the experiences of former Chicago police officer Chuck Adamson and his pursuit in the 1960s of a criminal named McCauley, after whom De Niro's character is named.
"Heat" was a commercial success, grossing $67 million in the United States and $187 million worldwide (about $<br>{Inflation} - Amount must not have "" prefix: 187.44.   million in 2015) against a $60 million budget. It was well received by critics. The film-critic aggregator Rotten Tomatoes reports 86% positive reviews, calling the film "an engrossing crime drama that draws compelling performances from its stars – and confirms Michael Mann's mastery of the genre."
Plot.
Career criminal Neil McCauley (Robert De Niro) and his crew, Chris Shiherlis (Val Kilmer), Michael Cheritto (Tom Sizemore) and Trejo (Danny Trejo), hire new recruit Waingro (Kevin Gage) and commit an armored car heist, stealing $1.6 million in bearer bonds belonging to money launderer Roger Van Zant (William Fichtner). However, Waingro impulsively kills one of the guards, forcing the robbers to kill the remaining two so as to leave no witnesses. An infuriated McCauley tries to kill Waingro afterwards, but he escapes. Afterwards McCauley's fence, Nate (Jon Voight), suggests they try to sell the bonds back to Van Zant, who agrees but secretly instructs his men to kill McCauley at the meeting. With backup from his crew, McCauley thwarts the ambush and vows revenge.
Lieutenant Vincent Hanna (Al Pacino) of the LAPD leads the investigation of the heist and learns that McCauley's crew plans to rob a precious metals depository. Hanna and his unit—Sergeant Drucker (Mykelti Williamson) and Detectives Sammy Casals (Wes Studi), Mike Bosko (Ted Levine), and Danny Schwartz (Jerry Trimble), stake out the depository, but when an officer inadvertently makes a noise, McCauley is alerted and calls off the robbery, forcing Hanna to let the crew go. Despite becoming aware of the police surveillance, McCauley and his crew take on a final heist, a brazen bank holdup worth $12 million, to secure their financial futures. Hanna's unit investigates the murder of a prostitute by Waingro, putting them on his trail. Waingro later approaches Van Zant in search of work and revenge against McCauley.
Hanna learns that his wife Justine (Diane Venora) is having an affair and moves into a hotel, while McCauley finds a relationship with Eady (Amy Brenneman), a woman he meets in a cafe. Hanna deliberately intercepts McCauley and invites him to coffee. Meeting face to face, each concedes to the other the problems of his personal life. Hanna describes his concern for his depressed stepdaughter Lauren (Natalie Portman) and the failure of his third marriage due to his obsession with work, and McCauley confesses that life as a criminal forbids attachment and requires mobility, making his relationship with his girlfriend tenuous. Both men admit their commitment to their work and that they will not hesitate to kill the other if the circumstances demand it.
After the meeting, Hanna discovers that McCauley and his crew have evaded their surveillance, but Trejo is compromised. In need of a new getaway driver, McCauley recruits Donald Breedan (Dennis Haysbert), an ex-convict working a dead-end job at a diner. Hanna's unit is alerted to the robbery by a confidential informant and surprises McCauley's crew as they exit the bank. Cheritto, Breedan, and several police officers, including Detective Bosko, are killed in the ensuing shootout. McCauley narrowly escapes with an injured Shiherlis, and leaves him with a doctor to treat his wounds. He tracks down Trejo, whom he finds beaten to a bloody pulp with his wife murdered. Trejo reveals that Waingro was the informant, with Van Zant assisting him. McCauley finishes off Trejo at his own request, then kills Van Zant at his home. He makes plans to flee to New Zealand with Eady, with whom he has reconciled after she became aware of his criminal activities. The police surveil Waingro in a hotel near the Los Angeles International Airport, and Hanna attempts to bait McCauley into coming out of hiding by releasing Waingro's whereabouts through his contacts.
Chris Shiherlis' estranged wife Charlene (Ashley Judd) is lured by her lover Alan Marciano (Hank Azaria) to a police safe house, where Drucker threatens to charge Charlene as an accomplice and send her son into foster care if she doesn't betray her husband to the police. Charlene initially agrees, but, when Chris shows up in disguise, she surreptitiously warns him, and he slips through the dragnet. Hanna finds Lauren unconscious in his hotel room from a suicide attempt and rushes her to the hospital. As he and Justine wait in the lobby, they commiserate but admit their marriage will never work. McCauley and Eady are en route to the airport when Nate calls with Waingro's location. McCauley has a change of heart, risking his assured freedom to exact his revenge. He infiltrates the hotel, creates a distraction by pulling a fire alarm, and kills Waingro. Moments away from escape, he is forced to abandon Eady when Hanna approaches through the crowd. Hanna chases McCauley into a field outside the LAX freight terminal and mortally wounds him then holds his hand as he dies.
Cast.
De Niro was the first cast member to get the film script, showing it to Pacino who also wanted to be a part of the film. De Niro believed "Heat" was a "very good story, had a particular feel to it, a reality and authenticity." Xander Berkeley had played Waingro in "L.A. Takedown", an earlier rendition of Mann's script for "Heat". He was cast in a minor role in "Heat".
In order to prepare the actors for the roles of McCauley's crew, Mann took Kilmer, Sizemore and De Niro to Folsom State Prison to interview actual career criminals. While researching her role, Ashley Judd met several former prostitutes who became housewives.
Historical background.
"Heat" is based on the true story of a real Neil McCauley, a calculating criminal and ex-Alcatraz inmate who was tracked down by Det. Chuck Adamson in 1964. Neil McCauley was raised in Wisconsin where his father worked as steam fitter to provide his family with a middle-class life. The normalcy of Neil's youth faded following the adoption of another child and his father's death in 1928. At 14, he quit school to find work to support his mother and five siblings. The McCauleys soon relocated to Chicago. In Chicago, McCauley began his criminal career after his mother began drinking heavily. By the time he was 20, he'd already done three stints in county jail for larceny.
In 1961, McCauley was transferred from Alcatraz to McNeil, as mentioned in the film, and he was released in 1962. Upon his release, he immediately began planning new heists. With ex-cons Michael Parille and William Pinkerton they used bolt cutters and drills to burglarize a manufacturing company of diamond drill bits, a scene which is closely recreated in the film. Detective Chuck Adamson, upon whom Al Pacino's character is largely based, began keeping tabs on McCauley’s crew around this time, knowing that he had become active again. The two even met for coffee once, just as portrayed in the film. Their dialogue in the script was almost exactly word for word the conversation that McCauley and Adamson had. The next time the two would meet, guns would be drawn, just as the movie portrays. 
On March 25, 1964, McCauley and members of his regular crew followed an armored car that delivered money to a Chicago grocery store. Once the drop was made, three of the robbers entered the store. They threatened the clerks and stole money bags worth $10,000 before they sped off amid a hail of police gunfire.
McCauley's crew was unaware that Adamson and eight other detectives had blocked off all potential exits, and when the getaway car turned down an alley and the bandits saw the blockade, they realized they were trapped. All four suspects exited the vehicle and began firing. Two of his crew, men named Breaden and Parille, were slain in an alley while a third man, Polesti (on whom Chris Shiherlis is very loosely based), shot his way out and escaped. McCauley was shot to death on the lawn of a nearby home. He was 50 years old and the prime suspect in several burglaries. Polesti was caught days later and sent to prison. As of 2011 Polesti was still alive.
Detective Adamson went on to a successful career as a television and film producer, and died in 2008 at age 71. Michael Mann's 2009 film "Public Enemies" stated in its end credits "In memory of Chuck Adamson". As an additional inspiration for Hanna, in an 1995 interview Mann cited an unnamed man working internationally against drug cartels. Additionally, the character of Nate, played by Jon Voight, is closely based on real-life former career criminal and fence turned writer Edward Bunker, who served as a consultant to Mann on the film.
Development.
In 1979, Mann wrote a 180-page draft of "Heat". He re-wrote it after making "Thief" in 1981 hoping to find a director to make it and mentioning it publicly in a promotional interview for his 1983 film "The Keep". In the late 1980s, he offered the film to his friend, film director Walter Hill, who turned him down. Following the success of "Miami Vice" and "Crime Story", Mann was to produce a new crime television show for NBC. He turned the script that would become "Heat" into a 90-minute pilot for a television series featuring the Los Angeles Police Department Robbery–Homicide division, featuring Scott Plank in the role of Hanna and Alex McArthur playing the character of Neil McCauley, renamed to Patrick McLaren. The pilot was shot in only nineteen days, atypical for Mann. The script was abridged down to almost a third of its original length, omitting many subplots that made it into "Heat". The network was unhappy with Plank as the lead actor, and asked Mann to recast Hanna's role. Mann declined and the show was cancelled and the pilot aired on August 27, 1989 as a television film entitled "L.A. Takedown", which was later released on VHS and DVD in Europe.
Production.
Pre-production.
In April 1994, Mann was reported to have abandoned his earlier plan to shoot a biopic of James Dean in favor of directing "Heat", producing it with Art Linson. The film was marketed as the first on-screen appearance of Robert De Niro and Al Pacino together in the same scene – both actors starred in "", but owing to the nature of their roles, they were never seen in the same scene. Pacino and De Niro were Mann's first choices for the roles of Hanna and McCauley, respectively, and they both immediately agreed to act.
Mann assigned Janice Polley, a former collaborator on "The Last of the Mohicans", as the film's location manager. Scouting locations lasted from August to December 1994. Mann requested locations which did not appear on film before, in which Polley was successful – fewer than 10 of the 85 filming locations were previously used. The most challenging shooting location proved to be Los Angeles Airport, with the film crew almost missing out due to a threat to the airport by Unabomber.
Filming.
Principal photography for "Heat" lasted 107 days. All of the shooting was done on location, Mann deciding not to use a soundstage.
Release.
Box office.
"Heat" was released on December 15, 1995, and opened #3 in the box office with $8,445,656 opening weekend in 1,325 theaters (behind "Jumanji" and "Toy Story" respectively). It grossed $67,436,818 in United States box offices, and $120 million in foreign box offices. "Heat" was ranked the #25 highest-grossing film of 1995.
Home media.
"Heat" was released on VHS in June 1996. Due to its running time, the film had to be released on two cassettes. A DVD release followed in 1999. A two-disc special-edition DVD was released in 2005, featuring an audio commentary by Michael Mann, deleted scenes, and numerous documentaries detailing the film's production. This edition contains the original theatrical cut.
The Blu-ray Disc was released on November 10, 2009, featuring a high-definition film transfer, supervised by Mann. Among the disc extras were Mann's audio commentary, a one-hour documentary about the making of the film and ten minutes worth of scenes cut from the film. As well as approving the look of the transfer, Mann also recut two scenes slightly differently, referring to them as "new content changes".
Reception.
"Heat" was well received by critics. The film-critic aggregator Rotten Tomatoes reports 86% positive reviews based on 76 reviews, summarizing the film as "an engrossing crime drama that draws compelling performances from its stars – and confirms Michael Mann's mastery of the genre." Metacritic gives the film a score of 76 based on 22 reviews. Roger Ebert gave the film 3½ stars out of 4, writing: "It's not just an action picture. Above all, the dialogue is complex enough to allow the characters to say what they're thinking: They are eloquent, insightful, fanciful, poetic when necessary. They're not trapped with cliches. Of the many imprisonments possible in our world, one of the worst must be to be inarticulate — to be unable to tell another person what you really feel." Simon Cote of "The Austin Chronicle" called the film "[o]ne of the most intelligent crime-thrillers to come along in years", and said Pacino and De Niro's scenes together were "poignant and gripping".
Kenneth Turan of the "Los Angeles Times" called the film a "sleek, accomplished piece of work, meticulously controlled and completely involving. The dark end of the street doesn't get much more inviting than this." Todd McCarthy of "Variety" wrote, "Stunningly made and incisively acted by a large and terrific cast, Michael Mann's ambitious study of the relativity of good and evil stands apart from other films of its type by virtue of its extraordinarily rich characterizations and its thoughtful, deeply melancholy take on modern life." Owen Gleiberman of "Entertainment Weekly" gave it a B− rating, saying that "Mann's action scenes [...] have an existential, you-are-there jitteriness," but called the heist-planning and Hanna's investigation scenes "dry, talky."
American Film Institute
Impact.
The explicit nature of several of the film's scenes was cited as the model of a spate of robberies since its release. This included armored car robberies in South Africa, Colombia, Denmark, and Norway and most famously the 1997 North Hollywood shootout, in which Larry Phillips, Jr. and Emil Mătăsăreanu robbed the North Hollywood branch of the Bank of America and, similarly to the film, were confronted by the LAPD as they left the bank. This shootout is considered one of the longest and bloodiest events of its type in American police history. Both robbers were killed, and eleven police officers and seven civilians were injured during the shootout. "Heat" was widely referenced during the coverage of the shootout.
For his film "The Dark Knight", director Christopher Nolan drew inspiration in his portrayal of Gotham City from "Heat" in order "to tell a very large, city story or the story of a city".
Soundtrack album.
On December 19, 1995, Warner Bros. Records released a soundtrack album on cassette and CD to accompany the film, entitled Heat: Music from the Motion Picture. The album was produced by Matthias Gohl. It contains a 29-minute selection of the film score composed by Elliot Goldenthal, as well as songs by other artists such as U2 and Brian Eno (collaborating as Passengers), Terje Rypdal, Moby, and Lisa Gerrard. "Heat" used an abridged instrumental rendition of the Joy Division song "New Dawn Fades" by Moby, which also features in the same form on the soundtrack album. Mann reused the Einstürzende Neubauten track "Armenia" in his 1999 film "The Insider". The film ends with Moby's "God Moving Over the Face of the Waters", a different version of which was included at the end of the soundtrack album.
Mann and Goldenthal decided on an atmospheric situation for the film soundtrack. Goldenthal used a setup consisting of multiple guitars, which he termed "guitar orchestra", and thought it brought the film score closer to a European style. The soundtrack was noted for lack of a central theme. Christian Clemmensen of Filmtracks.com criticized the omission from the album of much music heard in the film due to the film's length, but praised the album as a decent listening experience, and Goldenthal's score as "psychologically engaging and intellectually challenging", believing it to be one of Goldenthal's best. AllMusic called it a "soundtrack for the mind [...] full of twists and turns". Musicfromthemovies.com thought of the album as uncharacterist for Goldenthal's style, calling the atmosphere "absolutely electrifying".

</doc>
<doc id="43567" url="http://en.wikipedia.org/wiki?curid=43567" title="Hindu Kush">
Hindu Kush

The Hindu Kush (; Pashto/Persian/Urdu: هندوکش‎), also known as Pāriyātra Parvata (Sanskrit: पारियात्र पर्वत), Caucasus Indicus, or Paropamisadae (ancient Greek: Παροπαμισάδαι), is an 800 km long mountain range that stretches between central Afghanistan and northern Pakistan. It is a western subrange of the Himalayas. It divides the valley of the Amu Darya (the ancient "Oxus") to the north from the Indus River valley to the south.
The highest point in the Hindu Kush is Tirich Mir or Terichmir (7,708 m or 25,289 ft) in Chitral District of Khyber Pakhtunkhwa, Pakistan. To the east the Hindu Kush buttresses the Pamir range near the point where the borders of China, Pakistan and Afghanistan meet, after which it runs southwest through Pakistan and into Afghanistan, finally merging into minor ranges in western Afghanistan. The mountain range separates Central Asia from South Asia.
Origin of name.
The origins of the name "Hindu Kush" are uncertain, with multiple theories being propounded by different scholars and writers. In the time of Alexander the Great, the Hindu Kush range was referred to as the "Caucasus Indicus" or the "Caucasus of the Indus" (as opposed to the Greater Caucasus range between the Caspian and Black Seas), and some past authors have considered this as a possible derivation of the name "Hindu Kush". However, many other theories have been propounded by different scholars and writers for the origins of the modern name "Hindu Kush". "Hindū Kūh" (ھندوکوه) and "Kūh-e Hind" (کوهِ ھند) are usually applied to the entire range separating the basins of the Kabul and Helmand rivers from that of the Amu River (ancient "Oxus"), or more specifically, to that part of the range lying northwest of the Afghan capital Kabul. Sanskrit documents refer to the Hindu Kush as "Pāriyātra Parvata" (पारियात्र पर्वत).
The mountain range was called "Paropamisadae" by Hellenic Greeks in the late first millennium BC.
Other sources state that the term "Hindu Kush" originally applied only to the peak in the area of the Kushan Pass, which had become a center of the Kushan Empire by the 1st century AD.
The Persian-English dictionary indicates that the word 'koš' [kʰoʃ] is derived from the verb ('koštan' کشتن [kʰoʃˈt̪ʰæn]), meaning to kill. Although the derivation is only a possible one, some authors have proposed the meaning 'Kills the Hindu' for "Hindu Kush", a derivation that is reproduced in "Encyclopedia Americana":
The name Hindu Kush means literally 'Kills the Hindu', a reminder of the days when Indian slaves from the Indian subcontinent died in the harsh weather typical of the Afghan mountains while being transported to Central Asia.
The World Book Encyclopedia states that "the name Kush, .. means Death". While Encyclopædia Britannica says 'The name Hindu Kush first appears in 1333 AD in the writings of Ibn Battutah, the medieval Berber traveller, who said the name meant 'Hindu Killer', a meaning still given by Afghan mountain dwellers who are traditional enemies of Indian plainsmen.
The word "Koh" or "Kuh" means "mountain" in some local language Khowar. According to Nigel Allan, "Hindu Kush" meant both "mountains of India" and "sparkling snows of India", as he notes, from a Central Asian perspective. Furthermore, some believe it to be the name derived from the rule of the Hindu god Rama's son, Kusha, who ruled in Kasur, in present day Punjab, Pakistan.
History.
The mountains have historical significance in the Indian subcontinent and China.
There has been a military presence in the mountains since the time of Darius the Great. The Great Game of the 19th century often involved military, intelligence and/or espionage personnel from both the Russian and British Empires operating in areas of the Hindu Kush. The Hindu Kush were considered, informally, the dividing line between Russian and British areas of influence in Afghanistan.
During the Cold War the mountains again became militarized, especially during the 1980s when Soviet forces and their Afghan allies fought the mujahideen. After the Soviet withdrawal, Afghan warlords fought each other and later the Taliban and the Northern Alliance and others fought in and around the mountains.
The American and ISAF campaign against Al Qaeda and their Taliban allies has once again resulted in a major military presence in the Hindu Kush.
Alexander the Great explored the Afghan areas between Bactria and the Indus River after his conquest of the Achaemenid Empire in 330 BCE. It became part of the Seleucid Empire before falling to the Indian Maurya Empire around 305 BCE.
Alexander took these away from the Persians and established settlements of his own, but Seleucus Nicator gave them to Sandrocottus (Chandragupta), upon terms of intermarriage and of receiving in exchange 500 elephants.—Strabo, 64 BCE–24 CE
Indo-Scythians expelled the Indo-Greeks by the mid 1st century BCE, but lost the area to the Kushan Empire about 100 years later.
Before the Christian era, and afterwards, there was an intimate connection between the Kabul Valley and India. All the passes of the Hindu-Kush descend into that valley; and travellers from the north as soon as they crossed the watershed, found a civilization and religion, the same as that which prevailed in India. The great range was the boundary in those days and barrier that was at times impassable. Hindu-Kuh—the mountain of Hind—was similarly derived.
Pre-Islamic populations of the Hindu Kush included Shins, Yeshkun,
Chiliss,
Neemchas
Koli,
Palus,
Gaware,
Yeshkuns,
Krammins,
Indo-Scythians,
Bactrian Greeks, and
Kushans.
Mountains.
The mountains of the Hindu Kush system diminish in height as they stretch westward: Toward the middle, near Kabul, they extend from 4,500 to; in the west, they attain heights of 3,500 to. The average altitude of the Hindu Kush is 4,500 m. The Hindu Kush system stretches about 966 km laterally, and its median north-south measurement is about 240 km. Only about 600 km of the Hindu Kush system is called the Hindu Kush mountains. The rest of the system consists of numerous smaller mountain ranges including the Koh-e Baba, Salang, Koh-e Paghman, Spin Ghar (also called the eastern Safēd Kōh), Suleiman Range, Siah Koh, Koh-e Khwaja Mohammad and Selseleh-e Band-e Turkestan. The western Safid Koh, the Malmand, Chalap Dalan, Siah Band and Doshakh are commonly referred to as the Paropamise by western scholars, though that name has been slowly falling out of use over the last few decades.
Rivers that flow from the mountain system include the Helmand River, the Hari River and the Kabul River, watersheds for the Sistan Basin.
Numerous high passes ("kotal") transect the mountains, forming a strategically important network for the transit of caravans. The most important mountain pass is the Salang Pass (Kotal-e Salang) (3,878 m); it links Kabul and points south of it to northern Afghanistan. The completion of a tunnel within this pass in 1964 reduced travel time between Kabul and the north to a few hours. Previously access to the north through the Kotal-e Shibar (3,260 m) took three days. The Salang tunnel at 3,363 m and the extensive network of galleries on the approach roads were constructed with Soviet financial and technological assistance and involved drilling 1.7 miles through the heart of the Hindu Kush.
Before the Salang road was constructed, the most famous passes in the Western historical perceptions of Afghanistan were those leading to India. They include the Khyber Pass (1,027 m), in Pakistan, and the Kotal-e Lataband (2,499 m) east of Kabul, which was superseded in 1960 by a road constructed within the Kabul River's most spectacular gorge, the Tang-e Gharu. This remarkable engineering feat reduced travel time between Kabul and the Pakistan border from two days to a few hours.
The roads through the Salang and Tang-e Gharu passes played critical strategic roles during the U.S. invasion of Afghanistan and were used extensively by heavy military vehicles. Consequently, these roads are in very bad repair. Many bombed out bridges have been repaired, but numbers of the larger structures remain broken. Periodic closures due to conflicts in the area seriously affect the economy and well-being of many regions, for these are major routes carrying commercial trade, emergency relief and reconstruction assistance supplies destined for all parts of the country.
There are a number of other important passes in Afghanistan. The Wakhjir Pass (4,923 m), proceeds from the Wakhan Corridor into Xinjiang, China, and into Northern Areas of Pakistan. Passes which join Afghanistan to Chitral, Pakistan, include the Baroghil (3,798 m) and the Kachin (5,639 m), which also cross from the Wakhan. Important passes located farther west are the Shotorgardan (3,720 m), linking Logar and Paktiya provinces; the Bazarak (2,713 m), leading into Mazari Sharif; the Khawak Pass (4,370 m) in the Panjsher Valley, and the Anjuman Pass (3,858 m) at the head of the Panjsher Valley giving entrance to the north. The Hajigak (2,713 m) and Unai (3,350 m) lead into the eastern Hazarajat and Bamyan Valley. The passes of the Paropamisus in the west are relatively low, averaging around 600 meters; the most well-known of these is the Sabzak between the Herat and Badghis provinces, which links the western and northwestern parts of Afghanistan.
These mountainous areas are mostly barren, or at the most sparsely sprinkled with trees and stunted bushes. Very ancient mines producing lapis lazuli are found in Kowkcheh Valley, while gem-grade emeralds are found north of Kabul in the valley of the Panjsher River and some of its tributaries. The famous 'balas rubies', or spinels, were mined until the 19th century in the valley of the Ab-e Panj or Upper Amu Darya River, considered to be the meeting place between the Hindu Kush and the Pamir ranges. These mines now appear to be exhausted.
Eastern Hindu Kush.
The Eastern Hindu Kush range, also known as the High Hindu Kush range, is mostly located in northern Pakistan and the Nuristan and Badakhshan provinces of Afghanistan. The Chitral District of Pakistan is home to Tirich Mir, Noshaq, and Istoro Nal, the highest peaks in the Hindu Kush. The range also extends into Ghizar, Yasin Valley, and Ishkoman in Pakistan's Northern Areas.
Chitral is considered to be the pinnacle of the Hindu Kush region. The highest peaks, as well as countless passes and massive glaciers, are located in this region. The Chiantar, Kurambar, and Terich glaciers are amongst the most extensive in the Hindu Kush and the meltwater from these glaciers form the Kunar River, which eventually flows south into Afghanistan and joins the Bashgal, Panjshir, and eventually the much smaller Kabul River.

</doc>
<doc id="43568" url="http://en.wikipedia.org/wiki?curid=43568" title="Tom Hanks">
Tom Hanks

Thomas Jeffrey "Tom" Hanks (born July 9, 1956) is an American actor and filmmaker. He is known for his roles in "Big" (1988), "Philadelphia" (1993), "Forrest Gump" (1994), "Apollo 13" (1995), "Saving Private Ryan", "You've Got Mail" (both 1998), "Cast Away" (2000), "The Da Vinci Code" (2006), "Captain Phillips", and "Saving Mr. Banks" (both 2013), as well as for his voice work in the animated films "The Polar Express" (2004) and the "Toy Story" series.
Hanks has been nominated for numerous awards during his career. He won a Golden Globe Award and an Academy Award for Best Actor for his role in "Philadelphia", as well as a Golden Globe, an Academy Award, a Screen Actors Guild Award, and a People's Choice Award for Best Actor for his role in "Forrest Gump". In 2004, he received the Stanley Kubrick Britannia Award for Excellence in Film from the British Academy of Film and Television Arts (BAFTA).
Hanks is also known for his collaboration with film director Steven Spielberg on "Saving Private Ryan", "Catch Me If You Can" (2002), and "The Terminal" (2004), as well as the 2001 miniseries "Band of Brothers", which launched Hanks as a successful director, producer, and writer. In 2010, Spielberg and Hanks were executive producers on the HBO miniseries "The Pacific" (a companion piece to "Band of Brothers").
As of 2014, Hanks' films have grossed over $4.2 billion at the U.S. and Canada box offices, and over $8.4 billion worldwide, making him one of the highest-grossing actors in film history.
Early life and education.
Hanks was born in Concord, California. His father, Amos Mefford Hanks, was an itinerant cook. His hospital-worker mother, Janet Marylyn (Frager), was of Portuguese ancestry (her family's surname was originally Fraga), while two of Hanks' paternal great-grandparents emigrated from the United Kingdom. Hanks' parents divorced in 1960. The family's three oldest children, Sandra (later Sandra Hanks Benoiton, a writer), Larry (Lawrence M. Hanks, PhD, an entomology professor at the University of Illinois at Urbana-Champaign) and Tom, went with their father, while the youngest, Jim, who became an actor and filmmaker, remained with their mother in Red Bluff, California.
While Hanks' family religious history was Catholic and Mormon, he has characterized himself as being a "Bible-toting evangelical" for several years as a teenager. In school, Hanks was unpopular with students and teachers alike, later telling "Rolling Stone" magazine: "I was a geek, a spaz. I was horribly, painfully, terribly shy. At the same time, I was the guy who'd yell out funny captions during filmstrips. But I didn't get into trouble. I was always a real good kid and pretty responsible." In 1965, Amos Hanks married Frances Wong, a San Francisco native of Chinese descent. Frances had three children, two of whom lived with Tom during his high school years. Hanks acted in school plays, including "South Pacific", while attending Skyline High School in Oakland, California.
Hanks studied theater at Chabot College in Hayward, and transferred to California State University, Sacramento two years later. Hanks told "New York" magazine in 1986: "Acting classes looked like the best place for a guy who liked to make a lot of noise and be rather flamboyant ...I spent a lot of time going to plays. I wouldn't take dates with me. I'd just drive to a theater, buy myself a ticket, sit in the seat and read the program, and then get into the play completely. I spent a lot of time like that, seeing Brecht, Tennessee Williams, Ibsen, and all that."
During his years studying theater, Hanks met Vincent Dowling, head of the Great Lakes Theater Festival in Cleveland, Ohio. At Dowling's suggestion, Hanks became an intern at the festival. His internship stretched into a three-year experience that covered most aspects of theater production, including lighting, set design, and stage management, prompting Hanks to drop out of college. During the same time, Hanks won the Cleveland Critics Circle Award for Best Actor for his 1978 performance as Proteus in Shakespeare's "The Two Gentlemen of Verona", one of the few times he played a villain.
Career.
Early acting career (1979–85).
In 1979, Hanks moved to New York City, where he made his film debut in the low-budget slasher film "He Knows You're Alone" (1980) and landed a starring role in the television movie "Mazes and Monsters". Early that year, he was cast in the lead, Callimaco, in the Riverside Shakespeare Company's production of Niccolò Machiavelli's "The Mandrake", directed by Daniel Southern. As a high profile Off Off Broadway showcase, the production helped Tom land an agent, Joe Ohla, with the J. Michael Bloom Agency.
The following year, Hanks landed one of the lead roles, that of character Kip Wilson, on the ABC television pilot of "Bosom Buddies". He and Peter Scolari played a pair of young advertising men forced to dress as women so they could live in an inexpensive all-female hotel. Hanks had previously partnered with Scolari on the 1970s game show "Make Me Laugh". After landing the role, Hanks moved to Los Angeles. "Bosom Buddies" ran for two seasons, and, although the ratings were never strong, television critics gave the program high marks. "The first day I saw him on the set," co-producer Ian Praiser told "Rolling Stone", "I thought, 'Too bad he won't be in television for long.' I knew he'd be a movie star in two years." However, although Praiser knew it, he was not able to convince Hanks. "The television show had come out of nowhere," Hanks' best friend Tom Lizzio told "Rolling Stone". "Then out of nowhere it got canceled. He figured he'd be back to pulling ropes and hanging lights in a theater."
"Bosom Buddies" and a guest appearance on a 1982 episode of "Happy Days" ("A Case of Revenge," in which he played a disgruntled former classmate of Fonzie) prompted director Ron Howard to contact Hanks. Howard was working on the film "Splash" (1984), a romantic comedy fantasy about a mermaid who falls in love with a human. At first, Howard considered Hanks for the role of the main character's wisecracking brother, a role that eventually went to John Candy. Instead, Hanks landed the lead role and subsequent career boost from "Splash", which went on to become a surprise box office hit, grossing more than US$69 million. He also had a sizable hit with the sex comedy "Bachelor Party", also in 1984. In 1983–84, Hanks made three guest appearances on "Family Ties" as Elyse Keaton's alcoholic brother, Ned Donnelly.
Period of successes and failures (1986–91).
With "Nothing in Common" (1986) – a story of a young man alienated from his father (played by Jackie Gleason) – Hanks began to extend himself from comedic roles to dramatic roles. In an interview with "Rolling Stone" magazine Hanks commented on his experience: "It changed my desires about working in movies. Part of it was the nature of the material, what we were trying to say. But besides that, it focused on people's relationships. The story was about a guy and his father, unlike, say, "The Money Pit", where the story is really about a guy and his house."
After a few more flops and a moderate success with the comedy "Dragnet", Hanks' stature in the film industry rose. The broad success of the fantasy comedy "Big" (1988) established Hanks as a major Hollywood talent, both as a box office draw and within the industry as an actor. For his performance in the film, Hanks earned his first nominations for the Academy Award for Best Actor. "Big" was followed later that year by "Punchline", in which he and Sally Field co-starred as struggling comedians.
Hanks then suffered a run of box-office under-performers: "The 'Burbs" (1989), "Joe Versus the Volcano" (1990), and "The Bonfire of the Vanities" (1990). In the latter, he portrayed a greedy Wall Street figure who gets enmeshed in a hit-and-run accident. Only the 1989 movie "Turner & Hooch" brought success for Hanks during this period. 
Progression into dramatic roles (1992–95).
Hanks climbed back to the top again with his portrayal of a washed-up baseball legend turned manager in "A League of Their Own" (1992). Hanks has admitted that his acting in earlier roles was not great, but that he has improved. In an interview with "Vanity Fair", Hanks noted his "modern era of moviemaking ... because enough self-discovery has gone on ... My work has become less pretentiously fake and over the top". This "modern era" began in 1993 for Hanks, first with "Sleepless in Seattle" and then with "Philadelphia". The former was a blockbuster success about a widower who finds true love over the radio airwaves. Richard Schickel of "TIME" called his performance "charming," and most critics agreed that Hanks' portrayal ensured him a place among the premier romantic-comedy stars of his generation.
In "Philadelphia", he played a gay lawyer with AIDS who sues his firm for discrimination. Hanks lost 35 pounds and thinned his hair in order to appear sickly for the role. In a review for "People", Leah Rozen stated, "Above all, credit for "Philadelphia"‍ '​s success belongs to Hanks, who makes sure that he plays a character, not a saint. He is flat-out terrific, giving a deeply felt, carefully nuanced performance that deserves an Oscar." Hanks won the 1993 Academy Award for Best Actor for his role in "Philadelphia". During his acceptance speech, he revealed that his high school drama teacher Rawley Farnsworth and former classmate John Gilkerson, two people with whom he was close, were gay.
Hanks followed "Philadelphia" with the 1994 hit "Forrest Gump" which grossed a worldwide total of over $600 million at the box office. Hanks remarked: "When I read the script for "Gump", I saw it as one of those kind of grand, hopeful movies that the audience can go to and feel ... some hope for their lot and their position in life ... I got that from the movies a hundred million times when I was a kid. I still do." Hanks won his second Best Actor Academy Award for his role in "Forrest Gump", becoming only the second actor to have accomplished the feat of winning consecutive Best Actor Oscars. (Spencer Tracy was the first, winning in 1937–38. Hanks and Tracy were the same age at the time they received their Academy Awards: 37 for the first and 38 for the second.)
Hanks' next role—astronaut and commander Jim Lovell, in the 1995 film "Apollo 13"—reunited him with Ron Howard. Critics generally applauded the film and the performances of the entire cast, which included actors Kevin Bacon, Bill Paxton, Gary Sinise, Ed Harris, and Kathleen Quinlan. The movie also earned nine Academy Award nominations, winning two. Later that year, Hanks starred in the Disney•Pixar film "Toy Story", as the voice of Sheriff Woody.
Continued success (1996–99).
Hanks made his directing debut with his 1996 film "That Thing You Do!" about a 1960s pop group, also playing the role of a music producer. Hanks and producer Gary Goetzman went on to create Playtone, a record and film production company named after the record company in the film.
Hanks executive produced, co-wrote, and co-directed the HBO docudrama "From the Earth to the Moon". The 12-part series chronicled the space program from its inception, through the familiar flights of Neil Armstrong and Jim Lovell, to the personal feelings surrounding the reality of moon landings. The Emmy Award-winning project was, at US$68 million, one of the most expensive ventures undertaken for television.
Hanks' next project was no less expensive. For "Saving Private Ryan", he teamed up with Steven Spielberg to make a film about a search through war-torn France after D-Day to bring back a soldier. It earned the praise and respect of the film community, critics, and the general public. It was labeled one of the finest war films ever made and earned Spielberg his second Academy Award for direction, and Hanks another Best Actor nomination. Later in 1998, Hanks re-teamed with his "Sleepless in Seattle" co-star Meg Ryan for "You've Got Mail", a remake of 1940's "The Shop Around the Corner". In 1999, Hanks starred in an adaptation of the Stephen King novel "The Green Mile". He also returned as the voice of Woody in "Toy Story 2", the sequel to "Toy Story". The following year, he won a Golden Globe for Best Actor and an Academy Award nomination for his portrayal of a marooned FedEx systems analyst in Robert Zemeckis's "Cast Away".
International recognition (2000–09).
In 2001, Hanks helped direct and produce the Emmy-Award winning HBO miniseries "Band of Brothers". He also appeared in the September 11 television special "" and the documentary "Rescued From the Closet". He then teamed up with "American Beauty" director Sam Mendes for the adaptation of Max Allan Collins's and Richard Piers Rayner's graphic novel "Road to Perdition", in which he played an anti-hero role as a hitman on the run with his son. That same year, Hanks collaborated once again with director Spielberg, starring opposite Leonardo DiCaprio in the hit crime comedy "Catch Me If You Can", based on the true story of Frank Abagnale, Jr. The same year, Hanks and his wife Rita Wilson produced the hit movie "My Big Fat Greek Wedding". In August 2007, he along with co-producers Rita Wilson and Gary Goetzman, and writer and star Nia Vardalos, initiated a legal action against the production company Gold Circle Films for their share of profits from the movie. At the age of 45, Hanks became the youngest ever recipient of the American Film Institute's Life Achievement Award on June 12, 2002.
In 2004, he appeared in three films: The Coen brothers' "The Ladykillers", another Spielberg film, "The Terminal", and "The Polar Express", a family film from Robert Zemeckis. In a "USA Weekend" interview, Hanks discussed how he chooses projects: "[Since] "A League of Their Own", it can't be just another movie for me. It has to get me going somehow ... There has to be some all-encompassing desire or feeling about wanting to do that particular movie. I'd like to assume that I'm willing to go down any avenue in order to do it right". In August 2005, Hanks was voted in as vice president of the Academy of Motion Picture Arts and Sciences.
Hanks next starred in the highly anticipated film "The Da Vinci Code", based on the best-selling novel by Dan Brown. The film was released May 19, 2006, in the US and grossed over US$750 million worldwide. He followed the film with Ken Burns's 2007 documentary "The War". For the documentary, Hanks did voice work, reading excerpts from World War II-era columns by Al McIntosh. In 2006, Hanks topped a 1,500-strong list of "most trusted celebrities" compiled by "Forbes" magazine.
Hanks next appeared in a cameo role as himself in "The Simpsons Movie", in which he appeared in an advertisement claiming that the U.S. government has lost its credibility and is hence buying some of his. He also made an appearance in the credits, expressing a desire to be left alone when he is out in public. Later in 2006, Hanks produced the British film "Starter for Ten", a comedy based on working class students attempting to win on "University Challenge".
In 2007, Hanks starred in Mike Nichols's film "Charlie Wilson's War" (written by screenwriter Aaron Sorkin) in which he played Democratic Texas Congressman Charles Wilson. The film opened on December 21, 2007, and Hanks received a Golden Globe nomination. In the comedy-drama film "The Great Buck Howard" (2008), Hanks played the on-screen father of a young man (played by Hanks' real-life son, Colin) who chooses to follow in the footsteps of a fading magician (John Malkovich). His character was less than thrilled about his son's career decision. In the same year, he executive produced the musical comedy, "Mamma Mia" and the miniseries, "John Adams".
Hanks' next endeavor, released on May 15, 2009, was a film adaptation of "Angels & Demons", based on the novel of the same name by Dan Brown. Its April 11, 2007, announcement revealed that Hanks would reprise his role as Robert Langdon, and that he would reportedly receive the highest salary ever for an actor. The following day he made his 10th appearance on NBC's "Saturday Night Live", impersonating himself for the "Celebrity Jeopardy" sketch. Hanks produced the Spike Jonze film "Where The Wild Things Are", based on the children's book by Maurice Sendak in 2009.
Recent projects (2010–present).
In 2010, Hanks reprised his voice role of Woody in "Toy Story 3", after he, Tim Allen, and John Ratzenberger were invited to a movie theater to see a complete story reel of the movie. The film went on to become the first animated film to gross a worldwide total of over $1 billion as well as the highest grossing animated film of all time. This record held until the 2013 Disney film, "Frozen", surpassed it. He also was executive producer of the miniseries, "The Pacific".
In 2011, he directed and starred opposite Julia Roberts in the title role in the romantic comedy "Larry Crowne". The movie received poor reviews, with only 35% of the 175 Rotten Tomatoes reviews giving it high ratings. Also in 2011, he starred in the drama film "Extremely Loud and Incredibly Close". In 2012, he voiced the character Cleveland Carr for a web series he created titled "Electric City". He also starred in the Wachowskis-directed film adaptation of the novel of the same name, "Cloud Atlas" and was executive producer of the miniseries "Game Change".
In 2013, Hanks starred in two critically acclaimed films—"Captain Phillips "and" Saving Mr. Banks—"which each earned him praise, including a BAFTA Award for Best Actor in a Leading Role and Golden Globe Award for Best Actor – Motion Picture Drama for the former role. In "Captain Phillips", he starred as Captain Richard Phillips with Barkhad Abdi, which was based on the Maersk Alabama hijacking. In "Saving Mr. Banks", co-starring Emma Thompson and directed by John Lee Hancock, he played Walt Disney, being the first actor to portray Disney in a mainstream film. That same year, Hanks made his Broadway debut, starring in Nora Ephron's "Lucky Guy", for which he was nominated for the Tony Award for Best Actor in a Play.
In 2014, Hanks' short story was published in the October 27 issue of "The New Yorker". Revolving around four friends who make a voyage to the moon, the short story is titled after the Apollo 12 astronaut Alan Bean. "Slate" magazine's Katy Waldman found Hanks' first published short story "mediocre", writing that "Hanks' shopworn ideas about technology might have yet sung if they hadn't been wrapped in too-clever lit mag-ese". In an interview with "The New Yorker", Hanks said he has always been fascinated by space. He told the magazine that he built plastic models of rockets when he was a child and watched live broadcasts of space missions back in the 1960s.
In March 2015, Hanks appeared in the Carly Rae Jepsen music video for "I Really Like You", lip-syncing most of the song's lyrics as he goes through his daily routine.
Personal life.
Hanks was married to American actress Samantha Lewes from 1978 until they divorced in 1987. Together, the couple had two children, son Colin Hanks, born in 1977, and daughter Elizabeth Hanks.
In 1988, Hanks married actress Rita Wilson, with whom he costarred in the film "Volunteers" They have two sons. The elder, Chester (Chet) Marlon Hanks, had a minor role as a student in "Indiana Jones and the Kingdom of the Crystal Skull" and released a rap single in 2011. Their younger, Truman Theodore, was born in 1995.
On October 7, 2013, on "The Late Show with David Letterman", Hanks announced that he has Type 2 diabetes.
Before marrying his second wife, Hanks converted to the Greek Orthodox Church, the religion of Wilson and her family. Hanks said, "I must say that when I go to church—and I do go to church—I ponder the mystery. I meditate on the 'why?' of 'why people are as they are' and 'why bad things happen to good people,' and 'why good things happen to bad people'... The mystery is what I think it is, almost, the grand unifying theory of mankind."
Hanks collects manual typewriters, writing an article about them in "The New York Times" in August 2013.
Politics.
Hanks has made donations to many Democratic politicians and has been open about his support for same-sex marriage, environmental causes, and alternative fuels. Hanks made public his presidential candidate choice in the 2008 election, uploading a video to his MySpace account in which he announced his endorsement of Barack Obama.
A proponent of environmentalism, Hanks is an investor in electric vehicles and owns both a Toyota RAV4 EV and the first production AC Propulsion eBox. Hanks was a lessee of an EV1 before it was recalled, as chronicled in the documentary "Who Killed the Electric Car?" He was on the waiting list for an Aptera 2 Series.
Hanks was extremely outspoken about his opposition to the 2008 Proposition 8, an amendment to the California constitution that defined marriage as a union only between a man and a woman. Hanks and others who were in opposition to the proposition raised over US$44 million, in contrast to the supporters' $39 million, but Proposition 8 passed with 52% of the vote. It was only overruled in June 2013, when the Ninth Circuit lifted its stay of the district court's ruling, enabling Governor Jerry Brown to order same-sex marriage officiations to resume.
While premiering a TV series in January 2009, Hanks called supporters of Proposition 8 "un-American" and criticized the LDS Church members, who were major proponents of the bill, for their views on marriage and their role in supporting the bill. About a week later, Hanks apologized for the remark, saying that nothing is more American than voting one's conscience. Hanks narrated a video created by Obama for America, titled "The Road We've Traveled".
Other activities.
A supporter of NASA's manned space program, Hanks has said that he originally wanted to be an astronaut, but "didn't have the math." Hanks is a member of the National Space Society, serving on the Board of Governors of the nonprofit educational space advocacy organization founded by Dr. Wernher Von Braun. He also produced the HBO miniseries "From the Earth to the Moon" about the Apollo program to send astronauts to the moon. In addition, Hanks co-wrote and co-produced "", an IMAX film about the moon landings. Hanks provided the voice over for the premiere of the show "Passport to the Universe" at the Rose Center for Earth and Space in the Hayden Planetarium at the American Museum of Natural History in New York.
In 2006, the Space Foundation awarded Hanks the Douglas S. Morrow Public Outreach Award, given annually to an individual or organization that has made significant contributions to public awareness of space programs.
In June 2006, Hanks was inducted as an honorary member of the United States Army Rangers Hall of Fame for his accurate portrayal of a Captain in the movie "Saving Private Ryan"; Hanks, who was unable to attend the induction ceremony, was the first actor to receive such an honor. In addition to his role in "Saving Private Ryan", Hanks was cited for serving as the national spokesperson for the World War II Memorial Campaign, for being the honorary chairperson of the D-Day Museum Capital Campaign, and for his role in writing and helping to produce the Emmy Award-winning miniseries, "Band of Brothers". On March 10, 2008, Hanks was on hand at the Rock & Roll Hall of Fame to induct the Dave Clark Five.
Legacy and impact.
Hanks is perceived to be amiable and congenial to his fans. In 2013, when he was starring in Nora Ephron's Lucky Guy on Broadway, he had crowds of 300 fans waiting for a glimpse of him after every performance. This is the highest number of expectant fans post-show of any Broadway performance.
Hanks is ranked as the highest all-time box office star with over $3.639 billion total box office gross, an average of $107 million per film. He has been involved with 17 films that grossed over $100 million at the worldwide box office, the highest grossing of which was 2010's "Toy Story 3". As of 2014, Hanks' films have grossed over $4.2 billion at the United States and Canada box office and over $8.4 billion worldwide, making him the highest all-time box office star. Asteroid 12818 Tomhanks is named after him.

</doc>
<doc id="43569" url="http://en.wikipedia.org/wiki?curid=43569" title="Ghost town">
Ghost town

A ghost town is an abandoned village, town or city, usually one which contains substantial visible remains. A town often becomes a ghost town because the economic activity that supported it has failed, or due to natural or human-caused disasters such as floods, government actions, uncontrolled lawlessness, war, or nuclear disasters. The term can sometimes refer to cities, towns, and neighborhoods which are still populated, but significantly less so than in years past; for example those affected by high levels of unemployment and dereliction.
Some ghost towns, especially those that preserve period-specific architecture, have become tourist attractions. Some examples are Bannack, Montana; Calico, California; Centralia, Pennsylvania; and Oatman, Arizona in the United States; Barkerville, British Columbia in Canada; Craco in Italy; Elizabeth Bay and Kolmanskop in Namibia; and Pripyat in Ukraine. Visiting, writing about, and photographing ghost towns is a minor industry. A recent modern-day example is Ōkuma, Fukushima, which was abandoned due to the Fukushima Daiichi nuclear disaster in 2011.
There is a ghost town that is an incumbent de jure capital: Plymouth in Montserrat.
Definition.
The definition of a ghost town varies between individuals, and between cultures. Some writers discount settlements that were abandoned as a result of a natural or human-made disaster, using the term only to describe settlements which were deserted because they were no longer economically viable; T. Lindsey Baker, author of "Ghost Towns of Texas", defines a ghost town as "a town for which the reason for being no longer exists". Some believe that any settlement with visible tangible remains should not be called a ghost town; others say, conversely, that a ghost town should contain the tangible remains of buildings. Whether or not the settlement must be completely deserted, or may contain a small population, is also a matter for debate. Generally, though, the term is used in a looser sense, encompassing any and all of these definitions. The American author Lambert Florin's preferred definition of a ghost town was simply "a shadowy semblance of a former self".
Reasons for abandonment.
Factors leading to abandonment of towns include depleted natural resources, economic activity shifting elsewhere, railroads and roads bypassing or no longer accessing the town, human intervention, disasters, massacres, wars, and the shifting of politics or fall of empires. A town can also be abandoned when it is part of an exclusion zone due to natural or man-made causes.
Economic activity shifting elsewhere.
Ghost towns may result when the single activity or resource that created a boomtown (e.g., nearby mine, mill or resort) is depleted or the resource economy undergoes a "bust" (e.g., catastrophic resource price collapse). Boomtowns can often decrease in size as fast as they initially grew. Sometimes, all or nearly the entire population can desert the town, resulting in a ghost town.
The dismantling of a boomtown can often occur on a planned basis. Mining companies nowadays will create a temporary community to service a mine site, building all the accommodation shops and services, and then remove it as the resource is worked out. A gold rush would often bring intensive but short-lived economic activity to a remote village, only to leave a ghost town once the resource was depleted.
In some cases, multiple factors may remove the economic basis for a community; some former mining towns on U.S. Route 66 suffered both mine closures when the resources were depleted and loss of highway traffic as US 66 was diverted away from places like Oatman, Arizona onto a more direct path.
The Middle East has many ghost towns that were created when the shifting of politics or the fall of empires caused capital cities to be socially or economically non-viable, such as Ctesiphon.
Human intervention.
Railroads and roads bypassing or no longer accessing a town can create a ghost town. This was the case in many of the ghost towns along Ontario's historic Opeongo Line, and along U.S. Route 66 after motorists bypassed the latter on the faster moving highways I-44 and I-40. Some current ghost towns were originally founded along railways where steam trains formerly stopped at periodic intervals to take on water. Amboy, California was part of one such series of villages along the Atlantic and Pacific Railroad across the Mojave Desert.
River re-routing is another factor, one example being the towns along the Aral Sea.
Ghost towns may be created when land is expropriated by a government and residents are required to relocate. One example is the village of Tyneham in Dorset, England, acquired during World War II to build an artillery range.
A similar situation occurred in the U.S. when NASA acquired land to construct the John C. Stennis Space Center (SSC), a rocket testing facility in Hancock County, Mississippi (on the Mississippi side of the Pearl River, which is the Mississippi-Louisiana state line). This required NASA to acquire a large (approximately 34 sqmi) buffer zone because of the loud noise and potential dangers associated with testing such rockets. Five thinly populated rural Mississippi communities (Gainesville, Logtown, Napoleon, Santa Rosa, and Westonia), plus the northern portion of a sixth (Pearlington), along with 700 families in residence, had to be completely relocated off the facility.
Sometimes the town might cease to officially exist, but the physical infrastructure remains. For example, the five Mississippi communities that had to be abandoned to build SSC still have remnants of those communities within the facility itself. These include city streets (now overgrown with forest flora and fauna) and a one-room school house. Another example of infrastructure remaining is the former town of Weston, Illinois, which voted itself out of existence and turned the land over for construction of the Fermi National Accelerator Laboratory. Many houses and even a few barns remain, used for housing visiting scientists and storing maintenance equipment, while roads that used to cross through the site have been blocked off at the edges of the property, with gatehouses or simply barricades to prevent unsupervised access.
Flooding by dams.
Construction of dams has produced ghost towns that have been left underwater. Examples include the settlement of Loyston, Tennessee, U.S., inundated by the creation of Norris Dam. The town was reorganised and reconstructed on nearby higher ground. Other examples are The Lost Villages of Ontario flooded by Saint Lawrence Seaway construction in 1958, the hamlets of Nether Hambleton and Middle Hambleton in Rutland, England, which were flooded to create Rutland Water, and the villages of Ashopton and Derwent, England, flooded during the construction of the Ladybower Reservoir. Mologa in Russia was flooded by the creation of Rybinsk reservoir, and in France the Tignes Dam flooded the village of Tignes, displacing 78 families. Many ancient villages had to be abandoned during construction of the Three Gorges Dam in China, leading to displacement of many rural people. In the Costa Rican province of Guanacaste, the town of Arenal was rebuilt to make room for the man-made Lake Arenal. The old town now lies submerged below the lake. Old Adaminaby was flooded by a dam of the Snowy River Scheme.
Massacres.
Some towns become deserted when their populations are massacred. The original French village at Oradour-sur-Glane was destroyed on 10 June 1944 when 642 of its 663 inhabitants, including women and children, were killed by a German Waffen-SS company. A new village was built after the war on a nearby site, and the ruins of the original have been maintained as a memorial. No ghost towns exist however at the massacred Czech villages of Lidice (where its 173 male occupants were executed on 10 June 1942) and Ležáky (where all of its 33 adults were shot on 24 June 1942); there, both villages were systematically levelled to the ground to leave no trace of their existence.
Disasters, actual and anticipated.
Natural and man-made disasters can create ghost towns. For example, after being flooded more than 30 times since their town was founded in 1845, residents of Pattonsburg, Missouri, decided to relocate after two floods in 1993. With government help, the whole town was rebuilt 3 mi away.
Craco, a medieval village in the Italian region of Basilicata, was evacuated after a landslide in 1963. Nowadays it is a famous filming location for many movies, to name a few "The Passion of The Christ" by Mel Gibson, "Christ Stopped at Eboli" by Francesco Rosi, "The Nativity Story" by Catherine Hardwicke and "Quantum of Solace" by Marc Forster.
In 1984, Centralia, Pennsylvania was abandoned due to an uncontainable mine fire, which began in 1962 and still rages to this day; eventually the fire reached an abandoned mine underneath the nearby town of Byrnesville, Pennsylvania, which caused that mine to catch on fire too and forced the evacuation of that town as well.
Ghost towns may also occasionally come into being due to an "anticipated" natural disaster – for example, the Canadian town of Lemieux, Ontario was abandoned in 1991 after soil testing revealed that the community was built on an unstable bed of Leda clay. Two years after the last building in Lemieux was demolished, a landslide swept part of the former town-site into the South Nation River. Two decades earlier, the Canadian town of Saint-Jean-Vianney, Québec, also constructed on a Leda clay base, had been abandoned after a landslide on 4 May 1971, which swept away 41 homes, killing 31 people.
Following the Chernobyl disaster of 1986, dangerously high levels of nuclear radiation escaped into the surrounding area, and nearly 200 towns and villages in Ukraine and neighbouring Belarus were evacuated, including the cities of Chernobyl and Pripyat. The area was, and still is, so contaminated with nuclear radiation that many of the evacuees were never permitted to return to their homes. Pripyat is the most famous of these abandoned towns; it was built for the workers of the Chernobyl Nuclear Power Plant and had a population of almost 50,000 at the time of the disaster. A similarly large-scale evacuation followed the 2011 Fukushima Daiichi nuclear disaster in Japan, and some towns within the exclusion zone, such as Tomioka, remain abandoned.
Disease and contamination.
Significant fatality rates from epidemics have produced ghost towns. Some places in eastern Arkansas were abandoned after over 7,000 Arkansans died during the Spanish Flu epidemic of 1918 and 1919. Several communities in Ireland, particularly in the west of the country, were wiped out due to the Great Famine in the latter half of the 19th century, and the years of economic decline that followed.
Catastrophic environmental damage caused by long-term contamination can also create a ghost town. Some notable examples are Times Beach, Missouri, whose residents were exposed to a high level of dioxins, and Wittenoom, Western Australia, which was once Australia's largest source of blue asbestos, but was shut down in 1966 due to health concerns. Treece and Picher, twin communities straddling the Kansas-Oklahoma border, were once one of the United States' largest sources of zinc and lead, but over a century of unregulated disposal of mine tailings led to groundwater contamination and lead poisoning in the town's children, eventually resulting in a mandatory Environmental Protection Agency buyout and evacuation. Also contaminations due to ammunition caused by military use may lead to the development of ghost towns.
Revived ghost towns.
A few ghost towns get a second life, often due to heritage tourism generating a new economy able to support residents. For example Walhalla, Victoria, Australia, became almost deserted after its gold mine ceased operation in 1914, but owing to its accessibility and proximity to other attractive locations it has had a recent economic and holiday population surge.
Alexandria, the second largest city of Egypt, was a flourishing city in the Ancient era, but declined during the Middle Ages. It underwent a dramatic revival during the 19th century; from a population of 5,000 in 1806, it grew into a city of over 200,000 inhabitants by 1882, and is now home to over four million people.
In Algeria, many cities became hamlets after the end of Late Antiquity. They were revived with shifts in population during and after French colonization of Algeria. Oran, currently the nation's second largest city with 1 million people, was a village of only a few thousand people before colonization.
Foncebadón, a village in León, Spain that was mostly abandoned and only inhabited by a mother and son, is slowly being revived owing to the ever-increasing stream of pilgrims on the road to Santiago de Compostela.
Ghost towns around the world.
Africa.
Wars and rebellions in some African countries have left many towns and villages deserted. Since 2003, when President François Bozizé came to power, thousands of citizens of the Central African Republic have been forced to flee their homes as a result of the escalating conflict between armed rebels and government troops. Villages accused of supporting the rebels, such as Beogombo Deux near Paoua, are ransacked by government soldiers; those who are not killed have no choice but to escape to refugee camps. The instability in the region also leaves organized and well-equipped bandits free to terrorize the populace, often leaving villages abandoned in their wake. Elsewhere in Africa, the town of Lukangol was burnt to the ground during tribal clashes in South Sudan. Before its destruction, the town had a population of 20,000.
Many of the ghost towns in mineral-rich Africa are former mining towns. Shortly after the start of the 1908 diamond rush in German South-West Africa, now known as Namibia, the German Imperial government claimed sole mining rights by creating the Sperrgebiet ("forbidden zone"), effectively criminalizing new settlement. The small mining towns of this area, among them Pomona, Elizabeth Bay and Kolmanskop, were exempt from this ban, but the denial of new land claims soon rendered all of them ghost towns.
Asia.
In most countries, ghost towns are towns which were occupied and then abandoned. In China, there are ghost cities that have never been occupied. In India, Dhanushkodi and Bhangarh Fort are among the popular examples. 
Many abandoned towns and settlements in the former Soviet Union were established near Gulag concentration camps to supply necessary services. Since most of these camps were abandoned in the 1950s, the towns were abandoned as well. One such town is located near the former Gulag camp called Butugychag (also called Lower Butugychag). Other towns were deserted due to deindustrialisation and the economic crises of the early 1990s attributed to post-Soviet conflicts.
A recent example of a ghost town is Ōkuma, Fukushima in Japan after the 2011 Tōhoku earthquake and tsunami.
Antarctica.
The oldest ghost town in Antarctica is on Deception Island, where in 1906, a Norwegian-Chilean company set up a whaling station at Whalers Bay, which they used as a base for their factory ship, the "Gobernador Bories". Other whaling operations followed suit, and by 1914 there were thirteen factory ships based there. The station ceased to be profitable during the Great Depression, and was abandoned in 1931. In 1969, the station was partially destroyed by a volcanic eruption. There are also many abandoned scientific and military bases in Antarctica, especially in the Antarctic Peninsula.
The Antarctic island of South Georgia used to have several thriving whaling settlements during the first half of the 20th century, with a combined population exceeding 2,000 in some years. These included Grytviken (operating 1904-64), Leith Harbour (1909–65), Ocean Harbour (1909–20), Husvik (1910–60), Stromness (1912–61) and Prince Olav Harbour (1917–34). The abandoned settlements have become increasingly dilapidated, and remain uninhabited nowadays except for the Museum curator's family at Grytviken. The jetty, the church, and dwelling and industrial buildings at Grytviken have recently been renovated by the South Georgian Government, becoming a popular tourist destination. Some historical buildings in the other settlements are being restored as well.
Europe.
Urbanization – the migration of a country's rural population into the cities – has left many European towns and villages deserted. An increasing number of settlements in Bulgaria are becoming ghost towns for this reason; at the time of the 2011 census, the country had 181 uninhabited settlements. In Hungary, dozens of villages are also threatened with abandonment. The first village officially declared as "dead" was Gyűrűfű in the late 1970s, but later it was repopulated as an eco-village. Some other depopulated villages were successfully saved as small rural resorts, such as Kán, Tornakápolna, Szanticska, Gorica, and Révfalu.
In Spain, large zones of the mountainous Iberian System and the Pyrenees have undergone heavy depopulation since the early 20th century, leaving a string of ghost towns in areas such as the Solana Valley. Traditional agricultural practices such as sheep and goat rearing, on which the mountain village economy was based, were not taken over by the local youth, especially after the lifestyle changes that swept over rural Spain during the second half of the 20th century.
In the United Kingdom, thousands of villages were abandoned during the Middle Ages, as a result of Black Death, climate change, revolts, and enclosure, the process by which vast amounts of farmland became privately owned. Since there are rarely any visible remains of these settlements, they are not generally considered ghost towns; instead, they are referred to in archaeological circles as deserted medieval villages.
Sometimes, wars and genocide end a town's life. In 1944, occupying German Waffen-SS troops murdered the population of the French village Oradour-sur-Glane. A new settlement was built nearby after the war, but the old town was left depopulated on the orders of President Charles de Gaulle, as a permanent memorial. In Germany, numerous smaller towns and villages in the former eastern territories were completely destroyed in the last two years of the war. These territories later became part of Poland and the Soviet Union, and many of the smaller settlements were never rebuilt or repopulated. Some villages in England were also abandoned during the war, but for different reasons. Imber and Tyneham, along with several villages in the Stanford Battle Area, were commandeered by the War Office for use as training grounds for British and US troops. Although this was intended to be a temporary measure, the residents were never allowed to return, and the villages have been used for military training ever since.
Disasters have played a part in the abandonment of settlements within Europe. After the Chernobyl disaster of 1986, the cities of Pripyat and Chernobyl were evacuated due to dangerous radiation levels within the area. As of today, Pripyat remains completely abandoned, and Chernobyl has around 500 remaining inhabitants.
North America.
Canada.
There are ghost towns in parts of British Columbia, Alberta, Ontario, Saskatchewan, Newfoundland and Labrador, and Quebec. Some were logging towns or dual mining and logging sites, often developed at the behest of the company. In Alberta and Saskatchewan, most ghost towns were once farming communities that have since died off due to the removal of the railway through the town or the bypass of a highway. The ghost towns in British Columbia were predominantly mining towns and prospecting camps as well as canneries and, in one or two cases, large smelter and pulp mill towns. British Columbia has more ghost towns than any other jurisdiction on the North American continent, with one estimate at the number of abandoned and semi-abandoned towns and localities upwards of 1500. Among the most notable are Anyox, Kitsault, and Ocean Falls.
Some ghost towns have revived their economies and populations due to historical and eco-tourism, such as Barkerville. Barkerville, once the largest town north of Kamloops, is now a year-round Provincial Museum. In Québec, Val-Jalbert is a well-known tourist ghost town; founded in 1901 around a mechanical pulp mill which became obsolete when paper mills began to break down wood fibre by chemical means, it was abandoned when the mill closed in 1927 and re-opened as a park in 1960.
United States.
There are many ghost towns, or semi-ghost towns (some of them unincorporated communities), in the American Great Plains, the rural areas of which have lost a third of their population since 1920. Thousands of communities in the northern plains states (such as Montana, Nebraska, North Dakota, and South Dakota) became railroad ghost towns when a rail line failed to materialize. Hundreds more were abandoned when the US Highway System replaced the railroads as the United States' favorite mode of travel. Ghost towns are common in mining or old mill town areas: Arizona, California, Colorado, Minnesota, Montana, Nevada, New Mexico, Oregon, and Washington in the western United States and West Virginia in the eastern USA. Some unincorporated towns become ghost towns due to flooding for man made lakes, such as Oketeyeconne. They can be observed as far south as Arkansas, Florida, Georgia, Louisiana, and Texas. When the resources that had created an employment boom in these towns were consumed, the businesses ceased to exist, and the people moved to more productive areas. Sometimes a ghost town consists of many old abandoned buildings (as in Bodie); elsewhere, there remain only foundations of former buildings (e.g., Graysonia). Old mining camps that have lost most of their population at some stage of their history, such as Aspen, Central City, Crested Butte, Cripple Creek, Deadwood, Marysville, Oatman, Park City, St. Elmo, Tombstone, and Virginia City, are sometimes included in the category, although they are active towns and cities today.
Some of the earliest settlements in the US, though they no longer exist in any tangible sense, once had the characteristics of a ghost town. Jamestown, the first permanent English settlement in the Americas, was abandoned in 1699 when Williamsburg became the new capital of the colony; the Zwaanendael Colony became a ghost town in 1632, when every one of the colonists were massacred by Indians; and in 1590, mapmaker John White arrived at the Roanoke Colony in North Carolina to find it deserted, its inhabitants having vanished without a trace.
Starting in 2002, an attempt to declare an "official ghost town" in California stalled when the adherents of the town of Bodie, in Northern California, and those of Calico, in Southern California, could not agree on the most deserving settlement for the recognition. A compromise was eventually reached – Bodie became the "official state gold rush ghost town", while Calico was named the "official state silver rush ghost town".
On April 10, 2015, the West Texas Historical Association at its 92nd annual meeting at Amarillo College in Amarillo, presented a program on ghost towns in Texas. Scheduled participants were James B. Hays of Brownwood, "Walthall and the Early Settlement of Southern Runnels County"; Mildred Sentell of Snyder, "Black Gold and the Ghost Town of Burnham, Garza County", and Christena Stephens of Nazareth, Texas, "How Mortality Records Can Provide a Historical Picture of a German Community."
Latin America.
In the late 19th and early 20th centuries, a wave of European immigrants arrived in Argentina and settled in the cities, which offered jobs, education, and other opportunities that enabled newcomers to enter the middle class. Many also settled in the growing small towns along the expanding railway system. Since the 1930s, many rural workers have moved to the big cities. Other ghost towns were created in the aftermath of dinosaur fossil rushes.
A number of ghost towns throughout Latin America were once mining camps or lumber mills, such as the many saltpeter mining camps that prospered in Chile from the end of the Saltpeter War until the invention of synthetic saltpeter during World War I. Some of these towns, such as the Humberstone and Santa Laura Saltpeter Works in the Atacama Desert, have been declared UNESCO World Heritage Sites. Another former mining town, Real de Catorce in Mexico, has been used as a backdrop for Hollywood movies such as "The Mexican", "Bandidas", and "The Treasure of the Sierra Madre".
References.
</dl>

</doc>
<doc id="43573" url="http://en.wikipedia.org/wiki?curid=43573" title="Lager Beer Riot">
Lager Beer Riot

The Lager Beer Riot occurred in Chicago, Illinois in 1855 after Mayor Levi Boone, great-nephew of Daniel Boone, renewed enforcement of an old local ordinance mandating that taverns be closed on Sundays and led the city council to raise the cost of a liquor license from $50 per year to $300 per year, renewable quarterly. This move was seen as targeting German and Irish immigrants. On April 21, after several tavern owners were arrested for selling beer on Sunday, protesters clashed with police near the Cook County Court House. Waves of angry immigrants stormed the downtown area and the mayor ordered the swing bridges opened to stop further waves of protestors from crossing the river. This left some trapped on the bridges, police then fired shots at protesters stuck on the Clark Street Bridge over the Chicago River. A policeman named George W. Hunt was shot in the arm by a rioter named Peter Martin. Martin was killed by police, and Hunt's arm had to be amputated. Rumors flew throughout the city that more protesters were killed, although there is no evidence to support this. Loaded cannons set on the public square contributed to these rumors. The following year, after Boone was turned out of office, the prohibition was repealed. This riot concluded in one known death and about sixty arrests.
Background.
Chicago's rapid growth in the 1840s and 50s was due in large part to German and Irish Catholic immigrants. These immigrants settled in their own neighborhoods, German immigrants congregating mainly on the North Side, across the Chicago River from City Hall and the older, Protestant part of the city. The German settlers worked a six-day week, leaving Sunday as their primary day to socialize; much of this socialization took place in the small taverns that dotted the North Side. German-language newspapers, the Turners, and German craft unions gave the German population of Chicago a high degree of political cohesiveness; the Forty-Eighters among them were used to demonstrations as a political tool.
As in much of the rest of the country, distrust of Catholic influence produced a backlash in the form of the “Know-Nothing” movement. In the election of 1854, the Temperance Party candidate, Amos Throop, lost by nearly 20% points to Isaac Lawrence Milliken. Nevertheless, after winning the election, Milliken declared himself in favor of temperance as well. Milliken lost the following year to Levi Boone, the American Party candidate. Boone, a Baptist and temperance advocate, believed that the Sabbath was profaned by having drinking establishments open on Sunday. Boone's actions were in anticipation of Illinois enacting by referendum a Maine law that would prohibit the sale of alcohol for recreational purposes. The referendum failed in June 1855, by a statewide vote of 54% to 46%.

</doc>
<doc id="43577" url="http://en.wikipedia.org/wiki?curid=43577" title="Carmelites">
Carmelites

The Order of the Brothers of Our Lady of Mount Carmel or Carmelites (sometimes simply Carmel by synecdoche; Latin: "Ordo Fratrum Beatissimæ Virginis Mariæ de Monte Carmelo") is a Roman Catholic religious order founded, probably in the 12th century, on Mount Carmel, hence its name. However, historical records about its origin remain uncertain. Saint Bertold has traditionally been associated with the founding of the order, but few clear records of early Carmelite history have survived and this is likely to be a later extrapolation by hagiographers. There is a very small body of Anglican Carmelites.
Charism.
The charism (or spiritual focus) of the Carmelite Order is contemplation. Carmelites understand contemplation in a broad sense encompassing prayer, community, and service. These three elements are at the heart of the Carmelite charism. The most recent statement about the charism of Carmel was in the 1995 Constitutions of the Order, in which Chapter 2 is entirely devoted to the idea of charism. Carmel understands contemplation and action to be complementary, not contradictory. What is distinctive of Carmelites is the way that they practice the elements of prayer, community and service, taking particular inspiration from the prophet Elijah and the Blessed Virgin Mary, patrons of the Order.
The Order is considered by the Church to be under the special protection of the Blessed Virgin Mary, and thus has a strong Marian devotion to Our Lady of Mount Carmel. As in most of the orders dating to medieval times, the First Order is the friars (who are active/contemplative), the Second Order is the nuns (who are cloistered) and the Third Order consists of laypeople who continue to live in the world, and can be married, but participate in the charism of the order by liturgical prayers, apostolates, and contemplative prayer. There are also offshoots such as active Carmelite sisters.
History.
Origins.
Carmelite tradition traces the origin of the order to a community of hermits on Mount Carmel, that succeeded the schools of the prophets in ancient Israel although there are no certain records of hermits on this mountain before the 1190s. By this date a group of men had gathered at the well of Elijah on Mount Carmel. These men, who had gone to Palestine from Europe either as pilgrims or as crusaders, chose Mount Carmel in part because it was the traditional home of Elijah. 
The foundation is believed to have been dedicated to the Blessed Virgin Mary. (The Carmelites were forced to leave the site, and the Holy Land, in 1291, and the original conventual buildings were destroyed several times, but they were able to return in the nineteenth century, when a monastery of Discalced Carmelite friars was built close to the original site under the auspices of Fr. Julius of the Saviour and consecrated on 12 June 1836.)
Some time between 1206 and 1214 the hermits, about whom very little is known, approached the Latin Patriarch of Jerusalem and Papal legate, St. Albert of Jerusalem, for a Rule. (Albert had been responsible for giving a rule to the Humiliati during his long tenure as Bishop of Vercelli, and was well-versed in diplomacy, being sent by Pope Innocent III as Papal Legate to what was known as the Eastern Province.) Albert created a document, the Rule of St Albert, which is both juridically terse and replete with Scriptural allusions, thereby rooting the hermits in the life of the universal Church and their own aspirations.
The rule consisted of sixteen articles, which enjoined strict obedience to their prior, residence in individual cells, constancy in prayer, the hearing of Mass every morning in the oratory of the community, vows of poverty and toil, daily silence from vespers until terce the next morning, abstinence from all forms of meat except in cases of severe illness, and fasting from Holy Cross Day (September 14) until the Easter of the following year.
The Rule of St. Albert addresses a prior whose name is only listed as "B." When later required to name their founders, the Brothers referred to both Elijah and the Blessed Virgin as early models of the community. Later, under pressure from other European Mendicant orders to be more specific, the name "Saint Bertold" was given, possibly drawn from the oral tradition of the Order.
Early history.
Virtually nothing is known of the Carmelites from 1214, when Albert died, until 1238. The Rule of St. Albert was approved by Pope Honorius III in 1226, and again by Pope Gregory IX in 1229, with a modification regarding ownership of property and permission to celebrate divine services. The Carmelites next appear in the historical record, however, when in 1238, with the increasing cleavage between the West and the East, the Carmelites began to find it advisable to leave their original home, and many moved to Cyprus and Sicily.
In 1242 settlements were established at Aylesford, Kent, England, and Hulne, near Alnwick in Northumberland, and two years later in southern France. Settlements were established at Losenham, Kent, and Bradmer, on the north Norfolk coast, before 1247. By 1245 they were so numerous that they were able to hold their first general chapter at Aylesford, where Saint Simon Stock, then eighty years old, was chosen general. During his rule of twenty years the order prospered: foundations were made at London and Cambridge (1247), Marseilles (1248), Cologne (1252), York (before 1253), Monpellier (before 1256), Norwich, Oxford and Bristol (1256), Paris (1258), and elsewhere. By 1274, there were 22 Carmelites houses in England, about the same number in France, eleven in Catalonia, three in Scotland, as well as some in Italy, Germany and elsewhere.
Acknowledging the changed circumstances of life outside the Holy Land, the Carmelites appealed to the papal curia for a modification of the rule. Pope Innocent IV entrusted the drafting of a modified Rule to two Dominicans, and the new rule was then promulgated by Pope Innocent IV in his 1247 Bull "Quem honorem Conditoris". This both brought it closer to the model generally envisaged for mendicant orders in Europe at the time, and made allowances for the changed needs of an Order now based in Europe rather than the Holy Land: for instance, foundations no longer needed to be made in desert places, the canonical office was recited, and abstinence was mitigated.
There is scholarly debate over the significance for the Carmelites of the decree at the Second Council of Lyon in 1274 that no order founded after 1215 should be allowed to continue in existence - an order which put an end to several other mendicant orders including the Sack Friars, and the Pied, Crutched and Apostolic Friars. The Carmelites, as an order whose Rule had only been promulgated by the Pope after 1215, should in theory have been included in this set. Certainly, the rapidly expansion of the Order was halted after 1274, with far fewer houses established in subsequent years. Later Carmelite apologists, from the fourteenth century onwards, however, interpreted the Second Council of Lyon as a confirmation of the order. Such tensions may in part explain why, at a General Chapter in London in 1281, the order asserted that it had ancient origins from Elijah and Elisha at Mount Carmel.
Such tension appears to have lessened under subsequent popes, however. In 1286, Honorius IV confirmed the Carmelite Rule, and in 1298 Boniface VIII formally removed the restrictions placed on the Order by the Second Council of Lyon. In 1326, John XXII's bull "Super Cathedram" extended to the order all the rights and exemptions that existed for the older existing Franciscans and Dominicans, signalling an acceptance of the Carmelites at the heart of Western religious life.
The Order grew quickly once in Europe. By the end of the thirteenth century, the order had around 150 houses in Europe, divided into twelve provinces throughout Europe and the Mediterranean. In England, the Order had 30 houses under four 'distinctions': London, Norwich, Oxford and York, as well as new houses in Scotland and Ireland. It has been estimated that the total Carmelite population in England between 1296 and 1347 was about 720, with the largest house (London), having over 60 friars, but most averaging between 20 and 30.
Reforms.
Quite early in their history the Carmelites began to develop ministries in keeping with their new status as mendicant religious. This resulted in the production in 1270 of a letter "Ignea Sagitta "("Flaming Arrow") by the ruling prior general from 1266 to 1271, Nicholas of Narbonne (also known as Nicholas Gallicus, or Nicholas the Frenchman), who called for a return to a strictly eremitical life, with his belief that most friars were ill-suited to an active apostolate was based on a number of scandals. The letter is symbolic of the tensions the Carmelites experienced in the thirteenth century between their eremitical origins (expressed particular in a desire for solitude and a focus on contemplation) and their more recent transformation into a fundamentally mendicant order (expressed in the desire to respond to the Church's apostolic mission).
By the late 14th century, the Carmelites were becoming increasingly interested in their origins; the lack of a distinctive named founder (by contrast with the Dominicans and Franciscans) may have been a factor in the development of numerous legends surrounding Carmelite origins. One particularly influential book was the "Institution of the First Monks", the first part of a four-part work from the late fourteenth century. It was almost certainly composed by Philip Ribot, Catalonian Carmelite provincial, though Ribot passed off his work as a collection of earlier writings that he merely edited, claiming that the "Institution" itself was written by John XLIV, supposedly patriarch of Jerusalem, who allegedly wrote the text in Greek in 412. The "Institution" tells of the founding of the Carmelite order by the prophet Elijah and a fanciful history of the order in the pre- and early Christian era. It was hugely influential, and has been described as the "chief book of spiritual reading in the Carmelite order" until the seventeenth century.
In the late 14th and 15th centuries the Carmelites, like a number of other religious orders, declined and reform became imperative. In 1432 the Carmelites obtained from Pope Eugenius IV the bull "Romani pontificis", which mitigated the Rule of St Albert and the 1247 modification, on the ground that the original demanded too much of the friars. The main clauses modified concerned fasting and remaining within individual cells: the bull allowed them to eat meat three days a week and to perambulate in the cloisters of their convents. This reform brought the Carmelites closer into line with other mendicant orders, but would be the source of much subsequent tension, as others refused to accept this change in the nature of the Order, seeing it as a loss of Carmel's original vision and spirit.
Such tension erupted almost immediately. Shortly before 1433 three priories in Valais, Tuscany, and Mantua were reformed by the preaching of Thomas Conecte of Rennes and formed the Congregation of Mantua, refusing to accept the mitigation of 1432, and instead insisting on a more severe monastic observance than that applied between 1247 and 1432. Under the Mantuan observance, entrance to the cloister was forbidden to outsiders, the friars were banned from being outside the convent without good reason, and money was distributed from a common chest. In 1443, they obtained a bull from Pope Eugenius IV which effectively declared them independent of the rest of the Order, with its own special set of Constitutions and governed by its own vice prior general. Under the reconciliatory efforts of prior-general Blessed John Soreth (c. 1395–1471; prior-general 1451–71), however, the Mantuan congregation was brought closer to the main Carmelite order, such that in 1462 the Mantuans even accepted parts of the 1432 mitigation.
This was doubtless in part because of Soreth's own reforming impulses. In 1459, for instance, Pope Pius II left the regulation of fasts to the discretion of the prior general; Soreth accordingly sought until his death in 1471 to restore the primitive asceticism.
Soreth also made an important step for the later order, founding the order of Carmelite nuns in 1452 (with authorisation from the papal bull "Cum Nulla"). The first convent, Our Lady of Angels, was in Florence, but the movement rapidly spread to Belgium (in 1452), France, and Spain (with the foundation of the Incarnation in Avila in 1479).
In 1476, a papal bull cum nulla of Pope Sixtus IV founded the Carmelites of the Third Order, who received a special rule in 1635, which was amended in 1678.
Recognition of a need for reform of the Carmelite order existed in the early sixteenth century, and some early attempts at reform were made then, notably from 1523 onwards by Nicholas Audet, vicar-general of the order. His plans saw some fruit: during three years of travels through France and Germany, introducing his reforms into the houses of the order, more than one hundred houses were reformed. Audet met resistance in other places, however: in the Spanish province of Castile, more than half the friars walked away.
Reform in Spain began in earnest in the 1560s, with the work of Saint Teresa of Ávila, who, together with Saint John of the Cross, established the Discalced Carmelites. Teresa's foundations, although welcomed by King Philip II of Spain - who was most anxious for all Orders to be reformed according to the principles of the Council of Trent (1545–1563) - did create practical problems at grassroots level. The proliferation of new religious houses in towns that were already struggling to cope was an unwelcome prospect, creating a backlash from local townspeople to nobility and diocesan clergy. Teresa made a point of trying to make her monasteries as self-sufficient as was practicable and restricted the number of nuns per community accordingly. The Discalced Carmelites also faced much opposition from other unreformed Carmelite houses (famously exemplified in the arrest and imprisonment in their own monastery of John of the Cross by Carmelites from Toledo). Only in the 1580s did the Discalced Carmelites gain official approval of their status. In 1593, the Discalced Carmelites had their own superior general styled propositus general - the first being Fr. Nicholas Doria. Due to the politics of foundation, the Discalced friars in Italy were canonically erected as a separate juridical entity.
The split with the Discalced Carmelites in Spain, and the rise of Protestantism in many other parts of Europe, made the need for wider reform of the Carmelite order even more apparent. This was gradually achieved in the seventeenth century as a result of reforms from within the order begun in the convent in Rennes in 1600 by Pierre Thibault, under rules which became known as the Observance of Rennes. One of the renowned figures of the Observance was Br. John of St. Samson, a blind lay brother whose spiritual direction was highly regarded by many. Renamed as the Reform of Touraine (after the Province of the same name), the Observance of Rennes spread across the Carmelite order, in great part thanks to the efforts for a systematic programme of reform of Fr John Baptist Rossi, prior-general of the order.
Controversies with other orders.
By the middle of the 17th century the Carmelites had reached their zenith. At this period, however, they became involved in controversies with other orders, particularly with the Jesuits. The special objects of attack were the traditional origin of the Carmelites and the source of their scapular. The Sorbonne, represented by Jean Launoy, joined the Jesuits in their polemics against the Carmelites.
Papebroch, the Bollandist editor of the "Acta Sanctorum", was answered by the Carmelite Sebastian of St. Paul, who made such serious charges against the orthodoxy of his opponent's writings that the very existence of the Bollandists was threatened. The peril was averted, however, and in 1696 a decree of Juan Tomás de Rocaberti, archbishop of Valencia and inquisitor-general of the Holy Office, forbade all further controversies between the Carmelites and Jesuits. Two years later, on November 20, 1698, Pope Innocent XII issued a brief which definitely ended the controversy on pain of excommunication, and placed all writings in violation of the brief on the Index Librorum Prohibitorum.
Modern history.
Since the 1430s, the Congregation of Mantua had continued to function in its little corner of Italy. It was only at the end of the 19th century that those following the reform of Tourraine (by this time known as the "strict observance") and the Mantuan Congregation were formally merged under one set of Constitutions. The friars following Mantua conceded to Tourraine's Constitutions but insisted that the older form of the habit - namely their own - should be adopted. In a photograph of the period Blessed Titus Brandsma is shown in the habit of Tourraine as a novice; in all subsequent images he wears that of the newly styled Ancient Observance.
The French Revolution, the secularization in Germany, and the repercussions on religious orders following the unification of Italy were heavy blows to the Carmelites. By the last decades of the 19th century, there were approximately 200 Carmelite men throughout the world. At the beginning of the 20th century, however, new leadership and less political interference allowed a rebirth of the Order. Existing provinces began re-founding provinces that had gone out of existence. The theological preparation of the Carmelites was strengthened, particularly with the foundation of St. Albert's College in Rome.
By 2001, the membership had increased to approximately 2,100 men in 25 provinces, 700 enclosed nuns in 70 monasteries, and 13 affiliated Congregations and Institutes. In addition, the Third Order of lay Carmelites count 25,000-30,000 members throughout the world. Provinces exist in Australia, Brazil, Britain, Canada, Chile, Hungary, Germany, India, Indonesia, Ireland, Italy, Malta, the Netherlands, Poland, Singapore, Spain, Portugal and the United States. Delegations directly under the Prior General exist in Argentina, France, the Czech Republic, the Dominican Republic, Lebanon, the Philippines and Portugal.
Carmelite Missions exist in Bolivia, Burkino Faso, Cameroon, Colombia, India, Kenya, Lithuania, Mexico, Mozambique, Peru, Romania, Tanzania, Trinidad, Venezuela and Zimbabwe.
Monasteries of enclosed Carmelite nuns exist in Brazil, Denmark, the Dominican Republic, Finland, Germany, Hungary, Indonesia, Iceland, Ireland, Italy, Kenya, the Netherlands, New-Zealand (Christchurch NZ since 1933)Nicaragua, Norway, Peru, the Philippines, Spain, Sweden, Portugal, the United Kingdom and the United States. Hermit communities of either men or women exist in Brazil, France, Indonesia, Lebanon, Italy and the United States.
The Discalced Carmelite Order also built the priory of Elijah (1911) at the site of Elijah's epic contest with the prophets of Ba'al (1 Kings 18:20-40). The monastery is situated about 25 kilometers south of Haifa on the eastern side of the Carmel, and stands on the foundations of a series of earlier monasteries. The site is held sacred by Christians, Jews and Muslims; the name of the area, is el-Muhraqa, an Arabic construction meaning "place of burning", and is a direct reference to the biblical account.
There are several Carmelite figures who have received significant attention in the 20th century, including Saint Thérèse of Lisieux, one of only four female Doctors of the Church, so named because of her famous teaching on the "way of confidence and love" set forth in her best-selling memoir, "Story of a Soul"; Titus Brandsma, a Dutch scholar and writer who was killed in Dachau Concentration Camp because of his stance against Nazism; and Saint Teresa Benedicta of the Cross (née Edith Stein), a Jewish convert to Catholicism who was also imprisoned and died at Auschwitz. Saint Raphael Kalinowski (1835–1907) was the first friar to be sainted in the Order since co-founder Saint John of the Cross. The writings and teachings of Brother Lawrence of the Resurrection, a Carmelite friar of the 17th century, continue as a spiritual classic under the title "The Practice of the Presence of God". Other non-religious ("i.e.," non-vowed monastic) great figures include Saint George Preca, a Maltese priest and Carmelite Tertiary.
Habit and scapular.
In 1287, the original way of life of the order was changed to conform to that of the mendicant orders on the initiative of St. Simon Stock and at the command of Pope Innocent IV. Their former habit of a mantle with black and white or brown and white stripes—the black or brown stripes representing the scorches the mantle of Elijah received from the fiery chariot as it fell from his shoulders—was discarded and they wore the same habit as the Dominicans, except that the cloak was white. They also borrowed much from the Dominican and Franciscan constitutions. Their distinctive garment was a scapular of two strips of dark cloth, worn on the breast and back, and fastened at the shoulders. Tradition holds that this was given to St. Simon Stock by the Blessed Virgin Mary, who appeared to him and promised that all who wore it with faith and piety and who died clothed in it would be saved. There arose a sodality of the scapular, which affiliated a large number of laymen with the Carmelites. The order made some grandiose claims, however, contesting the "invention" of the rosary with the Dominicans, terming themselves the brothers of the Virgin, and asserting, on the basis of their traditional association with Elijah, that all the prophets of the Old Testament, as well as the Virgin and the Apostles, had been Carmelites.
A miniature version of the Carmelite scapular is popular among Roman Catholics and is one of the most popular devotions in the Church. Wearers usually believe that if they faithfully wear the Carmelite scapular (also called "the brown scapular" or simply "the scapular") and die in a state of grace, they will be saved from eternal damnation. Catholics who decide to wear the scapular are usually enrolled by a priest, and some choose to enter the Scapular Confraternity. The Lay Carmelites of the Third Order of Our Lady of Mount Carmel wear a scapular which is smaller than the shortened scapular worn by some Carmelite religious for sleeping, but still larger than the devotional scapulars.
Visions and devotions.
Among the various Catholic orders, Carmelite nuns have had a proportionally high ratio of visions of Jesus and Mary and have been responsible for key Catholic devotions.
From the time of her clothing in the Carmelite religious habit (1583) until her death (1607) the life of Saint Mary Magdalene de Pazzi is said to have had a series of raptures and ecstasies.
In the Carmelite convent of Beja, in Portugal, two Carmelite nuns of the Ancient Observance reported several apparitions and mystical revelations throughout her life: Venerable Mother Mariana of the Purification received numerous apparitions of the Child Jesus and her body was found incorrupt after her death; Venerable Mother Maria Perpétua da Luz wrote 60 books with messages from heaven; both religious died with the odor of sanctity.
Sister Antónia d'Astónaco, a Carmelite nun from Portugal, reported during her life a private revelation by Saint Michael the Archangel. Based on that revelation, the Archangel Michael had told in an apparition to the devoted Servant of God that he would like to be honored, and God glorified, by the praying of nine special invocations. These nine invocations correspond to invocations to the nine choirs of angels and origins the Chaplet of Saint Michael. This private revelation and prayers were fully approved by Pope Pius IX in 1851.
Sister Marie of Saint Peter, a Carmelite nun in Tours France, started the devotion to the Holy Face of Jesus. She said that in an 1844 vision Jesus told her: "Oh if you only knew what great merit you acquire by saying even once, Admirable is the Name of God, in a spirit of reparation for blasphemy."
In the 19th century, another Carmelite nun, Saint Thérèse of the Child Jesus and the Holy Face, was instrumental in spreading this devotion throughout France in the 1890s with her many poems and prayers. Eventually Pope Pius XII approved the devotion in 1958 and declared the Feast of the Holy Face of Jesus as Shrove Tuesday (the day before Ash Wednesday) for all Catholics. Therese of Lisieux emerged as one of the most popular saints for Catholics in the 20th century, and a statue of her can be found in many European and North American Catholic churches built prior to the Second Vatican Council (after which the number of statues tended to be reduced when churches were built).
In the 20th century, in the last apparition of the Blessed Virgin Mary in Fátima, Portugal, Sister Lúcia, one of the most famous visionaries of Our Lady, said that the Virgin appeared to her as Our Lady of Mount Carmel (holding the Brown Scapular). Many years after, Lúcia became a Carmelite nun. When Sister Lúcia was asked in an interview why the Blessed Virgin appeared as Our Lady of Mount Carmel in her last apparition, she replied: "Because Our Lady wants all to wear the Scapular... The reason for this," she explained, "is that the Scapular is our sign of consecration to the Immaculate Heart of Mary". When asked if the Brown Scapular is as necessary to the fulfillment of Our Lady’s requests as the Rosary, Sister Lúcia answered: "The Scapular and the Rosary are inseparable".

</doc>
<doc id="43581" url="http://en.wikipedia.org/wiki?curid=43581" title="President of Poland">
President of Poland

The President of the Republic of Poland (Polish: "Prezydent Rzeczypospolitej Polskiej", shorter form: "Prezydent RP") is the Polish head of state. His or her rights and obligations are determined in the Constitution of Poland. The president heads the executive branch. In addition the president has a right to dissolve the parliament in certain cases, and represents Poland in the international arena. 
History.
The first president of Poland, Gabriel Narutowicz, was sworn in as president of the Second Republic on 11 December 1922. He was elected by the National Assembly (the Sejm and the Senate) under the terms of the 1921 Constitution of Poland. Previously Józef Piłsudski had been "Chief of State" ("Naczelnik Państwa") under the provisional 1919 Constitution. In 1926, Piłsudski. who was fed up with regional bickering, staged a coup deposed the president and had the National Assembly elect a new one, Ignacy Mościcki, under the thumb of Sanacja. Just before Piłsudski died, parliament passed the 1935 April Constitution of Poland which incorporated Piłsudski's ideals, but was not in accord with the amendment procedures of the 1921 Constitution. Mościcki continued as president until he resigned following the German invasion of Poland in 1939. Mościcki and his government went into exile first into Romania, where Mościcki was interned, then to Angers in France where Władysław Raczkiewicz, at the time the Speaker of the Senate, assumed the presidency following Mościcki's resignation on 29 September 1939, and then on to London. The transfer from Mościcki to Raczkiewicz was in accordance with Article 24 of the 1935 April Constitution.
Following the invasion of Poland by the Red Army in 1944, Bolesław Bierut assumed the reins of government, and in July 1945 was internationally recognized as the head of state, but it wasn't until 1947 when the Sejm passed an interim constitution based in part on the March 1921 constitution, that Bierut was elected president by the Sejm, the Senate having been abolished the previous year by the Polish people's referendum, 1946. He served until the 1952 Constitution eliminated the office of the president.
Following the 1989 amendments to the constitution which restored the presidency, Wojciech Jaruzelski, the existing head of state, took office. In Poland's first direct presidential election, Lech Wałęsa won and was sworn in on 22 December 1990. The office of the president was continued in the Constitution of 1997, which now provides the requirements for, the duties of and the authority of the office.
Election.
The President of Poland is elected directly by the people to serve for five years and can be reelected only once. Pursuant to the provisions of the Constitution, the President is elected by an absolute majority of valid votes. If no candidate succeeds in passing this threshold, a second round of voting is held with the participation of the two candidates who received the largest and second largest number of votes respectively.
In order to be registered as a candidate in the presidential election, one must be a Polish citizen, be at least 35 years old on the day of the first round of the election and collect at least 100,000 signatures of voters.
Powers.
The President has a free choice in selecting the Prime Minister, yet in practice he usually gives the task of forming a new government to a politician supported by the political party with the majority of seats in the Sejm (usually, though not always, it is the leader of that political party).
The President has the right to initiate the legislative process. He also has the opportunity to directly influence it by using his veto to stop a bill; however, his veto can be overruled by a three-fifths majority vote in the presence of at least half of the statutory number of members of the Sejm (230). Before signing a bill into law, the President can also ask the Constitutional Tribunal to verify its compliance with the Constitution, which in practice bears a decisive influence on the legislative process.
In his role as supreme representative of the Polish state, the President has power to ratify and revoke international agreements, nominates and recalls ambassadors, and formally accepts the accreditations of representatives of other states. The President also makes decisions on award of highest academic titles, as well as state distinctions and orders. In addition, he has the right of clemency, viz. he can dismiss final court verdicts (in practice, the President consults such decisions with the Minister of Justice).
The President is also the Supreme Commander of the Armed Forces; he appoints the Chief of the General Staff and the commanders of all of the service branches; in wartime he nominates the Commander-in-Chief of the Armed Forces and can order a general mobilization. The President performs his duties with the help of the following offices: the Chancellery of the President, the Office of National Security, and the Body of Advisors to the President.
Presidential residencies and properties.
Several properties are owned by the Office of the President and are used by the Head of State as his or her official residence, private residence, residence for visiting foreign officials etc.
Acting President of Poland.
The constitution states that the President is an elected office, there is no directly elected presidential line of succession. If the President is unable to execute his/her powers and duties, the marshall of the sejm will have the powers of a President for a maximum of 60 days until elections are called.
On 10 April 2010 a plane carrying Polish President Lech Kaczyński, his wife, and 94 others including many Polish officials crashed near Smolensk-North Airport in Russia. There were no survivors. Bronisław Komorowski took over acting presidential powers following the incident. On 8 July Bronislaw Komorowski resigned from marshall power after winning the presidential election. According to the constitution, the acting president then became the marshall of senate, Bogdan Borusewicz. In the afternoon Grzegorz Schetyna was elected as a new marshall of the Sejm and he became acting president. Schetyna served as the interim head of state until Komorowski's swearing-in on 6 August.
Former Presidents.
Within Poland, former presidents are entitled to lifetime personal security protection by Biuro Ochrony Rządu officers, in addition to receiving a substantial pension and a private office. On 10 April 2010, Lech Kaczyński, president at the time, and Ryszard Kaczorowski, the last president-in-exile although not internationally recognized, died in the crash of the Polish Air Force Tu-154 enroute to Russia.
As of 2015, two former Presidents of Poland are alive:
Also, three former Acting Presidents are alive:

</doc>
<doc id="43582" url="http://en.wikipedia.org/wiki?curid=43582" title="Begging the question">
Begging the question

Begging the question means assuming the conclusion of an argument—a type of circular reasoning. This is an informal fallacy where someone includes the conclusion they are attempting to prove in the initial premise of their argument—often in an indirect way that conceals it.
The term "begging the question" originated in the 16th century as a mistranslation of Latin "petitio principii" ("assuming the initial point"). In modern vernacular usage, "to beg the question" sometimes also means "to raise the question" (as in "This begs the question of whether...") or "to dodge the question". This usage is often proscribed.
History.
The original phrase used by Aristotle from which "begging the question" descends is: τὸ ἐξ ἀρχῆς (or sometimes ἐν ἀρχῇ) αἰτεῖν, "asking for the initial thing." Aristotle's intended meaning is closely tied to the type of dialectical argument he discusses in his "Topics", book VIII: a formalized debate in which the defending party asserts a thesis that the attacking party must attempt to refute by asking yes-or-no questions and deducing some inconsistency between the responses and the original thesis. 
In this stylized form of debate, the proposition that the answerer undertakes to defend is called "the initial thing" (τὸ ἐξ ἀρχῆς, τὸ ἐν ἀρχῇ) and one of the rules of the debate is that the questioner cannot simply ask for it (that would be trivial and uninteresting). Aristotle discusses this in "Sophistical Refutations" and in "Prior Analytics" book II, (64b, 34–65a 9, for circular reasoning see 57b, 18 – 59b, 1).
The stylized dialectical exchanges Aristotle discusses in the "Topics" included rules for scoring the debate, and one important issue was precisely the matter of "asking for the initial thing"—which included not just making the actual thesis adopted by the answerer into a question, but also making a question out of a sentence that was too close to that thesis (for example, "PA" II 16).
The term was translated into English from Latin in the 16th century. The Latin version, "petitio principii", "asking for the starting point," can be interpreted in different ways. "Petitio" (from "peto"), in the post-classical context in which the phrase arose, means "assuming" or "postulating", but in the older classical sense means "petition", "request" or "beseeching". "Principii", genitive of "principium", means "beginning", "basis" or "premise" (of an argument). Literally "petitio principii" means "assuming the premise" or "assuming the original point."
The Latin phrase comes from the Greek τὸ ἐν ἀρχῇ αἰτεῖσθαι ("to en archei aiteisthai", "asking the original point") in Aristotle's "Prior Analytics" II xvi 64b28–65a26:
Begging or assuming the point at issue consists (to take the expression in its widest sense) [of] failing to demonstrate the required proposition. But there are several other ways in which this may happen; for example, if the argument has not taken syllogistic form at all, he may argue from premises which are less known or equally unknown, or he may establish the antecedent by means of its consequents; for demonstration proceeds from what is more certain and is prior. Now begging the question is none of these. [...] If, however, the relation of B to C is such that they are identical, or that they are clearly convertible, or that one applies to the other, then he is begging the point at issue... [B]egging the question is proving what is not self-evident by means of itself...either because predicates which are identical belong to the same subject, or because the same predicate belongs to subjects which are identical.—Aristotle, Hugh Tredennick (trans.) "Prior Analytics"
Aristotle's distinction between apodictic science and other forms of non-demonstrative knowledge rests on an epistemology and metaphysics wherein appropriate first principles become apparent to the trained dialectician:
Aristotle's advice in "'S.E." 27 for resolving fallacies of Begging the Question is brief. If one realizes that one is being asked to concede the original point, one should refuse to do so, even if the point being asked is a reputable belief. On the other hand, if one fails to realize that one has conceded the point at issue and the questioner uses the concession to produce the apparent refutation, then one should turn the tables on the sophistical opponent by oneself pointing out the fallacy committed. In dialectical exchange it is a worse mistake to be caught asking for the original point than to have inadvertently granted such a request. The answerer in such a position has failed to detect when different utterances mean the same thing. The questioner, if he did not realize he was asking the original point, has committed the same error. But if he has knowingly asked for the original point, then he reveals himself to be ontologically confùsed: he has mistaken what is non-self-explanatory (known through other things) to be something self-explanatory (known through itself). In pointing this out to the false reasoner, one is not just pointing out a tactical psychological misjudgment by the questioner. It is not simply that the questioner falsely thought that the original point, if placed under the guise of a semantic equivalent, or a logical equivalent, or a covering universal, or divided up into exhaustive parts, would be more persuasive to the answerer. Rather, the questioner falsely thought that a non-self-explanatory fact about the world was an explanatory first principle. For Aristotle, that certain facts are self-explanatory while others are not is not a reflection solely of the cognitive abilities of humans. It is primarily a reflection of the structure of noncognitive reality. In short, a successful resolution of such a fallacy requires a firm grasp of the correct explanatory powers of things. Without a knowledge of which things are self-explanatory and which are not, the reasoner is liable to find a question-begging argument persuasive.—Scott Gregory Schreiber, "Aristotle on False Reasoning: Language and the World in the Sophistical Refutations"
Thomas Fowler believed that "Petitio Principii" would be more properly called "Petitio Quæsiti", which is literally "begging the question."
Definition.
The fallacy of "petitio principii", or "begging the question" is committed when someone attempts to prove a proposition based on a premise that itself requires proof.
When the fallacy of begging the question is committed in a single step, it is sometimes called a "hysteron proteron", as in the statement
Such fallacies may not be immediately obvious—obscured by synonyms or synonymous phrases. One way to beg the question is to make a statement first in concrete terms, then in abstract ones, or vice versa. Another is to "bring forth a proposition expressed in words of Saxon origin, and give as a reason for it the very same proposition stated in words of Norman origin," as in this example:
When the fallacy of begging the question is committed in more than one step, some authors consider it "circulus in probando" or "reasoning in a circle". However, there is no fallacy if the missing premise is acknowledged, and if not, there is no circle.
"Begging the question" can also refer to an argument in which the unstated premise is essential to, but not identical with the conclusion, or is "controversial or questionable for the same reasons that typically might lead someone to question the conclusion."
...[S]eldom is anyone going to simply place the conclusion word-for-word into the premises ... Rather, an arguer might use phraseology that conceals the fact that the conclusion is masquerading as a premise. The conclusion is rephrased to look different and is then placed in the premises.—Paul Herrick
Begging the question is not considered a formal fallacy (an argument that is defective because it uses an incorrect deductive step). Rather, it is a type of informal fallacy that is logically valid but unpersuasive, in that it fails to prove anything other than what is already assumed.
Related fallacies.
Closely connected with begging the question is the fallacy of circular reasoning ("circulus in probando"), a fallacy in which the reasoner begins with the conclusion. The individual components of a circular argument can be logically valid because if the premises are true, the conclusion must be true, and does not lack relevance. However, circular reasoning is not persuasive because a listener who doubts the conclusion also doubts the premise that leads to it.
In fact, begging the question is often considered a type of circular reasoning.
Begging the question is similar to the "complex question" (also known as "trick question" or "fallacy of many questions"): a question that, to be valid, requires the truth of another question that has not been established. For example, "Which color dress is Mary wearing?" may be fallacious because it presupposes that Mary is wearing a dress. Unless it has previously been established that her outfit is a dress, the question is fallacious because she could be wearing a pantsuit.
Another related fallacy is "ignoratio elenchi" or "irrelevant conclusion": an argument that fails to address the issue in question, but appears to do so. An example might be a situation where A and B are debating whether the law permits A to do something. If A attempts to support his position with an argument that the law "ought" to allow him to do the thing in question, then he is guilty of "ignoratio elenchi".
Modern usage.
Many English speakers use "begs the question" to mean "raises the question," "evades the question," or even "ignores the question," and follow that phrase with the question, for example: "I am 120 Kg and have severely clogged arteries, which begs the question: why have I not started exercising?"
In philosophical, logical, grammatical, and legal contexts, most commenters believe that such usage is mistaken, or at best, unclear.
References.
</dl>

</doc>
<doc id="43583" url="http://en.wikipedia.org/wiki?curid=43583" title="Teresa of Ávila">
Teresa of Ávila

Teresa of Ávila, also called Saint Teresa of Jesus, baptized as Teresa Sánchez de Cepeda y Ahumada (28 March 1515 – 4 October 1582), was a prominent Spanish mystic, Roman Catholic saint, Carmelite nun, author during the Counter Reformation, and theologian of contemplative life through mental prayer. She was a reformer of the Carmelite Order and is considered to be a founder of the Discalced Carmelites along with John of the Cross.
In 1622, forty years after her death, she was canonized by Pope Gregory XV, and on 27 September 1970 was named a Doctor of the Church by Pope Paul VI. Her books, which include her autobiography ("The Life of Teresa of Jesus") and her seminal work "El Castillo Interior" (trans.: "The Interior Castle"), are an integral part of Spanish Renaissance literature as well as Christian mysticism and Christian meditation practices. She also wrote "Camino de Perfección" (trans.: "The Way of Perfection").
After her death, Saint Teresa's cult was known in Spain during the 1620s, and for a time she was considered a candidate to become a national patron saint. A Santero image of the Our Lady of the Conception, said to have been sent with one of her brothers to Nicaragua by the saint, is now venerated as the country's national patroness at the Shrine of El Viejo. Pious Catholic beliefs also associate Saint Teresa with the esteemed religious image called Infant Jesus of Prague with claims of former ownership and devotion.
Early life.
Teresa de Cepeda y Ahumada was born in 1515 in Gotarrendura, in the province of Ávila, Spain. Her paternal grandfather, Juanito de Hernandez, was a marrano (Jewish convert to Christianity) and was condemned by the Spanish Inquisition for allegedly returning to the Jewish faith. Her father, Alonso Sánchez de Cepeda, bought a knighthood and successfully assimilated into Christian society. Teresa's mother, Beatriz de Ahumada y Cuevas, was especially keen to raise her daughter as a pious Christian. Teresa was fascinated by accounts of the lives of the saints, and ran away from home at age seven with her brother Rodrigo to find martyrdom among the Moors. Her uncle stopped them as he was returning to the town, having spotted the two outside the town walls.
When Teresa was 14 her mother died, this resulted in Teresa becoming grief-stricken. This prompted her to embrace a deeper devotion to the Virgin Mary as her spiritual mother. Along with this good resolution, however, she also developed immoderate interests in reading popular fiction (consisting, at that time, mostly of medieval tales of knighthood) and caring for her own appearance. Teresa was sent for her education to the Augustinian nuns at Ávila.
In the cloister, she suffered greatly from illness. Early in her sickness, she experienced periods of religious ecstasy through the use of the devotional book "Tercer abecedario espiritual", translated as the "Third Spiritual Alphabet" (published in 1527 and written by Francisco de Osuna). This work, following the example of similar writings of medieval mystics, consisted of directions for examinations of conscience and for spiritual self-concentration and inner contemplation (known in mystical nomenclature as "oratio recollectionis" or "oratio mentalis"). She also employed other mystical ascetic works such as the "Tractatus de oratione et meditatione" of Saint Peter of Alcantara, and perhaps many of those upon which Saint Ignatius of Loyola based his "Spiritual Exercises" and possibly the "Spiritual Exercises" themselves.
She claimed that during her illness she rose from the lowest stage, "recollection", to the "devotions of silence" or even to the "devotions of ecstasy", which was one of perfect union with God (see below). During this final stage, she said she frequently experienced a rich "blessing of tears." As the Catholic distinction between mortal and venial sin became clear to her, she says she came to understand the awful terror of sin and the inherent nature of original sin. She also became conscious of her own natural impotence in confronting sin, and the necessity of absolute subjection to God.
Around 1556, various friends suggested that her newfound knowledge was diabolical, not divine. She began to inflict various tortures and mortifications of the flesh upon herself. But her confessor, the Jesuit Saint Francis Borgia, reassured her of the divine inspiration of her thoughts. On St. Peter's Day in 1559, Teresa became firmly convinced that Jesus Christ presented himself to her in bodily form, though invisible. These visions lasted almost uninterrupted for more than two years. In another vision, a seraph drove the fiery point of a golden lance repeatedly through her heart, causing an ineffable spiritual-bodily pain.
This vision was the inspiration for one of Bernini's most famous works, the "Ecstasy of Saint Teresa" at Santa Maria della Vittoria in Rome.
The memory of this episode served as an inspiration throughout the rest of her life, and motivated her lifelong imitation of the life and suffering of Jesus, epitomized in the motto usually associated with her: "Lord, either let me suffer or let me die".
Activities as reformer.
Teresa entered a Carmelite Monastery of the Incarnation in Ávila, Spain, on 2 November 1535. She found herself increasingly in disharmony with the spiritual malaise prevailing at the Incarnation. Among the 150 nuns living there, the observance of cloister — designed to protect and strengthen the spirit and practice of prayer — became so lax that it actually lost its very purpose. The daily invasion of visitors, many of high social and political rank, vitiated the atmosphere with frivolous concerns and vain conversations. These violations of the solitude absolutely essential to progress in genuine contemplative prayer grieved Teresa to the extent that she longed to do something.
The incentive to give outward practical expression to her inward motive was inspired in Teresa by the Franciscan priest Saint Peter of Alcantara who became acquainted with her early in 1560, and became her spiritual guide and counselor. She now resolved to found a reformed Carmelite convent, correcting the laxity which she had found in the Cloister of the Incarnation and others. Guimara de Ulloa, a woman of wealth and a friend, supplied the funds. Teresa worked for many years encouraging Spanish Jewish converts to follow Christianity.
The absolute poverty of the new monastery, established in 1562 and named St. Joseph's (San José), at first excited a scandal among the citizens and authorities of Ávila, and the little house with its chapel was in peril of suppression; but powerful patrons, including the bishop himself, as well as the impression of well-secured subsistence and prosperity, turned animosity into applause.
In March 1563, when Teresa moved to the new cloister, she received the papal sanction to her prime principle of absolute poverty and renunciation of property, which she proceeded to formulate into a "Constitution". Her plan was the revival of the earlier, stricter rules, supplemented by new regulations such as the three disciplines of ceremonial flagellation prescribed for the divine service every week, and the discalceation of the nun. For the first five years, Teresa remained in pious seclusion, engaged in writing.
In 1567, she received a patent from the Carmelite general, Rubeo de Ravenna, to establish new houses of her order, and in this effort and later visitations she made long journeys through nearly all the provinces of Spain. Of these she gives a description in her "Libro de las Fundaciones." Between 1567 and 1571, reform convents were established at Medina del Campo, Malagón, Valladolid, Toledo, Pastrana, Salamanca, and Alba de Tormes.
As part of her original patent, Teresa was given permission to set up two houses for men who wished to adopt the reforms; she convinced John of the Cross and Anthony of Jesus to help with this. They founded the first convent of Discalced Carmelite Brethren in November 1568 at Duruello. Another friend, Gerónimo Gracian, Carmelite visitator of the older observance of Andalusia and apostolic commissioner, and later provincial of the Teresian reforms, gave her powerful support in founding convents at Segovia (1571), Beas de Segura (1574), Seville (1575), and Caravaca de la Cruz (Murcia, 1576), while the deeply mystical John, by his power as teacher and preacher, promoted the inner life of the movement.
In 1576 a series of persecutions began on the part of the older observant Carmelite order against Teresa, her friends, and her reforms. Pursuant to a body of resolutions adopted at the general chapter at Piacenza, the "definitors" of the order forbade all further founding of convents. The general chapter condemned her to voluntary retirement to one of her institutions. She obeyed and chose St. Joseph's at Toledo. Her friends and subordinates were subjected to greater trials.
Finally, after several years her pleadings by letter with King Philip II of Spain secured relief. As a result, in 1579, the processes before the inquisition against her, Gracian, and others were dropped, which allowed the reform to continue. A brief of Pope Gregory XIII allowed a special provincial for the younger branch of the discalced nuns, and a royal rescript created a protective board of four assessors for the reform.
During the last three years of her life, Teresa founded convents at Villanueva de la Jara in northern Andalusia (1580), Palencia (1580), Soria (1581), Burgos, and Granada (1582). In total seventeen convents, all but one founded by her, and as many men's cloisters were due to her reform activity of twenty years.
Her final illness overtook her on one of her journeys from Burgos to Alba de Tormes. She died in 1582, just as Catholic nations were making the switch from the Julian to the Gregorian calendar, which required the removal of 5–14 October from the calendar. She died either before midnight of 4 October or early in the morning of 15 October which is celebrated as her feast day. Her last words were: "My Lord, it is time to move on. Well then, may your will be done. O my Lord and my Spouse, the hour that I have longed for has come. It is time to meet one another."
In 1622, forty years after her death, she was canonized by Pope Gregory XV. The Cortes exalted her to patroness of Spain in 1617, and the University of Salamanca previously conferred the title "Doctor ecclesiae" with a diploma. The title is Latin for "Doctor of the Church", but is distinct from the papal honor of Doctor of the Church, which is always conferred posthumously and was finally bestowed upon her by Pope Paul VI in December 27, 1970 along with Saint Catherine of Siena making them the first women to be awarded the distinction. Teresa is revered as the Doctor of Prayer. The mysticism in her works exerted a formative influence upon many theologians of the following centuries, such as Francis of Sales, Fénelon, and the Port-Royalists.
Mysticism.
The kernel of Teresa's mystical thought throughout all her writings is the ascent of the soul in four stages ("The Autobiography" Chs. 10-22):
The first, or "mental prayer", is that of devout contemplation or concentration, the withdrawal of the soul from without and especially the devout observance of the passion of Christ and penitence ("Autobiography" 11.20).
The second is the "prayer of quiet", in which at least the human will is lost in that of God by virtue of a charismatic, supernatural state given by God, while the other faculties, such as memory, reason, and imagination, are not yet secure from worldly distraction. While a partial distraction is due to outer performances such as repetition of prayers and writing down spiritual things, yet the prevailing state is one of quietude ("Autobiography" 14.1).
The "devotion of union" is not only a supernatural but an essentially ecstatic state. Here there is also an absorption of the reason in God, and only the memory and imagination are left to ramble. This state is characterized by a blissful peace, a sweet slumber of at least the higher soul faculties, or a conscious rapture in the love of God.
The fourth is the "devotion of ecstasy or rapture," a passive state, in which the feeling of being in the body disappears (). Sense activity ceases; memory and imagination are also absorbed in God or intoxicated. Body and spirit are in the throes of a sweet, happy pain, alternating between a fearful fiery glow, a complete impotence and unconsciousness, and a spell of strangulation, sometimes by such an ecstatic flight that the body is literally lifted into space . This after half an hour is followed by a reactionary relaxation of a few hours in a swoon-like weakness, attended by a negation of all the faculties in the union with God. The subject awakens From this in tears; it is the climax of mystical experience, producing a trance. Indeed, she was said to have been observed levitating during Mass on more than one occasion.
Teresa is one of the foremost writers on mental prayer, and her position among writers on mystical theology is unique. In all her writings on this subject she deals with her personal experiences. Her deep insight and analytical gifts helped her to explain them clearly. Her definition was used in the "Catechism of the Catholic Church": "Contemplative prayer ["oración mental"] in my opinion is nothing else than a close sharing between friends; it means taking time frequently to be alone with him who we know loves us." She used a metaphor of mystic prayer as watering a garden throughout her writings.
Writings.
Teresa's writings, produced for didactic purposes, stand among the most remarkable in the mystical literature of the Catholic Church:
Excerpts.
Saint Teresa, who reported visions of Jesus and Mary, was a strong believer in the power of holy water and wrote that she used it with success to repel evil and temptations. She wrote:
 I know by frequent experience that there is nothing which puts the devils to
flight like holy water.
 Let nothing disturb you.
Let nothing make you afraid.
All things are passing.
God alone never changes.
Patience gains all things.
If you have God you will want for nothing.
God alone suffices.
 — St Teresa, "The bookmark of Teresa of Ávila", 
The modern poem "Christ has no body", though widely attributed to Teresa, is not found in her writings.
 Christ has no body but yours,
No hands, no feet on earth but yours,
Yours are the eyes with which he looks
Compassion on this world,
Yours are the feet with which he walks to do good,
Yours are the hands, with which he blesses all the world.
Yours are the hands, yours are the feet,
Yours are the eyes, you are his body.
Christ has no body now but yours,
No hands, no feet on earth but yours,
Yours are the eyes with which he looks
compassion on this world.
Christ has no body now on earth but yours.
 — Teresa of Ávila (attributed)
Saint Teresa and the Infant Jesus of Prague.
Though there are no written historical accounts proving that Teresa of Ávila ever owned the Infant Jesus of Prague statue, according to a pious legend Teresa once owned the statue and gave it to a noblewoman travelling to Prague. The age of the statue dates to approximately the same era as Teresa.
It was thought that Teresa carried a portable statue of the Child Jesus wherever she went.
Contemporary history cannot confirm that the Prague image was what she was thought to have owned. Catholic pious beliefs follow the local legend, certainly already circulated by the early 1700s.
Saint Teresa is also portrayed in the biographical 1984 film "Teresa de Jesús", and shown in the movie protecting this infant statue in her many calamitous travels. In some scenes, the other religious sisters take turn in changing its vestments. The devotion to the Child Jesus spread quickly in Spain, possibly due to her mystical visions. The Spanish nuns who established Carmel in France brought this devotion with them, and it became widespread in France. Indeed, one of Teresa's most famous disciples, Saint Thérèse of Lisieux, a French Carmelite, herself named for Teresa, had as her religious name "Sister Thérèse of the Child Jesus and the Holy Face".
Patron saint.
In the 1620s Spain debated who should be the country's patron saint; the choices were either the current patron, Saint James Matamoros ("Moor-slayer") or a combination of him and the newly canonised Saint Teresa of Ávila. Teresa's promoters said Spain faced newer challenges, especially the threat of Protestantism and societal decline at home, thus needing a more contemporary patron who understood those issues and could guide the Spanish nation. Santiago's supporters ("Santiaguistas") fought back viciously and eventually won the argument, but Teresa of Ávila remained far more popular at the local level.

</doc>
<doc id="43585" url="http://en.wikipedia.org/wiki?curid=43585" title="753 BC">
753 BC

 

</doc>
<doc id="43589" url="http://en.wikipedia.org/wiki?curid=43589" title="Fluorite">
Fluorite

Fluorite (also called fluorspar) is the mineral form of calcium fluoride, CaF2. It belongs to the halide minerals. It crystallizes in isometric cubic habit, although octahedral and more complex isometric forms are not uncommon.
Fluorite is a colorful mineral, both in visible and ultraviolet light, and the stone has ornamental and lapidary uses. Industrially, fluorite is used as a flux for smelting, and in the production of certain glasses and enamels. The purest grades of fluorite are a source of fluoride for hydrofluoric acid manufacture, which is the intermediate source of most fluorine-containing fine chemicals. Optically clear transparent fluorite lenses have low dispersion, so lenses made from it exhibit less chromatic aberration, making them valuable in microscopes and telescopes. Fluorite optics are also usable in the far-ultraviolet range, where conventional glasses are too absorbent for use.
History and etymology.
Fluorite derives from the Latin noun "fluo", meaning a stream or flow of water. In verb form this was "fluor" or "fluere", meaning "to flow". The mineral is used as a flux in iron smelting to decrease the viscosity of slags. The melting point of calcium fluoride is 1676 K. The term flux comes from the Latin noun "fluxus", a wash or current of water. The mineral fluorite was originally termed fluorospar and was first discussed in print in a 1530 work "Bermannus, sive de re metallica dialogus" [Bermannus; or a dialogue about the nature of metals], by Georgius Agricola, as a mineral noted for its usefulness as a flux. Agricola, a German scientist with expertise in philology, mining, and metallurgy, named fluorspar as a Neo Latinization of the German "Flussspat" from "Fluß" (stream, river) and "Spat" (meaning a nonmetallic mineral akin to gypsum, spærstān, "spear stone", referring to its crystalline projections).
In 1852, fluorite gave its name to the phenomenon of fluorescence, which is prominent in fluorites from certain locations, due to certain impurities in the crystal. Fluorite also gave the name to its constitutive element fluorine. Presently, the word "fluorspar" is most commonly used for fluorite as the industrial and chemical commodity, while "fluorite" is used mineralogically and in most other senses.
Structure.
Fluorite crystallises in a cubic motif. Crystal twinning is common and adds complexity to the observed crystal habits. Fluorite has four perfect cleavage planes that help produce octahedral fragments.
Element substitution for the calcium cation often includes certain rare earth elements (REE), such as yttrium and cerium. Iron, sodium, and barium are also common impurities. Some fluorine may be replaced by the chlorine anion.
Occurrence and mining.
Fluorite is a widely occurring mineral that occurs globally with significant deposits in over 9,000 areas. It may occur as a vein deposit, especially with metallic minerals, where it often forms a part of the gangue (the surrounding "host-rock" in which valuable minerals occur) and may be associated with galena, sphalerite, barite, quartz, and calcite. It is a common mineral in deposits of hydrothermal origin and has been noted as a primary mineral in granites and other igneous rocks and as a common minor constituent of dolostone and limestone.
The world reserves of fluorite are estimated at 230 million tonnes (Mt) with the largest deposits being in South Africa (about 41 Mt), Mexico (32 Mt) and China (24 Mt). China is leading the world production with about 3 Mt annually (in 2010), followed by Mexico (1.0 Mt), Mongolia (0.45 Mt), Russia (0.22 Mt), South Africa (0.13 Mt), Spain (0.12 Mt) and Namibia (0.11 Mt).
One of the largest deposits of fluorspar in North America is located in the Burin Peninsula, Newfoundland, Canada. The first official recognition of fluorspar in the area was recorded by geologist J.B. Jukes in 1843. He noted an occurrence of "galena" or lead ore and fluorite of lime on the west side of St. Lawrence harbour. It is recorded that interest in the commercial mining of fluorspar began in 1928 with the first ore being extracted in 1933. Eventually at Iron Springs Mine, the shafts reached depths of 970 ft. In the St. Lawrence area, the veins are persistent for great lengths and several of them have wide lenses. The area with veins of known workable size comprises about 60 sqmi.
Cubic crystals up to 20 cm across have been found at Dalnegorsk, Russia. The largest documented single crystal of fluorite was a cube 2.12 m in size and weighing ~16 tonnes.
"Blue John".
One of the most famous of the older-known localities of fluorite is Castleton in Derbyshire, England, where, under the name of Derbyshire Blue John, purple-blue fluorite was extracted from several mines or caves. During the 19th century, this attractive fluorite was mined for its ornamental value. The mineral Blue John is now scarce, and only a few hundred kilograms are mined each year for ornamental and lapidary use. Mining still takes place in Blue John Cavern and Treak Cliff Cavern.
Recently discovered deposits in China have produced fluorite with coloring and banding similar to the classic Blue John stone.
Fluorescence.
George Gabriel Stokes named the phenomenon of "fluorescence" from fluorite, in 1852.
Many samples of fluorite exhibit fluorescence under ultraviolet light, a property that takes its name from fluorite. Many minerals, as well as other substances, fluoresce. Fluorescence involves the elevation of electron energy levels by quanta of ultraviolet light, followed by the progressive falling back of the electrons into their previous energy state, releasing quanta of visible light in the process. In fluorite, the visible light emitted is most commonly blue, but red, purple, yellow, green and white also occur. The fluorescence of fluorite may be due to mineral impurities, such as yttrium and ytterbium, or organic matter, such as volatile hydrocarbons in the crystal lattice. In particular, the blue fluorescence seen in fluorites from certain parts of Great Britain responsible for the naming of the phenomenon of fluorescence itself, has been attributed to the presence of inclusions of divalent europium in the crystal.
One fluorescent variety of fluorite is chlorophane, which is purple in color, and fluoresces brightly in emerald green when heated or exposed to ultraviolet light.
The color of visible light emitted when a sample of fluorite is fluorescing depends on where the original specimen was collected; different impurities having been included in the crystal lattice in different places. Neither does all fluorite fluoresce equally brightly, even from the same locality. Therefore, ultraviolet light is not a reliable tool for the identification of specimens, nor for quantifying the mineral in mixtures. For example, among British fluorites, those from Northumberland, County Durham, and eastern Cumbria are the most consistently fluorescent, whereas fluorite from Yorkshire, Derbyshire, and Cornwall, if they fluoresce at all, are generally only feebly fluorescent.
Fluorite also exhibits the property of thermoluminescence.
Color.
Fluorite is allochromatic, meaning that it can be tinted with elemental impurities. Fluorite comes in a wide range of colors and has consequently been dubbed "the most colorful mineral in the world". Every color of the rainbow in various shades are represented by fluorite samples, along with white, black, and clear crystals. The most common colors are purple, blue, green, yellow, or colorless. Less common are pink, red, white, brown, and black. Color zoning or banding is commonly present. The color of the fluorite is determined by factors including impurities, exposure to radiation, and the absence or voids of the color centers.
Uses.
Source of fluorine and fluoride.
Fluorite is a major source of hydrogen fluoride, a commodity chemical used to produce a wide range of materials. Hydrogen fluoride is liberated from the mineral by the action of concentrated sulfuric acid:
The resulting HF is converted into fluorine, fluorocarbons, and diverse fluoride materials. As of the late 1990s, five billion kilograms were mined annually.
There are three principal types of industrial use for natural fluorite, commonly referred to as "fluorspar" in these industries, corresponding to different grades of purity. Metallurgical grade fluorite (60–85% CaF2), the lowest of the three grades, has traditionally been used as a flux to lower the melting point of raw materials in steel production to aid the removal of impurities, and later in the production of aluminium. Ceramic grade fluorite (85–95% CaF2) is used in the manufacture of opalescent glass, enamels and cooking utensils. The highest grade, "acid grade fluorite" (97% or more CaF2), accounts for about 95% of fluorite consumption in the US where it is used to make hydrogen fluoride and hydrofluoric acid by reacting the fluorite with sulfuric acid.
Internationally, acid-grade fluorite is also used in the production of AlF3 and cryolite (Na3AlF6), which are the main fluorine compounds used in aluminium smelting. Alumina is dissolved in a bath that consists primarily of molten Na3AlF6, AlF3, and fluorite (CaF2) to allow electrolytic recovery of aluminium. Fluorine losses are replaced entirely by the addition of AlF3, the majority of which will react with excess sodium from the alumina to form Na3AlF6.
Niche uses.
Lapidary uses.
Natural fluorite mineral has ornamental and lapidary uses. Fluorite may be drilled into beads and used in jewelry, although due to its relative softness it is not widely used as a semiprecious stone. It is also used for ornamental carvings, with expert carvings taking advantage of the stone's zonation.
Optics.
In the laboratory, calcium fluoride is commonly used as a window material for both infrared and ultraviolet wavelengths, since it is transparent in these regions (about 0.15 µm to 9 µm) and exhibits extremely low change in refractive index with wavelength. Furthermore, the material is attacked by few reagents. At wavelengths as short as 157 nm, a common wavelength used for semiconductor stepper manufacture for integrated circuit lithography, the refractive index of calcium fluoride shows some non-linearity at high power densities, which has inhibited its use for this purpose. In the early years of the 21st century, the stepper market for calcium fluoride collapsed, and many large manufacturing facilities have been closed. Canon and other manufacturers have used synthetically grown crystals of calcium fluoride components in lenses to aid apochromatic design, and to reduce light dispersion. This use has largely been superseded by newer glasses and computer-aided design. As an infrared optical material, calcium fluoride is widely available and was sometimes known by the Eastman Kodak trademarked name "Irtran-3", although this designation is obsolete.
Fluorite has a very low dispersion, so lenses made from it exhibit less chromatic aberration than those made of ordinary glass. However, naturally occurring fluorite crystals without optical defects were only large enough to produce microscope elements.
With the advent of synthetically grown fluorite (calcium fluoride crystal), it could be used instead of glass in some high-performance telescopes and camera lens elements. Its use for prisms and lenses was studied and promoted by Victor Schumann near the end of the 19th century.
In telescopes, fluorite elements allow high-resolution images of astronomical objects at high magnifications. Canon Inc. produces synthetic fluorite crystals that are used in their more expensive telephoto lenses.
Exposure tools for the semiconductor industry make use of fluorite optical elements for ultraviolet light at wavelengths of about 157 nanometers. Fluorite has a uniquely high transparency at this wavelength. Fluorite objective lenses are manufactured by the larger microscope firms (Nikon, Olympus, Carl Zeiss and Leica). Their transparence to ultraviolet light enables them to be used for fluorescence microscopy. The fluorite also serves to correct optical aberrations in these lenses. Nikon has previously manufactured at least one all-fluorite element camera lens (105 mm f/4.5 UV) for the production of ultraviolet images. Konica produced a fluorite lens for their SLR cameras – the Hexanon 300 mm f6.3.
Source of fluorine gas in nature.
In 2012, the first source of naturally occurring fluorine gas was found in fluorite mines in Bavaria, Germany. It was previously thought that fluorine gas did not occur naturally because it is so reactive and would rapidly react with other chemicals. Fluorite is normally colorless, but some varied forms found nearby look black and are known as 'fetid fluorite' or antozonite. The minerals, containing small amounts of uranium and its daughter products, release radiation sufficiently energetic to induce oxidation of fluoride anions within the structure to fluorine that becomes trapped inside the mineral. The color of fetid fluorite is predominantly due to the calcium atoms remaining. Solid state fluorine-19 NMR was carried out on the gas escaping the antozonite revealed a peak at 425 ppm, which is consistent with F2.
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="43590" url="http://en.wikipedia.org/wiki?curid=43590" title="Flux">
Flux

In the various subfields of physics, there exist two common usages of the term flux, each with rigorous mathematical frameworks. A simple and ubiquitous concept throughout physics and applied mathematics is the flow of a physical property in space, frequently also with time variation. It is the basis of the field concept in physics and mathematics, with two principal applications: in transport phenomena and surface integrals. The terms "flux", "current", "flux density", "current density", can sometimes be used interchangeably and ambiguously, though the terms used below match those of the contexts in the literature.
Origin of the term.
The word "flux" comes from Latin: "fluxus" means "flow", and "fluere" is "to flow". As "fluxion", this term was introduced into differential calculus by Isaac Newton.
Flux as flow rate per unit area.
In transport phenomena (heat transfer, mass transfer and fluid dynamics), flux is defined as the "rate of flow of a property per unit area," which has the dimensions [quantity]·[time]−1·[area]−1. For example, the magnitude of a river's current, i.e. the amount of water that flows through a cross-section of the river each second, or the amount of sunlight that lands on a patch of ground each second is also a kind of flux.
General mathematical definition (transport).
In this definition, flux is generally a vector due to the widespread and useful definition of vector area, although there are some cases where only the magnitude is important (like in number fluxes, see below). The frequent symbol is "j" (or "J"), and a definition for scalar flux of physical quantity "q" is the limit:
where:
is the flow of quantity "q" per unit time "t", and "A" is the area through which the quantity flows.
For vector flux, the surface integral of j over a surface "S", followed by an integral over the time duration "t"1 to "t"2, gives the total amount of the property flowing through the surface in that time ("t"2 − "t"1):
The area required to calculate the flux is real or imaginary, flat or curved, either as a cross-sectional area or a surface. The vector area is a combination of the magnitude of the area through which the mass passes through, "A", and a unit vector normal to the area, formula_4. The relation is formula_5.
If the flux j passes through the area at an angle θ to the area normal formula_4, then
where · is the dot product of the unit vectors. This is, the component of flux passing through the surface (i.e. normal to it) is "j" cos θ, while the component of flux passing tangential to the area is "j" sin θ, but there is "no" flux actually passing "through" the area in the tangential direction. The "only" component of flux passing normal to the area is the cosine component.
One could argue, based on the work of James Clerk Maxwell, that the transport definition precedes the more recent way the term is used in electromagnetism. The specific quote from Maxwell is:
In the case of fluxes, we have to take the integral, over a surface, of the flux through every element of the surface. The result of this operation is called the surface integral of the flux. It represents the quantity which passes through the surface. —James Clerk Maxwell
Transport fluxes.
Eight of the most common forms of flux from the transport phenomena literature are defined as follows:
These fluxes are vectors at each point in space, and have a definite magnitude and direction. Also, one can take the divergence of any of these fluxes to determine the accumulation rate of the quantity in a control volume around a given point in space. For incompressible flow, the divergence of the volume flux is zero.
Chemical diffusion.
As mentioned above, chemical molar flux of a component A in an isothermal, isobaric system is defined in Fick's law of diffusion as:
where the nabla symbol ∇ denotes the gradient operator, "DAB" is the diffusion coefficient (m2·s−1) of component A diffusing through component B, "cA" is the concentration (mol/m3) of component A.
This flux has units of mol·m−2·s−1, and fits Maxwell's original definition of flux.
For dilute gases, kinetic molecular theory relates the diffusion coefficient "D" to the particle density "n" = "N"/"V", the molecular mass "m", the collision cross section formula_9, and the absolute temperature "T" by
where the second factor is the mean free path and the square root (with Boltzmann's constant "k") is the mean velocity of the particles.
In turbulent flows, the transport by eddy motion can be expressed as a grossly increased diffusion coefficient.
Quantum mechanics.
In quantum mechanics, particles of mass "m" in the quantum state ψ(r, t) have a probability density defined as
So the probability of finding a particle in a differential volume element d3r is
Then the number of particles passing perpendicularly through unit area of a cross-section per unit time is the probability flux;
This is sometimes referred to as the probability current or current density, or probability flux density.
Flux as a surface integral.
General mathematical definition (surface integral).
As a mathematical concept, flux is represented by the surface integral of a vector field,
where F is a vector field, and d"A" is the vector area of the surface "A", directed as the surface normal.
The surface has to be orientable, i.e. two sides can be distinguished: the surface does not fold back onto itself. Also, the surface has to be actually oriented, i.e. we use a convention as to flowing which way is counted positive; flowing backward is then counted negative.
The surface normal is directed usually by the right-hand rule.
Conversely, one can consider the flux the more fundamental quantity and call the vector field the flux density.
Often a vector field is drawn by curves (field lines) following the "flow"; the magnitude of the vector field is then the line density, and the flux through a surface is the number of lines. Lines originate from areas of positive divergence (sources) and end at areas of negative divergence (sinks).
See also the image at right: the number of red arrows passing through a unit area is the flux density, the curve encircling the red arrows denotes the boundary of the surface, and the orientation of the arrows with respect to the surface denotes the sign of the inner product of the vector field with the surface normals.
If the surface encloses a 3D region, usually the surface is oriented such that the influx is counted positive; the opposite is the outflux.
The divergence theorem states that the net outflux through a closed surface, in other words the net outflux from a 3D region, is found by adding the local net outflow from each point in the region (which is expressed by the divergence).
If the surface is not closed, it has an oriented curve as boundary. Stokes' theorem states that the flux of the curl of a vector field is the line integral of the vector field over this boundary. This path integral is also called circulation, especially in fluid dynamics. Thus the curl is the circulation density.
We can apply the flux and these theorems to many disciplines in which we see currents, forces, etc., applied through areas.
Electromagnetism.
One way to better understand the concept of flux in electromagnetism is by comparing it to a butterfly net. The amount of air moving through the net at any given instant in time is the flux. If the wind speed is high, then the flux through the net is large. If the net is made bigger, then the flux is larger even though the wind speed is the same. For the most air to move through the net, the opening of the net must be facing the direction the wind is blowing. If the net is parallel to the wind, then no wind will be moving through the net. The simplest way to think of flux is "how much air goes through the net", where the air is a velocity field and the net is the boundary of an imaginary surface.
Electric flux.
Two forms of electric flux are used, one for the E-field:
and one for the D-field (called the electric displacement):
This quantity arises in Gauss's law – which states that the flux of the electric field E out of a closed surface is proportional to the electric charge "QA" enclosed in the surface (independent of how that charge is distributed), the integral form is:
where ε0 is the permittivity of free space.
If one considers the flux of the electric field vector, E, for a tube near a point charge in the field the charge but not containing it with sides formed by lines tangent to the field, the flux for the sides is zero and there is an equal and opposite flux at both ends of the tube. This is a consequence of Gauss's Law applied to an inverse square field. The flux for any cross-sectional surface of the tube will be the same. The total flux for any surface surrounding a charge "q" is "q"/ε0.
In free space the electric displacement is given by the constitutive relation D = ε0 E, so for any bounding surface the D-field flux equals the charge "QA" within it. Here the expression "flux of" indicates a mathematical operation and, as can be seen, the result is not necessarily a "flow", since nothing actually flows along electric field lines.
Magnetic flux.
The magnetic flux density (magnetic field) having the unit Wb/m2 (Tesla) is denoted by B, and magnetic flux is defined analogously:
with the same notation above. The quantity arises in Faraday's law of induction, in integral form:
where "d"{{ell}} is an infinitesimal vector line element of the closed curve "C", with magnitude equal to the length of the infinitesimal line element, and direction given by the tangent to the curve "C", with the sign determined by the integration direction.
The time-rate of change of the magnetic flux through a loop of wire is minus the electromotive force created in that wire. The direction is such that if current is allowed to pass through the wire, the electromotive force will cause a current which "opposes" the change in magnetic field by itself producing a magnetic field opposite to the change. This is the basis for inductors and many electric generators.
Poynting flux.
Using this definition, the flux of the Poynting vector S over a specified surface is the rate at which electromagnetic energy flows through that surface, defined like before:
The flux of the Poynting vector through a surface is the electromagnetic power, or energy per unit time, passing through that surface. This is commonly used in analysis of electromagnetic radiation, but has application to other electromagnetic systems as well.
Confusingly, the Poynting vector is sometimes called the "power flux", which is an example of the first usage of flux, above. It has units of watts per square metre (W/m2).

</doc>
<doc id="43591" url="http://en.wikipedia.org/wiki?curid=43591" title="Édouard Roche">
Édouard Roche

Édouard Albert Roche (17 October 1820 – 27 April 1883) was a French astronomer and mathematician, who is best known for his work in the field of celestial mechanics. His name was given to the concepts of the Roche sphere, Roche limit and Roche lobe. He also was the author of works in meteorology.
Biography.
He was born in Montpellier, and studied at the University of Montpellier, receiving his D.Sc. in 1844 and later becoming a professor at the same institution, where he served in the Faculté des Sciences starting in 1849. Roche made a mathematical study of Laplace's nebular hypothesis and presented his results in a series of papers to the Academy of Montpellier from his appointment until 1877. The most important were on comets (1860) and the nebular hypothesis itself (1873). Roche's studies examined the effects of strong gravitational fields upon swarms of tiny particles.
He is perhaps most famous for his theory that the planetary rings of Saturn were formed when a large moon came too close to Saturn and was pulled apart by gravitational forces. He described a method of calculating the distance at which an object held together only by gravity would break up due to tidal forces; this distance became known as the Roche limit. 
His other best known works also involved orbital mechanics. The Roche lobe describes the limits at which an object which is in orbit around two other objects will be captured by one or the other, and the Roche sphere approximates the gravitational sphere of influence of one astronomical body in the face of perturbations from another heavier body around which it orbits.
Works.
Roche's works are in French.

</doc>
<doc id="43592" url="http://en.wikipedia.org/wiki?curid=43592" title="John Herschel">
John Herschel

Sir John Frederick William Herschel, 1st Baronet, KH, FRS (7 March 1792 – 11 May 1871)
was an English polymath, mathematician, astronomer, chemist, inventor, and experimental photographer, who in some years also did valuable botanical work. He was the son of Mary Baldwin and astronomer William Herschel and the father of twelve children.
Herschel originated the use of the Julian day system in astronomy. He named seven moons of Saturn and four moons of Uranus. He made many contributions to the science of photography, and investigated colour blindness and the chemical power of ultraviolet rays.
Early life and work on astronomy.
Herschel was born in Slough, Berkshire, the son of Mary Baldwin and William Herschel. He studied shortly at Eton College and St John's College, Cambridge, graduating as Senior Wrangler in 1813. It was during his time as an undergraduate that he became friends with Charles Babbage and George Peacock. He took up astronomy in 1816, building a reflecting telescope with a mirror 18 in in diameter and with a 20 ft focal length. Between 1821 and 1823 he re-examined, with James South, the double stars catalogued by his father. For this work, in 1826 he was presented with the Gold Medal of the Royal Astronomical Society (which he won again in 1836), and with the Lalande Medal of the French Academy of Sciences in 1825, while in 1821 the Royal Society bestowed upon him the Copley Medal for his mathematical contributions to their Transactions. Herschel was made a Knight of the Royal Guelphic Order in 1831.
He served as President of the Royal Astronomical Society three times: 1827–29, 1839–41 and 1847–49.
His "A preliminary discourse on the study of natural philosophy" published early in 1831 as part of "Dionysius Lardner's Cabinet cyclopædia" set out methods of scientific investigation with an orderly relationship between observation and theorising. He described nature as being governed by laws which were difficult to discern or to state mathematically, and the highest aim of natural philosophy was understanding these laws through inductive reasoning, finding a single unifying explanation for a phenomenon. This became an authoritative statement with wide influence on science, particularly at the University of Cambridge where it inspired the student Charles Darwin with "a burning zeal" to contribute to this work.
Herschel published a catalogue of his astronomical observations in 1864, as the "General Catalogue of Nebulae and Clusters", a compilation of his own work and that of his father's, expanding on the senior Hershel's "Catalogue of Nebulae". A further complementary volume was published posthumously, as the "General Catalogue of 10,300 Multiple and Double Stars".
He also conceptualized a practical contact lens design in 1823.
Visit to South Africa.
Declining an offer from the Duke of Sussex that they travel to South Africa on a Navy ship, Herschel and his wife paid £500 for passage on the S.S. "Mountstuart Elphinstone", which departed from Portsmouth on 13 November 1833.
The voyage to South Africa was made in order to catalogue the stars, nebulae, and other objects of the southern skies. This was to be a completion as well as extension of the survey of the northern heavens undertaken initially by his father William Herschel. He arrived in Cape Town on 15 January 1834 and set up a private 21 ft telescope at Feldhausen at Claremont, a suburb of Cape Town. Amongst his other observations during this time was that of the return of Comet Halley. Herschel collaborated with Thomas Maclear, the Astronomer Royal at the Cape of Good Hope and the members of the two families became close friends.
In addition to his astronomical work, however, this voyage to a far corner of the British empire also gave Herschel an escape from the pressures under which he found himself in London, where he was one of the most sought-after of all British men of science. While in southern Africa, he engaged in a broad variety of scientific pursuits free from a sense of strong obligations to a larger scientific community. It was, he later recalled, probably the happiest time in his life.
In an extraordinary departure from astronomy, he combined his talents with those of his wife, Margaret, and between 1834 and 1838 they produced 131 botanical illustrations of fine quality, showing the Cape flora. Herschel used a camera lucida to obtain accurate outlines of the specimens and left the details to his wife. Even though their portfolio had been intended as a personal record, and despite the lack of floral dissections in the paintings, their accurate rendition makes them more valuable than contemporary collections. Some 112 of the 132 known flower studies were collected and published as "Flora Herscheliana" in 1996.
As their home during their stay in the Cape, the Herschels had selected 'Feldhausen'("Field Houses"), an old estate on the south-eastern side of Table Mountain. Here John set up his reflector to begin his survey of the southern skies. Herschel, meanwhile, read widely. Intrigued by the ideas of gradual formation of landscapes set out in Charles Lyell's "Principles of Geology", he wrote to Lyell on 20 February 1836 praising the book as a work that would bring "a complete revolution in [its] subject, by altering entirely the point of view in which it must thenceforward be contemplated" and opening a way for bold speculation on "that mystery of mysteries, the replacement of extinct species by others." Herschel himself thought catastrophic extinction and renewal "an inadequate conception of the Creator" and by analogy with other intermediate causes, "the origination of fresh species, could it ever come under our cognizance, would be found to be a natural in contradistinction to a miraculous process". He prefaced his words with the couplet:
Taking a gradualist view of development and referring to the evolution of language, he commented Words are to the Anthropologist what rolled pebbles are to the Geologist — battered relics of past ages often containing within them indelible records capable of intelligent interpretation — and when we see what amount of change 2000 years has been able to produce in the languages of Greece & Italy or 1000 in those of Germany France & Spain we naturally begin to ask how long a period must have lapsed since the Chinese, the Hebrew, the Delaware & the Malesass [Malagasy] had a point in common with the German & Italian & each other — Time! Time! Time! — we must not impugn the Scripture Chronology, but we "must" interpret it in accordance with "whatever" shall appear on fair enquiry to be the "truth" for there cannot be two truths. And really there is scope enough: for the lives of the Patriarchs may as reasonably be extended to 5000 or 50000 years apiece as the days of Creation to as many thousand millions of years.
The document was circulated, and Charles Babbage incorporated extracts in his ninth and unofficial "Bridgewater Treatise", which postulated laws set up by a divine programmer. When HMS "Beagle" called at Cape Town, Captain Robert FitzRoy and the young naturalist Charles Darwin visited Herschel on 3 June 1836. Later on, Darwin would be influenced by Herschel's writings in developing his theory advanced in "The Origin of Species". In the opening lines of that work, Darwin writes that his intent is "to throw some light on the origin of species — that mystery of mysteries, as it has been called by one of our greatest philosophers", referring to Herschel.
Herschel returned to England in 1838, was created a baronet, of Slough in the County of Buckingham, and published "Results of Astronomical Observations made at the Cape of Good Hope" in 1847. In this publication he proposed the names still used today for the seven then-known satellites of Saturn: Mimas, Enceladus, Tethys, Dione, Rhea, Titan, and Iapetus.
In the same year, Herschel received his second Copley Medal from the Royal Society for this work. A few years later, in 1852, he proposed the names still used today for the four then-known satellites of Uranus: Ariel, Umbriel, Titania, and Oberon.
Photography.
Herschel made numerous important contributions to photography. He made improvements in photographic processes, particularly in inventing the cyanotype process and variations (such as the chrysotype), the precursors of the modern blueprint process. In 1839, he made a photograph on glass, which still exists, and experimented with color reproduction, noting that rays of different parts of the spectrum tended to impart their own color to a photographic paper. Herschel made experiments using photosensitive emulsions of vegetable juices, called phytotypes and published his discoveries in the Philosophical Transactions of the Royal Society of London in 1842. He collaborated in the early 1840s with Henry Collen, portrait painter to Queen Victoria. Herschel originally discovered the platinum process on the basis of the light sensitivity of platinum salts, later developed by William Willis.
. He may, however, have been preceded by Brazilian Hércules Florence, who used the French equivalent, "photographie", in private notes which one historian dates to 1834. Herschel was also the first to apply the terms "negative" and "positive" to photography.
He discovered sodium thiosulfate to be a solvent of silver halides in 1819, and informed Talbot and Daguerre of his discovery that this "hyposulphite of soda" ("hypo") could be used as a photographic fixer, to "fix" pictures and make them permanent, after experimentally applying it thus in early 1839.
His ground-breaking research on the subject was read at the Royal Society in London in March 1839 and January 1840.
General.
Herschel wrote many papers and articles, including entries on meteorology, physical geography and the telescope for the eighth edition of the "Encyclopædia Britannica". He also translated the "Iliad" of Homer.
He invented the actinometer in 1825 to measure the direct heating power of the sun's rays,
and his work with the instrument is of great importance in the early history of photochemistry.
He proposed a correction to the Gregorian calendar, making years that are multiples of 4000 not leap years, thus reducing the average length of the calendar year from 365.2425 days to 365.24225.
Although this is closer to the mean tropical year of 365.24219 days, his proposal has never been adopted because the Gregorian calendar is based on the mean time between vernal equinoxes (currently days).
In 1836, he was elected a foreign member of the Royal Swedish Academy of Sciences.
In 1835, the "New York Sun" newspaper wrote a series of satiric articles that came to be known as the Great Moon Hoax, with statements falsely attributed to Herschel about his supposed discoveries of animals living on the Moon, including batlike winged humanoids.
The village of Herschel in western Saskatchewan (Canada), Mount Herschel (Antarctica), the crater J. Herschel on the Moon, and the Herschel Girls' School in Cape Town (South Africa), are all named after him. While it is commonly accepted that Herschel Island (in the Arctic Ocean, part of the Yukon Territory) was named after him, the entries in the expedition journal of Sir John Franklin state that the latter wished to honour the Herschel name, about which John Herschel’s father (Sir William Herschel) and his aunt (Caroline Herschel) are two other notable members of this family.
Family.
He married Margaret Brodie Stewart (1810–1884) on 3 March 1829 at Edinburgh and was father of the following children:
On his death at Collingwood, his home near Hawkhurst in Kent, he was given a national funeral and buried in Westminster Abbey.
Notes and references.
</dl>
Further reading.
</dl>

</doc>
<doc id="43594" url="http://en.wikipedia.org/wiki?curid=43594" title="De facto">
De facto

De facto (, , ]) is a Latin expression that means "in fact, in reality, in actual existence, force, or possession, as a matter of fact" (literally "from fact"). In law, it often means "in practice but not necessarily ordained by law" or "in practice or actuality, but not officially established". It is commonly used in contrast to "<dfn >de jure</dfn>" (which means "according to (the) law"; literally "from law") when referring to matters of law, governance, or technique (such as standards) that are found in the common experience as created or developed without or contrary to a regulation. When discussing a legal situation, "de jure" designates what the law says, while "de facto" designates action of what happens in practice.
Examples.
Segregation (during the Civil Rights era in the United States).
De facto racial discrimination and segregation in the USA during the 1950s and 1960s was simply discrimination that was not segregation by law (de jure).
Jim Crow Laws, which were enacted in the 1870s, brought legal racial segregation against black Americans residing in the American South. These laws were legally ended in 1964 by the Civil Rights Act of 1964.
Continued practices of expecting blacks to ride in the back of buses or to step aside onto the street if not enough room was present for a white person and "separate but equal" facilities are instances of de facto segregation. The NAACP fought for the de jure law to be upheld and for de facto segregation practices to be abolished.
Public schools in any region of the USA may be de facto racially segregated (or nearly so) simply because they are in neighborhoods whose residents are all, or nearly all, of one race (such as urban ghettos or conversely, affluent suburbs).
This is opposed to de jure segregation, which prevailed in the American South and border states through the 1960s. Under de jure segregation, the law provided entirely separate schools for black and white students, which they legally had to attend, despite in many cases actually living closer to a school designated for the other race. In many cases, the schools for black students were older, had fewer resources of all kinds, and paid their teachers less than in white schools.
Standards.
A <dfn >de facto standard</dfn> is a standard (formal or informal) that has achieved a dominant position by tradition, enforcement, or market dominance. It has not necessarily received formal approval by way of a standardization process, and may not have an official standards document.
<dfn >Technical standard</dfn>s are usually voluntary, like ISO 9000 requirements, but may be obligatory, enforced by government norms, like drinking water quality requirements. The term "de facto standard" is used for both: to contrast obligatory standards (also known as "de jure standards"); or to express a dominant standard, when there are more than one proposed standards.
In social sciences, a voluntary standard that is also a de facto standard, is a typical solution to a coordination problem.
National languages.
Several countries with a de facto national language, including Australia, Japan, the United Kingdom, the United States, and the Argentine, have no de jure official national language.
Some countries have a de facto national language in addition to an official language. In Lebanon and Morocco the official language is Arabic, but an additional de facto language is French. In New Zealand, Maori and New Zealand Sign Language are de jure official languages, while English is a de facto official language.
Russian was the de facto official language of the central government and, to a large extent, republican governments of the former Soviet Union, but was not declared de jure state language until 1990. A short-lived law effected April 24, 1990, installed Russian as the sole de jure official language of the Union.
Politics.
A de facto government is a government wherein all the attributes of sovereignty have, by usurpation, been transferred from those who had been legally invested with them to others, who, sustained by a power above the forms of law, claim to act and do really act in their stead.
In politics, a de facto leader of a country or region is one who has assumed authority, regardless of whether by lawful, constitutional, or legitimate means; very frequently, the term is reserved for those whose power is thought by some faction to be held by unlawful, unconstitutional, or otherwise illegitimate means, often because it had deposed a previous leader or undermined the rule of a current one. De facto leaders sometimes do not hold a constitutional office and may exercise power informally.
Not all dictators are de facto rulers. For example, Augusto Pinochet of Chile initially came to power as the chairperson of a military junta, which briefly made him de facto leader of Chile, but he later amended the nation's constitution and made himself president for life, making him the formal and legal ruler of Chile. Similarly, Saddam Hussein's formal rule of Iraq is often recorded as beginning in 1979, the year he assumed the Presidency of Iraq. However, his de facto rule of the nation began earlier: during his time as vice president, he exercised a great deal of power at the expense of the elderly Ahmed Hassan al-Bakr, the de jure president.
In Argentina, the successive military coups that overthrew constitutional governments installed de facto governments in 1930, 1943–1945, 1955–1958, 1966–1973 and 1976–1983, the last of which combined the powers of the presidential office with those of the National Congress. The subsequent legal analysis of the validity of such actions led to the formulation of a doctrine of the de facto governments, a case law (precedential) formulation which essentially said that the actions and decrees of past de facto governments, although not rooted in legal legitimacy when taken, remained binding until and unless such time as they were revoked or repealed de jure by a subsequent legitimate government.
That doctrine was nullified by the constitutional reform of 1994. :
In 1526, after seizing power Imam Ahmad ibn Ibrahim al-Ghazi made his brother, Umar Din, the de jure Sultan of the Adal Sultanate. Ahmad, however, was in all practice the de facto Sultan. Some other notable true de facto leaders have been Deng Xiaoping of the People's Republic of China and General Manuel Noriega of Panama. Both of these men exercised nearly all control over their respective nations for many years despite not having either legal constitutional office or the legal authority to exercise power. These individuals are today commonly recorded as the "leaders" of their respective nations; recording their legal, correct title would not give an accurate assessment of their power. Terms like "strongman" or "dictator" are often used to refer to de facto rulers of this sort. In the Soviet Union, after Vladimir Lenin incapacitated from a stroke in 1923, Joseph Stalin—who, as General Secretary of the Communist Party had the power to appoint anyone he chose to top party positions—eventually emerged as leader of the Party and the legitimate government. Until the 1936 Soviet Constitution officially declared the Party "...the vanguard of the working people", thus legitimizing Stalin's leadership, Stalin ruled the USSR as the de facto dictator.
Another example of a de facto ruler is someone who is not the actual ruler but exerts great or total influence over the true ruler, which is quite common in monarchies. Some examples of these de facto rulers are Empress Dowager Cixi of China (for son Tongzhi and nephew Guangxu Emperors), Prince Alexander Menshikov (for his former lover Empress Catherine I of Russia), Cardinal Richelieu of France (for Louis XIII), and Queen Marie Caroline of Naples and Sicily (for her husband King Ferdinand I of the Two Sicilies).
The term "de facto head of state" is sometimes used to describe the office of a governor general in the Commonwealth realms, since a holder of that office has the same responsibilities in their country as the de jure head of state (the sovereign) does within the United Kingdom.
In the Westminster system of government, executive authority is often split between a de jure executive authority of a head of state and a de facto executive authority of a prime minister and cabinet who implement executive powers in the name of the de jure executive authority. In the United Kingdom, the Sovereign is the de jure executive authority, even though executive decisions are made by the elected Prime Minister and his Cabinet on the Sovereign's behalf, hence the term Her Majesty's Government.
The de facto boundaries of a country are defined by the area that its government is actually able to enforce its laws in, and to defend against encroachments by other countries that may also claim the same territory de jure. The Durand Line is an example of a de facto boundary. As well as cases of border disputes, de facto boundaries may also arise in relatively unpopulated areas in which the border was never formally established or in which the agreed border was never surveyed and its exact position is unclear. The same concepts may also apply to a boundary between provinces or other subdivisions of a federal state.
Other uses.
A de facto monopoly is a system where many suppliers of a product are allowed, but the market is so completely dominated by one that the others might as well not exist. The related terms oligopoly and monopsony are similar in meaning and this is the type of situation that antitrust laws are intended to eliminate.
Relationships.
A domestic partner outside marriage is referred to as a de facto husband or wife by some authorities. In Australia and New Zealand, the phrase "de facto" by itself has become a colloquial term for one's domestic partner. In Australian law, it is the legally recognized relationship of a couple living together (opposite-sex or same-sex).
The above sense of "de facto" is related to the relationship between common law traditions and formal (statutory, regulatory, civil) law, and common-law marriages. Common law norms for settling disputes in practical situations, often worked out over many generations to establishing precedent, are a core element informing decision making in legal systems around the world. Because its early forms originated in England in the Middle Ages, this is particularly true in Anglo-American legal traditions and in former colonies of the British Empire, while also playing a role in some countries that have mixed systems with significant admixtures of civil law.
Relationships not recognized outside of Australia.
Due to Australian federalism, de facto partnerships can only be legally recognised whilst the couple lives within a state in Australia. This is because the power to legislate on de facto matters relies on referrals by States to the Commonwealth in accordance with Section 51(xxxvii) of the Australian Constitution, where it states the new federal law can only be applied back within a state. There must be a state nexus between the de facto relationship itself and the Australian state.
If an Australian de facto couple moves out of a state, they do not take the state with them and the new federal law is tied to the territorial limits of a state. The legal status and rights and obligations of the de facto or unmarried couple would then be recognised by the laws of the country where they are ordinarily resident. See the section on Family Court of Australia for further explanation on jurisdiction on de facto relationships.
This is unlike marriage and ‘matrimonial causes’ which are recognised by sections 51(xxi) and (xxii) of the and internationally by Marriage law and conventions, Hague Convention on Marriages (1978).
Non-marital relationship contract.
"<dfn >De Facto [Relationship]</dfn>" is comparable to non-marital relationship contracts (sometimes called "Palimony Agreements") and certain limited forms of domestic partnership, which are found in many jurisdictions throughout the world.
A "De Facto Relationship" is not comparable to common-law marriage, which is a fully legal marriage that has merely been contracted in an irregular way (including by habit and repute). Only nine U.S. states and the District of Columbia still permit this irregular form of marriage; but common law marriages are otherwise valid and recognised by and in all jurisdictions whose rules of comity mandate the recognition of any marriage that was legally formed in the jurisdiction where it was contracted.
Other uses of the term.
In finance, the World Bank has a pertinent definition:
A de facto state of war is a situation where two nations are actively engaging, or are engaged, in aggressive military actions against the other without a formal declaration of war.
In engineering, a "<dfn >de facto technology</dfn>" refers to systems in which the intellectual property and know-how is privately held. Usually only the owner of the technology manufactures the related equipment. Meanwhile, a "<dfn >standard technology</dfn>" consists of systems that have been publicly released to a certain degree so that anybody can manufacture equipment supporting the technology. For instance, in cell phone communications, CDMA1X is a "de facto technology", while GSM is a "standard technology".

</doc>
<doc id="43596" url="http://en.wikipedia.org/wiki?curid=43596" title="United States Naval Observatory">
United States Naval Observatory

The United States Naval Observatory (USNO) is one of the oldest scientific agencies in the United States, with a primary mission to produce Positioning, Navigation and Timing (PNT) for the U.S. Navy and the U.S. Department of Defense. Located in Northwest Washington, D.C. at the Northwestern end of Embassy Row, it is one of the pre-1900 astronomical observatories located in an urban area; at the time of its construction, it was far from the light pollution thrown off by the (then-smaller) city center. The USNO operates the "Master Clock", which provides precise time to the GPS satellite constellation run by the U.S. Air Force. The USNO performs radio VLBI-based positions of quasars with numerous global collaborators, in order to produce Earth Orientation parameters.
Aside from its scientific mission, since 1974, the official residence of the Vice President of the United States has been Number One Observatory Circle, a house on the grounds of the Naval Observatory.
History.
Established by the order of the Secretary of the Navy John Branch on 6 December 1830 as the Depot of Charts and Instruments, the Observatory rose from humble beginnings. Placed under the command of Lieutenant Louis M. Goldsborough, with an annual budget of $330, its primary function was the restoration, repair, and rating of navigational instruments. It was made into a national observatory in 1842 via a federal law and a Congressional appropriation of $25,000. Lieutenant James Melville Gilliss was put in charge of "obtaining the instruments needed and books." Lt. Gilliss visited the principal observatories of Europe with the mission to purchase telescopes and scientific devices and books.
The observatory's primary mission was to care for the United States Navy's marine chronometers, charts, and other navigational equipment. It calibrated ships' chronometers by timing the transit of stars across the meridian. Initially located downtown at in Foggy Bottom (near the Lincoln Memorial), the observatory moved in 1893 to its present location on a 2000-foot circle of land atop Observatory Hill overlooking Massachusetts Avenue.
The first superintendent was Navy Commander Matthew Fontaine Maury. Maury had the world's first vulcanized time ball, created to his specifications by Charles Goodyear for the U.S. Observatory. It was the first time ball in the United States, being placed into service in 1845, and the 12th in the world. Maury kept accurate time by the stars and planets. The time ball was dropped every day except Sunday precisely at the astronomically defined moment of Mean Solar Noon, enabling all ships and civilians to know the exact time. By the end of the American Civil War, the Observatory's clocks were linked via telegraph to ring the alarm bells in all of the Washington, D.C. firehouses three times a day, and by the early 1870s the Observatory's daily noon time signal was being distributed nationwide via the Western Union Telegraph Company. Time was also "sold" to the railroads and was used in conjunction with railroad chronometers to schedule American rail transport. Early in the 20th century, the Arlington Time Signal broadcast this service to wireless receivers.
The names "National Observatory" and "Naval Observatory" were both used for 10 years, until a ruling was passed to officially use the latter. The precedent to have been ascribed the title 'national observatory' can in part be attributed to the efforts of John Quincy Adams during that period, who made protracted efforts to bring astronomy to a national level at that time.
In 1849 the Nautical Almanac Office (NAO) was established in Cambridge, Massachusetts as a separate organization. It was moved to Washington, D. C. in 1866, colocating with the U. S. Naval Observatory in 1893. On September 20, 1894, the NAO became a "branch" of USNO, however it remained autonomous for several years after this.
President John Quincy Adams, who in 1825 signed the bill for the creation of a national observatory just before leaving presidential office, had intended for it to be called the National Observatory. He spent many nights at the observatory with Maury, watching and charting the stars, which had always been one of Adams' avocations.
An early scientific duty assigned to the Observatory was the U.S. contribution to the definition of the Astronomical Unit, or the AU, which defines a standard mean distance between the Sun and the Earth, conducted under the auspices of the Congressionally funded U.S. Transit of Venus Commission. The astronomical measurements taken of the transit of Venus by a number of countries since 1639 resulted in a progressively more accurate definition of the AU. Relying heavily on photographic methods, the naval observers returned 350 photographic plates in 1874, and 1,380 measurable plates in 1882. The results of the surveys conducted simultaneously from several locations around the world (for each of the two transits) produced a final value of the solar parallax, after adjustments, of 8.809", with a probable error of 0.0059", yielding a "U.S. defined" Earth-Sun distance of 92,797,000 mi, with a probable error of 59,700 mi miles. This calculated distance was a significant improvement over several previous estimates.
The telescope used for the discovery of the Moons of Mars was the 26-inch (66 cm) refractor (a telescope with a lens), then located at Foggy Bottom. In 1893 it was moved to the present location, into the 21st century.
In November 1913 the Paris Observatory, using the Eiffel Tower as an antenna, exchanged sustained wireless (radio) signals with the United States Naval Observatory, using an antenna in Arlington, Virginia to determine the exact difference of longitude between the two institutions.
In 1934, the last large telescope to be installed at USNO saw "first light". This 40-inch aperture instrument was also the second (and final) telescope made by famed optician, George Ritchey. The Ritchey-Chrétien telescope design has since become the de facto optical design for nearly all major telescopes, including the famed Keck telescopes and the spaceborne Hubble Telescope. Unfortunately, light pollution forced USNO to think of other more viable locations to continue work, and so began a search. The final dark sky site chosen was Flagstaff, Arizona, and so the 40-inch telescope was moved to that location, beginning operations at the new Navy command, now called the Naval Observatory Flagstaff Station (NOFS). Those operations commenced in 1955, and within a decade, the Navy's largest telescope, the 61-inch "Kaj Strand Astrometric Reflector" was built, seeing light at NOFS in 1964.
The modern United States Naval Observatory continues to be a major authority in the areas of Precise Time and Time Interval, Earth orientation, astrometry and celestial observation. In collaboration with many national and international scientific establishments, it determines the timing and astronomical data required for accurate navigation, astrometry, and fundamental astronomy and calculation methods—and distributes this information (such as astronomical catalogs) in the Astronomical Almanac, the Nautical Almanac, and on-line.
Perhaps it is best known to the general public for its highly accurate ensemble of atomic clocks and its year 2000 Timeball replacement. The site also houses the largest astronomy library in the United States (and the largest astrophysical periodicals collection in the world). The library includes a large collection of rare, often famous, physics and astronomy books from across the past millennium.
USNO continues to maintain its dark-sky observatory, NOFS, near Flagstaff, Arizona, which also now oversees the Navy Precision Optical Interferometer. NOFS opens to the public one weekend annually, in the autumn. The Alternate Master Clock, mentioned above, also continues to operate at Schriever AFB in Colorado.
Departments.
In 1990, the Orbital Mechanics Department and Astronomical Applications Department were established, and Nautical Almanac Office became a division of the Astronomical Applications Department. The Orbital Mechanics Department operated under P.K. Seidelmann until 1994 when the department was abolished, and its functions were moved to a group within the Astronomical Applications Department. In 2010, USNO's astronomical 'department' known as the Naval Observatory Flagstaff Station was officially made autonomous as an Echelon Five command separate from USNO, but reporting to it. In the 7000-plus foot alpine woodlands adjacent to Flagstaff, Arizona, USNO performs its national, Celestial Reference Frame (CRF) mission under dark skies unique to that region.
Official residence of the Vice President of the United States.
Since 1974, and separated from auspices of the Naval Observatory, Number One Observatory Circle, a house situated in the grounds of the observatory, which was formerly the residence of its superintendent, and later the home of the Chief of Naval Operations, has been the official residence of the Vice President of the United States.
According to a May 15, 2009 blog posting by "Newsweek's" Eleanor Clift, Vice President Joe Biden revealed the existence of what Clift described as a bunker-like room in the residence. The bunker is the secure, undisclosed location where former Vice President Dick Cheney remained under protection in secret after the 9/11 attacks: according to Clift's report, entitled "Shining Light on Cheney's Hideaway": "Biden said a young naval officer giving him a tour of the residence showed him the hideaway, which is behind a massive steel door secured by an elaborate lock with a narrow connecting hallway lined with shelves filled with communications equipment." Biden's press office subsequently issued a statement denying the bunker report, suggesting that Biden had instead been describing "an upstairs workspace".
Time service.
The U.S. Naval Observatory operates two Master Clock facilities. The primary facility, in Washington, D.C. maintains 57 HP/Agilent/Symmetricom 5071A-001 high performance cesium atomic clocks and 24 hydrogen masers. The alternate master clock, at Schriever Air Force Base, maintains 12 cesium clocks and 3 masers. The observatory also operates four rubidium atomic fountain clocks, which have a stability reaching 7×10−16. The observatory intends to build several more of this type for use at its two facilities.
The U.S. Naval Observatory provides public time service via 26 NTP servers on the public Internet, and via telephone voice announcements:
The voice of actor Fred Covington (1928–1993) has been announcing the USNO time since 1978.
The voice announcements follow the same pattern at both sites. They always begin with the local time (daylight or standard), and include a background of 1-second ticks. Local time announcements are made on the minute, and 15, 30, and 45 seconds after the minute. Coordinated Universal Time (UTC) is announced five seconds after the local time. Upon connecting, only the second-marking ticks are heard for the few seconds before the next scheduled local time announcement
The USNO also operates a modem time service, and provides time to the Global Positioning System.
Instrument Shop.
The United States Naval Observatory Instrument shop has been manufacturing precise instrumentation since the early 1900s. Today's Instrument Shop has highly dedicated, motivated and knowledgeable instrument makers.
External links.
Jump to: navigation, search

</doc>
<doc id="43597" url="http://en.wikipedia.org/wiki?curid=43597" title="Exciton">
Exciton

An exciton is a bound state of an electron and an electron hole which are attracted to each other by the electrostatic Coulomb force. It is an electrically neutral quasiparticle that exists in insulators, semiconductors and in some liquids. The exciton is regarded as an elementary excitation of condensed matter that can transport energy without transporting net electric charge.
An exciton can form when a photon is absorbed by a semiconductor. This excites an electron from the valence band into the conduction band. In turn, this leaves behind a positively-charged electron hole (an abstraction for the location from which an electron was moved). The electron in the conduction band is then effectively attracted to this localized hole by the repulsive Coulomb forces from large numbers of electrons surrounding the hole and excited electron. This attraction provides a stabilizing energy balance. Consequently, the exciton has slightly less energy than the unbound electron and hole. The wavefunction of the bound state is said to be "hydrogenic", an exotic atom state akin to that of a hydrogen atom. However, the binding energy is much smaller and the particle's size much larger than a hydrogen atom. This is because of both the screening of the Coulomb force by other electrons in the semiconductor (i.e., its dielectric constant), and the small effective masses of the excited electron and hole. The recombination of the electron and hole, i.e. the decay of the exciton, is limited by resonance stabilization due to the overlap of the electron and hole wave functions, resulting in an extended lifetime for the exciton.
The electron and hole may have either parallel or anti-parallel spins. The spins are coupled by the exchange interaction, giving rise to exciton fine structure. In periodic lattices, the properties of an exciton show momentum (k-vector) dependence.
The concept of excitons was first proposed by Yakov Frenkel in 1931, when he described the excitation of atoms in a lattice of insulators. He proposed that this excited state would be able to travel in a particle-like fashion through the lattice without the net transfer of charge.
Classification.
Excitons may be treated in two limiting cases, depending on the properties of the material in question.
Frenkel excitons.
In materials with a small dielectric constant, the Coulomb interaction between an electron and a hole may be strong and the excitons thus tend to be small, of the same order as the size of the unit cell. Molecular excitons may even be entirely located on the same molecule, as in fullerenes. This "Frenkel exciton", named after Yakov Frenkel, has a typical binding energy on the order of 0.1 to 1 eV. Frenkel excitons are typically found in alkali halide crystals and in organic molecular crystals composed of aromatic molecules, such as anthracene and tetracene.
Wannier-Mott excitons.
In semiconductors, the dielectric constant is generally large. Consequently, electric field screening tends to reduce the Coulomb interaction between electrons and holes. The result is a "Wannier exciton", which has a radius larger than the lattice spacing. As a result, the effect of the lattice potential can be incorporated into the effective masses of the electron and hole. Likewise, because of the lower masses and the screened Coulomb interaction, the binding energy is usually much less than that of a hydrogen atom, typically on the order of . This type of exciton was named for Gregory Wannier and Nevill Francis Mott. Wannier-Mott excitons are typically found in semiconductor crystals with small energy gaps and high dielectric constants, but have also been identified in liquids, such as liquid xenon.
In single-wall carbon nanotubes, excitons have both Wannier-Mott and Frenkel character. This is due to the nature of the Coulomb interaction between electrons and holes in one-dimension. The dielectric function of the nanotube itself is large enough to allow for the spatial extent of the wave function to extend over a few to several nanometers along the tube axis, while poor screening in the vacuum or dielectric environment outside of the nanotube allows for large (0.4 to ) binding energies.
Often there is more than one band to choose from for the electron and the hole leading to different types of excitons in the same material. Even high-lying bands can be effective as femtosecond two-photon experiments have shown. At cryogenic temperatures, many higher excitonic levels can be observed approaching the edge of the band, forming a series of spectral absorption lines that are in principle similar to hydrogen spectral series.
Charge-transfer excitons.
An intermediate case between Frenkel and Wannier excitons, "charge-transfer excitons" (sometimes called simply "CT excitons") occur when the electron and the hole occupy adjacent molecules. They occur primarily in ionic crystals. Unlike Frenkel and Wannier excitons they display a static electric dipole moment.
Surface excitons.
At surfaces it is possible for so called "image states" to occur, where the hole is inside the solid and the electron is in the vacuum. These electron-hole pairs can only move along the surface.
Atomic and molecular excitons.
Alternatively, an exciton may be an excited state of an atom, ion, or molecule, the excitation wandering from one cell of the lattice to another.
When a molecule absorbs a quantum of energy that corresponds to a transition from one molecular orbital to another molecular orbital, the resulting electronic excited state is also properly described as an exciton. An electron is said to be found in the lowest unoccupied orbital and an electron hole in the highest occupied molecular orbital, and since they are found within the same molecular orbital manifold, the electron-hole state is said to be bound. Molecular excitons typically have characteristic lifetimes on the order of nanoseconds, after which the ground electronic state is restored and the molecule undergoes photon or phonon emission. Molecular excitons have several interesting properties, one of which is energy transfer (see Förster resonance energy transfer) whereby if a molecular exciton has proper energetic matching to a second molecule's spectral absorbance, then an exciton may transfer ("hop") from one molecule to another. The process is strongly dependent on intermolecular distance between the species in solution, and so the process has found application in sensing and "molecular rulers".
Interaction.
Excitons are the main mechanism for light emission in semiconductors at low temperature (when the characteristic thermal energy "kT" is less than the exciton binding energy), replacing the free electron-hole recombination at higher temperatures.
The existence of exciton states may be inferred from the absorption of light associated with their excitation. Typically, excitons are observed just below the band gap.
When excitons interact with photons a so-called polariton (also exciton-polariton) is formed. These excitons are sometimes referred to as "dressed excitons".
Provided the interaction is attractive, an exciton can bind with other excitons to form a biexciton, analogous to a dihydrogen molecule. If a large density of excitons is created in a material, they can interact with one another to form an electron-hole liquid, a state observed in k-space indirect semiconductors.
Additionally, excitons are integer-spin particles obeying Bose statistics in the low-density limit. In some systems, where the interactions are repulsive, a Bose–Einstein condensed state is predicted to be the ground state. Exciton condensates have been seen in a double quantum well systems.
Direct and Indirect.
Normally, excitons in a semiconductor have a very short lifetime due to the close proximity of the electron and hole. However, by placing the electron and hole in spatially separated quantum wells with an insulating barrier layer in between so called 'indirect' excitons can be created. In contrast to ordinary (direct), these indirect excitons can have large spatial separation between the electron and hole, and thus possess a much longer lifetime. This is often used to cool excitons to very low temperatures in order to study Bose Einstein condensation (or rather it's 2 dimensional analog) 

</doc>
<doc id="43598" url="http://en.wikipedia.org/wiki?curid=43598" title="Fjord">
Fjord

Geologically, a fjord (British English: /fjɔːd/, /fɪɔːd/; American English: /fɪɔɹd/, rare: /fj-/; Australian English: /fɪ̝oːd/; Norwegian Bokmål and Nynorsk: /fjur/, dialectal: /-uɽ/) (variant spelling: fiord, especially in New Zealand) is a long, narrow inlet with steep sides or cliffs, created by glacial erosion. The word comes to English from Norwegian, in many cases to refer to any long narrow body of water other than the more specific meaning it has in English. There are many fjords on the coasts of Norway, Iceland, Greenland, Alaska, Kerguelen Islands, New Zealand, British Columbia, Nunavut, Washington and Chile. The Norwegian definition of "fjord" differs from that of English - in Norwegian "fjord" refers to any inlet or channel (see Oslofjord).
Formation.
A fjord is formed when a glacier cuts a U-shaped valley by ice segregation and abrasion of the surrounding bedrock. Glacial melting is accompanied by the rebounding of the Earth's crust as the ice load and eroded sediment is removed (also called isostasy or glacial rebound). In some cases this rebound is faster than sea level rise. Most fjords are deeper than the adjacent sea; Sognefjord, Norway, reaches as much as 1300 m below sea level. Fjords generally have a sill or shoal (bedrock) at their mouth caused by the previous glacier's reduced erosion rate and terminal moraine. In many cases this sill causes extreme currents and large saltwater rapids (see skookumchuck). Saltstraumen in Norway is often described as the world's strongest tidal current. These characteristics distinguish fjords from rias (e.g. the Bay of Kotor), which are drowned valleys flooded by the rising sea.
Fjord features and variations.
Coral reefs.
As late as 2000, some coral reefs were discovered along the bottoms of the Norwegian fjords. These reefs were found in fjords from the north of Norway to the south. The marine life on the reefs is believed to be one of the most important reasons why the Norwegian coastline is such a generous fishing ground. Since this discovery is fairly new, little research has been done. The reefs are host to thousands of lifeforms such as plankton, coral, anemones, fish, several species of shark, and many more. Most are specially adapted to life under the greater pressure of the water column above it, and the total darkness of the deep sea.
New Zealand's fiords are also host to deep-water corals, but a surface layer of dark fresh water allows these corals to grow in much shallower water than usual. An underwater observatory in Milford Sound allows tourists to view them without diving.
Skerries.
In some places near the seaward margins of areas with fjords, the ice-scoured channels are so numerous and varied in direction that the rocky coast is divided into thousands of island blocks, some large and mountainous while others are merely rocky points or rock reefs, menacing navigation. These are called skerries. The term skerry is derived from the Old Norse "sker", which means a rock in the sea.
Skerries most commonly formed at the outlet of fjords where submerged glacially formed valleys perpendicular to the coast join with other cross valleys in a complex array. The island fringe of Norway is such a group of skerries (called a "skjærgård"); many of the cross fjords are so arranged that they parallel the coast and provide a protected channel behind an almost unbroken succession of mountainous islands and skerries. By this channel one can travel through a protected passage almost the entire 1601 km route from Stavanger to North Cape, Norway. The Blindleia is a skerry-protected waterway that starts near Kristiansand in southern Norway, and continues past Lillesand. The Swedish coast along Bohuslän is likewise skerry guarded. The Inside Passage provides a similar route from Seattle, Washington, and Vancouver, British Columbia, to Skagway, Alaska. Yet another such skerry protected passage extends from the Straits of Magellan north for 800 km.
 Hardangerfjord in Hordaland, Norway
Etymology.
With Indo European origin ("*prtús" from "*por-" or "*per") in the verb "fara" (travelling/ferrying), the Norse noun substantive "fjǫrðr" means a "lake-like" waterbody used for passage and ferrying.
The Scandinavian "fjord", Proto-Scandinavian *"ferþuz", is the origin for similar European words: Icelandic "fjörður", Swedish "fjärd" (for Baltic waterbodies), Scots "firth". The Danish even use "fjord" for shallow lagoons as well as minor bodies of water cut into land; compare Scottish "loch". The Germans call the narrow long bays of Schleswig-Holstein "Förde" but the Norwegian bays "Fjord". The word is also related to English "ford" (in German "Furt", Low German "Ford" or "Vörde", in Dutch names "voorde", cf. Vilvoorde), Greek "poros", and Latin "portus". Fjord/firth/Förde as well as ford/Furt/Vörde/voorde refer to a Germanic verb for "to travel": Swedish "fara", Danish "fare", Dutch "varen", German "fahren"; English "to fare" has partially lost that meaning. The one geographic object is a waterbody that allows the traveller to enter the land by boat, the other one is the shallow site in a waterbody that allows the traveller to cross the water on foot, horse or wheels.
As a loanword from Norwegian, it is one of the few words in the English language to start with the sequence "fj", although the word was for a long time normally rendered "fiord", a spelling preserved in place names such as Grise Fiord, but now generally current only in New Zealand English.
Scandinavian usage.
The use of the word fjord in Norwegian, Danish and Swedish is more general than in English and in international scientific terminology. In Scandinavia, "fjord" is used for a narrow inlet of the sea in Norway, Denmark and western Sweden, but this is not its only application. In Norway and Iceland, the usage is closest to the Old Norse, with fjord used for both a firth and for a long, narrow inlet. In eastern Norway, the term is also applied to long narrow freshwater lakes (for instance Mjøsa [commonly referred to as "fjorden"], Randsfjorden and Tyrifjorden) and sometimes even to rivers (in local usage, for instance in Flå in Hallingdal, the Hallingdal river is referred to as "fjorden"). In east Sweden, the name "fjärd" is used in a synonymous manner for bays, bights and narrow inlets on the Swedish Baltic Sea coast, and in most Swedish lakes. This latter term is also used for bodies of water off the coast of Finland where Finland Swedish is spoken. In Danish, the word may even apply to shallow lagoons. In modern Icelandic, "fjörður" is still used with the broader meaning of firth or inlet. In Faroese "fjørður" is used both about inlets and about broader sounds, whereas a narrower sound is called "sund". In the Finnish language, a word "vuono" is used although there is only one fjord in Finland. Small waterfalls within these fjords are also used as freshwater resources for the people of Scandinavia and, in particular, Norway.
The German use of the word "Förde" for long narrow bays on their Baltic Sea coastline, indicates a common Germanic origin of the word. The landscape consists mainly of moraine heaps. The "Förden" and some "fjords" on the east side of Jutland, Denmark are also of glacial origin. But while the glaciers digging "real" fjords moved from the mountains to the sea, in Denmark and Germany they were tongues of a huge glacier covering the basin of which is now the Baltic Sea. See Förden and East Jutland Fjorde.
Whereas fjord names mostly describe bays (though not always geological fjords), straits in the same regions typically are named "Sund", in Scandinavian languages as well as in German. The word is related to "to sunder" in the meaning of "to separate". So the use of "Sound" to name fjords in North America and New Zealand differs from the European meaning of that word.
The name of Wexford in Ireland is originally derived from "Veisafjǫrðr" ("inlet of the mud flats") in Old Norse, as used by the Viking settlers—though the inlet at that place in modern terms is an estuary, not a fjord.
False fjords.
The differences in usage between the English and the Scandinavian languages have contributed to confusion in the use of the term fjord. Bodies of water that are clearly fjords in Scandinavian languages are not considered fjords in English; similarly bodies of water that would clearly not be fjords in the Scandinavian sense have been named or suggested to be fjords. Examples of this confused usage follow.
The Bay of Kotor in Montenegro has been suggested by some to be a fjord, but is in fact a drowned river canyon or ria. Similarly the Lim bay in Istria, Croatia, is sometimes called "Lim fjord" although it is not actually a fjord carved by glacial erosion but instead a ria dug by the river Pazinčica. The Croats call it "Limski kanal", which does not translate precisely to the English equivalent either.
In the Danish language any inlet is called a fjord, but none of the "fjords" of Denmark may be considered a fjord in the geological sense. Limfjord in English terminology is a sound, since it separates the North Jutlandic Island (Vendsyssel-Thy) from the rest of Jutland. Ringkøbing Fjord on the western coast of Jutland is a lagoon. The long narrow "fjords" of Denmark's Baltic Sea coast like the German Förden were dug by ice moving from the sea upon land, while fjords in the geological sense were dug by ice moving from the mountains down to the sea.
The fjords in Finnmark (Norway), which are fjords in the Scandinavian sense of the term, are considered by some to be false fjords. Although glacially formed, most Finnmark fjords lack the classic hallmark steep-sided valleys of the more southerly Norwegian fjords since the glacial pack was deep enough to cover even the high grounds when they were formed. The Oslofjord on the other hand is a rift valley, and not glacially formed.
In Acapulco, Mexico, the calanques—narrow, rocky inlets—on the western side of the city, where the famous cliff-divers perform daily, are described in the city's tourist literature as being fjords.
Freshwater fjords.
Some Norwegian freshwater lakes that have formed in long glacially carved valleys with terminal moraines blocking the outlet follow the Norwegian naming convention; they are named fjords. Outside of Norway, the three western arms of New Zealand's Lake Te Anau are named North Fiord, Middle Fiord and South Fiord. Another freshwater "fjord" in a larger lake is Western Brook Pond, in Newfoundland's Gros Morne National Park; it is also often described as a fjord, but is actually a freshwater lake cut off from the sea, so is not a fjord in the English sense of the term. Such lakes are sometimes called "fjord lakes". Okanagan Lake was the first North American lake to be so described, in 1962. The bedrock there has been eroded up to 650 m "below" sea level, which is 2000 m below the surrounding regional topography. Fjord lakes are common on the inland lea of the Coast Mountains and Cascade Range; notable ones include Lake Chelan, Seton Lake, Chilko Lake, and Atlin Lake. Kootenay Lake, Slocan Lake and others in the basin of the Columbia River are also fjord-like in nature, and created by glaciation in the same way. Along the British Columbia Coast, a notable fjord-lake is Owikeno Lake, which is a freshwater extension of Rivers Inlet. Quesnel Lake, located in central British Columbia, is claimed to be the deepest fjord formed lake on earth. Another area notable for fjord lakes is northern Italy and southern Switzerland: Lake Como and its neighbours.
Great Lakes.
Unique family of freshwater fjords are embayments of the North American Great Lakes. Baie Fine is located on the northwestern coast of Georgian Bay of Lake Huron in Ontario, and Huron Bay is located on the southern shore of Lake Superior in Michigan. 
Locations.
The principal mountainous regions where fjords have formed are in the higher middle latitudes and the high latitudes reaching to 80°N (Svalbard, Greenland), where, during the glacial period, many valley glaciers descended to the then-lower sea level. The fjords develop best in mountain ranges against which the prevailing westerly marine winds are orographically lifted over the mountainous regions, resulting in abundant snowfall to feed the glaciers. Hence coasts having the most pronounced fjords include the west coast of Norway, the west coast of North America from Puget Sound to Alaska, the southwest coast of New Zealand, and the west and to south-western coasts of South America, for example in Chile.
Other glaciated or formerly glaciated regions.
Other regions have fjords, but many of these are less pronounced due to more limited exposure to westerly winds and less pronounced relief. Areas include:
Extreme fjords.
The longest fjords in the world are:
Deep fjords include:

</doc>
<doc id="43600" url="http://en.wikipedia.org/wiki?curid=43600" title="Crayfish">
Crayfish

Crayfish, also known as crawfish, crawdads, freshwater lobsters, or mudbugs, are freshwater crustaceans resembling small lobsters, to which they are related; taxonomically, they are members of the superfamilies Astacoidea and Parastacoidea. They breathe through feather-like gills and are found in bodies of water. Some species are found in brooks and streams where there is fresh water running, while others thrive in swamps, ditches, and rice paddies. Most crayfish cannot tolerate polluted water, although some species such as "Procambarus clarkii" are hardier. Crayfish feed on living and dead animals and plants.
Names.
The name "crayfish" comes from the Old French word "escrevisse" (Modern French "écrevisse"). The word has been modified to "crayfish" by association with "fish" (folk etymology). The largely American variant "crawfish" is similarly derived.
Some kinds of crayfish are known locally as lobsters, crawdads, mudbugs, and yabbies. In the Eastern United States, "crayfish" is more common in the north, while "crawdad" is heard more in central and southwestern regions, and "crawfish" further south, although there are considerable overlaps.
The study of crayfish is called astacology.
Other animals.
In Australia (on the eastern seaboard), New Zealand and South Africa, the term "crayfish" or "cray" generally refers to a saltwater spiny lobster, of the genus "Jasus" that is indigenous to much of southern Oceania, while the freshwater species are usually called "yabby" or "kōura", from the indigenous Australian and Māori names for the animal respectively, or by other names specific to each species. Exceptions included Western Rock Lobster of the Palinuridae family found on the west coast of Australia; from the Parastacidae family Tasmanian giant freshwater crayfish found only in Tasmania, and the Murray crayfish found on Australia's Murray River.
In Singapore, the term "crayfish" typically refers to "Thenus orientalis", a seawater crustacean from the slipper lobster family. True crayfish are not native to Singapore, but are commonly found as pets, or as an invasive species ("Cherax quadricarinatus") in the many water catchment areas, and are alternatively known as "freshwater lobsters".
Anatomy.
The body of a decapod crustacean, such as a crab, lobster, or prawn (shrimp), is made up of twenty body segments grouped into two main body parts, the cephalothorax and the abdomen. Each segment may possess one pair of appendages, although in various groups these may be reduced or missing. On average, crayfish grow to 17.5 cm in length, but some grow larger. Walking legs have a small claw at the end.
Geographical distribution and classification.
There are three families of crayfish, two in the Northern Hemisphere and one in the Southern Hemisphere. The Southern-Hemisphere (Gondwana-distributed) family Parastacidae lives in South America, Madagascar and Australasia, and is distinguished by the lack of the first pair of pleopods. Of the other two families, members of the Astacidae live in western Eurasia and western North America and members of the family Cambaridae live in eastern Asia and eastern North America.
Madagascar has an endemic genus, "Astacoides", containing seven species.
Europe is home to seven species of crayfish in the genera "Astacus" and "Austropotamobius".
"Cambaroides" is native to Japan and eastern mainland Asia.
North America.
The greatest diversity of crayfish species is found in southeastern North America, with over 330 species in nine genera, all in the family Cambaridae. A further genus of astacid crayfish is found in the Pacific Northwest and the headwaters of some rivers east of the Continental Divide. Many crayfish are also found in lowland areas where the water is abundant in calcium, and oxygen rises from underground springs.
Crayfish were introduced purposely into a few Arizona reservoirs and other bodies of water decades ago, primarily as a food source for sport fish. They have since dispersed beyond those original sites.
Australasia.
Australasia has over 100 species in a dozen genera. In Australia many of the better-known crayfish are of the genus "Cherax", and include the marron from Western Australia (now believed to be two species, "Cherax tenuimanus" and "C. cainii"), red-claw crayfish ("Cherax quadricarinatus"), common yabby ("Cherax destructor") and western yabby ("Cherax preissii"). The marron are some of the largest crayfish in the world. They grow up to several pounds in size. "C. tenuimanus" is critically endangered, while other large Australasian crayfish are threatened or endangered. Australia is home to the world's two largest freshwater crayfish – the Tasmanian giant freshwater crayfish "Astacopsis gouldi", which can achieve a mass of up to 5 kg and is found in the rivers of northern Tasmania, and the Murray crayfish "Euastacus armatus", which can reach 2 kg and is found in much of the southern Murray-Darling basin.
In New Zealand two species of "Paranephrops" are endemic, and are known by the Māori name "kōura".
Fossil record.
Fossil records of crayfish older than 30 million years are rare, but fossilised burrows have been found from strata as old as the late Palaeozoic or early Mesozoic. The oldest records of the Parastacidae are in Australia, and are 115 million years old.
Crayfish plague.
Some crayfish suffer from a disease called crayfish plague, caused by the water mould "Aphanomyces astaci". Species of the genus "Astacus" are particularly susceptible to infection, allowing the more resistant signal crayfish to invade parts of Europe. Crayfish plague was transmitted to Europe when North American species of crayfish were introduced.
Uses.
Food.
Crayfish are eaten worldwide. Like other edible crustaceans, only a small portion of the body of a crayfish is eaten. In most prepared dishes, such as soups, bisques and étouffées, only the tail portion is served. At crawfish boils or other meals where the entire body of the crayfish is presented, other portions, such as the claw meat, may be eaten. Like all crustaceans, crayfish are not kosher because they are aquatic animals that do not have both fins and scales. They are therefore not eaten by observant Jews.
As of 2005, Louisiana supplies 95% of the crayfish harvested in the US. In 1987, Louisiana produced 90% of the crayfish harvested in the world, 70% of which were consumed locally. In 2007, the Louisiana crawfish harvest was about 54,800 tons, almost all of it from aquaculture. About 70%–80% of crayfish produced in Louisiana are "Procambarus clarkii" (red swamp crawfish), with the remaining 20%–30% being "Procambarus zonangulus" (white river crawfish).
Bait.
Crayfish are commonly sold and used as bait, either live or with only the tail meat, and are good at attracting channel catfish, walleye, trout, largemouth bass, smallmouth bass, pike and muskellunge. Sometimes the claws are removed so that the crayfish don't stop fish from biting the hook. Crayfish easily fall off the hook, so casting should be slow.
The result of using crayfish as bait has led to various ecological problems at times. According to a report prepared by Illinois State University, on the Fox River and Des Plaines River watershed, "The rusty crayfish (used as bait) has been dumped into the water and its survivors outcompete the native clearwater crayfish". This situation has been repeated elsewhere, as the crayfish bait eliminates native species.
The use of crayfish as bait has been cited as one of the ways zebra mussels have spread to different waterways, as members of this invasive species are known to attach themselves to crayfish.
Pets.
Crayfish are kept as pets in freshwater aquariums. Crayfish kept as pets in the US from local waters are usually kept with bluegill or bass, rather than goldfish or tropical or subtropical fish. They prefer foods like shrimp pellets or various vegetables, but will also eat tropical fish food, regular fish food, algae wafers, and small fish that can be captured with their claws. They will sometimes consume their old exoskeleton after it has moulted. Their disposition towards eating almost anything will also cause them to explore the edibility of aquarium plants in a fish tank. However, most species of dwarf crayfish, such as "Cambarellus patzcuarensis", will not destructively dig or eat live aquarium plants. They are also relatively non-aggressive and can be kept safely with dwarf shrimp. Because of their very small size of 1.5 in or less, some fish are often a threat to the crayfish.
Since crayfish are accustomed to being in ponds or rivers, they will have a tendency to shift gravel around on the bottom of the tank, creating mounds or trenches to emulate a burrow. Crayfish will often try to climb out of the tank, especially if an opening exists at the top that they can fit through.
In some nations, such as the United Kingdom, United States, Australia, and New Zealand, imported alien crayfish are a danger to local rivers. The three species commonly imported to Europe from the Americas are "Orconectes limosus", "Pacifastacus leniusculus" and "Procambarus clarkii". Crayfish may spread into different bodies of water because specimens captured for pets in one river are often released into a different catchment. There is a potential for ecological damage when crayfish are introduced into non-native bodies of water (e.g., crayfish plague in Europe).
Invasive species.
Crayfish have been recorded as an invasive species from Louisiana to Europe to China. They have been known to consume local rice crops in China. 

</doc>
<doc id="43601" url="http://en.wikipedia.org/wiki?curid=43601" title="Gnuplot">
Gnuplot

gnuplot is a command-line program that can generate two- and three-dimensional plots of functions, data, and data fits. It is frequently used for publication-quality graphics as well as education. The program runs on all major computers and operating systems (GNU/Linux, Unix, Microsoft Windows, Mac OS X, and others). It is a program with a fairly long history, dating back to 1986. Despite its name, this software is not distributed under the GNU General Public License (GPL), but its own more restrictive open source license.
Features.
gnuplot can produce output directly on screen, or in many formats of graphics files, including Portable Network Graphics (PNG), Encapsulated PostScript (EPS), Scalable Vector Graphics (SVG), JPEG and many others. It is also capable of producing LaTeX code that can be included directly in LaTeX documents, making use of LaTeX's fonts and powerful formula notation abilities. The program can be used both interactively and in batch mode using scripts.
The program is well supported and documented. Extensive help can also be found on the Internet.
The gnuplot core code is programmed in C. Modular subsystems for output via Qt, wxWidgets, and LaTeX/TikZ/ConTeXt are written in C++ and lua.
The code below creates the graph to the right.
The name of this program was originally chosen to avoid conflicts with a program called "newplot", and was originally a compromise between "llamaplot" and "nplot".
Distribution terms.
Despite gnuplot's name, it is not named after, part of or related to the GNU Project, nor does it use the GNU General Public License. It was named as part of a compromise by the original authors, punning on "gnu" (the animal) and "newplot".
Official source code to gnuplot is freely redistributable, but modified versions thereof are not. The gnuplot license instead recommends distribution of patches against official releases, optionally accompanied by officially released source code. Binaries may be distributed along with the unmodified source code and any patches applied thereto. Contact information must be supplied with derived works for technical support for the modified software.
Permission to modify the software is granted, but not the right to distribute the complete modified source code. Modifications are to be distributed as patches to the released version.
Despite this restriction, gnuplot is accepted and used by many GNU packages and is widely included in Linux distributions including the stricter ones such as Debian and Fedora. The OSI Open Source Definition and the Debian Free Software Guidelines specifically allow for restrictions on distribution of modified source code, given explicit permission to distribute both patches and source code.
Newer gnuplot modules (e.g. Qt, wxWidgets, and cairo drivers) have been contributed under dual-licensing terms, e.g. gnuplot + BSD or gnuplot + GPL.
GUIs and programs that use gnuplot.
Several third-party programs have graphical user interfaces that can be used to generate graphs using gnuplot as the plotting engine. These include:
Maxima is a text-based computer algebra system which itself has several third-party GUIs.
Other programs that use gnuplot include:
Programming and application interfaces.
gnuplot can be used from various programming languages to graph data, including Perl (via CPAN), Python (via and SAGE), Julia (via ), Java (via and ), Ruby (via ), Ch (via ), Haskell (via ) and Smalltalk (Squeak and GNU Smalltalk).
gnuplot also supports piping, which is typical of scripts. For script-driven graphics, gnuplot is by far the most popular program.
Alternatives.
Programmatic alternatives.
Pyxplot () is a free plotting program that has a very similar syntax but tries to enhance gnuplot's data processing and scripting capabilities.
Application programs could be linked with a graphics library such as:
Graphical alternatives.
For interactive plotting through a graphical user interface (GUI), the following open-source programs are available:

</doc>
<doc id="43602" url="http://en.wikipedia.org/wiki?curid=43602" title="Wassenaar Arrangement">
Wassenaar Arrangement

The Wassenaar Arrangement (full name: The Wassenaar Arrangement on Export Controls for Conventional Arms and Dual-Use Goods and Technologies) is a multilateral export control regime (MECR) with 41 participating states including many former COMECON (Warsaw Pact) countries.
The Wassenaar Arrangement has been established in order to contribute to regional and international security and stability, by promoting transparency and greater responsibility in transfers of conventional arms and dual-use goods and technologies, thus preventing destabilising accumulations. Participating States seek, through their national policies, to ensure that transfers of these items do not contribute to the development or enhancement of military capabilities which undermine these goals, and are not diverted to support such capabilities.
It is the successor to the Cold War-era Coordinating Committee for Multilateral Export Controls (COCOM), and was established on 12 July 1996, in Wassenaar, the Netherlands, which is near The Hague. The Wassenaar Arrangement is considerably less strict than COCOM, focusing primarily on the transparency of national export control regimes and not granting veto power to individual members over organizational decisions. A Secretariat for administering the agreement is located in Vienna, Austria. Like COCOM, however, it is not a treaty, and therefore is not legally binding.
Every six months member countries exchange information on deliveries of conventional arms to non-Wassenaar members that fall under eight broad weapons categories: battle tanks, armored combat vehicles (ACVs), large-caliber artillery, military aircraft, military helicopters, warships, missiles or missile systems, and small arms and light weapons.
Control lists.
The outline of the arrangement is set out in a document entitled "Guidelines & Procedures, including the Initial Elements". The is broken into two parts, the "List of Dual-Use Goods and Technologies" (also known as the "Basic List") and the "Munitions List". The Basic List is composed of ten categories based on increasing levels of sophistication:
Basic List has two nested subsections—Sensitive and Very Sensitive. Items of the Very Sensitive List include materials for stealth technology—i.e., equipment that could be used for submarine detection, advanced radar, and jet engine technologies.
The Munitions List has 22 categories, which are not labeled.
In order for an item to be placed on the lists, Member States must take into account the following :
Membership.
s of 2012[ [update]], the 41 participating states are: Argentina, Australia, Austria, Belgium, Bulgaria, Canada, Croatia, the Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Japan, Latvia, Lithuania, Luxembourg, Malta, Mexico, the Netherlands, New Zealand, Norway, Poland, Portugal, the Republic of Korea, Romania, the Russian Federation, Slovakia, Slovenia, South Africa, Spain, Sweden, Switzerland, Turkey, Ukraine, the United Kingdom and the United States. China and Israel are also not members, but they have aligned their export controls with Wassenaar lists, and are significant arms exporters.
The Arrangement is open on a global and non-discriminatory basis to prospective adherents that comply with the agreed criteria. Admission of new members requires the consensus of all members.
Admission requires states to:
Future memberships.
During a state visit to India in November 2010, U.S. president Barack Obama announced U.S. support for India's bid for permanent membership to UN Security Council as well as India's entry to Nuclear Suppliers Group, Wassenaar Arrangement, Australia Group, and Missile Technology Control Regime.

</doc>
<doc id="43604" url="http://en.wikipedia.org/wiki?curid=43604" title="Bud">
Bud

In botany, a bud is an undeveloped or embryonic shoot and normally occurs in the axil of a leaf or at the tip of the stem. Once formed, a bud may remain for some time in a dormant condition, or it may form a shoot immediately. Buds may be specialized to develop flowers or short shoots, or may have the potential for general shoot development. The term bud is also used in zoology, where it refers to an outgrowth from the body which can develop into a new individual.
Overview.
The buds of many woody plants, especially in temperate or cold climates, are protected by a covering of modified leaves called "scales" which tightly enclose the more delicate parts of the bud. Many bud scales are covered by a gummy substance which serves as added protection. When the bud develops, the scales may enlarge somewhat but usually just drop off, leaving on the surface of the growing stem a series of horizontally-elongated scars. By means of these scars one can determine the age of any young branch, since each year's growth ends in the formation of a bud, the formation of which produces an additional group of bud scale scars. Continued growth of the branch causes these scars to be obliterated after a few years so that the total age of older branches cannot be determined by this means.
In many plants scales are not formed over the bud, which is then called a naked bud. The minute underdeveloped leaves in such buds are often excessively hairy. Naked buds are found in some shrubs, like some species of the Sumac and Viburnums ("Viburnum alnifolium" and "V. lantana") and in herbaceous plants. In many of the latter, buds are even more reduced, often consisting of undifferentiated masses of cells in the axils of leaves. A terminal bud occurs on the end of a stem and lateral buds are found on the side. A head of cabbage (see Brassica) is an exceptionally large terminal bud, while Brussels sprouts are large lateral buds.
Since buds are formed in the axils of leaves, their distribution on the stem is the same as that of leaves. There are alternate, opposite, and whorled buds, as well as the terminal bud at the tip of the stem. In many plants buds appear in unexpected places: these are known as adventitious buds.
Often it is possible to find a bud in a remarkable series of gradations of bud scales. In the buckeye, for example, one may see a complete gradation from the small brown outer scale through larger scales which on unfolding become somewhat green to the inner scales of the bud, which are remarkably leaf-like. Such a series suggests that the scales of the bud are in truth leaves, modified to protect the more delicate parts of the plant during unfavorable periods.
Types of buds.
Buds are often useful in the identification of plants, especially for woody plants in winter when leaves have fallen. Buds may be classified and described according to different criteria: location, status, morphology, and function.
Botanists commonly use the following terms:
Within zoology.
The term bud (as in budding) is used by analogy within zoology as well, where it refers to an outgrowth from the body which develops into a new individual. It is a form of asexual reproduction limited to animals or plants of relatively simple structure. In this process a portion of the wall of the parent cell softens and pushes out. The protuberance thus formed enlarges rapidly while at this time the nucleus of the parent cell divides (see: mitosis, meiosis). One of the resulting nuclei passes into the bud, and then the bud is cut off from its parent cell and the process is repeated. Often the daughter cell will begin to bud before it becomes separated from the parent, so that whole colonies of adhering cells may be formed. Eventually cross walls cut off the bud from the original cell.

</doc>
<doc id="43607" url="http://en.wikipedia.org/wiki?curid=43607" title="Six Degrees of Kevin Bacon">
Six Degrees of Kevin Bacon

Six Degrees of Kevin Bacon is a parlour game based on the "six degrees of separation" concept, which posits that any two people on Earth are six or fewer acquaintance links apart. That idea eventually morphed into this parlour game wherein movie buffs challenge each other to find the shortest path between an arbitrary actor and prolific Hollywood character actor Kevin Bacon. It rests on the assumption that any individual involved in the Hollywood, California, film industry can be linked through his or her film roles to Kevin Bacon within six steps. The game requires a group of players to try to connect any such individual to Kevin Bacon as quickly as possible and in as few links as possible. It can also be described as a trivia game based on the concept of the small world phenomenon. In 2007, Bacon started a charitable organization named SixDegrees.org.
History.
In a January 1994 "Premiere" magazine interview about the film "The River Wild", Kevin Bacon commented that he had worked with everybody in Hollywood or someone who's worked with them. On April 7, 1994, a lengthy newsgroup thread headed "Kevin Bacon is the Center of the Universe" appeared. The game was created in early 1994 by three Albright College students, Craig Fass, Brian Turtle, and Mike Ginelli. According to an interview with the three in the spring 1999 issue of the college's magazine, "The Albright Reporter", they were watching "Footloose" during a heavy snowstorm. When the film was followed by "The Air Up There", they began to speculate on how many movies Bacon had been in and the number of people he had worked with. In the interview, Brian Turtle said, "It became one of our stupid party tricks I guess. People would throw names at us and we'd connect them to Kevin Bacon."
The trio wrote a letter to talk show host Jon Stewart, telling him that "Kevin Bacon was the center of the entertainment universe" and explaining the game. They appeared on "The Jon Stewart Show" and "The Howard Stern Show" with Bacon to explain the game. Bacon admitted that he initially disliked the game because he believed it was ridiculing him, but he eventually came to enjoy it. The three inventors released a book, "Six Degrees of Kevin Bacon" (ISBN 9780452278448), with an introduction written by Bacon. A board game based on the concept was released by Endless Games.
Bacon also appeared in a commercial for the Visa check card that parodied the game. In the commercial, Bacon wants to write a check to buy a book, but the clerk asks for his ID, which he does not have. He leaves and returns with a group of people, then says to the clerk, "Okay, I was in a movie with an extra, Eunice, whose hairdresser, Wayne, attended Sunday school with Father O'Neill, who plays racquetball with Dr. Sanjay, who recently removed the appendix of Kim, who dumped you sophomore year. So you see, we're practically brothers." In a similar vein, Dave Barry, in a column describing the unexpected complications that emerged when he attempted to find out the precise wording of the Lone Ranger's catchphrase, connected the Lone Ranger to Kevin Bacon in the following way: the Lone Ranger was the Green Hornet's great-uncle; the Green Hornet and O. J. Simpson both hung out with people named Kato; Simpson and Robert Wagner co-starred in "The Towering Inferno"; Wagner and Bacon co-starred in "Wild Things".
The concept was also presented in an episode of the TV show "Mad About You" dated November 19, 1996, in which a character expressed the opinion that every actor is only three degrees of separation from Kevin Bacon. Bacon spoofed the concept himself in a cameo he performed for the independent film "We Married Margo". Playing himself in a 2003 episode of "Will and Grace", Bacon connects himself to Val Kilmer through Tom Cruise and jokes "Hey, that was a short one!".The headline of "The Onion", a satirical newspaper, on October 30, 2002, was "Kevin Bacon Linked To Al-Qaeda". Bacon provides the voice-over commentary for the NY Skyride attraction at the Empire State Building in New York City. At several points throughout the commentary, Bacon alludes to his connections to Hollywood stars via other actors with whom he has worked.
In 2009, Bacon narrated a National Geographic Channel show "The Human Family Tree" – a program charting the work of the Genographic Project and its work on the genetic interconnectedness of all humans. In 2011, James Franco made reference to Six Degrees of Kevin Bacon while hosting the 83rd Academy Awards. In the summer of 2012, Google began to offer the ability to find an actor's Bacon number on its main page, by searching for the actor's name preceded by the phrase "bacon number".
EE began a UK television advertising campaign on November 3, 2012, based on the Six Degrees concept, where Kevin Bacon illustrates his connections and draws attention to how the EE 4G network allows similar connectivity.
The most highly connected nodes of the Internet have been referred to as "the 'Kevin Bacons' of the Web," inasmuch as they enable most users to navigate to most sites in 19 clicks or less. In "Weird Al" Yankovic's song "Lame Claim to Fame," one of the lines is, "I know a guy who knows a guy who knows a guy who knows a guy who knows a guy who knows Kevin Bacon," leading to a Bacon Number of 6.
Bacon numbers.
The Bacon number of an actor or actress is the number of degrees of separation he or she has from Bacon, as defined by the game. This is an application of the Erdős number concept to the Hollywood movie industry. The higher the Bacon number, the farther away from Kevin Bacon the actor is.
The computation of a Bacon number for actor X is a "shortest path" algorithm, applied to the co-stardom network:
Here is an example, using Elvis Presley:
Therefore, Asner has a Bacon number of 1, and Presley (who never appeared in a film with Bacon) has a Bacon number of 2.
s of 6 2013[ [update]], the highest finite Bacon number reported by the Oracle of Bacon is 11.
Because some people have both Bacon "and" Erdős numbers because of acting and publications, there are a rare few who have an Erdős–Bacon number, which is defined as the sum of a person's independent Erdős and Bacon numbers.
Undefined Bacon numbers.
A small percentage of actors have an undefined Bacon number, meaning that they cannot be linked to Bacon in any number of connections at all. According to the "Oracle of Bacon" website, approximately 12% of all actors cannot be linked to Bacon using its criteria, but this number is difficult to verify.
Center of the Hollywood Universe.
While at the University of Virginia, Brett Tjaden created the Oracle of Bacon, a computer program that uses information on some 800,000 people from the Internet Movie Database (IMDb). The algorithm calculates "how good a center" an individual IMDb personality is, i.e. a weighted average of the degree of separation of all the people that link to that particular person. The site returns an average "personality" number, e.g. for Clint Eastwood, it returns an average "Clint Eastwood Number." From there the Oracle site posits "The Center of the Hollywood Universe" as being the person with the lowest average personality number. Kevin Bacon, as it turns out, is not the "Center of the Hollywood Universe" (i.e. the most linkable actor). In fact, Bacon does not even make the top 100 list of average personality numbers. While he is not the most linkable actor, this still signifies being a better center than more than 99% of the people who have ever appeared in a film. Since each actor's average personality number can change with each new film made, the center can and does shift. "Centers" have included Rod Steiger, Donald Sutherland, Dennis Hopper and Harvey Keitel (the current center as of April 2013). Karen Black was the current and all-time highest woman in the list.
Photography book.
Inspired by the game, the British photographer Andy Gotts tried to reach Bacon through photographic links instead of film links. He wrote to 300 actors asking to take their pictures, and received permission from one, Joss Ackland. Ackland then suggested that Gotts photograph Greta Scacchi, with whom he had appeared in the film "White Mischief". Gotts proceeded from there, asking each actor to refer him to one or more friends or colleagues. Eventually, Christian Slater referred him to Bacon. Gotts' photograph of Bacon completed the project, eight years after it began. Gotts published the photos in a book, "Degrees" (ISBN 0-9546843-6-2), with text by Alan Bates, Pierce Brosnan, and Bacon.

</doc>
<doc id="43608" url="http://en.wikipedia.org/wiki?curid=43608" title="William Rufus Shafter">
William Rufus Shafter

William Rufus Shafter (October 16, 1835 – November 12, 1906) was a Union Army officer during the American Civil War who received America's highest military decoration, the Medal of Honor, for his actions at the Battle of Fair Oaks. Shafter also played a prominent part as a major general in the Spanish–American War. Fort Shafter, Hawaii, is named for him, as well as the city of Shafter, California and the ghost town of Shafter, Texas. He was nicknamed Pecos Bill.
Early life.
Shafter was born in Galesburg, Michigan on October 16, 1835. He worked as a teacher and farmer in the years preceding the Civil War.
Civil War and Indian campaigns.
Shafter served as a 1st lieutenant the Union Army's 7th Michigan Volunteer Infantry Regiment at the battles of Ball's Bluff and Fair Oaks. He was wounded at the Battle of Fair Oaks and later received the Medal of Honor for heroism during the battle. He led a charge on the first day of the battle and was wounded towards the close of that day's fighting. In order to stay with his regiment he concealed his wounds, fighting on the second day of the battle. On August 22, 1862 he was mustered out of the volunteer service but returned to the field as major in the 19th Michigan Volunteer Infantry Regiment. He was captured at the Battle of Thompson's Station and spent 3 months in a Confederate prison. In April 1864 after his release he was appointed colonel of the 17th Regiment of U.S. Colored Troops and led the regiment at the Battle of Nashville.
By the end of the war, he had been promoted to brevet brigadier general of volunteers. He stayed in the regular army when the war ended. During his subsequent service in the Indian Wars, he received his "Pecos Bill" nickname. He led the 24th Infantry, another United States Colored Troops regiment, in campaigns against the Cheyenne, Comanche, Kickapoo and Kiowa Indians in Texas. While commander of Fort Davis, he started a controversial court-martial of second lieutenant Henry Flipper, the first black cadet to graduate from West Point. In May 1897 he was appointed as a brigadier general.
Spanish–American War.
Just before the outbreak of the Spanish–American War, Shafter was commander of the Department of California. Shafter was an unlikely candidate for command of the expedition to Cuba. He was aged 63, weighed over 300 pounds and suffered from gout. Nevertheless he received a promotion to Major General of Volunteers and command of the Fifth Army Corps being assembled in Tampa, Florida. One possible reason for his being given this command was his lack of political ambitions.
Shafter appeared to maintain a very loose control over the expedition to Cuba from the beginning, commencing with a very disorganized landing at Daiquiri on the southern coast of Cuba. Confusion prevailed over landing priorities and the chain of command. When General Sumner refused to allow the Army's Gatling Gun Detachment - which had priority - to disembark from the transport "Cherokee" on the grounds that the lieutenant commanding the detachment did not have the rank to enforce his priority, Shafter had to personally intervene, returning to the ship in a steam launch to enforce his demand that the guns come off immediately.
During the disembarkation, Shafter sent forward Fifth Corps' Cavalry Division under Joseph Wheeler to reconnoiter the road to Santiago de Cuba. In a complete disregard of orders, Wheeler brought on a fight which escalated into the Battle of Las Guasimas. Shafter apparently did not realize the battle was even underway nor did he say anything to Wheeler about it afterward.
A plan was finally developed for the attack on Santiago. Shafter would send his 1st Division (at the time, brigade and division numbers were not unique outside their parent formation) to attack El Caney while his 2d Division and the Cavalry Division would attack the heights south of El Caney known as San Juan Hill. Originally, Shafter planned to lead his forces from the front, but he suffered greatly from the tropical heat and was confined to his headquarters far to the rear and out of sight of the fighting. Unable to see the battle firsthand, he never developed a coherent chain of command. Shafter's offensive battle plans were both simplistic and extremely vague. He seemed to be unaware or unconcerned about the mass killing effect of modern military weapons technology possessed by the Spanish. Further, his intelligence-gathering efforts on Spanish troop dispositions and equipment was extremely meagre, though he had a number of sources available to him, including reconnaissance reports by Cuban rebel forces as well as espionage obtained from indigenous Cubans.
During the hurried attack on El Caney and San Juan Heights, American forces, who had packed the available roads and were unable maneuvre, suffered heavy losses from Spanish troops equipped with modern repeating smokeless powder rifles and breech-loading artillery, while the short-ranged black-powder guns of U.S. artillery units were unable to respond effectively. Additional casualties were incurred in the actual assault, which was marked by a series of brave but disorganized and uncoordinated advances. After suffering some 1,400 casualties, and aided by a single Gatling Gun detachment for fire support, American troops successfully stormed and occupied both El Caney and San Juan Heights.
The next task for Shafter was the investment and siege of the city of Santiago and its garrison. However, the extent of the American losses were becoming known at Shafter's headquarters back at Sevilla (his gout, poor physical condition, and huge bulk did not allow him to go to the front). The casualties were delivered not only by messenger report, but also by "meat wagons" delivering the wounded and dying to the hospital. Viewing the carnage, Shafter began to waver in his determination to defeat the Spanish at Santiago. He knew his troops' position was tenuous, but again had little intelligence on the hardships of the Spanish inside beleaguered Santiago. Shafter felt the Navy was doing little to relieve the pressure on his forces. Supplies could not be delivered to the front, leaving his the men in want of necessities, particularly food rations. Shafter himself was ill, and very weak. With this view of events, Shafter sent a dramatic message to Washington. He suggested that the army should give up its attack and all its gains for the day, and withdraw to safer ground about five miles away. Fortunately, by the time this message reached Washington, Shafter changed his mind, and instead renewed siege operations after demanding the Spanish surrender the city and garrison of Santiago. With the victory of the U.S. Navy at the Battle of Santiago de Cuba, by Admirals William T. Sampson and Winfield Scott Schley, the fate of the Spanish position at Santiago was sealed. Shortly afterward, the Spanish commander surrendered the city.
Postwar career and retirement.
With disease rampant in the American army in Cuba, Shafter and many of his officers favored a quick withdrawal from Cuba. Shafter personally left Cuba in September 1898, and after a stay in quarantine at Camp Wikoff, Shafter returned to command the Department of California. There he oversaw the supplying of the expedition to the Philippines. In January 1900, Shafter offered the following opinion on the war in the Philippines: "My plan would be to disarm the natives of the Philippine Islands, even if I have to kill half of them to do it. Then I would treat the rest of them with perfect justice."
Shafter was a member of the Military Order of the Loyal Legion of the United States, the Military Order of Foreign Wars and the Sons of the American Revolution.
Shafter retired in 1901 and retired to a 60 acre farm, next to his daughter's land in Bakersfield, California. He died there in 1906 and is buried at San Francisco National Cemetery.
In popular culture.
Shafter was portrayed by Rodger Boyce in the 1997 film "Rough Riders".
Medal of Honor citation.
Rank and Organization:
Citation:

</doc>
<doc id="43609" url="http://en.wikipedia.org/wiki?curid=43609" title="Jumping">
Jumping

Jumping or leaping is a form of locomotion or movement in which an organism or non-living (e.g., robotic) mechanical system propels itself through the air along a ballistic trajectory. Jumping can be distinguished from running, galloping, and other gaits where the entire body is temporarily airborne by the relatively long duration of the aerial phase and high angle of initial launch.
Some animals, such as the kangaroo, employ jumping (commonly called "hopping" in this instance) as their primary form of locomotion, while others, such as frogs, use it only as a means to escape predators. Jumping is also a key feature of various activities and sports, including the long jump, high jump, and show jumping.
Physics of jumping.
All jumping involves the application of force against a substrate, which in turn generates a reactive force that propels the jumper away from the substrate. Any solid or liquid capable of producing an opposing force can serve as a substrate, including ground or water. Examples of the latter include dolphins performing traveling jumps, and Indian skitter frogs executing standing jumps from water.
Jumping organisms are rarely subject to significant aerodynamic forces and, as a result, their jumps are governed by the basic physical laws of ballistic trajectories. Consequently, while a bird may jump into the air to initiate flight, no movement it performs once airborne is considered jumping, as the initial jump conditions no longer dictate its flight path. 
Following the moment of launch (i.e., initial loss of contact with the substrate), a jumper will traverse a parabolic path. The launch angle and initial launch velocity determine the travel distance, duration, and height of the jump. The maximum possible horizontal travel distance occurs at a launch angle of 45 degrees, but any launch angle between 35 and 55 degrees will result in ninety percent of the maximum possible distance.
Muscles (or other actuators in non-living systems) do physical work, adding kinetic energy to the jumper's body over the course of a jump's propulsive phase. This results in a kinetic energy at launch that is proportional to the square of the jumper's velocity. The more work the muscles do, the greater the launch velocity and thus the greater the acceleration and the shorter the time interval of the jump's propulsive phase.
Mechanical power (work per unit time) and the distance over which that power is applied (e.g., leg length) are the key determinants of jump distance and height. As a result, many jumping animals have long legs and muscles that are optimized for maximal power according to the force-velocity relationship of muscles. The maximum power output of muscles is limited, however. To circumvent this limitation, many jumping species slowly pre-stretch elastic elements, such as tendons or apodemes, to store work as strain energy. Such elastic elements can release energy at a much higher rate (higher power) than equivalent muscle mass, thus increasing launch energy to levels beyond what muscle alone is capable of.
A jumper may be either stationary or moving when initiating a jump. In a jump from stationary (i.e., a "standing jump"), all of the work required to accelerate the body through launch is done in a single movement. In a "moving jump" or "running jump", the jumper introduces additional vertical velocity at launch while conserving as much horizontal momentum as possible. Unlike stationary jumps, in which the jumper's kinetic energy at launch is solely due to the jump movement, moving jumps have a higher energy that results from the inclusion of the horizontal velocity preceding the jump. Consequently, jumpers are able to jump greater distances when starting from a run.
Anatomy.
Animals use a wide variety of anatomical adaptations for jumping. These adaptations are exclusively concerned with the launch, as any post-launch method of extending range or controlling the jump must use aerodynamic forces, and thus is considered gliding or parachuting.
Aquatic species rarely display any particular specializations for jumping. Those that are good jumpers usually are primarily adapted for speed, and execute moving jumps by simply swimming to the surface at a high velocity. A few primarily aquatic species that can jump while on land, such as mud skippers, do so via a flick of the tail.
Limb morphology.
In terrestrial animals, the primary propulsive structure is the legs, though a few species use their tails. Typical characteristics of jumping species include long legs, large leg muscles, and additional limb elements.
Long legs increase the time and distance over which a jumping animal can push against the substrate, thus allowing more power and faster, farther jumps. Large leg muscles can generate greater force, resulting in improved jumping performance. In addition to elongated leg elements, many jumping animals have modified foot and ankle bones that are elongated and possess additional joints, effectively adding more segments to the limb and even more length.
Frogs are an excellent example of all three trends: frog legs can be nearly twice the body length, leg muscles may account for up to twenty percent of body weight, and they have not only lengthened the foot, shin and thigh, but extended the ankle bones into another limb joint and similarly extended the hip bones and gained mobility at the sacrum for a second 'extra joint'. As a result, frogs are the undisputed champion jumpers of vertebrates, leaping over fifty body lengths, a distance of more than eight feet.
Power amplification through stored energy.
Grasshoppers are known to use elastic energy storage to increase jumping distance. As noted above, power output is a principal determinant of jump distance, but physiological constraints limit muscle power to approximately 375 Watts per kilogram of muscle. To overcome this limitation, grasshoppers anchor their legs via an internal "catch mechanism" while their muscles stretch an elastic apodeme (similar to a vertebrate tendon). When the catch is released, the apodeme rapidly releases its energy. Because the apodeme releases energy more quickly than muscle, its power output exceeds that of the muscle that produced the energy.
This is analogous to a human throwing an arrow by hand versus using a bow; the use of elastic storage (the bow) allows the muscles to operate closer to isometric on the force-velocity curve. This enables the muscles to do work over a longer time and thus produce more energy than they otherwise could, while the elastic element releases that work faster than the muscles can. The use of elastic energy storage has been found in jumping mammals as well as in frogs, with commensurate increases in power ranging from two to seven times that of equivalent muscle mass.
Classification.
One way to classify jumping is by the manner of foot transfer. In this classification system, five basic jump forms are distinguished:
Leaping gaits, which are distinct from running gaits (see Locomotion), include cantering, galloping, and pronging.
Devices and techniques for enhancing jumping height.
The height of a jump may be increased by using a trampoline or by converting horizontal velocity into vertical velocity with the aid of a device such as a half pipe.
Various exercises can be used to increase an athlete's vertical jumping height. One category of such exercises—plyometrics—employs repetition of discrete jumping-related movements to increase speed, agility, and power.

</doc>
<doc id="43610" url="http://en.wikipedia.org/wiki?curid=43610" title="Grease">
Grease

Grease may refer to: 

</doc>
<doc id="43611" url="http://en.wikipedia.org/wiki?curid=43611" title="Hari">
Hari

Sanskrit Hari (Devanagari: हरि) is in origin a colour term for yellowish hues, including yellow, golden, yellowish-brown or reddish brown, fallow or khaki, pale yellow, greenish or green-yellow
It has important symbolism in the Rigveda and hence in Hinduism; in Rigvedic symbolism, it unites the colours of Soma, the Sun, and bay horses under a single term.
The word Hari is widely used in later Sanskrit and Prakrit literature, Hindu, Buddhist, Jain and Sikh religions.
It appears as 650th name of Vishnu in the Vishnu sahasranama of the Mahabharata and hence rose to special importance in Hindu Vaishnavism.
Etymology.
The Sanskrit word is cognate with Avestan "zari", with the same meaning ("zari" has (dubiously) been identified as the first part of the name of Zarathustra). 
The English words "gold" and "yellow" (from Germanic "gulþan, gelwaz") as well as Latin "helvus" "light-yellow" are from the same Indo-European root, reconstructed as "*ǵʰelH-". In Greek Hari means grace or kindness.
Some words in non-Indo-European languages which fell under Hindu dominance during the medieval period also have loanwords derived from the Sanskrit term, including the word for "day" in Malay and Indonesian, and the word for "king" in Tagalog.

</doc>
<doc id="43613" url="http://en.wikipedia.org/wiki?curid=43613" title="Pension">
Pension

A pension is a fixed sum to be paid regularly to a person, typically following retirement from service. There are many different types of pensions, including defined benefit plans, defined contribution plans, as well as several others. Pensions should not be confused with severance pay; the former is paid in regular installments, while the latter is paid in one lump sum.
The terms retirement plan and superannuation tend to refer to a pension granted upon retirement of the individual. Retirement plans may be set up by employers, insurance companies, the government or other institutions such as employer associations or trade unions. Called "retirement plans" in the United States, they are commonly known as "pension schemes" in the United Kingdom and Ireland and "superannuation plans" (or "super") in Australia and New Zealand. Retirement pensions are typically in the form of a guaranteed life annuity, thus insuring against the risk of longevity.
A pension created by an employer for the benefit of an employee is commonly referred to as an occupational or employer pension. Labor unions, the government, or other organizations may also fund pensions. Occupational pensions are a form of deferred compensation, usually advantageous to employee and employer for tax reasons. Many pensions also contain an additional insurance aspect, since they often will pay benefits to survivors or disabled beneficiaries. Other vehicles (certain lottery payouts, for example, or an annuity) may provide a similar stream of payments.
The common use of the term "pension" is to describe the payments a person receives upon retirement, usually under pre-determined legal or contractual terms. A recipient of a retirement pension is known as a "pensioner" or "retiree".
Types of pensions.
Employment-based pensions.
A retirement plan is an arrangement to provide people with an income during retirement when they are no longer earning a steady income from employment. Often retirement plans require both the employer and employee to contribute money to a fund during their employment in order to receive defined benefits upon retirement. It is a tax deferred savings vehicle that allows for the tax-free accumulation of a fund for later use as a retirement income. Funding can be provided in other ways, such as from labor unions, government agencies, or self-funded schemes. Pension plans are therefore a form of "deferred compensation". A SSAS is a type of employment-based Pension in the UK.
Some countries also grant pensions to military veterans. Military pensions are overseen by the government; an example of a standing agency is the United States Department of Veterans Affairs. "Ad hoc" committees may also be formed to investigate specific tasks, such as the U.S. Commission on Veterans' Pensions (commonly known as the "Bradley Commission") in 1955–56. Pensions may extend past the death of the veteran himself, continuing to be paid to the widow; see, for example, the case of Esther Sumner Damon, who was the last surviving American Revolutionary War widow at her death in 1906.
Social and state pensions.
Many countries have created funds for their citizens and residents to provide income when they retire (or in some cases become disabled). Typically this requires payments throughout the citizen's working life in order to qualify for benefits later on. A basic state pension is a "contribution based" benefit, and depends on an individual's contribution history. For examples, see National Insurance in the UK, or Social Security in the United States of America.
Many countries have also put in place a "social pension". These are regular, tax-funded non-contributory cash transfers paid to older people. Over 80 countries have social pensions. Some are universal benefits, given to all older people regardless of income, assets or employment record. Examples of universal pensions include New Zealand Superannuation and the Basic Retirement Pension of Mauritius. Most social pensions, though, are means-tested, such as Supplemental Security Income in the United States of America or the "older person's grant" in South Africa.
Disability pensions.
Some pension plans will provide for members in the event they suffer a disability. This may take the form of early entry into a retirement plan for a disabled member below the normal retirement age.
Benefits.
Retirement plans may be classified as "defined benefit" or "defined contribution" according to how the benefits are determined. A defined benefit plan guarantees a certain payout at retirement, according to a fixed formula which usually depends on the member's salary and the number of years' membership in the plan. A defined contribution plan will provide a payout at retirement that is dependent upon the amount of money contributed and the performance of the investment vehicles utilized.
Some types of retirement plans, such as "cash balance" plans, combine features of both defined benefit and defined contribution plans. They are often referred to as "hybrid" plans. Such plan designs have become increasingly popular in the US since the 1990s. Examples include Cash Balance and Pension Equity plans.
Defined benefit plans.
A traditional defined benefit (DB) plan is a plan in which the benefit on retirement is determined by a set formula, rather than depending on investment returns. In the US, 26 U.S.C.  specifies a defined benefit plan to be any pension plan that is not a defined contribution plan (see below) where a defined contribution plan is any plan with individual accounts. A traditional pension plan that "defines" a "benefit" for an employee upon that employee's retirement is a defined benefit plan. In the U.S., corporate defined benefit plans, along with many other types of defined benefit plans, are governed by the Employee Retirement Income Security Act of 1974 (ERISA).
Traditionally, retirement plans have been administered by institutions which exist specifically for that purpose, by large businesses, or, for government workers, by the government itself. A traditional form of defined benefit plan is the "final salary" plan, under which the pension paid is equal to the number of years worked, multiplied by the member's salary at retirement, multiplied by a factor known as the "accrual rate". The final accrued amount is available as a monthly pension or a lump sum, but usually monthly.
The benefit in a defined benefit pension plan is determined by a formula that can incorporate the employee's pay, years of employment, age at retirement, and other factors. A simple example is a Dollars Times Service plan design that provides a certain amount per month based on the time an employee works for a company. For example, a plan offering $100 a month per year of service would provide $3,000 per month to a retiree with 30 years of service. While this type of plan is popular among unionized workers, Final Average Pay (FAP) remains the most common type of defined benefit plan offered in the United States. In FAP plans, the average salary over the final years of an employee's career determines the benefit amount.
Averaging salary over a number of years means that the calculation is averaging different dollars. For example, if salary is averaged over five years, and retirement is in 2009, then salary in 2004 dollars is averaged with salary in 2005 dollars, etc., with 2004 dollars being worth more than the dollars of succeeding years. The pension is then paid in first year of retirement dollars, in this example 2009 dollars, with the lowest value of any dollars in the calculation. Thus inflation in the salary averaging years has a considerable impact on purchasing power and cost, both being reduced equally by inflation
This effect of inflation can be eliminated by converting salaries in the averaging years to first year of retirement dollars, and then averaging.
In the United Kingdom, benefits are typically indexed for inflation (known as Retail Prices Index (RPI)) as required by law for registered pension plans. Inflation during an employee's retirement affects the purchasing power of the pension; the higher the inflation rate, the lower the purchasing power of a fixed annual pension. This effect can be mitigated by providing annual increases to the pension at the rate of inflation (usually capped, for instance at 5% in any given year). This method is advantageous for the employee since it stabilizes the purchasing power of pensions to some extent.
If the pension plan allows for early retirement, payments are often reduced to recognize that the retirees will receive the payouts for longer periods of time. In the United States, under the Employee Retirement Income Security Act of 1974, any reduction factor less than or equal to the actuarial early retirement reduction factor is acceptable.
Many DB plans include early retirement provisions to encourage employees to retire early, before the attainment of normal retirement age (usually age 65). Companies would rather hire younger employees at lower wages. Some of those provisions come in the form of additional "temporary" or "supplemental benefits", which are payable to a certain age, usually before attaining normal retirement age.
Funding.
Defined benefit plans may be either "funded" or "unfunded".
In an "unfunded" defined benefit pension, no assets are set aside and the benefits are paid for by the employer or other pension sponsor as and when they are paid. Pension arrangements provided by the state in most countries in the world are unfunded, with benefits paid directly from current workers' contributions and taxes. This method of financing is known as "Pay-as-you-go" (PAYGO or PAYG). The social security systems of many European countries are unfunded, having benefits paid directly out of current taxes and social security contributions, although several countries have hybrid systems which are partially funded. Spain set up the Social Security Reserve Fund and France set up the Pensions Reserve Fund; in Canada the wage-based retirement plan (CPP) is funded, with assets managed by the CPP Investment Board while the U.S. Social Security system is funded by investment in special U.S. Treasury Bonds.
In a "funded" plan, contributions from the employer, and sometimes also from plan members, are invested in a fund towards meeting the benefits. All plans must be funded in some way, even if they are pay-as-you-go, so this type of plan is more accurately known as "pre-funded". The future returns on the investments, and the future benefits to be paid, are not known in advance, so there is no guarantee that a given level of contributions will be enough to meet the benefits. Typically, the contributions to be paid are regularly reviewed in a valuation of the plan's assets and liabilities, carried out by an actuary to ensure that the pension fund will meet future payment obligations. This means that in a defined benefit pension, investment risk and investment rewards are typically assumed by the sponsor/employer and not by the individual. If a plan is not well-funded, the plan sponsor may not have the financial resources to continue funding the plan.
In many countries, such as the USA, the UK and Australia, most private defined benefit plans are funded, because governments there provide tax incentives to funded plans (in Australia they are mandatory). In the United States, non-church-based private employers must pay an insurance-type premium to the Pension Benefit Guaranty Corporation(PBGC), a government agency whose role is to encourage the continuation and maintenance of voluntary private pension plans and provide timely and uninterrupted payment of pension benefits. When the PBGC steps in and takes over a pension plan, it provides payment for pension benefits up to certain maximum amounts, which are indexed for inflation.
Criticisms.
Traditional defined benefit plan designs (because of their typically flat accrual rate and the decreasing time for interest discounting as people get closer to retirement age) tend to exhibit a J-shaped accrual pattern of benefits, where the present value of benefits grows quite slowly early in an employee's career and accelerates significantly in mid-career: in other words it costs more to fund the pension for older employees than for younger ones (an "age bias"). Defined benefit pensions tend to be less portable than defined contribution plans, even if the plan allows a lump sum cash benefit at termination. Most plans, however, pay their benefits as an annuity, so retirees do not bear the risk of low investment returns on contributions or of outliving their retirement income. The open-ended nature of these risks to the employer is the reason given by many employers for switching from defined benefit to defined contribution plans over recent years. The risks to the employer can sometimes be mitigated by discretionary elements in the benefit structure, for instance in the rate of increase granted on accrued pensions, both before and after retirement.
The age bias, reduced portability and open ended risk make defined benefit plans better suited to large employers with less mobile workforces, such as the public sector (which has open-ended support from taxpayers). This coupled with a lack of foresight on the employers part means a large proportion of the workforce are kept in the dark over future investment schemes.
Defined benefit plans are sometimes criticized as being paternalistic as they enable employers or plan trustees to make decisions about the type of benefits and family structures and lifestyles of their employees. However they are typically more valuable than defined contribution plans in most circumstances and for most employees (mainly because the employer tends to pay higher contributions than under defined contribution plans), so such criticism is rarely harsh.
The "cost" of a defined benefit plan is not easily calculated, and requires an actuary or actuarial software. However, even with the best of tools, the cost of a defined benefit plan will always be an estimate based on economic and financial assumptions. These assumptions include the average retirement age and lifespan of the employees, the returns to be earned by the pension plan's investments and any additional taxes or levies, such as those required by the Pension Benefit Guaranty Corporation in the U.S. So, for this arrangement, the benefit is relatively secure but the contribution is uncertain even when estimated by a professional. This has serious cost considerations and risks for the employer offering a pension plan.
One of the growing concerns with defined benefit plans is that the level of future obligations will outpace the value of assets held by the plan. This “underfunding” dilemma can be faced by any type of defined benefit plan, private or public, but it is most acute in governmental and other public plans where political pressures and less rigorous accounting standards can result in excessive commitments to employees and retirees, but inadequate contributions. Many states and municipalities across the country now face chronic pension crises.
Examples.
Many countries offer state-sponsored retirement benefits, beyond those provided by employers, which are funded by payroll or other taxes. In the United States, the Social Security system is similar in function to a defined benefit pension arrangement, albeit one that is constructed differently from a pension offered by a private employer; however, Social Security is distinct in that there is no legally guaranteed level of benefits derived from the amount paid into the program.
Individuals that have worked in the UK and have paid certain levels of national insurance deductions can expect an income from the state pension scheme after their normal retirement. The state pension is currently divided into two parts: the basic state pension, State Second [tier] Pension scheme called S2P. Individuals will qualify for the basic state pension if they have completed sufficient years contribution to their national insurance record. The S2P pension scheme is earnings related and depends on earnings in each year as to how much an individual can expect to receive. It is possible for an individual to forgo the S2P payment from the state, in lieu of a payment made to an appropriate pension scheme of their choice, during their working life. For more details see UK pension provision.
Defined contribution plans.
In a defined contribution plan, contributions are paid into an individual account for each member. The contributions are invested, for example in the stock market, and the returns on the investment (which may be positive or negative) are credited to the individual's account. On retirement, the member's account is used to provide retirement benefits, sometimes through the purchase of an annuity which then provides a regular income. Defined contribution plans have become widespread all over the world in recent years, and are now the dominant form of plan in the private sector in many countries. For example, the number of defined benefit plans in the US has been steadily declining, as more and more employers see pension contributions as a large expense avoidable by disbanding the defined benefit plan and instead offering a defined contribution plan.
Money contributed can either be from employee salary deferral or from employer contributions. The portability of defined contribution pensions is legally no different from the portability of defined benefit plans. However, because of the cost of administration and ease of determining the plan sponsor's liability for defined contribution plans (you do not need to pay an actuary to calculate the lump sum equivalent that you do for defined benefit plans) in practice, defined contribution plans have become generally portable.
In a defined contribution plan, investment risk and investment rewards are assumed by each individual/employee/retiree and not by the sponsor/employer, and these risks may be substantial. In addition, participants do not necessarily purchase annuities with their savings upon retirement, and bear the risk of outliving their assets. (In the United Kingdom, for instance, it is a legal requirement to use the bulk of the fund to purchase an annuity.)
The "cost" of a defined contribution plan is readily calculated, but the benefit from a defined contribution plan depends upon the account balance at the time an employee is looking to use the assets. So, for this arrangement, the "contribution is known" but the "benefit is unknown" (until calculated).
Despite the fact that the participant in a defined contribution plan typically has control over investment decisions, the plan sponsor retains a significant degree of fiduciary responsibility over investment of plan assets, including the selection of investment options and administrative providers.
A defined contribution plan typically involves a number of service providers, including in many cases:
Examples.
In the United States, the legal definition of a defined contribution plan is a plan providing for an individual account for each participant, and for benefits based solely on the amount contributed to the account, plus or minus income, gains, expenses and losses allocated to the account (see 26 U.S.C. ). Examples of defined contribution plans in the United States include Individual Retirement Accounts (IRAs) and 401(k) plans. In such plans, the employee is responsible, to one degree or another, for selecting the types of investments toward which the funds in the retirement plan are allocated. This may range from choosing one of a small number of pre-determined mutual funds to selecting individual stocks or other securities. Most self-directed retirement plans are characterized by certain tax advantages, and some provide for a portion of the employee's contributions to be matched by the employer. In exchange, the funds in such plans may not be withdrawn by the investor prior to reaching a certain age—typically the year the employee reaches 59.5 years old-- (with a small number of exceptions) without incurring a substantial penalty.
In the US, defined contribution plans are subject to IRS limits on how much can be contributed, known as the section 415 limit. In 2009, the total deferral amount, including employee contribution plus employer contribution, was limited to $49,000 or 100% of compensation, whichever is less. The employee-only limit in 2009 was $16,500 with a $5,500 catch-up. These numbers usually increase each year and are indexed to compensate for the effects of inflation. For 2015, the limits were raised to $53,000 and $18,000, respectively.
Examples of defined contribution pension schemes in other countries are, the UK’s personal pensions and proposed National Employment Savings Trust (NEST), Germany’s Riester plans, Australia’s Superannuation system and New Zealand’s KiwiSaver scheme. Individual pension savings plans also exist in Austria, Czech Republic, Denmark, Greece, Finland, Ireland, Netherlands, Slovenia and Spain
Hybrid and cash balance plans.
Hybrid plan designs combine the features of defined benefit and defined contribution plan designs.
A cash balance plan is a defined benefit plan made by the employer, with the help of consulting actuaries (like Kwasha Lipton, who it is said created the cash balance plan) to appear as if they were defined contribution plans. They have "notional balances" in hypothetical accounts where, typically, each year the plan administrator will contribute an amount equal to a certain percentage of each participant's salary; a second contribution, called "interest credit", is made as well. These are not actual contributions and further discussion is beyond the scope of this entry suffice it to say that there is currently much controversy.
In general, they are usually treated as defined benefit plans for tax, accounting and regulatory purposes. As with defined benefit plans, investment risk in hybrid designs is largely borne by the plan sponsor. As with defined contribution designs, plan benefits are expressed in the terms of a notional "account balance," and are usually paid as cash balances upon termination of employment. These features make them more portable than traditional defined benefit plans and perhaps more attractive to a more highly mobile workforce.
Target benefit plans are defined contribution plans made to match (or resemble) defined benefit plans.
Contrasting types of retirement plans.
Advocates of defined contribution plans point out that each employee has the ability to tailor the investment portfolio to his or her individual needs and financial situation, including the choice of how much to contribute, if anything at all. However, others state that these apparent advantages could also hinder some workers who might not possess the financial savvy to choose the correct investment vehicles or have the discipline to voluntarily contribute money to retirement accounts. This debate parallels the discussion currently going on in the U.S., where many Republican leaders favor transforming the Social Security system, at least in part, to a self-directed investment plan.
Financing.
There are various ways in which a pension may be financed.
Defined contribution pensions, by definition, are funded, as the "guarantee" made to employees is that specified (defined) contributions will be made during an individual's working life.
There are many ways to finance your pension and save for retirement. Pension plans can be set up by your employer, matching your contribution each month, by the state or personally through a pension scheme with a financial institution, such as a bank or brokerage firm. Pension plans often come with a tax break depending on the country and plan type.
For example Canadians have the option to open a Registered Retirement Savings Plan (RRSP), as well as a range of employee and state pension programs. This plan allows contributions to this account to be marked as un-taxable income and remain un-taxed until withdrawal. Most country’s governments will provide advice on pension schemes.
History.
Widows' funds were among the first pension type arrangement to appear, for example Duke Ernest the Pious of Gotha in Germany, founded a widows' fund for clergy in 1645 and another for teachers in 1662. 'Various schemes of provision for ministers' widows were then established throughout Europe at about the start of the eighteenth century, some based on a single premium others based on yearly premiums to be distributed as benefits in the same year.'
Germany.
As part of Otto von Bismarck's social legislation, the Old Age and Disability Insurance Bill was enacted in 1889. The Old Age Pension program, financed by a tax on workers, was originally designed to provide a pension annuity for workers who reached the age of 70 years, though this was lowered to 65 years in 1916. It is sometimes claimed that at the time life expectancy for the average Prussian was 45 years; in fact this figure ignores the very high infant mortality and high maternal death rate from childbirth of this era.
In fact, an adult entering into insurance under the scheme would on average live to 70 years of age, a figure used in the actuarial assumptions included in the legislation.
Ireland.
There is a history of pensions in Ireland that can be traced back to Brehon Law imposing a legal responsibility on the kin group to take care of its members who were aged, blind, deaf, sick or insane. For a discussion on pension funds and early Irish law, see F Kelly, "A Guide to Early Irish Law" (Dublin, Dublin Institute for Advanced Studies, 1988). In 2010, there were over 76,291 pension schemes operating in Ireland.
Today the Republic of Ireland has a two-tiered approach to the provision of pensions or retirement benefits. First, there is a state social welfare retirement pension, which promises a basic level of pension. This is a flat rate pension, funded by the national social insurance system and is termed Pay Related Social Insurance or PRSI. Secondly, there are the occupational pension schemes and self-employed arrangements, which supplement the state pension.
United Kingdom.
Until the 20th century, poverty was seen as a quasi-criminal state, and this was reflected in the Vagabonds and Beggars Act 1495 that imprisoned beggars. During Elizabethan and Victorian times, English poor laws represented a shift whereby the poor were seen merely as morally degenerate, and were expected to perform forced labour in workhouses.
The beginning of the modern state pension was the Old Age Pensions Act 1908, that provided 5 shillings (£0.25) a week for those over 70 whose annual means do not exceed £31.50. It coincided with the Royal Commission on the Poor Laws and Relief of Distress 1905-09 and was the first step in the Liberal welfare reforms to the completion of a system of social security, with unemployment and health insurance through the National Insurance Act 1911.
After the Second World War, the National Insurance Act 1946 completed universal coverage of social security. The National Assistance Act 1948 formally abolished the poor law, and gave a minimum income to those not paying national insurance.
The early 1990s established the existing framework for state pensions in the Social Security Contributions and Benefits Act 1992 and Superannuation and other Funds (Validation) Act 1992. Following the highly respected Goode Report, occupational pensions were covered by comprehensive statutes in the Pension Schemes Act 1993 and the Pensions Act 1995.
In 2002 the Pensions Commission was established as a cross party body to review pensions in the United Kingdom. The first Act to follow was the Pensions Act 2004 that updated regulation by replacing OPRA with the Pensions Regulator and relaxing the stringency of minimum funding requirements for pensions, while ensuring protection for insolvent businesses. In a major update of the state pension, the Pensions Act 2007, which aligned and raised retirement ages. Following that, the Pensions Act 2008 has set up automatic enrolment for occupational pensions, and a public competitor designed to be a low-cost and efficient fund manager, called the National Employment Savings Trust (or "Nest").
United States.
Public pensions got their start with various 'promises', informal and legislated, made to veterans of the Revolutionary War and, more extensively, the Civil War. They were expanded greatly, and began to be offered by a number of state and local governments during the early Progressive Era in the late nineteenth century.
Federal civilian pensions were offered under the Civil Service Retirement System (CSRS), formed in 1920. CSRS provided retirement, disability and survivor benefits for most civilian employees in the US Federal government, until the creation of a new Federal agency, the Federal Employees Retirement System (FERS), in 1987.
Pension plans became popular in the United States during World War II, when wage freezes prohibited outright increases in workers' pay. The defined benefit plan had been the most popular and common type of retirement plan in the United States through the 1980s; since that time, defined contribution plans have become the more common type of retirement plan in the United States and many other western countries.
In April 2012, the Northern Mariana Islands Retirement Fund filed for Chapter 11 bankruptcy protection. The retirement fund is a defined benefit type pension plan and was only partially funded by the government, with only $268.4 million in assets and $911 million in liabilities. The plan experienced low investment returns and a benefit structure that had been increased without raises in funding.
According to "Pensions and Investments", this is "apparently the first" US public pension plan to declare bankruptcy.
Current challenges.
A growing challenge for many nations is population ageing. As birth rates drop and life expectancy increases an ever-larger portion of the population is elderly. This leaves fewer workers for each retired person. In many developed countries this means that government and public sector pensions could potentially be a drag on their economies unless pension systems are reformed or taxes are increased. One method of reforming the pension system is to increase the retirement age. Two exceptions are Australia and Canada, where the pension system is forecast to be solvent for the foreseeable future. In Canada, for instance, the annual payments were increased by some 70% in 1998 to achieve this. These two nations also have an advantage from their relative openness to immigration: immigrants tend to be of working age. However, their populations are not growing as fast as the U.S., which supplements a high immigration rate with one of the highest birthrates among Western countries. Thus, the population in the U.S. is not ageing to the extent as those in Europe, Australia, or Canada.
Another growing challenge is the recent trend of states and businesses in the United States purposely under-funding their pension schemes in order to push the costs onto the federal government. For example, in 2009, the majority of states have unfunded pension liabilities exceeding all reported state debt. Bradley Belt, former executive director of the PBGC (the Pension Benefit Guaranty Corporation, the federal agency that insures private-sector defined-benefit pension plans in the event of bankruptcy), testified before a Congressional hearing in October 2004, "I am particularly concerned with the temptation, and indeed, growing tendency, to use the pension insurance fund as a means to obtain an interest-free and risk-free loan to enable companies to restructure. Unfortunately, the current calculation appears to be that shifting pension liabilities onto other premium payers or potentially taxpayers is the path of least resistance rather than a last resort."
Challenges have further been increased by the post-2007 credit crunch. Total funding of the nation's 100 largest corporate pension plans fell by $303bn in 2008, going from a $86bn surplus at the end of 2007 to a $217bn deficit at the end of 2008.
Notable examples of pension systems.
Some of the listed systems might also be considered social insurance.
Market structure.
The market for pension fund investments is still centered around the U.K.and U.S. economies. Japan and the EU are conspicuous by absence. As of 2005 the U.S. was the largest market for pension fund investments followed by the UK.
Pension reforms have gained pace worldwide in recent years and funded arrangements are likely to play an increasingly important role in delivering retirement income security and also affect securities markets in future years.
Obtaining survey data on pensions.
Numerous worldwide health, aging and retirement surveys contain questions pertaining to pensions. The Gateway to Global Aging Data - developed and maintained by the Program on Global Aging, Health and Policy at the University of Southern California and sponsored by the National Institute on Aging at the National Institutes of Health - provides access to meta data for these questions as well as links to obtain respondent data from the originating surveys.
See also.
Specific:

</doc>
<doc id="43614" url="http://en.wikipedia.org/wiki?curid=43614" title="Arsinoe">
Arsinoe

Arsinoe (Ancient Greek: Ἀρσινόη), sometimes spelled Arsinoë, pronounced Arsinoi in modern Greek, may refer to:

</doc>
<doc id="43615" url="http://en.wikipedia.org/wiki?curid=43615" title="Pension fund">
Pension fund

A pension fund, also known as a superannuation fund in some countries, is any plan, fund, or scheme which provides retirement income.
Pension funds typically have large amounts of money to invest and are the major investors in listed and private companies. They are especially important to the stock market where large institutional investors dominate. The largest 300 pension funds collectively hold about $6 trillion in assets. In January 2008, "The Economist" reported that Morgan Stanley estimates that pension funds worldwide hold over US$20 trillion in assets, the largest for any category of investor ahead of mutual funds, insurance companies, currency reserves, sovereign wealth funds, hedge funds, or private equity.
Although the (Japanese) Government Pension Investment Fund (GPIF) lost 0.25 percent, in the year ended March 31, 2011 GPIF was still the world's largest public pension fund which oversees 114 trillion Yen ($1.5 trillion).
Classifications.
Open vs. closed pension funds.
Open pension funds support at least one pension plan with no restriction on membership while closed pension funds support only pension plans that are limited to certain employees.
Closed pension funds are further subclassified into:
Public vs. private pension funds.
A public pension fund is one that is regulated under public sector law while a private pension fund is regulated under private sector law.
In certain countries the distinction between public or government pension funds and private pension funds may be difficult to assess. In others, the distinction is made sharply in law, with very specific requirements for administration and investment. For example, local governmental bodies in the United States are subject to laws passed by the states in which those localities exist, and these laws include provisions such as defining classes of permitted investments and a minimum municipal obligation.
By country.
Romania.
The pension system in Romania is made of 3 pillars, one is the state pension (Pillar I - Mandatory), second is a private mandatory pension were the state transfers a percentage of the contribution it collects for the public pension and third an optional private pension (Pillar III - Voluntary).
The Financial Supervisory Authority - Private Pension is responsible for the supervision and regulation of the private pension system.
Turkey.
Government.
Social Security Institution was established by the Social Security Institution Law No:5502 which was published in the Official Gazette No: 26173 dated 20.06.2006 and brings the Social Insurance Institution, General Directorate of Bağ-kur and General Directorate of Emekli Sandığı whose historical development are summarized above under a single roof in order to transfer five different retirement regimes which are civil servants, contractual paid workers, agricultural paid workers, self-employers and agricultural self-employers into a single retirement regime that will offer equal actuarial rights and obligations. The Social Security Institution is continuing its activities to provide better quality of services for our citizens with the participation of whole staff putting all their energy individually and institutionally.
Private.
OYAK, (Ordu Yardımlaşma Kurumu/Armed Forces Pension Fund) provides its members with "supplementary retirement benefits" apart from the official retirement fund, T.C.Emekli Sandığı/SSK, to which they are primarily affiliated.
In addition to the retirement benefit, OYAK pays "disability benefits" to the members on duty when they become partially or fully disabled as well as "death benefits" to the heirs of the deceased member if the death occurs during the member's subscription to the Foundation.
OYAK is incorporated as a private entity under its own law subject to Turkish civic and commercial codes. OYAK while fulfilling its legal duties, as set in the Law, also provides its members with social services such as loans, home loans and retirement income systems.
The initial source of OYAK's funds is a compulsory 10 percent levy on the base salary of Turkey's 200,000 serving officers who, together with 25,000 current pensioners, make up OYAK's members.
Some other Turkish private pension funds:
United States.
In the United States pension funds include schemes which result in a deferral of income by employees, even if retirement income provision isn't the intent. The United States has $24.5 trillion in retirement and pension assets ($15.5 trillion in private funds, $9 trillion in public funds) as of June 30, 2014. The largest 200 pension funds accounted for $4.540 trillion as of September 30, 2009.

</doc>
<doc id="43616" url="http://en.wikipedia.org/wiki?curid=43616" title="Arsinoe II">
Arsinoe II

←"For other uses see, Arsinoe"
Arsinoë II (Ancient Greek: Ἀρσινόη, 316 BC–unknown date from July 270 BC until 260 BC) was a Ptolemaic Greek Princess of Ancient Egypt and through marriage was Queen of Thrace, Asia Minor and Macedonia as wife of King Lysimachus (Greek: Λυσίμαχος) and later co-ruler of Egypt with her brother-husband Ptolemy II Philadelphus (Greek: Πτολεμαῖος Φιλάδελφος, which means "Ptolemy the sibling-loving").
Biography.
She was the first daughter of Pharaoh Ptolemy I Soter (Greek: Πτολεμαίος Σωτήρ, which means "Ptolemy the Savior"), the founder of the Hellenistic state of Egypt, and his second wife Berenice I of Egypt.
Arsinoe II at the age of 15, married Lysimachus to whom she bore three sons: Ptolemy I Epigone, Lysimachus and Philip. In order to position her sons for the throne, she had Lysimachus' first son, Agathocles, poisoned on account of treason. After Lysimachus' death in battle in 281 BC, she fled to Cassandreia (Greek: Κασσάνδρεια) and married her paternal half-brother Ptolemy Keraunos, one of the sons of Ptolemy I from his previous wife, Eurydice of Egypt. The marriage was for political reasons as they both claimed the throne of Macedonia and Thrace (by the time of his death Lysimachus was ruler of both regions, and his power extended to Southern Greece and Asia Minor). Their relationship was never good. As Ptolemy Keraunos was becoming more powerful, she decided it was time to stop him and conspired against him with her sons. This action caused Ptolemy Keraunus to kill two of her sons, Lysimachus and Philip, while the eldest, Ptolemy, was able to escape and to flee north, to the kingdom of the Dardanians. She herself went to Alexandria, Egypt to seek protection from her brother, Ptolemy II Philadelphus.
In Egypt, she continued her intrigues and probably instigated the accusation and exile of her brother Ptolemy II's first wife, Arsinoe I. Arsinoe II then married her brother; as a result, both were given the epithet "Philadelphoi" (Greek: Φιλάδελφοι, "Sibling-loving (plural)") by the presumably scandalized Greeks. Arsinoe II shared all of her brother's titles and apparently was quite influential, having towns dedicated to her, her own cult (as was Egyptian custom), and appearing on coinage. Apparently, she contributed greatly to foreign policy, including Ptolemy II's victory in the First Syrian War (274-271 BC) between Egypt and the Seleucid Empire in the Middle East.
According to Posidippus, she won three chariot races at the Olympic Games, probably in 272 BC.
After her death, Ptolemy II continued to refer to her on official documents, as well as supporting her coinage and cult. He also established her worship as a Goddess, a clever move, because by doing this he established also his own worship as a god.

</doc>
<doc id="43617" url="http://en.wikipedia.org/wiki?curid=43617" title="Shark">
Shark

Sharks are a group of fish characterized by a cartilaginous skeleton, five to seven gill slits on the sides of the head, and pectoral fins that are not fused to the head. Modern sharks are classified within the clade Selachimorpha (or Selachii) and are the sister group to the rays. However, the term "shark" has also been used for extinct members of the subclass Elasmobranchii outside the Selachimorpha, such as "Cladoselache" and "Xenacanthus", as well as other Chondrichthyes such as the holocephalid eugenedontidans. Under this broader definition, the earliest known sharks date back to more than 420 million years ago. Acanthodians are often referred to as "spiny sharks"; though they are not part of Chondrichthyes proper, they are a paraphyletic assemblage leading to cartilaginous fish as a whole.
Since then, sharks have diversified into over 500 species. They range in size from the small dwarf lanternshark ("Etmopterus perryi"), a deep sea species of only 17 cm in length, to the whale shark ("Rhincodon typus"), the largest fish in the world, which reaches approximately 12 m in length. Sharks are found in all seas and are common to depths of 2000 m. They generally do not live in freshwater although there are a few known exceptions, such as the bull shark and the river shark, which can survive and be found in both seawater and freshwater. They breathe through five to seven gill slits. Sharks have a covering of dermal denticles that protects their skin from damage and parasites in addition to improving their fluid dynamics. They have several sets of replaceable teeth.
Well-known species such as the great white shark, tiger shark, blue shark, mako shark, and the hammerhead shark are apex predators—organisms at the top of their underwater food chain. Many shark populations are threatened by human activities.
Etymology.
Until the 16th century, sharks were known to mariners as "sea dogs".
The etymology of the word "shark" is uncertain. One theory is that it derives from the Yucatec Maya word "xok", pronounced 'shok'.
Evidence for this etymology comes from the Oxford English Dictionary, which notes "shark" first came into use after Sir John Hawkins' sailors exhibited one in London in 1569 and posted "sharke" to refer to the large sharks of the Caribbean Sea. However, the Middle English Dictionary records an isolated occurrence of the word "shark" (referring to a sea fish) in a letter written by Thomas Beckington in 1442, which rules out a New World etymology.
An alternate etymology states that the original sense of the word was that of "predator, one who preys on others" from the German "Schorck", a variant of "Schurke" "villain, scoundrel" (cf. "card shark", "loan shark", etc.), which was later applied to the fish due to its predatory behaviour.
Evolution.
Evidence for the existence of sharks dates from the Ordovician period, 450–420 million years ago, before land vertebrates existed and before many plants had colonized the continents. Only scales have been recovered from the first sharks and not all paleontologists agree that these are from true sharks, suspecting that these scales are actually those of thelodont agnathans. The oldest generally accepted shark scales are from about 420 million years ago, in the Silurian period. The first sharks looked very different from modern sharks. The majority of modern sharks can be traced back to around 100 million years ago. Most fossils are of teeth, often in large numbers. Partial skeletons and even complete fossilized remains have been discovered. Estimates suggest that sharks grow tens of thousands of teeth over a lifetime, which explains the abundant fossils. The teeth consist of easily fossilized calcium phosphate, an apatite. When a shark dies, the decomposing skeleton breaks up, scattering the apatite prisms. Preservation requires rapid burial in bottom sediments.
Among the most ancient and primitive sharks is "Cladoselache", from about 370 million years ago, which has been found within Paleozoic strata in Ohio, Kentucky, and Tennessee. At that point in Earth's history these rocks made up the soft bottom sediments of a large, shallow ocean, which stretched across much of North America. "Cladoselache" was only about 1 m long with stiff triangular fins and slender jaws. Its teeth had several pointed cusps, which wore down from use. From the small number of teeth found together, it is most likely that "Cladoselache" did not replace its teeth as regularly as modern sharks. Its caudal fins had a similar shape to the great white sharks and the pelagic shortfin and longfin makos. The presence of whole fish arranged tail-first in their stomachs suggest that they were fast swimmers with great agility.
Most fossil sharks from about 300 to 150 million years ago can be assigned to one of two groups. The Xenacanthida was almost exclusive to freshwater environments. By the time this group became extinct about 220 million years ago, they had spread worldwide. The other group, the hybodonts, appeared about 320 million years ago and lived mostly in the oceans, but also in freshwater. The results of a 2014 study of the gill structure of an unusually well-preserved 325 million year old fossil suggested that sharks are not "living fossils", but rather have evolved more extensively than previously thought over the hundreds of millions of years they have been around.
Modern sharks began to appear about 100 million years ago. Fossil mackerel shark teeth date to the Early Cretaceous. One of the most recently evolved families is the hammerhead shark (family Sphyrnidae), which emerged in the Eocene. The oldest white shark teeth date from 60 to 66 million years ago, around the time of the extinction of the dinosaurs. In early white shark evolution there are at least two lineages: one lineage is of white sharks with coarsely serrated teeth and it probably gave rise to the modern great white shark, and another lineage is of white sharks with finely serrated teeth. These sharks attained gigantic proportions and include the extinct megatoothed shark, "C. megalodon". Like most extinct sharks, "C. megalodon" is also primarily known from its fossil teeth and vertebrae. This giant shark reached a total length (TL) of more than 16 m. "C. megalodon" may have approached a maxima of 20.3 m in total length and 103 MT in mass. Paleontological evidence suggests that this shark was an active predator of large cetaceans.
Taxonomy.
Sharks belong to the superorder Selachimorpha in the subclass Elasmobranchii in the class Chondrichthyes. The Elasmobranchii also include rays and skates; the Chondrichthyes also include Chimaeras. It is currently thought that the sharks form a polyphyletic group: some sharks are more closely related to rays than they are to some other sharks.
The superorder Selachimorpha is divided into Galea (or Galeomorphii), and Squalea (or Squalomorphii). The Galeans are the Heterodontiformes, Orectolobiformes, Lamniformes, and Carcharhiniformes. Lamnoids and Carcharhinoids are usually placed in one clade, but recent studies show the Lamnoids and Orectoloboids are a clade. Some scientists now think that Heterodontoids may be Squalean. The Squaleans are divided into Hexanchoidei and Squalomorpha. The Hexanchoidei includes the Hexanchiformes and Chlamydoselachiformes. The Squalomorpha contains the Squaliformes and the Hypnosqualea. The Hypnosqualea may be invalid. It includes the Squatiniformes, and the Pristorajea, which may also be invalid, but includes the Pristiophoriformes and the Batoidea.
There are more than 470 species of sharks split across thirteen orders, including four orders of sharks that have gone extinct:
Anatomy.
Teeth.
Shark teeth are embedded in the gums rather than directly affixed to the jaw, and are constantly replaced throughout life. Multiple rows of replacement teeth grow in a groove on the inside of the jaw and steadily move forward in comparison to a conveyor belt; some sharks lose 30,000 or more teeth in their lifetime. The rate of tooth replacement varies from once every 8 to 10 days to several months. In most species, teeth are replaced one at a time as opposed to the simultaneous replacement of an entire row, which is observed in the cookiecutter shark.
Tooth shape depends on the shark's diet: those that feed on mollusks and crustaceans have dense and flattened teeth used for crushing, those that feed on fish have needle-like teeth for gripping, and those that feed on larger prey such as mammals have pointed lower teeth for gripping and triangular upper teeth with serrated edges for cutting. The teeth of plankton-feeders such as the basking shark are small and non-functional.
Skeleton.
Shark skeletons are very different from those of bony fish and terrestrial vertebrates. Sharks and other cartilaginous fish (skates and rays) have skeletons made of cartilage and connective tissue. Cartilage is flexible and durable, yet is about half the normal density of bone. This reduces the skeleton's weight, saving energy. Because sharks do not have rib cages, they can easily be crushed under their own weight on land.
Jaw.
Jaws of sharks, like those of rays and skates, are not attached to the cranium. The jaw's surface (in comparison to the shark's vertebrae and gill arches) needs extra support due to its heavy exposure to physical stress and its need for strength. It has a layer of tiny hexagonal plates called "tesserae", which are crystal blocks of calcium salts arranged as a mosaic. This gives these areas much of the same strength found in the bony tissue found in other animals.
Generally sharks have only one layer of tesserae, but the jaws of large specimens, such as the bull shark, tiger shark, and the great white shark, have two to three layers or more, depending on body size. The jaws of a large great white shark may have up to five layers. In the rostrum (snout), the cartilage can be spongy and flexible to absorb the power of impacts.
Fins.
Fin skeletons are elongated and supported with soft and unsegmented rays named ceratotrichia, filaments of elastic protein resembling the horny keratin in hair and feathers. Most sharks have eight fins. Sharks can only drift away from objects directly in front of them because their fins do not allow them to move in the tail-first direction.
Dermal denticles.
Unlike bony fish, sharks have a complex dermal corset made of flexible collagenous fibers and arranged as a helical network surrounding their body. This works as an outer skeleton, providing attachment for their swimming muscles and thus saving energy. Their dermal teeth give them hydrodynamic advantages as they reduce turbulence when swimming.
Tails.
Tails provide thrust, making speed and acceleration dependent on tail shape. Caudal fin shapes vary considerably between shark species, due to their evolution in separate environments. Sharks possess a heterocercal caudal fin in which the dorsal portion is usually noticeably larger than the ventral portion. This is because the shark's vertebral column extends into that dorsal portion, providing a greater surface area for muscle attachment. This allows more efficient locomotion among these negatively buoyant cartilaginous fish. By contrast, most bony fish possess a homocercal caudal fin.
Tiger sharks have a large upper lobe, which allows for slow cruising and sudden bursts of speed. The tiger shark must be able to twist and turn in the water easily when hunting to support its varied diet, whereas the porbeagle shark, which hunts schooling fish such as mackerel and herring, has a large lower lobe to help it keep pace with its fast-swimming prey. Other tail adaptations help sharks catch prey more directly, such as the thresher shark's usage of its powerful, elongated upper lobe to stun fish and squid.
Physiology.
Buoyancy.
Unlike bony fish, sharks do not have gas-filled swim bladders for buoyancy. Instead, sharks rely on a large liver filled with oil that contains squalene, and their cartilage, which is about half the normal density of bone. Their liver constitutes up to 30% of their total body mass. The liver's effectiveness is limited, so sharks employ dynamic lift to maintain depth when not swimming. Sand tiger sharks store air in their stomachs, using it as a form of swim bladder. Most sharks need to constantly swim in order to breathe and cannot sleep very long without sinking (if at all). However, certain species, like the nurse shark, are capable of pumping water across their gills, allowing them to rest on the ocean bottom.
Some sharks, if inverted or stroked on the nose, enter a natural state of tonic immobility. Researchers use this condition to handle sharks safely.
Respiration.
Like other fish, sharks extract oxygen from seawater as it passes over their gills. Unlike other fish, shark gill slits are not covered, but lie in a row behind the head. A modified slit called a spiracle lies just behind the eye, which assists the shark with taking in water during respiration and plays a major role in bottom–dwelling sharks. Spiracles are reduced or missing in active pelagic sharks. While the shark is moving, water passes through the mouth and over the gills in a process known as "ram ventilation". While at rest, most sharks pump water over their gills to ensure a constant supply of oxygenated water. A small number of species have lost the ability to pump water through their gills and must swim without rest. These species are "obligate ram ventilators" and would presumably asphyxiate if unable to move. Obligate ram ventilation is also true of some pelagic bony fish species.
The respiration and circulation process begins when deoxygenated blood travels to the shark's two-chambered heart. Here the shark pumps blood to its gills via the ventral aorta artery where it branches into afferent brachial arteries. Reoxygenation takes place in the gills and the reoxygenated blood flows into the efferent brachial arteries, which come together to form the dorsal aorta. The blood flows from the dorsal aorta throughout the body. The deoxygenated blood from the body then flows through the posterior cardinal veins and enters the posterior cardinal sinuses. From there blood enters the heart ventricle and the cycle repeats.
Thermoregulation.
Most sharks are "cold-blooded" or, more precisely, poikilothermic, meaning that their internal body temperature matches that of their ambient environment. Members of the family Lamnidae (such as the shortfin mako shark and the great white shark) are homeothermic and maintain a higher body temperature than the surrounding water. In these sharks, a strip of aerobic red muscle located near the center of the body generates the heat, which the body retains via a countercurrent exchange mechanism by a system of blood vessels called the rete mirabile ("miraculous net"). The common thresher shark has a similar mechanism for maintaining an elevated body temperature, which is thought to have evolved independently.
Osmoregulation.
In contrast to bony fish, with the exception of the coelacanth, the blood and other tissue of sharks and Chondrichthyes is generally isotonic to their marine environments because of the high concentration of urea (up to 2.5%) and trimethylamine N-oxide (TMAO), allowing them to be in osmotic balance with the seawater. This adaptation prevents most sharks from surviving in freshwater, and they are therefore confined to marine environments. A few exceptions exist, such as the bull shark, which has developed a way to change its kidney function to excrete large amounts of urea. When a shark dies, the urea is broken down to ammonia by bacteria, causing the dead body to gradually smell strongly of ammonia.
Digestion.
Digestion can take a long time. The food moves from the mouth to a J-shaped stomach, where it is stored and initial digestion occurs. Unwanted items may never get past the stomach, and instead the shark either vomits or turns its stomachs inside out and ejects unwanted items from its mouth.
One of the biggest differences between the digestive systems of sharks and mammals is that sharks have much shorter intestines. This short length is achieved by the spiral valve with multiple turns within a single short section instead of a long tube-like intestine. The valve provides a long surface area, requiring food to circulate inside the short gut until fully digested, when remaining waste products pass into the cloaca.
Senses.
Smell.
Sharks have keen olfactory senses, located in the short duct (which is not fused, unlike bony fish) between the anterior and posterior nasal openings, with some species able to detect as little as one part per million of blood in seawater.
Sharks have the ability to determine the direction of a given scent based on the timing of scent detection in each nostril. This is similar to the method mammals use to determine direction of sound.
They are more attracted to the chemicals found in the intestines of many species, and as a result often linger near or in sewage outfalls. Some species, such as nurse sharks, have external barbels that greatly increase their ability to sense prey.
Sight.
Shark eyes are similar to the eyes of other vertebrates, including similar lenses, corneas and retinas, though their eyesight is well adapted to the marine environment with the help of a tissue called tapetum lucidum. This tissue is behind the retina and reflects light back to it, thereby increasing visibility in the dark waters. The effectiveness of the tissue varies, with some sharks having stronger nocturnal adaptations. Many sharks can contract and dilate their pupils, like humans, something no teleost fish can do. Sharks have eyelids, but they do not blink because the surrounding water cleans their eyes. To protect their eyes some species have nictitating membranes. This membrane covers the eyes while hunting and when the shark is being attacked. However, some species, including the great white shark ("Carcharodon carcharias"), do not have this membrane, but instead roll their eyes backwards to protect them when striking prey. The importance of sight in shark hunting behavior is debated. Some believe that electro- and chemoreception are more significant, while others point to the nictating membrane as evidence that sight is important. Presumably, the shark would not protect its eyes were they unimportant. The use of sight probably varies with species and water conditions. The shark's field of vision can swap between monocular and stereoscopic at any time. A micro-spectrophotometry study of 17 species of shark found 10 had only rod photoreceptors and no cone cells in their retinas giving them good night vision while making them colorblind. The remaining seven species had in addition to rods a single type of cone photoreceptor sensitive to green and, seeing only in shades of grey and green, are believed to be effectively colorblind. The study indicates that an object's contrast against the background, rather than colour, may be more important for object detection.
Hearing.
Although it is hard to test the hearing of sharks, they may have a sharp sense of hearing and can possibly hear prey from many miles away. A small opening on each side of their heads (not the spiracle) leads directly into the inner ear through a thin channel. The lateral line shows a similar arrangement, and is open to the environment via a series of openings called lateral line pores. This is a reminder of the common origin of these two vibration- and sound-detecting organs that are grouped together as the acoustico-lateralis system. In bony fish and tetrapods the external opening into the inner ear has been lost.
Electroreception.
The ampullae of Lorenzini are the electroreceptor organs. They number in the hundreds to thousands. Sharks use the ampullae of Lorenzini to detect the electromagnetic fields that all living things produce. This helps sharks (particularly the hammerhead shark) find prey. The shark has the greatest electrical sensitivity of any animal. Sharks find prey hidden in sand by detecting the electric fields they produce. Ocean currents moving in the magnetic field of the Earth also generate electric fields that sharks can use for orientation and possibly navigation.
Lateral line.
This system is found in most fish, including sharks. It detects motion or vibrations in water. The shark can sense frequencies in the range of 25 to 50 Hz.
Life history.
Shark lifespans vary by species. Most live 20 to 30 years. The spiny dogfish has the longest lifespan at more than 100 years. Whale sharks ("Rhincodon typus") may also live over 100 years.
Reproduction.
Unlike most bony fish, sharks are K-selected reproducers, meaning that they produce a small number of well-developed young as opposed to a large number of poorly developed young. Fecundity in sharks ranges from 2 to over 100 young per reproductive cycle. Sharks mature slowly relative to many other fish. For example, lemon sharks reach sexual maturity at around age 13–15.
Sexual.
Sharks practice internal fertilization. The posterior part of a male shark's pelvic fins are modified into a pair of intromittent organs called claspers, analogous to a mammalian penis, of which one is used to deliver sperm into the female.
Mating has rarely been observed in sharks. The smaller catsharks often mate with the male curling around the female. In less flexible species the two sharks swim parallel to each other while the male inserts a clasper into the female's oviduct. Females in many of the larger species have bite marks that appear to be a result of a male grasping them to maintain position during mating. The bite marks may also come from courtship behavior: the male may bite the female to show his interest. In some species, females have evolved thicker skin to withstand these bites.
Asexual.
There are two documented cases in which a female shark who has not been in contact with a male has conceived a pup on her own through parthenogenesis. The details of this process are not well understood, but genetic fingerprinting showed that the pups had no paternal genetic contribution, ruling out sperm storage. The extent of this behavior in the wild is unknown, as is whether other species have this capability. Mammals are now the only major vertebrate group in which asexual reproduction has not been observed.
Scientists say that asexual reproduction in the wild is rare, and probably a last-ditch effort to reproduce when a mate is not present. Asexual reproduction diminishes genetic diversity, which helps build defenses against threats to the species. Species that rely solely on it risk extinction. Asexual reproduction may have contributed to the blue shark's decline off the Irish coast.
Brooding.
Sharks display three ways to bear their young, varying by species, oviparity, viviparity and ovoviviparity.
Ovoviviparity.
Most sharks are ovoviviparous, meaning that the eggs hatch in the oviduct within the mother's body and that the egg's yolk and fluids secreted by glands in the walls of the oviduct nourishes the embryos. The young continue to be nourished by the remnants of the yolk and the oviduct's fluids. As in viviparity, the young are born alive and fully functional. Lamniforme sharks practice "oophagy", where the first embryos to hatch eat the remaining eggs. Taking this a step further, sand tiger shark pups cannibalistically consume neighboring embryos. The survival strategy for ovoviviparous species is to brood the young to a comparatively large size before birth. The whale shark is now classified as ovoviviparous rather than oviparous, because extrauterine eggs are now thought to have been aborted. Most ovoviviparous sharks give birth in sheltered areas, including bays, river mouths and shallow reefs. They choose such areas for protection from predators (mainly other sharks) and the abundance of food. Dogfish have the longest known gestation period of any shark, at 18 to 24 months. Basking sharks and frilled sharks appear to have even longer gestation periods, but accurate data are lacking.
Oviparity.
Some species are oviparous like most other fish, laying their eggs in the water. In most oviparous shark species, an egg case with the consistency of leather protects the developing embryo(s). These cases may be corkscrewed into crevices for protection. Once empty, the egg case is known as a "mermaid's purse", and can wash up on shore. Oviparous sharks include the horn shark, catshark, Port Jackson shark, and swellshark.
Viviparity.
Finally some sharks maintain a "placental" link to the developing young, this method is called viviparity. This is more analogous to mammalian gestation than that of other fishes. The young are born alive and fully functional. Hammerheads, the requiem sharks (such as the bull and blue sharks), and smoothhounds are viviparous.
Behavior.
The classic view describes a solitary hunter, ranging the oceans in search of food. However, this applies to only a few species. Most live far more social, sedentary, benthic lives, and appear likely to have their own distinct personalities. Even solitary sharks meet for breeding or at rich hunting grounds, which may lead them to cover thousands of miles in a year. Shark migration patterns may be even more complex than in birds, with many sharks covering entire ocean basins.
Sharks can be highly social, remaining in large schools. Sometimes more than 100 scalloped hammerheads congregate around seamounts and islands, e.g., in the Gulf of California. Cross-species social hierarchies exist. For example, oceanic whitetip sharks dominate silky sharks of comparable size during feeding.
When approached too closely some sharks perform a threat display. This usually consists of exaggerated swimming movements, and can vary in intensity according to the threat level.
Speed.
In general, sharks swim ("cruise") at an average speed of 8 km/h, but when feeding or attacking, the average shark can reach speeds upwards of 19 km/h. The shortfin mako shark, the fastest shark and one of the fastest fish, can burst at speeds up to 50 km/h. The great white shark is also capable of speed bursts. These exceptions may be due to the warm-blooded, or homeothermic, nature of these sharks' physiology. Sharks can travel 70 to 80 km in a day.
Intelligence.
Sharks possess brain-to-body mass ratios that are similar to mammals and birds, and have exhibited apparent curiosity and behavior resembling play in the wild.
Sleep.
All sharks need to keep water flowing over their gills in order for them to breathe, however not all species need to be moving to do this. Those that are able to breathe while not swimming breathe by using their spiracles to force water over their gills, thereby allowing them to extract oxygen from the water. It has been recorded that their eyes remain open while in this state and actively follow the movements of divers swimming around them and as such they are not truly asleep.
Species that do need to swim continuously to breathe go through a process known as sleep swimming, in which the shark is essentially unconscious. It is known from experiments conducted on the spiny dogfish that its spinal cord, rather than its brain, coordinates swimming, so spiny dogfish can continue to swim while sleeping, and this also may be the case in larger shark species.
Ecology.
Feeding.
Most sharks are carnivorous. Basking sharks, whale sharks, and megamouth sharks have independently evolved different strategies for filter feeding plankton: basking sharks practice ram feeding, whale sharks use suction to take in plankton and small fishes, and megamouth sharks make suction feeding more efficient by using the luminescent tissue inside of their mouths to attract prey in the deep ocean. This type of feeding requires gill rakers—long, slender filaments that form a very efficient sieve—analogous to the baleen plates of the great whales. The shark traps the plankton in these filaments and swallows from time to time in huge mouthfuls. Teeth in these species are comparatively small because they are not needed for feeding.
Other highly specialized feeders include cookiecutter sharks, which feed on flesh sliced out of other larger fish and marine mammals. Cookiecutter teeth are enormous compared to the animal's size. The lower teeth are particularly sharp. Although they have never been observed feeding, they are believed to latch onto their prey and use their thick lips to make a seal, twisting their bodies to rip off flesh.
Some seabed–dwelling species are highly effective ambush predators. Angel sharks and wobbegongs use camouflage to lie in wait and suck prey into their mouths. Many benthic sharks feed solely on crustaceans which they crush with their flat molariform teeth.
Other sharks feed on squid or fish, which they swallow whole. The viper dogfish has teeth it can point outwards to strike and capture prey that it then swallows intact. The great white and other large predators either swallow small prey whole or take huge bites out of large animals. Thresher sharks use their long tails to stun shoaling fishes, and sawsharks either stir prey from the seabed or slash at swimming prey with their tooth-studded rostra.
Many sharks, including the whitetip reef shark are cooperative feeders and hunt in packs to herd and capture elusive prey. These social sharks are often migratory, traveling huge distances around ocean basins in large schools. These migrations may be partly necessary to find new food sources.
Range and habitat.
Sharks are found in all seas. They generally do not live in fresh water, with a few exceptions such as the bull shark and the river shark which can swim both in seawater and freshwater. Sharks are common down to depths of 2000 m, and some live even deeper, but they are almost entirely absent below 3000 m. The deepest confirmed report of a shark is a Portuguese dogfish at 3700 m.
Relationship with humans.
Attacks.
In 2006 the International Shark Attack File (ISAF) undertook an investigation into 96 alleged shark attacks, confirming 62 of them as unprovoked attacks and 16 as provoked attacks. The average number of fatalities worldwide per year between 2001 and 2006 from unprovoked shark attacks is 4.3.
Contrary to popular belief, only a few sharks are dangerous to humans. Out of more than 470 species, only four have been involved in a significant number of fatal, unprovoked attacks on humans: the great white, oceanic whitetip, tiger, and bull sharks. These sharks are large, powerful predators, and may sometimes attack and kill people. Despite being responsible for attacks on humans they have all been filmed without using a protective cage.
The perception of sharks as dangerous animals has been popularized by publicity given to a few isolated unprovoked attacks, such as the Jersey Shore shark attacks of 1916, and through popular fictional works about shark attacks, such as the "Jaws" film series. "Jaws" author Peter Benchley, as well as "Jaws" director Steven Spielberg later attempted to dispel the image of sharks as man-eating monsters.
To help avoid an unprovoked attack, humans should not wear jewelry or metal that is shiny and refrain from splashing around too much.
In captivity.
Until recently only a few benthic species of shark, such as hornsharks, leopard sharks and catsharks had survived in aquarium conditions for a year or more. This gave rise to the belief that sharks, as well as being difficult to capture and transport, were difficult to care for. More knowledge has led to more species (including the large pelagic sharks) living far longer in captivity. At the same time, safer transportation techniques have enabled long distance movement. One shark that never had been successfully held in captivity for long was the great white. But in September 2004 the Monterey Bay Aquarium successfully kept a young female for 198 days before releasing her.
Most species are not suitable for home aquaria and not every species sold by pet stores are appropriate. Some species can flourish in home saltwater aquaria. Uninformed or unscrupulous dealers sometimes sell juvenile sharks like the nurse shark, which upon reaching adulthood is far too large for typical home aquaria. Public aquaria generally do not accept donated specimens that have outgrown their housing. Some owners have been tempted to release them. Species appropriate to home aquaria represent considerable spatial and financial investments as they generally approach adult lengths of 3 ft and can live up to 25 years.
In Hawaii.
Sharks figure prominently in Hawaiian mythology. Stories tell of men with shark jaws on their back who could change between shark and human form. A common theme was that a shark-man would warn beach-goers of sharks in the waters. The beach-goers would laugh and ignore the warnings and get eaten by the shark-man who warned them. Hawaiian mythology also includes many shark gods. Among a fishing people, the most popular of all aumakua, or deified ancestor guardians, are shark aumakua. Kamaku describes in detail how to offer a corpse to become a shark. The body transforms gradually until the kahuna can point the awe-struck family to the markings on the shark's body that correspond to the clothing in which the beloved's body had been wrapped. Such a shark aumakua becomes the family pet, receiving food, and driving fish into the family net and warding off danger. Like all aumakua it had evil uses such as helping kill enemies. The ruling chiefs typically forbade such sorcery. Many Native Hawaiian families claim such an aumakua, who is known by name to the whole community.
Kamohoali'i is the best known and revered of the shark gods, he was the older and favored brother of Pele, and helped and journeyed with her to Hawaii. He was able to assume all human and fish forms. A summit cliff on the crater of Kilauea is one of his most sacred spots. At one point he had a "heiau" (temple or shrine) dedicated to him on every piece of land that jutted into the ocean on the island of Molokai. Kamohoali'i was an ancestral god, not a human who became a shark and banned the eating of humans after eating one herself. In Fijian mytholog, Dakuwanga was a shark god who was the eater of lost souls.
In popular culture.
In contrast to the complex portrayals by Hawaiians and other Pacific Islanders, the European and Western view of sharks has historically been mostly of fear and malevolence. Sharks are used in popular culture commonly as eating machines, notably in the "Jaws" novel and the film of the same name, along with its sequels. Sharks are threats in other films such as "Deep Blue Sea", "The Reef", and others, although they are sometimes used for comedic effect such as in "Finding Nemo" and the "Austin Powers" series. These comedic effects can sometimes be unintentional, as seen in "Batman: The Movie" and various Syfy channel films like "Dinoshark" and "Sharktopus".
Sharks tend to be seen quite often in cartoons whenever a scene involves the ocean. Such examples include the "Tom and Jerry" cartoons, "Jabberjaw", and other shows produced by Hanna-Barbera. They also are used commonly as a clichéd means of killing off a character that is held up by a rope or some similar object as the sharks swim right below them, or the character may be standing on a plank above shark infested waters.
Popular misconceptions.
A popular myth is that sharks are immune to disease and cancer, but this is not scientifically supported. Sharks have been known to get cancer. Both diseases and parasites affect sharks. The evidence that sharks are at least resistant to cancer and disease is mostly anecdotal and there have been few, if any, scientific or statistical studies that show sharks to have heightened immunity to disease.
Other apparently false claims are that fins prevent cancer and treat osteoarthritis. No scientific proof supports these claims; at least one study has shown shark cartilage of no value in cancer treatment.
Conservation.
Fishery.
It is estimated that 100 million sharks are killed by people every year, due to commercial and recreational fishing. Shark finning yields are estimated at 1.44 million metric tons for 2000, and 1.41 million tons for 2010. Based on an analysis of average shark weights, this translates into a total annual mortality estimate of about 100 million sharks in 2000, and about 97 million sharks in 2010, with a total range of possible values between 63 and 273 million sharks per year. Sharks are a common seafood in many places, including Japan and Australia. In the Australian state of Victoria, shark is the most commonly used fish in fish and chips, in which fillets are battered and deep-fried or crumbed and grilled. In fish and chip shops, shark is called flake. In India, small sharks or baby sharks (called sora in Tamil language, Telugu language) are sold in local markets. Since the flesh is not developed, cooking the flesh breaks it into powder, which is then fried in oil and spices (called sora puttu/sora poratu). The soft bones can be easily chewed. They are considered a delicacy in coastal Tamil Nadu. Icelanders ferment Greenland sharks to produce hákarl, which is widely regarded as a national dish. During a four-year period from 1996 to 2000, an estimated 26 to 73 million sharks were killed and traded annually in commercial markets.
Sharks are often killed for shark fin soup. Fishermen capture live sharks, fin them, and dump the finless animal back into the water. Shark finning involves removing the fin with a hot metal blade. The resulting immobile shark soon dies from suffocation or predators. Shark fin has become a major trade within black markets all over the world. Fins sell for about $300/lb in 2009. Poachers illegally fin millions each year. Few governments enforce laws that protect them. In 2010 Hawaii became the first U.S. state to prohibit the possession, sale, trade or distribution of shark fins. From 1996 to 2000, an estimated 38 million sharks had been killed per year for harvesting shark fins.
Shark fin soup is a status symbol in Asian countries, and is considered healthy and full of nutrients. Sharks are also killed for meat. European diners consume dogfishes, smoothhounds, catsharks, makos, porbeagle and also skates and rays. However, the U.S. FDA lists sharks as one of four fish (with swordfish, king mackerel, and tilefish) whose high mercury content is hazardous to children and pregnant women.
Sharks generally reach sexual maturity only after many years and produce few offspring in comparison to other harvested fish. Harvesting sharks before they reproduce severely impacts future populations.
The majority of shark fisheries have little monitoring or management. The rise in demand for shark products increases pressure on fisheries. Major declines in shark stocks have been recorded—some species have been depleted by over 90% over the past 20–30 years with population declines of 70% not unusual. A study by the International Union for Conservation of Nature suggests that one quarter of all known species of sharks and rays are threatened by extinction and 25 species were classified as critically endangered.
Other threats.
Other threats include habitat alteration, damage and loss from coastal development, pollution and the impact of fisheries on the seabed and prey species. The 2007 documentary, "Sharkwater" exposed how sharks are being hunted to extinction.
Protection.
In 1991, South Africa was the first country in the world to declare Great White sharks a legally protected species.
Intending to ban the practice of shark finning while at sea, the United States Congress passed the Shark Finning Prohibition Act in 2000. Two years later the Act saw its first legal challenge in "United States v. Approximately 64,695 Pounds of Shark Fins". In 2008 a Federal Appeals Court ruled that a loophole in the law allowed non-fishing vessels to "purchase" shark fins from fishing vessels while on the high seas. Seeking to close the loophole, the Shark Conservation Act was passed by Congress in December 2010, and it was signed into law in January 2011.
In 2003, the European Union introduced a general shark finning ban for all vessels of all nationalities in Union waters and for all vessels flying a flag of one of its member states. This prohibition was amended in June 2013 to close remaining loopholes.
In 2009, the International Union for Conservation of Nature "IUCN Red List of Endangered Species" named 64 species, one-third of all oceanic shark species, as being at risk of extinction due to fishing and shark finning.
In 2010, the Convention on International Trade in Endangered Species (CITES) rejected proposals from the United States and Palau that would have required countries to strictly regulate trade in several species of scalloped hammerhead, oceanic whitetip and spiny dogfish sharks. The majority, but not the required two-thirds of voting delegates, approved the proposal. China, by far the world's largest shark market, and Japan, which battles all attempts to extend the convention to marine species, led the opposition. In March 2013, three endangered commercially valuable sharks, the hammerheads, the oceanic whitetip and porbeagle were added to Appendix 2 of CITES, bringing shark fishing and commerce of these species under licensing and regulation.
In 2010, Greenpeace International added the school shark, shortfin mako shark, mackerel shark, tiger shark and spiny dogfish to its seafood red list, a list of common supermarket fish that are often sourced from unsustainable fisheries. Advocacy group Shark Trust campaigns to limit shark fishing. Advocacy group Seafood Watch directs American consumers to not eat sharks.
Under the auspices of the Convention on the Conservation of Migratory Species of Wild Animals (CMS), also known as the Bonn Convention, the Memorandum of Understanding on the Conservation of Migratory Sharks was concluded and came into effect in March 2010. It was the first global instrument concluded under CMS and aims at facilitating international coordination for the protection, conservation and management of migratory sharks, through multilateral, intergovernmental discussion and scientific research.
In July 2013, New York state, a major market and entry point for shark fins, banned the shark fin trade joining
seven other states of the United States and the three Pacific U.S territories in providing legal protection to sharks.
References.
</dl>

</doc>
<doc id="43618" url="http://en.wikipedia.org/wiki?curid=43618" title="Black panther">
Black panther

A black panther is typically a melanistic colour variant of any "Panthera" species. Black panthers in Asia and Africa are leopards ("Panthera pardus"). Black panthers in the Americas are black jaguars ("Panthera onca").
Melanism.
Melanism in the jaguar ("Panthera onca") is conferred by a dominant allele, and in the leopard ("Panthera pardus") by a recessive allele. Close examination of the color of these black cats will show that the typical markings are still present, but are hidden by the excess black pigment melanin, giving an effect similar to that of printed silk. This is called "ghost striping". Melanistic and non-melanistic animals can be littermates. It is thought that melanism may confer a selective advantage under certain conditions since it is more common in regions of dense forest, where light levels are lower. Recently, preliminary studies also suggest that melanism might be linked to beneficial mutations in the immune system.
Leopard.
The Javan leopard was initially described as being black with dark black spots and silver-grey eyes.
Black leopards are common in the equatorial rainforest of Malaya and the tropical rainforest on the slopes of some African mountains such as Mount Kenya. They are also common in Java, and are reported from densely forested areas in southwestern China, Myanmar, Assam and Nepal, from Travancore and other parts of southern India where they may be more numerous than spotted panthers. One was recorded in the equatorial forest of Cameroon.
In captivity.
Melanistic leopards are the most common form of black panther in captivity and they have been selectively bred for decades in the zoo and exotic pet trades. According to Funk and Wagnalls' Wildlife Encyclopedia, captive black leopards are less fertile than normal leopards, with average litter sizes of 1.8 and 2.1, respectively. This is likely due to inbreeding depression. 
In the early 1980s, Glasgow Zoo acquired a 10-year-old black leopard, nicknamed the Cobweb Panther, from Dublin Zoo. She was exhibited for several years before being moved to the Madrid Zoo. This leopard had a uniformly black coat profusely sprinkled with white hairs as though draped with spider webs. The condition appeared to be vitiligo; as she aged, the white became more extensive. Since then, other "cobweb panthers" have been reported and photographed in zoos.
Jaguar.
In jaguars, the melanism allele is dominant. Consequently, black jaguars may produce either black or spotted cubs, but a pair of spotted jaguars can only produce spotted cubs. Individuals with two copies of the allele are darker (the black background colour is more dense) than ones with just one copy, whose background colour may appear to be dark charcoal rather than black.
The black jaguar was considered a separate species by indigenous peoples.
W H Hudson wrote,
"The jaguar is a beautiful creature, the ground-colour of the fur a rich golden-red tan, abundantly marked with black rings, enclosing one or two small spots within. This is the typical colouring and it varies little in the temperate regions; in the hot region the Indians recognise three strongly marked varieties, which they regard as distinct species – the one described; the smaller jaguar, less aquatic in his habits and marked with spots, not rings; and, thirdly, the black variety. They scout the notion that their terrible "black tiger" is a mere melanic variation, like the black leopard of the Old World and the wild black rabbit. They regard it as wholly distinct, and affirm that it is larger and much more dangerous than the spotted jaguar; that they recognise it by its cry; that it belongs to the terra firma rather than to the water-side; finally, that black pairs with black, and that the cubs are invariably black. Nevertheless, naturalists have been obliged to make it specifically one with "Felis onca" ["Panthera onca"], the familiar spotted jaguar, since, when stripped of its hide, it is found to be anatomically as much like that beast as the black is like the spotted leopard."
A black jaguar named "Diablo" was inadvertently crossed with a lioness named "Lola" at the Bear Creek Wildlife Sanctuary in Barrie, Ontario, Canada. The offspring were a charcoal black jaglion female and a tan-coloured, spotted jaglion male. It therefore appears that the jaguar melanism gene is also dominant over normal lion colouration (the black jaguar sire was presumably carrying the black on only one allele). In preserved, stuffed specimens, black leopards often fade to a rusty colour but black jaguars fade to a chocolate brown colour.
Cougar.
There are no authenticated cases of truly melanistic cougars (pumas). Melanistic cougars have never been photographed or shot in the wild and none has ever been bred. Unconfirmed sightings, known as the "North American black panther", are currently attributed to errors in species identification by non-experts, and by the memetic exaggeration of size. Black panthers in the American Southeast feature prominently in Choctaw folklore where, along with the owl, they are often thought to symbolize Death.
In his "Histoire Naturelle" (1749), Georges-Louis Leclerc, Comte de Buffon, wrote of the "Black Cougar":
"M. de la Borde, King’s physician at Cayenne, informs me, that in the [South American] Continent there are three species of rapacious animals; that the first is the jaguar, which is called the tiger; that the second is the couguar ["sic"], called the red tiger, on account of the uniform redness of his hair; that the jaguar is of the size of a large bull-dog, and weighs about 200 pounds [90 kg]; that the cougar is smaller, less dangerous, and not so frequent in the neighbourhood of Cayenne as the jaguar; and that both these animals take six years in acquiring their full growth. He adds, that there is a third species in these countries, called the black tiger, of which we have given a figure under the appellation of the black cougar. The head is pretty similar to that of the common cougar; but the animal has long black hair, and likewise a long tail, with strong whiskers. He weighs not much above forty pounds [18 kg]. The female brings forth her young in the hollows of old trees."
This "black cougar" was most likely a margay or ocelot, which are under 40 lb in weight, live in trees, and do have melanistic phases.
Another description of a black cougar was provided by Thomas Pennant:
"Black tiger, or cat, with the head black, sides, fore part of the legs, and the tail, covered with short and very glossy hairs, of a dusky colour, sometimes spotted with black, but generally plain: Upper lips white: At the corner of the mouth a black spot: Long hairs above each eye, and long whiskers on the upper lip: Lower lip, throat, belly and the inside of the legs, whitish, or very pale ash-colour: Paws white: Ears pointed: Grows to the size of a heifer of a year old: Has vast strength in its limbs.-- Inhabits Brasil and Guiana: Is a cruel and fierce beast; much dreaded by the Indians; but happily is a scarce species."
According to his translator Smellie (1781), the description was taken from two black cougars exhibited in London some years previously.
Reports of black panthers.
Reports of black panthers in Australia.
Black panther sightings are frequently recorded in rural Victoria and New South Wales and Western Australia. The Australian "phantom panthers" are said to be responsible for the disappearances and deaths of numerous cats, dogs and livestock.
"" led an investigation into the phantom panther. Mike Williams, a local researcher, said he had sent feces and hair found by locals to labs for analysis, which identified it as feces from dogs that had feasted on swamp wallaby, and hair from a domestic cat. Mr Williams said he also had known leopard feces and hair collected from a private zoo tested by one of the same labs, but that these samples came back with the same results of dog feces and domestic cat hair. This indicated the lab incapable of distinguishing between leopard hairs and those of domestic animals, casting doubt on the previous findings. The lab used was not identified in the episode.
Pseudo-melanism.
Pseudo-melanism (abundism) occurs in leopards. A pseudo-melanistic leopard has a normal background color, but the spots are more densely packed than normal and merge to obscure the golden-brown background color. Any spots on the flanks and limbs that have not merged into the mass of swirls and stripes are unusually small and discrete, rather than forming rosettes. The face and underparts are paler and dappled like those of ordinary spotted leopards.
Richard Lydekker described specimens of pseudo-melanistic leopards found in South Africa in the late nineteenth century:
"The ground-color of this animal was a rich tawny, with an orange tinge; but the spots, instead of being of the usual rosette-like form, were nearly all small and solid, like those on the head of an ordinary leopard; while from the top of the head to near the root of the tail the spots became almost confluent, producing the appearance of a broad streak of black running down the back. A second skin had the black area embracing nearly the whole of the back and flanks, without showing any trace of the spots. These dark-coloured South African leopards differ from the black leopards of the northern and eastern parts of Africa and Asia in that while in the latter the rosette-like spots are always retained and clearly visible, in the former the rosettes are lost..."—Lydekker, R. (1910), "Harmsworth Natural History"
Most other color morphs of leopards are known only from paintings or museum specimens. In May 1936, the British Natural History Museum exhibited the mounted skin of an unusual Somali leopard. The pelt was richly decorated with an intricate pattern of swirling stripes, blotches, curls and fine-line traceries. This is different from a spotted leopard, but similar to a king cheetah, hence the modern cryptozoology term king leopard. Between 1885 and 1934, six pseudo-melanistic leopards were recorded in the Albany and Grahamstown districts of South Africa. This indicated a mutation in the local leopard population. Other king leopards have been recorded from Malabar in southwestern India. Shooting for trophies may have contributed to the loss of these populations.

</doc>
