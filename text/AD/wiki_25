<doc id="46863" url="http://en.wikipedia.org/wiki?curid=46863" title="Asymmetric warfare">
Asymmetric warfare

Asymmetric warfare is war between belligerents whose relative military power differs significantly, or whose strategy or tactics differ significantly.
"Asymmetric warfare" can describe a conflict in which the resources of two belligerents differ in essence and in the struggle, interact and attempt to exploit each other's characteristic weaknesses. Such struggles often involve strategies and tactics of unconventional warfare, the weaker combatants attempting to use strategy to offset deficiencies in quantity or quality. Such strategies may not necessarily be militarized. This is in contrast to "symmetric warfare", where two powers have similar military power and resources and rely on tactics that are similar overall, differing only in details and execution.
The term is also frequently used to describe what is also called "guerrilla warfare", "insurgency", "terrorism", "counterinsurgency", and "counterterrorism", essentially violent conflict between a formal military and an informal, less equipped and supported, undermanned but resilient opponent.
Definition and differences.
The popularity of the term dates from Andrew J. R. Mack's 1975 article "Why Big Nations Lose Small Wars" in "World Politics", in which "asymmetric" referred simply to a significant disparity in power between opposing actors in a conflict. "Power," in this sense, is broadly understood to mean material power, such as a large army, sophisticated weapons, an advanced economy, and so on. Mack's analysis was largely ignored in its day, but the end of the Cold War sparked renewed interest among academics. By the late 1990s, new research building on Mack's insights was beginning to mature, and, after 2004, the U.S. military began once again to seriously consider the problems associated with asymmetric warfare.
Discussion since 2004 has been complicated by the tendency of academic and military communities to use the term in different ways, and by its close association with guerrilla warfare, insurgency, terrorism, counterinsurgency, and counterterrorism. Military authors tend to use the term "asymmetric" to refer to the indirect nature of the strategies many weak actors adopt, or even to the nature of the adversary itself (e.g., "asymmetric adversaries can be expected to …") rather than to the correlation of forces.
Academic authors tend to focus more on explaining the puzzle of weak actor victory in war: if "power," conventionally understood, conduces to victory in war, then how is the victory of the "weak" over the "strong" explained? Key explanations include
Asymmetric conflicts include both interstate and civil wars, and over the past two hundred years have generally been won by strong actors. Since 1950, however, weak actors have won a majority of all asymmetric conflicts.
Strategic basis.
In most conventional warfare, the belligerents deploy forces of a similar type and the outcome can be predicted by the quantity of the opposing forces or by their quality, for example better command and control of their forces (c2). There are times where this is not true because the composition or strategy of the forces makes it impossible for either side to close in battle with the other. An example of this is the standoff between the continental land forces of the French army and the maritime forces of the United Kingdom's Royal Navy during the French Revolutionary and Napoleonic Wars. In the words of Admiral Jervis during the campaigns of 1801, "I do not say, my Lords, that the French will not come. I say only they will not come by sea", and a confrontation that Napoleon Bonaparte described as that between the elephant and the whale.
Tactical basis.
The tactical success of asymmetric warfare is dependent on at least some of the following assumptions:
Use of terrain.
Terrain can be used as a force multiplier by the smaller force and as a force inhibitor against the larger force. Such terrain is called difficult terrain.
The contour of the land is an aid to the army; sizing up opponents to determine victory, assessing dangers and distance. "Those who do battle without knowing these will lose." ― Sun Tzu, "The Art of War"
The guerrillas must move amongst the people as a fish swims in the sea. ― Mao Zedong.
A good example of this type of strategy is the Battle of Thermopylae, where the narrow terrain of a defile was used to funnel the Persian forces, who were numerically superior, to a point where they could not use their size as an advantage.
For a detailed description of the advantages for the weaker force in the use of built-up areas when engaging in asymmetric warfare, see the article on urban warfare.
War by proxy.
Where asymmetric warfare is carried out (generally covertly) by allegedly non-governmental actors who are connected to or sympathetic to a particular nation's (the "state actor's") interest, it may be deemed "war by proxy." This is typically done to give "deniability" to the state actor. The deniability can be important to keep the state actor from being tainted by the actions, to allow the state actor to negotiate in apparent good faith by claiming they are not responsible for the actions of parties who are merely sympathizers, or to avoid being accused of belligerent actions or war crimes. If proof emerges of the true extent of the state actor's involvement, this strategy can backfire; for example see Iran-contra and Philip Agee.
Asymmetric warfare and terrorism.
There are two different viewpoints on the relationship between asymmetric warfare and terrorism. In the modern context, asymmetric warfare is increasingly considered a component of fourth generation warfare. When practiced outside the laws of war, it is often defined as terrorism, though rarely by its practitioners or their supporters. For example, terrorists often use women and children as human shields, which practice is not considered either moral or part of traditional symmetrical warfare.
The other view is that asymmetric warfare does not coincide with terrorism. The use of terror by the much lesser Mongol forces in the creation and control of the Mongol empire could be viewed as asymmetric warfare. The other is the use of state terrorism by the superior Nazi forces in the Balkans, in an attempt to suppress the resistance movement.
Examples.
American Revolutionary War.
From its initiation, the American Revolutionary War was, necessarily, a showcase for asymmetric techniques. In the 1920s, Harold Murdock of Boston attempted to solve the puzzle of the first shots fired on Lexington Green, and came to the suspicion that the few score militia men who gathered before sunrise to await the arrival of hundreds of well-prepared British soldiers were sent specifically to provoke an incident which could be used for propaganda purposes. The return of the British force to Boston following the search operations at Concord was subject to constant skirmishing, using partisan forces gathered from communities all along the route, making maximum use of the terrain (particularly trees and stone field walls) to overcome the limitations of their weapons- muskets with an effective range of only about 50–70 metres. Throughout the war, skirmishing tactics against British troops on the move continued to be a key factor in the Patriots' success; however, they may also have encouraged the occasional incidents, particularly in the later stages, where British troops used alleged surrender violations as a justification for killing large numbers of captives (e.g., Waxhaw and Groton Heights).
Another feature of the long march from Concord was the urban warfare technique of using buildings along the route as additional cover for snipers. When revolutionary forces forced their way into Norfolk, Virginia, and used waterfront buildings as cover for shots at British vessels out in the river, the response of destruction of those buildings was ingeniously used to the advantage of the rebels, who encouraged the spread of fire throughout the largely Loyalist town, and spread propaganda blaming it on the British. Shortly afterwards they destroyed the remaining houses, on the grounds that they might provide cover for British soldiers. On the subject of propaganda, it should be borne in mind that, contrary to the impression given in the popular American film "The Patriot", British forces never adopted a popular response to partisan-style asymmetric warfare — retribution massacres of groups selected on a semi-random basis from the population at large.
The rebels also adopted a form of asymmetric sea warfare, by using small, fast vessels to avoid the Royal Navy, and capturing or sinking large numbers of merchant ships; however the British responded by issuing letters of marque permitting private armed vessels to undertake reciprocal attacks on enemy shipping. John Paul Jones became notorious in Britain for his expedition from France in the little sloop of war "Ranger" in April 1778, during which, in addition to his attacks on merchant shipping, he made two landings on British soil. The effect of these raids, particularly when coupled with his capture of the Royal Navy's "HMS Drake"—the first such success in British waters, but not Jones's last—was to force the British government to increase resources for coastal defence, and to create a climate of fear among the British public which was subsequently fed by press reports of his preparations for the 1779 "Bonhomme Richard" mission.
From 1776, the conflict turned increasingly into a proxy war on behalf of France, following a strategy proposed in the 1760s but initially resisted by the idealistic young King Louis XVI, who came to the throne at the age of 19 a few months before Lexington. France also encouraged proxy wars against the British in India, but ultimately drove itself to the brink of state bankruptcy by entering the war(s) directly, on several fronts throughout the world.
American Civil War.
The American Civil War saw the rise of asymmetric warfare in the Border States, and in particular on the US Western Territorial Border after the Kansas-Nebraska Act of 1854 opened the territories to voting on the expansion of slavery beyond the Missouri Compromise lines. Political implications of this broken 1820s compromise were nothing less than the potential expansion of slavery all across the North American continent, including the northern reaches of the annexed Mexican territories to California and Oregon. So the stakes were high and it caused a flood of immigration to the border: some to grab land and expand slavery west, others to grab land and vote down the expansion of slavery. The pro-slavery land grabbers began asymmetric violent attacks against the more pacifist abolitionists who had settled Lawrence and other territorial towns for suppressing slavery. John Brown travelled to Osawatomie in the Kansas Territory expressly to foment retaliatory attacks back against the pro-slavery guerrillas who, by 1858, had twice ransacked both Lawrence and Osawatomie (where one of Brown's sons was shot dead). The abolitionists would not return the attacks and Brown theorized that a violent spark set off on "the Border" would be a way to finally ignite his long hoped-for slave rebellion. Brown had broad-sworded slave owners at Potawatomi Creek, so the bloody civilian violence was initially symmetrical; however, once the American Civil War ignited in 1861, and when the state of Missouri voted overwhelmingly not to secede from the Union, the pro-slavers on the MO-KS border were driven either south to Arkansas and Texas, or underground—where they became guerrilla fighters and "Bushwhackers" living in the brushy ravines throughout northwest Missouri across the (now) state line from Kansas. The bloody "Border War" lasted all during the Civil War (and long after with guerrilla partisans like the James brothers cynically robbing and murdering, aided and abetted by lingering lost-causers). Tragically the Western Border War was an asymmetric war: pro-slavery guerrillas and paramilitary partisans on the pro-Confederate side attacking pro-Union townspeople and commissioned Union military units; with the Union army trying to keep both in check: blocking Kansans and pro-Union Missourians from organizing militarily against the marauding Bushwhackers. The worst act of domestic terror in US history came in August 1863 when paramilitary guerrillas amassed 350 strong and rode all night 50 miles across eastern Kansas to the abolitionist stronghold of Lawrence (purely a political target) and destroyed the town, gunning down on their front porches 150 civilians. The Confederate officer whose company had joined Quantrill's gang that day witnessed the civilian slaughter and forbade his soldiers from joining in the carnage. The commissioned officer refused to participate in Quantrill's asymmetric warfare on civilians. 
Philippine-American War.
The Philippine–American War (Filipino/Tagalog: "Digmaang Pilipino-Amerikano") (1899–1902) was an armed conflict between the United States and Filipino revolutionaries. Estimates of the Filipino forces vary between 100,000 to 1,000,000, with tens of thousands of auxiliaries. Lack of weapons and ammunition was a significant impediment to the Filipinos, so most of the forces were only armed with bolo knives, bows and arrows, spears and other primitive weapons that, in practice, proved vastly inferior to U.S. firepower.
The goal, or end-state, sought by the First Philippine Republic was a sovereign, independent, socially stable Philippines led by the "ilustrado" (intellectual) oligarchy. Local chieftains, landowners, and businessmen were the "principales" who controlled local politics. The war was strongest when "illustrados", "principales", and peasants were unified in opposition to annexation. The peasants, who provided the bulk of guerrilla manpower, had interests different from their "illustrado" leaders and the "principales" of their villages. Coupled with the ethnic and geographic fragmentation, unity was a daunting task. The challenge for Aguinaldo and his generals was to sustain unified Filipino public opposition; this was the revolutionaries' strategic center of gravity. The Filipino operational center of gravity was the ability to sustain its force of 100,000 irregulars in the field. The Filipino general Francisco Macabulos described the Filipinos' war aim as, "not to vanquish the U.S. Army but to inflict on them constant losses." They initially sought to use conventional tactics and an increasing toll of U.S. casualties to contribute to McKinley's defeat in the 1900 presidential election. Their hope was that as President the avowedly anti-imperialist future Secretary of state William Jennings Bryan would withdraw from the Philippines. They pursued this short-term goal with guerrilla tactics better suited to a protracted struggle. While targeting McKinley motivated the revolutionaries in the short term, his victory demoralized them and convinced many undecided Filipinos that the United States would not depart precipitously. For most of 1899, the revolutionary leadership had viewed guerrilla warfare strategically only as a tactical option of final recourse, not as a means of operation which better suited their disadvantaged situation. On November 13, 1899, Emilio Aguinaldo decreed that guerrilla war would henceforth be the strategy. This made American occupation of the Philippine archipelago all the more difficult over the next few years. In fact, during just the first four months of the guerrilla war, the Americans had nearly 500 casualties. The Philippine Revolutionary Army began staging bloody ambushes and raids, such as the guerrilla victories at Paye, Catubig, Makahambus, Pulang Lupa, Balangiga and Mabitac. At first, it even seemed as if the Filipinos would fight the Americans to a stalemate and force them to withdraw. This was even considered by President McKinley at the beginning of the phase. The shift to guerrilla warfare drove the US Army to adopt counter-insurgency tactics. Civilians were given identification and forced into concentration camps with a publicly announced deadline after which all persons found outside of camps without identification would be shot on sight. Thousands of civilians died in these camps due to poor conditions.
20th century.
Second Boer War.
Asymmetric warfare featured prominently during the Second Boer War. After an initial phase, which was fought by both sides as a conventional war, the British captured Johannesburg, the Boers' largest city, and captured the capitals of the two Boer Republics. The British then expected the Boers to accept peace as dictated by the victors in the traditional European way. However instead of capitulating, the Boers fought a protracted guerrilla war. Between twenty and thirty thousand Boer commandos were only defeated after the British brought to bear four hundred and fifty thousand troops, about ten times as many as were used in the conventional phase of the war. During this phase the British introduced internment in concentration camps for the Boer civilian population and also implemented a scorched earth policy. Later, the British began using blockhouses built within machine gun range of one another and flanked by barbed wire to slow the Boers' movement across the countryside and block paths to valuable targets. Such tactics eventually evolved into today's counter insurgency tactics.
The Boer commando raids deep into the Cape Colony, which were organized and commanded by Jan Smuts, resonated throughout the century as the British and others adopted and adapted the tactics used by the Boer commandos in later conflicts.
After World War II.
Cold War.
The end of World War II established the two most powerful victors, the United States of America (USA, or just the United States) and the Union of Soviet Socialist Republics (USSR, or just the Soviet Union) as the two dominant world superpowers.
Cold War examples of proxy wars.
In Southeast Asia, specifically Vietnam, the Viet Minh, NLF and other insurgencies engaged in asymmetrical guerrilla warfare with France, at first, then, later, the United States during the period of the Vietnam War.
Likewise, the war between the mujahideen and the Red Army during the Soviet war in Afghanistan has been claimed as the source of the term "asymmetric warfare", although this war occurred years after Mack wrote of "asymmetric conflict," it is notable that the term became well known in the West only in the 1990s. The aid given by the U.S. to the mujahadeen during the war was only covert at the tactical level, the Reagan Administration told the world that it was helping the "freedom-loving people of Afghanistan". This proxy war was aided by many countries including the USA against the USSR during the Cold War. It was considered cost effective and politically successful, as it gave the USSR a military defeat which was a contributing factor to its collapse.
21st century.
Israel/Palestinians.
The ongoing battle between the Israelis and some Palestinian organizations (such as Hamas and Islamic Jihad) is a classic case of asymmetric warfare. Israel has a powerful army, air force and navy, while the Palestinian organisations have no access to large-scale military equipment with which to conduct operations; instead, they utilize asymmetric tactics, such as: small gunfights, cross-border sniping, rocket attacks, use of civilian structures and civilian shields for military attacks, and suicide bombing.
Sri Lanka.
The Sri Lankan Civil War, which raged on and off from 1983 to 2009, between the Sri Lankan government and the Liberation Tigers of Tamil Eelam (LTTE) saw large-scale asymmetric warfare. The war started as an insurgency and progressed to a large-scale conflict with the mixture of guerrilla and conventional warfare. The LTTE pioneered the use of suicide bombing and perfected it with the use of male/female suicide bombers both on and off battlefield; use of explosive-filled boats for suicide attacks on military shipping; use of light aircraft targeting military installations.
Kashmir.
Pakistan claims territorial rights to the region of Kashmir, where it has been engaged in a proxy war with India since 1988.
Iraq.
The victory by the US-led coalition forces in the 1991 Persian Gulf War and the 2003 invasion of Iraq, demonstrated that training, tactics and technology can provide overwhelming victories in the field of battle during modern conventional warfare. After Saddam Hussein's regime was removed from power, the Iraq campaign moved into a different type of asymmetric warfare where the coalition's use of superior conventional warfare training, tactics and technology were of much less use against continued opposition from the various partisan groups operating inside Iraq.
Syria.
Much of the 2012-2013 Syrian civil war has been fought asymmetrically. The Syrian National Coalition along with the Mujahideen and Kurdish Democratic Union Party, has been engaging with the forces of the Syrian government through asymmetric means. The conflict has seen large-scale asymmetric warfare across the country, with the forces opposed to the government unable to engage symmetrically with the Syrian government so other tactics such as suicide bombings and targeted assassinations have been put to effective use.
See also.
US organisations:
Documents:

</doc>
<doc id="46865" url="http://en.wikipedia.org/wiki?curid=46865" title="Manzanar">
Manzanar

Manzanar is most widely known as the site of one of ten camps where over 110,000 Japanese Americans were incarcerated during World War II. Located at the foot of the Sierra Nevada in California's Owens Valley between the towns of Lone Pine to the south and Independence to the north, it is approximately 230 mi northeast of Los Angeles. Manzanar (which means "apple orchard" in Spanish) was identified by the United States National Park Service as the best-preserved of the former camp sites, and is now the Manzanar National Historic Site, which preserves and interprets the legacy of Japanese American incarceration in the United States.
Long before the first incarcerees arrived in March 1942, Manzanar was home to Native Americans, who mostly lived in villages near several creeks in the area. Ranchers and miners formally established the town of Manzanar in 1910, but abandoned the town by 1929 after the City of Los Angeles purchased the water rights to virtually the entire area. As different as these groups were, their histories displayed a common thread of forced relocation.
Since the last incarcerees left in 1945, former incarcerees and others have worked to protect Manzanar and to establish it as a National Historic Site to ensure that the history of the site, along with the stories of those who were unjustly incarcerated there, are remembered by current and future generations. The primary focus is the Japanese American incarceration era, as specified in the legislation that created the Manzanar National Historic Site. The site also interprets the former town of Manzanar, the ranch days, the settlement by the Owens Valley Paiute, and the role that water played in shaping the history of the Owens Valley.
Terminology.
Since the end of World War II, there has been debate over the terminology used to refer to Manzanar, and the other camps in which Americans of Japanese ancestry and their immigrant parents, were incarcerated by the United States Government during the war. Manzanar has been referred to as a "War Relocation Center," "relocation camp," "relocation center," "internment camp", and "concentration camp", and the controversy over which term is the most accurate and appropriate continues to the present day.
Dr. James Hirabayashi, Professor Emeritus and former Dean of Ethnic Studies at San Francisco State University, wrote an article in 1994 in which he stated that he wonders why euphemistic terms used to describe camps such as Manzanar are still being used.
Let us review the main points of the debate. Over 120,000 residents of the U.S.A., two thirds of whom were American citizens, were incarcerated under armed guard. There were no crimes committed, no trials, and no convictions: the Japanese Americans were political incarcerees. To detain American citizens in a site under armed guard surely constitutes a "concentration camp." But what were the terms used by the government officials who were involved in the process and who had to justify these actions? Raymond Okamura provides us with a detailed list of terms. Let's consider three such euphemisms: "evacuation," "relocation," and "non-aliens." Earthquake and flood victims are evacuated and relocated. The words refer to moving people in order to rescue and protect them from danger. The official government policy makers consistently used "evacuation" to refer to the forced removal of the Japanese Americans and the sites were called "relocation centers." These are euphemisms (Webster: "the substitution of an inoffensive term for one considered offensively explicit") as the terms do not imply forced removal nor incarceration in enclosures patrolled by armed guards. The masking was intentional.
Hirabayashi went on to describe the harm done by the use of such euphemisms and also addressed the issue of whether or not only the Nazi camps can be called "concentration camps."
The harm in continuing to use the government's euphemisms is that it disguises or softens the reality which subsequently has been legally recognized as a grave error. The actions abrogated some fundamental principles underlying the Constitution, the very document under which we govern ourselves. This erosion of fundamental rights has consequences for all citizens of our society and we must see that it is never repeated. Some have argued that the Nazi Germany camps during the Holocaust were concentration camps and to refer to the Japanese American camps likewise would be an affront to the Jews. It is certainly true that the Japanese Americans did not suffer the harsh fate of the Jews in the terrible concentration camps or death camps where Nazi Germany practiced a policy of genocide. Although the loss of life was minimal in America's concentration camps, it does not negate the reality of the unconstitutional incarceration of Japanese American citizens. Michi and Walter Weglyn's research concerning Nazi Germany's euphemisms for their concentration camps revealed such phrases as "protective custody camps," "reception centers," and "transit camps." Ironically, two Nazi euphemisms were identical to our government's usage: "assembly centers" and "relocation centers." It might be well to point out, also, that the Nazis were not operating under the U.S. Constitution. Comparisons usually neglect to point out that Hitler was operating under the rules of the Third Reich. In America all three branches of the U.S. government, ostensibly operating under the U.S. Constitution, ignored the Bill of Rights in order to incarcerate Japanese Americans.
In 1998, use of the term "concentration camps" gained greater credibility prior to the opening of an exhibit about the American camps at Ellis Island. Initially, the American Jewish Committee (AJC) and the National Park Service, which manages Ellis Island, objected to the use of the term in the exhibit. However, during a subsequent meeting held at the offices of the AJC in New York City, leaders representing Japanese Americans and Jewish Americans reached an understanding about the use of the term. After the meeting, the Japanese American National Museum and the AJC issued a joint statement (which was included in the exhibit) that read in part:
A concentration camp is a place where people are imprisoned not because of any crimes they have committed, but simply because of who they are. Although many groups have been singled out for such persecution throughout history, the term 'concentration camp' was first used at the turn of the [20th] century in the Spanish American and Boer Wars During World War II, America's concentration camps were clearly distinguishable from Nazi Germany's. Nazi camps were places of torture, barbarous medical experiments and summary executions; some were extermination centers with gas chambers. Six million Jews were slaughtered in the Holocaust. Many others, including Gypsies, Poles, homosexuals and political dissidents were also victims of the Nazi concentration camps. In recent years, concentration camps have existed in the former Soviet Union, Cambodia and Bosnia. Despite differences, all had one thing in common: the people in power removed a minority group from the general population and the rest of society let it happen.
The "New York Times" published an unsigned editorial supporting the use of "concentration camp" in the exhibit. An article quoted Jonathan Mark, a columnist for "The Jewish Week", who wrote, "Can no one else speak of slavery, gas, trains, camps? It's Jewish malpractice to monopolize pain and minimize victims." AJC Executive Director David A. Harris stated during the controversy, "We have not claimed Jewish exclusivity for the term 'concentration camps.'"
On July 7, 2012, at their annual convention, the National Council of the Japanese American Citizens League unanimously ratified the "Power of Words Handbook," calling for the use of "...truthful and accurate terms, and retiring the misleading euphemisms created by the government to cover up the denial of Constitutional and human rights, the force, oppressive conditions, and racism against 120,000 innocent people of Japanese ancestry locked up in America's World War II concentration camps." 
According to the "Power Of Words Handbook":
From government documents and propaganda, to public discourse and newspapers, many euphemisms have been used to describe the experiences of Japanese Americans who were forced from their homes and communities during World War II. Words like "evacuation", "relocation", and "assembly centers" imply that the United States Government was trying to rescue Japanese Americans from a disastrous environment on the West Coast and simply help them move to a new gathering place. These terms strategically mask the fact that thousands of Japanese Americans were denied their rights as US citizens, and forcibly ordered to live in poorly constructed barracks on sites that were surrounded by barbed wire and guard towers. Although the use of euphemisms was commonplace during World War II, and in many subsequent years, we realize that the continued use of these inaccurate terms is highly problematic.
Before World War II.
Owens Valley Paiute.
Manzanar was first inhabited by Native Americans nearly 10,000 years ago. Approximately 1,500 years ago, the area was settled by the Owens Valley "Paiute", who ranged across the Owens Valley from Long Valley on the north to Owens Lake on the south, and from the crest of the Sierra Nevada on the west to the Inyo Mountains on the east. Other Native American nations in the region included the "Miwok", Western "Mono", and "Tubatulabal" to the west, the "Shoshone" to the south and east, and the Mono Lake Paiute to the north. The Owens Valley Paiute hunted and fished, collected pine nuts, and raised crops utilizing irrigation in the Manzanar area. They also traded brown-ware pottery for salt from the Saline Valley, and traded other wares and goods across the Sierra Nevada during the summer and fall.
The Owens Valley had received scant attention from European Americans before the early 1860s, as it was little more than a crossroads of the routes through the area. When gold and silver were discovered in the Sierra Nevada and the Inyo Mountains, the resulting sudden influx of miners, farmers, cattlemen and their hungry herds brought conflict with the Owens Valley Paiute, whose crops were being destroyed. The Owens Valley Indian War of 1861–1863 ensued; at the end, the Owens Valley Paiute, along with other native peoples in the region, were forced at gunpoint by the United States Army to walk almost 200 mi to Fort Tejon, in one of the many forced relocations or "Trails of Tears" inflicted upon Native Americans in the United States.
Approximately one-third of the Native Americans in the Owens Valley were forcibly relocated to Fort Tejon. After 1863, many returned to their permanent villages that had been established along creeks flowing down from the Sierra Nevada mountains. In the Manzanar area, the Owens Valley Paiute had established villages along Bairs, Georges, Shepherds, and Symmes creeks. Evidence of Paiute settlement in the area is still present.
Ranchers.
When European American white settlers first arrived in the Owens Valley in the mid–19th century, they found a number of large Paiute villages in the Manzanar area. John Shepherd, one of the first of the new settlers, homesteaded 160 acre of land 3 mi north of Georges Creek in 1864. With the help of Owens Valley Paiute field workers and laborers, he expanded his ranch to 2000 acre.
In 1905, George Chaffey, an agricultural developer from Southern California, purchased Shepherd's ranch and subdivided it, along with other adjacent ranches. He founded the town of Manzanar in 1910. Chaffey's Owens Valley Improvement Company built an irrigation system and planted thousands of fruit trees. By 1920, the town had more than twenty-five homes, a two-room school, a town hall, and a general store. Also at that time, nearly 5000 acre of apple, pear, and peach trees were under cultivation; along with crops of grapes, prunes, potatoes, corn and alfalfa; and large vegetable and flower gardens.
"Manzanar was a very happy place and a pleasant place to live during those years, with its peach, pear, and apple orchards, alfalfa fields, tree-lined country lanes, meadows and corn fields," said Martha Mills, who lived at Manzanar from 1916 to 1920.
Some of the early orchards, along with remnants of the town and ranches, are still present at Manzanar today.
Quenching Los Angeles' thirst.
As early as March 1905, the City of Los Angeles began secretly acquiring water rights in the Owens Valley. In 1913, it completed construction of its 233 mi Los Angeles Aqueduct, But it did not take long for Los Angeles water officials to realize that Owens River water was not enough to supply the rapidly growing metropolis. In 1920, they began to purchase more of the water rights on the Owens Valley floor. As the decade went on, the City of Los Angeles bought out one Owens Valley farmer after another, and extended its reach northward into Mono County, including Long Valley. By 1933, the City owned 85% of all town property and 95% of all ranch and farm land in the Owens Valley, including Manzanar.
Although some residents sold their land for prices that made them financially independent and relocated, a significant number chose to stay. In dry years, Los Angeles pumped ground water and drained all surface water, diverting all of it into its aqueduct and leaving Owens Valley ranchers without water. Without water for irrigation, the holdout ranchers were forced off their ranches and out of their communities; that included the town of Manzanar, which was abandoned by 1929.
'There was so much water during those early years, that when a horse pulled a buggy, the water frequently came up to the horse's knees,' said Lucille DeBoer, who lived on a ranch at Manzanar. 'When this happened, the children took off their shoes and socks to walk home. In the early 1900s the City of Los Angeles started to purchase ranches in the Owens Valley for the sole purpose of supplying water to the people in Los Angeles. People started to sell their land to the City; the City put in wells to drain the water out of the ground; the trees began to die; and the land finally turned to vacant dirt. This ended the Land of the Big Red Apples.'
Manzanar remained uninhabited until the United States Army leased 6200 acre from the City of Los Angeles for the Manzanar War Relocation Center.
Wartime: 1942–45.
After the December 7, 1941, attack on Pearl Harbor, the United States Government swiftly moved to begin solving the "Japanese Problem" on the West Coast of the United States. In the evening hours of that same day, the Federal Bureau of Investigation (FBI) arrested selected "enemy" aliens, including 2,192 who were of Japanese descent. The California government pressed for action by the national government, as many citizens were alarmed about potential activities by people of Japanese descent.
On February 19, 1942, President Franklin D. Roosevelt signed Executive Order 9066, which authorized the Secretary of War to designate military commanders to prescribe military areas and to exclude "any or all persons" from such areas. The order also authorized the construction of what would later be called "relocation centers" by the War Relocation Authority (WRA) to house those who were to be excluded. This order resulted in the forced relocation of over 120,000 Japanese Americans, two-thirds of whom were native-born American citizens. The rest had been prevented from becoming citizens by federal law. Over 110,000 were incarcerated in the ten concentration camps located far inland and away from the coast.
Manzanar was the first of the ten concentration camps to be established. Initially, it was a temporary "reception center", known as the "Owens Valley Reception Center" from March 21, 1942, to May 31, 1942. At that time, it was operated by the US Army's Wartime Civilian Control Administration (WCCA).
The Owens Valley Reception Center was transferred to the WRA on June 1, 1942, and officially became the "Manzanar War Relocation Center." The first Japanese American incarcerees to arrive at Manzanar were volunteers who helped build the camp. By mid–April, up to 1,000 Japanese Americans were arriving daily, and by July, the population of the camp neared 10,000. Over 90 percent of the incarcerees were from the Los Angeles area, with the rest coming from Stockton, California; and Bainbridge Island, Washington. Many were farmers and fishermen. Manzanar held 10,046 incarcerees at its peak, and a total of 11,070 people were incarcerated there.
Climate.
The weather at Manzanar caused suffering for the incarcerees, few of whom were accustomed to the extremes of the area's climate. The temporary buildings were not adequate to shield people from the weather. The Owens Valley lies at an elevation of about 4000 ft. Summers on the desert floor of the Owens Valley are generally hot, with temperatures exceeding 100 °F not uncommon. Winters bring occasional snowfall and daytime temperatures that often drop into the 40 °F range. At night, temperatures are generally 30 to 40 °F (17 to 22 °C) lower than the daytime highs, and high winds are common day or night. The area's mean annual precipitation is barely five inches (12.7 cm). The ever-present dust was a continual problem due to the frequent high winds; so much so that incarcerees usually woke up in the morning covered from head to toe with a fine layer of dust, and they constantly had to sweep dirt out of the barracks.
"In the summer, the heat was unbearable," said former Manzanar incarceree Ralph Lazo ("see" Notable Manzanar incarcerees section, below). "In the winter, the sparsely rationed oil didn't adequately heat the tar paper-covered pine barracks with knotholes in the floor. The wind would blow so hard, it would toss rocks around."
Camp layout and facilities.
The camp site was situated on 6200 acre at Manzanar, leased from the City of Los Angeles, with the developed portion covering approximately 540 acre. The residential area was about one square mile (2.6 km2), and consisted of 36 blocks of hastily constructed, 20 ft by 100 ft tarpaper barracks, with each incarceree family living in a single 20 ft by 25 ft "apartment" in the barracks. These apartments consisted of partitions with no ceilings, eliminating any chance of privacy. Lack of privacy was a major problem for the incarcerees, especially since the camp had communal men's and women's latrines.
"...One of the hardest things to endure was the communal latrines, with no partitions; and showers with no stalls," said former Manzanar incarceree Rosie Kakuuchi.
Each residential block also had a communal mess hall, a laundry room, a recreation hall, an ironing room, and a heating oil storage tank, although Block 33 lacked a recreation hall. In addition to the residential blocks, Manzanar had 34 additional blocks that had staff housing, camp administration offices, two warehouses, a garage, a camp hospital, and 24 firebreaks. The camp also had school facilities, a high school auditorium, staff housing, chicken and hog farms, churches, a cemetery, a post office, a cooperative store, other shops, a camp newspaper, and other necessary amenities that one would expect to find in most American cities.
Manzanar also had a camouflage net factory, an experimental plantation for producing natural rubber from the Guayule plant, and an orphanage called Children's Village, which housed 101 Japanese American orphans. The camp perimeter had eight watchtowers manned by armed Military Police, and it was enclosed by five-strand barbed wire. There were sentry posts at the main entrance.
Life behind the barbed wire.
After being uprooted from their homes and communities, the incarcerees found themselves having to endure primitive, sub-standard conditions, and lack of privacy. They had to wait in one line after another for meals, at latrines, and at the laundry room. Each camp was intended to be self-sufficient, and Manzanar was no exception. Cooperatives operated various services, such as the camp newspaper, beauty and barber shops, shoe repair, and more. In addition, incarcerees raised chickens, hogs, and vegetables, and cultivated the existing orchards for fruit. Incarcerees made their own soy sauce and tofu.
Food at Manzanar was based on military requirements. Meals usually consisted of hot rice and vegetables, since meat was scarce due to rationing. In early 1944, a chicken ranch began operation, and in late April of the same year, the camp opened a hog farm. Both operations provided welcome meat supplements to the incarcerees' diet.
Most incarcerees were employed at Manzanar to keep the camp running. Unskilled workers earned US$8 per month ($ per month as of 2015), semi-skilled workers earned $12 per month ($ per month as of 2015), skilled workers made $16 per month ($ per month as of 2015), and professionals earned $19 per month ($ per month as of 2015). In addition, all incarcerees received $3.60 per month ($ per month as of 2015) as a clothing allowance.
The incarcerees made Manzanar more livable through recreation. They participated in sports, including baseball and football, and martial arts. They also personalized and beautified their barren surroundings by building elaborate gardens, which often included pools, waterfalls, and rock ornaments. There was even a nine-hole golf course. Remnants of some of the gardens, pools, and rock ornaments are still present at Manzanar.
Resistance.
Although most incarcerees quietly accepted their fate during World War II, there was some resistance in the camps. Poston, Heart Mountain, Topaz, and Tule Lake each had civil disturbances about wage differences, black marketing of sugar, intergenerational friction, rumors of "informers" reporting to the camp administration or the FBI, and other issues. However, the most serious incident occurred at Manzanar on December 5–6, 1942, and became known as the Manzanar Riot.
After several months of tension between incarcerees who supported the Japanese American Citizens League (JACL) and a group of "Kibei" (Japanese Americans educated in Japan), rumors spread that sugar and meat shortages were the result of black marketing by camp administrators. To make matters worse, incarceree and JACL leader Fred Tayama was beaten by six masked men. Harry Ueno, the leader of the Kitchen Workers Union, was suspected of involvement and was arrested and removed from Manzanar. Soon after, 3,000 to 4,000 incarcerees gathered and marched to the administration area, protesting Ueno's arrest. After Ueno's supporters negotiated with the camp administration, he was returned to the Manzanar jail. A crowd of several hundred returned to protest, and when the people surged forward, military police threw tear gas to disperse them. As people ran to avoid the tear gas, some in the crowd pushed a driverless truck toward the jail. At that moment, the military police fired into the crowd, killing a 17-year–old boy instantly. A 21-year–old man who was shot in the abdomen died days later. Nine other prisoners were wounded, and a military police corporal was wounded by a ricocheting bullet.
Closure.
On November 21, 1945, the WRA closed Manzanar, the sixth camp to be closed. Although the incarcerees had been brought to the Owens Valley by the United States Government, they had to leave the camp and travel to their next destinations on their own. The WRA gave each person $25 ($ today), one-way train or bus fare, and meals to those who had less than $600 ($ today). While many left the camp voluntarily, a significant number refused to leave because they had no place to go after having lost everything when they were forcibly uprooted and removed from their homes. As such, they had to be forcibly removed once again, this time from Manzanar. Indeed, those who refused to leave were generally removed from their barracks, sometimes by force, even if they had no place to go.
146 incarcerees died at Manzanar. Fifteen incarcerees were buried there, but only five graves remain, as most were later reburied elsewhere by their families.
The Manzanar cemetery site is marked by a monument that was built by incarceree stonemason Ryozo Kado in 1943. An inscription in Japanese on the front of the monument reads, 慰靈塔 (Soul Consoling Tower). The inscription on the back reads "Erected by the Manzanar Japanese" on the left, and "August 1943" on the right. Today, the monument is often draped in strings of "origami", and sometimes survivors and other visitors leave offerings of personal items as mementos. The National Park Service periodically collects and catalogues such items.
After the camp was closed, the site eventually returned to its original state. Within a couple of years, all the structures had been removed, with the exception of the two sentry posts at the entrance, the cemetery monument, and the former Manzanar High School auditorium, which was purchased by the County of Inyo. The County leased the auditorium to the Independence Veterans of Foreign Wars, who used it as a meeting facility and community theater until 1951. After that, the building was used as a maintenance facility by the Inyo County Road Department.
As of 2007, the site also retains numerous building foundations, portions of the water and sewer systems, the outline of the road grid, remains of the landscaping constructed by incarcerees, and much more. Despite four years of use by the incarcerees, the site also retains evidence of the ranches and of the town of Manzanar, as well as artifacts from the days of the Owens Valley Paiute settlement.
Notable incarcerees.
'Embrey took her pain and anger from the unjust internment and turned it into a life dedicated to making certain that would never happen again,' said Rose Ochi, legal counsel for the Manzanar Committee, after Embrey died on May 15, 2006. 'She was just tireless and as a teacher she was making certain that our history books did talk about the tragic episode.'
'The reason [that the Manzanar National Historic Site has] been accepted by Japanese Americans, local Owens Valley residents and general visitors is in large part because of [Embrey's] knowledge and her personal experience,' said Alisa Lynch, Chief of Interpretation, Manzanar National Historic Site. 'She had the insight to help us be able to be truthful, to be accurate. She was a historian and an internee, she could wear many different hats.'
...(The class action lawsuit) remained active until after Congress had passed the redress legislation. While it remained alive, it played a significant part in publicizing the issues. The NCJAR lawsuit demanded $220,000 for each individual whose liberties had been denied. This was more than 20 times greater than the $20,000 per surviving incarcerated person that the redress bills proposed, allowing proponents to portray the legislative solution as a moderate alternative.
"Internment was immoral," Lazo told the "Los Angeles Times". "It was wrong, and I couldn't accept it." "These people hadn't done anything that I hadn't done except to go to Japanese language school."
In 1944, Lazo was elected president of his class at Manzanar High School. He remained at Manzanar until August of that year, when he was inducted into the US Army. He served as a Staff Sergeant in the South Pacific until 1946, helping liberate the Philippines. Lazo was awarded the Bronze Star for heroism in combat. After the war, he was a strong supporter of redress and reparations for Japanese Americans incarcerated during the war. The film, "", from Visual Communications, documents his life story, particularly his stand against the incarceration.
Other notable Manzanar incarcerees are: Koji Ariyoshi, Jeanne Wakatsuki Houston, Isao Kikuchi, Tura Satana, Gordon H. Sato, Tak Shindo, Larry Shinoda, Iwao Takamoto, Takuji Yamashita and Wendy Yoshimura.
Preservation and remembrance.
Manzanar Pilgrimage.
On December 21, 1969, about 150 people departed Los Angeles by car and bus, headed for Manzanar. It was the "first" annual Manzanar Pilgrimage. But as it turned out, two ministers, Reverends Sentoku Mayeda and Shoichi Wakahiro, had been making annual pilgrimages to Manzanar since the camp closed in 1945.
The non-profit Manzanar Committee, formerly led by Sue Kunitomi Embrey, has sponsored the Pilgrimage since 1969. The event is held annually on the last Saturday of April with hundreds of visitors of all ages and backgrounds, including some former incarcerees, gathering at the Manzanar cemetery to remember the incarceration. The hope is that participants can learn about it and help ensure that what is generally accepted to be a tragic chapter in American History is neither forgotten nor repeated. The program traditionally consists of speakers, cultural performances, an interfaith service to memorialize those who died at Manzanar, and "Ondo" dancing.
"My mother was a very staunch Buddhist and she would always say, 'Those poor people that are buried over there at Manzanar in the hot sun—they must be so dry. Be sure to take some water [as offerings],'" said Embrey. "She always thought it was important to go back and remember the people who had died."
In 1997, the Manzanar At Dusk program became a part of the Pilgrimage. The program attracts local area residents, as well as descendants of Manzanar's ranch days and the town of Manzanar. Through small group discussions, the event gives participants the opportunity to hear directly about the experiences of former incarcerees first-hand, to share their experiences and feelings about what they learned, and talk about the relevance of what happened at Manzanar to their own lives.
Since the September 11 attacks, American Muslims have participated in the Pilgrimage to promote and increase awareness of civil rights protections in the wake of widespread suspicions harbored against them post-9/11.
California Historical Landmark and Los Angeles Historic-Cultural Monument.
The Manzanar Committee's efforts resulted in the State of California naming Manzanar as California Historical Landmark #850 in 1972, with an historical marker being placed at the sentry post on April 14, 1973.
Manzanar, which had been historically owned by the City of Los Angeles, was registered as a Los Angeles Historic-Cultural Monument in 1976.
National Historic Landmark and National Historic Site.
The Manzanar Committee also spearheaded efforts for Manzanar to be listed in the National Register of Historic Places, and in February 1985, Manzanar was designated a National Historic Landmark. Embrey and the Committee also led the effort to have Manzanar designated a National Historic Site, and on March 3, 1992, President George H. W. Bush signed House Resolution 543 into law (; 106 Stat. ). This act of Congress established the Manzanar National Historic Site "to provide for the protection and interpretation of the historical, cultural, and natural resources associated with the relocation of Japanese Americans during World War II." Five years later, the National Park Service acquired 814 acre of land at Manzanar from the City of Los Angeles.
The site features an Interpretive Center housed in the historically restored Manzanar High School Auditorium, which has a permanent exhibit that tells the stories of the incarcerees at Manzanar, the Owens Valley Paiute, the ranchers, the town of Manzanar, and water in the Owens Valley.
'...Stories like this need to be told, and too many of us have died without telling our stories,' Embrey said during her remarks at the Grand Opening ceremonies for the Manzanar National Historic Site Interpretive Center on April 24, 2004. 'The Interpretive Center is important because it needs to show to the world that America is strong as it makes amends for the wrongs it has committed, and that we will always remember Manzanar because of that.'
The site, which has seen 1,074,471 people visit from 2000 through 2014, features restored sentry posts at the camp entrance, a replica of a camp guard tower built in 2005, a self-guided tour road, and informational markers. Staff offer guided tours and other educational programs, including a Junior Ranger educational program for children between four and fifteen years of age.
The National Park Service is reconstructing one of the 36 residential blocks as a demonstration block. One barrack appears as it would have when Japanese Americans first arrived at Manzanar in 1942, while another has been reconstructed to represent barracks life in 1945. Exhibits in these barracks opened on April 16, 2015.A restored World War II mess hall, moved to the site from Bishop Airport in 2002, was opened to visitors in late 2010.
In late 2008, historically appropriate vegetation was planted near the Interpretive Center. The Manzanar National Historic Site also unveiled its virtual museum on May 17, 2010 and continues to collect oral histories of former incarcerees and others from all periods of Manzanar's history.
Opposition to the creation of the Manzanar National Historic Site.
After Congress named Manzanar a National Historic Site and gave the National Park Service the job of restoring the site in 1992, protests against its creation emerged. Letters flooded the National Park Service, demanding that Manzanar be portrayed as a guest housing center for the Japanese Americans. William Hastings, of Bishop, California, wrote to the National Park Service, saying that the portrayal of Manzanar as a concentration camp amounts to "treason." Protesters threatened to start dismissal campaigns against Bill Michael, a member of the Manzanar Advisory Commission who was the Director of the in Independence, California, and Superintendent Ross Hopkins, the National Park Service employee assigned to the site.They also threatened to destroy any buildings erected or restored at Manzanar. Further, Lillian Baker, and others in California, objected to the words, "concentration camp" on the California State historical marker, which has been hacked and stained, with the first "C" of "concentration camp" having been ground off. Further, a man, who described himself as a World War II veteran, called Hopkins to say that he had driven 200 miles to urinate on the marker.
In popular culture.
A made-for-television movie, "Farewell to Manzanar", directed by John Korty, aired on March 11, 1976, on NBC. It was based on the 1973 memoir of the same name, written by Jeanne Wakatsuki Houston, who was incarcerated at Manzanar as a child, and her husband James D. Houston. The book and the movie tell the story of the Wakatsuki family and their experiences behind the barbed wire through young Jeanne's eyes. On October 7, 2011, the Japanese American National Museum (JANM) announced that they had negotiated the rights to the movie, and that they would make it available for purchase on DVD.
"Come See The Paradise" was a feature film about how forced relocation and imprisonment at Manzanar affected a Japanese American family from Los Angeles and a European American union organizer. The film, released in 1990, starred Dennis Quaid and Tamlyn Tomita, and was written and directed by Alan Parker.
Folk/country musician Tom Russell wrote "Manzanar", a song about the Japanese American internment, that was released on his album "Box of Visions" (1993). Laurie Lewis covered the song on her album "Seeing Things" (1998), adding the Japanese string instrument, the koto, to her performance.
The 1994 award-winning novel, "Snow Falling on Cedars" by David Guterson, contains many scenes and details relating to Japanese Americans from the Puget Sound, Washington, area and their incarceration experiences at Manzanar. The 2000 film based on the book also details that connection.
The Asian American jazz fusion band Hiroshima has a song entitled "Manzanar" on its album "The Bridge" (2003). It is an instrumental song inspired by Manzanar and the Japanese American incarceration. Also, its song "Living In America", on its album titled "East" (1990), contains the phrase "I still remember Manzanar."
Fort Minor's song "Kenji", from the album "The Rising Tied" (2005), tells the true story of Mike Shinoda's family and their experiences before, during, and after World War II, including their imprisonment at Manzanar.
Channel 3's song titled "Manzanar" is about the incarceration.
A 2007 episode of the CBS television crime drama "Cold Case", titled "Family 8108", dealt with the 1945 murder of a Japanese American man in Philadelphia, Pennsylvania after he and his family were released from Manzanar. The episode originally aired on December 9, 2007.

</doc>
<doc id="46875" url="http://en.wikipedia.org/wiki?curid=46875" title="Puff pastry">
Puff pastry

In baking, a puff pastry, also referred to as "pâte feuilletée," is a light, flaky, mechanically leavened pastry containing several layers of fat which is in solid state at 20 °C (68 °F). In raw form, puff pastry is a laminated dough composed of two elements: a "dough packet", the "détrempe" and a "butter packet" or other solid fat, the "beurrage." In classic puff pastry, an envelope is formed by placing the beurrage inside the détrempe. In "inverse puff" pastry, an envelope is formed by placing the détrempe inside the beurrage. The resulting "paton" is repeatedly folded and rolled out.
The gaps that form between the layers left by the fat melting are pushed (leavened) by the water turning into steam during the baking process. Piercing the dough will prevent excessive puffing, and crimping along the sides will prevent the layers from flaking all of the way to the edges.
History.
Puff pastry seems to be a relative of the Middle Eastern phyllo, and is used in a similar manner to create layered pastries. While traditionally ascribed to the French painter and cook Claude Gelée who lived in the 17th century (the story goes that Gelée was making a type of very buttery bread for his sick father, and the process of rolling the butter into the bread dough created a croissant-like finished product), references appear before the 17th century, indicating a history that came originally through Muslim Spain and was converted from thin sheets of dough spread with olive oil to laminated dough with layers of butter, perhaps in Italy or Germany.
Production.
The production of puff pastry dough can be time-consuming, because it must be kept at a temperature of approximately 16 °C (60 °F) to keep shortening from becoming runny, and must rest in between folds to allow gluten strands time to link up and thus retain layering.
The number of layers in puff pastry is calculated with the equation:
where formula_2 is the number of finished layers, formula_3 the number of folds, and formula_4 the number of times the dough has been folded. For example, twice-folding (i.e. in three) for four times gives formula_5 layers. Chef Julia Child recommends 73 layers for regular "pâte feuilletée" and 729 (i.e. 36) layers for "pâte feuilletée fine" (in Volume II of her "Mastering the Art of French Cooking" textbook).
Commercially made puff pastry is available in grocery stores. Common types of fat used include butter, vegetable shortenings, and lard. Butter is the most common type used because it provides a richer taste and superior mouthfeel. Since shortenings and lard have a higher melting point, puff pastry made with either will rise more than pastry made with butter if made correctly; however it will often have a waxy mouthfeel and a blander flavor. Specialized margarine formulated for high plasticity (the ability to spread very thin without breaking apart) is used for industrial production of puff pastry. 
Puff pastry is not the same as phyllo (filo) pastry, although puff pastry can be substituted for phyllo in some applications. Phyllo dough is made with flour, water, and fat and is stretched to size rather than rolled. Usually when using phyllo dough, a small amount of oil or melted fat (usually butter) is brushed on one layer of phyllo dough and is topped with another layer. This process can be repeated as many times as desired. When it bakes, it becomes crispy but, since it contains somewhat less water, does not expand to the same degree as puff pastry does.
Nor is puff pastry the same as Austrian strudel dough, or Strudelteig. Strudel dough is more like the phyllo described above.
Variants.
Puff pastry can also be leavened with baker's yeast to create croissants or Danish pastry or Spanish/Portuguese milhoja and empanadilla, though such doughs are not universally known as puff pastries.
In addition, since the process of making puff pastry is generally somewhat laborious and quite time-intensive, faster recipes (known as "blitz", "rough puff", or "Flaky pastry") are fairly common. Many of these recipes combine the butter into the "détrempe" rather than adding it in the folding process and are thus similar to a folded short crust.

</doc>
<doc id="46876" url="http://en.wikipedia.org/wiki?curid=46876" title="Mince pie">
Mince pie

A mince pie is a small British fruit-based mincemeat sweet pie traditionally served during the Christmas season. Its ingredients are traceable to the 13th century, when returning European crusaders brought with them Middle Eastern recipes containing meats, fruits and spices.
The early mince pie was known by several names, including mutton pie, shrid pie and Christmas pie. Typically its ingredients were a mixture of minced meat, suet, a range of fruits, and spices such as cinnamon, cloves and nutmeg. Served around Christmas, the savoury Christmas pie (as it became known) was associated with supposed Catholic "idolatry" and during the English Civil War was frowned on by the Puritan authorities. Nevertheless, the tradition of eating Christmas pie in December continued through to the Victorian era, although by then its recipe had become sweeter and its size reduced markedly from the large oblong shape once observed. Today the mince pie remains a popular seasonal treat enjoyed by many across the United Kingdom.
History.
The ingredients for the modern mince pie can be traced to the return of European crusaders from the Holy Land. Middle Eastern methods of cooking, which sometimes combined meats, fruits and spices, were popular at the time. Pies were created from such mixtures of sweet and savoury foods; in Tudor England, shrid pies (as they were known then) were formed from shredded meat, suet and dried fruit. The addition of spices such as cinnamon, cloves and nutmeg was, according to the English antiquary John Timbs, "in token of the offerings of the Eastern Magi." Several authors, including Timbs, viewed the pie as being derived from an old Roman custom practised during Saturnalia, where Roman fathers in the Vatican were presented with sweetmeats. Early pies were much larger than those consumed today, and oblong shaped; the jurist John Selden presumed that "the coffin of our "Christmas"-Pies, in shape long, is in Imitation of the Cratch [Jesus's crib]", although writer T. F. Thistleton-Dyer thought Selden's explanation unlikely, as "in old English cookery books the crust of a pie is generally called 'the coffin.'"
The modern mince pie's precursor was known by several names. The antiquary John Brand claimed that in Elizabethan and Jacobean-era England they were known as minched pies, but other names include mutton pie, and starting in the following century, Christmas pie. Gervase Markham's 1615 recipe recommends taking "a leg of mutton", and cutting "the best of the flesh from the bone", before adding mutton suet, pepper, salt, cloves, mace, currants, raisins, prunes, dates and orange peel. He also suggested that beef or veal might be used in place of mutton. In the north of England, goose was used in the pie's filling, but more generally neat's tongue was also used; a North American filling recipe published in 1854 includes chopped neat's tongue, beef suet, blood raisins, currants, mace, cloves, nutmeg, brown sugar, apples, lemons, brandy and orange peel. During the English Civil War, along with the censure of other Catholic customs, they were banned: "Nay, the poor rosemary and bays, and "Christmas pie", is made an abomination." Puritans were opposed to the Christmas pie, on account of its connection with Catholicism. In his "History of the Rebellion", Marchamont Needham wrote "All Plums the Prophets Sons defy, And Spice-broths are too hot; Treason's in a "December"-Pye, And Death within the Pot." Some considered them unfit to occupy the plate of a clergyman, causing Philo-Clericus to comment:
In his essay "The Life of Samuel Butler", Samuel Johnson wrote of "an old Puritan, who was alive in my childhood ... would have none of his superstitious meats and drinks." Another essay, published in the December 1733 issue of "The Gentleman's Magazine", explained the popularity of "Christmas Pye" as perhaps "owing to the Barrenness of the Season, and the Scarcity of Fruit and Milk, to make Tarts, Custards, and other Desserts", but also possibly bearing "a religious kind of Relation to the Festivity from which it takes its Name." The author also mentions the Quakers' objection to the treat, "who distinguish their Feasts by an heretical Sort of Pudding, known by their Names, and inveigh against Christmas Pye, as an Invention of the Scarlet Whore of "Babylon", an Hodge-Podge of Superstition, Popery, the Devil and all his Works." Nevertheless, the Christmas pie remained a popular treat at Christmas, although smaller and sweeter, and lacking in post-Reformation England any sign of supposed Catholic idolatry. People began to prepare the fruit and spice filling months before it was required, storing it in jars, and as Great Britain entered the Victorian age, the addition of meat had, for many, become an afterthought (although the use of suet remains). Its taste then was broadly similar to that experienced today, although some 20th-century writers continued to advocate the inclusion of meat.
Although the modern recipe is no longer the same list of 13 ingredients once used (representative of Christ and his 12 Apostles according to author Margaret Baker), and lacks the religious meaning contained therein, the mince pie remains a popular Christmas treat. Bakers Greggs reported sales of 7.5 million mince pies during Christmas 2011. The popular claim that the consumption of mince pies on Christmas Day is illegal is in fact an urban myth.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="46877" url="http://en.wikipedia.org/wiki?curid=46877" title="Shortcrust pastry">
Shortcrust pastry

Shortcrust pastry is a type of pastry often used for the base of a tart, quiche or pie. It does not puff up during baking because it usually contains no leavening agent. It is possible to make shortcrust pastry with self-raising flour, however. Shortcrust pastry can be used to make both sweet and savory pies such as apple pie, quiche, lemon meringue or chicken pie. Many shortcrust pastries are prepared using vegetable shortening, a fat food product that is solid at room temperature, the composition of which tends to create crumbly, shortcrust-style pastries and pastry crusts.
Proportions.
It is based on a "half-fat-to-flour" ratio (by weight). Fat (lard, shortening, butter or full-fat margarine) is rubbed into plain flour to create a loose mixture that is then bound using a small amount of ice water, rolled out, then shaped and placed to create the top or bottom of a flan or pie. Ideally, equal amounts of butter and lard are used to make the pastry, ensuring that the ratio of the two fat products is half that of the flour. The butter is employed to give the pastry a rich flavor, whilst the lard ensures optimum texture.
Techniques.
In both sweetcrust and shortcrust pastry, care must be taken to ensure that fat and flour are blended thoroughly before liquid is added. This ensures that the flour granules are adequately coated with fat and are less likely to develop gluten and may be achieved with the use of a specialized kitchen utensil called a pastry blender, or through various alternatives, like a pair of table knives held in one hand.
Overworking the dough is also a hazard. Overworking elongates the gluten strands, creating a product that is tough, rather than light and crumbly or flaky.

</doc>
<doc id="46878" url="http://en.wikipedia.org/wiki?curid=46878" title="La Caletta">
La Caletta

La Caletta is a small town, a harbour and a tourist destination in Sardinia, Italy. Caletta means a small bay or little harbour.
The town is located approximately 50 km south of Olbia, in the administrative territory of Siniscola (province of Nuoro), on the Tyrrhenian coast of the island.
An ancient village of fishermen, its small gulf has been transformed in the 1970s into a touristic harbour, and recently renewed and enlarged. The town (est. 1,000 inhabitants, that become more than 10,000 in Summer) is today deeply dependent on tourism and borders, at its northern side, with San Giovanni, the coastal fraction of Posada.
La Caletta is in front of a well known beach (appr. 10 km of pure white sand) that ends at the small town of Santa Lucia.
Local population requires that La Caletta can have in its port a ferry line for "the Continent" (Italian mainland), and some experiments were practiced a few years ago, that confirmed the potential success of such an eventual initiative, but (also due to the particular administrative competence of an external organ and being part of the port under the authority of the bordering territory of Posada) administrative problems and local rivalries actually stop any further evolution in this sense.

</doc>
<doc id="46880" url="http://en.wikipedia.org/wiki?curid=46880" title="Japanese American service in World War II">
Japanese American service in World War II

During the early years of World War II, Japanese Americans were forcibly relocated from their homes in the Pacific Coast states because military leaders and public opinion combined to fan unproven fears of sabotage. As the war progressed, many of the young "Nisei", Japanese immigrants' children who were born with American citizenship, volunteered or were drafted to serve in the United States military. Japanese Americans served in all the branches of the United States Armed Forces, including the United States Merchant Marines.
Servicemen in the U.S. Army.
The majority of Japanese Americans serving in the American Armed Forces during World War II enlisted in the army.
100th Infantry Battalion.
The 100th Infantry Battalion was engaged in heavy action during the war taking part in multiple campaigns. The 100th was made up of "Nisei" who were originally members of the Hawaii National Guard. Sent to the mainland as the Hawaii Provisional Infantry Battalion on June 5, 1942, the 1,432 original members of the 100th were stationed first at Camp McCoy and later at Camp Shelby for combat training. Meanwhile, an earlier decision to demote "Nisei" soldiers to 4-C class was reversed and the Army in 1943 issued a call for Japanese American volunteers. Most of the initial recruits came from Hawaii, as those on the mainland were reluctant to volunteer while they and their families remained in camp. The 2,686 accepted Hawaiians (out of 10,000 volunteers) and about 1,000 mainlanders were sent to Camp Shelby, where they joined the 100th. The Battalion shipped out in August 1943, landing in North Africa before fighting in Italy, eventually participating in the liberation of Rome. Their exemplary military record, and the patriotic activities of the Varsity Victory Volunteers, paved the way for the creation of the 442nd Regimental Combat Team.
442nd Regimental Combat Team.
The 442nd Regimental Combat Team was an all-"Nisei" U.S. Army regiment which served in Europe during World War II. Japanese Americans already in training at the start of the war had been removed from active duty shortly after Pearl Harbor, and the Army stopped accepting new "Nisei" recruits in early 1942. However, Japanese American leaders like Mike Masaoka and War Department officials like John J. McCloy soon began to push the Roosevelt administration to allow "Nisei" to serve in combat. A military board was convened in June 1942 to address the issue, but their final report opposed forming a "Nisei" unit, citing "the universal distrust in which they [Japanese Americans] are held." Despite resistance from military and War Relocation Authority leaders, the President eventually sided with the War Department, and on February 1, 1943, Roosevelt announced the creation of a segregated battalion composed of "Nisei" soldiers and commanded by white officers. While the first group of volunteers fought in Europe as part of the 100th Infantry Battalion, additional recruits and draftees began combat training at Camp Shelby. The 1st Battalion of the 442nd soon after began sending replacement troops to join the 100th, which suffered an extremely high casualty rate, and the 2nd and 3rd Battalions shipped out on May 1, 1944, joining the 100th in Italy the next month. These men arrived in Europe after the 100th Infantry Battalion had already established its reputation as a fighting unit, and in time, the 442nd became, for its size and length of service, the most decorated unit in U.S. military history.
522nd Field Artillery Battalion.
The all-"Nisei" 522nd Field Artillery Battalion was organized as part of the 442nd Regimental Combat Team; but towards the end of the war, the 522nd became a roving battalion, shifting to whatever command most needed the unit. The 522nd had the distinction of liberating survivors of the Dachau concentration camp system, from the Nazis on April 29, 1945. "Nisei" scouts west of Munich near the small Bavarian town of Lager Lechfeld encountered some barracks encircled by barbed wire. Technician Fourth Grade Ichiro Imamura described it in his diary:
Holocaust historians have clarified the "Nisei" 522nd liberated about 3,000 prisoners at "Kaufering IV Hurlach." Hurlach was one of 169 subordinate slave labor camps of Dachau. Dachau, like Auschwitz, Buchenwald, Mauthausen and Ravensbrück, was surrounded by hundreds of sub-camps.
Pierre Moulin in his recent book 'Dachau, Holocaust and US Samurais' writes that the first Nisei arrived at Dachau's gate not on April 29, the date of the liberation of the camp, but on April 28, 1945.
Servicemen in the Army Air Forces.
Japanese Americans were generally forbidden to fight a combat role in the Pacific theatre; although no such limitations were placed on Americans of German or Italian ancestry who fought against the Axis Powers. Up to this point, the United States government has only been able to find records of five Japanese Americans who were members of the Army Air Forces during World War II, one of them being Kenje Ogata. There was at least one "Nisei", U.S. Army Air Forces Technical Sergeant Ben Kuroki, who participated initially in 35 missions as a dorsal turret gunner over Europe, followed by 28 bombing missions over mainland Japan and other locations in the Pacific Theater.
Japanese-American Service in the Pacific Theater.
Military Intelligence Service.
Approximately 6,000 Japanese Americans served in the Military Intelligence Service (MIS). The first class received their training at the Presidio in San Francisco, but in June 1942 the MIS Language School was moved to Camp Savage, Minnesota, which offered larger facilities, removed the complications of training Japanese American students in an area they were technically prohibited from entering, and had less anti-Japanese prejudice. In August 1944, the language school was moved again to Fort Snelling. Most of the MIS Language School graduates were attached to the Allied Translator and Interpreter Section (ATIS) as linguists and in other non-combatant roles, interpreting captured enemy documents and interrogating prisoners of war. (At the end of the war, MIS linguists had translated 18,000 enemy documents, created 16,000 propaganda leaflets and interrogated over 10,000 Japanese POWs.) However, MIS servicemen were present at every major battle against Japanese forces, and those who served in combat faced extremely dangerous and difficult conditions, sometimes coming under friendly fire from U.S. soldiers unable to distinguish them from the Japanese and often encountering former friends on the battlefield.
Japanese American MIS linguists translated Japanese documents known as the "Z Plan", which contained Japan's counterattack strategy in the Central Pacific. This information led to Allied victories at the Battle of the Philippine Sea, in which the Japanese lost most of their aircraft carrier planes, and the Battle of Leyte Gulf. An MIS radio operator intercepted a message describing Admiral Isoroku Yamamoto's flight plans, which led to P-38 Lightning fighter planes shooting down his plane over the Solomon Islands.
Merrill's Marauders.
Fourteen Nisei were assigned to the 5307th Composite Unit (Provisional), AKA Merrill's Marauders, which operated in Burma during the war. The Nisei assigned to Merrill's Marauders were called the "Marauder Samurai". Amongst those who were assigned to Merrill's Marauders was Roy Matsumoto.
Dixie Mission.
Five Nisei were members of the US Army Observer Group, nicknamed the Dixie Mission, which operated in the Chinese Communist HQ in Yan'an from July 22, 1944 to July 24, 1946. Sho Nomura and George Itsuo Nakamura accompanied the Dixie Mission when they arrived in Yan'an on July 22, 1944. They were followed by Koji Ariyoshi in Fall of 1944, and Jack Togo Ishii and Toshi Uesato in August 1945. All five of them attended the MIS Language School. Nomura, Nakamura, Ishii, and Uesato were assigned to collect military intelligence through interrogation of Japanese prisoners of war. While stationed in Yan'an, the Nisei met Mao Zedong, Zhou Enlai, and other communist leaders, including Sanzo Nosaka (AKA Susumu Okano) head of the Japan Communist Party.
Office of Strategic Services.
The Office of Strategic Services (OSS) recruited, and sent Nisei to the China-Burma-India Theater. Lt. Junichi Buto, Tom Baba, Dick Hamada, Fumio Kido, Shoichi Kurahashi and Calvin Tottori, and Ralph Yempuku were assigned to Detachment 101 guerrilla operations in Burma. After Detachment 101 disbanded on July 12, 1945, Lt. Buto, Hamada, Kido and Yempuku received assignments to Detachment 202 in Kunming.
Women's Army Corps.
Like their male counterparts, "Nisei" women were at first prohibited from serving in the U.S. military; this changed in November 1943, and 142 young women volunteered to join the WAC. Because their number was relatively small, the "Nisei" WACs were not restricted to a segregated corps, but instead were spread out and served alongside other ethnic groups. The idea of female auxiliary service was still new at this time (the Women's Army Corps was only nine months old when it opened its ranks to "Nisei" volunteers), and these women were most often assigned to clerical duties or other "women's work." Additionally, WACs were often portrayed in media and propaganda as highly sexualized and were encouraged by male supervisors to play into this role. The "Nisei" WACs faced another difficulty in that they were expected to translate Japanese military documents; even those who were fluent in Japanese struggled to understand the military language, and eventually some were sent to the Military Intelligence Language School for training.
Other Duties.
Personal Justice Denied: Report of the Commission on Wartime Relocation and Internment of Civilians stated: "There were Japanese American medics, mechanics and clerks in the Quartermaster Corps and Nisei women in the WACs. "Nisei" and "Issei" served as language instructors, employees in the Army Map Service, and behind the scenes in the Office of Strategic Services (OSS) and Office of War Information (OWI). In the latter groups were primarily younger "Issei" who had fled Japan after World War I to avoid political persecution. At OWI and OSS, some made broadcasts to Japan, while others wrote propaganda leaflets urging Japanese troops to surrender or pamphlets dropped over Japan to weaken civilian morale." 
Prisoners of War.
Only two Japanese-Americans were imprisoned by the Japanese during the war. One of them was Frank Fujita. Fujita was stationed in Java in the Netherlands East Indies when he was captured by the Japanese on March 1942. He was imprisoned in Changi Prison in Singapore, and then Omori Prison Island in Tokyo Bay. Fujita was liberated after the war.
Recognition.
The nation's highest award for combat valor, the Medal of Honor, was conferred upon only one "Nisei" during the war. Twenty-one members of the 100th Infantry Battalion/442nd Regimental Combat Team received Distinguished Service Crosses during or immediately after their World War II service, but in the 1990s, after a study revealed that racial discrimination had caused them to be overlooked, their awards were upgraded to Medals of Honor. On October 5, 2010, the Congressional Gold Medal was awarded to the 442nd Regimental Combat Team and the 100th Infantry Battalion, as well as the 6,000 Japanese Americans who served in the Military Intelligence Service during the war.

</doc>
<doc id="46883" url="http://en.wikipedia.org/wiki?curid=46883" title="Babylonia">
Babylonia

 
Babylonia () was an ancient Akkadian-speaking Semitic state and cultural region based in central-southern Mesopotamia (present-day Iraq). A small Amorite-ruled state emerged in 1894 BC, which contained at this time the minor city of Babylon. Babylon greatly expanded during the reign of Hammurabi in the first half of the 18th century BC, becoming a major capital city. During the reign of Hammurabi and afterwards, Babylonia was called Mât Akkadî "the country of Akkad" in Akkadian. It was often involved in rivalry with its older fellow Akkadian state of Assyria in northern Mesopotamia. Babylonia briefly became the major power in the region after Hammurabi (fl. c. 1792 – 1752 BC middle chronology, or c. 1696 – 1654 BC, short chronology) created a short-lived empire, succeeding the earlier Akkadian Empire, Neo-Sumerian Empire, and Old Assyrian Empire; however, the Babylonian empire rapidly fell apart after the death of Hammurabi.
The Babylonian state retained the written Semitic Akkadian language for official use (the language of its native populace), despite its Amorite founders and Kassite successors not being native Akkadians, and speaking a Northwest Semitic Canaanite language and a Language Isolate respectively. It retained the Sumerian language for religious use (as did Assyria), but by the time Babylon was founded this was no longer a spoken language, having been wholly subsumed by Akkadian. The earlier Akkadian and Sumerian traditions played a major role in Babylonian (and Assyrian) culture, and the region would remain an important cultural center, even under protracted periods of outside rule.
The earliest mention of the city of Babylon can be found in a tablet from the reign of Sargon of Akkad (2334–2279 BC), dating back to the 23rd century BC. Babylon was merely a religious and cultural centre at this point and neither an independent state nor a large city; like the rest of Mesopotamia, it was subject to the Akkadian Empire which united all the Akkadian and Sumerian speakers under one rule. After the collapse of the Akkadian empire, the south Mesopotamian region was dominated by the Gutians for a few decades before the rise of the Neo-Sumerian Empire (third dynasty of Ur), which, apart from northern Assyria, encompassed the whole of Mesopotamia, including the city of Babylon.
Periods.
Pre-Babylonian Sumero-Akkadian period in Mesopotamia.
Mesopotamia had already enjoyed a long history prior to the emergence of Babylon. 
During the third millennium BC, there had developed an intimate cultural symbiosis between the Sumerians and the Akkadians, which included widespread bilingualism. The influence of Sumerian on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the third millennium as a "sprachbund".
Akkadian gradually replaced Sumerian as the spoken language of Mesopotamia somewhere around the turn of the third and the second millennium BC (the precise timeframe being a matter of debate), but Sumerian continued to be used as a sacred, ceremonial, literary and scientific language in Mesopotamia as late as the 1st century AD.
From c. 3500 BC until the rise of the Akkadian Empire in the 24th century BC, Mesopotamia had been dominated by largely Sumerian city states, such as Ur, Lagash, Uruk, Kish, Isin, Larsa, Adab, Eridu, Nuzi, Awan, Hamazi, Akshak and Umma, although Semitic Akkadian names began to appear on the king lists of some of these states (such as Eshnunna and Assyria) between the 29th and 25th centuries BC. Traditionally, the major religious center of all Mesopotamia was the city of Nippur, and it would remain so until replaced by Babylon during the reign of Hammurabi in the mid 18th century BC.
The Akkadian Empire (2334–2154 BC) saw the Akkadian Semites and Sumerians of Mesopotamia unite under one rule, and the Akkadians fully attain ascendancy over the Sumerians and indeed come to dominate much of the ancient Near East.
The empire eventually disintegrated due to economic decline, climate change and civil war, followed by attacks by the Gutians from the Zagros Mountains. The Sumerians rose up with the Neo-Sumerian Empire (Third Dynasty of Ur) in the late 22nd century BC, and ejected the Gutians from southern Mesopotamia. They also seem to have gained ascendancy over most of the territory of the Akkadian kings of Assyria in northern Mesopotamia for a time.
Following the collapse of the Sumerian "Ur-III" dynasty at the hands of the Elamites in 2002 BC, the Amorites, a foreign Northwest Semitic people who spoke a Canaanite language, began to migrate into southern Mesopotamia from the northern Levant, gradually gained control over most of southern Mesopotamia, where they formed a series of small kingdoms, while the native Assyrians reasserted their independence in the north. The Sumero-Akkadian states of the south were unable to stem the Amorite advance.
King Ilushuma (ca. 2008–1975 BC) of Assyria in a known inscription describes his exploits to the south as follows: "The freedom of the Akkadians and their children I established. I purified their copper. I established their freedom from the border of the marshes and Ur and Nippur, Awal, and Kish, Der of the goddess Ishtar, as far as the City of (Ashur)." Past scholars originally extrapolated from this text that it means he defeated the invading Amorites to the south, but there is no explicit record of that. More recently, the text has been taken to mean that Asshur supplied the south with copper from Anatolia and "established freedom" from tax duties.
These policies were continued by his successors Erishum I and Ikunum.
However, when Sargon I (1920–1881 BC) succeeded as king in Assyria in 1920 BC he eventually withdrew Assyria from the region, preferring to concentrate on continuing to vigorously expand Assyrian colonies in Asia Minor, and eventually southern Mesopotamia fell to the Amorites. During the first centuries of what is called the "Amorite period", the most powerful city states in the south were Isin, Eshnunna and Larsa, together with Assyria in the north.
First Babylonian Dynasty — Amorite Dynasty 1894–1595 BC.
One of these Canaanite speaking Amorite dynasties founded a small kingdom which included the then still minor town of Babylon circa 1894 BC, which would ultimately take over the others and form the short-lived first Babylonian empire, also called the Old Babylonian Period.
An Amorite chieftain named Sumuabum appropriated a tract of land which included the then relatively small city of Babylon from the neighbouring Amorite ruled Mesopotamian city state of Kazallu, of which it had initially been a territory, turning it into a state in its own right. His reign was concerned with establishing statehood amongst a sea of other minor city states and kingdoms in the region. However Sumuabum appears never to have bothered to give himself the title of "King of Babylon", suggesting that Babylon itself was still only a minor town or city, and not worthy of kingship.
He was followed by Sumu-la-El, Sabium, Apil-Sin, who each ruled in the same vague manner as Sumuabum, with no reference to kingship of Babylon being made in any written records of the time. Sin-muballit was the first of these Amorite rulers to be regarded officially as a king of Babylon, and then only on one single clay tablet. Under these kings, the nation in which Babylon lay remained a small nation which controlled very little territory, and was overshadowed by neighbouring kingdoms that were both older, larger, and more powerful, such as; Isin, Larsa, Assyria and Elam. The Elamites in particular, occupied huge swathes of southern Mesopotamia, and the early Amorite rulers were largely held in vassalage to Elam.
The Empire of Hammurabi
Babylon remained a minor territory for a century after it was founded, until the reign of its sixth Amorite ruler, Hammurabi (1792- 1750 BC, or fl. c. 1728 – 1686 BC (short). He conducted major building work in Babylon, expanding it from a minor town into a great city worthy of kingship. He was a very efficient ruler, establishing a bureaucracy, with taxation and centralized government. Hammurabi freed Babylon from Elamite dominance, and indeed drove them from southern Mesopotamia entirely. He then gradually expanded Babylonian dominance over the whole of southern Mesopotamia, conquering the cities and states of the region, such as; Isin, Larsa, Eshnunna, Kish, Lagash, Nippur, Borsippa, Ur, Uruk, Umma, Adab and Eridu. The conquests of Hammurabi gave the region stability after turbulent times and coalesced the patchwork of states of southern and central Mesopotamia into one single nation, and it is only from the time of Hammurabi that southern Mesopotamia came to be known historically as Babylonia.
The armies of Babylonia under Hammurabi were well-disciplined. He turned eastwards and invaded what was a thousand years later to become Persia (Iran), conquering the "pre Iranic" Elamites, Gutians and Kassites. To the west, the Semitic states of the Levant (modern Syria) including the powerful kingdom of Mari were conquered.
Hammurabi then entered into a protracted war with the Old Assyrian Empire for control of Mesopotamia and the Near East. Assyria had extended control over parts of Asia Minor from the 21st century BC, and from the latter part of the 19th century BC had asserted itself over north east Syria and central Mesopotamia also. After a protracted unresolved struggle over decades with the Assyrian king Ishme-Dagan, Hammurabi forced his successor Mut-Ashkur to pay tribute to Babylon c. 1751 BC, thus giving Babylonia control over Assyria's centuries old Hattian and Hurrian colonies in Asia Minor.
One of the most important works of this "First Dynasty of Babylon", as it was called by the native historians, was the compilation of a code of laws which were both influenced by and improved upon the much earlier written laws of Sumer, Akkad and Assyria. This was made by order of Hammurabi after the expulsion of the Elamites and the settlement of his kingdom. In 1901, a copy of the Code of Hammurabi was discovered on a stele by J. De Morgan and V. Scheil at Susa, where it had later been taken as plunder. That copy is now in the Louvre.
From before 3000 BC until the reign of Hammurabi, the major cultural and religious center of southern Mesopotamia had been the ancient city of Nippur, where the god Enlil was supreme. However, with the rise of Hammurabi, this honour was transferred to Babylon, and the south Mesopotamian god Marduk rose to supremacy in the pantheon of southern Mesopotamia (with the god Ashur remaining the dominant deity in the northern Mesopotamian state of Assyria). The city of Babylon became known as a "holy city" where any legitimate ruler of "southern Mesopotamia" had to be crowned. Hammurabi turned what had previously been a minor administrative town into a major city, increasing its size and population dramatically, and conducting a number of impressive architectural works.
The Babylonians, like their predecessor Sumero-Akkadian states, engaged in regular trade with the Amorite and Canaanite city-states to the west; with Babylonian officials or troops sometimes passing to the Levant and Canaan, with Amorite merchants operating freely throughout Mesopotamia. The Babylonian monarchy's western connections remained strong for quite some time. An Amorite chieftain named Abi-ramu or Abram (possibly the Biblical Abraham) was the father of a witness to a deed dated to the reign of Hammurabi's grandfather; Ammi-Ditana, great-grandson of Hammurabi, still titled himself "king of the land of the Amorites". Ammi-Ditana's father and son also bore Canaanite names: Abi-Eshuh and Ammisaduqa.
Babylonian Decline
However, southern Mesopotamia had no natural, defensible boundaries, making it vulnerable to attack. After the death of Hammurabi, his empire began to disintegrate rapidly. Under his successor Samsu-iluna (1749–1712 BC) the far south of Mesopotamia was lost to a native Akkadian king called Ilum-ma-ili and became the Sealand Dynasty, remaining free of Babylon for the next 272 years.
Both the Babylonians and their Amorite rulers were driven from Assyria to the north by an Assyrian-Akkadian governor named Puzur-Sin c. 1740 BC, who regarded Mut-Ashkur as a foreign Amorite and a former lackey of Babylon. After six years of civil war in Assyria, a native king named Adasi seized power c. 1735 BC, and went on to appropriate former Babylonian and Amorite territory in central Mesopotamia, as did his successor Bel-bani.
Amorite rule survived in a much reduced Babylon, Samshu-iluna's successor Abi-Eshuh made a vain attempt to recapture the Sealand Dynasty for Babylon, but met defeat at the hands of king Damqi-ilishu II. By the end of his reign Babylonia had shrunk to the small and relatively weak nation it had been upon its foundation, although the city itself was far larger than it had been prior to the rise of Hammurabi..
He was followed by Ammi-Ditana and then Ammisaduqa, both of whom were in too weak a position to make any attempt to regain the many territories lost after the death of Hammurabi, contenting themselves with peaceful building projects in Babylon itself.
Samsu-Ditana was to be the last Amorite ruler of Babylon. Early in his reign he came under pressure from the Kassites, a people originating in the mountains of north west Iran. Babylon was then attacked by the Indo-European speaking and Asia Minor based Hittite Empire in 1595 BC. Shamshu-Ditana was overthrown following the "sack of Babylon" by the Hittite king Mursili I. The Hittites did not remain for long, but the destruction wrought by them finally enabled the Kassites to gain control.
The sack of Babylon and ancient Near East chronology.
The date of the sack of Babylon by the Hittite king Mursili I is considered crucial to the various calculations of the early chronology of the ancient Near East, since both a solar and a lunar eclipse are said to have occurred in the month of Sivan that year, according to ancient records.
The fall of Babylon is taken as a fixed point in the discussion of the chronology of the ancient Near East. Suggestions for its precise date vary by as much as 230 years, corresponding to the uncertainty regarding the length of the "Dark Age" of the ensuing Bronze Age collapse, resulting in the shift of the entire Bronze Age chronology of Mesopotamia with regard to the chronology of Ancient Egypt. Possible dates for the sack of Babylon are:
Kassite Dynasty 1595–1155 BC.
The Kassite dynasty was founded by Gandash of Mari. The Kassites, like the Amorite rulers who had preceded them, were not originally native to Mesopotamia. Rather, they had first appeared in the Zagros Mountains of what is today northwestern Iran.
The ethnic affiliation of the Kassites is unclear, though like the Sumerian and Akkadian Mesopotamian peoples and the Amorites, the Kassites were Caucasoid in appearance. However their Kassite language was not Semitic, and is thought to have been either a language isolate or possibly related to the Hurro-Urartian family of Asia Minor, although the evidence for its genetic affiliation is meager due to the scarcity of extant texts. However, several Kassite leaders bore Indo-European names, and they may have had an Indo-European elite similar to the Mitanni elite that ruled over the Hurrians of central and eastern Asia Minor.
The Kassites renamed Babylon "Kar-Duniash", and their rule lasted for 576 years, the longest dynasty in Babylonian history.
This new foreign dominion offers a striking analogy to the roughly contemporary rule of the Semitic Hyksos in ancient Egypt. Most divine attributes ascribed to the Semitic Amorite kings of Babylonia disappeared at this time; the title of God was never given to a Kassite sovereign. However, Babylon continued to be the capital of the kingdom and one of the 'holy' cities of western Asia, where the priests of Mesopotamian Religion were all-powerful, and the only place where the right to inheritance of the short lived old Babylonian empire could be conferred.
Babylonia experienced short periods of power, but in general proved to be relatively weak under the long rule of the Kassites, and spent long periods under Assyrian and Elamite domination and interference.
It is not clear precisely when Kassite rule of Babylon began, but the Indo-European Hittites from Asia Minor did not remain in Babylonia for long after the sacking of the city, and it is likely the Kassites moved in soon afterwards. Agum II took the throne for the Kassites in 1595 BC, and ruled a state that extended from Iran to the middle Euphrates; The new king retained peaceful relations with Assyria, but successfully went to war with the Hittite Empire of Asia Minor, and twenty four years after the Hittites took the sacred statue of Marduk, he recovered it and declared the god equal to the Kassite deity Shuqamuna.
Burnaburiash I succeeded him and drew up a peace treaty with the Assyrian king Puzur-Ashur III, and had a largely uneventful reign, as did his successor Kashtiliash III.
Southern Mesopotamia (The Sealand Dynasty) remained independent of Babylonia and in native Akkadian hands. However Ulamburiash managed to attack it conquered parts of the land from "Ea-gamil", a king with a distinctly Sumerian name, around 1450 BC, whereupon Ea-Gamil fled to Elam. The Sealand Dynasty region remained independent however, and the Kassite king seems to have been unable to finally conquer it. Ulamburiash began making treaties with the Egyptians then ruling in the southern Levant, and Assyria to the north. Karaindash built a bas-relief temple in Uruk and Kurigalzu I (1415–1390 BC) built a new capital named after himself. Both of these kings continued to struggle unsuccessfully against The Sealand Dynasty.
Agum II also campaigned against the Sealand Dynasty, finally wholly conquering the far south of Mesopotamia for Babylon, destroying its capital Dur-Enlil in the process. From there Agum III extended further south still, conquering the "pre-Arab" state of Dilmun (in modern Bahrain).
Karaindash strengthened diplomatic ties with the Assyrian king Ashur-bel-nisheshu and the Egyptian Pharaoh Thutmosis III and protected Babylonian borders with Elam.
Kadašman-Ḫarbe I succeeded Karaindash, and briefly invaded Elam before being eventually ejected by its king Tepti Ahar. He then had to contend with the Suteans, a Semitic people from the western Levant who invaded Babylonia and sacked Uruk. He describes having “annihilated their extensive forces", then constructed fortresses in a mountain region called Ḫiḫi, in the desert to the west (modern Syria) as security outposts, and “he dug wells and settled people on fertile lands, to strengthen the guard”.
Kurigalzu III succeeded the throne, and soon came into conflict with Elam, to the east. When Ḫur-batila, the successor of Tepti Ahar took the throne of Elam, he began raiding the Babylonia, taunting Kurigalzu to do battle with him at Dūr-Šulgi. Kurigalzu launched a campaign which resulted in the abject defeat and capture of Ḫur-batila, who appears in no other inscriptions. He went on to conquer the eastern lands of Susiana and Elam. This took his army to the Elamite capital, the city of Susa, which was sacked. After this a puppet ruler was placed on the Elamite throne. Kurigalzu III maintained friendly relations with Assyria, Egypt and the Hittites throughout his reign. Kadashman-Enlil I (1374-1360 BC) succeeded him, and continued his diplomatic policies.
Burnaburiash II ascended to the throne in 1359 BC, he retained friendly relations with Egypt, but the resurgent Middle Assyrian Empire to the north was now encroaching into northern Babylonia, and as a symbol of peace, the Babylonian king took the daughter of the powerful Assyrian king Ashur-uballit I in marriage. He also maintained friendly relations with Suppiluliuma I, ruler of the Hittite Empire.
He was succeeded by Kara-hardash (who was half Assyrian, and the grandson of the Assyrian king) in 1333 BC, however a usurper named Nazi-Bugash deposed him, enraging Ashur-uballit I, who invaded and sacked Babylon, slew Nazi-Bugash, annexed Babylonian territory for the Middle Assyrian Empire, and installed Kurigalzu II (1345–1324 BC) as his vassal ruler.
Soon after Arik-den-ili succeeded the throne of Assyria in 1327 BC, Kurigalzu III attacked Assyria in an attempt to reassert Babylonian power. After some impressive initial successes he was ultimately defeated, and lost yet more territory to Assyria. Between 1307 BC and 1232 BC his successors, such as Nazi-Maruttash, Kadashman-Turgu, Kadashman-Enlil II, Kudur-Enlil and Shagarakti-Shuriash, allied with the empires of the Hittites and the Mitanni, (who were both also losing swathes of territory to the Assyrians). in a failed attempt to stop Assyrian expansion, which continued unchecked.
Kashtiliash IV's (1242–1235 BC) reign ended catastrophically as the Assyrian king Tukulti-Ninurta I routed his armies, sacked and burned Babylon and set himself up as king, ironically becoming the first "native" Mesopotamian to rule the state, its previous rulers having all been non Mesopotamian Amorites and Kassites. Kashtiliash himself was taken to Ashur as a prisoner of war.
An Assyrian governor/king named Enlil-nadin-shumi was placed on the throne to rule as viceroy to Tukulti-Ninurta I, and Kadashman-Harbe II and Adad-shuma-iddina succeeded as Assyrian governor/kings, subject to Tukulti-Ninurta I until 1216 BC.
Babylon did not begin to recover until late in the reign of Adad-shuma-usur (1216–1189 BC), as he remained a vassal of Assyria until 1193 BC. However, he was able to prevent the Assyrian king Enlil-kudurri-usur from retaking Babylonia, which, apart from its northern reaches, had mostly shrugged off Assyrian domination during a period of civil war in Assyria, in the years after the death of Tukulti-Ninurta.
Meli-Shipak II (1188–1172 BC) seems to have had a peaceful reign. Despite not being able to regain northern Babylonia from Assyria, no further territory was lost, Elam did not threaten, and the Bronze Age Collapse now affecting the Levant, Canaan, Egypt, The Caucasus, Asia Minor, Mediterranean and Balkans seemed to have little impact on Babylonia (or indeed Assyria).
War resumed under subsequent kings such as Marduk-apla-iddina I (1171–1159 BC) and Zababa-shuma-iddin (1158 BC). The Assyrian king Ashur-Dan I conquered further parts of northern Babylonia from both kings, and the Elamite ruler Shutruk-Nahhunte eventually conquered most of eastern Babylonia. Enlil-nadin-ahhe (1157–1155 BC) was finally overthrown and the Kassite Dynasty ended after Ashur-Dan I conquered yet more of northern and central Babylonia, and the Elamite king Shutruk-Nahhunte pushed deep into the heart of Babylonia itself, sacking the city and slaying the king. Poetical works have been found lamenting this disaster.
Despite the loss of territory, military weakness, and evident reduction in literacy and culture, the Kassite dynasty was the longest-lived dynasty of Babylon, lasting until 1157 BC, when Babylon was conquered by Shutruk-Nahhunte of Elam, and reconquered a few years later by the native Akkadian-Babylonian Nebuchadrezzar I, part of the larger Bronze Age collapse.
Early Iron Age — Native Rule, Second Dynasty of Isin 1155–1026 BC.
The Elamites did not remain in control of Babylonia long, and Marduk-kabit-ahheshu (1155–1139 BC) established the Second Dynasty of Isin. This was the very first native Akkadian speaking south Mesopotamian dynasty to rule Babylon, and was to remain in power for some 125 years. The new king successfully drove out the Elamites and prevented any possible Kassite revival. Later in his reign he went to war with Assyria, and had some initial success, briefly capturing the city of Ekallatum before suffering defeat at the hands of the Assyrian king Ashur-Dan I.
Itti-Marduk-balatu succeeded his father in 1138 BC, and successfully repelled Elamite attacks on Babylonia during his 8-year reign. He too made attempts to attack Assyria, but also met with failure.
Ninurta-nadin-shumi took the throne in 1137 BC, and also attempted an invasion of Assyria, his armies seem to have skirted through eastern Syria and then made an attempt to attack the Assyrian city of Arbela (modern Erbil) from the west. However this bold move met with defeat at the hands of Ashur-resh-ishi I who then forced a treaty in his favour upon Babylon.
Nebuchadnezzar I (1124–1103 BC) was the most famous ruler of this dynasty. He fought and defeated the Elamites and drove them from Babylonian territory, invading Elam itself, sacking the Elamite capital Susa, and recovering the sacred statue of Marduk that had been carried off from Babylon. Shortly afterwards, the king of Elam was assassinated and his kingdom disintegrated into civil war. However, Nebuchadnezzar failed to extend Babylonian territory further, being defeated a number of times by Ashur-resh-ishi I, king of the Assyrians for control of formerly Hittite controlled territories in Aramea (Syria). The Hittite Empire had been largely annexed by Assyria, and its heartland finally overrun by invading Phrygians. In the later years of his reign, he devoted himself to peaceful building projects and securing Babylonia's borders.
Nebuchadnezzar was succeeded by his two sons, firstly Enlil-nadin-apli (1103–1100), who lost territory to Assyria. The second of them, Marduk-nadin-ahhe (1098–1081 BC) also went to war with Assyria. Some initial success in these conflicts gave way to catastrophic defeat at the hands of Tiglath-pileser I who annexed huge swathes of Babylonian territory, thus further expanding the Assyrian Empire. Following this a terrible famine gripped Babylon, inviting attacks from Semitic Aramean tribes from the west.
In 1072 BC Marduk-shapik-zeri signed a peace treaty with Ashur-bel-kala of Assyria, however his successor Kadašman-Buriaš was not so friendly to Assyria, prompting the Assyrian king to invade Babylonia and depose him, placing Adad-apla-iddina on the throne as his vassal. Assyrian domination continued until c. 1050 BC, with Marduk-ahhe-eriba and Marduk-zer-X regarded as vassals of Assyria. After 1050 BC Assyria descended into a period of civil war, followed by constant warfare with the Arameans and Phrygians, allowing Babylonia to once more largely free itself from the Assyrian yoke for a few decades.
However East Semitic Babylonia soon began to suffer repeated incursions from West Semitic nomadic peoples migrating from The Levant, and during the 11th century BC large swathes of Babylonia were appropriated and occupied by these newly arrived Arameans and Suteans, followed in the late 10th or early 9th century BC by the Chaldeans . The Chaldeans (not to be confused with modern Chaldean Catholics who are in fact ethnic Assyrians) settled in the far south east of Babylonia, the Arameans much of the countryside in eastern and central Babylonia and the Suteans in the western deserts.
Period of Chaos 1026–911 BC.
The native dynasty, then ruled by Nabu-shum-libur was deposed by marauding Arameans in 1026 BC, and the heart of Babylonia, including the capital city itself descended into anarchic state, and no king was to rule Babylon for over 20 years.
However, in southern Mesopotamia (a region corresponding with the old Dynasty of the Sealand), Dynasty V (1025–1004 BC) arose, this was ruled by Simbar-shipak, leader of a Kassite clan, and was in effect a separate state from Babylon. The state of anarchy allowed the Assyrian ruler Ashur-nirari IV the opportunity to attack Babylonia in 1018 BC, and he invaded and captured the Babylonian city of Atlila and some northern regions for Assyria.
This dynasty was replaced by another Kassite Dynasty (Dynasty VI; 1003–984 BC) which also seems to have regained control over Babylon. The Elamites deposed this brief Kassite revival, with king Mar-biti-apla-usur founding Dynasty VII (984–977 BC). However, this dynasty too fell, when the Arameans once more ravaged Babylon.
Native rule was restored by Nabu-mukin-apli in 977 BC, ushering in Dynasty VIII. Dynasty IX begins with Ninurta-kudurri-usur II, who ruled from 941 BC. Babylonia remained weak during this period, with whole areas of Babylonia now under firm Aramean and Sutean control, and by 850 BC the migrant Chaldeans had established their own land in the extreme south east. Babylonian rulers were often forced to bow to pressure from Assyria and Elam, both of which had appropriated Babylonian territory.
Assyrian Rule 911–619 BC.
From 911 BC with the founding of the Neo-Assyrian Empire by Adad-nirari II, Babylon found itself under the domination and rule of its fellow Mesopotamian state for the next three centuries. Adad-nirari II twice attacked and defeated Shamash-mudammiq of Babylonia, annexing a large area of land north of the Diyala River and the towns of Hīt and Zanqu in mid Mesopotamia. He made further gains over Babylonia under Nabu-shuma-ukin I later in his reign. Tukulti-Ninurta II and Ashurnasirpal II also forced Babylonia into vassalage, and Shalmaneser III sacked Babylon itself, slew king Nabu-apla-iddina, subjugated the Aramean, Sutean and Chaldean tribes settled within Babylonia, and installed Marduk-zakir-shumi I (855–819 BC) followed by Marduk-balassu-iqbi (819–813 BC) as his vassals. It was during the late 850's BC, in the annals of Shalmaneser III, that the Chaldeans and Arabs are first mentioned in the pages of written recorded history.
Upon the death of Shalmaneser II, Baba-aha-iddina was reduced to vassalage by the Assyrian queen Shammuramat ( known as Semiramis to the Persians and Greeks), acting as regent to his successor Adad-nirari III who was merely a boy. Adad-nirari III eventually killed him and ruled there directly until 800 BC until Ninurta-apla-X was crowned. However he too was subjugated by Adad-Nirari II. The next Assyrian king, Shamshi-Adad V then made a vassal of Marduk-bel-zeri.
Babylonia briefly fell to another foreign ruler when Marduk-apla-usur ascended the throne in 780 BC, taking advantage of a period of civil war in Assyria. He was a member of the Chaldean tribe who had a century or so earlier settled in a small region in the far south eastern corner of Mesopotamia, bordering the Persian Gulf and south western Iran. Shamshi-Adad V attacked him and retook northern Babylonia, forcing a border treaty in Assyria's favour upon him. However he was allowed to remain on the throne, and successfully stabilised Babylonia. Eriba-Marduk, another Chaldean, succeeded him in 769 BC and his son, Nabu-shuma-ishkun in 761 BC. Babylonia appears to have been in a state of chaos during this time, with the north occupied by Assyria, its throne occupied by foreign Chaldeans, and civil unrest prominent throughout the land.
A native Babylonian king named Nabonassar overthrew the Chaldean usurpers in 748 BC, and successfully stabilised Babylonia, remaining untroubled by Ashur-nirari V of Assyria. However with the accession of Tiglath-Pileser III (745-727 BC) Babylonia came under renewed attack. Babylon was invaded and sacked and Nabonassar reduced to vassalage. His successors Nabu-nadin-zeri, Nabu-suma-ukin II and Nabu-mukin-zeri were also in servitude to Tiglath-Pileser III, until in 729 BC the Assyrian king decided to rule Babylon directly as its king instead of allowing Babylonian kings to remain as vassals of Assyria as his predecessors had done for two hundred years.
It was during this period that an Akkadian influenced form of Eastern Aramaic was introduced by the Assyrians as the lingua franca of their vast empire, and Mesopotamian Aramaic began to supplant Akkadian as the spoken language of the general populace of both Assyria and Babylonia.
The Assyrian king Shalmaneser V was declared king of Babylon in 727 BC, but died whilst besieging Samaria in 722 BC.
Revolt was then fomented against Assyrian domination by Merodach-Baladan, a Chaldean malka (chieftain) of the far south east of Mesopotamia, with strong Elamite support. Merodach-Baladan managed to take the throne of Babylon itself between 721- 710 BC whilst the Assyrian king Sargon II were otherwise occupied in defeating the Scythians and Cimmerians who had attacked Assyria's Persian and Median vassal colonies in Ancient Iran. Merodach-Baladan was eventually defeated and ejected by Sargon II of Assyria, and fled to his protectors in Elam. Sargon II was then declared king in Babylon.
Sennacherib succeeded Sargon II, and after ruling directly for a while, he placed his son Ashur-nadin-shumi on the throne. However Merodach-Baladan and the Elamites continued to unsuccessfully agitate against Assyrian rule. Nergal-ushezib, an Elamite, murdered the Assyrian prince and briefly took the throne. This led to the infuriated Assyrian king Sennacherib invading and subjugating Elam and sacking Babylon, laying waste to and largely destroying the city. Babylon was regarded as a sacred city by all Mesopotamians, including Assyrians, and this act eventually led Sennacherib to be murdered by his own sons while praying to the god Nisroch in Nineveh. A puppet king Marduk-zakir-shumi II was placed on the throne by the new Assyrian king Esarhaddon. However, Merodach-Baladan returned from exile in Elam, and briefly deposed him, forcing Esarhaddon to attack and defeat him, whereupon he once more fled to his masters in Elam, where he died in exile.
Esarhaddon (681–669 BC) ruled Babylon personally, he completely rebuilt the city, bringing rejuvenation and peace to the region. Upon his death, and in an effort to maintain harmony within his vast empire (which stretched from the Caucasus to Nubia and from Cyprus to Persia), he installed his eldest son Shamash-shum-ukin as a subject king in Babylon, and his youngest, Ashurbanipal in the more senior position as king of Assyria and overlord of Shamash-shum-ukin.
Shamash-shum-ukin, after decades peacefully subject to his brother Ashurbanipal, eventually became infused with Babylonian nationalism despite being an Assyrian himself, declaring that the city of Babylon (and not the Assyrian city of Nineveh) should be the seat of the immense empire. He raised a major revolt against his brother, Ashurbanipal. He led a powerful coalition of peoples also resentful of Assyrian subjugation and rule, including; Elam, the Persians, Medes, the Babylonians, Chaldeans and Suteans of southern Mesopotamia, the Arameans of the Levant and southwest Mesopotamia, the Arabs of the Arabian Peninsula and the Canaanites-Phoenicians. After a bitter struggle Babylon was sacked and its allies vanquished, Shamash-shum-ukim being killed in the process. Elam was destroyed once and for all, and the Babylonians, Persians, Chaldeans, Arabs, Medes, Elamites, Arameans, Suteans and Canaanites were violently subjugated, with Assyrian troops exacting savage revenge on the rebelling peoples. An Assyrian governor named Kandalanu was placed on the throne to rule on behalf of the Assyrian king. Upon Ashurbanipal's death in 627 BC, his son Ashur-etil-ilani became ruler of Babylon and Assyria.
However, Assyria soon descended into a series of brutal internal civil wars which were to cause its downfall. Ashur-etil-ilani was deposed by one of his own generals, named Sin-shumu-lishir in 623 BC, who also set himself up as king in Babylon. After only one year on the throne amidst continual civil, Sin-shar-ishkun ousted him as ruler of Assyria and Babylonia in 622 BC. However, he too was beset by constant unremitting civil war in the Assyrian heartland. Babylonia took advantage of this and rebelled under Nabopolassar, a previously unknown "malka" (chieftain) of the Chaldeans, who had settled in south eastern Mesopotamia c. 950 BC.
It was during the reign of Sin-shar-ishkun that Assyria's vast empire began to unravel, and many of its former subject peoples ceased to pay tribute, most significantly for the Assyrians; the Babylonians, Chaldeans, Medes, Persians, Scythians, Arameans and Cimmerians.
Neo-Babylonian Empire (Chaldean Era).
In 620 BC Nabopolassar seized control over much of Babylonia with the support of most of the inhabitants, with only the city of Nippur and some northern regions showing any loyalty to the Assyrian king. Nabopolassar was unable to yet utterly secure Babylonia, and for the next four years he was forced to contend with an occupying Assyrian army encamped in Babylonia trying to unseat him. However, the Assyrian king, Sin-shar-ishkun was plagued by constant revolts among his own people in Nineveh, and was thus prevented from ejecting Nabopolassar.
The stalemate ended in 615 BC, when Nabopolassar entered the Babylonians and Chaldeans into alliance with Cyaxares, an erstwhile vassal of Assyria, and king of the Medes, Persians and Parthians. Cyaxares had also taken advantage of the Assyrian destruction of the formerly regionally dominant Elam and the subsequent anarchy in Assyria to free the Iranic peoples from three centuries of the Assyrian yoke and regional Elamite domination. The Scythians from north of the Caucasus, and the Cimmerians from the Black Sea who had both also been subjugated by Assyria, joined the alliance, as did regional Aramean tribes.
In 615 BC, while the Assyrian king was fully occupied fighting rebels in both Babylonia and Assyria itself, Cyaxares launched a surprise attack on the Assyrian heartlands, sacking the cities of Kalhu (the Biblical Calah, Nimrud) and Arrapkha (modern Kirkuk).
From this point on the coalition of Babylonians, Chaldeans, Medes, Persians, Scythians, Cimmerians and Arameans fought in unison against a civil war ravaged Assyria. Major Assyrian cities such as Ashur, Arbela (modern Irbil), Guzana, Dur Sharrukin (modern Khorsabad), Imgur-Enlil, Nibarti-Ashur, Kar Ashurnasipal and Tushhan fell to the alliance during 614 BC. Sin-shar-ishkun somehow managed to rally against the odds during 613 BC, and drove back the combined forces ranged against him.
However, the alliance launched a renewed combined attack the following year, and after five years of fierce fighting Nineveh was sacked in late 612 BC after a bitter prolonged siege, followed by street by street fighting, in which Sin-shar-ishkun was killed defending his capital.
House to house fighting continued in Nineveh, and an Assyrian general and member of the royal household, took the throne as Ashur-uballit II. He was offered the chance of accepting a position of vassalage by the leaders of the alliance according to the Babylonian Chronicle. However he refused and managed to somehow successfully fight his way out of Nineveh and to the northern Assyrian city of Harran in Upper Mesopotamia where he founded a new capital. The fighting continued, as the Assyrian king held out against the alliance until 608 BC, when he was eventually ejected by the Medes, Babylonians, Scythians and their allies, and prevented in an attempt to regain the city the same year.
The Egyptian Pharaoh Necho II, whose dynasty had been installed as vassals of Assyria in 671 BC, belatedly tried to aid Egypt's former Assyrian masters, possibly out of fear that Egypt would be next to succumb to the new powers without Assyria to protect them. The Assyrians fought on with Egyptian aid until a final victory was achieved against them at Carchemish in north western Assyria in 605 BC.
The seat of empire was thus transferred to Babylonia for the first time since Hammurabi over a thousand years before.
Nabopolassar was followed by his son Nebuchadnezzar II (605–562 BC), whose reign of 43 years made Babylon once more the mistress of much of the civilized world, taking over a fair portion of the former Assyrian Empire once ruled by its Assyrian brethren, the eastern and north eastern portion being taken by the Medes and the far north by the Scythians.
The Scythians and Cimmerians, erstwhile allies of Babylonia under Nabopolassar, now became a threat, and Nebuchadnezzar II was forced to march into Asia Minor and rout their forces, ending the northern threat to his Empire.
The Egyptians attempted to remain in the Near East, possibly in an effort to aid in restoring Assyria as a secure buffer against Babylonia and the Medes and Persians, or to carve out an empire of their own. Nebuchadnezzar II campaigned against the Egyptians and drove them back over the Sinai. However an attempt to take Egypt itself as his Assyrian predecessors had succeeded, failed, mainly due to a series of rebellions among the Judeans, Phoenicians of Caanan and the Levant. The Babylonian king crushed these rebellions, deposed Jehoiakim, the king of Judah and deported a sizeable part of the population to Babylonia. Cities like Tyre, Sidon and Damascus were also subjugated. The Arabs who dwelt in the deserts to the south of the borders of Mesopotamia were then also subjugated.
In 567 BC he went to war with Pharaoh Amasis, and briefly invaded Egypt itself. After securing his empire, which included marrying a Median princess, he devoted himself to maintaining the empire and conducting numerous impressive building projects in Babylon. He is credited with building the fabled Hanging Gardens of Babylon.
Amel-Marduk succeeded to the throne and reigned for only two years. Little contemporary record of his rule survives, though Berosus later stated that he was deposed and murdered in 560 BC by his successor Neriglissar for conducting himself in an "improper manner".
Neriglissar (560–556 BC) also had a short reign. He was the son in law of Nebuchadnezzar II, and it is unclear if he was a Chaldean or native Babylonian who married into the dynasty. He campaigned in Aram and Phoenicia, successfully maintaining Babylonian rule in these regions. Neriglissar died young however, and was succeeded by his son Labashi-Marduk (556 BC), who was still a boy. He was deposed and killed during the same year in a palace conspiracy.
Of the reign of the last Babylonian king, Nabonidus ("Nabu-na'id", 556–539 BC) who is the son of the Assyrian priestess Adda-Guppi and who managed to kill the last Chaldean king, Labashi-Marduk, and took the reign, there is a fair amount of information available. Nabonidus (hence his son, the regent Belshazzar) was, at least from the mother's side, neither Chaldean nor Babylonian, but ironically Assyrian, hailing from its final capital of Harran (Kharranu). Information regarding Nabonidus is chiefly derived from a chronological tablet containing the annals of Nabonidus, supplemented by another inscription of Nabonidus where he recounts his restoration of the temple of the Moon-god Sin at Harran; as well as by a proclamation of Cyrus issued shortly after his formal recognition as king of Babylonia.
A number of factors arose which would ultimately lead to the fall of Babylon. The population of Babylonia became restive and increasingly disaffected under Nabonidus. He excited a strong feeling against himself by attempting to centralize the religion of Babylonia in the temple of Marduk at Babylon, and while he had thus alienated the local priesthoods, the military party also despised him on account of his antiquarian tastes. He seemed to have left the defense of his kingdom to Belshazzar (a capable soldier but poor diplomat who alienated the political elite), occupying himself with the more congenial work of excavating the foundation records of the temples and determining the dates of their builders. He also spent time outside Babylonia, rebuilding temples in the Assyrian city of Harran, and also among his Arab subjects in the deserts to the south of Mesopotamia. Nabonidus and Belshazzar's Assyrian heritage is also likely to have added to this resentment. In addition, Mesopotamian military might had usually been concentrated in the martial state of Assyria. Babylonia had always been more vulnerable to conquest and invasion than its northern neighbour, and without the might of Assyria to keep foreign powers in check, Babylonia was ultimately exposed.
It was in the sixth year of Nabonidus (549 BC) that Cyrus the Great, the Achaemenid Persian "king of Anshan" in Elam, revolted against his suzerain Astyages, "king of the Manda" or Medes, at Ecbatana. Astyages' army betrayed him to his enemy, and Cyrus established himself at Ecbatana, thus putting an end to the empire of the Medes and making the Persian faction dominant among the Iranic peoples. Three years later Cyrus had become king of all Persia, and was engaged in a campaign to put down a revolt among the Assyrians. Meanwhile, Nabonidus had established a camp in the desert of his colony of Arabia, near the southern frontier of his kingdom, leaving his son Belshazzar ("Belsharutsur") in command of the army.
In 539 BC Cyrus invaded Babylonia. A battle was fought at Opis in the month of June, where the Babylonians were defeated; and immediately afterwards Sippar surrendered to the invader. Nabonidus fled to Babylon, where he was pursued by Gobryas, and on the 16th day of Tammuz, two days after the capture of Sippar, "the soldiers of Cyrus entered Babylon without fighting." Nabonidus was dragged from his hiding place, where the services continued without interruption. Cyrus did not arrive until the 3rd of "Marchesvan" (October), Gobryas having acted for him in his absence. Gobryas was now made governor of the province of Babylon, and a few days afterwards Belshazzar the son of Nabonidus died in battle. A public mourning followed, lasting six days, and Cyrus' son Cambyses accompanied the corpse to the tomb.
One of the first acts of Cyrus accordingly was to allow the Jewish exiles to return to their own homes, carrying with them their sacred temple vessels. The permission to do so was embodied in a proclamation, whereby the conqueror endeavored to justify his claim to the Babylonian throne.
Cyrus now claimed to be the legitimate successor of the ancient Babylonian kings and the avenger of Bel-Marduk, who was assumed to be wrathful at the impiety of Nabonidus in removing the images of the local gods from their ancestral shrines to his capital Babylon.
The Chaldean tribe had lost control of Babylonia decades before the end of the era that sometimes bears their name, and they appear to have blended into the general populace of Babylonia, and during the Persian Achaemenid Empire Chaldeans disappeared as a distinct people, and the term Chaldean ceased to refer to a race of men and instead to a social class only, regardless of ethnicity.
Persian Babylonia.
Babylonia was absorbed into the Achaemenid Empire in 539 BC.
A year before Cyrus' death, in 529 BC, he elevated his son Cambyses II in the government, making him king of Babylon, while he reserved for himself the fuller title of "king of the (other) provinces" of the empire. It was only when Darius Hystaspis acquired the Persian throne and ruled it as a representative of the Zoroastrian religion, that the old tradition was broken and the claim of Babylon to confer legitimacy on the rulers of western Asia ceased to be acknowledged.
Immediately after Darius seized Persia, Babylonia briefly recovered its independence under a native ruler, Nidinta-Bel, who took the name of Nebuchadnezzar III, and reigned from October 522 BC to August 520 BC, when Darius took the city by storm, during this period Assyria to the north also rebelled. A few years later, probably 514 BC, Babylon again revolted under the Armenian King Arakha; on this occasion, after its capture by the Persians, the walls were partly destroyed. E-Saggila, the great temple of Bel, however, still continued to be kept in repair and to be a center of Babylonian religious feelings.
Alexander the Great conquered Babylon in 333 BC for the Greeks, and died there in 323 BC. Babylonia and Assyria then became part of the Greek Seleucid Empire.
It has long been maintained that the foundation of Seleucia diverted the population to the new capital of Babylonia, and that the ruins of the old city became a quarry for the builders of the new seat of government, but the recent publication of the Babylonian Chronicles of the Hellenistic Period has shown that urban life was still very much the same well into the Parthian age (150 BC to 226 AD). The Parthian king Mithridates conquered the region into the Arsacid Empire in 150 BC, and the region became something of a battleground between Greeks and Parthians.
There was a brief interlude of Roman conquest (Roman Assyria, Roman Mesopotamia; AD 116 to 118) under Trajan, after which the Parthians reasserted control.
The satrapy of Babylonia was absorbed into Asuristan (Assyria) in the Sassanid period, which began in 226 AD, and by this time Eastern Rite Syriac Christianity (which emerged in Assyria and Upper Mesopotamia the 1st century AD) had become the dominant religion among the native populace, who had never adopted the Zoroastrian or Hellenic religions of their rulers. Apart from the small 1st century BC to 3rd century AD independent Assyrian states of Adiabene, Osroene and Assur in the north, Mesopotamia remained under largely Persian control until the Arab Islamic conquest in the 7th century AD. After this Asuristan-Assyria was also dissolved as a geopolitical entity, and the native Aramaic speaking and largely Christian populace of southern and central Mesopotamia gradually underwent an (often forced) process of "Arabisation" and "Islamification", with only the Assyrians of the north (known as Ashuriyun by the Arabs) and Mandeans of the south retaining their religions and a distinct Mesopotamian identity, culture, history and language, which they still do to this day.
Babylonian culture.
Bronze Age to Early Iron Age Mesopotamian culture is sometimes summarized as "Assyro-Babylonian", because of the close cultural interdependence of the two political centers. The term "Babylonia", especially in writings from around AD 1900, was formerly used to include Southern Mesopotamia's earliest history, and not only in reference to the later city-state of Babylon proper. This geographic usage of the name "Babylonia' has generally been replaced by the more accurate term "Sumer" in more recent writing.
Babylonian culture.
Art and architecture.
In Babylonia, an abundance of clay, and lack of stone, led to greater use of mudbrick; Babylonian temples were massive structures of crude brick, supported by buttresses, the rain being carried off by drains. One such drain at Ur was made of lead. The use of brick led to the early development of the pilaster and column, and of frescoes and enameled tiles. The walls were brilliantly coloured, and sometimes plated with zinc or gold, as well as with tiles. Painted "terra-cotta" cones for torches were also embedded in the plaster. In Babylonia, in place of the "bas-relief", there was greater use of three-dimensional figures—the earliest examples being the Statues of Gudea, that are realistic if somewhat clumsy. The paucity of stone in Babylonia made every pebble precious, and led to a high perfection in the art of gem-cutting.
Astronomy.
Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the 'Enūma Anu Enlil'. The oldest significant astronomical text that we possess is Tablet 63 of 'Enūma Anu Enlil', the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The oldest rectangular astrolabe dates back to Babylonia c. 1100 BC. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water-clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.
Medicine.
Medical diagnosis and prognosis
We find [medical semiotics] in a whole constellation of disciplines... There was a real common ground among these [Babylonian] forms of knowledge... an approach involving analysis of particular cases, constructed only through traces, symptoms, hints... In short, we can speak about a symptomatic or divinatory [or conjectural] paradigm which could be oriented toward past present or future, depending on the form of knowledge called upon. Toward future... that was the medical science of symptoms, with its double character, diagnostic, explaining past and present, and prognostic, suggesting likely future...—Carlo Ginzburg
The oldest Babylonian texts on medicine date back to the First Babylonian Dynasty in the first half of the 2nd millennium BC. The most extensive Babylonian medical text, however, is the "Diagnostic Handbook" written by the "ummânū", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069-1046 BC).
Along with contemporary ancient Egyptian medicine, the Babylonians introduced the concepts of diagnosis, prognosis, physical examination, and prescriptions. In addition, the "Diagnostic Handbook" introduced the methods of therapy and aetiology and the use of empiricism, logic and rationality in diagnosis, prognosis and therapy. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis.
The symptoms and diseases of a patient were treated through therapeutic means such as bandages, creams and pills. If a patient could not be cured physically, the Babylonian physicians often relied on exorcism to cleanse the patient from any curses. Esagil-kin-apli's "Diagnostic Handbook" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its aetiology and future development, and the chances of the patient's recovery.
Esagil-kin-apli discovered a variety of illnesses and diseases and described their symptoms in his "Diagnostic Handbook". These include the symptoms for many varieties of epilepsy and related ailments along with their diagnosis and prognosis. Later Babylonian medicine resembles early Greek medicine in many ways. In particular, the early treatises of the Hippocratic Corpus show the influence of late Babylonian medicine in terms of both content and form.
Literature.
There were libraries in most towns and temples; an old Sumerian proverb averred that "he who would excel in the school of the scribes must rise with the dawn." Women as well as men learned to read and write, and in Semitic times, this involved knowledge of the extinct Sumerian language, and a complicated and extensive syllabary.
A considerable amount of Babylonian literature was translated from Sumerian originals, and the language of religion and law long continued to be written in the old agglutinative language of Sumer. Vocabularies, grammars, and interlinear translations were compiled for the use of students, as well as commentaries on the older texts and explanations of obscure words and phrases. The characters of the syllabary were all arranged and named, and elaborate lists of them were drawn up.
There are many Babylonian literary works whose titles have come down to us. One of the most famous of these was the Epic of Gilgamesh, in twelve books, translated from the original Sumerian by a certain Sin-liqi-unninni, and arranged upon an astronomical principle. Each division contains the story of a single adventure in the career of Gilgamesh. The whole story is a composite product, and it is probable that some of the stories are artificially attached to the central figure.
Neo-Babylonian culture.
The brief resurgence of a "Babylonian" identity in the 7th to 6th centuries BC was accompanied by a number of important cultural developments.
Astronomy.
Among the sciences, astronomy and astrology still occupied a conspicuous place in Babylonian society. Astronomy was of old standing in Babylonia. The zodiac was a Babylonian invention of great antiquity; and eclipses of the sun and moon could be foretold. There are dozens of cuneiform records of original Mesopotamian eclipse observations.
Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian, Byzantine and Syrian astronomy, in medieval Islamic astronomy, and in Central Asian and Western European astronomy.
Neo-Babylonian astronomy can thus be considered the direct predecessor of much of ancient Greek mathematics and astronomy, which in turn is the historical predecessor of the European (Western) scientific revolution.
During the 8th and 7th centuries BC, Babylonian astronomers developed a new approach to astronomy. They began studying philosophy dealing with the ideal nature of the early universe and began employing an internal logic within their predictive planetary systems. This was an important contribution to astronomy and the philosophy of science and some scholars have thus referred to this new approach as the first scientific revolution. This new approach to astronomy was adopted and further developed in Greek and Hellenistic astronomy.
In Seleucid and Parthian times, the astronomical reports were of a thoroughly scientific character; how much earlier their advanced knowledge and methods were developed is uncertain. The Babylonian development of methods for predicting the motions of the planets is considered to be a major episode in the history of astronomy.
The only Babylonian astronomer known to have supported a heliocentric model of planetary motion was Seleucus of Seleucia (b. 190 BC). Seleucus is known from the writings of Plutarch. He supported the heliocentric theory where the Earth rotated around its own axis which in turn revolved around the Sun. According to Plutarch, Seleucus even proved the heliocentric system, but it is not known what arguments he used.
Mathematics.
Babylonian mathematical texts are plentiful and well edited. In respect of time they fall in two distinct groups: one from the First Babylonian Dynasty period (1830–1531 BC), the other mainly Seleucid from the last three or four centuries BC. In respect of content there is scarcely any difference between the two groups of texts. Thus Babylonian mathematics remained stale in character and content, with very little progress or innovation, for nearly two millennia.
The Babylonian system of mathematics was sexagesimal, or a base 60 numeral system (see: Babylonian numerals). From this we derive the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle. The Babylonians were able to make great advances in mathematics for two reasons. First, the number 60 has many divisors (2, 3, 4, 5, 6, 10, 12, 15, 20, and 30), making calculations easier. Additionally, unlike the Egyptians and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values (much as in our base-ten system: 734 = 7×100 + 3×10 + 4×1). Among the Babylonians' mathematical accomplishments were the determination of the square root of two correctly to seven places (). They also demonstrated knowledge of the Pythagorean theorem well before Pythagoras, as evidenced by this tablet translated by Dennis Ramsey and dating to c. 1900 BC:
4 is the length and 5 is the diagonal.
What is the breadth?
Its size is not known.
4 times 4 is 16. And 5 times 5 is 25.
You take 16 from 25 and there remains 9.
What times what shall I take in order to get 9?
3 times 3 is 9. 3 is the breadth.
The "ner" of 600 and the "sar" of 3600 were formed from the unit of 60, corresponding with a degree of the equator. Tablets of squares and cubes, calculated from 1 to 60, have been found at Senkera, and a people acquainted with the sun-dial, the clepsydra, the lever and the pulley, must have had no mean knowledge of mechanics. A crystal lens, turned on the lathe, was discovered by Austen Henry Layard at Nimrud along with glass vases bearing the name of Sargon; this could explain the excessive minuteness of some of the writing on the Assyrian tablets, and a lens may also have been used in the observation of the heavens.
The Babylonians might have been familiar with the general rules for measuring the areas. They measured the circumference of a circle as three times the diameter and the area as one-twelfth the square of the circumference, which would be correct if π were estimated as 3. The volume of a cylinder was taken as the product of the base and the height, however, the volume of the frustum of a cone or a square pyramid was incorrectly taken as the product of the height and half the sum of the bases. Also, there was a recent discovery in which a tablet used π as 3 and 1/8. The Babylonians are also known for the Babylonian mile, which was a measure of distance equal to about seven miles today. This measurement for distances eventually was converted to a time-mile used for measuring the travel of the Sun, therefore, representing time. (Eves, Chapter 2)
Philosophy.
The origins of Babylonian philosophy can be traced back to early Mesopotamian wisdom literature, which embodied certain philosophies of life, particularly ethics, in the forms of dialectic, dialogs, epic poetry, folklore, hymns, lyrics, prose, and proverbs. Babylonian reasoning and rationality developed beyond empirical observation.
It is possible that Babylonian philosophy had an influence on Greek philosophy, particularly Hellenistic philosophy. The Babylonian text "Dialogue of Pessimism" contains similarities to the agonistic thought of the sophists, the Heraclitean doctrine of contrasts, and the dialogs of Plato, as well as a precursor to the maieutic Socratic method of Socrates. The Milesian philosopher Thales is also known to have studied philosophy in Mesopotamia.
Legacy.
Babylonia, and particularly its capital city Babylon, has long held a place in Abrahamic religions as a symbol of excess and dissolute power. Many references are made to Babylon in the Bible, both literally and allegorically. The mentions in the Tanakh tend to be historical or prophetic, while New Testament references are more likely figurative, or cryptic references possibly to pagan Rome, or some other archetype. The legendary Hanging Gardens of Babylon and the Tower of Babel are seen as symbols of luxurious and arrogant power respectively.

</doc>
<doc id="46884" url="http://en.wikipedia.org/wiki?curid=46884" title="Internment of Japanese Americans">
Internment of Japanese Americans

The internment of Japanese Americans in the United States was the forced relocation and incarceration during World War II of between 110,000 and 120,000 people of Japanese ancestry who lived on the Pacific coast in camps in the interior of the country. Sixty-two percent of the internees were United States citizens. The U.S. government ordered the removal of Japanese Americans in 1942, shortly after Imperial Japan's attack on Pearl Harbor.
Such incarceration was applied unequally due to differing population concentrations and, more importantly, state and regional politics: more than 110,000 Japanese Americans, nearly all who lived on the West Coast, were forced into interior camps, but in Hawaii, where the 150,000-plus Japanese Americans comprised over one-third of the population, only 1,200 to 1,800 were interned. The forced relocation and incarceration has been determined to have resulted more from racism and discrimination among whites on the West Coast, rather than any military danger posed by the Japanese Americans.
President Franklin D. Roosevelt authorized the deportation and incarceration with Executive Order 9066, issued February 19, 1942, which allowed regional military commanders to designate "military areas" from which "any or all persons may be excluded." This power was used to declare that all people of Japanese ancestry were excluded from the entire West Coast, including all of California and much of Oregon, Washington and Arizona, except for those in government camps. Approximately 5,000 Japanese Americans voluntarily relocated outside the exclusion zone, and some 5,500 community leaders arrested after Pearl Harbor were already in custody, but the majority of mainland Japanese Americans were evacuated (forcibly relocated) from their West Coast homes during the spring of 1942. The United States Census Bureau assisted the internment efforts by providing confidential neighborhood information on Japanese Americans. The Bureau denied its role for decades, but this was finally proven in 2007. In 1944, the Supreme Court upheld the constitutionality of the removal by ruling against Fred Korematsu's appeal for violating an exclusion order. The Court limited its decision to the validity of the exclusion orders, avoiding the issue of the incarceration of U.S. citizens with no due process.
In 1980, under mounting pressure from the Japanese American Citizens League and redress organizations, President Jimmy Carter opened an investigation to determine whether the decision to put Japanese Americans into internment camps had been justified by the government. He appointed the Commission on Wartime Relocation and Internment of Civilians (CWRIC) to investigate the camps. The Commission's report, titled "“Personal Justice Denied,”" found little evidence of Japanese disloyalty at the time and, concluding the incarceration had been the product of racism, recommended that the government pay reparations to the survivors. In 1988, President Ronald Reagan signed into law the Civil Liberties Act, which apologized for the internment on behalf of the U.S. government and authorized a payment of $20,000 to each individual camp survivor. The legislation admitted that government actions were based on "race prejudice, war hysteria, and a failure of political leadership". The U.S. government eventually disbursed more than $1.6 billion in reparations to 82,219 Japanese Americans who had been interned and their heirs.
Of 127,000 Japanese Americans living in the continental United States at the time of the Pearl Harbor attack, 112,000 resided on the West Coast. About 80,000 were "nisei" (literal translation: "second generation"; American-born Japanese with U.S. citizenship) and "sansei" ("third generation"; the children of Nisei). The rest were "issei" ("first generation", immigrants born in Japan who were ineligible for U.S. citizenship by U.S. law).
Japanese Americans before World War II.
Due in large part to socio-political changes stemming from the Meiji Restoration — and a recession caused by the abrupt opening of Japan's economy to the world market — people began migrating from Japan in 1868 to find work to survive. From 1869 to 1924 approximately 200,000 immigrated to the islands of Hawaii, mostly laborers arriving to work on the islands' sugar plantations. Some 180,000 went to the U.S. mainland, with the majority settling on the West Coast and establishing farms or small businesses. Most arrived before 1908, when the Gentlemen's Agreement between Japan and the United States banned the immigration of unskilled laborers. A loophole allowed the wives of men already in the country to join their husbands. The practice of women marrying by proxy and immigrating to the U.S. resulted in a large increase of "picture brides" and, soon after, children.
As the Japanese American population continued to grow, European Americans on the West Coast resisted the new group, fearing competition and exaggerating the idea of hordes of Asians keen to take over white-owned farmland and businesses. Groups such as the Japanese Exclusion League, the California Joint Immigration Committee, and the Native Sons of the Golden West organized in response to this "Yellow Peril" and lobbied successfully to restrict the property and citizenship rights of Japanese immigrants, as similar groups had previously organized against Chinese immigrants. Several laws and treaties attempting to slow immigration from Japan were introduced beginning in the late 19th century. The Immigration Act of 1924, following the example of the 1882 Chinese Exclusion Act, effectively banned all immigration from Japan and other "undesirable" Asian countries.
The 1924 ban on immigration produced unusually well-defined generational groups within the Japanese American community. The Issei were exclusively those who had immigrated before 1924; some retained longings to return to their homeland. Because no new immigration was permitted, all Japanese Americans born after 1924 were, by definition, born in the U.S. and automatically U.S. citizens. This Nisei generation were a distinct cohort from their parents. In addition to the usual generational differences, Issei men were typically ten to fifteen years older than their wives, making them significantly older than the younger children of their often large families. U.S. law prohibited Japanese immigrants from becoming naturalized citizens, making them dependent on their children to rent or purchase property. Communication between English-speaking children and parents who spoke mostly or completely in Japanese was often difficult. A significant number of older Nisei, many of whom were born prior to the immigration ban, had married and already started families of their own by the outbreak of World War II.
Despite racist legislation that prevented Issei from becoming naturalized citizens (and therefore from owning property, voting, or running for political office), these Japanese immigrants established communities in their new hometowns. Japanese Americans contributed to the agriculture of California and other Western states, by introducing irrigation methods that enabled the cultivation of fruits, vegetables, and flowers on previously inhospitable land. In both rural and urban areas, "kenjinkai", community groups for immigrants from the same Japanese prefecture, and "fujinkai", Buddhist women's associations, organized community events and charitable work, provided loans and financial assistance, and built Japanese language schools for their children. Excluded from setting up shop in white neighborhoods, Nikkei-owned small businesses thrived in the "Nihonmachi", or Japantowns of urban centers such as Los Angeles, San Francisco, and Seattle.
In the 1930s the Office of Naval Intelligence, concerned by Imperial Japan's rising military power in the East, began conducting surveillance on Japanese American communities in Hawaii. From 1936, at the behest of President Roosevelt, the ONI began compiling a "special list of those who would be the first to be placed in a concentration camp in the event of trouble" between Japan and the United States. In 1939, again by order of the President, the ONI, Military Intelligence Division, and FBI began working together to compile a larger Custodial Detention Index. Early in 1941, Roosevelt commissioned Charles Munson to conduct an investigation on Japanese Americans living on the West Coast and in Hawaii. After working with FBI and ONI officials and interviewing Japanese Americans and those familiar with them, Munson determined that the "Japanese problem" was nonexistent. His final report to the President, submitted November 7, 1941, "certified a remarkable, even extraordinary degree of loyalty among this generally suspect ethnic group." A subsequent report by Kenneth Ringle, delivered to the President in January 1942, also found little evidence to support claims of Japanese American disloyalty and argued against mass incarceration.
After Pearl Harbor.
The attack on Pearl Harbor on December 7, 1941 led military and political leaders to suspect that Imperial Japan was preparing a full-scale attack on the West Coast of the United States. Due to Japan's rapid military conquest of a large portion of Asia and the Pacific between 1936 and 1942, some Americans feared that its military forces were unstoppable.
American public opinion initially stood by the large population of Japanese Americans living on the West Coast, with the "Los Angeles Times" characterizing them as "good Americans, born and educated as such." Many Americans believed that their loyalty to the United States was unquestionable.
But, six weeks after the attack, public opinion along the Pacific began to turn against Japanese Americans living on the West Coast, as the press and other Americans became nervous about the potential for fifth column activity. Though the administration (including the President Franklin D. Roosevelt and FBI Director J. Edgar Hoover) dismissed all rumors of Japanese-American espionage on behalf of the Japanese War effort, pressure mounted upon the Administration as the tide of public opinion turned against Japanese Americans. Civilian and military officials had serious concerns about the loyalty of the ethnic Japanese after the Niihau Incident which immediately followed the attack on Pearl Harbor, when a civilian Japanese national and two Hawaiian-born ethnic Japanese on the island of Ni'ihau violently freed a downed and captured Japanese naval airman, attacking their fellow Ni'ihau islanders in the process.
Several concerns over the loyalty of ethnic Japanese seemed to stem from racial prejudice rather than any evidence of malfeasance. Major Karl Bendetsen and Lieutenant General John L. DeWitt, head of the Western Command, each questioned Japanese-American loyalty. DeWitt, who administered the internment program, repeatedly told newspapers that "A Jap's a Jap" and testified to Congress,
 I don't want any of them [persons of Japanese ancestry] here. They are a dangerous element. There is no way to determine their loyalty... It makes no difference whether he is an American citizen, he is still a Japanese. American citizenship does not necessarily determine loyalty... But we must worry about the Japanese all the time until he is wiped off the map.
DeWitt also sought approval to conduct search and seizure operations aimed at preventing alien Japanese from making radio transmissions to Japanese ships. The Justice Department declined, stating that there was no probable cause to support DeWitt's assertion, as the FBI concluded that there was no security threat. On January 2, the Joint Immigration Committee of the California Legislature sent a manifesto to California newspapers which attacked "the ethnic Japanese," who it alleged were "totally unassimilable." This manifesto further argued that all people of Japanese heritage were loyal subjects of the Emperor of Japan; the manifesto contended that Japanese language schools were bastions of racism which advanced doctrines of Japanese racial superiority.
The manifesto was backed by the Native Sons and Daughters of the Golden West and the California Department of the American Legion, which in January demanded that all Japanese with dual citizenship be placed in concentration camps. Internment was not limited to those who had been to Japan, but included a very small number of German and Italian enemy aliens. By February, Earl Warren, the Attorney General of California, had begun his efforts to persuade the federal government to remove all people of Japanese ethnicity from the West Coast.
Those who were as little as 1/16 Japanese could be placed in internment camps. There is evidence supporting the argument that the measures were racially motivated, rather than a military necessity. Bendetsen, promoted to colonel, said in 1942 "I am determined that if they have one drop of Japanese blood in them, they must go to camp."
Upon the bombing of Pearl Harbor and pursuant to the Alien Enemies Act, Presidential Proclamations 2525, 2526 and 2527 were issued designating Japanese, German and Italian nationals as enemy aliens. Information from the CDI was used to locate and incarcerate foreign nationals from Japan, Germany and Italy (although Germany and Italy did not declare war on the U.S. until December 11).
Presidential Proclamation 2537 was issued on January 14, 1942, requiring aliens to report any change of address, employment or name to the FBI. Enemy aliens were not allowed to enter restricted areas. Violators of these regulations were subject to "arrest, detention and internment for the duration of the war."
Executive Order 9066 and related actions.
Executive Order 9066, signed by Franklin D. Roosevelt on February 19, 1942, allowed authorized military commanders to designate "military areas" at their discretion, "from which any or all persons may be excluded." These "exclusion zones," unlike the "alien enemy" roundups, were applicable to anyone that an authorized military commander might choose, whether citizen or non-citizen. Eventually such zones would include parts of both the East and West Coasts, totaling about 1/3 of the country by area. Unlike the subsequent deportation and incarceration programs that would come to be applied to large numbers of Japanese Americans, detentions and restrictions directly under this Individual Exclusion Program were placed primarily on individuals of German or Italian ancestry, including American citizens.
These edicts included persons of part-Japanese ancestry as well. Anyone with at least one-sixteenth (equivalent to having one great-great grandparent) Japanese ancestry was eligible. Korean-Americans and Taiwanese, classified as ethnically Japanese because both Korea and Taiwan were Japanese colonies, were also included.
Non-military advocates for exclusion, removal, and detention.
The deportation and incarceration were popular among many white farmers who resented the Japanese American farmers. "White American farmers admitted that their self-interest required removal of the Japanese." These individuals saw internment as a convenient means of uprooting their Japanese-American competitors. Austin E. Anson, managing secretary of the Salinas Vegetable Grower-Shipper Association, told the "Saturday Evening Post" in 1942:
"We're charged with wanting to get rid of the Japs for selfish reasons. We do. It's a question of whether the white man lives on the Pacific Coast or the brown men. They came into this valley to work, and they stayed to take over... If all the Japs were removed tomorrow, we'd never miss them in two weeks, because the white farmers can take over and produce everything the Jap grows. And we do not want them back when the war ends, either."
The Roberts Commission Report, prepared at President Franklin D. Roosevelt's request, has been cited as an example of the fear and prejudice informing the thinking behind the internment program. The Report sought to link Japanese Americans with espionage activity, and to associate them with the bombing of Pearl Harbor. Columnist Henry McLemore, who wrote for the Hearst newspapers, reflected the growing public sentiment that was fueled by this report:
 "I am for the immediate removal of every Japanese on the West Coast to a point deep in the interior. I don't mean a nice part of the interior either. Herd 'em up, pack 'em off and give 'em the inside room in the badlands... Personally, I hate the Japanese. And that goes for all of them."
Other California newspapers also embraced this view. According to a "Los Angeles Times" editorial,
 "A viper is nonetheless a viper wherever the egg is hatched... So, a Japanese American born of Japanese parents, nurtured upon Japanese traditions, living in a transplanted Japanese atmosphere... notwithstanding his nominal brand of accidental citizenship almost inevitably and with the rarest exceptions grows up to be a Japanese, and not an American... Thus, while it might cause injustice to a few to treat them all as potential enemies, I cannot escape the conclusion... that such treatment... should be accorded to each and all of them while we are at war with their race."
State politicians joined the bandwagon that was embraced by Leland Ford of Los Angeles, who demanded that "all Japanese, whether citizens or not, be placed in [inland] concentration camps."
Incarceration of Japanese Americans, who provided critical agricultural labor on the West Coast, created a labor shortage, which was exacerbated by the induction of many American laborers into the Armed Forces. This vacuum precipitated a mass immigration of Mexican workers into the United States to fill these jobs, under the banner of what became known as the Bracero Program. Many Japanese internees were temporarily released from their camps – for instance, to harvest Western beet crops – to address this wartime labor shortage.
Non-military advocates against exclusion, removal, and detention.
Like many white American farmers, the white businessmen of Hawaii had their own motives for determining how to deal with the Japanese Americans, but they opposed internment. Instead, these individuals gained passage of legislation to retain in freedom the nearly 150,000 Japanese Americans who would have been otherwise sent to internment camps within Hawaii. As a result, only 1,200 to 1,800 Japanese Americans in Hawaii were interned.
The powerful businessmen of Hawaii concluded that imprisonment of such a large proportion of the islands' population would adversely affect the economic prosperity of the island state. The Japanese represented “over 90 percent of the carpenters, nearly all of the transportation workers, and a significant portion of the agricultural laborers” on the island.
General Delos Carleton Emmons, the military governor of Hawaii, also argued that Japanese labor was “‘absolutely essential’ for rebuilding the defenses destroyed at Pearl Harbor”. Recognizing the Japanese-American community’s contribution to the affluence of the Hawaiian economy, General Emmons fought against the internment of the Japanese Americans and had the support of most of the businessmen of Hawaii.
Coming to different conclusions about how to deal with the Japanese-American community, both the white farmers of the United States and the white businessmen of Hawaii placed priority on protecting their own economic interests.
Statement of military necessity as justification for internment.
Niihau Incident.
The Niihau Incident occurred in December 1941, just after the Japanese attack on Pearl Harbor. Three Japanese Americans on the Hawaiian island of Niihau assisted a Japanese pilot, Shigenori Nishikaichi, who crashed there. Despite the incident, the Territorial Governor of Hawaii rejected calls for the mass internment of the Japanese Americans living there. Shigenori Nishikaichi is buried in his hometown, Hashihama, Japan. On his grave stone is written, 'His meritorious deed will live forever.' 
Cryptography.
In "Magic: The Untold Story of U.S. Intelligence and the Evacuation of Japanese Residents From the West Coast During World War II", David Lowman, a former National Security Agency (NSA) operative, argues that Magic intercepts ("Magic" was the code-name for American code-breaking efforts) posed "the frightening specter of massive espionage nets," thus justifying internment. Lowman contended that incarceration served to ensure the secrecy of U.S. code-breaking efforts, because effective prosecution of Japanese Americans might necessitate disclosure of secret information. If U.S. code-breaking technology was revealed in the context of trials of individual spies, the Japanese Imperial Navy would change its codes, thus undermining U.S. strategic wartime advantage.
Some scholars have criticized or dismissed Lowman's reasoning that "disloyalty" among some individual Japanese Americans could legitimize "incarcerating 120,000 people, including infants, the elderly, and the mentally ill". Lowman's reading of the contents of the "Magic" cables has also been challenged, as some scholars contend that the cables demonstrate that Japanese Americans were not heeding the overtures of Imperial Japan to spy against the United States. According to one critic, Lowman's book has long since been "refuted and discredited".
The controversial conclusions drawn by Lowman were defended by conservative commentator Michelle Malkin in her book "In Defense of Internment; The Case for 'Racial Profiling' in World War II and the War on Terror" (2004). Malkin's defense of Japanese internment was due in part to reaction to what she describes as the "constant alarmism from Bush-bashers who argue that every counter-terror measure in America is tantamount to the internment". She criticized academia's treatment of the subject, and suggested that academics critical of Japanese internment had ulterior motives. Her book was widely criticized, particularly with regard to her reading of the "Magic" cables. Daniel Pipes, also drawing on Lowman, has defended Malkin, and said that Japanese American internment was "a good idea" which offers "lessons for today".
United States District Court opinions.
A letter by General DeWitt and Colonel Bendetsen expressing racist bias against Japanese Americans was circulated and then hastily redacted in 1943–1944. DeWitt's final report stated that, because of their race, it was impossible to determine the loyalty of Japanese Americans, thus necessitating internment. The original version was so offensive – even in the atmosphere of the wartime 1940s – that Bendetsen ordered all copies to be destroyed.
In 1980, a copy of the original "Final Report: Japanese Evacuation from the West Coast – 1942" was found in the National Archives, along with notes showing the numerous differences between the original and redacted versions. This earlier, racist and inflammatory version, as well as the FBI and Office of Naval Intelligence (ONI) reports, led to the "coram nobis" retrials which overturned the convictions of Fred Korematsu, Gordon Hirabayashi and Minoru Yasui on all charges related to their refusal to submit to exclusion and internment. The courts found that the government had intentionally withheld these reports and other critical evidence, at trials all the way up to the Supreme Court, which proved that there was no military necessity for the exclusion and internment of Japanese Americans. In the words of Department of Justice officials writing during the war, the justifications were based on "willful historical inaccuracies and intentional falsehoods."
The Ringle Report.
In May 2011, U.S. Solicitor General Neal Katyal, after a year of investigation, found Charles Fahy had intentionally withheld "The Ringle Report" drafted by the Office of Naval Intelligence, in order to justify the Roosevelt administration's actions in the cases of "Hirabayashi v. United States" and "Korematsu v. United States". The report would have undermined the administration's position of the military necessity for such action, as it concluded that most Japanese Americans were not a national security threat, and that allegations of communication espionage had been found to be without basis by the FBI and Federal Communications Commission.
Facilities.
While this event is most commonly called the "internment" of Japanese Americans, the government operated several different types of camps holding Japanese Americans. The best known facilities were the military-run Wartime Civil Control Administration (WCCA) "Assembly Centers" and the civilian-run War Relocation Authority (WRA) "Relocation Centers," which are generally (but unofficially) referred to as "internment camps." Scholars have urged dropping such euphemisms and refer to them as concentration camps and the people as incarcerated. The Department of Justice (DOJ) operated camps officially called "Internment Camps", which were used to detain those suspected of crimes or of "enemy sympathies." The government also operated camps for a limited number of German Americans and Italian Americans, who sometimes were assigned to share facilities with the Japanese Americans. The WCCA and WRA facilities were the largest and the most public. The WCCA Assembly Centers were temporary facilities that were first set up in horse racing tracks, fairgrounds and other large public meeting places to assemble and organize internees before they were transported to WRA Relocation Centers by truck, bus or train. The WRA Relocation Centers were semi-permanent camps that housed persons removed from the exclusion zone after March 1942, or until they were able to relocate elsewhere in the United States outside the exclusion zone.
DOJ and Army internment camps.
Eight U.S. Department of Justice Camps (in Texas, Idaho, North Dakota, New Mexico, and Montana) held Japanese Americans, primarily non-citizens and their families. The camps were run by the Immigration and Naturalization Service, under the umbrella of the DOJ, and guarded by Border Patrol agents rather than military police. The population of these camps included approximately 3,800 of the 5,500 Buddhist and Christian ministers, Japanese-language school instructors, newspaper workers, fishermen, and community leaders who had been accused of fifth column activity and arrested by the FBI after Pearl Harbor. (The remaining 1,700 were released to WRA relocation centers.) Immigrants and nationals of German and Italian ancestry were also held in these facilities, often in the same camps as Japanese Americans. Approximately 7,000 German Americans and 3,000 Italian Americans from Hawai'i and the U.S. mainland were interned in DOJ camps, along with 500 German seamen already in custody after being rescued from the "SS Columbus" in 1938. In addition 2,264 ethnic Japanese, 4,058 ethnic Germans, and 288 ethnic Italians were deported from 19 Latin American countries for a later-abandoned hostage exchange program with Axis countries or confinement in DOJ camps.
Several U.S. Army internment camps held Japanese, Italian and German American men considered "potentially dangerous." Camp Lordsburg, in New Mexico, was the only site built specifically to confine Japanese Americans. In May 1943, the Army was given responsibility for the detention of prisoners of war and all civilian internees were transferred to DOJ camps.
WCCA Civilian Assembly Centers.
Executive Order 9066 authorized the removal of all persons of Japanese ancestry from the West Coast; however, it was signed before there were any facilities completed to house the displaced Japanese Americans. After the voluntary evacuation program failed to result in many families leaving the exclusion zone, the military took charge of the now-mandatory evacuation. On April 9, 1942, the Wartime Civilian Control Administration (WCCA) was established by the Western Defense Command to coordinate the forced removal of Japanese Americans to inland concentration camps.
The relocation centers faced opposition from inland communities near the proposed sites who disliked the idea of their new "Jap" neighbors. In addition, government forces were struggling to build what would essentially be self-sufficient towns in very isolated, undeveloped and harsh regions of the country; they were not prepared to house the influx of over 110,000 internees. Since Japanese Americans living in the restricted zone were considered too dangerous to conduct their daily business, the military decided it had to house them in temporary centers until the relocation centers were completed.
Under the direction of Colonel Karl Bendetsen, existing facilities had been designated for conversion to WCCA use in March 1942, and the Army Corps of Engineers finished construction on these sites on April 21, 1942. All but four of the 15 confinement sites (12 in California, and one each in Washington, Oregon and Arizona) had previously been racetracks or fairgrounds. The stables and livestock areas were cleaned out and hastily converted to living quarters for families of up to six, while wood and tarpaper barracks were constructed for additional housing, as well as communal latrines, laundry facilities and mess halls. A total of 92,193 Japanese Americans were transferred to these temporary detention centers from March to August 1942. (18,026 more had been taken directly to two "reception centers" that were developed as the Manzanar and Poston WRA camps.) The WCCA was dissolved on March 15, 1943, when it became the War Relocation Authority and turned its attentions to the more permanent relocation centers.
WRA Relocation Centers.
The War Relocation Authority (WRA) was the U.S. civilian agency responsible for the relocation and detention. The WRA was created by President Roosevelt on March 18, 1942 with Executive Order 9102 and officially ceased to exist June 30, 1946. Milton S. Eisenhower, then an official of the Department of Agriculture, was chosen to head the WRA. Dillon S. Myer replaced Eisenhower three months later on June 17, 1942. Myer served as Director of the WRA until the centers were closed. Within nine months, the WRA had opened ten facilities in seven states, and transferred over 100,000 people from the WCCA facilities.
The WRA camp at Tule Lake, though initially like the other camps, eventually was used as a detention center for people believed to pose a security risk. Tule Lake also served as a "segregation center" for individuals and families who were deemed "disloyal," and for those who were to be deported to Japan.
List of camps.
There were three types of camps. "Civilian Assembly Centers" were temporary camps, frequently located at horse tracks, where Japanese Americans were sent as they were removed from their communities. Eventually, most were sent to "Relocation Centers," also known as "internment camps." "Detention camps" housed Nikkei considered to be disruptive or of special interest to the government.
Justice Department detention camps.
These camps often held German-American and Italian-American detainees in addition to Japanese Americans:
Citizen Isolation Centers.
The Citizen Isolation Centers were for those considered to be problem inmates.
Federal Bureau of Prisons.
Detainees convicted of crimes, usually draft resistance, were sent to these sites, mostly federal prisons:
US Army facilities.
These camps often held German and Italian detainees in addition to Japanese Americans:
Immigration and Naturalization Service facilities.
These immigration detention stations held the roughly 5,500 men arrested immediately after Pearl Harbor, in addition to several thousand German and Italian detainees, and served as processing centers from which the men were transferred to DOJ or Army camps:
Exclusion, removal, and detention.
Somewhere between 110,000 and 120,000 people of Japanese ancestry were subject to this mass exclusion program, of whom about two-thirds were U.S. citizens. The remaining one-third were non-citizens subject to internment under the Alien Enemies Act; many of these "resident aliens" had been inhabitants of the United States for decades, but had been deprived by law of being able to become naturalized citizens. Also part of the West Coast removal were 101 children of Japanese descent taken from orphanages and foster homes within the exclusion zone.
Internees of Japanese descent were first sent to one of 17 temporary "Civilian Assembly Centers," where most awaited transfer to more permanent relocation centers being constructed by the newly formed War Relocation Authority (WRA). Some of those who reported to the civilian assembly centers were not sent to relocation centers, but were released under the condition that they remain outside the prohibited zone until the military orders were modified or lifted. Almost 120,000 Japanese Americans and resident Japanese aliens were eventually removed from their homes in California, the western halves of Oregon and Washington and southern Arizona as part of the single largest forced relocation in U.S. history.
Most of these camps/residences, gardens, and stock areas were placed on Native American reservations, for which the Native Americans were formally compensated. The Native American councils disputed the amounts negotiated in absentia by US government authorities. They later sued to gain relief and additional compensation for some items of dispute.
Under the National Student Council Relocation Program (supported primarily by the American Friends Service Committee), students of college age were permitted to leave the camps to attend institutions willing to accept students of Japanese ancestry. Although the program initially granted leave permits to a very small number of students, this eventually included 2,263 students by December 31, 1943.
Curfew and exclusion.
On March 2, 1942, General John DeWitt, commanding general of the Western Defense Command, publicly announced the creation of two military restricted zones. Military Area No. 1 consisted of the southern half of Arizona and the western half of California, Oregon and Washington, as well as all of California south of Los Angeles. Military Area No. 2 covered the rest of those states. DeWitt's proclamation informed Japanese Americans they would be required to leave Military Area 1, but stated that they could remain in the second restricted zone. Removal from Military Area No. 1 initially occurred through "voluntary evacuation." Japanese Americans were free to go anywhere outside of the exclusion zone or inside Area 2, with arrangements and costs of relocation to be borne by the individuals. The policy was short-lived; DeWitt issued another proclamation on March 27 that prohibited Japanese Americans from leaving Area 1. A night-time curfew, also initiated on March 27, 1942, placed further restrictions on the movements and daily lives of Japanese Americans.
Eviction from the West Coast began on March 24, 1942 with Civilian Exclusion Order No. 1, which gave the 227 Japanese American residents of Bainbridge Island, Washington six days to prepare for their "evacuation" directly to Manzanar. Colorado governor Ralph Lawrence Carr was the only elected official to publicly denounce the internment of American citizens (an act that cost his reelection, but gained him the gratitude of the Japanese American community, such that a statue of him was erected in the Denver Japantown's Sakura Square). A total of 108 exclusion orders issued by the Western Defense Command over the next five months completed the removal of Japanese Americans from the West Coast in August 1942.
Conditions in the camps.
The quality of life in the camps was heavily influenced by which government entity was responsible for them. INS Camps were regulated by international treaty. The legal difference between interned and relocated had significant effects on those locked up. INS camps were required to provide food quality and housing at the minimum equal to that experienced by the lowest ranked person in the military. Food in INS camps was of better quality than that of WRA camps.
According to a 1943 War Relocation Authority report, internees were housed in "tar paper-covered barracks of simple frame construction without plumbing or cooking facilities of any kind." The spartan facilities met international laws, but left much to be desired. Many camps were built quickly by civilian contractors during the summer of 1942 based on designs for military barracks, making the buildings poorly equipped for cramped family living. Throughout many camps, twenty-five people were forced to live in space built to contain four, leaving no room for privacy. 
The Heart Mountain War Relocation Center in northwestern Wyoming was a barbed-wire-surrounded enclave with unpartitioned toilets, cots for beds, and a budget of 45 cents daily per capita for food rations.
Armed guards were posted at the camps, which were all in remote, desolate areas far from population centers. Internees were typically allowed to stay with their families, and were treated decently unless they violated the rules. There are documented instances of guards shooting internees who reportedly attempted to walk outside the fences. One such shooting, that of James Wakasa at Topaz, led to a re-evaluation of the security measures in the camps. Some camp administrations eventually allowed relatively free movement outside the marked boundaries of the camps. Nearly a quarter of the internees left the camps to live and work elsewhere in the United States, outside the exclusion zone. Eventually, some were authorized to return to their hometowns in the exclusion zone under supervision of a sponsoring American family or agency whose loyalty had been assured.
The phrase "shikata ga nai" (loosely translated as "it cannot be helped") was commonly used to summarize the interned families' resignation to their helplessness throughout these conditions. This was noticed by their children, as mentioned in the well-known memoir "Farewell to Manzanar" by Jeanne Wakatsuki Houston and James D. Houston.
Medical care.
Before the war, 87 physicians and surgeons, 137 nurses, 105 dentists, 132 pharmacists, 35 optometrists, and 92 lab technicians provided healthcare to the Japanese American population, with most practicing in urban centers like Los Angeles, San Francisco and Seattle. As the eviction from the West Coast was carried out, the Wartime Civilian Control Administration worked with the United States Public Health Service and many of these professionals to establish infirmaries within the temporary assembly centers. An Issei doctor was appointed to manage each facility, and additional healthcare staff worked under his supervision, although the USPHS recommendation of one physician for every 1,000 inmates and one nurse to 200 inmates was not met. Overcrowded and unsanitary conditions forced assembly center infirmaries to prioritize inoculations over general care, obstetrics and surgeries; at Manzanar, for example, hospital staff performed over 40,000 immunizations against typhoid and smallpox. Food poisoning was common and also demanded significant attention. Those who were interned in Topaz, Minidoka, and Jerome experienced outbreaks of dysentery.
Facilities in the more permanent "relocation centers" eventually surpassed the makeshift assembly center infirmaries, but in many cases these hospitals were incomplete when inmates began to arrive and were not fully functional for several months. Additionally, vital medical supplies such as medications and surgical and sterilization equipment were limited. The staff shortages suffered in the assembly centers continued in the WRA camps. The administration's decision to invert the management structure and demote Japanese American medical workers to positions below white employees, while capping their pay rate at a $20/month, further exacerbated this problem. (At Heart Mountain, for example, Japanese American doctors received $19/month compared to white nurses' $150/month.) The war had caused a shortage of healthcare professionals across the country, and the camps often lost potential recruits to outside hospitals that offered better pay and living conditions. When the WRA began to allow some Japanese Americans to leave camp, many Nikkei medical professionals resettled outside camp. Those who remained had little authority in administration of the hospitals. Combined with the inequitable payment of salaries between white and Japanese American employees, conflicts arose at several hospitals, and there were two Japanese American walk-outs at Heart Mountain in 1943.
Despite a shortage of healthcare workers, limited access to equipment, and tension between white administrators and Japanese American staff, these hospitals provided much needed medical care in camp. The extreme climates of the remote incarceration sites were hard on infants and elderly inmates. The frequent dust storms of the high desert locations led to increased cases of asthma and coccidioidomycosis, while the swampy, mosquito-infested Arkansas camps exposed residents to malaria, all of which were treated in camp. Almost 6,000 live deliveries were performed in these hospitals, and all mothers received pre- and postnatal care. The WRA recorded 1,862 deaths across the ten camps, with cancer, heart disease, tuberculosis, and vascular disease accounting for the majority.
Education in the camps.
Of the 110,000 Japanese Americans detained by the United States government during WWII, 30,000 were children. Most were school-age children, so educational facilities were set up in the camps. Allowing them to continue their education, however, did not erase the potential for traumatic experiences during their overall time in the camps. The government had not adequately planned for the camps, and no real budget or plan was set aside for the new camp educational facilities. Camp schoolhouses were crowded with insufficient materials, books, notebooks, and desks for students. These ‘schoolhouses’ were essentially prison blocks that contained few windows. In the Southwest, when temperatures rose and the schoolhouse filled, the rooms would be sweltering and unbearable. Class sizes were immense. At the height of it attendance, the Rohwer Camp of Arkansas reached 2,339, with only 45 certified teachers. The teacher to student ratio in the camps was 48:1 in elementary schools and 35:1 for secondary schools, compared to the national average of 28:1.
The rhetorical curriculum of the schools was based mostly on the study of “the democratic ideal and to discover its many implications.” English compositions researched at the Jerome and Rohwer camps in Arkansas focused on these ‘American ideals’, and many of the compositions pertained to the camps. Responses were varied, as schoolchildren of the Topaz camp were patriotic and believed in the war effort, but could not ignore the fact of their incarceration. To build patriotism, the Japanese language was banned in the camps, forcing the children to learn English and then go home and teach their parents. 
Sports in the camps.
Although life in the camps was very difficult, Japanese Americans formed many different sports teams, including baseball and football teams. In January 1942, President Franklin D. Roosevelt issued what came to be known as the "Green Light Letter," to MLB Commissioner Kenesaw Mountain Landis, which urged him to continue playing Major League Baseball games despite the ongoing war. In it Roosevelt said that "baseball provides a recreation," and this was true for Japanese American incarcerees as well. Over 100 baseball teams were formed in the Manzanar camp so that Japanese Americans could have some recreation, and some of the team names were carry-overs from teams formed before the incarceration.
Both men and women participated in the sports. In some cases, the Japanese American baseball teams from the camps traveled to outside communities to play other teams. Incarcerees from Idaho competed in the state tournament in 1943, and there were games between the prison guards and the Japanese American teams. Branch Rickey, who would be responsible for bringing Jackie Robinson into Major League Baseball in 1947, sent a letter to all of the WRA camps expressing interest in scouting some of the Nisei players. In fall of 1943, three players tried out for the Brooklyn Dodgers in front of MLB scout George Sisler, however, none of them made the team.
Student leave to attend Eastern colleges.
Although most Nisei college students followed their families into camp, a small number tried to arrange for transfers to schools outside the exclusion zone in order to continue their education. Their initial efforts expanded as sympathetic college administrators and the American Friends Service Committee began to coordinate a larger student relocation program. The Friends petitioned WRA Director Milton Eisenhower to place college students in Eastern and Midwestern academic institutions. The National Japanese American Student Relocation Council was formed on May 29, 1942, and the AFSC administered the program.
By September 1942, after the initial roundup of Japanese Americans, 250 students from assembly centers and WRA camps were back at school. Their tuition, book costs and living expenses were absorbed by the U.S. government, private foundations and church scholarships, in addition to significant fundraising efforts led by Issei parents in camp. Outside camp, the students took on the role of "ambassadors of good will," and the NJASRC and WRA promoted this image to soften anti-Japanese prejudice and prepare the public for the resettlement of Japanese Americans in their communities. At Earlham College, President William Dennis helped institute a program that enrolled several dozen Japanese-American students in order to spare them from incarceration. While this action was controversial in Richmond, Indiana, it helped strengthen the college's ties to Japan and the Japanese-American community. At Oberlin College, about 40 evacuated Nisei students were enrolled. One of them, Kenji Okuda, was elected as student council president. In total, over 600 institutions east of the exclusion zone opened their doors to more than 4,000 college-age youth who had been placed behind barbed wire, many of whom were enrolled in West Coast schools prior to their removal. The NJASRC ceased operations on June 7, 1946.
Loyalty questions and segregation.
In late 1943, War Relocation Authority officials, working with the War Department and the Office of Naval Intelligence, circulated a question form in an attempt to determine the loyalty of incarcerated Nisei men they hoped to recruit into military service. The "Statement of United States Citizen of Japanese Ancestry" was initially given only to Nisei who were eligible for service (or would have been, but for the 4-C classification imposed on them at the start of the war). Authorities soon revised the questionnaire and required all adults in camp to complete the form. Most of the 28 questions were designed to assess the "Americanness" of the respondent — had they been educated in Japan or the U.S.? were they Buddhist or Christian? did they practice "judo" or play on a baseball team? The final two questions on the form, which soon came to be known as the "loyalty questionnaire," were more direct: Question 27 asked whether an individual would be willing to serve in the armed forces (or, for women, the Auxiliary Corps), while Question 28 asked them to forswear their allegiance to the Emperor of Japan. Across the camps, persons who answered No to both questions became known as "No Nos."
While most camp inmates simply answered "yes" to both questions, several thousand — 17 percent of the total respondents, 20 percent of the Nisei — gave negative or qualified replies out of confusion, fear or anger at the wording and implications of the questionnaire. In regard to Question 27, many worried that expressing a willingness to serve would be equated with volunteering for combat, while others felt insulted at being asked to risk their lives for a country that had imprisoned them and their families. An affirmative answer to Question 28 brought up other issues. Some believed that renouncing their loyalty to Japan would suggest that they had at some point been loyal to Japan and disloyal to the United States. Many believed they were to be deported to Japan no matter how they answered; they feared an explicit disavowal of the Emperor would become known and make such resettlement extremely difficult.
On July 15, 1943, Tule Lake, the site with the highest number of "no" responses to the questionnaire, was designated to house inmates whose answers suggested they were "disloyal". During the remainder of 1943 and into early 1944, more than 12,000 men, women and children were transferred from other camps to the maximum-security Tule Lake Segregation Center.
After these insults, the government passed the Renunciation Act of 1944, a law that made it possible for Nisei and Kibei to renounce their American citizenship. A total of 5,589 internees opted to do so; 5,461 of these were sent to Tule Lake. Of those who renounced US citizenship, 1,327 were repatriated to Japan. Those persons who stayed in the US faced discrimination from the Japanese-American community, both during and after the war, for having made that choice of renunciation. At the time, they feared what their futures held were they to remain American, and remain interned.
These renunciations of American citizenship have been highly controversial, for a number of reasons. Some apologists for internment have cited the renunciations as evidence that "disloyalty" or anti-Americanism was well represented among the interned peoples, thereby justifying the internment. Many historians have dismissed the latter argument, for its failure to consider that the small number of individuals in question had been mistreated and persecuted by their own government at the time of the "renunciation":
[T]he renunciations had little to do with "loyalty" or "disloyalty" to the United States, but were instead the result of a series of complex conditions and factors that were beyond the control of those involved. Prior to discarding citizenship, most or all of the renunciants had experienced the following misfortunes: forced removal from homes; loss of jobs; government and public assumption of disloyalty to the land of their birth based on race alone; and incarceration in a "segregation center" for "disloyal" ISSEI or NISEI...
Minoru Kiyota, who was among those who renounced his citizenship and soon came to regret the decision, has said that he wanted only "to express my fury toward the government of the United States," for his internment and for the mental and physical duress, as well as the intimidation, he was made to face.
[M]y renunciation had been an expression of momentary emotional defiance in reaction to years of persecution suffered by myself and other Japanese Americans and, in particular, to the degrading interrogation by the FBI agent at Topaz and being terrorized by the guards and gangs at Tule Lake.
Civil rights attorney Wayne M. Collins successfully challenged most of these renunciations as invalid, owing to the conditions of duress and intimidation under which the government obtained them. Many of the deportees were Issei (first generation) or Kibei, who often had difficulty with English and often did not understand the questions they were asked. Even among those Issei who had a clear understanding, Question 28 posed an awkward dilemma: Japanese immigrants were denied U.S. citizenship at the time, so when asked to renounce their Japanese citizenship, answering "Yes" would have made them stateless persons.
When the government began seeking army volunteers from among the camps, only 6% of military-aged male inmates volunteered to serve in the U.S. Armed Forces. Most of those who refused tempered that refusal with statements of willingness to fight if they were restored their rights as American citizens. Eventually 20,000 Japanese-American men and many Japanese-American women served in the U.S. Army during World War II.
The 442nd Regimental Combat Team, which fought in Europe, was formed from those Japanese Americans who agreed to serve. This unit was the most highly decorated U.S. military unit of its size and duration. The 442nd's "Nisei" segregated field artillery battalion, then on detached service within the U.S. Army in Bavaria, liberated at least one of the satellite labor camps of the Nazis' original concentration camp at Dachau on April 29, 1945.
Proving commitment to the United States.
Many Nisei worked to prove themselves as loyal American citizens. Of the 20,000 Japanese Americans who served in the Army during World War II, “many Japanese-American soldiers had gone to war to fight racism at home” and they were “proving with their blood, their limbs, and their bodies that they were truly American,”. It was not only men either, some one hundred Nisei women volunteered for the WAC (Woman's Army Corps), where, after undergoing rigorous basic training, they had assignments as typists, clerks, and drivers. Satoshi Ito, an internment camp survivor, reinforces the idea of the immigrants’ children striving to demonstrate their patriotism to the United States. He notes that his mother would tell him, “‘you’re here in the United States, you need to do well in school, you need to prepare yourself to get a good job when you get out into the larger society’”. He said she would tell him, “‘don’t be a dumb farmer like me, like us’” to encourage Ito to successfully assimilate into American society. As a result, he worked exceptionally hard to excel in school and later became a professor at the College of William & Mary. His story, along with the countless Japanese Americans willing to risk their lives in war, demonstrate the lengths many in their community went to prove their American patriotism.
Other detention camps.
As early as 1939, when war broke out in Europe and while armed conflict began to rage in East Asia, the FBI and branches of the Department of Justice and the armed forces began to collect information and surveil influential members of the Japanese community in the United States. These data were included in the Custodial Detention index (CDI). Agents in the Department of Justice's Special Defense Unit classified the subjects into three groups: A, B and C, with A being "most dangerous," and C being "possibly dangerous."
After the Pearl Harbor attacks, Roosevelt authorized his attorney general to put into motion a plan for the arrest of individuals on the potential enemy alien lists. Armed with a blanket arrest warrant, the FBI seized these men on the eve of December 8, 1941. These men were held in municipal jails and prisons until they were moved to Department of Justice detention camps, separate from those of the Wartime Relocation Authority (WRA). These camps operated under far more stringent conditions and were subject to heightened criminal-style guards, despite the absence of criminal proceedings.
Crystal City, Texas, was one such camp where Japanese Americans, German Americans, Italian Americans, and a large number of U.S.-seized, Axis-descended nationals from several Latin-American countries were interned.
The Canadian government also confined citizens with Japanese ancestry during World War II (see Japanese Canadian internment), for much the same reasons of fear and prejudice. Some Latin American countries of the Pacific Coast, such as Peru, interned ethnic Japanese or sent them to the United States for internment. Brazil also restricted its Japanese Brazilians.
Hawaii.
Although Japanese Americans in Hawaii comprised more than one third of the population, businessmen resisted their being interned or deported to mainland concentration camps, as they recognized their contributions to the economy. In the hysteria of the time, some mainland Congressmen (Hawaii was only a U.S. territory at the time, and did not have a voting representative or senator in Congress) promoted that all Japanese Americans and Japanese immigrants should be removed from Hawaii but were unsuccessful. An estimated 1,200 to 1,800 Japanese nationals and American-born Japanese from Hawaii were interned, either in five camps on the islands or in one of the mainland internment camps, but this represented well under two percent of the total Japanese American residents in the islands. “No serious explanations were offered as to why ... the internment of individuals of Japanese descent was necessary on the mainland, but not in Hawaii, where the large Japanese-Hawaiian population went largely unmolested.”
The vast majority of Japanese Americans and their immigrant parents in Hawaii were not interned because the government had already declared martial law in Hawaii and this allowed it to significantly reduce the supposed risk of espionage and sabotage by residents of Japanese ancestry. Also, Japanese Americans comprised over 35% of the territory's population, with 157,905 of Hawaii's 423,330 inhabitants at the time of the 1940 census, making them the largest ethnic group at that time; detaining so many people would have been enormously challenging in terms of logistics. Additionally, the whole of Hawaiian society was dependent on their productivity. According to intelligence reports at the time, "the Japanese, through a concentration of effort in select industries, had achieved a virtual stranglehold on several key sectors of the economy in Hawaii,” and they "had access to virtually all jobs in the economy, including high-status, high-paying jobs (e.g., professional and managerial jobs).” To imprison such a large percentage of the islands' work force would have crippled the Hawaiian economy. Thus, the unfounded fear of Japanese Americans turning against the United States was overcome by the reality-based fear of massive economic loss.
Lieutenant General Delos C. Emmons, commander of the Hawaii Department, promised the local Japanese-American community that they would be treated fairly so long as they remained loyal to the United States. He succeeded in blocking efforts to relocate them to the outer islands or mainland by pointing out the logistical difficulties. Among the small number interned were community leaders and prominent politicians, including territorial legislators Thomas Sakakihara and Sanji Abe.
A total of five internment camps operated in the territory of Hawaii, referred to as the "Hawaiian Island Detention Camps". One camp was located at Sand Island at the mouth of Honolulu Harbor. This camp was prepared in advance of the war's outbreak. All prisoners held here were "detained under military custody... because of the imposition of martial law throughout the Islands". Another Hawaiian camp was the Honouliuli Internment Camp, near Ewa, on the southwestern shore of Oahu; it was opened in 1943 to replace the Sand Island camp. Another was located on the island of Maui in the town of Haiku, in addition to the Kilauea Detention Center on Hawaii and Camp Kalaheo on Kauai.
Japanese Latin Americans.
During World War II, over 2,200 Japanese from Latin America were held in internment camps run by the Immigration and Naturalization Service, part of the Department of Justice. Beginning in 1942, Latin Americans of Japanese ancestry were rounded up and transported to American internment camps run by the INS and the U.S. Justice Department. The majority of these internees, approximately 1,800, came from Peru. An additional 250 were from Panama, and Bolivia, Colombia, Costa Rica, Cuba, Ecuador, El Salvador, Mexico, Nicaragua, and Venezuela.
The first group of Japanese Latin Americans arrived in San Francisco on April 20, 1942, on board the "Etolin" along with 360 ethnic Germans and 14 ethnic Italians from Peru, Ecuador and Colombia. The 151 men — ten from Ecuador, the rest from Peru — had volunteered for deportation believing they were to be repatriated to Japan. They were denied visas by U.S. Immigration authorities and then detained on the grounds they had tried to enter the country illegally, without a visa or passport. Subsequent transports brought additional "volunteers," including the wives and children of men who had been deported earlier. A total of 2,264 Japanese Latin Americans, about two-thirds of them from Peru, were interned in facilities on the U.S. mainland during the war.
The United States originally intended to trade these Latin American internees as part of a hostage exchange program with Japan and other Axis nations. A thorough examination of the documents shows at least one trade occurred. Over 1,300 persons of Japanese ancestry were exchanged for a like number of non-official Americans in October 1943, at the port of Marmagao, India. Over half were Japanese Latin Americans (the rest being ethnic Germans and Italians) and of that number one-third were Japanese Peruvians.
On September 2, 1943, the Swedish ship "MS Gripsholm" departed the U.S. with just over 1,300 Japanese nationals (including nearly a hundred from Canada and Mexico) en route for the exchange location, Marmagao, the main port of the Portuguese colony of Goa on the west coast of India.
 After two more stops in South America to take on additional Japanese nationals, the passenger manifest reached 1,340. Of that number, Latin American Japanese numbered 55 percent of the Gripsholm's travelers, 30 percent of whom were Japanese Peruvian. Arriving in Marmagao on October 16, 1943, the Gripsholm's passengers disembarked and then boarded the Japanese ship "Teia Maru." In return, "non-official" Americans (secretaries, butlers, cooks, embassy staff workers, etc.) previously held by the Japanese Army boarded the "Gripsholm" while the "Teia Maru" headed for Tokyo. Because this exchange was done with those of Japanese ancestry officially described as "volunteering" to return to Japan, no legal challenges were encountered. The U.S. Department of State was pleased with the first trade and immediately began to arrange a second exchange of non-officials for February 1944. This exchange would involve 1,500 non-volunteer Japanese who were to be exchanged for 1,500 Americans. The US was busy with Pacific Naval activity and future trading plans stalled. Further slowing the program were legal and political "turf" battles between the State Department, the Roosevelt administration, and the DOJ, whose officials were not convinced of the legality of the program.
The completed October 1943 trade took place at the height of the Enemy Alien Deportation Program. Japanese Peruvians were still being "rounded up" for shipment to the U.S. in previously unseen numbers. Despite logistical challenges facing the floundering prisoner exchange program, deportation plans were moving ahead. This is partly explained by an early-in-the-war revelation of the overall goal for Latin Americans of Japanese ancestry under the Enemy Alien Deportation Program. The goal: that the hemisphere was to be free of Japanese. Secretary of State Cordell Hull wrote an agreeing President Roosevelt, "[that the US must] continue our efforts to remove all the Japanese from these American Republics for internment in the United States."
"Native" Peruvians expressed extreme animosity toward their Japanese citizens and expatriates, and Peru refused to accept the post-war return of Japanese Peruvians from the US. Although a small number asserting special circumstances, such as marriage to a non-Japanese Peruvian, did return, the majority were trapped. Their home country refused to take them back (a political stance Peru would maintain until 1950), they were generally Spanish speakers in the Anglo US, and in the postwar U.S., the Department of State started expatriating them to Japan. ACLU lawyer Wayne Collins filed injunctions on behalf of the remaining internees, helping them obtain "parole" relocation to the labor-starved Seabrook Farms in New Jersey. He started a legal battle that would not be resolved until 1953, when, after working as undocumented immigrants for almost ten years, those Japanese Peruvians remaining in the U.S. were finally offered citizenship.
Internment ends.
On December 18, 1944, the Supreme Court handed down two decisions on the legality of the incarceration under Executive Order 9066. "Korematsu v. United States", a 6–3 decision upholding a Nisei's conviction for violating the military exclusion order, stated that, in general, the removal of Japanese Americans from the West Coast was constitutional. However, "Ex parte Endo" unanimously declared that loyal citizens of the United States, regardless of cultural descent, could not be detained without cause. In effect, the two rulings held that, while the eviction of U.S. citizens in the name of military necessity was legal, the subsequent incarceration was not — thus paving the way for their release.
Although WRA Director Dillon Myer and others had pushed for an earlier end to the incarceration, the exclusion order was not rescinded until January 2, 1945 (postponed until after the November 1944 election, so as not to impede Roosevelt's reelection campaign). Many younger internees had already "resettled" in Midwest or Eastern cities to pursue work or educational opportunities. The remaining population began to leave the camps to try to rebuild their lives at home. Former inmates were given $25 and a train ticket to their pre-war places of residence, but many had little or nothing to return to, having lost their homes and businesses. Some emigrated to Japan, although many of these individuals were "repatriated" against their will. The camps remained open for residents who were not ready to return (mostly elderly Issei and families with young children), but the WRA pressured stragglers to leave by gradually eliminating services in camp. Those who had not left by each camp's close date were forcibly removed and sent back to the West Coast.
Nine of the ten WRA camps were shut down by the end of 1945, although Tule Lake, which held "renunciants" slated for deportation to Japan, was not closed until March 20, 1946. Japanese Latin Americans brought to the U.S. from Peru and other countries, who were still being held in the DOJ camps at Santa Fe and Crystal City, took legal action in April 1946 in an attempt to avoid deportation to Japan.
Following recognition of the injustices done to the Japanese Americans, in 1992 Manzanar camp was designated a National Historic Site to "provide for the protection and interpretation of historic, cultural, and natural resources associated with the relocation of Japanese Americans during World War II" (Public Law 102-248). In 2001, the site of the Minidoka War Relocation Center in Idaho was designated the Minidoka National Historic Site.
Hardship and material loss.
Many internees lost irreplaceable personal property due to restrictions that prohibited them from taking more than they could carry into the camps. These losses were compounded by theft and destruction of items placed in governmental storage. Leading up to their incarceration, Nikkei were prohibited from leaving the Military Zones or traveling more than 5 mi from home, forcing those who had to travel for work, like truck farmers and residents of rural towns, to quit their jobs. Many others were simply fired for their "Jap" heritage.
Alien land laws in the West Coast states barred the Issei from owning their pre-war homes and farms. Many had cultivated land for decades as tenant farmers, but they lost their rights to farm those lands when they were forced to leave. Other Issei (and Nisei who were renting or had not completed payments on their property) had found families willing to occupy their homes or tend their farms during their incarceration. However, those unable to strike a deal with caretakers had to sell their property, often in a matter of days and at great financial loss to predatory land speculators, who made huge profits.
In addition to these monetary and property losses, a number of persons died or suffered for lack of medical care in camp. Seven were shot and killed by sentries: Kanesaburo Oshima, 58, during an escape attempt from Fort Sill, Oklahoma; Toshio Kobata, 58, and Hirota Isomura, 59, during transfer to Lordsburg, New Mexico; James Ito, 17, and Katsuji James Kanegawa, 21, during the December 1942 Manzanar Riot; James Hatsuaki Wakasa, 65, while walking near the perimeter wire of Topaz; and Shoichi James Okamoto, 30, during a verbal altercation with a sentry at the Tule Lake Segregation Center.
Psychological injury was observed by Dillon S. Myer, director of the WRA camps. In June 1945, Myer described how the Japanese Americans had grown increasingly depressed, and overcome with feelings of helplessness and personal insecurity. Author Betty Furuta explains that the Japanese used "gaman," loosely meaning "perseverance", to overcome hardships; this was mistaken by non-Japanese as being introverted and lacking initiative.
Japanese Americans also encountered hostility and even violence when they returned to the West Coast. Concentrated largely in rural areas of Central California, there were dozens of reports of gun shots, fires, and explosions aimed at Japanese American homes, businesses and places of worship, in addition to non-violent crimes like vandalism and the defacing of Japanese graves. In one of the only cases to go to trial, four men were accused of attacking the Doi family of Placer County, California, setting off an explosion and starting a fire on the family's farm in January 1945. Despite a confession from one of the men that implicated the others, the jury accepted their defense attorney's framing of the attack as a justifiable attempt to keep California "a white man's country" and acquitted all four defendants.
To compensate former internees for their property losses, the US Congress, on July 2, 1948, passed the "American Japanese Claims Act," allowing Japanese Americans to apply for compensation for property losses which occurred as "a reasonable and natural consequence of the evacuation or exclusion." By the time the Act was passed, the IRS had already destroyed most of the internees' 1939–42 tax records. Due to the time pressure and strict limits on how much they could take to the camps, few were able to preserve detailed tax and financial records during the evacuation process. Therefore, it was extremely difficult for claimants to establish that their claims were valid. Under the Act, Japanese American families filed 26,568 claims totaling $148 million in requests; about $37 million was approved and disbursed.
Reparations and redress.
Beginning in the 1960s, a younger generation of Japanese Americans, inspired by the Civil Rights movement, began what is known as the "Redress Movement," an effort to obtain an official apology and reparations from the federal government for incarcerating their parents and grandparents during the war. They focused not on documented property losses but on the broader injustice and mental suffering caused by the internment. The movement's first success was in 1976, when President Gerald Ford proclaimed that the internment was "wrong," and a "national mistake" which "shall never again be repeated".
The campaign for redress was launched by Japanese Americans in 1978. The Japanese American Citizens League (JACL), which had cooperated with the administration during the war, became part of the movement. It asked for three measures: $25,000 to be awarded to each person who was detained, an apology from Congress acknowledging publicly that the U.S. government had been wrong, and the release of funds to set up an educational foundation for the children of Japanese-American families.
In 1980, Congress established the Commission on Wartime Relocation and Internment of Civilians (CWRIC) to study the matter. On February 24, 1983, the commission issued a report entitled "Personal Justice Denied", condemning the internment as unjust and motivated by racism and xenophobic ideas rather than factual military necessity. The Commission recommended that $20,000 in reparations be paid to those Japanese Americans who had suffered internment.
In 1988, U.S. President Ronald Reagan signed the Civil Liberties Act of 1988, which had been sponsored by Representative Norman Mineta and Senator Alan K. Simpson, who had met while Mineta was interned at Heart Mountain Relocation Center in Wyoming. It provided financial redress of $20,000 for each surviving detainee, totaling $1.2 billion. The question of to whom reparations should be given, how much, and even whether monetary reparations were appropriate were subjects of sometimes contentious debate within the Japanese-American community and Congress.
On September 27, 1992, the Civil Liberties Act Amendments of 1992, appropriating an additional $400 million to ensure all remaining internees received their $20,000 redress payments, was signed into law by President George H. W. Bush. He issued another formal apology from the U.S. government on December 7, 1991, on the 50th-Anniversary of the Pearl Harbor Attack, saying:
"In remembering, it is important to come to grips with the past. No nation can fully understand itself or find its place in the world if it does not look with clear eyes at all the glories and disgraces of its past. We in the United States acknowledge such an injustice in our history. The internment of Americans of Japanese ancestry was a great injustice, and it will never be repeated."
Under the 2001 budget of the United States, Congress authorized that the ten detention sites are to be preserved as historical landmarks: “places like Manzanar, Tule Lake, Heart Mountain, Topaz, Amache, Jerome, and Rohwer will forever stand as reminders that this nation failed in its most sacred duty to protect its citizens against prejudice, greed, and political expediency”.
On January 30, 2011, California first observed an annual "Fred Korematsu Day of Civil Liberties and the Constitution", the first such commemoration for an Asian American in the U.S. On June 14, 2011, Peruvian president Alan García apologized for his country's internment of Japanese immigrants during World War II, most of whom were transferred to the United States.
Legal legacy.
Several significant legal decisions arose out of Japanese-American internment, relating to the powers of the government to detain citizens in wartime. Among the cases which reached the US Supreme Court were "Yasui v. United States" (1943), "Hirabayashi v. United States" (1943), "ex parte Endo" (1944), and "Korematsu v. United States" (1944). In "Yasui" and "Hirabayashi," the court upheld the constitutionality of curfews based on Japanese ancestry; in "Korematsu," the court upheld the constitutionality of the exclusion order. In "Endo", the court accepted a petition for a writ of habeas corpus and ruled that the WRA had no authority to subject a loyal citizen to its procedures.
Korematsu's and Hirabayashi's convictions were vacated in a series of "coram nobis" cases in the early 1980s. In the "coram nobis" cases, federal district and appellate courts ruled that newly uncovered evidence revealed an unfairness which, had it been known at the time, would likely have changed the Supreme Court's decisions in the Yasui, Hirabayashi, and Korematsu cases.
These new court decisions rested on a series of documents recovered from the National Archives showing that the government had altered, suppressed and withheld important and relevant information from the Supreme Court, including the Final Report by General DeWitt justifying the internment program. The Army had destroyed documents in an effort to hide the fact that alterations had been made to the report to reduce their racist content. The "coram nobis" cases vacated the convictions of Korematsu and Hirabayashi (Yasui died before his case was heard, rendering it moot), and are regarded as part of the impetus to gain passage of the Civil Liberties Act of 1988.
The rulings of the US Supreme Court in the Korematsu and Hirabayashi cases, specifically in its expansive interpretation of government powers in wartime, have yet to be overturned. They are still the law of the land because a lower court cannot overturn a ruling by the US Supreme Court. The "coram nobis" cases totally undermined the factual underpinnings of the 1944 cases, leaving the original decisions without much logical basis. As these 1944 decisions prevail, a number of legal scholars have expressed the opinion that the original Korematsu and Hirabayashi decisions have taken on renewed relevance in the context of the War on Terror.
Former Supreme Court Justice Tom C. Clark, who represented the US Department of Justice in the "relocation," writes in the epilogue to the 1992 book "Executive Order 9066: The Internment of 110,000 Japanese Americans":
The truth is—as this deplorable experience proves—that constitutions and laws are not sufficient of themselves...Despite the unequivocal language of the Constitution of the United States that the writ of habeas corpus shall not be suspended, and despite the Fifth Amendment's command that no person shall be deprived of life, liberty or property without due process of law, both of these constitutional safeguards were denied by military action under Executive Order 9066.
Terminology debate.
Since the end of World War II, there has been debate over the terminology used to refer to camps in which Americans of Japanese ancestry and their immigrant parents, were incarcerated by the United States Government during the war. These camps have been referred to as "War Relocation Centers," "relocation camps," "relocation centers," "internment camps", and "concentration camps", and the controversy over which term is the most accurate and appropriate continues to the present day.
Dr. James Hirabayashi, Professor Emeritus and former Dean of Ethnic Studies at San Francisco State University, wrote an article in 1994 in which he stated that he wonders why euphemistic terms are still used to describe these camps.
In 1998, use of the term "concentration camps" gained greater credibility prior to the opening of an exhibit about the American camps at Ellis Island. Initially, the American Jewish Committee (AJC) and the National Park Service, which manages Ellis Island, objected to the use of the term in the exhibit. However, during a subsequent meeting held at the offices of the AJC in New York City, leaders representing Japanese Americans and Jewish Americans reached an understanding about the use of the term.
The "New York Times" published an unsigned editorial supporting the use of "concentration camp" in the exhibit. An article quoted Jonathan Mark, a columnist for "The Jewish Week", who wrote, "Can no one else speak of slavery, gas, trains, camps? It's Jewish malpractice to monopolize pain and minimize victims." AJC Executive Director David A. Harris stated during the controversy, "We have not claimed Jewish exclusivity for the term 'concentration camps.'"
On July 7, 2012, at their annual convention, the National Council of the Japanese American Citizens League unanimously ratified the "Power of Words Handbook," calling for the use of "...truthful and accurate terms, and retiring the misleading euphemisms created by the government to cover up the denial of Constitutional and human rights, the force, oppressive conditions, and racism against 120,000 innocent people of Japanese ancestry locked up in America's World War II concentration camps." 
Expulsions and population transfers of World War II.
The internment of Japanese Americans has sometimes been compared to the persecutions, expulsions, and dislocations of other ethnic minorities in the context of World War II, in Europe and Asia. An estimated 500,000 Volga Germans were rounded up and deported to Siberia and Kazakhstan when Germany invaded the Soviet Union, with many of them dying en route. In 1942, nearly all the able-bodied German population was conscripted to the NKVD labor columns. About one-third did not survive the camps.
The Volga Germans were deported prior to the Battle of Stalingrad, as they were regarded, in the "war hysteria of the moment", as a potential "Fifth Column".
In 1944, the Red Army rounded up about 500,000 Chechens and Ingushes for relocation; a third of this population perished in the first year, from starvation, cold, and disease. Other nationalities which faced ethnic cleansing for having been identified as potential collaborators with the Germans were the Balkars, Crimean Tatars, Karachi, Kalmyks, and Meskhetians.
Exhibitions and collections.
The Smithsonian Institution's National Museum of American History has more than 800 artifacts from its A More Perfect Union collection available online. Archival photography, publications, original manuscripts, artworks, and handmade objects comprise the collection of items related to the Japanese American experience.
On October 1, 1987, the Smithsonian Institution National Museum of American History opened an exhibition called, "A More Perfect Union: Japanese Americans and the U.S. Constitution." The exhibition examined the Constitutional process by considering the experiences of Americans of Japanese ancestry before, during, and after World War II. On view were more than 1,000 artifacts and photographs relating to the experiences of Japanese Americans during World War II. The exhibition closed on January 11, 2004. On November 8, 2011, the National Museum of American History launched an online exhibition of the same name with shared content.
On April 16, 2013, the Japanese American Internment Museum was opened in McGehee, Arkansas regarding the history at two internment camps.
Representation in other media.
°The 2002 novel "When the Emperor was Divine" by Julie Otsuka tells the story of an unnamed Japanese family who was incarcerated at the Topaz War Relocation Center in Utah. The novel is based on Otsuka's own family's experiences.
Further reading.
</dl>
External links.
Other sources.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="46886" url="http://en.wikipedia.org/wiki?curid=46886" title="Japanese relocation">
Japanese relocation

Japaneses relocation may refer to:

</doc>
<doc id="46889" url="http://en.wikipedia.org/wiki?curid=46889" title="Leaning Tower of Pisa">
Leaning Tower of Pisa

The Leaning Tower of Pisa (Italian: "Torre pendente di Pisa") or simply the Tower of Pisa ("Torre di Pisa") is the campanile, or freestanding bell tower, of the cathedral of the Italian city of Pisa, known worldwide for its unintended tilt to one side. It is situated behind the Cathedral and is the third oldest structure in Pisa's Cathedral Square ("Piazza del Duomo") after the Cathedral and the Baptistry. The tower's tilt began during construction, caused by an inadequate foundation on ground too soft on one side to properly support the structure's weight. The tilt increased in the decades before the structure was completed, and gradually increased until the structure was stabilized (and the tilt partially corrected) by efforts in the late 20th and early 21st centuries.
The height of the tower is 55.86 m from the ground on the low side and 56.67 m on the high side. The width of the walls at the base is 2.44 m. Its weight is estimated at 14500 metric ton. The tower has 296 or 294 steps; the seventh floor has two fewer steps on the north-facing staircase. Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now leans at about 3.99 degrees. This means that the top of the tower is displaced horizontally 3.9 m from where it would be if the structure were perfectly vertical.
Architect.
There has been controversy about the real identity of the architect of the Leaning Tower of Pisa. For many years, the design was attributed to Guglielmo and Bonanno Pisano, a well-known 12th-century resident artist of Pisa, famous for his bronze casting, particularly in the Pisa Duomo. Bonanno Pisano left Pisa in 1185 for Monreale, Sicily, only to come back and die in his home town. A piece of cast with his name was discovered at the foot of the tower in 1820, but this may be related to the bronze door in the façade of the cathedral that was destroyed in 1595. However, recent studies seem to indicate Diotisalvi as the original architect due to the time of construction and affinity with other Diotisalvi works, notably the bell tower of San Nicola and the Baptistery, both in Pisa. However, he usually signed his works and there is no signature by him in the bell tower which leads to further speculation.
Construction.
Construction of the tower occurred in three stages across 199 years. Work on the ground floor of the white marble campanile began on August 14, 1173, during a period of military success and prosperity. This ground floor is a blind arcade articulated by engaged columns with classical Corinthian capitals.
The tower began to sink after construction had progressed to the second floor in 1178. This was due to a mere three-metre foundation, set in weak, unstable subsoil, a design that was flawed from the beginning. Construction was subsequently halted for almost a century, because the Republic of Pisa was almost continually engaged in battles with Genoa, Lucca, and Florence. This allowed time for the underlying soil to settle. Otherwise, the tower would almost certainly have toppled. In 1198 clocks were temporarily installed on the third floor of the unfinished construction.
In 1272 construction resumed under Giovanni di Simone, architect of the Camposanto. In an effort to compensate for the tilt, the engineers built upper floors with one side taller than the other. Because of this, the tower is actually curved. Construction was halted again in 1284, when the Pisans were defeated by the Genoans in the Battle of Meloria.
The seventh floor was completed in 1319. It was built by Tommaso di Andrea Pisano, who succeeded in harmonizing the Gothic elements of the bell-chamber with the Romanesque style of the tower. The bell-chamber was finally added in 1372. There are seven bells, one for each note of the musical major scale. The largest one was installed in 1655. 
After a phase (1990–2001) of structural strengthening, the tower is currently undergoing gradual surface restoration, in order to repair visible damage, mostly corrosion and blackening. These are particularly pronounced due to the tower's age and its exposure to wind and rain.
History following construction.
Galileo Galilei is said to have dropped two cannonballs of different masses from the tower to demonstrate that their speed of descent was independent of their mass. However, this is considered an apocryphal tale, its only source being Galileo's secretary.
During World War II, the Allies discovered that the Germans were using the tower as an observation post. A U.S. Army sergeant sent to confirm the presence of German troops in the tower was impressed by the beauty of the cathedral and its campanile, and thus refrained from ordering an artillery strike, sparing it from destruction.
On February 27, 1964, the government of Italy requested aid in preventing the tower from toppling. It was, however, considered important to retain the current tilt, due to the role that this element played in promoting the tourism industry of Pisa.
A multinational task force of engineers, mathematicians, and historians gathered on the Azores islands to discuss stabilisation methods. It was found that the tilt was increasing in combination with the softer foundations on the lower side. Many methods were proposed to stabilise the tower, including the addition of 800 tonnes of lead counterweights to the raised end of the base.
In 1987 the tower was included in the Piazza del Duomo UNESCO World Heritage Site along with the neighbouring cathedral, baptistery and cemetery.
On January 7, 1990, after over two decades of stabilisation studies, and spurred by the abrupt collapse of the Civic Tower of Pavia in 1989, the tower was closed to the public. The bells were removed to relieve some weight, and cables were cinched around the third level and anchored several hundred meters away. Apartments and houses in the path of the tower were vacated for safety. The final solution to prevent the collapse of the tower was to slightly straighten the tower to a safer angle, by removing 38 m3 of soil from underneath the raised end. The tower was straightened by 45 cm, returning to its 1838 position. After a decade of corrective reconstruction and stabilization efforts, the tower was reopened to the public on December 15, 2001, and was declared stable for at least another 300 years.
In May 2008, after the removal of another 70 metric ton of ground, engineers announced that the Tower had been stabilized such that it had stopped moving for the first time in its history. They stated it would be stable for at least 200 years.
Alternative candidates.
Two German churches have challenged the tower's status as the world's most lop-sided building: the 15th-century square Leaning Tower of Suurhusen and the 14th-century bell tower in the town of Bad Frankenhausen. "Guinness World Records" measured the Pisa and Suurhusen towers, finding the former's tilt to be 3.97 degrees. In June 2010, "Guinness World Records" certified the Capital Gate building in Abu Dhabi, UAE as the "World's Furthest Leaning Man-made Tower". The Capital Gate tower has an 18-degree slope, almost five times more than the Pisa Tower; however the Capital Gate tower has been deliberately engineered to slant. The Leaning Tower of Wanaka in New Zealand, also deliberately built, leans at 53 degrees to the ground.
Technical information.
About the 5th bell: The name "Pasquareccia" comes from "Easter", because it used to ring on Easter day. However, this bell is older than the bell-chamber itself, and comes from the tower Vergata in "Palazzo Pretorio" in Pisa, where it was called "La Giustizia" (The Justice). The bell was tolled to announce executions of criminals and traitors, including Count Ugolino in 1289.
A new bell was installed in the bell tower at the end of the 18th century to replace the broken "Pasquareccia".

</doc>
<doc id="46890" url="http://en.wikipedia.org/wiki?curid=46890" title="Frequency-hopping spread spectrum">
Frequency-hopping spread spectrum

Frequency-hopping spread spectrum (FHSS) is a method of transmitting radio signals by rapidly switching a carrier among many frequency channels, using a pseudorandom sequence known to both transmitter and receiver. It is used as a multiple access method in the frequency-hopping code division multiple access (FH-CDMA) scheme.
Spread-spectrum.
A spread-spectrum transmission offers three main advantages over a fixed-frequency transmission:
Military use.
Spread-spectrum signals are highly resistant to deliberate jamming, unless the adversary has knowledge of the spreading characteristics. Military radios use cryptographic techniques to generate the channel sequence under the control of a secret Transmission Security Key (TRANSEC) that the sender and receiver share in advance.
By itself, frequency hopping provides only limited protection against eavesdropping and jamming. Most modern military frequency hopping radios also employ separate encryption devices such as the KY-57. U.S. military radios that use frequency hopping include the JTIDS/MIDS family, HAVE QUICK and SINCGARS
Civilian use.
In the US, since the Federal Communications Commission (FCC) amended rules to allow frequency hopping spread spectrum systems in the unregulated 2.4 GHz band, many consumer devices in that band have employed various spread-spectrum modes.
Some walkie-talkies that employ frequency-hopping spread spectrum technology have been developed for unlicensed use on the 900 MHz band. Several such radios are marketed under the name eXtreme Radio Service (eXRS). Despite the name's similarity to the FRS allocation, the system is a proprietary design, rather than an official U.S. Federal Communications Commission (FCC) allocated service.
Motorola has deployed a business-banded, license-free digital radio that uses FHSS technology: the DTR series, models 410, 550 and 650.
Technical considerations.
The overall bandwidth required for frequency hopping is much wider than that required to transmit the same information using only one carrier frequency. However, because transmission occurs only on a small portion of this bandwidth at any given time, the effective interference bandwidth is really the same. Whilst providing no extra protection against wideband thermal noise, the frequency-hopping approach does reduce the degradation caused by narrowband interference sources.
One of the challenges of frequency-hopping systems is to synchronize the transmitter and receiver. One approach is to have a guarantee that the transmitter will use all the channels in a fixed period of time. The receiver can then find the transmitter by picking a random channel and listening for valid data on that channel. The transmitter's data is identified by a special sequence of data that is unlikely to occur over the segment of data for this channel and the segment can have a checksum for integrity and further identification. The transmitter and receiver can use fixed tables of channel sequences so that once synchronized they can maintain communication by following the table. On each channel segment, the transmitter can send its current location in the table.
In the US, FCC part 15 on unlicensed system in the 902–928 MHz and 2.4 GHz bands permits more power than non-spread-spectrum systems. Both frequency hopping and direct sequence systems can transmit at 1 Watt. The limit is increased from 1 milliwatt to 1 watt or a thousand times increase. The Federal Communications Commission (FCC) prescribes a minimum number of channels and a maximum dwell time for each channel.
In a real multipoint radio system, space allows multiple transmissions on the same frequency to be possible using multiple radios in a geographic area. This creates the possibility of system data rates that are higher than the Shannon limit for a single channel. Spread spectrum systems do not violate the Shannon limit. Spread spectrum systems rely on excess signal to noise ratios for sharing of spectrum. This property is also seen in MIMO and DSSS systems. Beam steering and directional antennas also facilitate increased system performance by providing isolation between remote radios.
Multiple inventors.
Perhaps the earliest mention of frequency hopping in the open literature is in radio pioneer Jonathan Zenneck's book "Wireless Telegraphy" (German, 1908, English translation McGraw Hill, 1915), although Zenneck himself states that Telefunken had already tried it.
The German military made limited use of frequency hopping for communication between fixed command points in World War I to prevent eavesdropping by British forces, who did not have the technology to follow the sequence.
A Polish engineer, Leonard Danilewicz, came up with the idea in 1929. Several other patents were taken out in the 1930s, including one by Willem Broertjes (U.S. Patent , issued Aug. 2, 1932).
During World War II, the US Army Signal Corps was inventing a communication system called SIGSALY, which incorporated spread spectrum in a single frequency context. However, SIGSALY was a top-secret communications system, so its existence did not become known until the 1980s.
The most celebrated invention of frequency hopping, though it came decades after others had come up with the concept and technologies making use of it were in existence, was a patent awarded to actress Hedy Lamarr and composer George Antheil, who in 1942 received U.S. Patent for their "Secret Communications System". This intended early version of frequency hopping was supposed to use a piano-roll to change among 88 frequencies, and was intended to make radio-guided torpedoes harder for enemies to detect or to jam, but there is no record of a working device ever being produced. The patent was rediscovered in the 1950s during patent searches when private companies independently developed Code Division Multiple Access, a non-frequency-hopping form of spread-spectrum.
Variations of FHSS.
"Adaptive Frequency-hopping spread spectrum (AFH)" (as used in Bluetooth) improves resistance to radio frequency interference by avoiding crowded frequencies in the hopping sequence. This sort of adaptive transmission is easier to implement with FHSS than with DSSS.
The key idea behind AFH is to use only the “good” frequencies, by avoiding the "bad" frequency channels—perhaps those "bad" frequency channels are experiencing frequency selective fading, or perhaps some third party is trying to communicate on those bands, or perhaps those bands are being actively jammed. Therefore, AFH should be complemented by a mechanism for detecting good/bad channels.
However, if the radio frequency interference is itself dynamic, then the strategy of “bad channel removal”, applied in AFH might not work well. For example, if there are several colocated frequency-hopping networks (as Bluetooth Piconet), then they are mutually interfering and the strategy of AFH fails to avoid this interference. 
The problem of dynamic interference, gradual reduction of available hopping channels and backward compatibility with legacy bluetooth devices was resolved in version 1.2 of the Bluetooth Standard (2003). Other Strategies for dynamic adaptation of the frequency hopping pattern have been reported in the literature. Such a situation can often happen in the scenarios that use unlicensed spectrum. 
In addition, dynamic radio frequency interference is expected to occur in the scenarios related to cognitive radio, where the networks and the devices should exhibit frequency-agile operation.
Chirp modulation can be seen as a form of frequency-hopping that simply scans through the available frequencies in consecutive order to communicate.

</doc>
<doc id="46892" url="http://en.wikipedia.org/wiki?curid=46892" title="Issei">
Issei

Issei (一世, "first generation")
 is a Japanese language term used in countries in North America, South America and Australia to specify the Japanese people first to immigrate. Their children born in the new country are referred to as "Nisei" (second generation), and their grandchildren are "Sansei" (third generation). All of them come from the numbers "one, two, three" in the Japanese language, as Japanese numerals are "ichi, ni, san."
The character and uniqueness of the "Issei" is recognized in its social history.
History.
Although the earliest organized group of Japanese emigrants settled in Mexico in 1897, the four largest populations of Japanese and descendants of Japanese immigrants live in Brazil, the United States, Canada and Peru.
Brazilian "Issei".
Brazil is home to the largest Japanese population outside of Japan, numbering an estimate of more than 1.5 million (including those of mixed-race or mixed-ethnicity), more than that of the 1.2 million in the United States. The "Issei" Japanese Brazilians are an important part of that ethnic minority in that South American nation.
American "Issei".
The first members of the "Issei" did not emigrate directly to the mainland United States, but to Hawaii (when it was American-controlled but not yet one of the United States). These emigrants — the first of whom arrived on board the steamship "City of Tokio" in February 1885 — were common laborers escaping hard times in Japan, and their emigration was subsidized by the Hawaiian government, which needed cheap labor for its sugar plantations. A large number of Japanese eventually settled in Hawaii.
Emigration of Japanese directly to the United States began a little later in 1885, with the arrival of "student-laborers". The earliest of these emigrated to San Francisco, and their numbers continually expanded in the late 1880s and early 1890s. Their purpose in moving to America was to gain advanced knowledge and experience in order to develop the modern society at home. Both students and laborers were attracted by the image of America as a country that welcomes foreigners. When they first arrived in the U.S., they had not intended to live there permanently, but rather to learn from Americans and to bring that knowledge back home.
Canadian "Issei".
Within Japanese-Canadian communities across Canada, three distinct subgroups developed, each with different sociocultural referents, generational identity, and wartime experiences. The narrative of "Issei" Japanese-Canadians include post-Pearl Harbor experiences of uprooting, incarceration, and dispersal of the pre-war Japanese-Canadian communities.
Peruvian "Issei".
Among the approximately 80,000 Peruvians of Japanese descent, the "Issei" Japanese Peruvians comprise only a small number. Former Peruvian President Alberto Fujimori was the "Nisei" son of "Issei" emigrants from Kumamoto, Japan. Fujimori's political opponents tried unsuccessfully to prove that he was actually born in Japan — in which case he would have been "Issei" like his immigrant mother and father.
Cultural profile.
Generations.
Japanese-Americans and Japanese-Canadians have special names for each of their generations in North America. These are formed by combining one of the Japanese numbers corresponding to the generation with the Japanese word for generation ("sei" 世). The Japanese-American and Japanese-Canadian communities have themselves distinguished their members with terms like "Issei", "Nisei," and "Sansei" which describe the first, second and third generation of immigrants. The fourth generation is called "Yonsei" (四世) and the fifth is called "Gosei" (五世). The "Issei," "Nisei" and "Sansei" generations reflect distinctly different attitudes to authority, gender, non-Japanese involvement, and religious belief and practice, and other matters. The age when individuals faced the wartime evacuation and internment is the single, most significant factor which explains these variations in their experiences, attitudes and behaviour patterns.
The term "Nikkei" (日系) was coined by a multinational group of sociologists and encompasses all of the world's Japanese immigrants across generations. The collective memory of the "Issei" and older "Nisei" was an image of Meiji Japan from 1870 through 1911, which contrasted sharply with the Japan that newer immigrants had more recently left. These differing attitudes, social values and associations with Japan were often incompatible with each other. In this context, the significant differences in post-war experiences and opportunities did nothing to mitigate the gaps which separated generational perspectives.
In North America since the redress victory in 1988, a significant evolutionary change has occurred. The Nisei, their parents and their children are changing the way they look at themselves and their pattern of accommodation to the non-Japanese majority.
There are just over one hundred thousand British Japanese, mostly in London. Unlike other "Nikkei" communities in the world, these Britons do not identify themselves in such generational terms as "Issei," "Nisei," or "Sansei."
Issei.
The first generation of immigrants, born in Japan before moving to Canada or the United States, is called "Issei" (一世). In the 1930s, the term "Issei" came into common use, replacing the term "immigrant" ("ijusha"). This new term illustrated a changed way of looking at themselves. The term "Issei" represented the idea of beginning, a psychological transformation relating to being settled, having a distinctive community, and the idea of belonging to the new country.
"Issei" settled in close ethnic communities, and therefore did not learn English. They endured great economic and social losses during the early years of World War II, and they were not able to rebuild their lost businesses and savings. The external circumstances tended to reinforce the pattern of "Issei" being predominantly friends with other "Issei."
Unlike their children, the tend to rely primarily on Japanese-language media (newspapers, television, movies), and in some senses, they tend to think of themselves as more Japanese than Canadian or American.
"Issei" women.
"Issei" women's lives were somewhat similar, despite differences in context, because they were structured within interlocking webs of patriarchal relationships, and that consistent subordination was experienced both as oppressive and as a source of happiness. The "Issei" women lived lives of transition which were affected by three common factors: the dominant ideology of late "Meiji" Japan, which advanced the economic objectives of the Japanese state; the patriarchal traditions of the agricultural village, which arose partly as a form of adjustment to national objectives and the adjustment to changes imposed by modernization; and the constraints which arose within a Canadian or American society dominated by racist ideology. Substantive evidence of the working lives of "Issei" women is very difficult to find, partly for lack of data and partly because the data that do exist are influenced by their implicit ideological definition of women.
Within the framework of environmental contradictions, the narratives of these women revealed a surprisingly shared sense of inevitability, a perception that the events of life are beyond the control of the individual, which accounts for the consistency in the way in which Issei women, different and individual in many ways, seem to have structured their emotions—and this quality of emotional control was passed to their "Nisei" children.
Aging.
The "kanreki" (還暦), a traditional, pre-modern Japanese rite of passage to old age at 60, was sometimes celebrated by the "Issei" and is now being celebrated by increasing numbers of "Nisei." Rituals are enactments of shared meanings, norms, and values; and this Japanese rite of passage highlights a collective response among the Nisei to the conventional dilemmas of growing older.
Japanese American photographer and former social worker Mary Koga documented members of her parents' generation in their twilight years. Her "Portrait of the Issei in Illinois" series shows Koga at ease with these people in a relaxed atmosphere.
Done over a long period from the 1970s to the 1990s, the images show her elderly subjects at the day care facility and the Senior Citizens Work Center of the Japanese American Service Committee (JASC) and at Heiwa Terrace, a Japanese American senior residence, both located in Chicago.
History.
The experience of emigrants is inevitably affected by a range of factors directly related to the Japanese society they left behind. As immigrants, the conflicts between the old country and the new played out in unique ways for each individual, and yet common elements do begin to appear in the history of the Japanese Canadian and Japanese American communities.
Emigrants from Japan.
Japan was a closed country for more than two centuries, 1636 to 1853, since military rulers from the Tokugawa family wanted to keep foreigners away from Japanese society. The only exceptions were Chinese and some Dutch, but even they were discouraged from associating with Japanese citizens. Also, it was strictly prohibited by law for ordinary Japanese citizens to go abroad. Change came around the early 19th century when the visit of an American fleet commanded by Commodore Perry caused the new Japanese government to replace the Tokugawa system of economics and politics during the Meiji era in order to open its door to trade and contact with the outside world.
After 1866, the new Japanese government decided to send students and laborers to the U.S. in order to bring back the knowledge and experience necessary for the nation to grow strong.
After 1884, emigration of working classes was permitted; and the first issei began to arrive in North and South America soon after. For example, in 1890, only 25 Issei lived in Oregon. By 1891, 1,000 Japanese lived in Oregon. In 1900, 2,051 Japanese had come to live in Oregon. By 1915, Japanese men with savings of $800 were considered eligible to summon wives from Japan.
Immigrants in America.
Few Japanese workers came to North America intending to become immigrants. Initially, most of them came with vague plans for gaining new experiences and for making some money before returning to homes in Japan. This group of workers was overwhelmingly male. Many "Issei" arrived as laborers. They worked in employment sectors such as agriculture, mining, and railroad construction.
The Issei were born in Japan, and their cultural perspective was primarily Japanese; but they were in America by choice. Despite a certain nostalgia for the old country, they had created homes in a country far from Japan. If they had not been prohibited from becoming citizens, many would have become citizens of the United States.
In 1913, California's Alien Land Law prohibited non-citizens from owning land in the state, and several other states soon after passed their own restrictive alien land laws. This included the "Issei", Japanese residents born in Japan, but not their children, the Nisei, who were born in United States or Hawaii, and who therefore were American citizens by birth. Many of the Issei responded to the law by transferring title to their land to their "Nisei" children.
Americans' first impression of Issei.
Americans generally viewed the "Issei" as a crude, ill-educated lot. Possible reasons for this may be the fact that most Japanese were forced to work in menial jobs in the U.S., such as farming. Since there were many immigrants working in the U.S., Americans were relatively predisposed to have a negative view toward the immigrants. In fact, most of the "Issei" were well-educated. Most of them were better educated than the general Japanese public, and also compared with the average American population back then. Sixty percent of them actually completed middle school, and 21 percent were high school graduates.
Whether Christian, Buddhists, or nonbelievers, the "Issei" almost never caused trouble in the civil authority. The arrest rate for the "Issei" from 1902 to the 1960s was relatively lower than for any other major ethnic group in California. The only exceptions were that some young "Issei" committed crimes relating to gambling and prostitution, which stemmed from different cultural morals in Japan.
Since Buddhist social morals were deeply ingrained, the "Issei" tended to refrain from antisocial behavior. Also, they were concerned about the Japanese government, that the national image should not be sullied by misbehavior in the U.S.
Racial segregation and immigration law.
The post-1900 cause to renew the Chinese Exclusion Act became generalized protests against all Asian immigrants, including the Issei. Since many Chinese immigrants left the U.S., hostility fell on the "Issei." American labor organizations took an initiative in spreading Anti-Japanese sentiment. White Americans wanted to exclude them since they did not want any Asians to take their jobs away. As a result, they formed the Asiatic Exclusion League that viewed Japanese and Chinese as a threat of American workers. The protest of the league involved picketing and beatings of the Issei. In October 1906, amid this anti-Japanese milieu, the San Francisco School Board, carrying out a campaign promise of the mayor, ordered all Japanese and Korean pupils to join the Chinese students at a segregated school. The "Issei" were displeased with the situation and some reported to Japanese newspapers. This caused the Japanese government to protest against the former President, Theodore Roosevelt, and as a result, they signed the Gentlemen's Agreement of 1907. This agreement led the period of settling and family building to come.
By 1911, almost half of the Japanese immigrants were women who landed in the U.S. to reunite with their husbands. After the Gentleman's agreement, a number of "Nisei", the second-generation Japanese, were born in California. Yet, it did not stop some white Americans from segregating Japanese immigrants. The "Issei" were a role model of American citizens by being hardworking, law-abiding, devoted to family and the community. However, some Americans did not want to admit the virtues of the "Issei."
The Immigration Act of 1924 represented the Issei's failed struggle against the segregation. The experiences of the Issei extend from well before the period before July 1, 1924, when the Japanese Exclusion Act came into effect.
The "Issei," however, were very good at enhancing rice farming on "unusable" land. Japanese Californian farmers made rice a major crop of the state. The largest "Issei" community settled around Vacaville, California, near San Francisco.
Internment.
When the Canadian and American governments interned West Coast Japanese in 1942, neither distinguished between those who were citizens ("Nisei") and their non-citizen parents ("Issei"). When the apology and redress for injustices were enacted by the American Congress and the Canadian Parliament in 1988, most of the "Issei" were dead, or too old for it to make any significant difference in lives that had been disrupted.
Notable individuals.
The number of "issei" who have earned some degree of public recognition has continued to increase over time; but the quiet lives of those whose names are known only to family and friends are no less important in understanding the broader narrative of the "nikkei." Although the names highlighted here are over-represented by "issei" from North America, the Latin American member countries of the Pan American Nikkei Association (PANA) include Argentina, Bolivia, Brazil, Chile, Colombia, Mexico, Paraguay, Peru and Uruguay, in addition to the English-speaking United States and Canada.

</doc>
<doc id="46893" url="http://en.wikipedia.org/wiki?curid=46893" title="Sansei">
Sansei

Sansei (三世, "third generation")
 is a Japanese language term used in parts of the world such as South America, North America and Australia to specify the children of children born to Japanese people in the new country. The "Nisei" are considered the second generation, grandchildren of the Japanese-born immigrants are called "Sansei" and the fourth generation "Yonsei". The children of at least one "Nisei" parent are called "Sansei".
The character and uniqueness of the "Sansei" is recognized in its social history.
History.
Although the earliest organized group of Japanese emigrants settled in Mexico in 1897, the four largest populations of Japanese and descendants of Japanese immigrants live in Brazil, the United States, Canada and Peru.
Brazilian "Sansei".
Brazil is home to the largest Japanese population outside of Japan, numbering an estimate of more than 1.5 million (including those of mixed-race or mixed-ethnicity), more than that of the 1.2 million in the United States. The "Sansei" Japanese Brazilians are an important part of that ethnic minority in that South American nation.
American "Sansei".
The majority of American "Sansei" were born during the Baby Boom after the end of World War II; but older "Sansei" who were living in the western United States during WWII were forcibly interned with their parents ("Nisei") and grandparents ("Issei") after Executive Order 9066 was promulgated to exclude everyone of Japanese descent from large parts of the Western states. The " Sansei" were forceful activists in the redress movement, which resulted in an official apology to the internees. In some senses, the "Sansei" seem to feel they are caught in a dilemma between their "quiet" Nisei parents and their other identity model of "verbal" Americans.
In the United States, a representative "Sansei" is General Eric Shinseki (born November 28, 1942), the 34th Chief of Staff of the United States Army (1999–2003) and former United States Secretary of Veterans Affairs. He is the first Asian American in U.S. history to be a four-star general, and the first to lead one of the four U.S. military services.
The Sansei Japanese Americans (三世 lit. third generation) are American-born Japanese Americans citizens of the United States, the children of the Nisei Japanese Americans.
Canadian "Sansei".
Within Japanese-Canadian communities across Canada, three distinct subgroups developed, each with different sociocultural referents, generational identity, and wartime experiences.
Peruvian "Sansei".
Among the approximately 80,000 Peruvians of Japanese descent, the "Sansei" Japanese Peruvians comprise the largest number.
Cultural profile.
Generations.
Japanese-Americans and Japanese-Canadians have special names for each of their generations in North America. These are formed by combining one of the Japanese numbers corresponding to the generation with the Japanese word for generation ("sei" 世). The Japanese-American and Japanese-Canadian communities have themselves distinguished their members with terms like "Issei", "Nisei," and "Sansei" which describe the first, second and third generation of immigrants. The fourth generation is called "Yonsei" (四世) and the fifth is called "Gosei" (五世). The "Issei," "Nisei" and "Sansei" generations reflect distinctly different attitudes to authority, gender, non- Japanese involvement, and religious belief and practice, and other matters. The age when individuals faced the wartime evacuation and internment is the single, most significant factor which explains these variations in their experiences, attitudes and behaviour patterns. 
The term "Nikkei" (日系) was coined by a multinational group of sociologists and encompasses all of the world's Japanese immigrants across generations. The collective memory of the "Issei" and older "Nisei" was an image of Meiji Japan from 1870 through 1911, which contrasted sharply with the Japan that newer immigrants had more recently left. These differing attitudes, social values and associations with Japan were often incompatible with each other. In this context, the significant differences in post-war experiences and opportunities did nothing to mitigate the gaps which separated generational perspectives.
In North America since the redress victory in 1988, a significant evolutionary change has occurred. The "Sansei," their parents, their grandparents, and their children are changing the way they look at themselves and their pattern of accommodation to the non-Japanese majority.
There are currently just over one hundred thousand British Japanese, mostly in London; but unlike other "Nikkei" communities elsewhere in the world, these Britons do not conventionally parse their communities in generational terms as "Issei," "Nisei," or "Sansei."
Sansei.
The third generation of immigrants, born in the United States or Canada to parents born in the United States or Canada, is called "Sansei" (三世). Children born to the "Nisei" were generally born after 1945. They speak English as their first language and are completely acculturized in the contexts of Canadian or American society. They tend to identify with Canadian or American values, norms and expectations. Few speak Japanese, and most tend to express their identity as Canadian or American rather than Japanese. Among the "Sansei" there is an overwhelming percentage of marriages to persons of non-Japanese ancestry.
Aging.
The "kanreki" (還暦), a traditional, pre-modern Japanese rite of passage to old age at 60, was sometimes celebrated by the "Issei" and is now being celebrated by increasing numbers of "Nisei" and a few "Sansei." Rituals are enactments of shared meanings, norms, and values; and this Japanese rite of passage highlights a collective response among the Nisei to the conventional dilemmas of growing older.
History.
Internment and redress.
Some responded to internment with lawsuits and political action; and for others, poetry became an unplanned consequence:
Politics.
The "sansei" became known as the "activist generation".
Notable individuals.
The numbers of "sansei" who have earned some degree of public recognition has continued to increase over time; but the quiet lives of those whose names are known only to family and friends are no less important in understanding the broader narrative of the "Nikkei." Although the names highlighted here are over-represented by "sansei" from North America, the Latin American member countries of the Pan American Nikkei Association (PANA) include Argentina, Bolivia, Brazil, Chile, Colombia, Mexico, Paraguay, Peru, Uruguay, in addition to the English-speaking United States and Canada.

</doc>
<doc id="46895" url="http://en.wikipedia.org/wiki?curid=46895" title="Euler number">
Euler number

In number theory, the Euler numbers are a sequence "En" of integers (sequence in OEIS) defined by the following Taylor series expansion:
where cosh "t" is the hyperbolic cosine. The Euler numbers appear as a special value of the Euler polynomials.
The odd-indexed Euler numbers are all zero. The even-indexed ones (sequence in OEIS) have alternating signs. Some values are:
Some authors re-index the sequence in order to omit the odd-numbered Euler numbers with value zero, and/or change all signs to positive. This encyclopedia adheres to the convention adopted above.
The Euler numbers appear in the Taylor series expansions of the secant and hyperbolic secant functions. The latter is the function in the definition. They also occur in combinatorics, specifically when counting the number of alternating permutations of a set with an even number of elements.
Explicit formulas.
Iterated sum.
An explicit formula for Euler numbers is:
where "i" denotes the imaginary unit with "i"2=−1.
Sum over partitions.
The Euler number "E"2"n" can be expressed as a sum over the even partitions of 2"n",
as well as a sum over the odd partitions of 2"n" − 1,
where in both cases formula_5 and
is a multinomial coefficient. The Kronecker delta's in the above formulas restrict the sums over the "k"'s to formula_7 and to
formula_8, respectively. 
As an example,
Determinant.
"E"2"n" is also given by the determinant
Asymptotic approximation.
The Euler numbers grow quite rapidly for large indices as
they have the following lower bound
Euler zigzag numbers.
The Taylor series of formula_12 is formula_13, where formula_14 is the Euler zigzag numbers, beginning with
For all even "n", formula_14 = formula_16, where formula_17 is the Euler number, and for all odd "n", formula_14 = formula_19, where formula_20 is the Bernoulli number.
Generalized Euler numbers.
Generalizations of Euler numbers include poly-Euler numbers, which play an important role in multiple zeta functions.

</doc>
<doc id="46916" url="http://en.wikipedia.org/wiki?curid=46916" title="Otter">
Otter

Otter is a common name for a carnivorous mammal in subfamily Lutrinae. The 13 extant otter species are all semiaquatic, aquatic or marine, with diets based on fish and invertebrates. Lutrinae is a branch of the weasel family Mustelidae, which also includes weasels, martens, minks, polecats, Eurasian and American badgers, honey badgers and wolverines.
Etymology.
The word "otter" derives from the Old English word "otor" or "oter". This, and cognate words in other Indo-European languages, ultimately stem from the Proto-Indo-European language root *wódr̥, which also gave rise to the English word "water".
Terminology.
An otter's den is called a holt or couch. Male otters are called dogs, females are called bitches, and their offspring are called pups. The collective nouns for otters are bevy, family, lodge, romp (being descriptive of their often playful nature) or, when in water, raft.
The feces of otters are typically identified by their distinctive aroma, the smell of which has been described as ranging from freshly mown hay to putrefied fish; these are known as spraint.
Life cycle.
The gestation period in otters is about 60 to 86 days. The newborn pup is cared for by the mother, father and older offspring. Female otters reach sexual maturity at approximately two years of age and males at approximately three years. After one month, the pup can leave the holt and after two months, it is able to swim. The pup lives with its family for approximately one year. Otters live up to 16 years.
Characteristics.
Otters have long, slim bodies and relatively short limbs with webbed paws. Most have sharp claws on their feet and all except the sea otter have long, muscular tails. The 13 species range in adult size from 0.6 to in length and 1 to in weight. The Oriental small-clawed otter is the smallest otter species and the giant otter and sea otter are the largest. They have very soft, insulated underfur, which is protected by an outer layer of long guard hairs. This traps a layer of air which keeps them dry and warm under water.
Several otter species live in cold waters and have high metabolic rates to help keep them warm. European otters must eat 15% of their body weight each day, and sea otters 20 to 25%, depending on the temperature. In water as warm as 10 °C, an otter needs to catch 100 g of fish per hour to survive. Most species hunt for three to five hours each day and nursing mothers up to eight hours each day.
For most otters, fish is the staple of their diet. This is often supplemented by frogs, crayfish and crabs. Some otters are expert at opening shellfish, and others will feed on available small mammals or birds. Prey-dependence leaves otters very vulnerable to prey depletion. Sea otters are hunters of clams, sea urchins and other shelled creatures. 
Otters are active hunters, chasing prey in the water or searching the beds of rivers, lakes or the seas. Most species live beside water, but river otters usually enter it only to hunt or travel, otherwise spending much of their time on land to avoid their fur becoming waterlogged. Sea otters are considerably more aquatic and live in the ocean for most of their lives.
Otters are playful animals and appear to engage in various behaviors for sheer enjoyment, such as making waterslides and then sliding on them into the water. They may also find and play with small stones. Different species vary in their social structure, with some being largely solitary, while others live in groups – in a few species these groups may be fairly large.
Species.
Genus "Lutra "
Genus "Hydrictis"
Genus "Lutrogale"
Genus "Lontra"
Genus "Pteronura"
Genus "Aonyx"
Genus "Enhydra"
Genus †"Megalenhydris" 
Genus †"Sardolutra" 
Genus †"Algarolutra" 
Genus †"Cyrnaonyx" 
Genus †"Teruelictis"
Genus †"Enhydriodon"
Genus †"Enhydritherium"
Genus †"Teruelictis"
Genus †"Limnonyx"
Genus †"Lutravus"
Genus †"Sivaonyx"
Genus †"Torolutra"
Genus †"Tyrrhenolutra"
Genus †"Vishnuonyx"
European otter.
The European otter ("Lutra lutra"), also called the Eurasian otter, inhabits Europe, most of Asia and parts of North Africa. In the British Isles, they were common as recently as the 1950s, but became rare in many areas due to the use of chlorinated hydrocarbon pesticides, habitat loss and water pollution (they remained relatively common in parts of Scotland and Ireland). Population levels reached a low point in the 1980s, but are now recovering strongly. The UK Biodiversity Action Plan envisages the re-establishment of otters by 2010 in all the UK rivers and coastal areas they inhabited in 1960. Roadkill deaths have become one of the significant threats to the success of their re-establishment.
North American river otter.
The North American river otter ("Lontra canadensis") became one of the major animals hunted and trapped for fur in North America after European contact. River otters eat a variety of fish and shellfish, as well as small land mammals and birds. They grow to one meter (3 to 4 ft) in length and weigh from five to 15 kilograms (10 to 30 lb).
In some areas, this is a protected species, and some places have otter sanctuaries that help sick and injured otters to recover.
Sea otter.
Sea otters ("Enhydra lutris") are classified as marine mammals and live along the Pacific coast of North America. Their historic range included shallow waters of the Bering Strait and Kamchatka, and as far south as Japan. Sea otters have about 26,000 to 165,000 hairs per square centimeters of skin, a rich fur for which humans hunted them almost to extinction. By the time the 1911 Fur Seal Treaty gave them protection, so few sea otters remained that the fur trade had become unprofitable.
Sea otters eat shellfish and other invertebrates (especially clams, abalone, and sea urchins). They frequently carry a rock in a pouch under their forearm and use this to smash open shells, making them one of the relatively small number of animals that use tools. They grow to 1.0 to in length and weigh 30 kg. Although once near extinction, they have begun to spread again, from remnant populations in California and Alaska.
Unlike most marine mammals (such as seals or whales), sea otters do not have a layer of insulating blubber. As with other species of otter, they rely on a layer of air trapped in their fur, which they keep topped up by blowing into the fur from their mouths. They spend most of their time in the water, whereas other otters spend much of their time on land.
Giant otter.
The giant otter ("Pteronura brasiliensis") inhabits South America, especially the Amazon river basin, but is becoming increasingly rare due to poaching, habitat loss, and the use of mercury and other toxins in illegal alluvial gold mining. This gregarious animal grows to a length of up to 1.8 m, and is more aquatic than most other otters.
Relation with humans.
Hunting.
Otters have been hunted for their pelts from at least the 1700s, although it may have begun well before then. Early hunting methods included darts, arrows, nets and snares but later, traps were set on land and guns used.
There has been a long history of otter pelts being worn around the world. In China it was standard for the royalty to wear robes made from them. People that were financially high in status also wore them. The tails of otters were often made into items for men to wear. These included hats and belts. Even some types of mittens for children have been made from the fur of otters.
Otters have also been hunted using dogs, specifically the otterhound. From 1958 to 1963, the 11 otter hunts in England and Wales killed 1,065 otters between them. In such hunts, the hunters notched their poles after every kill. The prized trophy that hunters would take from the otters was the penis bone – which would be worn as a tie-pin.
Traffic (the wildlife trade monitoring network) reported that otters are at serious risk in Southeast Asia and have disappeared from parts of their former range. This decline in populations is due to hunting to supply the demand for skins.
Fishing for humans.
For many generations, fishermen in southern Bangladesh have bred smooth-coated otters and used them to chase fish into their nets. Once a widespread practice, passed down from father to son throughout many communities in Asia, this traditional use of domesticated wild animals is still in practice in the district of Narail, Bangladesh.
Religion and mythology.
Norse mythology tells of the dwarf Ótr habitually taking the form of an otter. The myth of "Otter's Ransom" is the starting point of the Volsunga saga.
In some Native American cultures, otters are considered totem animals.
The otter is held to be a clean animal belonging to Ahura Mazda in Zoroastrian belief, and taboo to kill.
In popular Korean mythology, it is told that people who see an otter ("soodal") will attract 'rain clouds' for the rest of their lives.
Japanese folklore.
In Japanese, they are called "kawauso" (獺、川獺). In Japanese folklore, they fool humans like the fox (kitsune) and tanuki. In the Noto region, Ishikawa Prefecture, there are stories where they shapeshift into beautiful women or children wearing checker-patterned clothing, and if a human attempts to speak to one, they will answer "oraya" and then answer "araya," and if anybody asks them anything, then they say cryptic things like "kawai," and there are also dreadful stories like the one in the Kaga Province (now Ishikawa Prefecture) where an otter that lives in the castle's moat would shapeshift into a woman, invite males, and eat and kill them.
In the kaidan, essays, and legends of the Edo period like the "Urami Kanawa" (裏見寒話), "Taihei Hyaku Monogatari" (太平百物語), and the "Shifu Goroku" (四不語録), there are tales about strange occurrences like otters that shapeshift into beautiful women and kill men.
In the town of Numatachi, Asa District, Hiroshima Prefecture (now Hiroshima), they are called "tomo no kawauso" (伴のカワウソ) and "ato no kawauso" (阿戸のカワウソ), and it is said that they would shapeshift into monks and appear before passers-by, and if the passer-by tries to get close and look up, its height would steadily increase until it became a large monk.
In the Tsugaru region, Aomori Prefecture, they are said to possess humans, and it is said that those possessed by otters would lose their stamina as if their soul has been extracted. They are also said to shapeshift into severed heads and get caught in fishing nets.
In the Kashima District and the Hakui District in Ishikawa Prefecture, they are seen as a yōkai under the name kabuso or kawaso, and they perform pranks like extinguishing the fire of the paper lanterns of people who walk on roads at night, shapeshift into a beautiful woman of 18–19 years of age and fool people, or fool people and make them try to engage in sumo against a rock or a tree stump It is said that they speak human words, and sometimes people would be called and stop while walking on roads.
In the Ishikawa and Kochi Prefectures, they are also said to be a type of kappa, and there are stories told about how they engage in sumo with otters. In places like the Hokuriku region, Kii, and Shikoku, the otters themselves area seen as a type of kappa. In the Kagakushū, a dictionary from the Muromachi period, an otter that grew old becomes a kappa.
In an Ainu folktale, in Urashibetsu (in Abashiri, Hokkaido), there are stories where monster otters would shapeshift into humans, go into homes where there were beautiful girls, and try to kill the girl and make her its wife.
In China, like in Japan, there are stories where otters would shapeshift into beautiful women in old books like In Search of the Supernatural and the Zhenyizhi (甄異志).

</doc>
<doc id="46918" url="http://en.wikipedia.org/wiki?curid=46918" title="Egyptian language">
Egyptian language

Egyptian is the oldest known language of Egypt and a branch of the Afroasiatic language family. The earliest known complete sentence in the Egyptian language has been dated to about 2690 BC, making it one of the oldest recorded languages known, along with Sumerian.
Egyptian was spoken until the late 17th century AD in the form of Coptic. The national language of modern-day Egypt is Egyptian Arabic, which gradually replaced Coptic as the language of daily life in the centuries after the Muslim conquest of Egypt.
Coptic is still used as the liturgical language of the Coptic Church. It has several hundred fluent speakers today.
Classification.
Egyptian belongs to the Afroasiatic language family. Among the typological features of Egyptian that are typically Afroasiatic are: fusional morphology, consonantal lexical roots, a series of emphatic consonants, a three-vowel system /a i u/, nominal feminine suffix *"-at", nominal "m-", adjectival *"-ī", and characteristic personal verbal affixes. Of the other Afroasiatic branches, Egyptian shows its greatest affinities with Semitic, and to a lesser extent Cushitic.
In Egyptian, the Proto-Afroasiatic voiced consonants */d z ð/ developed into pharyngeal ⟨ꜥ⟩ /ʕ/, e.g. Eg. "ꜥr.t" ‘portal’, Sem. *"dalt" ‘door’. Afroasiatic */l/ merged with Egyptian ⟨n⟩, ⟨r⟩, ⟨ꜣ⟩, and ⟨j⟩ in the dialect on which the written language was based, while being preserved in other Egyptian varieties. Original */k g ḳ/ palatalize to ⟨ṯ j ḏ⟩ in some environments and are preserved as ⟨k g q⟩ in others.
Egyptian has many biradical and perhaps monoradical roots, in contrast to the Semitic preference for triradical roots. Egyptian probably is more archaic in this regard, whereas Semitic likely underwent later regularizations converting roots into the triradical pattern.
Although Egyptian is the oldest Afroasiatic language documented in written form, its morphological repertoire is greatly different from that of the rest of the Afroasiatic in general and Semitic in particular. This suggests that Egyptian had already undergone radical changes from Proto-Afroasiatic before being recorded, that the Afroasiatic family has so far been studied with an excessively Semito-centric approach, or that Afroasiatic is a typological rather than genetic grouping of languages. (The general consensus is that Afroasiatic is indeed a genetic grouping, and that Egyptian did in fact diverge greatly in its prerecorded history, although there is almost certainly a Semitic bias in Afroasiatic reconstruction.)
History.
Scholars group the Egyptian language into six major chronological divisions:
The earliest Egyptian glyphs date back to around 3300 BC. These early texts are generally lumped together under the general term "Archaic Egyptian." They record names, titles and labels, but a few of them show morphological and syntactic features familiar from later, more complete, texts.
Old Egyptian is dated from the oldest known complete sentence, found in the tomb of Seth-Peribsen and dated to around 2690 BC. Extensive texts appear from about 2600 BC. Middle Egyptian was spoken from about 2000 BC for a further 700 years when Late Egyptian made its appearance; Middle Egyptian did, however, survive until the first few centuries AD as a written language, similar to the use of Latin during the Middle Ages and that of Classical Arabic today. Demotic Egyptian first appears about 650 BC and survived as a spoken language until the fifth century AD. Coptic Egyptian appeared in the fourth century AD and survived as a living language until the sixteenth century AD, when European scholars traveled to Egypt to learn it from native speakers during the Renaissance. It probably survived in the Egyptian countryside as a spoken language for several centuries after that. The Bohairic dialect of Coptic is still used by the Egyptian Christian Churches.
Old, Middle, and Late Egyptian were all written using hieroglyphs and hieratic. Demotic was written using a script derived from hieratic; its appearance is vaguely similar to modern Arabic script and is also written from right to left (although the two hardly hold any relation). Coptic is written using the Coptic alphabet, a modified form of the Greek alphabet with a number of symbols borrowed from Demotic for sounds that did not occur in contemporary Greek.
Arabic became the language of Egypt's political administration soon after the Arab conquest in the seventh century AD, and gradually replaced Coptic as the language spoken by the populace. Today, Coptic survives as the liturgical language of the Coptic Orthodox Church and the Coptic Catholic Church.
The Bible contains some words, terms and names thought by scholars to be Egyptian in origin. An example of this is Zaphnath-Paaneah, the Egyptian name given to Joseph.
Dialects.
Pre-Coptic Egyptian does not show great dialectal differences in the written language due to the centralized nature of Egyptian society. However, they must have existed in speech; this is evidenced by a letter from c. 1200 BCE complaining that the language of a correspondent is as unintelligible as the speech of a northern Egyptian to a southerner. Recently, some evidence of internal dialects has been found in pairs of similar words in Egyptian, which, based on similarities with later dialects of Coptic, may be derived from Northern and Southern dialects of Egyptian. Written Coptic has five major dialects which differ mainly in graphic conventions, most notably the southern Saidic dialect which was the main classical dialect and the northern Bohairic dialect which is currently used in Coptic Church services.
Orthography.
Most surviving texts in the Egyptian language are primarily written on stone in the hieroglyphic script. However, in antiquity, the majority of texts were written on perishable papyrus in hieratic and (later) demotic, which are now lost. There was also a form of cursive hieroglyphic script used for religious documents on papyrus, such as the Book of the Dead in the Ramesside Period; this script was simpler to write than the hieroglyphs in stone inscriptions, but was not as cursive as hieratic, lacking the wide use of ligatures. Additionally, there was a variety of stone-cut hieratic known as "lapidary hieratic". In the language's final stage of development, the Coptic alphabet replaced the older writing system. The native name for Egyptian hieroglyphic writing is "sẖꜣ n mdw nṯr" or "writing of the words of god". Hieroglyphs are employed in two ways in Egyptian texts: as ideograms that represent the idea depicted by the pictures; and more commonly as phonograms denoting their phonetic value.
Due to the fact that the phonetic realization of Egyptian cannot be known with certainty, Egyptologists use a system of transliteration to denote each sound which could be represented by a uniliteral hieroglyph. The two systems which are still in common use are the traditional system and the European system; in addition a third system is used for computer input.
Phonology.
While the consonantal phonology of the Egyptian language may be reconstructed, its exact phonetics are unknown, and there are varying opinions on how to classify the individual phonemes. In addition, because Egyptian is also recorded over a full two millennia, the Archaic and Late stages being separated by the amount of time that separates Old Latin from modern Italian, it must be assumed that significant phonetic changes would have occurred over that time.
Phonologically, Egyptian contrasted labial, alveolar, palatal, velar, uvular, pharyngeal, and glottal consonants, in a distribution rather similar to that of Arabic. It also contrasted voiceless and emphatic consonants, as with other Afroasiatic languages, although exactly how the emphatic consonants were realized is not precisely known. Early research had assumed opposition in stops was one of voicing, but is now thought to either be one of tenuis and emphatic stops, as in many of the Semitic languages, or one of aspirated and ejective stops, as in many of the Cushitic languages.
Since vowels were not written, reconstructions of the Egyptian vowel system are much more uncertain, relying mainly on the evidence from Coptic and foreign transcriptions of Egyptian personal and place names. The vocalization of Egyptian is partially known, largely on the basis of reconstruction from Coptic, in which the vowels are written. Recordings of Egyptian words in other languages provide an additional source of evidence. Scribal errors provide evidence of changes in pronunciation over time. The actual pronunciations reconstructed by such means are used only by a few specialists in the language. For all other purposes the Egyptological pronunciation is used, which is, of course, artificial and often bears little resemblance to what is known of how Egyptian was spoken.
Consonants.
The following consonant system is posited for Archaic (before 2600 BC) and Old Egyptian (2686–2181 BC), with IPA equivalents in square brackets where they differ from the usual transcription scheme:
The phoneme /l/ did not have an independent representation in the hieroglyphic orthography, and was frequently written with the sign for /n/ or /r/. The probable explanation is that the standard for written Egyptian was based on a dialect in which former /l/ had merged with other sonorants. /ʔ/ was rare and also not indicated orthographically. The phoneme /j/ was written as ⟨j⟩ in initial position (⟨jt⟩ = */ˈjaːtVj/ 'father') and immediately after a stressed vowel (⟨bjn⟩ = */ˈbaːjin/ 'bad'), as ⟨jj⟩ word-medially immediately before a stressed vowel (⟨ḫꜥjjk⟩ = */χaʕˈjak/ 'you will appear'), and as null word-finally (⟨jt⟩ = /ˈjaːtvj/ 'father').
In Middle Egyptian (2055–1650 BC), a number of consonantal shifts took place. By the beginning of the Middle Kingdom period, /z/ and /s/ had merged, and the graphemes ⟨s⟩ and ⟨z⟩ were used interchangeably. In addition, /j/ had become /ʔ/ word-initially in an unstressed syllable (e.g. ⟨jwn⟩ /jaˈwin/ > */ʔaˈwin/ 'color) and following a stressed vowel (e.g. ⟨ḥjpw⟩ */ˈħujpvw/ > /ˈħeʔp(vw)/ '[the god] Apis').
In Late Egyptian (1069–700 BC), the following changes are present: the phonemes "d ḏ g" gradually merge with their counterparts "t ṯ k" (⟨dbn⟩ */ˈdiːban/ > Akkadian transcription "ti-ba-an" 'dbn-weight'); "ṯ ḏ" often become /t d/, though they are retained in many lexemes; "ꜣ" becomes /ʔ/; and /t r j w/ become /ʔ/ at the end of a stressed syllable and eventually null word-finally (e.g. ⟨pḏ.t⟩ */ˈpiːɟat/ > Akk. transcription -"pi-ta" 'bow').
More consonantal changes occurred in the first millennium BCE and the first centuries CE, leading to the Coptic language (1st–17th century AD). In Sahidic "ẖ ḫ ḥ" merged into ϣ "š" (most often from "ḫ") and ϩ /h/ (most often "ẖ ḥ"). Bohairic and Akhmimic are more conservative, having also a velar fricative /x/ (ϧ in Bohairic, ⳉ in Akhmimic). Pharyngeal "*ꜥ" merged into glottal /ʔ/, after having affected the quality of surrounding vowels. /ʔ/ is only indicated orthographically when following a stressed vowel, in which case it is marked by doubling the vowel letter (except in Bohairic), e.g. Akhmimic ⳉⲟⲟⲡ /xoʔp/, Sahidic & Lycopolitan ϣⲟⲟⲡ "šoʔp", Bohairic ϣⲟⲡ "šoʔp" 'to be' < "ḫpr.w" */ˈχapraw/ 'has become'. The phoneme ⲃ /b/ probably was pronounced as a fricative [β], and became ⲡ /p/ after a stressed vowel in syllables which were closed in earlier Egyptian (compare ⲛⲟⲩⲃ < */ˈnaːbaw/ 'gold' and ⲧⲁⲡ < */dib/ 'horn'). The phonemes /d g z/ are only found in Greek borrowings, with rare exceptions triggered by a proximate /n/ (e.g. ⲁⲛⲍⲏⲃⲉ/ⲁⲛⲥⲏⲃⲉ < "ꜥ.t n.t sbꜣ.w" 'school').
Earlier "*d ḏ g q" were preserved as ejective "t' c' k' k'" in prevocalic position in Coptic. Despite the fact that these were written using the same graphemes as for the pulmonic stops (⟨ⲧ ϫ ⲕ⟩), their existence may be inferred based on the following evidence: The stops ⟨ⲡ ⲧ ϫ ⲕ⟩ /p t c k/ were allophonically aspirated [pʰ tʰ cʰ kʰ] before stressed vowels and sonorant consonants. In Bohairic these allophones were written with the special graphemes ⟨ⲫ ⲑ ϭ ⲭ⟩, while other dialects did not mark aspiration, thus Sahidic ⲡⲣⲏ vs. Bohairic ⲫⲣⲏ 'the sun'. It then may be observed that Bohairic does not mark aspiration for reflexes of older "*d ḏ g q", e.g. Sahidic & Bohairic ⲧⲁⲡ */dib/ 'horn'. Similarly, the definite article ⲡ is unaspirated when a word beginning with a glottal stop follows, e.g. Bohairic ⲡ + ⲱⲡ > ⲡⲱⲡ 'the account'.
The consonant system of Coptic is as follows:
Vowels.
The following is the vowel system posited for earlier Egyptian:
Vowels were always short in unstressed syllables (e.g. ⟨tpj⟩ = */taˈpij/ 'first'), long in open stressed syllables (e.g. ⟨rmṯ⟩ = */ˈraːmac/ 'man'), and either short or long in closed stressed syllables (e.g. ⟨jnn⟩ = */jaˈnan/ 'we' vs. ⟨mn⟩ = */maːn/ 'to stay').
Late New Kingdom, after Ramses II i.e. c. 1200 BC: */ˈaː/ > */ˈoː/ (parallel to Canaanite vowel shift), e.g. ⟨ḥrw⟩ '(the god) Horus' */ħaːruw/ > */ħoːrə/ (Akkadian transcription: -ḫuru). This provoked */uː/ > */eː/, e.g. ⟨šnj⟩ 'tree' */ʃuːn(?)j/ > */ʃeːnə/ (Akkadian transcription: -sini).
Early New Kingdom: short stressed */ˈi/ > */ˈe/, e.g. ⟨mnj⟩ 'Menes' */maˈnij/ > */maˈneʔ/ (Akkadian transcription: ma-né-e). Later, probably circa 1000–800 BC, short stressed */ˈu/ > */ˈe/, e.g. ⟨ḏꜥn.t⟩ 'Tanis' */ˈɟuʕnat/ was borrowed into Hebrew as *ṣuʕn but later transcribed as ⟨ṣe-e'-nu/ṣa-a'-nu⟩ in the Neo-Assyrian period.
Unstressed vowels, especially after the stress, became */ə/, e.g. ⟨nfr⟩ 'good' */ˈnaːfir/ > */ˈnaːfə/ (Akkadian transcription -na-a-pa). */iː/ > */eː/ next to /ʕ/ and /j/, e.g. ⟨wꜥw⟩ 'soldier' */wiːʕiw/ > */weːʕə/ (earlier Akkadian transcription: ú-i-ú, later: ú-e-eḫ).
In Sahidic and Bohairic Coptic, Late Egyptian stressed */ˈa/ becomes */ˈo/ and */ˈe/ becomes /ˈa/, while in the other dialects these are preserved, e.g. ⟨sn⟩ */san/ 'brother' > SB ⟨son⟩, ALF ⟨san⟩; ⟨rn⟩ 'name' */rin/ > */ren/ > SB ⟨ran⟩, ALF ⟨ren⟩. However, SB preserve */ˈa/ and Fayyumic renders it as ⟨e⟩ in the presence of guttural fricatives, e.g. ⟨ḏbꜥ⟩ '10000' */ˈbaʕ/ > SAL ⟨tba⟩, B ⟨tʰba⟩, F ⟨tbe⟩. In Akhmimic and Lycopolitan, */ˈa/ becomes /ˈo/ before etymological /ʕ, ʔ/, e.g. ⟨jtrw⟩ 'river' */ˈjatraw/ > */jaʔr(ə)/ > S ⟨eioor(e)⟩, B ⟨ior⟩, A ⟨ioore, iôôre⟩, F ⟨iaal, iaar⟩. Similarly the diphthongs */ˈaj/, */ˈaw/, which normally have reflexes /ˈoj/, /ˈow/ in Sahidic and are preserved in other dialects, in Bohairic are written ⟨ôi⟩ (in non-final position) and ⟨ôou⟩ respectively, e.g. "to me, to them" S ⟨eroi, eroou⟩, AL ⟨arai, arau⟩, F ⟨elai, elau⟩, B ⟨eroi, erôou⟩. Sahidic and Bohairic preserve */ˈe/ before /ʔ/ (either etymological or from lenited /t r j/ or tonic-syllable coda /w/), e.g. SB ⟨ne⟩ /neʔ/ 'to you (fem.)' < */ˈnet/ < */ˈnic/. */e/ may also have different reflexes before sonants, in proximity of sibilants, and in diphthongs.
Old */aː/ surfaces as /uː/ after nasals and occasionally other consonants, e.g. ⟨nṯr⟩ 'god' */ˈnaːcar/ > /ˈnuːte/ ⟨noute⟩ /uː/ has acquired phonemic status, as evidenced by minimal pairs like 'to approach' ⟨hôn⟩ /hoːn/ < */ˈçaːnan/ ẖnn vs. 'inside' ⟨houn⟩ /huːn/ < */ˈçaːnaw/ ẖnw. Etymological */uː/ > */eː/ often surfaces as /iː/ next to /r/ and after etymological pharyngeals, e.g. SL ⟨hir⟩ < */χuːr/ 'street' (Semitic loan).
Most Coptic dialects have two phonemic vowels in unstressed position. Unstressed vowels generally became /ə/, written as ⟨e⟩ or null (⟨i⟩ in Bohairic and Fayyumic word-finally), but pretonic unstressed /a/ occurs as a reflex of earlier unstressed */e/ in proximity to an etymological pharyngeal, velar, or sonant (e.g. 'to become many' ⟨ašai⟩ < ꜥšꜣ */ʕiˈʃiʀ/), or unstressed */a/. Pretonic [i] is underlyingly /əj/, e.g. S 'ibis' ⟨hibôi⟩ < h(j)bj.w */hijˈbaːj?w/.
Thus the following is the Sahidic vowel system c. 400 AD:
Phonotactics.
Earlier Egyptian had syllable structure CV(:)(C), where V was long in open, stressed syllables and short elsewhere. In addition, syllables of the type CV:C or CVCC could occur in word-final, stressed position. However CV:C only occurred in the infinitive of biconsonantal verbal roots, and CVCC only in some plurals. In later Egyptian stressed CV:C, CVCC, and CV became much more common because of the loss of final dentals and glides.
Stress.
Earlier Egyptian: penultimate or ultimate. According to some scholars this is a development from a stage in proto-Egyptian where the antepenult could be stressed; this was lost as open posttonic syllables lost their vowels, e.g. */ˈχupiraw/ > */ˈχupraw/ 'transformation'.
Egyptological pronunciation.
As a convention, Egyptologists make use of an "Egyptological pronunciation" in English, in which the consonants are given fixed values and vowels are inserted in accordance with essentially arbitrary rules. Two consonants, alef and the ayin, are generally pronounced /ɑː/. The yodh is pronounced /iː/, and "w" /uː/. Between other consonants, /ɛ/ is then inserted. Thus, for example, the Egyptian king whose name is most accurately transliterated as "Rꜥ-ms-sw" is transcribed as "Ramesses", meaning "Ra has Fashioned (lit., "Borne") Him". In transcription, ⟨a⟩, ⟨i⟩, and ⟨u⟩ all represent consonants; for example, the name Tutankhamen (1341–1323 BC) was written in Egyptian "twt-ꜥnḫ-ı͗mn". Experts have assigned generic sounds to these values as a matter of convenience, but this artificial pronunciation should not be mistaken for how Egyptian was actually pronounced at any point in time. For example, "twt-ꜥnḫ-ı͗mn" is conventionally pronounced in English, but in his time was likely realized as something like *].
Grammar.
Morphology.
Egyptian is a fairly typical Afroasiatic language. At the heart of Egyptian vocabulary is a root of three consonants. Sometimes there were only two, for example rꜥ ] "sun" (where the [ʕ] is thought to have been something like a voiced pharyngeal fricative), but larger roots are also common, some being as large as five: "sḫdḫd" "be upside-down". Vowels and other consonants were then inserted into the consonantal skeleton in order to derive different meanings, in the same way as Arabic, Hebrew, and other Afroasiatic languages do today. However, because vowels (and sometimes glides) were not written in any Egyptian script except Coptic, it can be difficult to reconstruct the actual forms of words; hence orthographic ⟨stp⟩ "to choose", for example, could represent the stative (as the stative endings can be left unexpressed) or imperfective verb forms or even a verbal noun ("i. e.", "a choosing").
Nouns.
Egyptian nouns can be either masculine or feminine (indicated as with other Afroasiatic languages by adding a "-t"), and singular, plural ("-w / -wt"), or dual ("-wy / -ty").
Articles (both definite and indefinite) did not develop until Late Egyptian, but are used widely thereafter.
Pronouns.
Egyptian has three different types of personal pronouns: suffix, enclitic (called "dependent" by Egyptologists) and independent pronouns. It also has a number of verbal endings added to the infinitive to form the stative, which are regarded by some linguists as a "fourth" set of personal pronouns. They bear close resemblance to their Semitic counterparts. The three main sets of personal pronouns are as follows:
It also has demonstrative pronouns (this, that, these and those), in masculine, feminine, and common plural:
Finally there are interrogative pronouns (what, who, etc.).They also bear close resemblance to their Semitic and Berber counterparts
Verbs.
The verbal morphology Egyptian can be divided into finite and non-finite forms. Finite verbs convey person, tense/aspect, mood, and voice. Each is indicated by a set of affixal morphemes attached to the verb — the basic conjugation is "sḏm.f" 'he hears'. The non-finite forms occur without a subject and they are the infinitive, the participles and the negative infinitive, which calls "negatival complement". There are two main tenses/aspects in Egyptian: past and temporally unmarked imperfective and aorist forms. The latter are determined from their syntactic context.
Adjectives.
Adjectives agree in gender and number with their nouns, for example: "s nfr" "(the) good man" and "st nfrt" "(the) good woman".
Attributive adjectives used in phrases fall after the noun they are modifying, such as in "(the) great god" ("nṯr ꜥꜣ"). However, when used independently as a predicate in an adjectival phrase, such as "(the) god (is) great" ("ꜥꜣ nṯr") (lit., "great (is the) god"), the adjective precedes the noun.
Prepositions.
Egyptian adpositions come before the noun.
Adverbs.
Adverbs are words such as "here" or "where?". In Egyptian, they come at the end of a sentence, "e.g.",
"zı͗.n nṯr ı͗m" "the god went there", "there" ("ı͗m") is the adverb.
Some common Egyptian adverbs:
Syntax.
Classical Egyptian's basic word order is verb–subject–object; this pattern holds true for Old Egyptian and Middle Egyptian. However, this is not true for the later stages of the language's development, including Late Egyptian, Demotic and Coptic. The equivalent to "the man opens the door", would be a sentence corresponding to "opens the man the door" ("wn s ꜥꜣ"). It uses the so-called status constructus to combine two or more nouns to express the genitive, similar to Semitic and Berber languages. The early stages of Egyptian possessed no articles, no words for "the" or "a"; later forms used the words "pꜣ", "tꜣ" and "nꜣ" for this purpose. Like other Afroasiatic languages, Egyptian uses two grammatical genders, masculine and feminine. It also uses three grammatical numbers, contrasting singular, dual, and plural forms, although there is a tendency for the loss of the dual as a productive form in later Egyptian.
Vocabulary.
While Egyptian culture is one of the influences of Western civilization, few words of Egyptian origin are found in English. Even those associated with ancient Egypt were usually transmitted in Greek forms. Some examples of Egyptian words that have survived in English include "ebony" (Egyptian hbny, via Greek and then Latin), "ivory" (Egyptian abw / abu, literally 'ivory; elephant'), "pharaoh" (Egyptian pr-ꜥꜣ, literally "great house"; transmitted through Greek), as well as the proper names "Phinehas" (Egyptian, pꜣ-nḥsy, used as a generic term for Nubian foreigners) and "Susan" (Egyptian, sšn, literally "lily flower"; probably transmitted first from Egyptian into Hebrew "Shoshanah").
Literature.
Online dictionaries.
Important Note: the old grammars and dictionaries of E. A. Wallis Budge have long been considered obsolete by Egyptologists, even though these books are still available for purchase.
More book information is available at 

</doc>
<doc id="46919" url="http://en.wikipedia.org/wiki?curid=46919" title="Coptic">
Coptic

Coptic may refer to:

</doc>
<doc id="46923" url="http://en.wikipedia.org/wiki?curid=46923" title="Nubian">
Nubian

Nubian may refer to:

</doc>
<doc id="46924" url="http://en.wikipedia.org/wiki?curid=46924" title="Caesarean section">
Caesarean section

A Caesarean section (often C-section, also other spellings) is a surgical procedure in which one or more incisions are made through a mother's abdomen and uterus to deliver one or more babies. A Caesarean section is often performed when a vaginal delivery would put the baby's or mother's life or health at risk. Some are also performed upon request without a medical reason to do so. The World Health Organization recommends that they should only be done based on medical need.
C-sections result in a small overall increase in bad outcomes in low risk pregnancies. The bad outcomes that occur with C-section differ from those that occur with vaginal delivery. Established guidelines recommend that caesarean sections not be used before 39 weeks without a medical indication to perform the surgery.
In many countries, caesarean section procedures are used more frequently than is necessary, and consequently governments and health organizations promote programs to reduce the use of caesarean section in favor of using vaginal delivery. The international healthcare community has considered the rate of 10% and 15% to be ideal for caesarean sections since 1985. The countries which report overuse of this procedure are not finding ways to decrease use of the procedure as much as they would like. The first modern Caesarean section was performed by German gynecologist Ferdinand Adolf Kehrer in 1881.
Uses.
Caesarean section is recommended when vaginal delivery might pose a risk to the mother or baby. C-sections are also carried out for personal preference but this is not recommended. Planned caesarean sections also known as elective caesarean sections should not be scheduled before 39 weeks gestational age unless there is a medical reason to do so.
Medical uses.
Some medical indications are below. Not all of the listed conditions represent a mandatory indication, and in many cases the obstetrician must use discretion to decide whether a Caesarean is necessary.
Complications of labor and factors impeding vaginal delivery, such as:
Other complications of pregnancy, pre-existing conditions and concomitant disease, such as:
Other
Prevention.
It is generally agreed that the prevalence of C-section is higher than needed in many countries and physicians are encouraged to actively lower the rate. Some of these efforts include: emphasizing that a long of labor is not abnormal and thus not a justification for C-section; changing the start of active labor from a cervical dilation of 4 cm to a dilation of 6 cm; and allowing at least 2 hours of pushing for women who have previously given birth and 3 hours of pushing for women who have not previously given birth before labor arrest is considered. Physical exercise during pregnancy also decreases the risk.
Risks.
Adverse outcomes in low risk pregnancies occur in 8.6% of vaginal deliveries and 9.2% of C-section deliveries.
Risks to the mother.
In those who are low risk the risk of death for Caesarian sections is 13 per 100,000 and for vaginal birth 3.5 per 100,000 in the developed world. The UK National Health Service gives the risk of death for the mother as three times that of a vaginal birth.
In Canada the difference in bad outcome in the mother (e.g. cardiac arrest, wound hematoma, or hysterectomy) was 1.8 additional cases per 100 or three times the risk.
As with all types of abdominal surgeries, a Caesarean section is associated with risks of postoperative adhesions, incisional hernias (which may require surgical correction) and wound infections. If a Caesarean is performed under emergency situations, the risk of the surgery may be increased due to a number of factors. The patient's stomach may not be empty, increasing the anaesthesia risk. Other risks include severe blood loss (which may require a blood transfusion) and postdural-puncture spinal headaches.
Women who had Caesarean sections were more likely to have problems with later pregnancies, and it is recommended that women who want larger families should not seek an elective Caesarean. The risk of placenta accreta, a potentially life-threatening condition, is 0.13% after two Caesarean sections, but increases to 2.13% after four and then to 6.74% after six or more. Along with this is a similar rise in the risk of emergency hysterectomies at delivery.
Mothers can experience increased incidence of postnatal depression, and can experience significant psychological birth trauma and ongoing birth-related post-traumatic stress disorder after obstetric intervention during the birthing process. Factors like pain in first stage of labor, feelings of powerlessness, intrusive emergency obstetric intervention are important in the development of birth trauma.
Subsequent pregnancies.
Women who have had a Caesarean for any reason are somewhat less likely to become pregnant or give birth again as compared to women who have previously only delivered vaginally.
Women who had just one previous Caesarean section are more likely to have problems with their second birth. Delivery after previous Caesarean section is by either of two main options:
Both have higher risks than a vaginal birth with no previous Caesarean section. Criteria for making VBAC include that the previous Caesarean section should be a low transverse one. VBAC (compared to ERCS) confers a higher risk for mainly uterine rupture and perinatal death of the child. Furthermore, opting for VBAC results in 20-40% of times in that Caesarean section is performed eventually anyway, with greater risks of complications in an emergent repeat Caesarean section than in an ERCS. On the other hand, VBAC confers less maternal morbidity and a decreased risk of complications in future pregnancies than ERCS.
Adhesions.
There are number of steps that can be taken during abdominal or pelvic surgery to minimize postoperative complications, such as the formation of adhesions. Such techniques and principles may include:
However, despite these proactive measures, abdominal or pelvic surgery can result in trauma that can lead to adhesions. In order to prevent adhesions from forming following a pelvic (gynecologic) surgery, such as hysterectomy, myomectomy or caesarean section, adhesion barrier can be placed during surgery to minimize the risk of adhesions between the uterus and ovaries, the small bowel, and almost any tissue in the abdomen or pelvis.
Adhesions can cause complications, such as:
Risks to the child.
Non-medically indicated (elective) childbirth before 39 weeks gestation "carry significant risks for the baby with no known benefit to the mother." Complications from elective caesarean before 39 weeks include: newborn mortality at 37 weeks may be up to 3 times the number at 40 weeks, and was elevated compared to 38 weeks of gestation. These “early term” births were also associated with increased death during infancy, compared to those occurring at 39 to 41 weeks ("full term"). Researchers in one study and another review found many benefits to going full term, but “no adverse effects” in the health of the mothers or babies.
The American Congress of Obstetricians and Gynecologists and medical policy makers review research studies and find increased incidence of suspected or proven sepsis, RDS, hypoglycemia, need for respiratory support, need for NICU admission, and need for hospitalization > 4–5 days. In the case of caesarean sections, rates of respiratory death were 14 times higher in pre-labor at 37 compared with 40 weeks gestation, and 8.2 times higher for pre-labor caesarean at 38 weeks. In this review, no studies found decreased neonatal morbidity due to non-medically indicated (elective) delivery prior to 39 weeks.
For otherwise healthy twin pregnancies where both twins are head down a trial of vaginal delivery is recommended at between 37 and 38 weeks. Vaginal delivery in this case does not worsen the outcome for the infant as compared with Caesarean section. There is controversy on the best method of delivery were the first twin is head first and the second is not. When the first twin is not head down a C-section is often recommended. Regardless of birth by section or vaginally, the medical literature recommends delivery of dichorionic twins at 38 weeks, and monochorionic twins (identical twins sharing a placenta) by 37 weeks due to the increased risk of stillbirth in monochorionic twins who remain in utero after 37 weeks. The consensus is that late preterm delivery of monochorionic twins is justified because the risk of stillbirth for post-37 week delivery is significantly higher than the risks posed by delivering monochorionic twins near term (i.e., 36–37 weeks).
The consensus concerning monoamniotic twins (identical twins sharing an amniotic sac), the highest risk type of twins, is that they should be delivered by caesarean section at or shortly after 32 weeks.
In a research study widely publicized, singleton children born earlier than 39 weeks may have developmental problems, including slower learning in reading and math.
Other risks include:
Classification.
Caesarean sections have been classified in various ways by different perspectives. One way to discuss all classification systems is to group them by their focus either on the urgency of the procedure, characteristics of the mother, or as a group based on other, less commonly discussed factors.
It is most common to classify c-sections by the urgency of performing them. When there is something unusual about the mother or the pregnancy, then it becomes common to classify that c-section by whatever unusual characteristics are seen. When discussing the actual technique or surgical conditions, then c-sections are classified by those techniques.
By urgency.
Conventionally, caesarean sections are classified as being either an elective surgery or an emergency surgery. Classification is used to note a strategy for using anesthesia, as in emergencies general anesthesia must be used but when time is available, it is preferable to use regional anesthesia.
A planned caesarean (or elective/scheduled caesarean), arranged ahead of time, is most commonly arranged for medical reasons and ideally as close to the due date as possible. A crash/emergent/emergency Caesarean section is performed in an obstetric emergency, where complications of pregnancy onset suddenly during the process of labour, and swift action is required to prevent the deaths of mother, child(ren) or both.
A planned or elective caesarean section is an elective surgery, meaning that it is scheduled in advance rather than performed because of an unscheduled emergency. This confers the ability to perform the delivery at a time when hospital resources are optimal, such as at daytime rather than what might otherwise turn out to be at night. The cost to the patient and the baby for unnecessary surgery may be substantial. Critics also argue that because physicians and institutions may benefit by reducing night time and weekend work, that an inappropriate incentive exists to suggest elective surgery.
Elective caesarean sections may be performed on the basis of an obstetrical or medical indication, or because of a non-indicated maternal request. Among women in the United Kingdom, Sweden and Australian about 7% preferred C-section as a method of delivery. In cases without medical indications the American Congress of Obstetricians and Gynecologists recommend a planned vaginal delivery. The National Institute for Health and Care Excellence recommends that if after a women has been provided information on the risk of a planned C-section and she still insists on the procedure it should be provided. If provided this should be done 39 weeks of gestation or later.
By characteristics of the mother.
Caesarean delivery on maternal request.
Caesarean delivery on maternal request (CDMR) is a medically unnecessary caesarean section, where the conduct of a childbirth via a caesarean section is requested by the pregnant patient even though there is not a medical indication to have the surgery.
After previous Caesarean.
Mothers who have previously had a caesarean section are more likely to have a c-section for future pregnancies than mothers who have never had a c-section. There is discussion about the circumstances under which women should have a vaginal birth after a previous caesarean.
Vaginal birth after caesarean (VBAC) is the practice of birthing a baby vaginally after a previous baby has been delivered through caesarean section (surgically). According to The American Congress of Obstetricians and Gynecologists (ACOG), VBAC is associated with decreased maternal morbidity and a decreased risk of complications in future pregnancies. According to the American Pregnancy Association, 90% of women who have undergone caesarean deliveries are candidates for VBAC. Approximately 60-80% of women opting for VBAC will successfully give birth vaginally, which is comparable to the overall vaginal delivery rate in the United States in 2010.
Twins.
For otherwise healthy twin pregnancies where both twins are head down a trial of vaginal delivery is recommended at between 37 and 38 weeks. Vaginal delivery in this case does not worsen the outcome for the infant as compared with C-section. There is controversy on the best method of delivery where the first twin is head first and the second is not. When the first twin is not head down, a C-section is often recommended. Although the second twin typically has a higher frequency of problems, it is not known if a planned C-section affects this. It is estimated that 75% of twin pregnancies in the United States were delivered by Caesarean section in 2008.
Breech birth.
A breech birth is the birth of a baby from a breech presentation, in which the baby exits the pelvis with the buttocks or feet first as opposed to the normal head-first presentation. In breech presentation, fetal heart sounds are heard just above the umbilicus.
The bottom-down position presents some hazards to the baby during the process of birth, and the mode of delivery (vaginal versus Caesarean) is controversial in the fields of obstetrics and midwifery.
Though vaginal birth is possible for the breech baby, certain fetal and maternal factors influence the safety of vaginal breech birth. The majority of breech babies born in the United States are delivered by Caesarean section as studies have shown increased risks of morbidity and mortality for vaginal breech delivery, and most hospital policies do not permit vaginal breech birth for this reason. As a result of reduced numbers of vaginal breech deliveries, most obstetricians do not receive training in the skill set required for safe vaginal breech delivery anymore.
Other ways, including by surgery technique.
There are several types of Caesarean section (CS). An important distinction lies in the type of incision (longitudinal or latitudinal) made on the uterus, apart from the incision on the skin.
The EXIT procedure is a specialized surgical delivery procedure used to deliver babies who have airway compression.
The Misgav Ladach method is a modified caesarean section which has been used nearly all over the world since the 1990s. It was described by Michael Stark, the president of the New European Surgical Academy, at the time he was the director of Misgav Ladach, a general hospital in Jerusalem. The method was presented during a FIGO conference in Montréal in 1994 and then distributed by the University of Uppsala, Sweden, in more than 100 countries. This method is based on minimalistic principles. He examined all steps in caesarean sections in use, analyzed them for their necessity and, if found necessary, for their optimal way of performance. For the abdominal incision he used the modified Joel Cohen incision and compared the longitudinal abdominal structures to strings on musical instruments. As blood vessels and muscles have lateral sway, it is possible to stretch rather than cut them. The peritoneum is opened by repeat stretching, no abdominal swabs are used, the uterus is closed in one layer with a big needle to reduce the amount of foreign body as much as possible, the peritoneal layers remain unsutured and the abdomen is closed with two layers only. Women undergoing this operation recover quickly and can look after the newborns soon after surgery. There are many publications showing the advantages over traditional caesarean section methods. However, there is an increased risk of abruptio placenta and uterine rupture in subsequent pregnancies for women who underwent this method in prior deliveries.
Technique.
Antibiotic prophylaxis is used before an incision. The uterus is incised, and this incision is extended with blunt pressure along a cephalad-caudad axis. The infant is delivered, and the placenta is then removed. The surgeon then makes a decision about uterine exteriorization. Single-layer uterine closure is used when the mother does not want a future pregnancy. When subcutaneous tissue is 2 cm thick or more, surgical suture is used. Discouraged practices include manual cervical dilation, any subcutaneous drain, or supplemental oxygen therapy with intent to prevent infection.
Caesarean section can be performed with single or double layer suturing of the uterine incision. A Cochrane review came to the result that single layer closure compared with double layer closure was associated with a statistically reduction in mean blood loss. Standard procedure includes the closure of the peritoneum. However, research questions this may not be needed, with some studies indicating peritoneal closure is associated with longer operative time and hospital stay.
In many hospitals, especially in Argentina, the United States, United Kingdom, Canada, Norway, Sweden, Finland, Australia, and New Zealand, the mother's partner is encouraged to attend the surgery to support the mother and share the experience. The anaesthetist will usually lower the drape temporarily as the child is delivered so the parents can see their newborn.
Anaesthesia.
Both general and regional anaesthesia (spinal, epidural or combined spinal and epidural anaesthesia) are acceptable for use during Caesarean section. Regional anaesthesia is preferred as it allows the mother to be awake and interact immediately with her baby. Other advantages of regional anesthesia include the absence of typical risks of general anesthesia: pulmonary aspiration (which has a relatively high incidence in patients undergoing anesthesia in late pregnancy) of gastric contents and Oesophageal intubation.
Regional anaesthesia is used in 95% of deliveries, with spinal and combined spinal and epidural anaesthesia being the most commonly used regional techniques in scheduled Caesarean section. Regional anaesthesia during Caesarean section is different from the analgesia (pain relief) used in labor and vaginal delivery. The pain that is experienced because of surgery is greater than that of labor and therefore requires a more intense nerve block. The dermatomal level of anesthesia required for Caesarean delivery is also higher than that required for labor analgesia.
General anesthesia may be necessary because of specific risks to mother or child. Patients with heavy, uncontrolled bleeding may not tolerate the hemodynamic effects of regional anesthesia. General anesthesia is also preferred in very urgent cases, such as severe fetal distress, when there is no time to perform a regional anesthesia.
Prevention of complications.
Postpartum infection is one of the main causes of bad outcomes and death around childbirth, accounting for around 10% of maternal deaths globally. Caesarean section greatly increases the risk of infection and associated morbidity (estimated to be between 5 and 20 times as high). Infection can occur in around 8% of women who have caesareans, largely endometritis, urinary tract infections and wound infections.
Antibiotic prophylaxis is effective for endometritis, preventing as many as 3 out of 4 cases. Taking antibiotics before skin incision rather than after cord clamping reduces the risk for the mother, without increasing adverse effects for the baby. Whether a particular type of skin cleaner improves outcomes in unclear.
Some doctors believe that during a caesarean section, mechanical cervical dilation with a finger or forceps will prevent the obstruction of blood and lochia drainage, and thereby benefit the mother by reducing risk of death. The available clinical evidence is not sufficient to draw a conclusion on the effect of this practice.
Recovery period.
Typically, the recovery time depends on the patient and her pain tolerance and inflammation levels. Doctors do recommend abstention from strenuous work (e.g., lifting objects over 10 lbs (4.5 kg)., running, walking up stairs, or athletics) for up to sixteen weeks
, and a waiting period of ~18 months before attempting to conceive another child.
Usage.
In the United Kingdom, in 2008, the Caesarean section rate was 24%. In Ireland the rate was 26.1% in 2009. The Canadian rate was 26% in 2005–2006. Australia has a high Caesarean section rate, at 31% in 2007. In the United States the rate of C-section is around 33% and varies from 23% to 40% depending on the state in question.
In Italy the incidence of Caesarean sections is particularly high, although it varies from region to region. In Campania, 60% of 2008 births reportedly occurred via Caesarean sections. In the Rome region, the mean incidence is around 44%, but can reach as high as 85% in some private clinics.
With nearly 1.3 million stays, Caesarean section was one of the most common procedures performed in U.S. hospitals in 2011. It was the second-most common procedure performed for people ages 18 to 44 years old. Caesarean rates in the U.S. have risen considerably since 1996. The procedure increased 60% from 1996 to 2009. In 2010, the Caesarean delivery rate was 32.8% of all births (a slight decrease from 2009's high of 32.9% of all births). A study found that in 2011, women covered by private insurance were 11% more likely to have a caesarean section delivery than those covered by Medicaid.
China has been cited as having the highest rates of C-sections in the world at 46% as of 2008.
Studies have shown that continuity of care with a known carer may significantly decrease the rate of Caesarean delivery but there is also research that appears to show that there is no significant difference in Caesarean rates when comparing midwife continuity care to conventional fragmented care.
More emergency Caesareans—about 66%—are performed during the day rather than during the night.
The rate has risen to 46% in China and to levels of 25% and above in many Asian, European and Latin American countries. The rate has increased in the United States, to 33% of all births in 2012, up from 21% in 1996. Across Europe, there are differences between countries: in Italy the Caesarean section rate is 40%, while in the Nordic countries it is 14%.
Increasing use.
In the United States C-section rates have increased from just over 20% in 1996 to 33% in 2011. This increase has not resulted in improved outcomes resulting in the position that C-sections may be done too frequently.
The World Health Organization officially withdrew its previous recommendation of a 15% C-section rates in June 2010. Their official statement read, "There is no empirical evidence for an optimum percentage. What matters most is that all women who need caesarean sections receive them."
The US National Institutes of Health says rises in rates of Caesarean sections are not, in isolation, a cause for concern, but may reflect changing reproductive patterns: "The World Health Organization has determined an “ideal rate” of all caesarean deliveries (such as 15 percent) for a population. One surgeon's opinion is that there is no consistency in this ideal rate, and artificial declarations of an ideal rate should be discouraged. Goals for achieving an optimal caesarean delivery rate should be based on maximizing the best possible maternal and neonatal outcomes, taking into account available medical and health resources and maternal preferences. This opinion is based on the idea that if left unchallenged, optimal caesarean delivery rates will vary over time and across different populations according to individual and societal circumstances."
Some have speculated that caesarean section rates have increased due to a relationship between birth weight and maternal pelvis size, positing on the basis of Darwinian-inspired logic that since the advent of successful Caesarean birth over the last 150 years, more mothers with small pelvises and babies with large birth weights have survived and contributed to these traits. However, this idea fails to take into account that historically disproportion in childbirth was caused by maternal malnutrition in childhood, in particular malformed pelvic bones due to childhood rickets. Improved maternal nutrition should have led to increased ease in vaginal birth, not an increase in caesarean sections.
History.
The mother of Bindusara (born c. 320 BCE, ruled 298 – c.272 BCE), the second Mauryan Samrat (emperor) of India, accidentally consumed poison and died when she was close to delivering him. Chanakya, the Chandragupta's teacher and adviser, made up his mind that the baby should survive. He cut open the belly of the queen and took out the baby, thus saving the baby's life.
According to the ancient Chinese "Records of the Grand Historian", Luzhong, a sixth-generation descendant of the Yellow Emperor, had six sons, all born by "cutting open the body". The sixth son Jilian founded the House of Mi that ruled the State of Chu (c. 1030–223 BCE).
In the Irish mythological text the Ulster Cycle, the character Furbaide Ferbend is said to have been born by posthumous caesarean section, after his mother was murdered by his evil aunt Medb.
The Babylonian Talmud, an ancient Jewish religious text, mentions a procedure similar to the caesarean section. The procedure is termed "yotzei dofen".
Pliny the Elder theorized that Julius Caesar's name came from an ancestor who was born by caesarean section, but the truth of this is debated (see the article on the Etymology of the name of Julius Caesar). The Ancient Roman caesarean section was first performed to remove a baby from the womb of a mother who died during childbirth. Caesar's mother, Aurelia, lived through childbirth and successfully gave birth to her son, ruling out the possibility the Roman ruler and general was born by caesarean section. His first wife however died in childbirth, giving birth to a stilborn son who might have lived had a caesarean taken place.
The Catalan saint Raymond Nonnatus (1204–1240), received his surname—from the Latin "non-natus" ("not born")—because he was born by caesarean section. His mother died while giving birth to him.
An early account of caesarean section in Iran is mentioned in the book of Shahnameh, written around 1000 AD, and relates to the birth of Rostam, the national legendary hero of Iran. According to the Shahnameh, the Simurgh instructed Zal upon how to perform a Caesarean section, thus saving Rudaba and the child Rostam.
Caesarean section usually resulted in the death of the mother; the first recorded incidence of a woman surviving a caesarean section was in the 1580s, in Siegershausen, Switzerland: Jakob Nufer, a pig gelder, is supposed to have performed the operation on his wife after a prolonged labour. However, there is some basis for supposing that women regularly survived the operation in Roman times. For most of the time since the 16th century, the procedure had a high mortality rate. However, it was long considered an extreme measure, performed only when the mother was already dead or considered to be beyond help. In Great Britain and Ireland, the mortality rate in 1865 was 85%. Key steps in reducing mortality were:
European travelers in the Great Lakes region of Africa during the 19th century observed Caesarean sections being performed on a regular basis. The expectant mother was normally anesthetized with alcohol, and herbal mixtures were used to encourage healing. From the well-developed nature of the procedures employed, European observers concluded they had been employed for some time. Dr. James Barry carried out the first successful Caesarean by a European doctor in Africa in Cape Town, while posted there between 1817 and 1828.
The first successful Caesarean section to be performed in America took place in what was formerly Mason County, Virginia (now Mason County, West Virginia), in 1794. The procedure was performed by Dr. Jesse Bennett on his wife Elizabeth.
On March 5, 2000, in Mexico, Inés Ramírez performed a Caesarean section on herself and survived, as did her son, Orlando Ruiz Ramírez. She is believed to be the only woman to have performed a successful Caesarean section on herself.
Society and culture.
Etymology.
The Roman "Lex Regia" (royal law), later the "Lex Caesarea" (imperial law), of Numa Pompilius (715–673 BCE), required the child of a mother dead in childbirth to be cut from her womb.
This seems to have begun as a religious requirement that mothers not be buried pregnant, and to have evolved into a way of saving the fetus, with Roman practice requiring a living mother to be in her tenth month of pregnancy before resorting to the procedure, reflecting the knowledge that she could not survive the delivery. Speculation that the Roman dictator Julius Caesar was born by the method now known as C-section is apparently false. Although Caesarean sections were performed in Roman times, no classical source records a mother surviving such a delivery – the earliest recorded survival dates to the 12th century scholar and physician Maimonides (see Commentary to Mishnah Bekhorot 8:2). The term has also been explained as deriving from the verb "caedere", "to cut", with children delivered this way referred to as "caesones". Pliny the Elder refers to a certain Julius Caesar (an ancestor of the famous Roman statesman) as "ab utero caeso", "cut from the womb" giving this as an explanation for the cognomen "Caesar" which was then carried by his descendents. Nonetheless, even if the etymological hypothesis linking the caesarean section to Julius Caesar is a false etymology, it has been widely believed. For example, the "Oxford English Dictionary" defines Caesarean birth as "the delivery of a child by cutting through the walls of the abdomen when delivery cannot take place in the natural way, as was done in the case of Julius Caesar". "Merriam-Webster's Collegiate Dictionary" (11th edition) leaves room for etymological uncertainty with the phrase, "from the legendary association of such a delivery with the Roman cognomen "Caesar""
Some link with Julius Caesar or with Roman emperors exists in other languages as well. For example, the modern German, Norwegian, Danish, Dutch, Swedish, Turkish and Hungarian terms are respectively "Kaiserschnitt", "keisersnitt", "kejsersnit","keizersnede", "kejsarsnitt", "sezaryen", and "császármetszés" (literally: "Emperor's cut"). The German term has also been imported into Japanese (帝王切開 "teiōsekkai") and Korean (제왕 절개 "jewang jeolgae"), both literally meaning "emperor incision". Similar in western Slavic (Polish) "cięcie cesarskie", (Czech)"císařský řez" and (Slovak) "cisársky rez" (literally "imperial cut"), whereas the south Slavic term is Serbian "царски рез" and Slovenian "cárski réz", which literally means "tzar" cut. The Russian term "kesarevo secheniye" (Кесарево сечение "késarevo sečénije") literally means "Caesar's section". The Arabic term (ولادة قيصرية "wilaada qaySaríyya") also means "Caesarean birth." The Hebrew term ניתוח קיסרי ("nitúakh Keisári") translates literally as Caesarean surgery. In Romania and Portugal, it is usually called "cesariana", meaning from (or related to) Caesar.
According to Shahnameh ancient Persian book, the hero Rostam was the first person who was born with this method and term رستمينه ("rostamineh") is corresponded to Caesarean. Also, Hindu mythical monkey god Hanuman was born through a similar procedure on her mother Anjani.
Finally, the Roman praenomen (given name) Caeso was said to be given to children who were born via C-section. While this was probably just folk etymology made popular by Pliny the Elder, it was well known by the time the term came into common use.
Orthography.
The term "Caesarean section" is spelled in many different ways.
One variation is the e/ae/æ variation which reflects American and British English spelling differences. Because some sources say the procedure is named after Julius Caesar, the procedure's name is sometimes capitalized. The capital-versus-lowercase variation reflects a style of lowercasing some eponymous terms (e.g., "caesarean, eustachian, fallopian, mendelian, parkinsonian, parkinsonism"). Capital and lowercase stylings coexist in prevalent usage.
Because of (1) the "e"-vs-"ae" digraph variation, (2) the related ae-vs-æ typographic ligature variation, (3) the capital-vs-lowercase variation (which is based on the idea of eponymous origin, whether that is historically accurate or not; see eponym > orthographic conventions), and (4) the "-ean"-vs-"-ian" suffix variation, these factors cross-multiplied in a table cause this word to be one of the very few words in present-day English orthography to have many different normative spellings or orthographic stylings, which amount to 12 from the point of view of character encoding (that is, there are 12 different character strings that are all accepted as normative orthographic representations of this one word):
Special populations.
In Judaism there is a dispute among the "poskim" (Rabbinic authorities) as to whether a first-born son from a Caesarean section has the laws of a "bechor". Traditionally, a male child delivered by Caesarean is not eligible for the "Pidyon HaBen" dedication ritual.
In rare cases, caesarean sections can be used to remove a dead fetus. A late-term abortion using Caesarean section procedures is termed a hysterotomy abortion and is very rarely performed.
Self-inflicted caesarean section is the concept of a mother alone performing her own caesarean section. There have apparently been a few successful cases, notably Inés Ramírez Pérez of Mexico who in March 2000, performed a successful Caesarean section on herself.

</doc>
<doc id="46925" url="http://en.wikipedia.org/wiki?curid=46925" title="Greyhawk">
Greyhawk

Greyhawk, also known as the World of Greyhawk, is a fictional world designed as a campaign setting for the "Dungeons & Dragons" fantasy roleplaying game. Although not the first campaign world developed for "Dungeons & Dragons"—Dave Arneson's Blackmoor campaign predates it by a few months—the world of Greyhawk was the setting most closely identified with the development of the game from 1972 until 2008. The world itself started as a simple dungeon under a castle designed by Gary Gygax for the amusement of his children and friends, but it rapidly expanded to include not only a complex multi-layered dungeon environment, but also the nearby city of Greyhawk, and eventually an entire world. In addition to the campaign world, which was published in several editions over twenty years, Greyhawk was also used as the setting for many adventures published in support of the game, as well as for RPGA's massively shared "Living Greyhawk" campaign from 2000–2008.
Early development.
In the late 1960s, Gary Gygax, a military history buff and pulp fantasy fan, started to add elements of fantasy into traditional tabletop medieval miniatures wargames at his games club in Lake Geneva, Wisconsin. He sometimes replaced typical medieval weapons with magical spells, or used dragons and other fantastical monsters in place of soldiers. In 1971, as part of a rule set for tabletop battles called "Chainmail" that he was co-writing, he created supplementary rules for magical spells and monsters as well as one-on-one combat.
Around the same time, in Minneapolis–St. Paul, another tabletop wargamer, Dave Arneson, was also developing a new type of game. Arneson had been impressed by the Napoleonic tabletop "Braunstein" campaigns of fellow wargamer David Wesely that incorporated elements of what is now called role-playing, such as using a neutral referee or judge and having conversations between the players and imaginary characters to resolve diplomatic issues. However, Arneson soon grew tired of the Napoleonic setting, and one night when the gaming group assembled, he presented a plastic model of a castle in place of the usual battlefield, and told the players that instead of controlling regiments that night, they would each take one individual character into the castle of the Barony of Blackmoor to explore its dangerous dungeons. For combat resolution, he started by using rock-paper-scissors, but quickly moved to a combination of rules that combined "Chainmail" and a nautical wargame he had co-written with Gary Gygax and Mike Carr called "Don't Give Up the Ship!" What set Arneson's game apart from Wesely's tabletop wargaming was that the players could keep the same characters from session to session, and that the characters "advanced" by developing better abilities or powers over time.
Arneson's Minneapolis-St. Paul Napoleonic gaming group was in touch with Gygax's Lake Geneva group, and Arneson mentioned the dungeons of his Blackmoor game that the group was playing on alternate weekends. Gygax was interested, so during a visit to Lake Geneva in 1972, Arneson demonstrated his Blackmoor dungeons to Gygax. Gygax was immediately intrigued by the concept of individual characters exploring a dungeon setting, and believed that this was a game that could be marketed and sold. He and Arneson agreed to co-develop a set of rules based on "Chainmail". In order to provide a playtesting environment in which to develop these rules, Gygax designed his own castle, Castle Greyhawk, and prepared the first level of a dungeon that lay beneath it. Two of his children, Ernie and Elise, were the first players, and during their first session, they fought and destroyed the first monsters of the Greyhawk dungeon; Gygax recalled them as being either giant centipedes or a nest of scorpions. During the same session, Ernie and Elise also found the first treasure, a chest of 3,000 copper coins which was too heavy to carry, much to the children's chagrin. After his children had gone to bed, Gygax immediately began working on a second level for the dungeon. At the next play session, Ernie and Elise were joined by Gygax's friends: Don Kaye, Rob Kuntz, and Terry Kuntz.
About a month after his first session, Gygax created the nearby city of Greyhawk, where the players' characters could sell their treasure and find a place to rest.
Home campaign (1972–1979).
As Gygax and Arneson worked to develop and publish the rules for "Dungeons & Dragons" through TSR, Gygax continued to design and present the dungeons and environs of Castle Greyhawk to his circle of friends and family, using them as playtesters for new rules and concepts. As the players began to explore more of the world outside of the castle and city, Gygax developed other regions and cities for them. With play sessions occurring seven or more times a week, Gygax didn't have the time or inclination to create the map for a whole new world; he simply drew his world over a map of North America, adding new cities and regions as his world slowly grew through ongoing adventures. The city and castle of Greyhawk were placed near the real-world position of Chicago, his birthplace; various other places were clustered around it. For instance, the rival city of Dyvers he placed in the area of real-world Milwaukee.
Gygax also continued to develop the dungeons underneath the castle. By the time he was finished, the complex labyrinth encompassed thirteen levels filled with devious traps, secret passageways, hungry monsters, and glittering treasure. Although details of these original Greyhawk dungeons have never been published in detail, Gygax gave some glimpses of them in an article he wrote for the European fanzine "Europa" in 1975:
Anyone who made it to the bottom level alive met Zagyg, the insane architect of the dungeons. ("Zagyg" is a reverse homophone of "Gygax", and it was Gygax's inside joke that the person who had designed the dungeon—himself—must be insane.) Only three players ever made it to the bottom level and met Zagyg, all of them during solo adventures: Rob Kuntz (playing Robilar), Gygax's son Ernie (playing Tenser), and Rob's brother Terry (playing Terik). Their reward was to be instantly transported to the far side of the world, where they each faced a long solo trek back to the city of Greyhawk. Terik and Tenser managed to catch up to Robilar along the way, and the three journeyed back to Greyhawk together.
By this time, over twenty players crowded Gygax's basement almost every night, and the effort needed to plan their adventures took up much of Gygax's spare time. He had been very impressed with Rob Kuntz's imaginative play as a player, and appointed Rob to be co-Dungeon Master of Greyhawk. This freed up Gygax to work on other projects, and also gave him an opportunity to participate as a player, creating characters like Yrag and Mordenkainen.
In order to make room for Rob Kuntz's dungeons, Gygax scrapped his bottom level and integrated Rob's work into the Greyhawk dungeons. Gygax and Kuntz continued to develop new levels for their players, and by the time the Greyhawk home campaign drew to a close in 1985, the castle dungeons encompassed more than fifty levels.
Significant player characters of the home campaign.
While many players participating in the Gygax and Kuntz home campaign were occasional players, sometimes not even naming their characters, others played far more frequently, and several of their characters became well-known to the general gaming world before publication of the Greyhawk campaign setting. Some of these characters became known when Gygax mentioned them in his various columns, interviews, and publications. In other cases, when Gygax created a new magical spell for the game, he would sometimes use the name of a wizard character from his home campaign to add verisimilitude to the spell name, such as "Melf's acid arrow", Melf being a character created by his son Luke. Some of the characters who became synonymous with Greyhawk at that time included:
Greyhawk firsts.
The first mention of Oerth.
In the first issue of "The Dragon" published in June 1976, Gygax prefaced Chapter 1 of his serialized novella "The Gnome Cache" with a note that the story's setting, Oerth, was very similar to Earth in terms of geography.
The first deities of Greyhawk.
One facet of culture that Gygax did not address during the first few years of his home campaign was organized religion. Since his campaign was largely built around the needs of lower-level characters, he did not think specific deities were necessary, since direct interaction between a god and a low-level character was very unlikely. Some of his players took matters into their own hands, calling upon Norse or Greek gods such as Odin or Zeus, or even Conan's Crom in times of dire need. However, some of the players wanted Gygax to create and customize a specific deity so that cleric characters could receive their powers from someone less ambiguous than "the gods". Gygax jokingly created two gods: Saint Cuthbert—who brought non-believers around to his point of view with whacks of his cudgel —and Pholtus, whose fanatical followers refused to believe that any other gods existed. Because both of these deities represented aspects of Good, Gygax eventually created a few evil deities to provide some villainy.
In Chapter 2 of "The Gnome Cache", which appeared in the second issue of "The Dragon", a shrine to St. Cuthbert (spelled "St. Cuthburt") was mentioned, which was the first published reference to a Greyhawk deity.
The first Greyhawk novel.
In 1976, Gygax invited the science fiction/fantasy writer Andre Norton to play "Dungeons & Dragons" in his Greyhawk world. Norton subsequently wrote "Quag Keep", which involved a group of gamers who travel from the real world to Greyhawk. It was the first novel to be set, at least partially, in the Greyhawk setting, and according to "Alternative Worlds", the first to be based on "D&D". "Quag Keep" was excerpted in Issue 12 of "The Dragon" (February 1978) just prior to the book's release.
The first Greyhawk adventures published by TSR.
From 1976–1979, Gygax also shared some glimpses of his home campaign with other gamers when he set several TSR "Dungeons & Dragons" adventures in the world of Greyhawk:
In addition, Lawrence Schick set his 1979 TSR adventure S2 "White Plume Mountain" in Greyhawk.
"The World of Greyhawk" folio edition (1980).
In 1975, Gygax and Kuntz published a booklet called "Supplement I: Greyhawk", an expansion of the rules for "Dungeons & Dragons" based on their play experiences in the Greyhawk campaign. Although it detailed new spells and character classes that had been developed in the dungeons of Greyhawk, it did not contain any details of their Greyhawk campaign world. The only two references to Greyhawk were an illustration of a large stone head in a dungeon corridor titled "The Great Stone Face, Enigma of Greyhawk" and mention of a fountain on the second level of the dungeons that continuously issued an endless number of snakes.
The 2004 publication "" suggested that details of Gygax's Greyhawk campaign were published in this booklet, but Gygax had no plans in 1975 to publish details of the Greyhawk world, since he believed that new players of "Dungeons & Dragons" would rather create their own worlds than use someone else's. In addition, he didn't want to publish all the material he had created for his players; he thought he would be unlikely to recoup a fair investment for the thousands of hours he had spent on it; and since his secrets would be revealed to his players, he would be forced to recreate a new world for them afterward.
With the release of the "AD&D Players Handbook" in 1978, many players were intrigued by the connection of Greyhawk characters to magical spells such as "Tenser's floating disc", "Bigby's crushing hand", and "Mordenkainen's faithful hound". The "AD&D Dungeon Masters Guide", released the following year, also made references to the dungeons of Castle Greyhawk. Players' curiosity was further piqued by the ten "Dungeons & Dragons" modules set in Greyhawk that were published between 1976–1979. Several of Gygax's regular columns in "Dragon" magazine also mentioned details of his home campaign and characters that inhabited his world. Gygax was surprised when he found out that players wanted to use Greyhawk as their campaign world.
Development of geography.
In response to this, Gygax changed his mind and decided he would publish his private campaign world, but with some important changes. Rather than using his own map, which was simply the real-world Earth overwritten with his cities, towns and regions, he decided to create a new world called Oerth. Gygax joked, "Say it as "Oi-th" as if you were from Brooklyn, and that's the way I pronounce it. That annoys all who take a fantasy world far too seriously." Once he had sketched out the entire planet, Gygax decided to concentrate his first efforts on one small corner of the world. One hemisphere of Oerth was dominated by a massive continent called Oerik. Gygax asked TSR's printing house about the maximum size of paper they could handle; the answer was 34" x 22" (86 cm x 56 cm). He found that, using the scale he desired, he could only fit the northeast corner of the continent of Oerik on two of the sheets. He therefore concentrated on providing details for less than a quarter of the landmass of Oerth, concentrating on the eastern part of the continent of Oerik that would be illustrated by his map.
Gygax gave only the most basic descriptions of each state; he expected that DMs would customize the setting in order to make it an integral part of their own individual campaigns. In order to give the campaign setting as much flexibility as possible in terms of geographic settings, his map included arctic wastes, desert, temperate forests, tropical jungles, mountainous cordillera, seas and oceans, rivers, archipelagos and volcanoes. He placed the city and castle of Greyhawk roughly in the centre of the map, in an area that would have about the same temperate climate as his home in Lake Geneva. For the other regions that had surrounded the city of Greyhawk on his old map, some were left relatively close to the city of Greyhawk; for instance, the rivalry between the cities of Dyvers, Hardby, and Greyhawk was a feature of Gygax's campaign, so the three cities were placed in close proximity to each other. However, most other regions were moved further away, scattered across the new map. Gygax also added many more new regions, countries and cities, bringing the number of political states to sixty.
Needing original names for all of the geographical and political places on his map, Gygax sometimes resorted to wordplay based on the names of friends and acquaintances. For instance, Perrenland was named after Jeff Perren, who co-wrote the rules for "Chainmail" with Gygax; Urnst was a homophone of Ernst (his son Ernie); and Sunndi was a near-homophone of Cindy, another of Gygax's children. From Gygax's prototype map, Darlene Pekul, a freelance artist in Lake Geneva, developed a full color map on a hex grid. Gygax was so pleased with the end result that he quickly switched his home Greyhawk campaign over to the new world he had created.
Development of history and politics.
Gygax set out to create a fractious place where chaos and evil were in the ascendant and courageous champions would be needed. In order to explain how his world had arrived at this state, he wrote an outline of a thousand years of history. As a military history buff, he was very familiar with the concept of waves of cultural invasions, such the Picts of Great Britain being invaded by the Celts, who were in turn invaded by the Romans. In creating a similar pattern of history for his world, Gygax decided that a thousand years before his campaign began, the northeast corner of the continent had been occupied by a peaceful but primitive people called the Flannae, whose name was the root for the name of that part of Oerik, the "Flanaess". At that time, far to the west of the Flanaess, two peoples were at war, the Bakluni and the Suloise. The war reached its climax when both sides used powerful magic to obliterate each other, in an event called the Twin Cataclysms. Refugees of these disasters were forced out of their lands, and the Suloise invaded the Flanaess, forcing the Flannae to flee to the outer edges of the continent. Several centuries later, a new invader appeared, the Oeridians, and they in turn forced the Suloise southward. One tribe of the Oeridians, the Aerdi, began to set up an empire. Several centuries later, the Aerdi's Great Kingdom ruled most of the Flanaess. The Aerdi overkings marked the beginning of what they believed would be perpetual peace with Year 1 of a new calendar, the Common Year (CY) Reckoning. However, several centuries later, the Empire became decadent, with their rulers losing their sanity, turning to evil, and enslaving their people. When the overking Ivid V came to the throne, the oppressed peoples rebelled.
It was at this point, in the year 576 CY, that Gygax set the world of Greyhawk. As Gygax wrote in his "World of Greyhawk" folio, "The current state of affairs in the Flanaess is confused indeed. Humankind is fragmented into isolationist realms, indifferent nations, evil lands, and states striving for good." Gygax did not issue monthly or yearly updates to the state of affairs as presented in the folio since he saw 576CY as a common starting point for every home campaign; because each would be moving forward at its own pace, there would be no practical way to issue updates that would be relevant to every Dungeon Master.
Gygax was also aware that different players would be using his world for different reasons. When he was the Dungeon Master of his home campaign, he found that his players were more interested in dungeon-delving than politics; but when he switched roles and became a player, often going one-on-one with Rob Kuntz as Dungeon Master, Gygax immersed his own characters in politics and large-scale battles. Knowing that there would be some players looking for a town in which to base their campaign, and others interested in politics or warfare, Gygax tried to include as much detail as possible about each region, including a short description of the region and its people, the title of its ruler, the racial makeup of its people, its resources and major cities, and its allies and enemies.
For the same reason that he had created a variety of geographical, political and racial settings, he also strove to create a world with some good, some evil, and some undecided areas. He felt that some players would be happiest playing in a mainly good country and fighting the evil that arose to threaten it; others might want to be a part of an evil country; and still others might take a neutral stance and simply try to collect gold and treasure from both sides.
Publication.
TSR originally intended to publish "The World of Greyhawk" (TSR 9025) early in 1979, but it was not released until August 1980. "The World of Greyhawk" consisted of a 32-page folio (the first edition is often called the "World of Greyhawk folio" to distinguish it from later editions) and a 34" x 44" (86 cm x 112 cm) two-piece color map of the Flanaess. Reviewers were generally impressed, but some remarked on the lack of a pantheon of Greyhawk-specific deities, as well as the lack of any mention of the infamous dungeons of Castle Greyhawk.
Game designer Jim Bambra found the original set "disappointing", because "there is only so much information you can cram into a 32-page booklet, particularly when covering such a large area".
Between editions (1980–1983).
Before the folio edition was released, Gygax planned to publish supplementary information, using his column "From the Sorcerer's Scroll" that appeared on a semi-regular basis in TSR's "Dragon Magazine".
In the May 1980 issue, Gygax gave a quick overview of the development of his new "The World of Greyhawk" folio. For players who planned to use large scale army tactics, he gave details of the private armies that were commanded by some prominent Greyhawk characters from his original home game: Bigby, Mordenkainen, Robilar, Tenser and Erac's Cousin. Gygax also mentioned some of the planned Greyhawk publications he was overseeing: a large-scale map of the city of Greyhawk; some adventure modules set in Greyhawk; a supplementary map of lands outside the Flanaess; all fifty levels of Castle Greyhawk's dungeon; and miniatures army combat rules. None of these projects, other than a few of the adventure modules, were published by TSR.
Although Gygax originally intended to immediately publish more details of Greyhawk in "Dragon" on a regular basis, other projects intervened, and it was not until the August 1981 issue of "Dragon" that Len Lakofka, in his column "Leomund's Tiny Hut", outlined methods for determining a character's place of birth and languages spoken. Gygax added an addendum concerning the physical appearances of the main Greyhawk races. In the November 1981 issue, Gygax gave further details of racial characteristics and modes of dress.
In the December 1982 issue, David Axler contributed a system for determining weather in the world of Greyhawk. Gygax later said he thought a system of fourteen charts for determining the weather was too cumbersome, and he personally didn't use it in his home campaign.
More information about every political region.
The folio edition had thirty two pages, and information about each region was condensed into a short paragraph or two. Gygax realized that some players needed more in-depth information about the motivations and aspirations of each region, and the history of interactions with surrounding regions. With this in mind, Gygax decided to publish a much longer description of each region in "Dragon". The first two articles, covering seventeen regions, appeared in the December 1981 and January 1982 issues. Due to his involvement in many other TSR projects, Gygax handed responsibility for completion of this project to Rob Kuntz, who covered the remaining forty three regions in the March 1982, July 1982 and September 1982 issues.
Deities of Greyhawk.
In the August 1982 issue of "Dragon", Gygax gave advice on how to adapt deities from the previously published "Deities and Demigods" for worship by non-human races in the Greyhawk world. A few months later, he published a five-part series of articles in the November 1982 through March 1983 issues of "Dragon" that outlined a pantheon of deities custom-made for humans in the world of Greyhawk. In addition to his original Greyhawk deities, St. Cuthbert and Pholtus, Gygax added seventeen more deities. Although later versions of the campaign setting would assign most of these deities to worship by specific races of humans, at this time they were generally worshiped by all humans of the Flanaess.
Shortly after the release of the folio edition, TSR released the adventure module C1 "The Hidden Shrine of Tamoachan", designed to familiarize players with the Olman race of the Amedio Jungle. Largely based on Aztec and Incan cultures, this adventure introduced the first published deities of the Greyhawk campaign: Mictlantecuhtli, god of death, darkness, murder and the underworld; Tezcatlipoca, god of sun, moon, night, scheming, betrayals and lightning; and Quetzalcoatl, god of air, birds and snakes. However, this area of the Flanaess was not explored further in any subsequent TSR adventures or source material, and these three gods would remain isolated from the main pantheon for almost twenty years.
Non-player characters of Greyhawk.
Also included in the March 1983 issue of "Dragon" was an article detailing four unique Greyhawk characters. The first two "quasi-deities"—Heward and Keoghtom—had been created by Gygax as non-player characters (NPCs). The third, Murlynd, was a character that had been created by Gygax's childhood friend Don Kaye before Kaye's untimely death in 1975. The fourth, a "hero-deity" named Kelanen, was developed to illustrate the "principle of advancement of power".
TSR Greyhawk adventures published after the folio edition.
Of the ten adventures set in Greyhawk published by TSR before the folio edition, all but one had been written by Gygax. However, the new availability of information about Gygax's campaign world and TSR's desire to make it central to "Dungeons & Dragons" encouraged many new writers to set their adventures in Greyhawk. This, combined with the fact that Gygax was increasingly involved in other areas of the company, meant that of the seventeen Greyhawk adventures published in the two years after the folio edition, only four were written or co-written by Gygax:
In 1981, TSR also published the "super-modules" D1-2 "Descent into the Depths of the Earth" and G1-2-3 "Against the Giants", both being compilations of previously published modules from the "Drow" series and the "Giant" series respectively.
Numerous projects were planned to add more depth and detail to the setting after the publication of the initial folio, but many of these projects never appeared for various reasons.
"World of Greyhawk" boxed set (1983).
In 1983, TSR published an expanded boxed set of the campaign world, "World of Greyhawk", which is usually called the "Greyhawk boxed set" to differentiate it from other editions. According to game designer Jim Bambra, "The second edition was much larger than the first and addressed itself to making the World of Greyhawk setting a more detailed and vibrant place." This edition quadrupled the number of pages from the original edition to 128, adding significantly greater detail. One major addition was a pantheon of deities: in addition to the nineteen deities outlined by Gygax in his "Dragon" article, another thirty one new deities were added, though only three received full write-ups of their abilities and worshipers. This brought the number of Greyhawk deities to an even fifty. For the next eight years, Greyhawk would be primarily defined by the information in this publication.
After publication of the boxed set (1984–1985).
Publication of the "World of Greyhawk" was the first step in Gygax's vision for Oerth. Over the next few years, he planned to unveil other areas of the continent of Oerik, giving each new area the same in-depth treatment of history, geography, and politics as had been accorded the Flanaess. Gygax had also mapped out the other hemisphere of Oerth in his personal notes. Part of this would be Gygax's work, but Len Lakofka and Francois Froideval had also created material that Gygax wanted to place on Oerth. Frank Mentzer, Creative Consultant at TSR at the time, wrote four RPGA tournament adventures taken from his home campaign setting of "Aquaria" (published by TSR as the first four of the R-series modules: R1 "To the Aid of Falx", R2 "The Investigation of Hydell", R3 "The Egg of the Phoenix", and R4 "Doc's Island"). Mentzer envisioned them as the first part of a new "Aqua-Oeridian" campaign set somewhere on Oerth outside of the Flanaess.
However, by this time, Gygax was in Hollywood on a semi-permanent basis, approving scripts for the Saturday morning "Dungeons & Dragons" cartoon series and trying to land a deal for a "D&D" movie. Not only was Gygax's own output of Greyhawk-related materials greatly reduced, but the company began to shift its focus and resources away from Greyhawk to a new campaign setting called "Dragonlance".
The success of the Dragonlance series of modules and books pushed aside the World of Greyhawk setting, as TSR concentrated on expanding and defining the world of Krynn. One of the factors that contributed to the success of the "Dragonlance" setting when it was published in 1984 was a series of concurrent novels by Tracy Hickman and Margaret Weis. Gygax realized that novels set in Greyhawk could have a similar benefit for his campaign world and wrote "Saga of Old City", the first in a series of novels that would be published under the banner "Greyhawk Adventures". The protagonist was Gord the Rogue, and this first novel told of his rise from the Slum Quarters of the city of Greyhawk to become a world traveler and thief extraordinaire. The novel was designed to promote sales of the boxed set by providing colorful details about the social customs and peoples of various cities and countries around the Flanaess.
Before "Saga of Old City" was released in November 1985, Gygax wrote a sequel, "Artifact of Evil". He also wrote a short story, "At Moonset Blackcat Comes", that appeared in the special 100th issue of "Dragon" in August 1985. This introduced Gord the Rogue to gamers just before "Saga of Old City" was scheduled to be released.
Greyhawk modules.
In the two years after the Greyhawk boxed set appeared, TSR published eight adventures set in Greyhawk. Five were written or co-written by Gygax, and the other three were from TSR's United Kingdom division:
Both of the EX adventures, although nominally set in Greyhawk, transported characters through a planar gate into an alternate reality.
"Dragon" articles.
From 1983–1985, the only notable supplement for the Greyhawk world was a five-part article by Len Lakofka in the June–October and December 1984 issues of "Dragon" that detailed the Suel gods who had been briefly mentioned in the boxed set. In the December 1984 issue, Gygax mentioned clerics of non-human races and indicated that the twenty four demihuman and humanoid deities that had been published in the February–June 1982 issues of "Dragon" were now permitted in Greyhawk; this increased the number of Greyhawk deities from fifty to seventy four.
Other than those articles, Greyhawk was only mentioned in passing in three other issues until Gygax's "Gord the Rogue" short story in the August 1985 issue "Dragon". Gygax then provided some errata for the boxed set in the September 1985 issue, which was the last mention of the Greyhawk world in "Dragon" for almost two years.
Gygax departs.
Shortly after the release of the boxed set, Gygax discovered that while he had been in Hollywood, TSR had run into serious financial difficulties. Returning to Lake Geneva, Gygax managed to get TSR back on firm financial footing. However, different visions of TSR's future caused a power struggle within the company, and Gygax was forced out of TSR on December 31, 1985.
By the terms of his settlement with TSR, Gygax kept the rights to Gord the Rogue as well as all "Dungeons & Dragons" characters whose names were anagrams or versions of his own name, such as Yrag and Zagyg. However, he lost the rights to all his other work, including the world of Greyhawk and the names of all the other characters he had ever used in TSR material.
Greyhawk without Gygax (1986–1987).
After Gygax left TSR, the continued development of Greyhawk became the work of many writers and creative minds. Rather than continuing forward with Gygax's plan for an entire planet, the setting was never expanded beyond the Flanaess, nor would other authors' work be linked to unexplored areas of the continent Oerik. According to Gygax, TSR's stewardship turned Greyhawk into something very different from what he had envisioned.
In 1986, in the months following Gygax's ousting, TSR turned away from development of Greyhawk and focused its energies on a new campaign setting called "Forgotten Realms". In 1986 and 1987, only three Greyhawk modules were released, A1-4 "Scourge of the Slave Lords", S1-4 "Realms of Horror" and GDQ1-7 "Queen of the Spiders", all being collections of previously published modules rather than new material.
Greyhawk novels continue without Gord the Rogue.
Gygax's novel "Saga of Old City", released in November 1985, and "Artifact of Evil", released two months after Gygax's departure from TSR, proved to be popular titles, and in 1987, TSR hired Rose Estes to continue the series, albeit without Gord the Rogue, to whom Gygax had retained all rights. From 1987–1989, Estes produced five more novels under the "Greyhawk Adventures" banner: "Master Wolf", "The Price of Power", "The Demon Hand", "The Name of the Game", and "The Eyes Have It". A sixth book, "Dragon in Amber", appeared in 1990 book catalogs, but was never written, and the series was discontinued.
The dungeons of Greyhawk revealed.
In its 1986 Summer Mail Order Hobby Shop catalog, TSR had listed a new Greyhawk adventure called WG7 "Shadowlords", a high-level adventure to be written by Gary Gygax and Skip Williams. However, this adventure was canceled after Gygax left TSR, and the catalog number WG7 was reassigned to a new adventure, "Castle Greyhawk", released in 1988. It was the first new Greyhawk adventure in three years, but it had nothing to do with Gygax's original Castle Greyhawk. Instead, it was a compilation of twelve humorous dungeons levels, each one written by a freelance author. The puns and jokes often referenced modern culture—the Amazing Driderman, King Burger, Bugsbear Bunny, and the crew of —and the module also included an appearance by Gygax's Mordenkainen in a movie studio.
Greyhawk revived (1988–1990).
By 1988, with the first series of "Dragonlance" adventures drawing to a close, and "Forgotten Realms" doing very well, TSR turned back to Greyhawk. In the January 1988 issue of "Dragon", Jim Ward—one of the original players in the dungeons of Greyhawk, creator of the wizard Drawmij, and now working for TSR in the post-Gygax era—requested player input about what should be included in a hardcover source book for Greyhawk. He received over five hundred letters in response. In the August 1988 issue of "Dragon", he outlined the ideas from readers that been included, and "Greyhawk Adventures" appeared shortly afterward as a response to requests from Greyhawk fans. The book's title was borrowed from Rose Estes' "Greyhawk Adventures" line of novels and used the same front-cover banner design. It was the thirteenth and final hardcover book published for the 1st edition Advanced Dungeons & Dragons rules.
The contents were designed to give Dungeon Masters ideas and play opportunities unique to the Greyhawk world, including new monsters, magical spells and items, a variety of geographical features, profiles of prominent citizens, and the avatars of deities. In the time since Gygax had left TSR, no original Greyhawk material had been published, and many letter writers had requested ideas for new adventures. Ward responded by including six plot outlines that could be inserted into a Greyhawk campaign.
"The City of Greyhawk" boxed set.
The publication of "Greyhawk Adventures" came just as TSR released the 2nd edition of "Dungeons & Dragons". TSR released "The City of Greyhawk" boxed set in 1989 under the "Greyhawk Adventures" banner. Written by Carl Sargent and Rik Rose, this was not the city created by Gygax and Kuntz, but a new plan built from references made in previously published material.
This release remolded Gary Gygax's old Circle of Eight into a new plot device. Instead of a group of eight companions based in the Obsidian Citadel who left periodically to fight evil, the Circle became eight wizards led by a ninth wizard, Gygax's former character Mordenkainen. In addition to Mordenkainen, seven of the wizards were previously existing characters from Gygax's original home game: Bigby, Otiluke, Drawmij, Tenser, Nystul, Otto, and Rary. The eighth was new: the female wizard Jallarzi Sallavarian. The Circle's mandate was to act as neutral referees between Good and Evil, never letting one side or the other gain the upper hand for long. In addition, Sargent & Rose took Gygax's original Obsidian Citadel, re-purposed it as Mordenkainen's castle, and placed it in an unspecified location in the Yatil Mountains.
The following year, in conjunction with this boxed set, TSR published a trilogy of "World of Greyhawk" Adventure (WGA) modules by Richard & Anne Brown—WGA1 "Falcon's Revenge", WGA2 "Falconmaster" and WGA3 "Flames of the Falcon"—set in the city and centered on a mysterious villain called The Falcon. A fourth WGA module, WGA4 "Vecna Lives!" by David Cook, was published the same year, and featured the first appearance by Vecna, formerly a mythic lich in "Dungeons & Dragons" lore, now promoted to demigod status.
Modules released under the "Greyhawk Adventures" banner.
TSR also released five new "World of Greyhawk" (WG) adventures which used the "Greyhawk Adventures" banner:
In 1990, TSR also published WGR1 "Greyhawk Ruins", a module and source book about Castle Greyhawk by TSR writers Blake Mobley and Timothy Brown. Although this was not the Castle Greyhawk of Gygax and Kuntz, it was the first serious attempt to publish details of the castle.
A new vision of the Flanaess (1991–1997).
In 1990, TSR decided that the decade-old world of Greyhawk needed to be refreshed. Rather than expand beyond the boundaries of the Flanaess to develop new lands, the decision was made to stay within the Flanaess and move the campaign time line forward a decade, from 576 CY to 586 CY, in order to provide the setting for a new storyline. The main story vehicle would be a war fomented by an evil half-demon named Iuz that involved the entire Flanaess, which would allow TSR to radically alter the pattern of regions, alliances, and rulers from Gygax's original setting.
Game designer Rick Swan described the Greyhawk setting up to this point as "a crazy quilt, where odd-shaped scraps of material are randomly sewn together and everybody hopes for the best. How else to explain a setting that encompasses everything from the somber A1-4 "Scourge of the Slave Lords" adventure to the King Kong-inspired WG6 "Isle of the Ape" to the cornball humor of WG7 "Castle Greyhawk"? It makes for an interesting mess, but it's a mess nonetheless." He considered "The City of Greyhawk" "the most credible attempt at smoothing out the rough spots", and felt that "Greyhawk Wars" "took another step in the right direction by shaking things up with a much-needed dose of epic conflict"; he felt that by drawing on these earlier works, "veteran designer Carl Sargent has continued the overhaul with the ambitious "From the Ashes". By combining heroic tradition with elements of dark fantasy, he's come up with a Greyhawk campaign that is both familiar and refreshingly unexpected."
The "Greyhawk Wars".
In order to move players from Gygax's familiar "World of Greyhawk" to their new vision, TSR planned a trilogy of modules that would familiarize players with events and conditions leading up to the coming war, and then take them through the war itself. Once players completed the war via the three modules, a new boxed set would be published to introduce the new storyline and the new Flanaess. Two "World of Greyhawk Swords" modules, WGS1 "Five Shall Be One" by Carl Sargent and WGS2 "Howl from the North" by Dale Henson, were released in 1991. These described events leading up to the war.
The third module was reworked into "Greyhawk Wars", a strategy war game that led players through the events, strategies, and alliances of the actual war. A booklet included with the game, "Greyhawk Wars Adventurer's Book", described the event of the war. In 582 CY (six years after Gygax's original setting of 576 CY), a regional conflict started by Iuz gradually widened until it was a war that affected almost every nation in the Flanaess. A peace treaty was signed in the city of Greyhawk two years later, which is why the conflict became known as the "Greyhawk Wars". On the day of the treaty-signing, Rary—once a minor spellcaster created and then discarded by Brian Blume, but now elevated by TSR to the Circle of Eight—attacked his fellow Circle members, aided and abetted by Robilar. After the attack, Tenser and Otiluke were dead, while Robilar and Rary fled to the deserts of the Bright Lands. Rob Kuntz, original creator of Robilar, objected to this storyline, since he believed that Robilar would never attack his old adventuring companion Mordenkainen. Although Kuntz did not own the creative rights to Robilar and no longer worked at TSR, he unofficially suggested an alternate storyline that Robilar had been visiting another plane and in his absence, a clone or evil twin of Robilar was responsible for the attack.
"From the Ashes".
In 1992, after the two "World of Greyhawk Swords" prequel modules and the "Greyhawk Wars" game had been on the market for some months, TSR released the new Greyhawk setting, "From the Ashes", a boxed set primarily written by Carl Sargent that described the Flanaess in the aftermath of the "Greyhawk Wars". It contained a large 4-color hex map of the area around the city of Greyhawk, a number of "quick adventure cards", and two 96-page books.
The first book, "Atlas of the Flanaess", was a replacement for Gygax's original "World of Greyhawk" boxed set, with some changes. Many human gods from previous editions were not included, although one new demigod, Mayaheine, was added. This had the net effect of reducing the total number of human deities from fifty to twenty-eight. Deities of other races were increased from twenty-four to thirty-eight, but unlike the full descriptions that were given to the human gods, these were simply listed by name. Like Gygax's original boxed set, each region was given a two to three hundred word description, although some details included in the older edition, such as trade goods, total population and racial mixes, were not included in this edition. A number of regions—Ahlissa, Almor, Medegia and South Province—no longer existed after the Wars or had been folded into other regions. One new region—the Olman Islands—was detailed. This had the net effect of reducing the total number of regions from sixty to fifty eight. Darlene Pekul's large 4-color 2-piece fold-out map of the Flanaess included in Gygax's setting was reduced to a small black & white map printed on the inside cover of the "Atlas".
The second book, the "Campaign Book", was designed to supplement, rather than replace, the four-year-old "City of Greyhawk" boxed set. It included updates to the city and its environs, and gave details of some new non-player characters and possible adventure hooks.
In Gygax's setting, the major conflict had been between the Great Kingdom and the lands that were trying to free themselves from the evil overking. In Sargent's world, the Great Kingdom storyline was largely replaced by the major new conflict between the land of Iuz and the regions that surrounded it. Southern lands outside of Iuz's were threatened by the Scarlet Brotherhood, while other countries had been invaded by monsters or taken over by agents of evil. Overall, the vision was of a darker world where good folk were being swamped by a tide of evil.
Sargent tried to generate interest for this grimmer vision of the Flanaess by following up with an article in "Dragon"'s March 1993 issue, writing, "...the powers of evil have waxed strong. The hand of Iuz, the Old One, extends across the central Flanaess, and the cruel Scarlet Brotherhood extends its power and influence around the southern lands bordering the Azure Sea. The "World of Greyhawk" setting has become a truly exciting world again..."
The boxed set was supported by the publication of two new source books in 1993, also written by Sargent. WGR4 "The Marklands" provided information about the good realms of Furyondy, Highfolk, and Nyrond that opposed Iuz, while WGR5 "Iuz the Evil" detailed information about the lands of Iuz, and emphasized the prominent new role that Iuz now played in the world order.
In addition, a number of adventures were also published, as much to provide more source material as for adventure:
As Gygax had done ten years before, Sargent also used the pages of "Dragon" to promote his new world. He was working on a new source book, "Ivid the Undying", and excerpted parts of it in the April, June and August 1994 issues.
TSR drops Greyhawk.
In late 1994, TSR canceled Sargent's new book just as it was being readied for publication, and stopped work on all other Greyhawk projects. Nothing more about Greyhawk was ever published by TSR, with one exception: in May 1995, a "Dragon" column devoted to industry gossip noted that the manuscript of "Ivid the Undying" had been released by TSR as a computer text file. Using this file, several people have reconstructed the book as it might have appeared in published form.
By the end of 1996, TSR found itself heavily in debt and unable to pay its printers. Just as bankruptcy in 1997 seemed inevitable, Wizards of the Coast stepped in and, fueled by income from its collectible card game "", bought TSR and all its properties.
Wizards of the Coast (1998–2008).
After Wizards of the Coast (WotC) and TSR merged, the determination was made that TSR had created too many settings for the "Dungeons & Dragons" game, and several of them were eliminated. However, WotC's CEO, Peter Adkison, was a fan of both "Dungeons & Dragons" and Greyhawk, and two major initiatives were created: a revival of Greyhawk, and a new third edition of "D&D" rules. A team of people was put together to revive the moribund Greyhawk setting by pulling together all the previously published information about it. Once that was done, the decision was made to update Carl Sargent's storyline, using similar prequel adventures to pave the way for the updated campaign setting.
First, Roger E. Moore created "Return of the Eight" in 1998. In the adventure, set in 586 CY, the same year as the "From the Ashes" boxed set, the players meet the surviving members of the Circle of Eight, which is called the Circle of Five because it is missing Tenser, Otiluke and Rary. If the players successfully finish the adventure, Tenser is rescued from death, though he refuses to rejoin the Circle, and the Circle is reconstituted as Eight with the addition of three new wizards: Alhamazad the Wise, Theodain Eriason and Warnes Starcoat.
Next, the "Greyhawk Player's Guide", by Anne Brown, was released. This 64-page booklet moved the storyline ahead six years to 591 CY, and it mostly condensed and reiterated material that had been released in Gygax's and Sargent's boxed sets. New material included important non-player characters, a guide to roleplaying in the Flanaess, and some new sights. The list of deities was both shrunk and expanded; the thirty eight non-human deities in the "From the Ashes" boxed set were eliminated and non-human concerns assigned to a handful of human deities, but the list of human deities was expanded from twenty four to fifty four.
With the groundwork for a new storyline prepared, TSR/WotC released the new campaign setting as a 128-page source book, "The Adventure Begins", by Roger E. Moore. Taking its lead from the "Greyhawk Player's Guide", the new campaign world was set in 591 CY. Unlike the darker feel of "From the Ashes", where the Flanaess was overrun by evil, Moore returned to Gygax's world of adventure.
The "Lost Tombs" trilogy of modules—"The Star Cairns" and "Crypt of Lyzandred the Mad", by Sean K. Reynolds, and "The Doomgrinder", by Steve Miller—were the first to be published in the new setting.
"25th anniversary of D&D".
The year 1999 marked twenty five years since the publication of the original "Dungeons & Dragons" rules, and WotC sought to lure older gamers back to Greyhawk by producing a series of nostalgia-tinged "Return to..." adventures that evoked the best-known Greyhawk modules from 20 years before, under the banner "25th Anniversary of D&D":
In conjunction with the publication of the "Return to" adventures, WotC also produced a series of companion novels known as the "Greyhawk Classics" series: "Against the Giants", "White Plume Mountain", "Descent into the Depths of the Earth", "Expedition to the Barrier Peaks", "The Temple of Elemental Evil", "Queen of the Demonweb Pits", "Keep on the Borderlands",
and "The Tomb of Horrors".
In an attempt to attract players of other "D&D" settings, WotC released "Die, Vecna, Die!", by Bruce R. Cordell and Steve Miller, a three-part adventure tying Greyhawk to the "Ravenloft" and "Planescape" campaign settings. Published in 2000, it was the last adventure to be written for "D&D"'s 2nd edition rules.
Third edition.
In the editions of "Dungeons & Dragons" published by TSR, the setting of the game had not been specifically defined—Dungeon Masters were expected to either create a new world, or purchase a commercial campaign setting such as Greyhawk or "Forgotten Realms". In 2000, after two years of work and playtesting, WotC released the 3rd edition of "D&D", and defined a default setting for the game for the first time. Under third edition rules, unless a Dungeon Master specifically chose to use a different campaign setting, his or her "D&D" game would be set in the world of Greyhawk.
"Living Greyhawk".
With the release of the 3rd edition of "Dungeons & Dragons", RPGA—the organized play division of WotC—announced a new massively shared living campaign, "Living Greyhawk", modeled on a 2nd edition campaign called "Living City". Although "Living City" was relatively successful, RPGA wanted to expand the scope of their new campaign—instead of one city as a setting, the new campaign would involve thirty different regions of Greyhawk, each specifically keyed to a particular country, state, or province of the real world. Each region would produce its own adventures, and in addition to these, RPGA would provide worldwide "Core" adventures. To provide the level of detail needed for such a venture, WotC published the "Living Greyhawk Gazetteer", the most in-depth examination of the world of Greyhawk ever produced, and the official starting point for not only the campaign, but also for all home campaigns from that point forward.
Concurrent with the release of the 3rd edition "Player's Handbook", "Living Greyhawk" debuted at Gen Con 2000 with three "Core" adventures: COR1-1 "Dragon Scales at Morningtide", by Sean K. Reynolds; COR1-2 "The Reckoning", by Sean Flaherty and John Richardson; and COR1-3 "River Of Blood", by Erik Mona. Unlike previous campaign settings, in which the calendar was frozen at a point chosen by the author, the "Living Greyhawk" calendar did advance one year in game time for every calendar year in real time: the campaign started in 591 CY (2001) and ended in 598 CY (2008), at which point over a thousand adventures had been produced for an audience of over ten thousand players. During this time, the campaign administrators incorporated most of WotC's new rules into the Greyhawk world, only excising material they felt would unbalance the campaign, by either providing too much power to the players or to the adventure writers. In 2005, the administrators incorporated every deity ever mentioned in official Greyhawk material previous to the "D&D" 3rd edition, as well as all deities mentioned in the new 3rd edition source books. This tripled the number of deities in the campaign from about seventy to almost two hundred.
Despite the massive amount of world and storyline development, none of the "Living Greyhawk" storylines or changes to the setting were considered official, since the regional adventure modules were produced by volunteers, and only received a cursory vetting by the campaign administrators of RPGA, and no review by WotC personnel.
Wizards of the Coast Greyhawk releases.
Despite the popularity of the "Living Greyhawk" campaign, Wizards of the Coast did not produce much material for Greyhawk after the 25th anniversary "Return to..." series of adventures, other than the "Living Greyhawk Gazetteer" and "The Fright at Tristor". "The Standing Stone", written by John D. Rateliff and released in 2001, did have several minor references to the Greyhawk setting, and "Red Hand of Doom", written by James Jacob and released in 2006, contained instructions for where to set the adventure within the world of Greyhawk, "Forgotten Realms", and "Eberron". Otherwise, Wizards of the Coast left the development of the Greyhawk world to RPGA's "Living Greyhawk" campaign and concentrated on producing new source books of expansion material for the core rules of "D&D".
2008 to present.
At Gen Con 2007, WotC announced that the 4th edition of "Dungeons & Dragons" would be released the following spring, and Greyhawk would no longer be the default campaign setting under the new rules system. For this reason, "Living Greyhawk" was not converted to the new rules system; instead, it was brought to a conclusion at Origins 2008.
In 2009, WotC released "The Village of Hommlet", by Andy Collins, which updated Gary Gygax's original 1st edition "Village of Hommlet" to the 4th edition rules . It was not available for purchase, but was sent as a reward for those who joined the RPGA.
When the "Player's Handbook" for fifth edition of "Dungeons & Dragons" was released in 2014, several reference to the world of Greyhawk appeared throughout the descriptions of various races and classes, and a partial list of Greyhawk deities appeared in the book
Unofficial Greyhawk sources.
Although TSR and WotC had each in turn owned the official rights to the "World of Greyhawk" since the first folio edition was published in 1980, the two people most responsible for its early development, Gary Gygax and Rob Kuntz, still had most of their original notes regarding the fifty levels of dungeons under Castle Greyhawk. Gygax also had his old maps of the city of Greyhawk, and still owned the rights to Gord the Rogue.
After Gygax left TSR in 1985, he continued to write a few more "Gord the Rogue" novels, which were published by New Infinities Productions: "Sea of Death" (1987), "City of Hawks" (1987), and "Come Endless Darkness" (1988). However, by this time, Gygax was furious with the new direction in which TSR was taking "his" world. In a literary declaration that his old world of Oerth was dead, and wanting to make a clean break with all things Greyhawk, Gygax destroyed his version of Oerth in the final "Gord the Rogue" novel, "Dance of Demons". For the next fifteen years, he worked to develop other game systems.
But there was still the matter of the unpublished dungeons under Castle Greyhawk. Although Gygax had given glimpses into the dungeons in his magazine columns and articles, the dungeons themselves had never been released to the public. Likewise, Gygax's version of the city of Greyhawk had never been published, although Frank Mentzer believed the reason for that was because "the City of Greyhawk was a later development, originally being but a location (albeit a capital). As such it was never fleshed out all that thoroughly... notes on certain locations and notorious personnel, a sketch map of great brevity, and otherwise quite loose. That is doubtless why Gary didn't publish it; it had never been completed."
However, in 2003, Gygax announced that he was working with Rob Kuntz to publish the original castle and city in six volumes, although the project would use the rules for "Castles and Crusades" rather than "Dungeons & Dragons". Since WotC still owned the rights to the name Greyhawk, Gygax changed the name of the castle to "Castle Zagyg"—the reverse homophone of his own name originally ascribed to the mad architect of his original thirteen level dungeon. Gygax also changed the name of the nearby city to "Yggsburgh", a play on his initials E.G.G.
This project proved to be much more work than Gygax and Kuntz had envisioned. By the time Gygax and Kuntz had stopped working on the original home campaign, the castle dungeons had encompassed fifty levels of maze-like passages and thousands of rooms and traps. This, plus plans for the city of Yggsburgh and encounter areas outside the castle and city, were found to be too much to fit into the proposed six volumes. Gygax decided he would recreate something like his original thirteen level dungeon, amalgamating the best of what could be gleaned from binders and boxes of old notes. However, neither Gygax nor Kuntz had kept careful or comprehensive plans. Because they had often made up details of play sessions as they went, they usually just drew a quick map as they played, with cursory notes about monsters, treasures, and traps. These sketchy maps contained just enough detail so that the two could combine their independent efforts, after determining the merits of each piece. Recreating the city was also a challenge; although Gygax still had his old maps of the original city, all of his previously published work on the city was owned by WotC, so he would have to create most of the city from scratch while maintaining the look and feel of his original.
The slow and laborious process came to a complete halt in April 2004, when Gygax suffered a serious stroke. Although he returned to his keyboard after a seven-month convalescence, his output was reduced from fourteen-hour work days to only one or two hours per day. Kuntz had to withdraw due to other projects, although he continued to work on an adventure module that would be published at the same time as the first book. Under these circumstances, work on the Castle Zagyg project continued even more slowly, although Jeffrey Talanian stepped in to help Gygax. In 2005, Troll Lord Games published Volume I, "Castle Zagyg: Yggsburgh". This 256-page hardcover book contained details of Gygax's original city, its personalities and politics, as well as over thirty encounters outside the city. The two part fold out map of the area was rendered by Darlene Pekul, the same artist who had produced the original map for the folio edition of "World of Greyhawk". Later that year, Troll Lord Games also published "Castle Zagyg: Dark Chateau", an adventure module written for the Yggsburgh setting by Rob Kuntz.
Book catalogs published in 2005 indicated several more volumes in the series would follow shortly, but it wasn't until 2008 that the second volume, "Castle Zagyg: The Upper Works", appeared. "The Upper Works" described details of the castle above ground, acting as a teaser for the volumes concerning the actual dungeons that would follow. However, Gygax died in March 2008 before any further books were published. After his death, Gygax Games, under the control of Gary's widow Gail, took over the project, but no more volumes of the "Castle Zagyg" project have been published.
Rob Kuntz also published some of his creative work from the Castle Greyhawk dungeons. In 2008, he released the adventure modules "The Living Room", about a whimsical but dangerous room that housed enormous furniture, and "Bottle City", about a bottle found on the second level of the dungeon that contained an entire city. 2009 saw Kuntz release "Daemonic & Arcane", a collection of Greyhawk and "Kalibruhn" magic items, and "The Stalk", a wilderness adventure. In October 2010, Black Blade Publishing announced that they would be publishing several of Kuntz's original Greyhawk levels, including the Machine Level, the Boreal Level, the Giants' Pool Hall, and the Garden of the Plantmaster.

</doc>
<doc id="46926" url="http://en.wikipedia.org/wiki?curid=46926" title="Flanaess">
Flanaess

The Flanaess is the eastern part of the continent of Oerik, one of the four continents of the fictional world of Oerth in the "World of Greyhawk" campaign setting for the "Dungeons & Dragons" fantasy roleplaying game. The Flanaess has been the setting of dozens of adventures published between the 1970s and 2000s and continues to be the central focus of the campaign world. As well as being home to a number of demihuman and humanoid races, it is also inhabited by the Suel, Bakluni, Oerid, and Flan subraces of humanity.
Development of the Flanaess.
In late 1972, Dave Arneson demonstrated a new type of game to a group of gamers in Lake Geneva, Wisconsin, including game designer Gary Gygax. Gygax agreed to develop a set of rules with Arneson and get the game published; the game eventually became known as "Dungeons & Dragons". Gygax designed a set of dungeons underneath the ruins of Castle Greyhawk as a testing ground for new rules, character classes and spells. In those early days, there was no "Flanaess"; the world map of "Oerth" was developed by Gygax as circumstances dictated, the new cities and lands simply drawn over a map of North America. Gygax and his friend Rob Kuntz further developed this campaign setting, and by 1976, the lands within a radius of 50 miles had been mapped in depth, and the lands within a radius of approximately 500 miles were in outline form. In addition, more distant lands had been roughly sketched out to accommodate various adventures. "(For more information about the first days of Gygax's home campaign, see Greyhawk.)"
Following yet more work, in 1978 Gygax agreed to publish his world and decided to redevelop Oerth from scratch. Once he had sketched out the entire planet to his satisfaction, one hemisphere of Oerth was dominated by a massive continent called Oerik. Gygax decided to concentrate his first efforts on the continent of Oerik and asked TSR's printing house about the maximum size of paper they could handle; the answer was 34" x 22" (86 cm x 56 cm). He found that, using the scale he desired, he could fit only the northeast corner of Oerik on two of the sheets. This corner of Oerik became known as "the Flanaess", so named in Gygax's mind because of the peaceful people known as the Flannae who had once lived there. Gygax also added many more new regions, countries and cities, bringing the number of political states to 60:
Needing original placenames for all of the geographical and political places on his map, Gygax sometimes resorted to wordplay based on the names of friends and acquaintances. For instance, Perrenland was named after Jeff Perren, who co-wrote the rules for "Chainmail" with Gygax; Urnst was a homophone of Ernst (his son Ernie); and Sunndi was a near-homophone of Cindy, another of Gygax's children.
From Gygax's prototype map, Darlene Pekul, a freelance artist in Lake Geneva, developed a full colour map on a hex grid. Gygax was so pleased with the end result that he quickly switched his home Greyhawk campaign over to the new world he had created.
This map formed the basis of the World of Greyhawk when it was published as the 32-page "The World of Greyhawk" folio in 1980.
Gygax also developed a thousand-year history for the Flanaess that involved a series of cultural and military invasions: the peaceful Flannae had been pushed out by the warlike Suloise; the Suloise in turn had been pushed out by the noble Oeridians, who set up a Great Kingdom of peace and prosperity. However, by 576 CY (the year Gygax chose for his setting), the Oeridian empire had grown decadent and evil, and many subservient regions were rising up in rebellion. The "From the Ashes" boxed set, published in 1992, advanced this storyline by a decade, involving a continental war and its aftermath. "The Adventure Begins" boxed set, published in 1998, further advanced the storyline to 591 CY.
Geography.
The Flanaess can be broken down geographically as follows: the Baklunish Basin in the northwest, the Empire of Iuz in the north, the Thillonrian Peninsula in the northeast, the "Sea of Dust" in the far west, the Sheldomar Valley in the west, old Ferrond and its southern frontier (including the City of Greyhawk) at the center of the Flanaess, old Sulm and the Aerdy frontier to the east, the old Great Kingdom to the far east, and the Amedio Jungle to the southwest.
Aerdy.
Aerdy properly refers to the now-defunct Kingdom of Aerdy. Sometimes "Aerdy" is used in reference to the Oeridian tribe which founded the kingdom, though the proper term for the tribe is "Aerdi" ( ) (which is also the adjectival form). "Aerdy" can also be used in reference to the Great Kingdom of Aerdy, the imperial successor state of the Kingdom of Aerdy.
Ahlissa.
The ancient Flan kingdom of Ahlissa was founded over 1,700 years ago by the legendary Queen Ehlissa the Enchanter. Modern Ahlissa is generally located in the southeastern portion of the Flanaess, and is one of the largest countries on the subcontinent. Ahlissa is a feudal empire with hereditary rulership, and is currently ruled by His Transcendent Imperial Majesty, Overking Xavener I, Grand Prince of Kalstrand, Crowned Head of House Darmen.
Almor.
In 586 CY King Lynwerd I of Nyrond seized the western half of Almor ( ), placing it under his protection and creating the Almorian Protectorate, and appointing a governor to administer the new province.
Amedio Jungle.
The Amedio Jungle is a large stretch of tropical rainforest located in the southwestern Flanaess, on the continent of Oerik. Lying east of the Hellfurnaces, the Amedio extends over 300 leagues south from its northernmost point at Jeklea Bay. The border of the Amedio Jungle and the Hellfurnaces was the setting of "Dungeon's" Shackled City Adventure Path. This adventure detailed the history of the region between Jeklea Bay, the Amedio, and the Hellfurnaces, going back 3000 years to the time of the spellweavers' domination of the area. In that time, a battle between the Abyssal hordes and the forces of good took place around what is now the city of Cauldron.
Baklunish Basin.
The Baklunish Basin is the northwestern portion of the Flanaess. Consisting chiefly of remnants of the ancient Baklunish Empire, the human peoples of these lands are known as the Baklunish. The most important parts of this region include the nations of Ekbir, Ket, Tusmit, Ull, and Zeif, as well as the Dry Steppes and the Plains of the Paynims. Though not technically part of the Baklunish Basin, the lands of the Tiger and Wolf Nomads are sometimes included, as they are populated by people of Baklunish stock.
The basin is bordered to the east by the Crystalmist Mountains, Yatil Mountains, and Barrier Peaks, and to the south by the Sulhaut Mountains. To the north, it is bordered by the Dramidj Ocean and to the west its limits are defined by the Tyurzi Mountains.
Bandit Kingdoms.
The Bandit Kingdoms are generally located in the central portion of the Flanaess. They are governed by various petty dictatorships theoretically owing fealty to Iuz.
Bissel.
Bissel, properly known as the March of Bissel, is a political state of the Flanaess.
Modern Bissel is generally located in the western-central portion of the Flanaess. It is bordered by the Barrier Peaks to the north and west, by the Dim Forest to the south, and by the Lorridges to the east.
As of 591 CY, the most populous towns are Thornward, and Pellak (pop. 2,300). Thornward itself was once the capital of Bissel, but is now a town governed jointly by Bissel, Gran March, Ket, and Veluna (resulting from an agreement known as the Thornward Division).
As of 591 CY, the population of Bissel totaled 123,880 persons, over 80% being humans of Oeridian, Suel, and Baklunish descent. Approximately 10% of the population is dwarven, with other humanoid races making up the remainder of the population.
The most popular deities among Bissel's citizens are Heironeous, Zilchus, Fharlanghn, Geshtai, Rao, and Istus.
The most widely spoken languages in Bissel include Common, various Baklunish dialects, and Dwarven.
Bissel is a feudal monarchy, and ruled by His Lofty Grace, Larrangin, the Margrave of Bissel, and owing fealty to Gran March and Veluna. The margrave is chosen by the leadership of the Knights of the Watch.
In the Living Greyhawk campaign, Bissel corresponded to New England (the states of Connecticut, Massachusetts, Rhode Island, Vermont, New Hampshire, and Maine).
The country itself is divided into twenty-six knight-baronies, eight townships, and one capital township, for the national capital of Pellak.
Bissel's coat of arms is blazoned thus: "Per pale indented gules and argent, over all a tower sable".
Bissel is noted for producing foodstuffs, cloth, gold, and low-quality gems from mining operations.
Bissel's standard coinage is modeled after Keoland's, and consists of the platinum griffon (pp), gold lion (gp), electrum eagle (ep), silver stag (sp), and copper roc (cp).
Blackmoor.
The original Blackmoor began life in the early 1970s as the personal setting of Dave Arneson, the co-creator of "Dungeons & Dragons", first as a setting for Arneson's miniature wargames, then as an early testing ground for what would become "D&D". Though published in booklet form by Tactical Studies Rules (TSR) in 1975, as the second supplement to "D&D" (the first being Greyhawk), Blackmoor actually predates Greyhawk as a campaign setting, a fact which Gary Gygax acknowledged in the foreword to the "Blackmoor" supplement.
Blackmoor as a location within the Greyhawk setting came about both as an inside-joke by Gygax, and as a way for him to acknowledge his fellow writers' creations (Len Lakofka's Lendore Isles occupy a similar position in the world). Having certain locations exist across the campaign settings of multiple DMs also became a convenient way to explain how player characters such as Mordenkainen (Gygax's character) and Robilar (Rob Kuntz's character) could be adventuring in Blackmoor's City of the Gods at one sitting, while exploring the dungeons of Castle Greyhawk the next.
Modern Blackmoor is generally located in the northwestern portion of the Flanaess. It is a small and inhospitable realm lying almost completely within the boundaries of the northern reaches of the Cold Marshes. The Burneal Forest forms a western boundary of sorts, while the Land of Black Ice does the same to the north.
As of 591 CY, the population of Blackmoor totaled 110,000 persons. Almost 40% of these inhabitants are humans of predominantly Flan and Oeridian stock, although some Suel and Baklunish can be found as well. Another 20% comprise orcish tribes that roam the region (some under the control of the Egg of Coot, some not). A slightly smaller percentage of halflings is also present, while elves, gnomes, half-orcs, half-elves, and other races make up the remainder of the population in increasingly smaller percentages.
As of 591 CY, the most populous town is Dantredun (pop. 700) on the edge of the Burneal. Blackmoor Town, the original capital of the archbarony, was conquered and destroyed in 541 CY by the "Egg of Coot", a mysterious being with a significant amount of magical power at his disposal. The "town" that serves as the Egg's home contains a number of automata (some reports place the number as high as 200), though very few living beings (if any) reside there
The actual structure of Blackmoor's government is unknown. His Luminous Preponderancy, Archbaron Bestmo of Blackmoor claims the title of "archbaron", and with it, rule over the ten underbaronies of the region. However, Teuod Fent, formerly of the Bandit Kingdoms, has claimed Ramshorn Castle as his own, and with it the title of "baron". Nonhuman tribes also roam the area, though their allegiances are usually only to themselves. The national capital is Dantredun (the seat of Bestmo's power).
Bone March.
The Bone March is a political state of the Flanaess. The Bone March is generally located in the eastern portion of the Flanaess. It rests in the narrow strip of land bounded by the Rakers in the west and the Teesar Torrent in the east, and curving around to encompass Johnsport on the coast of the Solnor Ocean. The cool, rocky farmland of the Bone March is relatively poor and never supported a dense population or large cities. Its farmlands are now desolate wilderness.
As of 591 CY, the population of Bone March totaled 310,000 persons, though only a large minority are human (mainly Oeridian stock, but some Suel and Flan as well). About 20% of the population are orcs, and nearly the same number are halflings; gnomes, Elves, half-orcs, half-elves, and dwarves make up decreasing numbers of the remaining citizens.
As of 591 CY, the most populous towns are Knurl (13,500), Spinecastle (pop. 6,300(?)), and Johnsport (pop. 3,500(?)). The numbers above do not take into account the large number of other uncounted goblins, gnolls, etc.
The most popular deities among Bone March's inhabitants are Hextor, the Oeridian agricultural gods, Erythnul, Kord, the orc pantheon, and Beltar.
The most widely spoken languages in Bone March include Common, Old Oeridian, Orc, Halfling, Gnomish, and Elven. Goblin, Gnoll, and other nonhuman languages are spoken as well, due to the large numbers of those races present in the land.
Bone March was formerly a feudal marchland of the Great Kingdom, and ruled by His Nobility, Clement, the Marquis of Bone March; since 563 CY, it has been controlled by a conclave of nonhuman chiefs. The national capital is Spinecastle. Bone March's coat of arms is blazoned thus: "Gules, three shin-bones, two in saltire, surmounted by one in pale, argent".
Bone March is noted for producing silver and low- to average-quality gems. Bone March's coinage is based on Aerdy's standard, and consists of the platinum orb (pp), gold ivid (gp), electrum noble (ep), silver penny (sp), and copper common (cp).
Bright Lands.
the Bright Lands, properly known as the Empire of the Bright Lands, is a dictatorial political state of the Flanaess.
The boundaries of the Bright Lands are contained entirely within the bounds of the Bright Desert. The Abbor Alz form the northern and eastern borders, while the Woolly Bay and the Sea of Gearnat form natural boundaries to the country's west and south.
As of 591 CY, the population of the Bright Lands totaled 26,500 persons, the vast majority (almost 80%) being humans of mainly Flan descent. Dwarves account for the next largest group, with the remainder of the population (barely 1%) consisting of mostly centaurs.
There is no state sponsored religion within the country. Among the Flan tribesmen, worship of Beory, Obad-Hai, and Pelor is most common. Geshtai and Istus are most popular with the few Paynim in the region, while a few scattered goblin tribes worship Maglubiyet and other deities of the goblin pantheon.
The most widely spoken languages in the Bright Lands are Flan and Ancient Suloise (by the native tribesmen), and various Baklunish dialects (by the Paynims).
The Empire of the Bright Lands is a dictatorship with Rary of Ket as the head of state. The realm functions as a minor city-state administered from Rary's tower in the Brass Hills. Rary maintains his power with the assistance of Robilar, a powerful warrior who serves as commander of Rary's forces in the Empire.
The state's coat of arms is blazoned thus: "Gules, a gauntlet proper gripping a crescent or; on a chief azure three mullets argent".
The Bright Lands does not currently produce its own coinage.
County of Ulek.
The modern County of Ulek is generally located in the southwestern portion of the Flanaess, and is considered part of the Sheldomar Valley region. It is bordered by the Lortmil Mountains to the north and east, by the Old River to the east and south, and by the Kewl River to the West. As of 591 CY, the most populous towns are Jurnre (pop. 13,100), Kewlbanks (pop. 10,900), and Courwood (pop. 7,800).
As of 591 CY, the population of the County of Ulek totaled 370,000 persons, the majority (almost 80%) being humans of Oeridian, Flan, and Suel descent. Gnomes are the next most populous group (8%), followed by halflings (6%, predominantly of the lightfoot variety), with the remainder being a mix of elves, dwarves, half-elves, and half-orcs.
Empire of Iuz.
The Empire of Iuz is an empire ruled by the demigod Iuz.
The Empire of Iuz is classified by the "Living Greyhawk Gazetteer" as an "imperial theocratic dictatorship." Though Iuz holds all power, his rule is inconsistent. Iuz's priesthood and other spellcasters manage the affairs of the empire in his name. Iuz and his servants have more control over some regions of the empire than others.
The capital of the empire is the city of Dorakaa.
Administrative divisions.
The Empire of Iuz consists of five loosely-defined political regions, though they are not considered actual provinces.
Land of Iuz.
Also called the "Homeland of Iuz", this region lies mainly between the Dulsi and Opicim Rivers, south of the Cold Marshes, and north of Whyestil Lake.
Bandit Lands.
Once referred to as the Bandit Kingdoms, this region lies west of the Zumker and Artonsamay Rivers, east of the Ritensa River, and is bounded in part by the Fellreev Forest and Bluff Hills. This region is actually made up of a number of warlord fiefs.
Barren Lands.
These grasslands south of the Icy Sea and the Wastes are also known as the Barrens, the Barren Plains, or the Northern Barrens. North of the Bandit Lands, the Barrens' eastern border is formed by the Forlorn Forest, western Griff Mountains, and the Bluff Hills. Their western border is formed by the Cold Marshes and Opicim River. Once ruled by the nomadic Flan tribes known as the Rovers of the Barrens, these people are but a shadow of their former glory.
Horned Lands.
This region is bound by Whyestil Lake, the Veng and Ritensa Rivers, and the Fellreev Forest. These lands were for a time ruled by the Horned Society, before Iuz once again took control during the Greyhawk Wars.
Shield Lands.
Bordered by the Ritensa river, Nyr Dyv, and Rift Barrens, this land was ruled before the Greyhawk Wars by the chivalrous order known as the Knights of Holy Shielding. Though a western portion of the Shield Lands were reclaimed in the Great Northern Crusade of 586-588, the majority of the area is still under Iuz's control.
Ferrond.
Ferrond most often refers to the former Great Kingdom province known as the Viceroyalty of Ferrond, or the general region where its successor states now lie.
The Viceroyalty of Ferrond was a vassal state of the Great Kingdom of Aerdy, ruled by a viceroy in the name of the Overking. The capital of the Viceroyalty was the city of Dyvers.
The Viceroyalty of Ferrond consisted of the modern-day states of Furyondy (Furyon), Highfolk, Perrenland (the Quaglands), the Shield Lands, Veluna (Voll), and the hilly regions northeast of the Vesve Forest, known then as part of the Northern Reaches.
Land of Black Ice.
The Land of Black Ice is an arctic wasteland located in the northwestern Flanaess, north of the "Burneal Forest" and Blackmoor, east of the "Drawmidj Ocean", and west of the "Icy Sea". The land gets its name from the vast sheet of blue-black ice that covers it as far as the eye can see.
The only humans living near the area are the Baklunish Guryik folk on the Drawmidj coast, and the Suloise Zeai, or Sea Barbarians, who dwell on a section of Icy Sea coastline known as the "Tusking Strand". Within the Land of Black Ice proper are said to dwell all manner of horrid arctic monsters, so few humans enter the region. Tales are told of wights, frost men, evil spirits, black-skinned trolls, and blue-furred bugbears inhabiting the wasteland.
The fabled City of the Gods is said to lie just outside the Land of Black Ice, near Blackmoor. Those few who have entered the city and escaped alive, among them Mordenkainen and Sir Robilar, tell tales of an artificially warm city of tall iron buildings, powerful magic, and strange automatons.
Legends tell of another region beyond the ice, a land of lush jungles and an unmoving sun. Such legends give some reason to believe that the Oerth is hollow, containing another world within the inside of the globe, to which entry is gained via great openings in the poles.
Nyrond.
Nyrond, or more properly the Kingdom of Nyrond, refers to one of the largest good-aligned states in the Flanaess.
In 356 CY, the opportunist junior branch of the Aerdy Celestial House of Rax-Nyrond declared independence from the Overking of the Great Kingdom. Weakened by warfare against the likes of Ferrond, Perrenland, and Tenh, the scions of House Rax nonetheless amassed a large retribution-minded army to teach the upstart "kingdom" a lesson. At this moment, a coalition of Fruztii, Schnai and mercenary barbarians invaded North Province, distracting the Overking's army and allowing Nyrond critical time to entrench and train its military to repulse Aerdy military actions. For the next two centuries relations between the two states varied from open war to preparing for war.
In 584 CY, during the Greyhawk Wars, Nyrond halted an invasion by the Great Kingdom after Ivid V's forces devastated Almor. Thereafter the Great Kingdom fell apart in magical disaster and civil war. Nyrond was in nearly as bad shape. After King Archbold III suffered a poison-induced stroke at the hands of his younger son in the fall of 585 CY, said son, Prince Sewarndt attempted to seize power. Sewarndt's coup was thwarted by his older brother, Crown Prince Lynwerd, who took the throne after Archbold abdicated in Fireseek of 586 CY, possibly the most popular decision of his time as ruler.
Under Lynwerd, Nyrond has been slowly recovering, though times are good only by comparison to Archbold times. Lynwerd did take over much of Almor in 586 CY as the Great Kingdom fell apart.
Nyrond lies between the Nesser River and Franz River to the west, the Harp River and Flinty Hills to the east, Relmor Bay to the south, and the Theocracy of the Pale to the north.
Nyrond is ruled by King Lynwerd, who appoints all lesser officials.
Nyrond's coat of arms is blazoned thus: "Quarterly; 1st and 4th, ermine; 2nd and 3rd, gules, a sun radiated or".
Nyrond in the Living Greyhawk campaign.
In the Living Greyhawk campaign, the following events took place in Nyrond: In early 595 CY another major revolt was successfully led by Sewarndt. He captured the capital city with the assistance of supernatural allies, and in the course of the assault he committed patricide. In autumn 595 CY, several campaigns around the country, led by Lynwerd's supporters and loyal nobles, succeeded in recapturing nearly all major provinces and cities. The reconquest ended in Sewarndt's death at the hands of his brother at the very gates of the Royal Palace in Rel Mord.
Lynwerd is married to Queen Xenia Sallavarian of the Duchy of Urnst. Their daughter and heir was born in 595 CY.
Plains of the Paynims.
The Plains of the Paynims, also known as the Paynims, is a political state and may also refer to the tribes of nomadic horsemen who live there. The plains are situated in the Baklunish Basin region of the Flanaess, west of Ket and south of Zeif. The native folk are Baklunish with mixed racial characteristics absorbed from subjugated travelers or settled merchants. There is no central authority; the government consists mainly of tribal khans and amirs ruled by progressively more powerful nobles (orakhans, ilkhans, or shahs) and royalty (padishahs, tarkhans, or kha-khans). There is great variation in governments between nomadic tribes. The coat of arms is blazoned thus: "Gules, a scimitar per bend sinister or, in dexter chief a cross moline in saltire of the last".
Rovers of the Barrens.
The Rovers of the Barrens, also known as the Barrens, and properly known as Arapahi (translated as "People of the Plentiful Huntinglands"), is a political state of the Flanaess. The name also applies to the tribes of nomadic horsemen who dwell within these lands.
Scarlet Brotherhood.
The Scarlet Brotherhood most often refers to the Great and Hidden Empire of the Scarlet Brotherhood, a nation located on the Tilvanot Peninsula in the southeastern Flanaess, though it can also refer to the secretive organization which rules that land. The Scarlet Brotherhood is described in great detail in the sourcebook "The Scarlet Brotherhood" (1999), by Sean K. Reynolds.
Sheldomar Valley.
The Sheldomar Valley refers to the large swath of fertile land in southwestern Flanaess, bordered by the Barrier Peaks to the north, Crystalmists and Hellfurnaces to the west, "Azure Sea" to the south and the Lortmils to the east. The valley is named after the Sheldomar River, which runs through a good portion of its length.
A number of nations make up the Sheldomar Valley, the largest being the Kingdom of Keoland. Other notable realms include Bissel, Geoff, the Gran March, the "Hold of the Sea Princes", the Ulek States, the "Valley of the Mage" and the Yeomanry.
The Sheldomar Valley was mentioned by Stephen Colbert on the "Colbert Report" as a location travelled to by his player character, Farynieth.
Sterich.
Sterich, properly known as the March of Sterich, is a political state generally located in the southwestern portion of the Flanaess. It lies in a broad basin formed by the Stark Mounds to the north and northwest, by the Crystalmist Mountains to the west, and by the Jotens to the south. It is separated from the Kingdom of Keoland by the Javan River. Sterich is currently ruled by Marquess Resbin Dren Emondav, a lawful neutral female human. The national capital is Istivin. The native folk of Sterich show strong Flan racial characteristics, tempered by Suel and Oeridian influences. Sterich's coat of arms is blazoned thus: "Per fess dancetty gules and sable a lion rampant counterchanged.".
Sulm.
Sulm refers to an ancient Flan kingdom which once occupied much of the Bright Desert, in the days when it was a fertile land. The ancestors of the Sulmi were nomads who lived in the grasslands where the Bright Desert lies today, living in harmony with the land and worshipping spirits of earth and air. Like most of the nomads in the region, they slowly became more sophisticated under the influence of the demigod Vathris. The Kingdom of Sulm came to prominence circa -1900 CY after it was bolstered by the necromantic adepts of Caerdiralor, who taught them dark secrets and promised wealth and glory in exchange for the favor of the Sulmi royal house. Throughout its history, the kingdom spent much of its time crushing its neighbors, the rival kingdoms of Durha, Itar, Ronhass, Rhugha, and Truun. Only the Kingdom of Itar was strong enough to stand against Sulm, though it, too, would eventually fall. After centuries of expansion, Sulm began its slow decline circa -1400 CY, some say due to the influence of Nerull and other dark powers. Kyuss was a powerful priest of Nerull during Sulm's imperial age, shortly before its destruction. He was exiled for his profane experiments on undead in the sacred mortuary city of Unaagh, and traveled with hundreds of followers to the Amedio Jungle. In circa -700 CY the Kingdom of Sulm fell, destroyed by its last king. The king, Shattados, used the power of a dark artifact known as the Scorpion Crown in an attempt to gain perpetual dominion over his subjects. Instead, the crown turned Shattados into a gigantic scorpion and his people into manscorpions and (possibly) dune stalkers. A few became asheratis instead due to the grace of Geshtai. The land itself was even changed, transformed into a vast wasteland now known as the Bright Desert.
Thillonrian Peninsula.
The Thillonrian Peninsula, also known as Rhizia in the Cold Tongue, is a geographical area in the northeastern portion of the Flanaess. A cold, mountainous land, this region is best known for being the home of the nations of the Frost, Ice, and Snow Barbarians, as well as the nation of Stonehold. The peninsula is dominated by the Griff-Corusk mountain range, and is surrounded by the Icy Sea and the Solnor Ocean.
The Thillonrian Peninsula was mentioned by Stephen Colbert on the "Colbert Report" as a location travelled to by his player character, Farynieth.
Ull.
Ull is a political state situated in the Baklunish Basin region of the Flanaess, south of the Plains of the Paynims.
The native folk of Ull are more or less pureblooded Baklunish.
Verbobonc.
Verbobonc, properly known as the Viscounty and Town of Verbobonc, is a semi-independent nation of the Flanaess. Though Verbobonc owes fealty to the Archclericy of Veluna, it is nearly autonomous in practice. In the Living Greyhawk campaign, Verbobonc corresponds to the states of Illinois and Indiana. It pioneered a system for allowing Living Greyhawk player characters to develop towns, open businesses, and establish strongholds, something previously beyond the scope of the Living Greyhawk campaign. Player-created towns tend to reflect the personalities of the player characters who pool their resources to "found" a town.
Wild Coast.
Little more than a collection of five major city-states for most of its history, the Wild Coast has long held a reputation for being untamed. Prior to the Greyhawk Wars, each city-state controlled its own affairs. However, things changed once Turrosh Mak gained control of the Pomarj. In 584 CY, Mak's forces marched north, taking the towns of Elredd, Badwall, and Fax, thus conquering the entire southern Wild Coast. The remaining towns of Safeton and Narwell escaped destruction only by swearing fealty to Greyhawk, resulting in the northern Wild Coast being absorbed into the Free City's domain.
Yeomanry.
The modern Yeomanry is located in the southwestern portion of the Flanaess. It is nestled entirely within a broad basin bounded by the Jotens to the north, the Crystalmist Mountains to the west and south, and the Little Hills to the east. The valley passage between the Little Hills and the Tors leads directly into the Hool Marshes. A long cave system known as the Passage of Slerotin tunnels under the Hellfurnaces from the town of Dark Gate in the Yeomanry, finally emerging over 200 miles away into the Sea of Dust.
Weather.
Except in the far north, temperatures reach below zero only in the winter months of Sunsebb and Fireseek and sometimes at night in late autumn and early spring. Even then temperatures seldom dip far below 0. The northeast and central regions of the Flanaess are the coldest, with winters extending into Ready'reat and Readying. The warm currents of the Dramidj Ocean moderate the climate of the lands that border it. Summer typically lasts five or more months. Prevailing winds blow from the northeast in winter and autumn and from the southeast the rest of the year. Wintry northern winds are sometimes known as "Telchur's breath," while easterly winds are sometimes called "Atroa's laugh." Plentiful rainfall supports bountiful agriculture throughout most of the Flanaess.

</doc>
<doc id="46927" url="http://en.wikipedia.org/wiki?curid=46927" title="Oerth">
Oerth

In the "Dungeons & Dragons" roleplaying game, Oerth is the name of the fictional planet on which one of the earliest campaign settings, the "World of Greyhawk", is located. Oerth has an axial tilt of 30 degrees, which would cause greater seasonal temperature variation than on Earth, were it not for wizardly and divine magic that shifts weather patterns to be more favorable to the populace (at least the god-fearing folks).
Publication history.
Oerth is detailed in the 1980 version of the "World of Greyhawk Fantasy Game Setting", which covers less than a quarter of the landmass of Oerth.
Physical features.
According to the paper by Gary Holian in "Oerth Journal", Oerth's circumference is about 42,024 km, and its radius measures 6,714 km, about 1% more than that of Earth. There is no flattening of the planet sphere due to rotation, and divine power is again hinted at.
Oerth has at least four continents, the largest of which is Oerik, lying mainly in the Northern Hemisphere. Southeast of Oerik lies the much smaller landmass of Hepmonaland, lying in the tropics. Hyperboria, also called Telchuria (after Telchur, the god of winter), lies at the north pole, while an unnamed fourth continent lies in the southern hemisphere. Oerth is thought to have four oceans, including the Solnor Ocean, the Dramidj Ocean that circles Telchuria, the Ocean of Storms south of Oerik, and the Sea of Thunder encompassing Polaria.
Oerth also has a number of islands, the largest being Fireland in the northern Solnor, depicted on maps as being roughly half the size of Hepmonaland. Polaria is a chain of mountainous islands covered for at least part of the year by the southern polar ice cap.
The easternmost part of Oerik, the Flanaess, has received the lion's share of detail, though Hepmonaland has received significant attention. Western Oerik was the setting for TSR's original miniature line, Chainmail, but few regard the material written for that setting as canon, especially after the miniature line's cancellation in 2002.
Author Frank Mentzer, in a series of early modules, added a small territory called New Empyrea to the world across the Solnor Ocean from the Flanaess, but it has not been mentioned in subsequent Greyhawk products.
Telchuria.
Telchuria is an ice-covered continent at the north pole of Oerth. Telchuria is sometimes referred to as "High Boros" or "Hyborea."
It is possible that, in the depth of winter, a natural ice bridge forms between Telchuria and the Land of Black Ice.
Hepmonaland.
Hepmonaland is a tropical landmass to the southeast of Oerik. Hepmonaland is a very small continent compared to the other known continents.
Oerth in the "Solar" System.
It takes exactly 24 hours for Oerth to make one revolution on its axis. Oerth lies at the center of Greyspace. Unlike most planets, Oerth does not orbit its sun, Liga. Instead, the Sun, and all other planets in the crystal sphere, orbit Oerth. Thus, Greyspace houses a geosystem, rather than a solar system. Oerth's "moons", Celene (Kule) and Luna (Raenei), are in fact the first and second planets in the geosystem, with Liga being the third. From Oerth, planets beyond Liga appear to the naked eye as wandering stars.
Liga orbits the Oerth once every 364 days, thus the Oerthly year is slightly shorter than that on Earth. Luna has a 28 day lunar cycle, while Celene's cycle is 91 days.
Alternate Oerths.
In a 1984 interview for "Polyhedron" Newszine, Gary Gygax revealed several "alternate Oerths" while explaining the setting for his HEROES CHALLENGE game books, co-written with author Flint Dille and published under the aegis of the Dungeons & Dragons Entertainment Corporation by the Wanderer Book division of Simon & Schuster.
"By the way, action takes place on Yarth, a place somewhat similar to Oerth, the setting of Greyhawk, "et al." It has fewer magical properties than Oerth but more than Earth. It is not impossible that additional works will be contracted for in months to come, action being set on Yarth or perhaps another alternate world, Aerth. On Earth, magic is virtually non-existent. On Uerth, dweomers are weak, chancy things. Yarth has a sprinkling of things magical, and Oerth is pure magic."
This implies that the planets differ in both their geographies as well as the relative presence of magic; but also that they operate in divergent realities.
Other references to these alternate Oerths appear in the Gord the Rogue short story anthology "Night Arrant" as well as in Gygax's "Epic of Aerth" campaign setting for the Dangerous Journeys roleplaying game. The five parallel worlds of Oerth, Aerth, Uerth, Yarth and Earth were recently mentioned in the 3rd Edition adventure "Expedition to the Ruins of Greyhawk" and thus found their way into 3rd edition canon.

</doc>
<doc id="46928" url="http://en.wikipedia.org/wiki?curid=46928" title="Forgotten Realms">
Forgotten Realms

The Forgotten Realms is a campaign setting for the "Dungeons & Dragons" ("D&D") fantasy role-playing game. Commonly referred to by players and game designers alike as "The Realms", it was created by game designer Ed Greenwood around 1967 as a setting for his childhood stories. Several years later, Greenwood brought the setting to the "D&D" game as a series of magazine articles, and the first Realms game products were released in 1987. Role-playing game products have been produced for the setting ever since, as have various licensed products including sword and sorcery novels, role-playing video game adaptations (including the first massively multiplayer online role-playing game to use graphics), and comic books. The Forgotten Realms is one of the most popular "D&D" settings, largely due to the success of novels by authors such as R. A. Salvatore and numerous role-playing video games, including "Pool of Radiance" (1988), "Baldur's Gate" (1998), "Icewind Dale" (2000) and "Neverwinter Nights" (2002).
"Forgotten Realms" is the name of a fantasy world setting, described as a world of strange lands, dangerous creatures, and mighty deities, where magic and supernatural phenomena are quite real. The premise is that, long ago, the Earth and the world of the Forgotten Realms were more closely connected. As time passed, the inhabitants of planet Earth have mostly forgotten about the existence of that other world—hence the term "Forgotten Realms". On the original Forgotten Realms logo, which was used until 2000, small runic letters read "Herein lie the lost lands", an allusion to the connection between the two worlds.
The world.
The focus of the Forgotten Realms setting is the continent of Faerûn, part of the fictional world of Abeir-Toril, usually called simply Toril, an Earth-like planet with many real-world influences. Unlike Earth, the lands of the Forgotten Realms are not all ruled by the human race: the planet Toril is shared by humans, dwarves, elves, goblins, orcs, and other peoples and creatures. Technologically, the world of the Forgotten Realms is not nearly as advanced as that of Earth; in this respect, it resembles the pre-industrial Earth of the 13th or 14th century. However, the presence of magic provides an additional element of power to the societies. There are several nation states and many independent cities, with loose alliances being formed for defense or conquest. Trade is performed by ship or horse-drawn vehicle, and manufacturing is based upon cottage industry.
Geography.
Toril consists of several large continents, including Faerûn, the western part of a continent that was roughly modeled after the Eurasian continent on Earth. Faerûn was first detailed in the original "Forgotten Realms Campaign Set", published in 1987 by TSR. The other continents include Kara-Tur, Zakhara, Maztica, and other yet unspecified landmasses. Kara-Tur, roughly corresponding to ancient East Asia, was later the focus of its own source book "", published in 1988. There is also a vast subterranean world called the Underdark beneath the surface.
Various products detailing specific areas of Faerûn, such as the 2nd edition "FR13 Anauroch" (1991), "FR15 Gold and Glory" (1992), "FR16 The Shining South" (1993), and "FRS1 The Dalelands" (1993), have been released, and through these much of the continent has been heavily detailed and documented, creating a highly developed setting.
In early editions of the setting, The Realms shared a unified cosmology with various other campaign settings called the Great Wheel. In this way each of the "Dungeons & Dragons" campaign settings were linked together to form one interwoven world connected by various planes of existence. With the release of the 2001 "Forgotten Realms Campaign Setting", the setting was given its own distinct and separate cosmological arrangement, with unique planes not explicitly connected to those of the other settings.
Religion.
Religion plays a large part in the Forgotten Realms, with deities and their followers being an integral part of the world. They do not have a passive role, but in fact interact directly in mortal affairs, answer prayers, and have their own personal agendas. All deities must have worshipers to survive, and all mortals must worship a patron deity to secure a good afterlife. A huge number of diverse deities exist within several polytheistic pantheons; a large number of supplements have documented many of them, some in more detail than others.
Much of the history of The Realms detailed in novels and source books concerns the actions of various deities and The Chosen (mortal representatives with a portion of their deities' power) such as Elminster, Fzoul Chembryl, Midnight (who later became the new embodiment of the goddess of magic, Mystra), and the Seven Sisters. Above all other deities is Ao, the Overlord. Ao does not sanction worshipers and distances himself from mortals. He is single-handedly responsible for the Time of Troubles, or Godswar, as seen in "The Avatar Trilogy".
Characters.
The setting is the home of several iconic characters popularized by authors, including Elminster the wizard, who has appeared in several series of novels created by Greenwood himself, and Drizzt Do'Urden, the highly popular Drow, or dark elf, ranger created by R. A. Salvatore.
History.
Early years.
Ed Greenwood began writing stories about the Forgotten Realms as a child, starting around 1967; they were his "dream space for swords and sorcery stories". Greenwood came up with the "Forgotten Realms" name from the notion of a multiverse of parallel worlds; Earth is one such world, and the Realms another. In Greenwood's original conception, the fantastic legends of Earth derive from a fantasy world, the way to which has been lost. Greenwood discovered the "Dungeons & Dragons" game in 1975, and became a serious role-playing enthusiast with the first "AD&D" game releases in 1978. The setting became the home of Greenwood's personal campaign. Greenwood began a Realms campaign in the city of Waterdeep, then started another group known as the Knights of Myth Drannor in Shadowdale. Greenwood felt that his players' thirst for detail made the Realms what it is: "They want it to seem real, and work on ‘honest jobs’ and personal activities, until the whole thing grows into far more than a casual campaign. Roleplaying always governs over rules, and the adventures seem to develop themselves." Greenwood has stated that his own version of the Forgotten Realms, as run in his personal campaign, is much darker than published versions.
Beginning with the periodical's 30th issue in 1979, Greenwood published a series of short articles that detailed the setting in "The Dragon" magazine, the first of which was about a monster known as the "curst". Greenwood wrote voluminous entries to "Dragon" magazine, using the Realms as a setting for his descriptions of magic items, monsters, and spells. In 1986, the American game publishing company TSR began looking for a new campaign setting for the "Advanced Dungeons & Dragons" game, and assigned Jeff Grubb to find out more about the setting used by Greenwood as portrayed in his articles in "Dragon". According to Greenwood, Grubb asked him "Do you just make this stuff up as you go, or do you really have a huge campaign world?"; Greenwood answered "yes" to both questions. TSR felt that the Forgotten Realms would be a more open-ended setting than the epic Dragonlance setting, and chose the Realms as a ready-made campaign setting upon deciding to publish "AD&D" 2nd Edition. Greenwood agreed to work on the project, and began working to get the Forgotten Realms officially published. Greenwood sent TSR a few dozen cardboard boxes stuffed with pencil notes and maps, and sold all rights to the Realms for a token fee. Greenwood noted that TSR altered his original conception of the Realms being a place that we could travel to from our world, "Concerns over possible lawsuits (kids getting hurt while trying to 'find a gate') led TSR to de-emphasize this meaning".
Advanced Dungeons & Dragons 1st edition.
Although the Realms were yet to be an official campaign world, the module "H1: Bloodstone Pass", released in 1985 by TSR, is now considered to be a part of the Forgotten Realms, although it wasn't until module H3 "The Bloodstone Wars" was released that Forgotten Realms became the official setting for the module series. The first official Forgotten Realms product was Douglas Niles's "Darkwalker on Moonshae", the first book in "The Moonshae Trilogy", which predates the "Forgotten Realms Campaign Set" by one month. The "Campaign Set" (often referred to as the "Grey Box") was later released in 1987 as a boxed set of two source books ("Cyclopedia of the Realms" and "DM's Sourcebook of the Realms") and four large maps, designed by Greenwood in collaboration with author Jeff Grubb. This boxed set introduced the campaign setting and explained how to use it. The Forgotten Realms became an instant hit. The compilation module "Desert of Desolation" was reworked to fit into the Forgotten Realms. The module "N5: Under Illefarn" bears the "Forgotten Realms" logo on the cover, as do the two modules released in 1988, "H4: The Throne of Bloodstone" and "I14: Swords of the Iron Legion".
"The Crystal Shard" was released in 1988, and was the first novel to feature the successful character Drizzt Do'Urden, who has since appeared in more than seventeen subsequent novels, many of which have appeared on the "New York Times" Best Seller list. In 1988, the first in a line of Forgotten Realms role-playing video games, "Pool of Radiance", was released by Strategic Simulations, Inc. The game was popular, winning the Origins Award for "Best Fantasy or Science Fiction Computer Game of 1988". In 1992, the game was ported to the Nintendo Entertainment System.
Several supplements to the original boxed set were released under the first edition rules, including FR1 "Waterdeep and the North" and FR2 "Moonshae" in 1987, and FR3 "Empires of the Sands", FR4 "The Magister", FR5 "The Savage Frontier", FR6 "Dreams of the Red Wizards", and REF5 "Lords of Darkness" in 1988. Also in 1988 came the "City System" boxed set, containing several maps of the city of Waterdeep. "Ruins of Adventure", a module based on the computer game "Pool of Radiance", was released in 1988.
The boxed set "" was released in 1988, giving details of the lands of Kara-Tur which had previously appeared in the 1986 book "Oriental Adventures", and were now officially placed in the Forgotten Realms world. The same year, the module "OA5: Mad Monkey vs. the Dragon Claw" was released for the Kara-Tur setting as a Forgotten Realms product.
In 1989, DC Comics began publishing a series of Forgotten Realms comics written by Jeff Grubb. Each issue contained twenty-six pages, illustrated primarily by Rags Morales and Dave Simons. Twenty-five issues were published in total, with the last being released in 1991. A fifty-six page annual "Forgotten Realms Comic Annual #1: Waterdhavian Nights", illustrated by various artists, was released in 1990.
Advanced Dungeons & Dragons 2nd edition.
An eponymous module, based on the role-playing video game "Curse of the Azure Bonds", was released in 1989, as was the "The Avatar Trilogy" series of novels, consisting of "Shadowdale", "Tantras", and "Waterdeep" that detailed the storyline which became known as the "Time of Troubles". A series of module adaptations for these novels ("Shadowdale", "Tantras", and "Waterdeep") were released in the same year, along with the "Hall of Heroes" accessory, detailing many of the major characters appearing in "Forgotten Realms" novels published up through that time. In early 1990, the hardcover "Forgotten Realms Adventures" by Jeff Grubb and Ed Greenwood was released, which introduced the Realms setting to the second edition of the "Advanced Dungeons & Dragons" game and detailed how the Time of Troubles had changed the setting. The RPGA used the Forgotten Realms city of Ravens Bluff as the setting for their first living campaign. Official RPGA support for this product line included the "Living City" modules series. A number of sub-settings of the Forgotten Realms were briefly supported in the early 1990s. Three more modules were produced for the Kara-Tur setting. "The Horde: Barbarian Campaign Setting", released in 1990, detailed The Hordelands, which also featured a series of three modules. The "Maztica Campaign Set", released in 1991, detailed the continent of Maztica.
The original gray boxed set received a revision in 1993 to update it to the second edition "Advanced Dungeons & Dragons (AD&D)" rules system, with the release of a new "Forgotten Realms Campaign Setting" boxed set, containing three books ("A Grand Tour of the Realms", "Running the Realms", and "Shadowdale") and various "monster supplements", with a new graphic look. Additional material for the setting was released steadily throughout the 1990s. Forgotten Realms novels, such as the "Legacy of the Drow" series, the first three books of "The Elminster Series", and numerous anthologies were also released throughout the 1990s, which lead to the setting being hailed as one of the most successful shared fantasy universes of the 1990s. These novels in turn sparked interest in role-playing activity by new gamers.
Numerous Forgotten Realms video games were released Between 1990 and 2000. The "Eye of the Beholder" PC game was released in 1990. This game was later followed by two sequels, the first in 1991, and the second in 1992. All three games were re-released for DOS on a single disk in 1995. Another 1991 release was "Neverwinter Nights" on America Online, the first graphical Massively Multiplayer Online Role-Playing Game (MMORPG). In 1998, "Baldur's Gate" was released, the first in a line of popular role-playing video games developed by BioWare and "considered by most pundits as the hands-down best PC roleplaying game ever". The game was followed by a sequel, ' in 2000 as well as "Icewind Dale", a separate game that utilized the same game engine as "Baldur's Gate". ' was released in 2001. Several popular Forgotten Realms characters such as Drizzt Do'Urden and Elminster made minor appearances in these games.
Dungeons & Dragons 3rd edition.
With the release of the 3rd edition "Dungeons & Dragons" rules system in 2000 by Wizards of the Coast, the "Forgotten Realms Campaign Setting" was released as a hardcover, in 2001, updating the official material and advancing the timeline of the game world. In 2002, the "Forgotten Realms Campaign Setting" won the Origins Award for "Best Role-Playing Game Supplement of 2001".
Several additional rulebooks were released for the new edition, including ' (2001), "Magic of Faerûn" (2001), "Lords of Darkness" (2001), "Faiths and Pantheons" (2002), "Silver Marches" (2002), "Races of Faerûn" (2003), and "Unapproachable East" (2003). Adventure modules included "Into the Dragon's Lair" (2000), ' (2001), and "City of the Spider Queen" (2002).
In 2002, Bioware released "Neverwinter Nights", set in the northern reaches of Faerûn and operating on the revised 3.0 rules for D&D. It was followed by two expansion packs, ' and '. A sequel using version 3.5 of the rules was produced by Obsidian Entertainment in 2006, itself followed by the expansion sets ' and '. The "Forgotten Realms Deluxe Edition" compilation was released in 2006, containing the "Baldur's Gate" series (excluding the "Dark Alliance" games), "Icewind Dale" series, and all "Neverwinter Nights" games before "Neverwinter Nights 2".
With the release of the version 3.5 update to the rules, the Forgotten Realms product line continued to expand. Accessories released included "Underdark" (2003), "Player's Guide to Faerûn" (2004), "Serpent Kingdoms" (2004), "Shining South" (2004), "Lost Empires of Faerûn" (2005), "Champions of Ruin" (2005), ' (2005), "Champions of Valor" (2005), "Power of Faerûn" (2006), "Mysteries of the Moonsea" (2006), "Dragons of Faerûn" (2006), and "The Grand History of the Realms" (2007). Adventure modules released included "Sons of Gruumsh" (2005), "The Twilight Tomb" (2006), "Expedition to Undermountain" (2007), ' (2007), ' (2007), and ' (2007).
Dungeons & Dragons 4th edition.
With the release of the "Dungeons & Dragons" 4th Edition, the Forgotten Realms were updated again to the new rules system, featuring a very changed Realms and moving the fictional world's timeline 104 years into the future. The "Forgotten Realms Campaign Guide", released August 2008, is a 288-page book for Dungeon Masters. The "Forgotten Realms Player's Guide" was released the following month, and contains information for players to help create Forgotten Realms characters. An adventure, "Scepter Tower of Spellgard", was also released in September 2008 and can be used in combination with the adventure in the "Forgotten Realms Campaign Guide" to start a Forgotten Realms campaign. In 2008, the Forgotten Realms also became the setting for the RPGA's sole living campaign, Living Forgotten Realms, replacing Living Greyhawk. In 2011, the Neverwinter Campaign Setting was released.
Dungeons & Dragons 5th edition.
With the release of the "Dungeons & Dragons" 5th Edition, Wizards of the Coast announced that the Forgotten Realms would continue to serve as the official campaign setting for its upcoming published adventure materials. To date three officially licensed products set in the Forgotten Realms have been published: Hoard of the Dragon Queen (September 2014), Rise of Tiamat (October 2014) and Princes of the Apocalypse (April 2015). With the 5th edition the world of Forgotten Realms was again returned to its previous, 3rd edition geography, and the event called The Sundering resulted in the worlds of Abeir-Toril to be split in two. Wizards of the Coast has also announced that its upcoming adventure series will also be set in the Forgotten Realms.
Reception.
In his book "The Fantasy Roleplaying Gamer's Bible", Sean Patrick Fannon describes the Forgotten Realms as being "the most ambitious fantasy game setting published since Tekumel", and that it "may be the most widely played-in game setting in RPG history." Similarly, in literature, the novels written in the Forgotten Realms setting have formed one of "the industry's leading fantasy series". Over time these novels have gained "unprecedented popularity", which led, as Marc Oxoby noted in his book, "The 1990s", to the novels having an "extraordinary shelf life", remaining in print for many years. This popular reception has also been reflected in public libraries. For example, Joyce Saricks states in "The Readers' Advisory Guide to Genre Fiction" that the novels have been among the most requested books by fans of the fantasy genre.

</doc>
<doc id="46930" url="http://en.wikipedia.org/wiki?curid=46930" title="Ravenloft">
Ravenloft

Ravenloft is a campaign setting for the "Dungeons & Dragons" roleplaying game. It is an alternate time-space existence known as a "pocket dimension" called the Demiplane of Dread, which consists of a collection of land pieces called "domains" brought together by a mysterious force known only as "The Dark Powers". Each domain is mystically ruled by a being called a "Darklord".
Setting.
"Ravenloft" is primarily a Gothic horror setting. Dungeon Masters are encouraged to use scenes that build apprehension and fear, culminating in the eventual face-to-face meeting with the nameless evil. Characters have a much greater significance attached to their acts, especially if they are morally impure, as they risk coming under the influence of the Dark Powers (through the game process called "dark powers checks") and gradually transforming themselves into figures of evil.
The magical mists of Ravenloft could appear anywhere in the "Dungeons & Dragons" universe, drawing evil-doers (or player characters) into the "Ravenloft" setting. One exception is the 'phlogiston' of the Spelljammer setting. The phlogiston blocks all planar travel, but the mists can appear in deep space inside crystal shells, according to the "Complete Spacefarer's Handbook". Another exception is in the Inner and Outer Planes, which for some reason they never appear in.
The Dark Powers.
The Dark Powers are a malevolent force who control the Demiplane of Dread. Their exact nature and number are deliberately kept vague, allowing for plot development in accordance with the Gothic tradition of storytelling—where the heroes are frequently outclassed and outnumbered by unknowably evil forces beyond their control.
The Dark Powers most frequently serve as a plot device for Ravenloft, especially concerning the Dark Lords, the de facto visible rulers of the Ravenloft Demiplane. Where the players are often tormented and opposed by the Dark Lords, the Dark Lords are themselves tormented and opposed by the Dark Powers. Of course, the difference lies in order of power—while many "D&D" adventures focus on allowing a band of heroes to prevail over a Dark Lord (much as in the spirit of Bram Stoker's novel "Dracula"), no such victory over the Dark Powers is conceivable. Vecna, (a demi-god and darklord) and Lord Soth "escaped" Ravenloft, but are the only two known to have done so.
Most frequently, the Dark Powers make their wishes and intents known through subtle manipulations of fate. Thus, Barovia's vampire lord Strahd von Zarovich's many attempts to win back his love, Tatyana, are doomed to failure, but the Dark Powers arrange such that he never truly loses hope. Each time, for example, Strahd's own actions may be partially culpable for his failure, and as such he may go through crippling self-recrimination, rather than cursing the gods solely and giving up. Most other Dark Lords have similar tales of frustration, kept all the more unbearable because the flicker of the possibility of success is never truly extinguished.
Not all Dark Lords acknowledge the Dark Powers directly, however. Strahd, for example, in his own memoirs, speaks only of a force known as "Death," who mocks him with the voices of his family and former colleagues throughout his life. Vlad Drakov, the Dark Lord of Falkovnia whose military expeditions are doomed to constant failure, seems even to be totally oblivious of any non-mortal factors in his repeated defeats.
The Dark Powers also seem capable of non-evil manipulations. Although their machinations are often directly responsible for the misery of many of Ravenloft's inhabitants, they also appear to play a role as dispensers of justice. Some tales of innocents who have escaped Ravenloft for happier environs are attributed to the Dark Powers, who have judged a being worthy of reward and release from their misty domain.
The precise nature of the Dark Powers of Ravenloft is never explicitly described in the game material, with the exception of a few of the novels based on the setting, and even those are considered non-canon. In a sense, the Dark Powers are intended to be eternal unknowns, an array of mercurial, unforeseeable, and inscrutable wills whose motives and actions the player characters cannot hope to understand.
System.
Since 2001, "Ravenloft" has used the d20 System, with a few modifications. Additional new saves are used within "Ravenloft": Fear, Horror and Madness, as well as the standard Fortitude, Will and Reflex saves. New Prestige Classes, spells and feats have also been added.
History.
First published in 1983 as a stand-alone Advanced Dungeons & Dragons adventure module, "I6:Ravenloft", was popular enough to spawn an Advanced Dungeons & Dragons Adventure Gamebook and a 1986 sequel (I10: The House on Gryphon Hill). "Ravenloft" was launched as a full-fledged campaign setting in 1990, with the "" boxed set, popularly known as the "Black Box". The campaign setting was revised twice during Second Edition — first as the "Ravenloft Campaign Setting" "Red Box", then as the "Domains of Dread" hardback. In 1994 "Ravenloft" spun off into sub-setting called "Masque of the Red Death", set on "Gothic Earth", an 1890s version of Earth where fantasy creatures exist only in the shadows of civilization. The "Ravenloft" line was cancelled by Wizards of the Coast after acquiring TSR. The "Black Box" won the Origins Award in 1991 for "Best Graphic Presentation of a Roleplaying Game, Adventure, or Supplement of 1990".
TSR also published a series of novels set in "Ravenloft". Each was typically focused on one of the darklords that inhabited the "Ravenloft" world, with several focusing on the figure of Count Strahd von Zarovich. Many of these early novels were by authors who would later receive wider fame as horror/dark fantasy authors. These authors have included Elaine Bergstrom, P. N. Elrod, Christie Golden and Laurell K. Hamilton.
"Ravenloft" was licensed to Arthaus Games for Dungeons & Dragons 3rd Edition (as "Ravenloft Campaign Setting - Core Rulebook") and Dungeons and Dragons v.3.5 (as "Ravenloft - Player's Handbook - v.3.5 Core Campaign Setting") and published by White Wolf Game Studio through the Sword & Sorcery Studios imprint. Arthaus' license to the "Ravenloft" setting was allowed to revert to Wizards of the Coast on August 15, 2005, but Sword & Sorcery retained the right to continue to sell its back stock until June 2006. The timing of this reversion meant that the "Ravenloft" supplement "Van Richten's Guide to the Mists" did not see print; instead, it was released as a free download in late September 2005.
The campaign setting published by White Wolf introduced a number of alterations, many based on conflicts with existing Wizards of the Coast intellectual property. Lord Soth, a character created for the Dragonlance setting, was removed, and the island featuring the demi-god Vecna and his rival, Kas, was likewise excised due to the characters' origins in the Greyhawk setting, and any references to D&D pantheon gods have been replaced with "Ravenloft"-specific names (for example, Bane is referred to as "The Lawgiver").
In October 2006, Wizards of the Coast released "Expedition to Castle Ravenloft", a hardcover version of the original 1st edition adventure updated for the Dungeons & Dragons version 3.5 rule set. The 2006 version includes maps from the original "Ravenloft" adventure, and new character generation options. "Expedition to Castle Ravenloft" is a stand-alone supplement set for any D&D worlds, and only requires the three core books for usage. This book is completely distinct from the Ravenloft of the Arthaus Games product line.
In September 2008, it was announced on Wizard of the Coast's "Digital Insider" #6 that Ravenloft would be reintroduced to 4th Edition Dungeons and Dragons in the October issue of the "Dragon' 'online magazine. It was noted that it would be "folded into the core [story]", implying that it would not be a campaign setting of its own, and instead become part of the canonical Dungeons & Dragons universe. In 2007, Wizards of the Coast announced the printing of two new "Ravenloft" novels for 2008, "Black Crusade" and "The Sleep of Reason", fueling more speculation. A short story by Ari Marmell, "Before I Wake," based on the realms of Darkon, Lamordia and Bluetspur was released on October 31, 2007 on the Wizards of the Coast website as a special for Halloween and featured characters inspired by H.P. Lovecraft and Clark Ashton Smith.
The Dungeons & Dragons 4th Edition supplement "Manual of the Planes" establishes that in the 4th Edition cosmology the Domains of Dread (and by extension the Ravenloft setting) are now located within the Shadowfell, a mirror-world of death and gloom lying adjacent to the mortal realm.
Novels.
A number of tie-in novels were released, set in the Demiplane of Dread:
Reception.
Game designer Rick Swan commented in 1994 that when the Ravenloft setting first came out, it "just didn't seem special, a Forgotten Realms variant with a few more bats", but after supplements like "Forbidden Lore", "The Created", and the "Van Richten's Guide" series, Swan felt that "the Ravenloft campaign has proven to be a credible adventure alternative for players interested in the dark side of the "AD&D" game. Though it lacks the flamboyance of "Call of Cthulhu" and the, er, bite of "", the Ravenloft setting remains the hobby's most enduring fusion of horror and fantasy."
References.
</dl>

</doc>
<doc id="46931" url="http://en.wikipedia.org/wiki?curid=46931" title="Mystara">
Mystara

Mystara is a campaign setting for the "Dungeons & Dragons" fantasy role playing game. Although it has officially been dropped from production by its creators, many fans continue to develop this fantasy setting jointly, continuing its original theme of group development.
Development.
It originated as the Known World, a semi-generic setting used in early adventure modules, first mentioned in the Module X1, "Isle of Dread", which was expanded upon in various D&D modules and sources, particularly a series of Gazeteers.
Mystara began as several semi-independent projects by different teams of writers who were each assigned to the task of developing different cultures and nations that would exist in the fantasy world that was supported by Dungeons & Dragons at the time. Their work was gathered and compiled, blended, and combined to form a fantasy world, Mystara.
The "D&D" Gazetteer series details the game's Known World setting. Each Gazetteer treats one nation or empire, and has three basic elements: cultural and geographic background, features, and adventures. The cultural and geographic campaign background section offers a brief history and timeline for each nation; basic geography, climate, and ecology; and, fundamental social and political concepts of the region. Each Gazetteer also offers a list of scenario ideas appropriate to the campaign setting.
Trenton Webb for the British "Arcane" RPG magazine described Mystara as "a traditional Tolkienesque world".
Mystara Planet.
Mystara's outer surface consists of three principal land masses: the continent of Brun, the continent of Skothar, and the continent of Davania, plus the island continent of Alphatia (up to AC 1010). In the officially published material, the Known World concentrated on the eastern portion of Brun along with the lands of the Sea of Dawn. The continents of Mystara resemble those of the earth approximately 135 million years ago.
The inhabitants of Mystara are diverse: humans of all races can be found here, along with myriad creatures such as elves, dwarves, halflings, orcs, and dragons.
Some of the notable nations of Mystara include the Thyatian Empire, the Grand Duchy of Karameikos, the Principalities of Glantri, the merchant-run Republic of Darokin, the Emirates of Ylaruam, the Dwarven nation of Rockhome, the Elven Kingdom of Alfheim, Halfling lands of the Five Shires and the chaotic Alphatian Empire.
The continent of Brun.
The most commonly known land mass on Mystara's outer surface is actually a tiny portion of the continent of Brun itself. In the officially published material, the Known World concentrated on the eastern portion of Brun along with the lands of the Sea of Dawn.
The Known World.
The Known World has cultures and a level of technological development that resemble the Europe of our Earth around the 15th century, but without gunpowder. Nations of the known world display a great range of government types. Some nations are populated entirely by demihumans and/or humanoids. By common convention, the boundaries of the "Known World" are those covered in the world map as originally published in the module X1, The Isle of Dread, plus Norwold, the Isle of Dawn, and (pre Wrath of the Immortals) Alphatia.
As the name implies, the "Known World" covers the most notable nations of Mystara, the ones most commonly used in Mystara-based campaigns and featured in fiction (both officially published "canon" and fan-based). It includes the Thyatian Empire, which could be compared to Byzantine Empire; the Grand Duchy of Karameikos (which includes the town of Threshold, the default setting of many classic D&D adventures), comparable to medieval southeastern Europe; the Principalities of Glantri, which is similar to medieval western Europe, ruled by wizard-princes; the Ethengar Khanate, a Mongol-like society; the merchant-run Republic of Darokin, which is based somewhat loosely on the mercantile states of Medieval Italy; the Emirates of Ylaruam which have an Arabic flavor; the Heldannic Territories, ruled by an order of religious Knights devoted to the Immortal Vanya, similar to the Teutonic Knights; the Atruaghin Clans, which have an Amerindian feel; the nation of Sind, based on India during the rule of the Mughals; the Northern Reaches Kingdoms of Ostland, Vestland, the Soderfjord Jarldoms, based on Scandinavian kingdoms at various periods of history; the Dwarven nation of Rockhome; the elven Kingdom of Alfheim; the Halfling lands of the Five Shires; and the Alphatian Empire, ruled by wizards and other spellcasters.
To the distant Northwest of the "Known World", across the Great Waste, lays the mysterious lands of Hule, ruled by Hosadus, also known as "The Master". Also on the periphery of the Known World are the Kingdoms of Wendar and Denagoth, the first an elven-dominated nation and the latter a mountainous and dark realm of evil, with ill-intentions towards Wendar. The Adri Varma lies between Sind, Wendar, the Great Waste, and The Black Mountains, forming the northern border of Glantri and defining the northwestern limits of the region.
The Savage Coast.
Mystara includes the Savage Coast, a coastal area located in the south central part of the Brun continent, to the south and west of Hule. This part of Mystara is affected by the Red Curse, a sinister enchantment which eventually kills its inhabitants through mutation unless the (fictional) metal "cinnabryl" is worn in contact with the body. This area was published in its own boxed set entitled Red Steel, and later republished on-line as the Savage Coast. Its swashbuckling flavor is very different from that of the "Known World", closer in atmosphere to that of the Age of Exploration than the fantasy middle-ages/renaissance tone of the Known World. The Savage Coast is complete with gunpowder ("Smokepowder") weaponry.
The specifics of the "Red Curse", which include mutilation of the body and extreme degeneration of physical and mental health, also tend to keep the inhabitants of the Savage Coast within the region, as debilitating effects result if they leave the cursed area.
The continent of Davania.
Even though most of the Known World civilizations historically originated from this part of the planet, it did not see much development while the Mystara product line was still in production. The only major appearance of the continent was in Dragon magazine, where parts of it were sketched out during the Voyage of the Princess Ark series, by Known World Product Manager Bruce Heard.
In recent years, many Mystara fans have been turning their attention to Davania with fan-made material.
The continent of Skothar.
Very little was officially developed for this part of Mystara. Ever since the Mystara product line was discontinued, fans have created their own material for this part of Mystara.
The Hollow World.
Mystara is a hollow planet, with a habitable surface on its interior called the Hollow World. This world is lit by an eternal red sun at the center of Mystara, and serves as a "cultural museum," preserving the societies that have become extinct in the outer world. The existence of the Hollow World is not, in general, known to the inhabitants of the outer world. The poles are actually huge, subtly curving holes that allow passage between the outer and inner world, although it is a long, hard trek through a cold, unlit, stormy and anti-magic area. The curvature of the holes is so subtle that explorers from either surface do not notice the transition until after it is already made, causing quite a shock for most.
Moons.
Two moons orbit the planet. Matera is a moon much like our own, whose phases govern lycanthropy (werewolves, werebears, etc.). Only the Immortals inhabit Matera. They live in a city, Pandius, where they can meet and watch over Mystara. Patera, or Myoshima to its inhabitants, is an invisible moon that cannot be seen from Mystara. Patera's inhabitants have a culture similar to that of medieval Japan.
Blackmoor.
Mystara (like Greyhawk) also incorporated the Blackmoor setting by placing it in the world's distant past. Blackmoor evolved from a feudal kingdom into a highly advanced civilization, using more and more powerful—and destructive—technology. It ended itself in an apocalyptic explosion so devastating that it changed the climate and geography of the planet as a whole.
Mystara video games.
Video games set in Mystara include the Capcom arcade Beat 'em up/role-playing video games ' (1993) and ' (1996).
Other Mystara video games are: "" (Sega Genesis, 1992), "Fantasy Empires" (PC, 1993), and "Order of the Griffon" (TurboGrafx 16, 1992).

</doc>
<doc id="46932" url="http://en.wikipedia.org/wiki?curid=46932" title="Planescape">
Planescape

Planescape is a campaign setting for the "Dungeons & Dragons" fantasy role-playing game, originally designed by Zeb Cook. The "Planescape" setting was published in 1994. As its name suggests, the setting crosses and comprises the numerous planes of existence, encompassing an entire cosmology called the Great Wheel, as originally developed in the "Manual of the Planes" by Jeff Grubb. This includes many of the other "Dungeons & Dragons" worlds, linking them via inter-dimensional magical portals.
Development.
"Planescape" is an expansion of ideas presented in the "Advanced Dungeons & Dragons" "Dungeon Master's Guide" (First Edition) and the original "Manual of the Planes". When "Advanced Dungeons & Dragons" 2nd edition was published, a decision was made not to include angelic or demonic creatures, and so the cosmology was largely ignored, being replaced (to a certain degree) by the "Spelljammer" setting. However, fan demand for a 2nd Edition "Manual of the Planes" was strong enough to justify its expansion into a full-fledged campaign setting, and so in 1994 Planescape was released.
David "Zeb" Cook developed Planescape when he was assigned to create "a complete campaign world (not just a place to visit), survivable by low-level characters, as compatible with the old "Manual of the Planes" as possible, filled with a feeling of vastness without overwhelming the referee, distinct from all other TSR campaigns, free of the words "demon" and "devil" and explainable to Marketing in 25 words or less". For inspiration, Cook listened to Pere Ubu, Philip Glass and "Alexander Nevsky", read "The Dictionary of the Khazars", "Einstein's Dreams", and "The Narrow Road to the Deep North", and for fun at "Bad Movie Nights", watched such films as "Naked Lunch" and "Wolf Devil Woman".
Cook came up with the idea that everything would revolve around factions, and that those factions would be ideas taken to the extreme. He also felt that Sigil came about because it was natural, because the planes needed a crossroads, and that the campaign needs a center which could be both a place for adventure and a place to hide, where characters could get to and from it quickly. Cook decided to adapt the "Manual of the Planes" because the older material made survival on the planes too difficult or complex; he ignored anything that complicated gameplay, which left the "descriptions of twisted and strange creations".
Cook conceived of the look for the setting from images such as "the gloomy prisons of Piranesi's "Le Carceri" etchings, and Brian Froud's illustrations and surrealist art", and Dana Knutson was assigned to draw whatever Cook wanted. "Before any of us knew it, [Knutson] drew the Lady of Pain. I'm very fond of the Lady of Pain; she really locks up the Planescape look. We all liked her so much that she became our logo.
Reception.
"Planescape" won the 1994 Origins Award and has received critical acclaim for its unique visual aspects, especially the work of artists Tony DiTerlizzi, Robh Ruppel, and Dana Knutson. "Pyramid" magazine reviewer Scott Haring said "Planescape" is "the finest game world ever produced for "Advanced Dungeons & Dragons". Period." Haring described the writing as "wonderful," also saying that it "has got one of the most distinctive graphic looks I've seen in any game product" and that the "unusual drawings remind [him] a little of Dr. Seuss." Trenton Webb of British RPG magazine "Arcane" called Planescape "the premier "AD&D" world", noting its hallmark as "a bizarre juxtaposition of legend and nightmare". Game designer Rick Swan said that the original "Manual of the Planes" had in a sense been "reincarnated as the Planescape setting ... TSR's most ambitious campaign world to date. Abandoning the straightforward but dry approach of the "Manual", the Planescape set reads less like a textbook and more like a story. Characters take precedence over game systems, high adventure supplants the physics lessons."
Cosmology.
The "Dungeons & Dragons" cosmology as reflected in "Planescape" consists of a number of planes, which can be divided into the following regions:
Sigil.
"Sigil", the "City of Doors", is located atop the "Spire" in the Outlands. It has the shape of a torus, and the city itself is located on the inner surface of the ring. There is no sky, simply an all-pervasive light that waxes and wanes to create day and night. Sigil cannot be entered or exited save via portals. Although this makes it quite safe from any would-be invader, it also makes it a prison of sorts for those not possessing a portal key. Thus, many call Sigil "The Bird Cage" or "The Cage." Though Sigil is commonly held to be located "at the center of the planes" (where it is positioned atop the infinitely tall Spire), some argue that this is impossible since the planes are infinite in all dimensions, and therefore there can never truly be a center to any of them, let alone all of them. Curiously, from the Outlands, one can see Sigil atop the supposedly infinite Spire.
Factions.
The Factions are the philosophically-derived power groups based in Sigil. Before the "Faction War", the factions controlled the political climate of the city. Each of the factions is based on one particular belief system; many of the factions' beliefs make them enemies where their other goals and actions might have made them allies. There are fifteen factions in total.
"The Faction War".
In 1998, TSR published "Faction War", an adventure that effectively closed the book on Planescape as it was then ending the product line. The culmination of several adventures leading up to that point, the Faction War brought an end to the factions' control of the city. Instigated by the power-hungry Duke Rowan Darkwood, factol of the Fated, in a bid to dethrone the Lady and rule Sigil himself, the war spread throughout the city before the Lady of Pain, with the aid of a group of adventurers (the players' characters), intervened.
Sects.
Sects are in many ways identical to the Factions, differing in that they are not based in Sigil. Sects are often highly specific to the particular planes they originate from, though historically many of the Factions were once Sects and some Sects were once Factions. A complete list of Sects is probably not possible due the infinite multitudes of the Planes.
Rules.
There are three principles (or heuristics) governing the world of Planescape: the Rule-of-Three, the Unity of Rings, and the Center of the Multiverse.
Rule-of-Three.
The first principle, the Rule-of-Three, says simply that things tend to happen in threes. The principles which govern the planes are themselves subject to this rule.
Unity of Rings.
The second principle is the Unity of Rings, and notes that many things on the planes are circular, coming back around to where they started. This is true geographically as well as philosophically.
Center of All.
The third principle (fitting neatly into the Rule-of-Three above) is the Center of All, and states that there is a center of everything — or, rather, wherever a person happens to be "is" the center of the multiverse... from their own perspective, at least. As most planes are functionally infinite, disproving anyone's centricity would be impossible. In "Planescape", this is meant philosophically just as much as it is meant in terms of multiversal geography.
The fact that anywhere could be the center of the multiverse in this view also implies that nowhere can be said to be the de facto true and only center. This sparks a lot of arguments and violence since some people believe the City of Doors to be the center due to its uncommon number of portals to other planes and position in the Outlands and some factions also claim different centers, each with their own significance.
Published material.
The campaign setting was followed by a series of expansions detailing the Planes of Chaos (by Wolfgang Baur and Lester Smith), the Planes of Law (by Colin McComb and Wolfgang Baur), and the Planes of Conflict (by Colin McComb and Dale Donovan).
Other expansions and adventures followed, as listed below. Upon the release of 3rd Edition, Planescape, along with most other settings, were discontinued, although fan sites such as planewalker.com were allowed to continue to use the material and update it to the new edition. The 3rd Edition "Manual of the Planes", the 3.5 Edition "Dungeon Master's Guide", and the 2004 "Planar Handbook" also used the general layout of the planes and some of the details from the setting, including Sigil, but these are not part of the Planescape line. Similar material has surfaced in 4th Edition rulebooks, as the "Dungeon Master Guide 2" includes a section on Sigil. The 5th Edition Player's Handbook also contains a section explaining the planes and Sigil.
The series had a small number of novels. The novels were not generally well received.
In 1995, "Planescape" won the Origins Award for "Best Graphic Presentation of a Roleplaying Game, Adventure, or Supplement of 1994".
Video game.
The setting was featured in the computer game "", which portrayed the "Planescape" world (specifically Sigil, the Outlands, Baator, Carceri, and the Negative Energy Plane). It is now a cult game and was out of print until its DVD re-release as a budget title in 2009. It was released as a download on GOG.com in 2010 and soon became the "second most wanted game" on the site.
Collectible Card Game.
TSR published a collectible card game based on the "Planescape" setting called "Blood Wars". The game featured major locations, personalities, and features of the Planescape setting and also introduced new creatures that were added to the role playing game setting as part of subsequent products.

</doc>
<doc id="46933" url="http://en.wikipedia.org/wiki?curid=46933" title="Spelljammer">
Spelljammer

Spelljammer is a campaign setting for the "Advanced Dungeons & Dragons" (2nd edition) role-playing game, which features a fantastic (as opposed to scientific) outer space environment.
"Spelljammer" introduced into the AD&D universe a comprehensive system of fantasy astrophysics, including the Ptolemaic concept of crystal spheres. Crystal spheres may contain multiple worlds and are navigable using ships equipped with "spelljamming helms". Ships powered by spelljamming helms are capable of flying into not only the sky but into space. With their own fields of gravity and atmosphere, the ships have open decks and tend not to resemble the spaceships of science fiction, but instead look more like galleons, animals, birds, fish or even more wildly fantastic shapes.
The "Spelljammer" setting is designed to allow the usual sword and sorcery adventures of "Dungeons & Dragons" to take place within the framework of outer space tropes. Flying ships travel through the vast expanses of interplanetary space, visiting moons and planets and other stellar objects.
Like the Planescape setting, "Spelljammer" unifies most of the other AD&D settings and provides a canonical method for allowing characters from one setting (such as Dragonlance) to travel to another (such as the Forgotten Realms). However, unlike Planescape it keeps all of the action on the Prime Material Plane and uses the crystal spheres, and the "phlogiston" between them, to form natural barriers between otherwise incompatible settings. Though the cosmology is derived largely from the Ptolemaic system of astronomy, many of the ideas owe much to the works of Jules Verne and his contemporaries, and to related games and fiction with a steampunk or planetary romance flavor. A strong Age of Sail flavor is also present.
Setting.
Spelljamming helms.
"Spelljamming helms" are the central setting concept which allow interplanetary and interstellar space travel for vessels which would otherwise not be spaceworthy, in the form of a helm (as in rudder, not armored helmet). Any spellcaster may sit on a spelljammer helm to move the ship. The mysterious race known as the Arcane is the sole manufacturer and distributor of spelljamming helms. Within the "Dungeons & Dragons" universe, they are a method of converting magical energy into motive power.
Gravity and Air.
All bodies of a sufficiently large size have gravity. This gravity usually (but not always) exerts a force equal to the standard gravitational attraction on the surface of an Earth-sized planetary body. Gravity in the "Spelljammer" universe is also an exceptionally convenient force, and almost always works such that "down" orients itself in a manner most humanoids would find sensible.
All bodies of any size carry with them an envelope of air whenever they leave the surface of a planet or other stellar object. Unlike real-world astrophysics, this air envelope is not dispersed by the vacuum of space. These bubbles of air provide breathable atmosphere for varying lengths of time, but 3 months is considered "standard".
Crystal Spheres.
A crystal sphere (also known as a crystal shell) is a gigantic spherical shell which contains an entire planetary system. Each sphere varies in size but typically they are twice the diameter of the orbit of the planet that is farthest from the sun or planet at the center of the sphere (the system's primary).
The surface of the sphere is called the "sphere wall" and separates the void of "wildspace" (within the sphere) from the "phlogiston" (that surrounds and flows outside the sphere). The sphere wall has no gravity and appears to be impossible to damage by any normal or magical means. Openings in the sphere wall called "portals" allow spelljamming ships or wildspace creatures to pass through and enter or exit from a crystal sphere. Portals can spontaneously open and close anywhere on the sphere wall. Magical spells (or magical items that reproduce their effects) can allow a portal to be located. Other magic can open a new portal or collapse an existing one. Ships or creatures passing through a portal when it closes may be cut in two.
Note that unlike the Ptolemaic system, the crystal spheres are not nested within each other.
Wildspace.
Wildspace is similar to the outer space of science fiction, with planets, asteroids and stars, but with different physics. Gravity is either none or the same as that of Earth, and is directed towards the center of planet-sized bodies; on large objects in space like spacecraft and enormous creatures gravity is directed towards a flat plane running through the object's long axis, allowing characters to stand on the decks of ships.
The Phlogiston.
The phlogiston is essentially a big ocean of a unique element that is neither air, fire, water, or earth. The phlogiston (also known as "the Flow") is a bright, extremely combustible gas-like medium that exists between the Crystal Spheres. A signature property of the substance is that it does not exist within the boundaries of a crystal sphere, to the degree that it cannot be brought into a crystal sphere by any known means up to and including the direct will of deities. Every crystal sphere floats in the phlogiston, very slowly bobbing up and down over time. Travel between Crystal Spheres is facilitated by the formation of "Flow rivers" — sections of the phlogiston which have a current and greatly reduce travel time. Travel through the "slow flow" (i.e. off the Flow rivers) is possible, but very dangerous.
The "Spelljammer".
The "Spelljammer" is a legendary ship which looks like a gigantic manta ray, and houses an entire city on its back. All spacefarers (people who live in wildspace) have heard of the "Spelljammer" but very few have ever seen it themselves. It is this ship that gives its name to "spelljamming", "spelljamming helms" and anything else connected with spelljamming. The ship has been reported to have been seen in countless spheres for as long as records go back. Even some groundlings (people who live on planets that have very little or no commerce with spelljamming communities) have legends about it. There are hundreds of conflicting legends about this ship, and a mythology has developed about the ship that is similar to the legends surrounding "The Flying Dutchman".
As a living thing (although it does not consume any matter, it does absorb heat and light through its ventral (or under) side and uses them to produce air and food for its inhabitants), the "Spelljammer" has a complex life cycle and means of procreation. Normally the ship has no captain and wanders the cosmos seemingly aimlessly. When the "Spelljammer" has a captain, obtained through another complex process, it will create Smalljammers (miniature versions of the "Spelljammer") that go forth as its spawn. Apparently there can only be one Spelljammer at any one time. One Smalljammer will mature into a full "Spelljammer" ship if its predecessor is ever destroyed.
Races.
Alien races inhabiting the Spelljammer universe included humans, dwarves, xenophobic beholders, rapacious neogi, militant giff (humanoid hippopotami), centaurlike dracons, hubristic elf armadas, spacefaring orcs called "scro", mysterious arcane, the Thri-kreen insectoids, and bumbling tinker gnomes. Illithids were another major race, but were presented as more mercantile and less overtly evil than in other D&D settings. The "Monstrous Compendium" series added many more minor races. The simian Hadozee were also introduced into the setting and, later, incorporated into the 3.5 rules in the supplemental book Stormwrack.
Publication history.
Advanced Dungeons & Dragons (2nd edition).
The "" space fantasy supplement was released in 1989. Several of TSR's other campaign worlds had their own sections in the "Spelljammer Boxed Set" - "Realmspace" for the Forgotten Realms, "Krynnspace" for Dragonlance, and "Greyspace" for Greyhawk. Along with the new sphere - "Clusterspace" - they were known as the "Big Three and Astromundi". For more details, see List of Spelljammer crystal spheres. Dark Sun, Ravenloft and Mystara weren't included, as the first two did not fit with the setting and the Mystara only used the D&D rules, not the AD&D rules.
The core "Spelljammer" product line consisted of four boxed sets: "" (ISBN 0-88038-762-9) introduced the setting and provided the basic rules for spelljamming travel. "The Legend of Spelljammer" (ISBN 1-56076-083-4) expanded on the setting, in particular the "Spelljammer" itself. The "War Captain's Companion" (ISBN 1-56076-343-4) provided more detailed ship-to-ship combat rules, and "The Astromundi Cluster" (ISBN 1-56076-632-8) provided a roleplaying campaign.
The line was expanded by a number of accessories. "Lost Ships" (ISBN 0-88038-831-5) expanded the number of ships, while "Practical Planetology" (ISBN 1-56076-134-2) assisted DMs who wished to create their own spelljamming setting. "The Rock of Bral" (ISBN 1-56076-345-0) provided a home base for adventuring parties, and "Realmspace" (ISBN 1-56076-052-4), "Krynnspace" (ISBN 1-56076-560-7) and "Greyspace" (ISBN 1-56076-348-5) gave information about the crystal spheres housing TSR's three main campaign worlds. TSR also published a DM's screen (ISBN 1-56076-053-2) and two "Monstrous Compendiums" (ISBN 0-88038-871-4 and ISBN 1-56076-071-0).
A series of five connected adventures was released in the modules "Wildspace" (ISBN 0-88038-819-6), "Skull & Crossbows" (ISBN 0-88038-845-5), "Crystal Spheres" (ISBN 0-88038-878-1), "Under the Dark Fist" (ISBN 1-56076-131-8) and "Goblins' Return" (ISBN 1-56076-149-0). Also published was a longer campaign, "Heart of the Enemy" (ISBN 1-56076-342-6) and "Space Lairs" (ISBN 1-56076-609-3) contained short standalone adventures.
When TSR started to produce a line of handbooks in 1992, they added the "Complete Spacefarer's Handbook" (ISBN 1-56076-347-7) to cover spelljamming travel.
A full product list is given in the external links.
Dungeons & Dragons (3rd edition).
The "Spelljammer" line of products was discontinued by TSR before they were incorporated into Wizards of the Coast. Although Wizards of the Coast has not produced any new "Spelljammer" products they "have" given permission for a non-profit website, named , to continue support for the setting. Beyond the Moons is the official "Spelljammer" website and produces new material for "Spelljammer" AD&D (2nd edition), and converts existing AD&D material to work with the 3rd edition game.
Spelljammer: Shadow of the Spider Moon.
In 2002, Paizo published a new campaign setting for Spelljammer in issue 151 of "Polyhedron" magazine (with "Dungeon" #92). Using the D20 system, it provided new rules for firearms and spelljamming, as well as skills, feats and prestige classes. Spelljammer monsters such as neogi and giff were not used. Instead, it featured creatures from the "Monster Manual" such as drow, formians and yuan-ti.
Dungeons & Dragons (4th edition).
"Spelljammer" content appears in the 4th edition Manual of the Planes, referencing Spelljammer ships as one method of traveling between planes (and providing information for in-game use for an example Spelljammer vessel).
Hackjammer and Hackmaster.
In 2005, Kenzer & Co. released "Hackjammer" (ISBN 1-59456-037-0), a campaign supplement which converted much of the Spelljammer setting to the Hackmaster system.
Spinoffs.
As was common practice at TSR at the time, a number of tie-in products were produced for the "Spelljammer" line.
Comics.
Fifteen comics set in the "Spelljammer" universe were published by DC Comics between September 1990 and November 1991 with the creative team of Barbara Kesel, Michael Collins and Dan Panosian. "Spelljammer" comics also uses Jasmine, a winged human character originally introduced from Forgotten Realms comics, as one of the lead characters.
Novels.
Six novels set in the "Spelljammer" universe were published by TSR, before TSR was incorporated into Wizards of the Coast. The novels were interconnected and formed "The Cloakmaster Cycle". The novels tell the story of Teldin Moore, a 'groundling' farmer on Krynn who has a powerful and apparently cursed magical cloak that was given to him. He then ends up on a quest, which takes him first into wildspace and then away from his home sphere to distant crystal spheres. The series showcases the wonders and perils of the Spelljammer universe. The novels are now out of print.
Computer games.
Only one computer game set in the "Spelljammer" universe was ever produced. It was published by SSI in 1992 and was called "".
In 2002 a team of freelance game modification developers created for "Neverwinter Nights". This tileset included Spelljamming ships, space and atmospheric terrains, along with monsters and NPCs, all set within the Spelljammer Campaign setting.

</doc>
<doc id="46934" url="http://en.wikipedia.org/wiki?curid=46934" title="Drow (Dungeons &amp; Dragons)">
Drow (Dungeons &amp; Dragons)

The drow ( or ) or dark elves are a generally evil, dark-skinned subrace of elves in the "Dungeons & Dragons" fantasy roleplaying game.
Publication history.
The word "drow" is from the Orcadian and Shetlandic dialects of Scots, an alternative form of "trow", which is a cognate with "troll". The "Oxford English Dictionary" gives no entry for "drow", but two of the citations under "trow" name it as an alternative form of the word. Trow/drow was used to refer to a wide variety of evil sprites. Except for the basic concept of "dark elves", everything else about the "Dungeons & Dragons" drow was invented by Gary Gygax. However, in the Prose Edda Snorri Sturluson wrote about the black elves: "[...] the dark elves however live down below the ground. [...] while the dark elves are blacker than pitch.":103
"Dungeons & Dragons" co-creator Gary Gygax stated that "Drow are mentioned in Keightley's "The Fairy Mythology", as I recall (it might have been "The Secret Commonwealth"--neither book is before me, and it is not all that important anyway), and as Dark Elves of evil nature, they served as an ideal basis for the creation of a unique new mythos designed especially for the AD&D game." The form "drow" can be found in neither work. Gygax later stated that he took the term from a "listing in the "Funk & Wagnall's Unexpurgated Dictionary", and no other source at all. "I wanted a most unusual race as the main power in the Underdark, so used the reference to 'dark elves' from the dictionary to create the Drow." There seems to be no work with this title. However, the following entry can be found in abridged editions of Funk & Wagnall's "Standard Dictionary of the English Language", such as "The Desk Standard Dictionary of the English Language": "[Scot.] In folk-lore, one of a race of underground elves represented as skilful workers in metal. Compare TROLL. [Variant of TROLL.] trow"
Advanced Dungeons & Dragons 1st edition (1977-1988).
The drow were first mentioned in the "Dungeons & Dragons" game in the 1st Edition 1977 "Advanced Dungeons & Dragons" "Monster Manual" under the "Elf" entry, where it is stated that "The 'Black Elves,' or drow, are only legend." No statistics are given for the drow in this book, apart from the statistics for normal elves, and only a single paragraph is written about them. The drow are described here as purportedly dwelling deep beneath the surface world, in strange subterranean realms. They are said to be evil, "as dark as faeries are bright", and pictured in tales as poor fighters but strong magic-users.
It is hinted in G1 "Steading of the Hill Giant Chief" (TSR, 1978) that there is a "secret force, some motivational power behind this unusual banding of different races of giants." G2 "The Glacial Rift of the Frost Giant Jarl" (TSR, 1978) mentions this guiding force again in its introduction. The third module in the series, G3 "Hall of the Fire Giant King", again mentions the party's need to find out whatever is behind the giants' alliance, and this time mentions the drow specifically by name. In the adventure, the player characters can discover the first hint of drow involvement in the fire giant king's council room, on a scroll which promises "powerful help from the Drow", signed by Eclavdra. Actual drow can be encountered starting on level #2 of the king's hall, beginning with a group of drow priests, and then other drow later.
Having discovered that the drow instigated the alliance between the races of giants and its warfare against mankind, in "D1 Descent into the Depths of the Earth" the party follows the fleeing drow into the tunnels leading northwest and deep into the earth, to eliminate the threat they pose. Examining a golden spider pin found on one of the drow priestesses, the party can discover runes in the drow language reading "Lolth, Death Queen Mother". The party continues to pursue the drow in D2 "Shrine of the Kuo-Toa", meeting the kuo-toa and the deep gnomes (also known as the svirfneblin). As the party travels, signs of the drow are noted all around; the drow clearly freely pass through these subterranean areas, even though they are hated and feared by the other local intelligent races. The drow and kuo-toa trade with each other openly, but the kuo-toa hate and fear the drow, resulting in frequent skirmishes between the two peoples. In "D3 Vault of the Drow" the adventurers follow one of two subterranean passages leading out from the kuo-toan temple to come upon the Vault of the Drow, "a hemispherical cyst in the crust of the earth, an incredibly huge domed fault over 6 miles long and nearly as broad." The party eventually makes it to Erelhei-Cinlu, the vast subterranean city of the drow, which is thoroughly described in the module. The characters may freely enter the city and spend time there, unless they attempt to organize any escaped slave groups for open warfare against the drow; the threat of a slave uprising will bring the chaotic drow into full cooperation. An extensive overview of the drow power structure is given for the purpose of creating any number of mini-campaigns or adventures taking place inside the drow capital. The House of Eilservs, led by Eclavdra, turned from worship of Lolth to the Elder Elemental God when the city's other noble houses allied against them after proclaiming that their mistress should be the Queen of All Drow. Eilservs attempted to establish a power base through a puppet kingdom in the surface world dedicated to the worship of their new deity, so that their demands of supreme power in the Vault can no longer be denied, but this scheme was recently ruined. The characters travel on to the Egg of Lolth, where they must enter the dungeon level and fight the demoness herself. The players may discover an astral gate leading to the plane of the Abyss, which sets them up for module Q1. The statistics and information for drow are reprinted from "Hall of the Fire Giant King" in the back of this module, along with statistics for Lolth herself. The story concludes in module "Q1 Queen of the Demonweb Pits". The astral gate from D3 leads to the Abyssal realm of Lolth, Demon Queen of Spiders, goddess of the drow elves, and architect of the sinister plot described in the two previous series of modules. Her realm, the 66th layer of the Abyss, is referred to as the Demonweb Pits. The Q1 module offers a glimpse into the Abyss itself, home to the "D&D" race of demons, where time and space stretch and twist in bizarre ways, and there are many portals that allow passage into entirely different worlds. At the very end of the module, the players face a final confrontation with Lolth, an exceptionally difficult challenge. The G1-G3 modules were later published together in 1981 as a single combined module as "G1-2-3 Against the Giants", and the entire series of modules in which the drow originally appeared were later published together in "Queen of the Spiders" (1986).
The first hardcover "D&D" rulebook featuring statistical information on the drow was the original "Fiend Folio". Gygax wrote this entry, listed under "Elf, Drow", according to the book's credits section. The text is a slightly abridged version of that found originally found in modules G3 and D3. Likewise, Lolth's description from module D3 is reprinted in the "Fiend Folio" under the "Demon" heading.
The drow are first presented as a player character race in "Unearthed Arcana" (1985), also written by Gygax. Several elven sub-races are described in the book, including gray elves, wood elves, wild elves, and valley elves; the dark elves are described as the most divergent sub-race, and dark elf player characters are considered outcasts from their homeland, either by choice, differing from the standard chaotic evil alignment of the race, or having lost in some family-wide power struggle.
Novels.
Gary Gygax's 1986 novel for TSR's "Greyhawk Adventures" series, "Artifact of Evil", was the first novel to feature the drow prominently. Gygax's subsequent "Gord the Rogue" novels, published by New Intinities, Inc., continued the story and the drow's involvement, in the novels "Sea of Death" (1987), "Come Endless Darkness" (1988), and "Dance of Demons" (1988).
R. A. Salvatore's 1988-1990 "The Icewind Dale Trilogy" featured the unlikely hero Drizzt Do'Urden as one of the protagonists, and the 1990-1991 followup "The Dark Elf Trilogy" focused on Drizzt and the drow of the Forgotten Realms setting. Salvatore continued the story of Drizzt and the drow in his subsequent series "Legacy of the Drow" (1992–1996), "Paths of Darkness" (1998–2001), and "The Hunter's Blades Trilogy" (2002–2004). Other works continuing the story of the drow in the Forgotten Realms include Elaine Cunningham's "Starlight and Shadows" series (1995–1996, 2003), the "War of the Spider Queen" series (2002–2005, various authors), and Lisa Smedman's "The Lady Penitent" series (2007–2008).
Keith Baker's "The Dreaming Dark" trilogy (2005–2006), overseen by R.A. Salvatore, featured the story of the drow in Baker's world of Eberron.
Advanced Dungeons & Dragons 2nd edition (1989-1999).
The drow appear first for this edition in the "Monstrous Compendium Volume Two" (1989), which expands the information on drow society. Also included in the entry for drow is a description and statistics for the drider. This entry is reprinted with some minor modifications in the "Monstrous Manual" (1993).
Drow society, religion, history, magic, craftwork, and language for the Forgotten Realms campaign setting is detailed significantly in "The Drow of the Underdark" (1991), by Ed Greenwood. Greenwood appears in the book's introduction as a narrator, explaining how he came across the information in the book: a discussion with Elminster, and chance encounter with a former apprentice of Elminster - the drow lady, Susprina Arkhenneld - as the two explain the drow of the world to the narrator.
The drow are presented as a player character race for 2nd edition in "The Complete Book of Elves" (1992). Drow deities Lolth, Kiaransalee, Vhaeraun, and Zinzerena are described in "Monster Mythology" (1992). The drow are later presented as a playable character race again in ' (1995).
Dungeons & Dragons 3rd edition (2000-2007).
The drow appears in the "Monster Manual" for this edition (2000).
The drow of the Forgotten Realms setting appear in the hardcover "Forgotten Realms Campaign Setting" (2001), and in "Races of Faerûn" (2003).
The drow also appears in the revised "Monster Manual" for this edition (2003).
The "Underdark" hardcover for the Forgotten Realms setting (2003) features the drow yet again as a player character race, as does the "Player's Guide to Faerûn" (2004). "Lost Empires of Faerûn" describes the drow werebat (2005).
The drow paragon 3-level prestige class appears in "Unearthed Arcana" (2004).
The umbragen for the Eberron campaign setting appeared as a player character race in "Dragon" #330 (April 2005).
The arcane guard drow, the dark sniper drow, the drow priestess, the Lolth's sting, and the Lolth-touched drow ranger appear in "Monster Manual IV" (2006). The deepwyrm drow is presented as a player character race in "Dragon Magic" (2006).
The drow are presented as a player character race for 3rd edition in "Expedition to the Demonweb Pits" (2007) and "Drow of the Underdark" (2007). "Drow of the Underdark" also features the arcane guards, the drow assassin, the house captain, the house wizard, the drow inquisitor, the favored consort, the arcane guard, the drow priestess, the drow slaver, the spider sentinel, the albino drow (szarkai), the szarkai fighters, the szarkai druids, and the drow warrior, along with numerous prestige classes and other monsters related to drow.
Open gaming.
The release of the Open Game License and the System Reference Document's inclusion of the drow race also led to a number of books related to drow being published by companies not affiliated with Wizards of the Coast or TSR, such as "The Quintessential Drow", "The Complete Guide to Drow", and "Encyclopaedia Arcane: Drow Magic".
Dungeons & Dragons 4th edition (2008-2014).
The drow appear in the "Monster Manual" for this edition (2008), including the drow warrior, the drow archanomancer, the drow blademaster, and the drow priest.
The drow appear as a playable race in the "Forgotten Realms Player's Guide".
The drow feature in a pre-written playable module called "Demon Queen's Enclave" which takes adventurers from levels 14 through 17 into the Underdark to battle the forces of Orcus and possibly ally with members of the treacherous dark elves and/or their minions.
Description.
The drow made their first statistical appearance in "Hall of the Fire Giant King" in the Hellfurnace Mountains of the Dungeons & Dragons World of Greyhawk campaign setting at the end of the module, and received a lengthy writeup. The history of the drow within the game is revealed; in ages past, the elves were torn by discord and warfare, driving out from their surface lands their selfish and cruel members, who sought safety in the underworld. These creatures, later known as the "dark elvenfolk" or drow, grew strong in the arcane arts over the centuries and content with their gloomy fairyland beneath the earth, though they still bear enmity towards and seek revenge against their distant kin, the elves and faeries who drove them down. They are described as chaotic evil in alignment, and highly intelligent. They are described as black-skinned and pale haired in appearance, around 5-feet tall and slight of build with somewhat sharp features, with large eyes and large pointed ears. Their equipment (magical boots and cloaks, and fine mesh armor similar to chainmail) is black in color and described as being empowered by exposure to the strange radiations of the Drow homeland, losing this power and eventually falling apart when exposed to direct sunlight and kept from the radiation for too long. Females are inherently more powerful than males, and only females may be clerics or fighter/clerics; male drow are commonly fighters, magic-users, or both classes at once. Drow move silently and with a graceful quickness, even when wearing their armor, and blend into shadows with ease. They carry long daggers and short swords of an adamantite alloy and small one-handed crossbows which shoot darts carrying a poison that causes unconsciousness. Drow are difficult to surprise as they are able to see very well in the dark, have an intuitive sense about their underground world similar to that of dwarves, and can detect hidden or secret doors as easily as other elves do. Drow are highly resistant to magic, while all drow have the ability to use some inherent magical abilities even if they are not strictly spellcasters. The module also reveals that there are rumors of vast caverns housing whole cities of drow which exist somewhere deep beneath the earth, and now that the drow have dwelled in these dark labyrinthe places they dislike daylight and other forms of bright light as it hampers their abilities. They are able to communicate using a silent language composed of hand movements, and when coupled with facial and body expression, movement, and posture, this form of communication is the equal of any spoken language.
The "Advanced Dungeons & Dragons" game's second edition product "Monstrous Compendium Volume Two" describes the world of the drow, where violent conflict is part of everyday life, so much so that most drow encountered are ready for a fight. Their inherent magic use comes from training in magic, which all drow receive. Not long after the creation of the elves, they were torn into rival factions, one evil and one good; after a great civil war, those who followed the path of evil and chaos were driven far from the world's forests and into the bleak, lightless caverns and tunnels of the underworld. Most creatures who live on the surface have never met a drow, but those who have seen a drow city report nightmarish buildings constructed of stone and minerals, carved into weird, fantastic shapes. Drow society is fragmented into opposing noble houses and merchant families, and they base their rigid class system on the belief that the strongest should rule. Female drow tend to fill many positions of great importance, with priests of the dark goddess Lolth holding a very high place in society. Drow fighters are required to go through rigorous training in their youth, and those who fail are put to death. Drow use giant lizards as pack animals, use bugbears and troglodytes as servants, and have alliances with many of the underworld's evil inhabitants such as mind flayers. Drow constantly war with other underground neighbors such as dwarves and dark gnomes (svirfneblin), and keep slaves of all types - including allies who fail to live up to drow expectations. "The Complete Book of Elves" by Colin McComb focuses some of its attention on the drow. The "Elfwar" is presented, an elven myth in which the elves were one people until the Spider Queen Lolth used the dissent among the elves to gain a foothold; the elves of Lolth took the name Drow to signify their new allegiance, but as they massed to conquer the other elves, Corellon Larethian and his followers drove Lolth and her people deep into the earth, where they chose to remain. The dark elves who became the drow were originally simply elves who held more with the tenets of might than those of justice, and as they quested for power they became corrupted and turned against their fairer brethren. Dark elves rarely produce half-elves, as they would slaughter any fool who trusts them. Drow infravision is described as so intense that their eyes actually radiate heat; therefore, a character viewing a drow through infravision would see two burning eyes atop a normally glowing torso. Any elf character of good or neutral alignment, even drow, is allowed into the realm of Arvanaith where elves go upon reaching old age. The book notes that drow player characters have a large number of benefits while suffering few disadvantages, but that "the major disadvantage to being a drow "is" being a drow." Drow characters are extraordinarily dexterous and intelligent, but have the typically low elf constitution; also, their personalities are described as grating at best, and all other elves "hate" the drow which affects their reactions to a drow character.
Forgotten Realms.
1991's "The Drow of the Underdark", a 128-page sourcebook all about the drow, expanded the drow significantly for the Advanced Dungeons & Dragons second edition version of the Forgotten Realms setting. The first chapter explains "The Nature of Dark Elves", augmenting the information in the "Monstrous Compendium" entry. It describes their variable physical builds, their alert and inquisitive intelligence, their highly developed senses, the personal magic that all drow are trained in; it also details drow wizards (the most dangerous drow likely to be encountered outside the Underdark), as well as the driders, misfit drow who have failed a test of Lolth. "Dark Elven Society" is detailed in the second chapter. Drow society, being strongly matriarchal, allows the females to hold all positions of power in the government, and to choose and discard mates freely. Social station is the most important thing in drow society, making ascension to greater power a drow's ultimate goal. Drow have a strong affinity for arachnids, as most worship the spider goddess Lolth, and spiders dwell freely among drow communities. The third chapter details "Drow Religion" in the Forgotten Realms setting; as the majority of drow worship Lolth (or "Lloth", formerly known as Araushnee in ancient times), they simply don't speak of or recognize those who do not. Drow deities in this world include Eilistraee, the "Dark Maiden", the goddess of good-aligned drow, and of song, dance, swordwork, and hunting; Ghaunadaur, That Which Lurks (also known as "The Elder Elemental Eye"), a tentacled dark purple blob served by ropers and patron of oozes and all things subterranean; Lolth; and Vhaeraun, the god of thievery and the patron god of drow males in opposition to the matriarchy of drow society. "The High History of the Drow" in the Realms is revealed in the fourth chapter, detailing the descent of the Ilythiiri (the original "Dark Elves") of the southern jungles into the underground, and their dark wars as they became the drow they are today. The fifth chapter details 27 new "Drow Spells" for both wizards and priests; the sixth chapter includes dozens of "Drow Magical Items", some of which previously appeared in first edition "AD&D" sources; the seventh chapter details "Drow Craftwork", discussing their unique clothing and weaponry, their poison, as well as mining and engineering, and drow artisans. The eighth chapter briefly describes "Drow Language", while the ninth chapter goes into "Drow Nomenclature" by providing example female and male given names and drow house names, and the tenth chapter provides "A Selected Glossary of Deep Drow". "Dark Elven Symbols" are described in the eleventh chapter, including drow runes such as way-marker runes, sacred glyphs, house defense glyphs; the twelfth chapter, "The Spider and the Axe: War in the Depths" details an "ideal longterm camapaign setting" involving a war between drow and dwarves; the thirteenth chapter provides a brief look at "The Underdark", but advises readers to see the "Queen of the Spiders" series and the "Dwarves Deep" sourcebook for further information. The fourteenth chapter provides game statistics for several "Monsters of the Underdark" that associate with drow, or compete with them, including the deep dragon, the myrlochar, the deep rothe, the yochlol, and several species of spider and spider-like creatures. According to "The Complete Book of Elves", drow are not welcome in Evermeet and are turned away.
Abilities.
With the ability to resist magic and powerful darkvision, drow are more powerful than many of "Dungeons & Dragons"‍ '​s races. Drow are naturally resistant to magic. They also possess darkvision superior to most other supernatural races. Drow have the ability to summon globes of darkness, outline targets in faerie fire which causes no harm but makes the target brightly visible to everyone who sees them, and create magical balls of light. They can also levitate for short periods of time. Female Drow are naturally inclined to priestly magic and males are naturally inclined towards arcane magic. Like other elves, they are more dexterous than humans, but have a weaker constitution. They live to extraordinarily long ages if not killed by violence first, over a thousand years in some cases. Their hearing and vision are better than that of a human being and they are difficult to sneak up on because of this. They also naturally excel at moving silently.
Reception.
The drow originally created by Gary Gygax are now "essentially the drow of fantasy fiction today", according to Ed Greenwood, who believes them to be "arguably Gary Gygax's greatest, most influential fantasy creation" after the D&D game itself. Designer James Jacobs considers the drow to be a rare example of a D&D-invented monster becoming mainstream, with even non-gamers recognizing them.
Drow have been proven to draw additional sales of products which feature them. While Paizo Publishing was printing "Dragon" and "Dungeon", covers featuring drow often sold better than other issues in the same year.
The drow, especially when used as player characters, are surrounded by much controversy, especially after the release of R. A. Salvatore's novel, "The Crystal Shard". Game designer James Jacobs has said that the drow player characters often spark arguments, with some players refusing to play in a campaign that allows drow PCs. Jacobs says that "even the name" is controversial, having at least two pronunciations.
Ecology.
Environment.
Within the context of the "Dungeons & Dragons" game, the drow were forced underground in what is now known as the Underdark after the great war amongst the elves, a vast system of caverns and tunnels spanning much of the continent. The drow live in city-states in the Underdark, becoming one of the most powerful races therein.
The drow are well adapted to seeing in the dark, and they loathe, are terrified of, and are easily blinded by the light of the surface. Some magic weapons, armor, and various other items of the drow disintegrate on contact with sunlight.
Typical physical characteristics.
Drow characters are extremely intelligent, charismatic and dexterous, but share surface elves' comparative frailty and slight frames. Females tend to be bigger and stronger than males. Drow are characterized by white or silver hair and obsidian black skin. Their eyes are red (or rarely gray, violet, or yellow) in darkness and can be many different colors in normal light. Drow have several kinds of innate spell powers and spell resistance. This is balanced by their weakness in daylight. Also, drow weapons and armor (usually made of adamantite or another metal unique to the Underdark) slowly lose their magical properties if exposed to the sun. In "Advanced Dungeons & Dragons" second edition, adamantite disintegrates upon exposure to sunlight unless specifically treated. Drow also employ the unusual hand crossbow, firing small, though very lethal, darts. Half-drow are the result of crossbreeding between another race and a drow, and share characteristics of both. (The term "half-drow" usually refers to one who is half drow and half human.) Half-drow are also generally evil; however, half-drow of differing alignments are more common than non-evil full drow.
Drow males are commonly wizards or fighters. Females are almost always clerics and almost never wizards.
Alignment.
As a race, drow are usually evil. Exceptions exist, the most notable being Drizzt Do'Urden, Jarlaxle Baenre, and Liriel Baenre, but these are highly unusual. (Note that even Liriel Baenre was arguably of evil alignment for the first portion of her life, only shifting to a good alignment after close relationships with several good-aligned characters.) Originally, drow were chaotic evil in alignment. Beginning with 3rd edition D&D, drow are usually neutral evil. There have been encounters with non-evil drow, but these are distrusted as much as their brethren, due to their reputation. The Drow followers of Eilistraee are the largest group of good Drow, as Eilistraee is the patron goddess of all Drow that have a good alignment.
Society.
Drow society is primarily matriarchal, with priestesses of their evil spider goddess Lolth (sometimes spelled "Lloth") in the highest seats of power. However in the original world of Greyhawk campaign setting created by Gary Gygax, Drow rank structure was based much more on personal experience level and proven personal abilities rather than on gender. Males were just as likely to have positions of authority over both males and females, and the tradition of Matriarchy, where the highest-ranking member was always a female, was not a special directive of the Demon Queen Lolth. The vast majority of Drow Elves both male and female in the original campaign setting of Greyhawk have no authority or ranking at all and live an idle and degenerate life in the great city of the Drow.
Drow society is based upon violence, murder, cunning, and the philosophy that only the strong survive (though in Drow tongue, a quirk of the language creates a reversal of cause-and-effect; more correctly, it can be translated as "those who survive are strong"). Hence, most Drow plot endlessly to murder or otherwise incapacitate their rivals and enemy Drow using deceit and betrayal. Drow, particularly in higher positions, are constantly wary of assassins and the like. One of the quirks of this constant infighting is the relatively short lifespan of the average Drow. While being just as long lived as their surface cousins, living as long as a thousand years; you are very unlikely to meet an elderly Drow. Consequently, they are the only race of Elves that matches the fertility of 'lesser' races, such as humans. Their society, as a whole, is seemingly nonviable. The only reason they do not murder themselves to extinction is by the will of Lolth, working primarily through her clergy. Lolth does not tolerate any Drow that threaten to bring down her society, and the clergy make certain that perpetrators cease their destructive actions by either threatening or killing them.
There are exceptions to the rule, of course. Some communities of drow worship other gods (like Vhaeraun or Eilistraee), and thus, their hierarchy changes, reverses the roles of males and females, or (such as in the case of Eilastree) even approaching something like a workable, progressive society.
Most drow societies hate surface elves, but will wage war with almost any surface race and other subterranean races, such as mind flayers, svirfneblin, duergar, kuo-toa, dwarves, and orcs, for spoils and territory.
Drow in various campaign settings.
Drow in "Eberron".
Inhabiting the jungles and Underdark in the continental isle of Xen'drik, the drow in Eberron have a much more tribalistic culture than their other "Dungeons & Dragons" counterparts. They are not an offshoot of the elven race like in many other worlds but rather a separate, if similar, race. Instead of the spider goddess Lolth, most tribes worship a male scorpion deity known as Vulkoor, though exceptions are common. It is believed that Vulkoor is actually one of the forms of the Mockery. The tribes are often xenophobic, and the social structure varies from tribe to tribe. It is known that the drow mastered elemental binding before gnomes did- including a cultural group of fire-elemental binders called the Sulatar. There is also a subgroup called the "umbragen", or shadow elves, who worship the Mockery in the form of a scorpion god and Khyber or the Umbra, the Consuming Shadow, for whom the umbragen are named; the umbragen dwell underground beneath Xen'drik and are noted for producing many warlocks and soulknives.
Drow in Eberron run the gamut from almost feral in nature to being fully civilized and on par with the cultural level of Khorvaire, varying from tribe to tribe.
Drow in the "Forgotten Realms".
In the "Forgotten Realms", the dark elves were once ancient tribes of Ilythiir and Miyeritar. They were transformed into drow by the Seldarine and were cast down and driven underground by the light-skinned elves because of the Ilythiirian's savagery during the Crown Wars. The drow had fallen under the influence of Araushnee, who was transformed into Lolth and was cast down into the Demonweb Pits along with her son Vhaeraun by the elven god Corellon Larethian because of Lolth's and Vhaeraun's attempt to take control of the elven pantheon (which included Araushnee's seduction of Corellon Larethian).
Prior to the Spellplague descendants of the Miyeritar dark elves later succeed in reversing their transformation and are recreated as a distinct dark elf race.
The largest drow civilization is the subterranean city of Llurth Dreier (population 400,000). However, Menzoberranzan is featured most prominently in the novels.
Previously drow could also worship Ghaunadaur, Kiaransalee, Selvetarm or Vhaeraun. A special case is Eilistraee, the only drow goddess who is chaotic good instead of chaotic evil; she wants the drow to return to the light. All of these alternative deities (except perhaps Ghaunadaur) were however killed or forgotten in the last years before the Spellplague.
Amongst the most infamous of drow are the members of House Baenre, whilst Abeir-Toril is also home to some famous benevolent drow including Drizzt Do'Urden and his deceased father Zaknafein (both of House Do' Urden), Liriel Baenre (formerly of Menzoberranzan's aforementioned House Baenre), and Qilué of the Seven Sisters. The drow Jarlaxle is also well-known, as he is one of the few males in Menzoberranzan to obtain a position of great power. He is the founder and leader of the mercenary band Bregan D'aerthe. These characters are from "The Dark Elf Trilogy" (1990–1991), a series of books by R. A. Salvatore (except for Liriel Baenre and Qilue). The six drow in the "War of the Spider Queen" series have also gained some renown since the novels have been published. The drow also have a long-standing, mutual racial hatred with the gloamings, a rare race found in the pages of the "Underdark" supplement.
Drow in "Greyhawk".
In the world of "Greyhawk", the drow were driven underground by their surface-dwelling relatives because of ideological differences. There they eventually adapted to their surroundings, especially by attracting the attention of the goddess Lolth, "Queen of Spiders". The center of drow civilization is the subterranean city Erelhei-Cinlu, and its surrounding Vault, commonly called the Vault of the Drow.
Known drow of Greyhawk include Clannair Blackshadow, Derken Gale, Jawal Severnain, and Landis Bree of Greyhawk City; Eclavdra of House Eilserv; and Edralve of the Slave Lords.
Some drow, especially of the House of Eilserv, worship a nameless Elder Elemental God (said to have ties to Tharizdun) instead of Lolth.
Drow in other campaign settings.
Different campaign settings portray drow in various ways.
In the "Dragonlance" setting, Drow do not exist; rather, "dark elves" are elves who have been cast out by the other elves for various crimes, such as worship of the evil deities. Dalamar, a student of Raistlin Majere, is the most notable of Krynn's dark elves. However, over the years Drow have accidentally appeared in a few "Dragonlance" modules and novels. Similar mistakes have occurred with other standard AD&D races, such as orcs and lycanthropes, which are not part of the Dragonlance setting. Some theories say that these rare Drow may have accidentally been sent there during a plane shifting spell or related magic, a misfire as like as not that is corrected before the respective timelines are tampered with too drastically.
In the "Mystara" / "Known World" setting, shadow elves are a race of subterranean elves who have been mutated via magic. Aside from living underground, they have nothing in common with Drow and are not known as Dark elves.
In Mongoose Publishing's "Drow War" trilogy, the drow are recast as lawful evil villains and likened to the Nazis. The author of the series has stated that this was a deliberate reaction to the prevalence of renegade, non-evil drow characters.
Drow appear as a playable race in Urban Arcana, which is a d20 Modern setting based on "Dungeons & Dragons". They are shown as very fashionable, often setting new trends. The symbol for most drow is a spider, and they often take the mage or acolyte classes.
A supplement book about the drow was produced by Green Ronin Publishing called "Plot & Poison: A Guidebook to the Drow" in 2002 and is based on the d20 System. It introduces several drow subtypes including aquatic drow and vupdrax (or winged drow) plus fleshes out drow life, such as how they treat slaves of the various fantasy types like elves and humans. Wizards of the Coast, seeing the heavy sales of the GRP supplement, released their own supplement book called "Drow of the Underdark" in May 2007.
Drow in the "Pathfinder Chronicles Campaign Setting" used to be elves but stayed on Golarion when the other elves left the world. Over time, the remaining elves turned into drow by powerful magic, and at this time any elf who is evil enough can spontaneously turn into a drow. The existence of drow in Golarion is virtually unknown to non-elves. Drow are also the main antagonists in the Second Darkness Adventure Path.
Related creatures.
Like elves, drow have other creatures associated with them either by environment or by blood. The drider is one of the most often cited examples, but it is not the only one.
Draegloths.
Draegloths are half-demon, half drow monstrosities. Found in any campaign setting, they are particularly numerous in the "Forgotten Realms". They are created by the unholy union between an ascending high priestess of the drow goddess Lolth and a glabrezu.
Draegloths are about ten feet tall and have four arms, the upper pair being much larger than the lower. They have large claws on the upper arms and they use them for hand-to-hand combat, for they usually prefer the feeling of tearing flesh and sinew under their claws and fangs. Their face is stretched so it resembles that of a dog. Their flesh is as dark as a drow's, and they are covered in a fine coat of fur; they also have a white mane. They are sacred creatures to the Lolthites and are usually treated with respect.
Triel Baenre of Menzoberranzan, in the "Forgotten Realms", had a draegloth son, Jeggred.
V3.5 statistics for the draegloth can be found in "Drow of the Underdark".

</doc>
<doc id="46936" url="http://en.wikipedia.org/wiki?curid=46936" title="Angband">
Angband

In J. R. R. Tolkien's fictional world of Middle-earth, Angband (Sindarin for 'iron prison') is the name of the fortress of Melkor, constructed before the First Age, located in the Iron Mountains in the enemy's land Dor Daedeloth north of Beleriand.
The fortress is described in Tolkien's "The Silmarillion". It was built by Melkor (later called "Morgoth") to guard against a possible attack from Aman by the Valar. Nonetheless, the Valars' attack succeeded in capturing Morgoth and destroying his main stronghold Utumno. 
However, while the Valar had focused on destroying Utumno utterly, Angband, though devastated, was only partially destroyed. Over time, the dark creatures in Morgoth's service would gather in its ruined pits. After three ages of imprisonment, Morgoth returned to Middle-earth and set himself up in Angband, raising the volcanic Thangorodrim over the fortress as protection. He seldom came out of it again, but did when challenged to single combat by the Elven king Fingolfin and earlier to investigate the first appearance of Men. He reigned there until the end of the First Age, when it was destroyed in the War of Wrath. In earlier versions of Tolkien's mythology (see "The History of Middle-earth") it was called Angamando, the Quenya form of the name.

</doc>
<doc id="46943" url="http://en.wikipedia.org/wiki?curid=46943" title="Audio time-scale/pitch modification">
Audio time-scale/pitch modification

Time stretching is the process of changing the speed or duration of an audio signal without affecting its pitch.
Pitch scaling or pitch shifting is the opposite: the process of changing the pitch without affecting the speed. Similar methods can change speed, pitch, or both at once, in a time-varying way.
These processes are used, for instance, to match the pitches and tempos of two pre-recorded clips for mixing when the clips cannot be reperformed or resampled. (A drum track containing no pitched instruments could be moderately resampled for tempo without adverse effects, but a pitched track could not). They are also used to create effects such as increasing the range of an instrument (like pitch shifting a guitar down an octave).
Resampling.
The simplest way to change the duration or pitch of a digital audio clip is to resample it. This is a mathematical operation that effectively rebuilds a continuous waveform from its samples and then samples that waveform again at a different rate. When the new samples are played at the original sampling frequency, the audio clip sounds faster or slower. Unfortunately, the frequencies in the sample are always scaled at the same rate as the speed, transposing its perceived pitch up or down in the process. In other words, slowing down the recording lowers the pitch, speeding it up raises the pitch, and using this method the two effects cannot be separated. This is analogous to speeding up or slowing down an analogue recording, like a phonograph record or tape, creating the Chipmunk effect.
Frequency domain.
Phase vocoder.
One way of stretching the length of a signal without affecting the pitch is to build a phase vocoder after Flanagan, Golden, and Portnoff.
Basic steps:
The phase vocoder handles sinusoid components well, but early implementations introduced considerable smearing on transient ("beat") waveforms at all non-integer compression/expansion rates, which renders the results phasey and diffuse. Recent improvements allow better quality results at all compression/expansion ratios but a residual smearing effect still remains.
The phase vocoder technique can also be used to perform pitch shifting, chorusing, timbre manipulation, harmonizing, and other unusual modifications, all of which can be changed as a function of time.
Sinusoidal spectral modeling.
Another method for time stretching relies on a spectral model of the signal. In this method, peaks are identified in frames using the STFT of the signal, and sinusoidal "tracks" are created by connecting peaks in adjacent frames. The tracks are then re-synthesized at a new time scale. This method can yield good results on both polyphonic and percussive material, especially when the signal is separated into sub-bands. However, this method is more computationally demanding than other methods.
Time domain.
SOLA.
Rabiner and Schafer in 1978 put forth an alternate solution that works in the time domain: attempt to find the period (or equivalently the fundamental frequency) of a given section of the wave using some pitch detection algorithm (commonly the peak of the signal's autocorrelation, or sometimes cepstral processing), and crossfade one period into another.
This is called time-domain harmonic scaling or the synchronized overlap-add method (SOLA) and performs somewhat faster than the phase vocoder on slower machines but fails when the autocorrelation mis-estimates the period of a signal with complicated harmonics (such as orchestral pieces).
Adobe Audition (formerly Cool Edit Pro) seems to solve this by looking for the period closest to a center period that the user specifies, which should be an integer multiple of the tempo, and between 30 Hz and the lowest bass frequency.
This is much more limited in scope than the phase vocoder based processing, but can be made much less processor intensive, for real-time applications. It provides the most coherent results for single-pitched sounds like voice or musically monophonic instrument recordings.
High-end commercial audio processing packages either combine the two techniques (for example by separating the signal into sinusoid and transient waveforms), or use other techniques based on the wavelet transform, or artificial neural network processing, producing the highest-quality time stretching.
Untangling phase and time.
Another way to shift pitch and stretch time is to separate phase and time in a monophonic sound such as the ones of melody instruments.
By altering only the time control, it is possible to stretch, shrink or reverse time, or generate loops as needed in sampling synthesizers.
Time shrinkage can also be used for compression purposes.
By altering only the phase control, the pitch can be shifted, FM synthesis distortions can be applied to an existing sound.
This can be used to play instruments alternatively to wavetable synthesis.
For controlling phase and time independently we would need to know the displacement of the sound for every pair of phase and time position.
This corresponds to a cylinder as shown in the figure.
However, a sound signal is a one-dimensional signal.
This sound signal can be considered as an observation of the full function on the cylinder. This is drawn as black line in the figure.
The full function on the cylinder can be approximated by interpolating between points on the helix with (approximately) the same phase.
From this function a different sound signal can be derived.
E.g. in the figure the grey line shows the path of a sound that has the same time progression but a frequency lower than the original one,
or a sound that has the same frequency and a faster time progression, or something between.
In the end the whole process can be implemented for discrete sound signals as interpolation between values with similar phase and similar time.
The described technique is used in the monophonic version of the software Melodyne
Speed hearing and speed talking.
For the specific case of speech, time stretching can be performed using PSOLA.
Time stretching can be used with audiobooks and recorded lectures.
Slowing down may improve comprehension of foreign languages .
While one might expect speeding up to reduce comprehension,
Herb Friedman says that "Experiments have shown that the brain works most efficiently if the information rate through the ears--via speech--is the "average" reading rate, which is about 200-300 wpm (words per minute), yet the average rate of speech is in the neighborhood of 100-150 wpm."
Speeding up audio is seen as the equivalent of "speed reading"
Time stretching is often used to adjust Radio commercials
 and the audio of Television advertisements to fit exactly into the 30 or 60 seconds available.
Pitch scaling.
These techniques can also be used to transpose an audio sample while holding speed or duration constant. This may be accomplished by time stretching and then resampling back to the original length. Alternatively, the frequency of the sinusoids in a sinusoidal model may be altered directly, and the signal reconstructed at the appropriate time scale.
Transposing can be called frequency scaling or pitch shifting, depending on perspective.
For example, one could move the pitch of every note up by a perfect fifth, keeping the tempo the same.
One can view this transposition as "pitch shifting", "shifting" each note up 7 keys on a piano keyboard, or adding a fixed amount on the Mel scale, or adding a fixed amount in linear pitch space.
One can view the same transposition as "frequency scaling", "scaling" (multiplying) the frequency of every note by 3/2.
Musical transposition preserves the ratios of the harmonic frequencies that determine the sound's timbre, unlike the "frequency shift" performed by amplitude modulation, which adds a fixed frequency offset to the frequency of every note. (In theory one could perform a literal "pitch scaling" in which the musical pitch space location is scaled [a higher note would be shifted at a greater interval in linear pitch space than a lower note], but that is highly unusual, and not musical).
Time domain processing works much better here, as smearing is less noticeable, but scaling vocal samples distorts the formants into a sort of Alvin and the Chipmunks-like effect, which may be desirable or undesirable.
A process that preserves the formants and character of a voice involves analyzing the signal with a channel vocoder or LPC vocoder plus any of several pitch detection algorithms and then resynthesizing it at a different fundamental frequency.
A detailed description of older analog recording techniques for pitch shifting can be found within the Alvin and the Chipmunks entry.

</doc>
<doc id="46944" url="http://en.wikipedia.org/wiki?curid=46944" title="Parallax scrolling">
Parallax scrolling

Parallax scrolling is a technique in computer graphics, where background images move by the camera slower than foreground images, creating an illusion of depth in a 2D scene and adding to the immersion. The technique grew out of the multiplane camera technique used in traditional animation since the 1930s. Parallax scrolling was popularized in 2D computer graphics and video games by the arcade games "Moon Patrol" and "Jungle Hunt", both released in 1982. Some parallax scrolling had earlier been used by the 1981 arcade game "Jump Bug".
Methods.
There are four main methods of parallax scrolling used in titles for arcade system board, video game console and personal computer systems.
Layer method.
Some display systems support multiple background layers that can be scrolled independently in horizontal and vertical directions and composited on one another, simulating a multiplane camera. On such a display system, a game can produce parallax by simply changing each layer's position by a different amount in the same direction. Layers that move more quickly are perceived to be closer to the virtual camera. Layers can be placed in front of the "playfield"—the layer containing the objects with which the player interacts—for various reasons such as to provide increased dimension, obscure some of the action of the game, or distract the player.
Sprite method.
Programmers may also make pseudo-layers of sprites—individually controllable moving objects drawn by hardware on top of or behind the layers—if they are available on the display system. For instance "Star Force", an overhead-view vertically scrolling shooter for NES, used this for its starfield, and "Final Fight" for the Super NES used this technique for the layer immediately in front of the main playfield.
The Amiga computer has sprites which can have any height and can be set horizontal with the copper co-processor, which makes them ideal for this purpose.
Risky Woods on the Amiga uses sprites multiplexed with the copper to create an entire fullscreen parallax background layer as an alternative to the system's dual playfield mode.
Repeating pattern/animation method.
Scrolling displays built up of individual tiles can be made to 'float' over a repeating background layer by animating the individual tiles' bitmaps in order to portray the parallax effect. Color cycling can be used to animate tiles quickly on the whole screen. This software effect gives the illusion of another (hardware) layer. Many games used this technique for a scrolling star-field, but sometimes a more intricate or multi-directional effect is achieved, such as in the game "Parallax" by Sensible Software.
Raster method.
In raster graphics, the lines of pixels in an image are typically composited and refreshed in top-to-bottom order with a slight delay (called the horizontal blanking interval) between drawing one line and drawing the next line.
Games designed for older graphical chipsets—such as those of the third and fourth generations of video game consoles, those of dedicated TV games, or those of similar handheld systems—take advantage of the raster characteristics to create the illusion of more layers.
Some display systems have only one layer. These include most of the classic 8-bit systems (such as the Commodore 64, Nintendo Entertainment System, Sega Master System, PC Engine/TurboGrafx-16 and original Game Boy). The more sophisticated games on such systems generally divide the layer into horizontal strips, each with a different position and rate of scrolling. Typically, strips higher on the screen will represent things farther away from the virtual camera or one strip will be held stationary to display status information. The program will then wait for horizontal blank and change the layer's scroll position just before the display system begins to draw each scanline. This is called a "raster effect" and is also useful for changing the system palette to provide a gradient background.
Some platforms (such as the Commodore 64, Amiga, Sega Master System, PC Engine/TurboGrafx-16, Sega Mega Drive/Genesis, Super NES, Game Boy, Game Boy Advance and Nintendo DS) provide a horizontal blank interrupt for automatically setting the registers independently of the rest of the program. Others, such as the NES, require the use of cycle-timed code, which is specially written to take exactly as long to execute as the video chip takes to draw one scanline, or timers inside game cartridges that generate interrupts after a given number of scanlines have been drawn. Many NES games use this technique to draw their status bars, and "Teenage Mutant Ninja Turtles II: The Arcade Game" and "" for NES use it to scroll background layers at different rates.
More advanced raster techniques can produce interesting effects. A system can achieve a very effective depth of field if layers with rasters are combined; "Sonic the Hedgehog", "Sonic The Hedgehog 2", "ActRaiser", "Lionheart" and "Street Fighter II" used this effect well. If each scanline has its own layer, the "Pole Position" effect is produced, which creates a pseudo-3D road (or a pseudo-3D ball court as in "NBA Jam") on a 2D system.
If the display system supports rotation and scaling in addition to scrolling—an effect popularly known as Mode 7—changing the rotation and scaling factors can draw a projection of a plane (as in "F-Zero" and "Super Mario Kart") or can warp the playfield to create an extra challenge factor.
Another advanced technique is row/column scrolling, where rows/columns of tiles on a screen can be scrolled individually. This technique is implemented in the graphics chips of various Sega arcade system boards since the Sega Space Harrier and System 16, the Sega Mega Drive/Genesis console, and the Capcom CP System, Irem M-92 and Taito F3 System arcade game boards.
Example.
In the following animation, three layers are moving leftward at different speeds. Their speeds decrease from front to back and correspond to increases in relative distance from the viewer. The ground layer is moving 8 times faster than the vegetation layer. The vegetation layer is moving two times faster than the cloud layer.
Parallax scrolling in Web design.
Web designers began incorporating parallax scrolling in 2011, using HTML5 and CSS3. Websites with parallax backgrounds are becoming an increasingly popular strategy, as advocates argue it is a simple way to embrace the fluidity of the Web. Additionally, proponents use parallax backgrounds as a tool to better engage users and improve the overall experience that a website provides. However, a Purdue University study, published in 2013, revealed the following findings: "... although parallax scrolling enhanced certain aspects of the user experience, it did not necessarily improve the overall user experience".

</doc>
<doc id="46945" url="http://en.wikipedia.org/wiki?curid=46945" title="2013">
2013

2013 ()
will be .
2013 was designated as:

</doc>
<doc id="46947" url="http://en.wikipedia.org/wiki?curid=46947" title="Elizabethan era">
Elizabethan era

The Elizabethan era was the epoch in English history of Queen Elizabeth I's reign (1558–1603). Historians often depict it as the golden age in English history. The symbol of Britannia was first used in 1572 and often thereafter to mark the Elizabethan age as a renaissance that inspired national pride through classical ideals, international expansion, and naval triumph over the hated Spanish foe. In terms of the entire century, the historian John Guy (1988) argues that "England was economically healthier, more expansive, and more optimistic under the Tudors" than at any time in a thousand years.
This "golden age" represented the apogee of the English Renaissance and saw the flowering of poetry, music and literature. The era is most famous for theatre, as William Shakespeare and many others composed plays that broke free of England's past style of theatre. It was an age of exploration and expansion abroad, while back at home, the Protestant Reformation became more acceptable to the people, most certainly after the Spanish Armada was repulsed. It was also the end of the period when England was a separate realm before its royal union with Scotland.
The Elizabethan Age is viewed so highly largely because of the periods before and after. It was a brief period of largely internal peace between the English Reformation and the battles between Protestants and Catholics and the battles between parliament and the monarchy that engulfed the seventeenth century. The Protestant/Catholic divide was settled, for a time, by the Elizabethan Religious Settlement, and parliament was not yet strong enough to challenge royal absolutism.
England was also well-off compared to the other nations of Europe. The Italian Renaissance had come to an end under the weight of foreign domination of the peninsula. France was embroiled in its own religious battles that would only be settled in 1598 with the Edict of Nantes. In part because of this, but also because the English had been expelled from their last outposts on the continent, the centuries-long conflict between France and England was largely suspended for most of Elizabeth's reign.
The one great rival was Spain, with which England clashed both in Europe and the Americas in skirmishes that exploded into the Anglo-Spanish War of 1585–1604. An attempt by Philip II of Spain to invade England with the Spanish Armada in 1588 was famously defeated, but the tide of war turned against England with an unsuccessful expedition to Portugal and the Azores, the Drake-Norris Expedition of 1589. Thereafter Spain provided some support for Irish Catholics in a debilitating rebellion against English rule, and Spanish naval and land forces inflicted a series of reversals against English offensives. This drained both the English Exchequer and economy that had been so carefully restored under Elizabeth's prudent guidance. English commercial and territorial expansion would be limited until the signing of the Treaty of London the year following Elizabeth's death.
England during this period had a centralised, well-organised, and effective government, largely a result of the reforms of Henry VII and Henry VIII. Economically, the country began to benefit greatly from the new era of trans-Atlantic trade.
Romance and reality.
The Victorian era and the early 20th century idealised the Elizabethan era. The "Encyclopædia Britannica" maintains that "The long reign of Elizabeth I, 1558–1603, was England's Golden Age...'Merry England,' in love with life, expressed itself in music and literature, in architecture and in adventurous seafaring." This idealising tendency was shared by Britain and an Anglophilic America. In popular culture, the image of those adventurous Elizabethan seafarers was embodied in the films of Errol Flynn.
In response and reaction to this hyperbole, modern historians and biographers have tended to take a more dispassionate view of the Tudor period.
Government.
Elizabethan England was not particularly successful in a military sense during the period, but it avoided major defeats and built up a powerful navy. On balance, it can be said that Elizabeth provided the country with a long period of general if not total peace and generally increasing prosperity. Having inherited a virtually bankrupt state from previous reigns, her frugal policies restored fiscal responsibility. Her fiscal restraint cleared the regime of debt by 1574, and ten years later the Crown enjoyed a surplus of £300,000. Economically, Sir Thomas Gresham's founding of the Royal Exchange (1565), the first stock exchange in England and one of the earliest in Europe, proved to be a development of the first importance, for the economic development of England and soon for the world as a whole. With taxes lower than other European countries of the period, the economy expanded; though the wealth was distributed with wild unevenness, there was clearly more wealth to go around at the end of Elizabeth's reign than at the beginning. This general peace and prosperity allowed the attractive developments that "Golden Age" advocates have stressed.
Plots, intrigues and conspiracies.
The Elizabethan Age was also an age of plots and conspiracies, frequently political in nature and often involving the highest levels of Elizabethan society. High officials in Madrid, Paris and Rome sought to kill Elizabeth, a Protestant, and replace her with Mary, Queen of Scots, a Catholic. That would be a prelude to the religious recovery of England for Catholicism. In 1570, the Ridolfi plot was thwarted. In 1584, the Throckmorton Plot was discovered, after Francis Throckmorton confessed his involvement in a plot to overthrow the Queen and restore the Catholic Church in England. Another major conspiracy was the Babington Plot – the event which most directly led to Mary's execution, the discovery of which involved a double agent Gilbert Gifford acting under the direction of Francis Walsingham, the Queen's highly effective spy master.
The Essex Rebellion of 1601 has a dramatic element as just before the uprising, supporters of the Earl of Essex, among them Charles and Joscelyn Percy (younger brothers of the Earl of Northumberland), paid for a performance of Richard II at the Globe Theatre, apparently with the goal of stirring public ill will towards the monarchy. It was reported at the trial of Essex by Chamberlain's Men actor Augustine Phillips, the conspirators paid the company forty shillings "above the ordinary" (i. e., above their usual rate) to stage the play, which the players felt was too old and "out of use" to attract a large audience.
In the Bye Plot of 1603, two Catholic priests planned to kidnap King James and hold him in the Tower of London until he agreed to be more tolerant towards Catholics. Most dramatic was the 1605 Gunpowder Plot to blow up the House of Lords during the State Opening of Parliament. It was discovered in time with eight conspirators executed, including Guy Fawkes, who became the iconic evil traitor in English lore.
Royal Navy and defeat of the Armada.
While Henry VIII had launched the Royal Navy, Edward and Mary had ignored it and it was little more than a system of coastal defense. Elizabeth made naval strength a high priority. She risked war with Spain by supporting the "Sea Dogs," such as John Hawkins and Francis Drake, who preyed on the Spanish merchant ships carrying gold and silver from the New World. The Navy yards were leaders in technical innovation, and the captains devised new tactics. Parker (1996) argues that the full-rigged ship was one of the greatest technological advances of the century and permanently transformed naval warfare. In 1573 English shipwrights introduced designs, first demonstrated in the "Dreadnaught," that allowed the ships to sail faster and maneuver better and permitted heavier guns. Whereas before warships had tried to grapple with each other so that soldiers could board the enemy ship, now they stood off and fired broadsides that would sink the enemy vessel. When Spain finally decided to invade and conquer England it was a fiasco. Superior English ships and seamanship foiled the invasion and led to the destruction of the Spanish Armada in 1588, marking the high point of Elizabeth's reign. Technically, the Armada failed because Spain's over-complex strategy required coordination between the invasion fleet and the Spanish army on shore. Also, the poor design of the Spanish cannons meant they were much slower in reloading in a close-range battle. Spain and France still had stronger fleets, but England was catching up.
Parker has speculated on the dire consequences if the Spanish had landed their invasion army in 1588. He argues that the Spanish army was larger, more experienced, better-equipped, more confident, and had better financing. The English defenses, on the other hand, were thin and outdated; England had too few soldiers and they were at best only partially trained. Spain had chosen England's weakest link and probably could have captured London in a week. Parker adds that a Catholic uprising in the north and in Ireland could have brought total defeat.
Colonising the New World.
The discoveries of Christopher Columbus electrified all of western Europe, especially maritime powers like England. King Henry VII commissioned John Cabot to lead a voyage to find a northern route to the Spice Islands of Asia; this began the search for the North West Passage. Cabot sailed in 1497 and reached Newfoundland. He led another voyage to the Americas the following year, but nothing was heard of him or his ships again.
In 1562 Elizabeth sent privateers Hawkins and Drake to seize booty from Spanish and Portuguese ships off the coast of West Africa. When the Anglo-Spanish Wars intensified after 1585, Elizabeth approved further raids against Spanish ports in the Americas and against shipping returning to Europe with treasure. Meanwhile, the influential writers Richard Hakluyt and John Dee were beginning to press for the establishment of England's own overseas empire. Spain was well established in the Americas, while Portugal, in union with Spain from 1580, had an ambitious global empire in Africa, Asia and South America. France was exploring the North America. England was stimulated to create its own colonies, with an emphasis on the West Indies rather than in North America.
Martin Frobisher landed at Frobisher Bay on Baffin Island in August 1576; He returned in 1577, claiming it in Queen Elizabeth's name, and in a third voyage tried but failed to found a settlement in Frobisher Bay.
From 1577 to 1580, Sir Francis Drake circumnavigated the globe. Combined with his daring raids against the Spanish and his great victory over them at Cadiz in 1587, he became a famous hero—his exploits are still celebrated—but England did not follow up on his claims. In 1583, Humphrey Gilbert sailed to Newfoundland, taking possession of the harbour of St John's together with all land within two hundred leagues to the north and south of it.
In 1584, the queen granted Sir Walter Raleigh a charter for the colonisation of Virginia; it was named in her honour. Raleigh and Elizabeth sought both immediate riches and a base for privateers to raid the Spanish treasure fleets. Raleigh sent others to found the Roanoke Colony; it remains a mystery why the settlers all disappeared. In 1600, the queen chartered the East India Company. It established trading posts, which in later centuries evolved into British India, on the coasts of what is now India and Bangladesh. Larger scale colonisation began shortly after Elizabeth's death.
Distinctions.
England in this era had some positive aspects that set it apart from contemporaneous continental European societies. Torture was rare, since the English legal system reserved torture only for capital crimes like treason—though forms of corporal punishment, some of them extreme, were practised. The persecution of witches began in 1563, and hundreds were executed, although there was nothing like the frenzy on the Continent Mary had tried her hand at an aggressive anti-Protestant Inquisition and was hated for it; it was not to be repeated.
Religion.
It was an age of intense religious passions, which Elizabeth managed to tone down in contrast to previous and succeeding eras of religious violence.
Elizabeth said "I have no desire to make windows into mens' souls". Her desire to moderate the religious persecutions of previous Tudor reigns — the persecution of Catholics under Edward VI, and of Protestants under Mary I — appears to have had a moderating effect on English society. Elizabeth reinstated the Protestant bible and English Mass, yet for a number of years refrained from persecuting Catholics.
In 1570, Pope Pius V declared Elizabeth a heretic who was not the legitimate Queen and her subjects no longer owed her obedience. The pope sent Jesuits and seminarians to secretly evangelize and support Catholics. After several plots to overthrow her, Catholic clergy were mostly considered to be traitors, and were pursued aggressively in England. Often priests were tortured or executed after capture unless they cooperated with the English authorities. People who publicly supported Catholicism were excluded from the professions; sometimes fined or imprisoned.
Science, technology and exploration.
Lacking a dominant genius or a formal structure for research (the following century had both Sir Isaac Newton and the Royal Society), the Elizabethan era nonetheless saw significant scientific progress. The astronomers Thomas Digges and Thomas Harriot made important contributions; William Gilbert published his seminal study of magnetism, "De Magnete," in 1600. Substantial advancements were made in the fields of cartography and surveying. The eccentric but influential John Dee also merits mention.
Much of this scientific and technological progress related to the practical skill of navigation. English achievements in exploration were noteworthy in the Elizabethan era. Sir Francis Drake circumnavigated the globe between 1577 and 1581, and Martin Frobisher explored the Arctic. The first attempt at English settlement of the eastern seaboard of North America occurred in this era—the abortive colony at Roanoke Island in 1587.
While Elizabethan England is not thought of as an age of technological innovation, some progress did occur. In 1564 Guilliam Boonen came from the Netherlands to be Queen Elizabeth's first coach-builder —thus introducing the new European invention of the spring-suspension coach to England, as a replacement for the litters and carts of an earlier transportation mode. Coaches quickly became as fashionable as sports cars in a later century; social critics, especially Puritan commentators, noted the "diverse great ladies" who rode "up and down the countryside" in their new coaches.
Education.
Education would begin at home, where children were taught the basic etiquette of proper manners and respecting others. It was necessary for boys to attend grammar school, but girls were rarely allowed in any place of education other than petty schools, and then only with a restricted curriculum. Petty schools were for all children aged from 5 to 7 years of age. Only the most wealthy people allowed their daughters to be taught, and only at home. During this time, endowed schooling became available. This meant that even boys of very poor families were able to attend school if they were not needed to work at home, but only in a few localities were funds available to provide support as well as the necessary education scholarship. Boys from families of nobility would often be taught at home by a private tutor.
Gender.
While the Tudor era presents an abundance of material on the women of the nobility—especially royal wives and queens—historians have recovered scant documentation about the average lives of women. There has, however, been extensive statistical analysis of demographic and population data which includes women, especially in their childbearing roles.
The role of women in society was, for the historical era, relatively unconstrained; Spanish and Italian visitors to England commented regularly, and sometimes caustically, on the freedom that women enjoyed in England, in contrast to their home cultures. England had more well-educated upper class women than was common anywhere in Europe.
The Queen's marital status was a major political and diplomatic topic. It also entered into the popular culture. Elizabeth's unmarried status inspired a cult of virginity. In poetry and portraiture, she was depicted as a virgin or a goddess or both, not as a normal woman. Elizabeth made a virtue of her virginity: in 1559, she told the Commons, "And, in the end, this shall be for me sufficient, that a marble stone shall declare that a queen, having reigned such a time, lived and died a virgin". Public tributes to the Virgin by 1578 acted as a coded assertion of opposition to the queen's marriage negotiations with the Duc d'Alençon.
In contrast to her father's emphasis on masculinity and physical prowess, Elizabeth emphasized the maternalism theme, saying often that she was married to her kingdom and subjects. She explained "I keep the good will of all my husbands — my good people — for if they did not rest assured of some special love towards them, they would not readily yield me such good obedience," and promised in 1563 they would never have a more natural mother than she. Coch (1996) argues that her figurative motherhood played a central role in her complex self-representation, shaping and legitimating the personal rule of a divinely appointed female prince.
Marriage.
Over ninety percent of English women (and adults, in general) entered marriage at the end of the 1500s and beginning of the 1600s, at an average age of about 25–26 years for the bride and 27–28 years for the groom. Among the nobility and gentry, the average was around 19-21 for brides and 24-26 for grooms. Many city and townswomen married for the first time in their thirties and forties and it was not unusual for orphaned young women to delay marriage until the late twenties or early thirties to help support their younger siblings, and roughly a fourth of all English brides were pregnant at their weddings.
Food.
England's food supply was plentiful throughout most of the reign; there were no famines. Bad harvests caused distress, but they were usually localized. The most widespread came in 1555–57 and 1596–98. In the towns the price of staples was fixed by law; in hard times the size of the loaf of bread sold by the baker was smaller.
The poor consumed a diet largely of bread, cheese, milk, and beer, with small portions of meat, fish and vegetables, and occasionally some fruit. Potatoes were just arriving at the end of the period, and became increasingly important. The typical poor farmer sold his best products on the market, keeping the cheap food for the family. Stale bread could be used to make bread puddings, and bread crumbs served to thicken soups, stews, and sauces. At a somewhat higher social level families ate an enormous variety of meats, especially beef, mutton, veal, lamb, and pork, as well as chickens, and ducks. The holiday goose was a special treat. Many rural folk and some townspeople tended a small garden which produced vegetables such as asparagus, cucumbers, spinach, lettuce, beans, cabbage, carrots, leeks, and peas, as well as medicinal and flavoring herbs. Some grew their own apricots, grapes, berries, apples, pears, plums, currants, and cherries. Families without a garden could trade with their neighbors to obtain vegetables and fruits at low cost.
England was exposed to new foods (such as the potato imported from South America), and developed new tastes during the era. The more prosperous enjoyed a wide variety of food and drink, including exotic new drinks such as tea, coffee, and chocolate. French and Italian chefs appeared in the country houses and palaces bringing new standards of food preparation and taste. For example, the English developed a taste for acidic foods—such as oranges for the upper class—and started to use vinegar heavily. The gentry paid increasing attention to their gardens, with new fruits, vegetables and herbs; pasta, pastries, and dried mustard balls first appeared on the table. The apricot was a special treat at fancy banquets. Roast beef remained a staple for those who could afford it. The rest ate a great deal of bread and fish. Every class had a taste for beer and rum.
At the rich end of the scale the manor houses and palaces were awash with large, elaborately prepared meals, usually for many people and often accompanied by entertainment. The upper classes often celebrated religious festivals, weddings, alliances and the whims of the king or queen. Feasts were commonly used to commemorate the "procession" of the crowned heads of state in the summer months, when the king or queen would travel through a circuit of other nobles' lands both to avoid the plague season of London, and alleviate the royal coffers, often drained through the winter to provide for the needs of the royal family and court. This would include a few days or even a week of feasting in each noble's home, who depending on his or her production and display of fashion, generosity and entertainment, could have his way made in court and elevate his or her status for months or even years.
Special courses after a feast or dinner which often involved a special room or outdoor gazebo (sometimes known as a folly) with a central table set with dainties of "medicinal" value to help with digestion. These would include wafers, comfits of sugar-spun anise or other spices, jellies and marmalades (a firmer variety than we are used to, these would be more similar to our gelatin jigglers), candied fruits, spiced nuts and other such niceties. These would be eaten while standing and drinking warm, spiced wines (known as hypocras) or other drinks known to aid in digestion. One must remember that sugar in the Middle Ages or Early Modern Period was often considered medicinal, and used heavily in such things. This was not a course of pleasure, though it could be as everything was a treat, but one of healthful eating and abetting the digestive capabilities of the body. It also, of course, allowed those standing to show off their gorgeous new clothes and the holders of the dinner and banquet to show off the wealth of their estate, what with having a special room just for banqueting.
High culture.
Theatre.
With William Shakespeare at his peak, as well as Christopher Marlowe and many other playwrights, actors and theatres constantly busy, the high culture of the Elizabethan Renaissance was best expressed in its theatre. Historical topics were especially popular, not to mention the usual comedies and tragedies.
Music.
Travelling musicians were in great demand at Court, in churches, at country houses, and at local festivals. Important composers included William Byrd (1543–1623), John Dowland (1563–1626) Thomas Campion (1567–1620), and Robert Johnson (c. 1583–c. 1634). The composers were commissioned by church and Court, and deployed two main styles, madrigal and ayre. The popular culture showed a strong interest in folk songs and ballads (folk songs that tell a story). It became the fashion in the late 19th century to collect and sing the old songs.
Fine arts.
It has often been said that the Renaissance came late to England, in contrast to Italy and the other states of continental Europe; the fine arts in England during the Tudor and Stuart eras were dominated by foreign and imported talent—from Hans Holbein the Younger under Henry VIII to Anthony van Dyck under Charles I. Yet within this general trend, a native school of painting was developing. In Elizabeth's reign, Nicholas Hilliard, the Queen's "limner and goldsmith," is the most widely recognized figure in this native development; but George Gower has begun to attract greater notice and appreciation as knowledge of him and his art and career has improved.
Popular culture.
Sports and entertainment.
There were many different types of Elizabethan sports and entertainment:
Festivals, holidays and celebrations.
During the Elizabethan era, people looked forward to holidays because opportunities for leisure were limited, with time away from hard work being restricted to periods after church on Sundays. For the most part, leisure and festivities took place on a public church holy day. Every month had its own holiday, some of which are listed below:
References.
Notes
Bibliography

</doc>
<doc id="46950" url="http://en.wikipedia.org/wiki?curid=46950" title="Libretto">
Libretto

A libretto is the text used in, or intended for, an extended musical work such as an opera, operetta, masque, oratorio, cantata or musical. The term, "libretto" is also sometimes used to refer to the text of major liturgical works, such as the Mass, requiem and sacred cantata, or the story line of a ballet.
"Libretto" (pl. "libretti"), from Italian, is the diminutive of the word "libro" (book). A libretto is distinct from a synopsis or scenario of the plot, in that the libretto contains all the words and stage directions, while a synopsis summarizes the plot. Some ballet historians also use the word "libretto" to refer to the 15–40 page books which were on sale to 19th century ballet audiences in Paris and contained a very detailed description of the ballet's story, scene by scene.
The relationship of the librettist (that is, the writer of a libretto) to the composer in the creation of a musical work has varied over the centuries, as have the sources and the writing techniques employed.
In the context of a modern English language musical theatre piece, the libretto is often referred to as the book of the work, though this usage typically excludes sung lyrics.
Relationship of composer and librettist.
Libretti for operas, oratorios and cantatas in the 17th and 18th centuries generally were written by someone other than the composer, often a well-known poet. Metastasio (1698–1782) (real name Pietro Trapassi) was one of the most highly regarded librettists in Europe. His libretti were set many times by many different composers. Another noted 18th century librettist was Lorenzo da Ponte, who wrote the libretti for three of Mozart's greatest operas, as well as for many other composers. 
Eugène Scribe was one of the most prolific librettists of the 19th century, providing the words for works by Meyerbeer (with whom he had a lasting collaboration), Auber, Bellini, Donizetti, Rossini and Verdi. The French writers' duo Henri Meilhac and Ludovic Halévy wrote a large number of opera and operetta libretti for the likes of Jacques Offenbach, Jules Massenet and Georges Bizet. Arrigo Boito, who wrote libretti for, among others, Giuseppe Verdi and Amilcare Ponchielli, also composed two operas of his own.
The libretto is not always written before the music. Some composers, such as Mikhail Glinka, Alexander Serov, Rimsky-Korsakov, Puccini and Mascagni wrote passages of music without text and subsequently had the librettist add words to the vocal melody lines. (This has often been the case with American popular song and musicals in the 20th century, as with Richard Rodgers and Lorenz Hart's collaboration, although with the later team of Rodgers and Hammerstein the lyrics were generally written first.)
Some composers wrote their own libretti. Richard Wagner is perhaps most famous in this regard, with his transformations of Germanic legends and events into epic subjects for his operas and music dramas. Hector Berlioz, too, wrote the libretti for two of his best-known works, "La Damnation de Faust" and "Les Troyens". Alban Berg adapted Georg Büchner's play "Woyzeck" for the libretto of "Wozzeck".
Sometimes the libretto is written in close collaboration with the composer; this can involve adaptation, as was the case with Rimsky-Korsakov and his librettist Bel'sky, or an entirely original work. In the case of musicals, the music, the lyrics and the "book" (i.e., the spoken dialogue and the stage directions) may each have their own author. Thus, a musical such as Fiddler on the Roof has a composer (Jerry Bock), a lyricist (Sheldon Harnick) and the writer of the "book" (Joseph Stein). In rare cases, the composer writes everything except the dance arrangements - music, lyrics and libretto, as Lionel Bart did for "Oliver!".
Other matters in the process of developing a libretto parallel those of spoken dramas for stage or screen. There are the preliminary steps of selecting or suggesting a subject and developing a sketch of the action in the form of a scenario, as well as revisions that might come about when the work is in production, as with out-of-town tryouts for Broadway musicals, or changes made for a specific local audience. A famous case of the latter is Wagner's 1861 revision of the original 1845 Dresden version of his opera Tannhäuser for Paris.
Literary characteristics.
The opera libretto from its inception (ca. 1600) was written in verse, and this continued well into the 19th century, although genres of musical theatre with spoken dialogue have typically alternated verse in the musical numbers with spoken prose. Since the late 19th century some opera composers have written music to prose or free verse libretti. Much of the recitative of George Gershwin's opera "Porgy and Bess", for instance, is merely DuBose and Dorothy Heyward's play "Porgy" set to music as written - in prose - with the lyrics of the arias, duets, trios and choruses written in verse.
The libretto of a musical, on the other hand, is almost always written in prose (except for the song lyrics). The libretto of a musical, if the musical is adapted from a play (or even a novel), may even borrow their source's original dialogue liberally - much as "Oklahoma!" used dialogue from Lynn Riggs's "Green Grow the Lilacs", "Carousel" used dialogue from Ferenc Molnár's "Liliom", "My Fair Lady" took most of its dialogue word-for-word from George Bernard Shaw's "Pygmalion", "Man of La Mancha" was adapted from the 1959 television play "I, Don Quixote", which supplied most of the dialogue, and the 1954 musical version of "Peter Pan" used J. M. Barrie's dialogue. Even the musical "Show Boat", which is greatly different from the Edna Ferber novel from which it was adapted, uses some of Ferber's original dialogue, notably during the miscegenation scene. And Lionel Bart's "Oliver!" uses chunks of dialogue from Charles Dickens's novel "Oliver Twist", although it bills itself as a "free adaptation" of the novel.
Language and translation.
As the originating language of opera, Italian dominated that genre in Europe (except in France) well through the 18th century, and even into the next century in Russia, for example, when the Italian opera troupe in Saint Petersburg was challenged by the emerging native Russian repertory. Significant exceptions before 1800 can be found in Purcell's works, German opera of Hamburg during the Baroque, ballad opera and Singspiel of the 18th century, etc.
Just as with literature and song, the libretto has its share of problems and challenges with translation. In the past (and even today), foreign musical stage works with spoken dialogue, especially comedies, were sometimes performed with the sung portions in the original language and the spoken dialogue in the vernacular. The effects of leaving lyrics untranslated depend on the piece. 
Many musicals, such as the old Betty Grable - Don Ameche - Carmen Miranda vehicles, are largely unaffected, but this practice is especially misleading in translations of musicals like "Show Boat", "The Wizard of Oz", "My Fair Lady" or "Carousel", in which the lyrics to the songs and the spoken text are often or always closely integrated, and the lyrics serve to further the plot. Availability of printed or projected translations today makes singing in the original language more practical, although one cannot discount the desire to hear a sung drama in one's own language.
The Spanish words "libretista" (playwright, script writer or screen writer) and "libreto" (script or screen play), which are used in the Hispanic TV and cinema industry, derived their meanings from the original operatic sense.
Status of librettists and the libretto.
Librettists have historically received less prominent credit than the composer. In some 17th century operas still being performed, the name of the librettist was not even recorded. As the printing of libretti for sale at performances became more common, these records often survive better than music left in manuscript. But even in late 18th century London, reviews rarely mentioned the name of the librettist, as Lorenzo da Ponte lamented in his memoirs.
By the 20th century some librettists became recognised as part of famous collaborations, as with Gilbert and Sullivan or Rodgers and Hammerstein. Today the composer (past or present) of the musical score to an opera or operetta is usually given top billing for the completed work, and the writer of the lyrics relegated to second place or a mere footnote, a notable exception being Gertrude Stein, who received top billing for "Four Saints in Three Acts". Another exception was Alberto Franchetti's 1906 opera "La figlia di Iorio" which was a close rendering of a highly successful play by its librettist, Gabriele D'Annunzio, a celebrated Italian poet, novelist and dramatist of the day. In some cases, the operatic adaptation has become more famous than the literary text on which it was based, as with Claude Debussy's "Pelléas et Mélisande" after a play by Maurice Maeterlinck.
On the other hand, the affiliation of a poor libretto to great music has sometimes given the libretto's author a kind of accidental immortality. Certainly it is common for works of classical music to be admired in spite of, rather than because of, their libretti. An example is Mozart's inept librettist Varesco.
The question of which is more important in opera — the music or the words — has been debated over time, and forms the basis of at least two operas, Richard Strauss's "Capriccio" and Antonio Salieri's "Prima la musica, poi le parole.
Publication of libretti.
Libretti have been made available in several formats, some more nearly complete than others. The text — i.e., the spoken dialogue, song lyrics and stage directions, as applicable — is commonly published separately from the music (such a booklet is usually included with sound recordings of most operas). Sometimes (particularly for operas in the public domain) this format is supplemented with melodic excerpts of musical notation for important numbers.
Printed scores for operas naturally contain the entire libretto, although there can exist significant differences between the score and the separately printed text. More often than not, this involves the extra repetition of words or phrases from the libretto in the actual score. For example, in the aria 'Nessun dorma' from Puccini's "Turandot", the final lines in the libretto are "Tramontate, stelle!
All'alba, vincerò!" (Fade, you stars! At dawn, I will win!). However, in the score they are sung as "Tramontate, stelle! Tramontate, stelle! All'alba, vincerò! Vincerò! Vincerò!"
Because the modern musical tends to be published in two separate but intersecting formats (i.e., the book and lyrics, with all the words, and the piano-vocal score, with all the musical material, including some spoken cues), both are needed in order to make a thorough reading of an entire show.

</doc>
<doc id="46953" url="http://en.wikipedia.org/wiki?curid=46953" title="Chroma key">
Chroma key

Chroma key compositing, or chroma keying, is a special effects / post-production technique for compositing (layering) two images or video streams together based on color hues (chroma range). The technique has been used heavily in many fields to remove a background from the subject of a photo or video – particularly the newscasting, motion picture and videogame industries. A color range in the top layer is made transparent, revealing another image behind. The chroma keying technique is commonly used in video production and post-production. This technique is also referred to as color keying, colour-separation overlay (CSO; primarily by the BBC), or by various terms for specific color-related variants such as green screen, and blue screen – chroma keying can be done with backgrounds of any color that are uniform and distinct, but green and blue backgrounds are more commonly used because they differ most distinctly in hue from most human skin colors. No part of the subject being filmed or photographed may duplicate a color used in the background.
It is commonly used for weather forecast broadcasts, wherein a news presenter is usually seen standing in front of a large CGI map during live television newscasts, though in actuality it is a large blue or green background. When using a blue screen, different weather maps are added on the parts of the image where the color is blue. If the news presenter wears blue clothes, his or her clothes will also be replaced with the background video. A complementary system is used for green screens. Chroma keying is also used in the entertainment industry for special effects in movies and videogames. The advanced state of the technology and much commercially available computer software, such as Autodesk Smoke, Final Cut Pro, Pinnacle Studio, Adobe After Effects, and dozens of other computer programs, makes it possible and relatively easy for the average home computer user to create videos using the "chromakey" function with easily affordable green screen or blue screen kits.
History.
In filmmaking, a complex and time-consuming process known as "travelling matte" was used prior to the introduction of digital compositing. The blue screen method was developed in the 1930s at RKO Radio Pictures. At RKO, Linwood Dunn used an early version of the travelling matte to create "wipes" – where there were transitions like a windshield wiper in films such as "Flying Down to Rio" (1933). Credited to Larry Butler, a scene featuring a genie escaping from a bottle was the first use of a proper bluescreen process to create a traveling matte for "The Thief of Bagdad" (1940), which won the Academy Award for Visual Effects that year. In 1950, Warner Brothers employee and ex-Kodak researcher Arthur Widmer began working on an ultraviolet travelling matte process. He also began developing bluescreen techniques: one of the first films to use them was the 1958 adaptation of the Ernest Hemingway novella, "The Old Man and the Sea", starring Spencer Tracy.
One drawback to the traditional traveling matte is that the cameras shooting the images to be composited cannot be easily synchronized. For decades, such matte shots had to be done "locked-down", so that neither the matted subject nor the background could shift their camera perspective at all. Later, computer-timed, motion-control cameras alleviated this problem, as both the foreground and background could be filmed with the same camera moves. Petro Vlahos was awarded an Academy Award for his refinement of these techniques in 1964. His technique exploits the fact that most objects in real-world scenes have a color whose blue-color component is similar in intensity to their green-color component. Zbigniew Rybczyński also contributed to bluescreen technology. An optical printer with two projectors, a film camera and a 'beam splitter', was used to combine the actor in front of a blue screen together with the background footage, one frame at a time. During the 1980s, minicomputers were used to control the optical printer. For the film "The Empire Strikes Back", Richard Edlund created a 'quad optical printer' that accelerated the process considerably and saved money. He received a special Academy Award for his innovation.
For "", an ultraviolet light matting process was proposed by Don Lee of CIS and developed by Gary Hutzel and the staff of Image G. This involved a fluorescent orange backdrop which made it easier to generate a holdout matte, thus allowing the effects team to produce effects in a quarter of the time needed for other methods.
Meteorologists on television often use a field monitor, to the side of the screen, to see where they are putting their hands against the background images. A newer technique is to project a faint image onto the screen.
Some films make heavy use of chroma key to add backgrounds that are constructed entirely using computer-generated imagery (CGI). Performances from different takes can even be composited together, which allows actors to be filmed separately and then placed together in the same scene. Chroma key allows performers to appear to be in any location without even leaving the studio. The comedy show Tosh.0 in particular is known for filming entirely with greenscreen and then cutting the image being produced by the computer to show Daniel Tosh standing directly on the purely green set. This is always done at the very end of the show.
Computer development also made it easier to incorporate motion into composited shots, even when using handheld cameras. Reference-points can be placed onto the colored background (usually as a painted grid, X's marked with tape, or equally spaced tennis balls attached to the wall). In post-production, a computer can use the references to compute the camera's position and thus render an image that matches the perspective and movement of the foreground perfectly. Modern advances in software and computational power have even eliminated the need to accurately place the markers - the software figures out their position in space (a disadvantage of this is that it requires a large camera movement, possibly encouraging modern film techniques where the camera is always in motion).
Process.
The principal subject is filmed or photographed against a background consisting of a single color or a relatively narrow range of colors, usually blue or green because these colors are considered to be the furthest away from skin tone. The portions of the video which match the preselected color are replaced by the alternate background video. This process is commonly known as "keying", "keying out" or simply a "key".
Processing a green backdrop.
Green is currently used as a backdrop more than any other color because image sensors in digital video cameras are most sensitive to green, due to the bayer pattern allocating more pixels to the green channel, mimicking the human eye's increased sensitivity to green light. Therefore, the green camera channel contains the least "noise" and can produce the cleanest key/matte/mask. Additionally, less light is needed to illuminate green, again because of the higher sensitivity to green in image sensors. Bright green has also become favored since a blue background may match a subject's eye color or common items of clothing such as jeans.
Processing a blue backdrop.
Before digital chroma keying, bluescreening was accomplished using film. The camera color negative was printed onto high-contrast black and white film, using either a filter or the color sensitivity of the black and white film to limit it to the blue channel. Assuming this film was a negative it produced clear where the bluescreen was, black elsewhere, except it also produced clear for any white objects (since they also contained blue). Removing these spots could be done by a suitable double-exposure with the color positive, and many other techniques. The end result was a clear background with an opaque shape of the subject in the middle. This is called a 'female matte', similar to an 'alpha matte' in digital keying. Copying this film onto another high-contrast negative produced the opposite 'male matte'. The background negative was then packed with the female matte and exposed onto a final strip of film, then the camera negative was packed with the male matte was double-printed onto this same film. These two images combined together creates the final effect. 
Blue was preferred as a backdrop before digital keying became commonplace because of the existence of high contrast film that was sensitive only to the blue color.
Major factors.
The most important factor for a key is the color separation of the foreground (the subject) and background (the screen) – a bluescreen will be used if the subject is predominately green (for example plants), despite the camera being more sensitive to green light.
In analog color TV, color is represented by the phase of the chroma subcarrier relative to a reference oscillator. Chroma key is achieved by comparing the phase of the video to the phase corresponding to the preselected color. In-phase portions of the video are replaced by the alternate background video.
In digital color TV, color is represented by three numbers (red, green, blue intensity levels). Chroma key is achieved by a simple numerical comparison between the video and the preselected color. If the color at a particular point on the screen matches (either exactly, or in a range), then the video at that point is replaced by the alternate background
Clothing.
A chroma key subject must avoid wearing clothes which are similar in color to the chroma key color(s) (unless intentional e.g. wearing a green top to make it appear that the subject has no body), because the clothing may be replaced with the background video. An example of intentional use of this is when an actor wears a blue covering over a part of his body to make it invisible in the final shot. This technique can be used to achieve an effect similar to that used in the "Harry Potter" films to create the effect of an invisibility cloak. The actor can also be filmed against a chroma-key background and inserted into the background shot with a distortion effect, in order to create a cloak that is marginally detectable.
Difficulties emerge with bluescreen when a costume in an effects shot must be blue, such as Superman's traditional blue outfit. In the 2002 film "Spider-Man", in scenes where both Spider-Man and the Green Goblin are in the air, Spider-Man had to be shot in front of the greenscreen and the Green Goblin had to be shot in front of a bluescreen. The color difference is because Spider-Man wears a costume which is red and blue in color and the Green Goblin wears a costume which is entirely green in color. If both were shot in front of the same screen, parts of one character would be erased from the shot.
Background.
Blue is generally used for both weather maps and special effects because it is complementary to human skin tone. The use of blue is also tied to the fact that the blue emulsion layer of film has the finest crystals and thus good detail and minimal grain (in comparison to the red and green layers of the emulsion.) In the digital world, however, green has become the favored color because digital cameras retain more detail in the green channel, and it requires less light than blue. Green not only has a higher luminance value than blue, but also in early digital formats, the green channel was sampled twice as often as the blue, making it easier to work with. The choice of color is up to the effects artists and the needs of the specific shot. In the past decade, the use of green has become dominant in film special effects. Also, the green background is favored over blue for outdoor filming where the blue sky might appear in the frame and could accidentally be replaced in the process. Although green and blue are the most common, any color can be used. Red is usually avoided due to its prevalence in normal human skin pigments, but can be often used for objects and scenes which do not involve people. For example, in John Pizzarelli's song "Birthday Emotions" from the Sesame Street television series, painting backgrounds made by Gerri Brioso are used as a live-action film sequence while the kids such as the Italian-American siblings are jumping through the air and celebrating one of their birthday parties.
Occasionally, a magenta background is used, as in some software applications where the magenta or fuchsia is sometimes referred to as "magic pink".
With other imaging and hardware, many companies avoid the confusion often experienced by weather presenters, who must otherwise watch themselves on a monitor to see the image shown behind them, by lightly projecting a copy of the background image onto the blue/green screen. This allows the presenter to accurately point and look at the map without referring to monitors.
A newer technique is to use a retroreflective curtain in the background, along with a ring of bright LEDs around the camera lens. This requires no light to shine on the background other than the LEDs, which use an extremely small amount of power and space unlike big stage lights, and require no rigging. This advance was made possible by the invention of practical blue LEDs in the 1990s, which also allow for emerald green LEDs.
There is also a form of color keying that uses light spectrum invisible to human eye. Called Thermo-Key, it uses infrared as the key color, which would not be replaced by background image during postprocessing.
Jefferson Airplane used chroma-key background in their performance of "White Rabbit" on "The Smothers Brothers Comedy Hour" to create psychedelic colors.
Even lighting.
The biggest challenge when setting up a bluescreen or greenscreen is even lighting and the avoidance of shadow, because it is best to have as narrow a color range as possible being replaced. A shadow would present itself as a darker color to the camera and might not register for replacement. This can sometimes be seen in low-budget or live broadcasts where the errors cannot be manually repaired. The material being used affects the quality and ease of having it evenly lit. Materials which are shiny will be far less successful than those that are not. A shiny surface will have areas that reflect the lights making them appear pale, while other areas may be darkened. A matte surface will diffuse the reflected light and have a more even color range. In order to get the cleanest key from shooting greenscreen it is necessary to create a value difference between the subject and the greenscreen. In order to differentiate the subject from the screen, a two-stop difference can be used, either by making the greenscreen two stops higher than the subject, or vice versa.
Sometimes a shadow can be used to create a special effect. Areas of the bluescreen or greenscreen with a shadow on them can be replaced with a darker version of the desired background video image, making it look like the person casting. Any spill of the chroma key color will make the result look unnatural. Even a difference in the focal length of the lenses used can affect the success of chroma key.
Exposure.
Another challenge for bluescreen or greenscreen is proper camera exposure. Underexposing or overexposing a colored backdrop can lead to poor saturation levels. In the case of video cameras, underexposed images can contain high amounts of noise, as well. The background must be bright enough to allow the camera to create a bright and saturated image.
Programming.
There are several different quality- and speed-optimized techniques for implementing color keying in software.
In most versions, a function "f"("r", "g", "b") → "α" is applied to every pixel in the image. α (alpha) has a meaning similar to that in alpha compositing techniques. "α" ≤ 0 means the pixel is the green screen, "α" ≥ 1 means the pixel is in the foreground object. Values between 0 and 1 indicate a pixel that is partially covered by the foreground object. A usable green screen example, which matches how chroma key was done on an optical printer, is "f"("r", "g", "b") = "K0" * "b" − "K1" * "g" + "K2" ("K0..2" are user-adjustable constants, 1 is a good initial guess for all of them).
Often the software does screen-spill removal from the colors as well as figure out the alpha. This may be a separate function "g"("r", "g", "b") → ("r", "g", "b"), a very simple green-screen example is "g"("r", "g", "b") → ("r", "min"("g", "b"), "b"). Or "f" is changed to return ("r", "g", "b", "a") all at once, this is useful if part of the calculation is shared.
Most keyers use far more complicated functions. A popular approach is to describe a closed 3D surface in RGB space and determine the signed distance the point ("r", "g", "b") is from this surface, or to find the distance the point ("r", "g", "b") is between two closed nested surfaces. It is also very common for "f"() to depend on more than just the current pixel's color, it may also use the ("x", "y") position, the values of nearby pixels, the value from reference images, and values from user-drawn masks.
A different class of algorithm tries to figure out a 2D path that separates the foreground from the background. This path can be the output, or the image can be drawn by filling the path with "α" = 1 as a final step. An example of such an algorithm is the use of active contour. Most research in recent years has been into these algorithms.

</doc>
<doc id="46954" url="http://en.wikipedia.org/wiki?curid=46954" title="Lock">
Lock

Lock may refer to:

</doc>
<doc id="46955" url="http://en.wikipedia.org/wiki?curid=46955" title="Identity map pattern">
Identity map pattern

In the design of database management systems, the identity map pattern is a database access design pattern used to improve performance by providing a context-specific, in-memory cache to prevent duplicate retrieval of the same object data from the database.
If the requested data has already been loaded from the database, the identity map returns the same instance of the already instantiated object, but if it has not been loaded yet, it loads it and stores the new object in the map. In this way, it follows a similar principle to lazy loading.
There are four types of Identity Map

</doc>
<doc id="46956" url="http://en.wikipedia.org/wiki?curid=46956" title="Cepstrum">
Cepstrum

A cepstrum() is the result of taking the Inverse Fourier transform (IFT) of the logarithm of the estimated spectrum of a signal. It may be pronounced in the two ways given, the second having the advantage of avoiding confusion with 'kepstrum' which also exists (see below). There is a complex cepstrum, a real cepstrum, a power cepstrum, and phase cepstrum.
The power cepstrum in particular finds applications in the analysis of human speech.
The name "cepstrum" was derived by reversing the first four letters of "spectrum". Operations on cepstra are labelled "quefrency analysis", "liftering", or "cepstral analysis".
Origin and definition.
The "power cepstrum" was defined in a 1963 paper by Bogert et al. The power cepstrum of a signal is defined as the squared magnitude of the inverse Fourier transform of the logarithm of the squared magnitude of the Fourier transform of a signal.
A short-time cepstrum analysis was proposed by Schroeder and Noll for application to pitch determination of human speech. 
The "complex cepstrum" was defined by Oppenheim in his development of homomorphic system theory and is defined as the Inverse Fourier transform of the logarithm (with unwrapped phase) of the Fourier transform of the signal. This is sometimes called the spectrum of a spectrum.
The "real cepstrum" uses the logarithm function defined for real values. The real cepstrum is related to the power via the relationship (4 * real cepstrum)^2 = power cepstrum, and is related to the complex cepstrum as real cepstrum = 0.5*(complex cepstrum + time reversal of complex cepstrum).
The complex cepstrum uses the complex logarithm function defined for complex values.
The phase cepstrum is related to the complex cepstrum as phase spectrum = (complex cepstrum - time reversal of complex cepstrum).^2
The complex cepstrum holds information about magnitude and phase of the initial spectrum, allowing the reconstruction of the signal. The real cepstrum uses only the information of the magnitude of the spectrum.
Many texts define the process as FT → abs() → log → IFT, i.e., that the cepstrum is the "inverse Fourier transform of the log-magnitude Fourier spectrum".
The "kepstrum", which stands for "Kolmogorov equation power series time response", is similar to the cepstrum and has the same relation to it as expected value has to statistical average, i.e. cepstrum is the empirically measured quantity while kepstrum is the theoretical quantity.
Applications.
The cepstrum can be seen as information about rate of change in the different spectrum bands. It was originally invented for characterizing the seismic echoes resulting from earthquakes and bomb explosions. It has also been used to determine the fundamental frequency of human speech and to analyze radar signal returns. Cepstrum pitch determination is particularly effective because the effects of the vocal excitation (pitch) and vocal tract (formants) are additive in the logarithm of the power spectrum and thus clearly separate.
The autocepstrum is defined as the cepstrum of the autocorrelation. The autocepstrum is more accurate than the cepstrum in the analysis of data with echoes.
The cepstrum is a representation used in homomorphic signal processing, to convert signals (such as a source and filter) combined by convolution into sums of their cepstra, for linear separation. In particular, the power cepstrum is often used as a feature vector for representing the human voice and musical signals. For these applications, the spectrum is usually first transformed using the mel scale. The result is called the mel-frequency cepstrum or MFC (its coefficients are called mel-frequency cepstral coefficients, or MFCCs). It is used for voice identification, pitch detection and much more. The cepstrum is useful in these applications because the low-frequency periodic excitation from the vocal cords and the formant filtering of the vocal tract, which convolve in the time domain and multiply in the frequency domain, are additive and in different regions in the quefrency domain.
Cepstral concepts.
The independent variable of a cepstral graph is called the quefrency. The quefrency is a measure of time, though not in the sense of a signal in the time domain. For example, if the sampling rate of an audio signal is 44100 Hz and there is a large peak in the cepstrum whose quefrency is 100 samples, the peak indicates the presence of a pitch that is 44100/100 = 441 Hz. This peak occurs in the cepstrum because the harmonics in the spectrum are periodic, and the period corresponds to the pitch. Note that a pure sine wave should not be used to test the cepstrum for its pitch determination from quefrency as a pure sine wave does not contain any harmonics. Rather, a test signal containing harmonics should be used (such as the sum of at least two sines where the second sine is some harmonic (multiple) of the first sine).
Liftering.
Playing further on the anagram theme, a filter that operates on a cepstrum might be called a "lifter". A low pass lifter is similar to a low pass filter in the frequency domain. It can be implemented by multiplying by a window in the quefrency domain and when converted back to the frequency domain, resulting in a smoother signal.
Convolution.
A very important property of the cepstral domain is that the convolution of two signals can be expressed as the addition of their complex cepstra:

</doc>
<doc id="46958" url="http://en.wikipedia.org/wiki?curid=46958" title="Lifter">
Lifter

Lifter may refer to:

</doc>
<doc id="46959" url="http://en.wikipedia.org/wiki?curid=46959" title="Koch snowflake">
Koch snowflake

The Koch snowflake (also known as the "Koch curve, star." or "island") is a mathematical curve and one of the earliest fractal curves to have been described. It is based on the Koch curve, which appeared in a 1904 paper titled "On a continuous curve without tangents, constructible from elementary geometry" (original French title: "Sur une courbe continue sans tangente, obtenue par une construction géométrique élémentaire") by the Swedish mathematician Helge von Koch.
Construction.
The Koch snowflake can be constructed by starting with an equilateral triangle, then recursively altering each line segment as follows:
After one iteration of this process, the resulting shape is the outline of a hexagram.
The Koch snowflake is the limit approached as the above steps are followed over and over again. The Koch curve originally described by Koch is constructed with only one of the three sides of the original triangle. In other words, three Koch curves make a Koch snowflake.
Properties.
The Koch curve has an infinite length because the total length of the curve increases by one third with each iteration. Each iteration creates four times as many line segments as in the previous iteration, with the length of each one being one-third the length of the segments in the previous stage. Hence the length of the curve after "n" iterations will be (4/3)n times the original triangle perimeter, which is unbounded as "n" tends to infinity.
The fractal dimension of the Koch curve is log 4/log 3 ≈ 1.26186. This is greater than the dimension of a line (1) but less than Peano's space-filling curve (2).
The Koch curve is continuous everywhere but differentiable nowhere.
Perimeter of the Koch snowflake.
After each iteration, the number of sides of the Koch snowflake increase by a factor of 4, so the number of sides after "n" iterations is given by:
If the original equilateral triangle has sides of length "s", the length of each side of the snowflake after "n" iterations is:
the perimeter of the snowflake after "n" iterations is:
Area of the Koch snowflake.
In each iteration a new triangle is added on each side of the previous iteration, so the number of new triangles added in iteration "n" is:
The area of each new triangle added in an iteration is one ninth of the area of each triangle added in the previous iteration, so the area of each triangle added in iteration "n" is:
where "a"0 is the area of the original triangle. The total new area added in iteration "n" is therefore:
The total area of the snowflake after "n" iterations is:
Collapsing the geometric sum gives:
Limits of area and perimeter.
As the number of iterations tends to infinity, the limit of the perimeter is:
since formula_10. 
The limit of the area is:
since formula_12.
So the area of the Koch snowflake is 8/5 of the area of the original triangle. Expressed in terms of the side length "s" of the original triangle this is formula_13. It is, however, wrong to state that the perimeter of the Koch snowflake is unbounded, for it is not 1-dimensional and therefore cannot be measured as an 1-dimensional line. A formula_14-dimensional measure exists, but has not been calculated so far. Only upper and lower bounds have been invented
Tessellation of the plane.
It is possible to tessellate the plane by copies of Koch snowflakes in two different sizes. However, such a tessellation is not possible using only snowflakes of the same size as each other. Since each Koch snowflake in the tessellation can be subdivided into seven smaller snowflakes of two different sizes, it is also possible to find tessellations that use more than two sizes at once.
Thue-Morse sequence and turtle graphics.
A turtle graphic is the curve that is generated if an automaton is programmed with a sequence.
If the Thue–Morse sequence members are used in order to select program states:
the resulting curve converges to the Koch snowflake.
Representation as Lindenmayer system.
The Koch Curve can be expressed by a rewrite system (Lindenmayer system).
Here, "F" means "draw forward", "+" means "turn right 60°", and "−" means "turn left 60°".
Variants of the Koch curve.
Following von Koch's concept, several variants of the Koch curve were designed, considering right angles (quadratic), other angles (Cesàro), circles and polyhedra and their extensions to higher dimensions (Sphereflake and Kochcube, respectively)
Squares can be used to generate similar fractal curves. Starting with a unit square and adding to each side at each iteration a square with dimension one third of the squares in the previous iteration, it can be shown that both the length of the perimeter and the total area are determined by geometric progressions. The progression for the area converges to 2 while the progression for the perimeter diverges to infinity, so as in the case of the Koch Snowflake, we have a finite area bounded by an infinite fractal curve. The resulting area fills a square with the same center as the original, but twice the area, and rotated by π/4 radians, the perimeter touching but never overlapping itself.
The total area covered at the nth iteration is :formula_15
While the total length of the perimeter is :formula_16 which approaches infinity as n increases
The Koch Curve Poem.
“Triangles outside triangles outside triangles ad infinitum the Koch curve goes, it's infinitely infinitesimal, this self-similarity shows. A length too great to measure, an area too small to see, what else can this contradiction be, behold fractal geometry."

</doc>
<doc id="46961" url="http://en.wikipedia.org/wiki?curid=46961" title="Thomas Crapper">
Thomas Crapper

Thomas Crapper (baptised 28 September 1836; died 27 January 1910) was a plumber who founded Thomas Crapper & Co in London. Contrary to widespread misconceptions, Crapper did not invent the flush toilet. He did, however, do much to increase the popularity of the toilet, and developed some important related inventions, such as the ballcock. He was noted for the quality of his products and received several royal warrants.
Manhole covers with Crapper's company's name on them in Westminster Abbey are now one of London's minor tourist attractions. Thomas Crapper & Co owned the world's first bath, toilet and sink showroom, in King's Road until 1966. The firm's lavatorial equipment was manufactured at premises in nearby Marlborough Road (now Draycott Avenue).
Company.
Crapper was born in Thorne, Yorkshire, in 1836; the exact date is unknown, but he was baptised on 28 September 1836. His father, Charles, was a sailor. In 1853 he was apprenticed to his brother George, who was a master plumber in Chelsea. After his apprenticeship and three years as a journeyman plumber, in 1861 Crapper set himself up as a sanitary engineer, with his own brass foundry and workshops in nearby Marlborough Road.
The flushing toilet was invented by John Harrington in 1596. Joseph Bramah of Yorkshire patented the first practical water closet in England in 1778. George Jennings in 1852 also took out a patent for the flush-out toilet. In a time when bathroom fixtures were barely spoken of, Crapper heavily promoted sanitary plumbing and pioneered the concept of the bathroom fittings showroom.
In the 1880s, Prince Edward (later Edward VII) purchased his country seat of Sandringham House in Norfolk and asked Thomas Crapper & Co. to supply the plumbing, including thirty lavatories with cedarwood seats and enclosures, thus giving Crapper his first Royal Warrant. The firm received further warrants from Edward as king and from George V both as Prince of Wales and as king.
In 1904, Crapper retired, passing the firm to his nephew George and his business partner Robert Marr Wharam. Crapper lived at 12 Thornsett Road, Anerley, for the last six years of his life and died on 27 January 1910. He was buried in the nearby Elmers End Cemetery.
In 1966 the company was sold by then owner Robert G. Wharam (son of Robert Marr Wharam) on his retirement, to their rivals John Bolding & Sons. Bolding went into liquidation in 1969. The company fell out of use until it was acquired by Simon Kirby, a historian and collector of antique bathroom fittings, who relaunched the company in Stratford-upon-Avon, producing authentic reproductions of Crapper's original Victorian bathroom fittings.
Siphonic flush toilet.
Crapper held nine patents, three of them for water closet improvements such as the floating ballcock, but none was for the flush toilet itself. Thomas Crapper's advertisements implied the siphonic flush was his invention; one having the text "Crapper's Valveless Water Waste Preventer (Patent #4,990) One movable part only", but patent 4990 (for a minor improvement to the water waste preventer) was not his, but that of Albert Giblin in 1898. Crapper's nephew, George, did improve the siphon mechanism by which the water flow is started. A patent for this development was awarded in 1898.
Origin of the word "crap".
It has often been claimed in popular culture that the slang term for human bodily waste, "crap", originated with Thomas Crapper because of his association with lavatories. A common version of this story is that American servicemen stationed in England during World War I saw his name on cisterns and used it as army slang, i.e. "I'm going to the crapper".
The word "crap" is actually of Middle English origin and predates its application to bodily waste. Its most likely etymological origin is a combination of two older words, the Dutch "krappen": to pluck off, cut off, or separate; and the Old French "crappe": siftings, waste or rejected matter (from the medieval Latin "crappa," chaff). In English, it was used to refer to chaff, and also to weeds or other rubbish. Its first application to bodily waste, according to the "Oxford English Dictionary", appeared in 1846 under a reference to a "crapping ken," or a privy, where "ken" means a house.
Further reading.
</dl>

</doc>
<doc id="46963" url="http://en.wikipedia.org/wiki?curid=46963" title="Cunt">
Cunt

Cunt is a vulgar term for female genitalia, and is used as a term of disparagement for females and males. The earliest known use of the word, according to the "Oxford English Dictionary", was as a placename for the London street Gropecunt Lane, "c" 1230. Scholar Germaine Greer said in 2006 that "cunt" "is one of the few remaining words in the English language with a genuine power to shock."
Use of the word as a term of abuse is relatively recent, dating from the late nineteenth century. Reflecting different national usages, "cunt" is described as "an unpleasant or stupid person" in the "Compact Oxford English Dictionary", whereas Merriam-Webster has a usage of the term as "usually disparaging and obscene: woman", noting that it is used in the U.S. as "an offensive way to refer to a woman"; and the "Macquarie Dictionary" of Australian English gives "a contemptible person." When used with a positive qualifier (good, funny, clever, etc.) in Britain, New Zealand, and Australia, it can convey a positive sense of the object or person referred to.
The word appears to have not been strongly taboo in the Middle Ages, but became taboo towards the end of the eighteenth century, and was then not generally admissible in print until the latter part of the twentieth century. The term has various derivative senses, including adjective and verb uses.
Etymology.
The etymology of "cunt" is a matter of debate, but most sources consider the word to have derived from a Germanic word (Proto-Germanic "*kuntō", stem "*kuntōn-"), which appeared as "kunta" in Old Norse. Scholars are uncertain of the origin of the Proto-Germanic form itself. There are cognates in most Germanic languages, such as the Swedish, Faroese and Nynorsk "kunta"; West Frisian and Middle Low German "kunte"; Middle Dutch "conte"; Dutch "kut" & "kont"; Middle Low German "kutte"; Middle High German "kotze" ("prostitute"); German "kott", and perhaps Old English "cot". The etymology of the Proto-Germanic term is disputed. It may have arisen by Grimm's law operating on the Proto-Indo-European root "*gen/gon" "create, become" seen in gonads, genital, gamete, genetics, gene, or the Proto-Indo-European root "*gʷneh₂/guneh₂" "woman" (Greek: "gunê", seen in gynaecology). Relationships to similar-sounding words such as the Latin "cunnus" ("vulva"), and its derivatives French "con", Spanish "coño", and Portuguese "cona", or in Persian "kun" (کون), have not been conclusively demonstrated. Other Latin words related to "cunnus" are "cuneus" ("wedge") and its derivative "cunēre" ("to fasten with a wedge", (figurative) "to squeeze in"), leading to English words such as "cuneiform" ("wedge-shaped"). In Middle English, "cunt" appeared with many spellings, such as "coynte", "cunte" and "queynte", which did not always reflect the actual pronunciation of the word.
The word in its modern meaning is attested in Middle English. "Proverbs of Hendyng", a manuscript from some time before 1325, includes the advice:
Ȝeue þi cunte to cunnig and craue affetir wedding.<br>(Give your cunt wisely and make [your] demands after the wedding.)
Offensiveness.
Generally.
The word "cunt" is generally regarded in English-speaking countries as unsuitable for normal public discourse. It has been described as "the most heavily tabooed word of all English words", although John Ayto, editor of the "Oxford Dictionary of Slang", says "nigger" is more taboo.
Feminist perspectives.
Some feminists of the 1970s sought to eliminate disparaging terms for women, including "bitch" and "cunt". In the context of pornography, Catharine MacKinnon argued that use of the word acts to reinforce a dehumanisation of women by reducing them to mere body parts; and in 1979 Andrea Dworkin described the word as reducing women to "the one essential – 'cunt: our essence ... our offence'".
Despite criticisms, there is a movement among feminists that seeks to reclaim cunt not only as acceptable, but as an honorific, in much the same way that "queer" has been reappropriated by LGBT people and the word "nigger" has been by the black community. Proponents include Inga Muscio in her book, "" and Eve Ensler in "Reclaiming Cunt" from "The Vagina Monologues". 
Germaine Greer, who had previously published a magazine article entitled "Lady, Love Your Cunt", discussed the origins, usage and power of the word in the BBC series "Balderdash and Piffle". She suggested at the end of the piece that there was something precious about the word, in that it was now one of the few remaining words in English that still retained its power to shock. Greer also alludes to the fact that the word "vagina", which is considered the non-vulgar term, was a Latin name given by male anatomists for all muscle coverings, meaning "sword-sheath". She considers it contentious as "cunt" has no such meaning, it simply refers to the entire female genitalia (she also mentions that vagina is applied purely to the internal canal).
Usage: pre-twentieth century.
"Cunt" has been attested in its anatomical meaning since at least the 13th century. While Francis Grose's 1785 "A Classical Dictionary of The Vulgar Tongue" listed the word as "C**T: a nasty name for a nasty thing", it did not appear in any major English dictionary from 1795 to 1961, when it was included in "Webster's Third New International Dictionary" with the comment "usu. considered obscene". Its first appearance in the "Oxford English Dictionary" was in 1972, which cites the word as having been in use since 1230 in what was supposedly a London street name of "Gropecunte Lane". It was, however, also used before 1230, having been brought over by the Anglo-Saxons, originally not an obscenity but rather a factual name for the vulva or vagina. "Gropecunt Lane" was originally a street of prostitution, a red light district. It was normal in the Middle Ages for streets to be named after the goods available for sale therein, hence the prevalence in cities having a medieval history of names such as "Silver Street" and "Fish Street". In some locations, the former name has been bowdlerised, as in the City of York, to the more acceptable "Grape Lane".
The word appears several times in Chaucer's "Canterbury Tales" (c. 1390), in bawdy contexts, but it does not appear to be considered obscene at this point, since it is used openly. A notable use is from the "Miller's Tale": "Pryvely he caught her by the queynte." The Wife of Bath also uses this term, "For certeyn, olde dotard, by your leave/You shall have queynte right enough at eve ... What aileth you to grouche thus and groan?/Is it for ye would have my queynte alone?" In modernised versions of these passages the word "queynte" is usually translated simply as "cunt". However, in Chaucer's usage there seems to be an overlap between the words "cunt" and "quaint" (possibly derived from the Latin for "known"). "Quaint" was probably pronounced in Middle English in much the same way as "cunt". It is sometimes unclear whether the two words were thought of as distinct from one another. Elsewhere in Chaucer's work the word "queynte" seems to be used with meaning comparable to the modern "quaint" (curious or old-fashioned, but nevertheless appealing). This ambiguity was still being exploited by the 17th century; Andrew Marvell's "... then worms shall try / That long preserved virginity, / And your quaint honour turn to dust, / And into ashes all my lust" in "To His Coy Mistress" depends on a pun on these two senses of "quaint".
By Shakespeare's day, the word seems to have become obscene. Although Shakespeare does not use the word explicitly (or with derogatory meaning) in his plays, he still plays with it, using wordplay to sneak it in obliquely. In Act III, Scene 2, of "Hamlet", as the castle's residents are settling in to watch the play-within-the-play, Hamlet asks Ophelia, "Lady, shall I lie in your lap?" Ophelia replies, "No, my lord." Hamlet, feigning shock, says, "Do you think I meant "country matters"?" Then, to drive home the point that the accent is definitely on the first syllable of "country", Shakespeare has Hamlet say, "That's a fair thought, to lie between maids' legs." In "Twelfth Night" (Act II, Scene V) the puritanical Malvolio believes he recognises his employer's handwriting in an anonymous letter, commenting "There be her very Cs, her Us, and her Ts: and thus makes she her great Ps", unwittingly punning on "cunt" and "piss", and while it has also been argued that the slang term "cut" is intended, Pauline Kiernan writes that Shakespeare ridicules "prissy puritanical party-poopers" by having "a Puritan spell out the word 'cunt' on a public stage". A related scene occurs in "Henry V": when Katherine is learning English, she is appalled at the "gros, et impudique" words "foot" and "gown", which her teacher has mispronounced as "coun". It is usually argued that Shakespeare intends to suggest that she has misheard "foot" as "foutre" (French, "fuck") and "coun" as "con" (French "cunt", also used to mean "idiot"). Similarly John Donne alludes to the obscene meaning of the word without being explicit in his poem "The Good-Morrow", referring to sucking on "country pleasures".
The 1675 Restoration comedy "The Country Wife" also features such word play, even in its title.
By the 17th century a softer form of the word, "cunny", came into use. A well-known use of this derivation can be found in the 25 October 1668 entry of the diary of Samuel Pepys. He was discovered having an affair with Deborah Willet: he wrote that his wife "coming up suddenly, did find me imbracing the girl con my hand sub su coats; and endeed I was with my main ("hand") in her cunny. I was at a wonderful loss upon it and the girl also..."
"Cunny" was probably derived from a pun on "coney", meaning "rabbit", rather as "pussy" is connected to the same term for a cat. (Philip Massinger (1583–1640): "A pox upon your Christian cockatrices! They cry, like poulterers' wives, 'No money, no coney.'") Because of this slang use as a synonym for a taboo term, the word coney, when it was used in its original sense to refer to rabbits, came to be pronounced as (rhymes with "phoney"), instead of the original /ˈkʌni/ (rhymes with "honey"). Eventually the taboo association led to the word "coney" becoming deprecated entirely and replaced by the word rabbit.
Robert Burns (1759–1796) used the word in his "Merry Muses of Caledonia", a collection of bawdy verses which he kept to himself and were not publicly available until the mid-1960s. In "Yon, Yon, Yon, Lassie", this couplet appears: "For ilka birss upon her cunt, Was worth a ryal ransom".
Usage: modern.
In modern literature.
James Joyce was one of the first of the major 20th-century novelists to put the word "cunt" into print. In the context of one of the central characters in "Ulysses", Leopold Bloom, Joyce refers to the Dead Sea and to ... the oldest people. Wandered far away over all the earth, captivity to captivity, multiplying, dying, being born everywhere. It lay there now. Now it could bear no more. Dead: an old woman's: the grey sunken cunt of the world.
Joyce uses the word figuratively rather than literally; but while Joyce used the word only once in "Ulysses", with four other wordplays ('cunty') on it, D. H. Lawrence used the word ten times in "Lady Chatterley's Lover", in a more direct sense. Mellors, the gamekeeper and eponymous lover, tries delicately to explain the definition of the word to Lady Constance Chatterley: If your sister there comes ter me for a bit o' cunt an' tenderness, she knows what she's after.
 The novel was the subject of an unsuccessful UK prosecution for obscenity in 1961 against its publishers, Penguin Books.
Henry Miller's novel "Tropic of Cancer" uses the word extensively, ensuring its banning in Britain between 1934 and 1961 and being the subject of the U.S. Supreme Court decision in "Grove Press, Inc. v. Gerstein", 378 U.S. (1964).
Samuel Beckett was an associate of Joyce, and in his "Malone Dies" (1956), he writes: "His young wife had abandoned all hope of bringing him to heel, by means of her cunt, that trump card of young wives."
In Ian McEwan's 2001 novel "Atonement", set in 1935, the word is used in a love letter mistakenly sent instead of a revised version, and although not spoken, is an important plot pivot.
Usage by meaning.
Term of abuse for people.
As a derogatory term, it is comparable to "prick" and means "a fool, a dolt, an unpleasant person – of either sex". This sense is common in New Zealand, British and Australian English, where it is usually applied to men or as referring "specifically" to "a despicable, contemptible or foolish" "man".
During the 1971 Oz trial for obscenity, prosecuting counsel asked writer George Melly "Would you call your 10-year-old daughter a cunt?" Melly replied "No, because I don't think she is."
In the 1975 film "One Flew Over the Cuckoo's Nest", the central character McMurphy, when pressed to explain exactly why he does not like the tyrannical Nurse Ratched, says, "Well, I don't want to break up the meeting or nothing, but she's something of a cunt, ain't she, Doc?"
In American slang, the term can be used to refer to a woman, or "a fellow male homosexual".
Other uses.
The word is sometimes used as a general expletive to show frustration, annoyance or anger, for example "I've had a cunt of a day!" and "This will be a cunt [of a job] to finish".
In the Survey of English Dialects the word was recorded in some areas as meaning "the vulva of a cow". This was pronounced as [kʌnt] in Devon, and [kʊnt] in the Isle of Man, Gloucestershire and Northumberland. Possibly related was the word "cunny" [kʌni], with the same meaning, at Wiltshire.
As a slang term it can be modified by a positive qualifier (funny, clever, etc.) in British, Irish, New Zealand, and Australian English, when referring to a person. For example, "This is my mate Brian. He's a good cunt."
The word "cunty" is also known, although used rarely: a line from Hanif Kureishi's "My Beautiful Laundrette" is the definition of England by a Pakistani immigrant as "eating hot buttered toast with cunty fingers," suggestive of hypocrisy and a hidden sordidness or immorality behind the country's quaint façade. This term is attributed to British novelist Henry Green.
Usage in modern popular culture.
Theatre.
Theatre censorship was effectively abolished in the UK in 1968; prior to that all theatrical productions had to be vetted by the Lord Chamberlain's Office. English stand-up comedian Roy "Chubby" Brown claims that he was the first person to say the word on stage in the United Kingdom.
In the 1996 play "The Vagina Monologues" the author, American writer Eve Ensler, says she has reclaimed the word and encourages the audience to repeat it with her. "Feeling a little irritated in the airport, just say 'cunt,' everything changes," she says. "Try it, go ahead, go ahead. Cunt. Cunt. Cunt."
Television.
Broadcast media are regulated for content, and media providers such as the BBC have guidelines as to how "cunt" and similar words should be treated. In a survey of 2000 commissioned by the British Broadcasting Standards Commission, Independent Television Commission, BBC and Advertising Standards Authority, "cunt" was regarded as the most offensive word which could be heard, above "motherfucker" and "fuck". Nevertheless, there have been occasions when, particularly in a live broadcast, the word has been aired outside editorial control:
The first scripted uses of the word on British television occurred in 1979, in the ITV drama "No Mama No", broadcast in 1979, and by 2005, the Christ character in "Jerry Springer – The Opera" (BBC, 2005) suggesting that he might be gay was found more controversial than the chant "cunting, cunting, cunting, cunting cunt" (a description of the Devil). The first scripted use on US television was on the "Larry Sanders Show" in 1992, and a notable use occurred in "Sex and the City".
In July 2007 BBC Three dedicated a full hour to the word in a detailed documentary ("The 'C' Word") about the origins, use and evolution of the word from the early 1900s to the present day. Presented by British comedian Will Smith, viewers were taken to a street in Oxford once called "Gropecunt Lane" and presented with examples of the acceptability of "cunt" as a word. In the US, an episode of the NBC TV show "30 Rock", titled "The C Word", centred around a subordinate calling protagonist Liz Lemon (Tina Fey) a "cunt" and her subsequent efforts to regain her staff's favour. Jane Fonda did utter the word on a live airing of the "Today Show", a network broadcast-TV news program, in 2008 when being interviewed about "The Vagina Monologues".
Radio.
On 6 December 2010 on the BBC Radio 4 Today programme, James Naughtie referred to the British Culture Secretary Jeremy Hunt as Jeremy Cunt; he covered this up explaining it as being a cough but still ended up giggling over his words while announcing the rest of the items in the next hour.
A little later Andrew Marr referred to the incident during Start the Week where it was said that "we won't repeat the mistake" whereupon Marr slipped up in the same way as Naughtie had. The use of the word was described by the BBC as being "...an offensive four-letter word..."
Film.
The first use of the word in mainstream cinema occurs in "Carnal Knowledge" (1971), in which Jonathan (Jack Nicholson) asks, "Is this an ultimatum? Answer me, you ball-busting, castrating, son of a cunt bitch! Is this an ultimatum or not?" Nicholson later used it again, in "One Flew Over the Cuckoo's Nest" (1975). Two early films by Martin Scorsese, "Mean Streets" (1973) and "Taxi Driver" (1976), use the word in the context of the virgin-whore dichotomy, with characters using it after they were rejected (in "Mean Streets") or after they have slept with the woman (in "Taxi Driver").
In notable instances, the word has been edited out. "Saturday Night Fever" (1977) was released in two versions, "R" (Restricted) and "PG" (Parental Guidance), the latter omitting or replacing dialogue such as Tony Manero (John Travolta)'s comment to Annette (Donna Pescow), "It's a decision a girl's gotta make early in life, if she's gonna be a nice girl or a cunt". This differential persists, and in "The Silence of the Lambs" (1991), Agent Starling (Jodie Foster) meets Dr. Hannibal Lecter (Anthony Hopkins) for the first time and passes the cell of "Multiple Miggs", who says to Starling: "I can smell your cunt." In versions of the film edited for television the word is dubbed with the word scent. The 2010 film "Kick-Ass" caused a controversy when the word was used by Hit-Girl because the actress playing the part, Chloë Grace Moretz, was only 11 at the time of filming.
In Britain, use of the word "cunt" may result in an "18" rating from the British Board of Film Classification (BBFC), and this happened to Ken Loach's film "Sweet Sixteen", because of an estimated twenty uses of "cunt". Still, the BBFC's guidelines at "15" state that "the strongest terms (for example, 'cunt') may be acceptable if justified by the context. Aggressive or repeated use of the strongest language is unlikely to be acceptable." The 2010 Ian Dury biopic "Sex & Drugs & Rock & Roll" was given a "15" rating despite containing seven uses of the word.
Comedy.
In their Derek and Clive dialogues, Peter Cook and Dudley Moore, particularly Cook, arguably made the word more accessible in the UK; in the 1976 sketch "This Bloke Came Up To Me", "cunt" is used approximately thirty-five times. The word is also used extensively by British comedian Roy 'Chubby' Brown, which ensures that his stand-up act has never been fully shown on UK television.
Australian stand-up comedian, Rodney Rude frequently refers to his audiences as "cunts" and makes frequent use of the word in his acts, which got him arrested in Queensland and Western Australia for breaching obscenity laws of those states in the mid-1980s. Australian comedic singer Kevin Bloody Wilson makes extensive use of the word, most notably in the songs "Caring Understanding Nineties Type" and "You Can't Say "Cunt" in Canada".
The word appears in American comic George Carlin's 1972 standup routine on the list of the seven dirty words that could not, at that time, be said on American broadcast television, a routine that led to a U. S. Supreme Court decision. While some of the original seven are now heard on US broadcast television from time to time, "cunt" remains generally taboo except on premium paid subscription cable channels like HBO or Showtime. Comedian Louis C. K. uses the term frequently in his stage act as well as on his television show "Louie" on FX network, which bleeps it out.
Popular music.
The 1977 Ian Dury and The Blockheads album, "New Boots and Panties" used the word in the opening line of the track Plaistow Patricia, thus: "Arseholes, bastards, fucking cunts and pricks", particularly notable as there is no musical lead-in to the lyrics.
In 1979, during a concert at New York's Bottom Line, Carlene Carter introduced a song about mate-swapping called "Swap-Meat Rag" by stating, "If this song don't put the cunt back in country, I don't know what will." The comment was quoted widely in the press, and Carter spent much of the next decade trying to live the comment down. However use of the word in lyrics is not recorded before the Sid Vicious' 1978 version of "My Way", which marked the first known use of the word in a UK Top Ten hit, as a line was changed to "You cunt/I'm not a queer". The following year, "cunt" was used more explicitly in the song "Why D'Ya Do It?" from Marianne Faithfull's album "Broken English": Why'd ya do it, she screamed, after all we've said,<br>
Every time I see your dick I see her cunt in my bed.
The Happy Mondays song, "Kuff Dam" (i.e. "Mad fuck" in reverse), from their 1987 debut album, "Squirrel and G-Man Twenty Four Hour Party People Plastic Face Carnt Smile (White Out)", includes the lyrics "You see that Jesus is a cunt / And never helped you with a thing that you do, or you don't." Biblical scholar James Crossley, writing in the academic journal, "Biblical Interpretation", analyses the Happy Mondays' reference to "Jesus is a cunt" as a description of the "useless assistance" of a now "inadequate Jesus". A phrase from the same lyric, "Jesus is a cunt" was included on the notorious Cradle of Filth T-shirt which depicted a masturbating nun on the front and the slogan "Jesus is a cunt" in large letters on the back. The T-shirt was banned in New Zealand, in 2008.
Liz Phair in "Dance of Seven Veils" on her 1993 album Exile in Guyville, uses the word in the line "I only ask because I'm a real cunt in spring".
The word has been used by numerous non-mainstream bands, such as Australian band TISM, who released an extended play in 1993 "Australia the Lucky Cunt" (a reference to Australia's label the "lucky country"). They also released a single in 1998 entitled "I Might Be a Cunt, but I'm Not a Fucking Cunt", which was banned. The American grindcore band Anal Cunt, on being signed to a bigger label, shortened their name to AxCx.
More recently, in 2012, the word appears at least 10 times in Azealia Banks' song "212". She is also known to refer to her fans on Facebook as "kuntz". Banks has said she is "tired" of defending her profanity-laden lyrics from critics, saying they reflect her everyday speech and experiences.
Computer and video games.
The 2004 video game "" was the first video game to use the word, only once (along with being the first in the series to use the words "nigga," "motherfucker," and "cocksucker"), used by the British character Kent Paul (voiced by Danny Dyer), who refers to Maccer as a "soppy cunt" in the mission "Don Peyote."
The 2004 title "" by SCEE used the word. It is used several times during the game.
In the 2008 title "Grand Theft Auto IV" (developed by Rockstar North and distributed by Take Two Interactive), available on the PlayStation 3 and Xbox 360 consoles, the word, amongst many other expletives, was used by James Pegorino after finding out that his personal bodyguard, who had turned states, who exclaimed "The world is a cunt!" while aiming a shotgun at the player.
Linguistic variants and derivatives.
Various euphemisms, minced forms and in-jokes are used to imply the word without actually saying it, thereby escaping obvious censure and censorship.
Spoonerisms and acronyms.
Deriving from a dirty joke: "What's the difference between a circus and a strip club?"- "The circus has a bunch of cunning stunts..." The phrase "cunning stunt" has been used in popular music. Its first documented appearance was by the English band Caravan who released the album "Cunning Stunts" in July 1975; the title was later used by Metallica for a CD/Video compilation, and in 1992 the Cows released an album with the same title. In his 1980s BBC television programme, Kenny Everett played a vapid starlet, "Cupid Stunt".
There are numerous informal acronyms, including various apocryphal stories concerning academic establishments, such as the "Cambridge University National Trust Society".
There are many variants of the covering phrase "See you next Tuesday", including a play of that title by Ronald Harwood. A more recent acronym is "Can't Use New Technology" which is thought to originate from IT staff.
Puns.
The name "Mike Hunt" is a frequent pun on "my cunt"; it has been used in a scene from the movie "Porky's", and for a character in the BBC radio comedy "Radio Active" in the 1980s. "Has Anyone Seen Mike Hunt?" were the words written on a "pink neon sculpture" representing the letter C, in a 2004 exhibition of the alphabet at the British Library in collaboration with the International Society of Typographic Designers.
As well as obvious references, there are also allusions. On "I'm Sorry I Haven't a Clue", Stephen Fry once defined "countryside" as the act of "murdering Piers Morgan". In "Two Pints of Lager and a Packet of Crisps", Donna and Gaz are perusing erotic novels when they come across "The Count of Monte Cristo"; Gaz helpfully informs Donna that 'it doesn't say Count'. Similarly, in an episode of "Spaced", Sophie tells Tim that she can't see him as there's been a misprint on the title of one of the magazines she works on – "Total Cult". In all these uses, the audience are left to make the connection.
Even Parliaments are not immune from punning uses; as recalled by former Australian prime minister Gough Whitlam:
Never in the House did I use the word which comes to mind. The nearest I came to doing so was when Sir Winton Turnbull, a member of the cavalleria rusticana, was raving and ranting on the adjournment and shouted: "I am a Country member". I interjected "I remember". He could not understand why, for the first time in all the years he had been speaking in the House, there was instant and loud applause from both sides.
and Mark Lamarr used a variation of this same gag on BBC TV's "Never Mind the Buzzcocks". "Stuart Adamson was a Big Country member... and we do remember".
Rhyming slang.
Several celebrities have had their names used as euphemisms, including footballer Roger Hunt, actor Gareth Hunt, singer James Blunt, politician Jeremy Hunt, and 1970s motor-racing driver James Hunt, whose name was once used to introduce the British radio show "I'm Sorry I Haven't A Clue" as "the show that is to panel games what James Hunt is to rhyming slang".
A canting form of some antiquity is "berk", short for "Berkeley Hunt" or "Berkshire Hunt", and in a Monty Python sketch, an idioglossiac man replaces the initial "c" of words with "b", producing "silly bunt". Scottish comedian Chic Murray claimed to have worked for a firm called "Lunt, Hunt & Cunningham".
Derived meanings.
The word "cunt" forms part of some technical terms used in seafaring and other industries.

</doc>
<doc id="46966" url="http://en.wikipedia.org/wiki?curid=46966" title="Sleep disorder">
Sleep disorder

A sleep disorder, or somnipathy, is a medical disorder of the sleep patterns of a person or animal. Some sleep disorders are serious enough to interfere with normal physical, mental, social and emotional functioning. Polysomnography and actigraphy are tests commonly ordered for some sleep disorders.
Disruptions in sleep can be caused by a variety of issues, from teeth grinding (bruxism) to night terrors. When a person suffers from difficulty falling asleep and/or staying asleep with no obvious cause, it is referred to as insomnia. 
Sleep disorders are broadly classified into dyssomnias, parasomnias, circadian rhythm sleep disorders involving the timing of sleep, and other disorders including ones caused by medical or psychological conditions and sleeping sickness. Some common sleep disorders include sleep apnea (stops in breathing during sleep), narcolepsy and hypersomnia (excessive sleepiness at inappropriate times), cataplexy (sudden and transient loss of muscle tone while awake), and sleeping sickness (disruption of sleep cycle due to infection). Other disorders include sleepwalking, night terrors and bed wetting. Management of sleep disturbances that are secondary to mental, medical, or substance abuse disorders should focus on the underlying conditions.
Common disorders.
The most common sleep disorders include:
Diagnosing insomnia.
Insomnia is characterized by an extended period of symptoms including trouble with retaining sleep, fatigue, decreased attentiveness, and dysphoria. To diagnose insomnia, these symptoms must persist for a minimum of 4 weeks. The DSM-IV categorizes insomnias into primary insomnia, insomnia associated with medical or mental illness, and insomnia associated with the consumption or abuse of substances. Individuals with insomnia often worry about the negative health consequences, which can lead to the development of anxiety and depression.
The following tests are used to diagnose insomnia as well as several other sleep disorders.
General principles of treatment.
Treatments for sleep disorders generally can be grouped into four categories:
None of these general approaches is sufficient for all patients with sleep disorders. Rather, the choice of a specific treatment depends on the patient's diagnosis, medical and psychiatric history, and preferences, as well as the expertise of the treating clinician. Often, behavioral/psychotherapeutic and pharmacological approaches are not incompatible and can effectively be combined to maximize therapeutic benefits. Management of sleep disturbances that are secondary to mental, medical, or substance abuse disorders should focus on the underlying conditions.
Medications and somatic treatments may provide the most rapid symptomatic relief from some sleep disturbances. Certain disorders like narcolepsy, are best treated with prescription drugs such as Modafinil. Others, such as chronic and primary insomnia, may be more amenable to behavioral interventions, with more durable results.
Chronic sleep disorders in childhood, which affect some 70% of children with developmental or psychological disorders, are under-reported and under-treated. Sleep-phase disruption is also common among adolescents, whose school schedules are often incompatible with their natural circadian rhythm. Effective treatment begins with careful diagnosis using sleep diaries and perhaps sleep studies. Modifications in sleep hygiene may resolve the problem, but medical treatment is often warranted.
Special equipment may be required for treatment of several disorders such as obstructive apnea, the circadian rhythm disorders and bruxism. In these cases, when severe, an acceptance of living with the disorder, however well managed, is often necessary.
Some sleep disorders have been found to compromise glucose metabolism.
Hypnosis treatment.
Research suggests that hypnosis may be helpful in alleviating some types and manifestations of sleep disorders in some patients. "Acute and chronic insomnia often respond to relaxation and hypnotherapy approaches, along with sleep hygiene instructions." Hypnotherapy has also helped with nightmares and sleep terrors. There are several reports of successful use of hypnotherapy for parasomnias specifically for head and body rocking, bedwetting and sleepwalking.
Hypnotherapy has been studied in the treatment of sleep disorders in both adults and children.
Sleep medicine.
Due to rapidly increasing knowledge about sleep in the 20th century, including the discovery of REM sleep and sleep apnea, the medical importance of sleep was recognized. The medical community began paying more attention than previously to primary sleep disorders, such as sleep apnea, as well as the role and quality of sleep in other conditions. By the 1970s in the USA, clinics and laboratories devoted to the study of sleep and sleep disorders had been founded, and a need for standards arose.
Sleep Medicine is now a recognized subspecialty within internal medicine, family medicine, pediatrics, otolaryngology, psychiatry and neurology in the United States. Certification in Sleep Medicine shows that the specialist:"has demonstrated expertise in the diagnosis and management of clinical conditions that occur during sleep, that disturb sleep, or that are affected by disturbances in the wake-sleep cycle. This specialist is skilled in the analysis and interpretation of comprehensive polysomnography, and well-versed in emerging research and management of a sleep laboratory." 
Competence in sleep medicine requires an understanding of a myriad of very diverse disorders, many of which present with similar symptoms such as excessive daytime sleepiness, which, in the absence of volitional sleep deprivation, "is almost inevitably caused by an identifiable and treatable sleep disorder", such as sleep apnea, narcolepsy, idiopathic hypersomnia, Kleine–Levin syndrome, menstrual-related hypersomnia, idiopathic recurrent stupor, or circadian rhythm disturbances. Another common complaint is insomnia, a set of symptoms which can have a great many different causes, physical and mental. Management in the varying situations differs greatly and cannot be undertaken without a correct diagnosis.
Sleep dentistry (bruxism, snoring and sleep apnea), while not recognized as one of the nine dental specialties, qualifies for board-certification by the American Board of Dental Sleep Medicine (ABDSM). The resulting Diplomate status is recognized by the American Academy of Sleep Medicine (AASM), and these dentists are organized in the Academy of Dental Sleep Medicine (USA). The qualified dentists collaborate with sleep physicians at accredited sleep centers and can provide oral appliance therapy and upper airway surgery to treat or manage sleep-related breathing disorders.
In the UK, knowledge of sleep medicine and possibilities for diagnosis and treatment seem to lag. Guardian.co.uk quotes the director of the Imperial College Healthcare Sleep Centre: "One problem is that there has been relatively little training in sleep medicine in this country – certainly there is no structured training for sleep physicians." The Imperial College Healthcare site shows attention to obstructive sleep apnea syndrome (OSA) and very few other sleep disorders.

</doc>
<doc id="46978" url="http://en.wikipedia.org/wiki?curid=46978" title="Castalia">
Castalia

Castalia (; Greek: Κασταλία), in Greek mythology, was a nymph whom Apollo transformed into a fountain at Delphi, at the base of Mount Parnassos, or at Mount Helicon. Castalia could inspire the genius of poetry to those who drank her waters or listened to their quiet sound; the sacred water was also used to clean the Delphian temples. Apollo consecrated Castalia to the Muses ("Castaliae Musae"). 
The 20th century German writer Hermann Hesse used Castalia as inspiration for the name of the futuristic fictional utopia in his 1943 magnum opus, "The Glass Bead Game". Castalia is home to an austere order of intellectuals with a twofold mission: to run boarding schools for boys, and to nurture and play the Glass Bead Game.

</doc>
<doc id="46980" url="http://en.wikipedia.org/wiki?curid=46980" title="Pollen">
Pollen

Pollen is a fine to coarse powder containing the microgametophytes of seed plants, which produce the male gametes (sperm cells). Pollen grains have a hard coat made of sporopollenin that protects the gametophytes during the process of their movement from the stamens to the pistil of flowering plants or from the male cone to the female cone of coniferous plants. If pollen lands on a compatible pistil or female cone, it germinates, producing a pollen tube that transfers the sperm to the ovule containing the female gametophyte. Individual pollen grains are small enough to require magnification to see detail. The study of pollen is called palynology and is highly useful in paleoecology, paleontology, archeology, and forensics.
Pollen in plants is used for transferring haploid male genetic material from the anther of a single flower to the stigma of another in cross-pollination. In a case of self-pollination, this process takes place from the anther of a flower to the stigma of the same flower.
The structure and formation of pollen.
Pollen itself is not the male gamete. Each pollen grain contains vegetative (non-reproductive) cells (only a single cell in most flowering plants but several in other seed plants) and a generative (reproductive) cell. In flowering plants the vegetative tube cell produces the pollen tube, and the generative cell divides to form the two sperm cells.
Formation.
Pollen is produced in the 'microsporangium' (contained in the anther of an angiosperm flower, male cone of a coniferous plant, or male cone of other seed plants). Pollen grains come in a wide variety of shapes (most often spherical), sizes, and surface markings characteristic of the species (see electron micrograph, right). Pollen grains of pines, firs, and spruces are winged. The smallest pollen grain, that of the forget-me-not ("Myosotis" spp.), is around 6 µm (0.006 mm) in diameter. Wind-borne pollen grains can be as large as about 90–100 µm.
In angiosperms, during flower development the anther is composed of a mass of cells that appear undifferentiated, except for a partially differentiated dermis. As the flower develops, four groups of sporogenous cells form within the anther. The fertile sporogenous cells are surrounded by layers of sterile cells that grow into the wall of the pollen sac. Some of the cells grow into nutritive cells that supply nutrition for the microspores that form by meiotic division from the sporogenous cells. In a process called microsporogenesis, four haploid microspores are produced from each diploid sporogenous cell (microsporocyte, pollen mother cell or meiocyte), after meiotic division. After the formation of the four microspores, which are contained by callose walls, the development of the pollen grain walls begins. The callose wall is broken down by an enzyme called callase and the freed pollen grains grow in size and develop their characteristic shape and form a resistant outer wall called the exine and an inner wall called the intine. The exine is what is preserved in the fossil record.
In the microgametogenesis, the unicellular microspores undergoes mitosis and develops into mature microgametophytes containing the gametes. In some flowering plants, germination of the pollen grain often begins before it leaves the microsporangium, with the generative cell forming the two sperm cells.
Structure.
Except in the case of some submerged aquatic plants, the mature pollen-grain has a double wall. The vegetative and generative cells are surrounded by a thin delicate wall of unaltered cellulose called the endospore or intine, and a tough resistant outer cuticularized wall composed largely of sporopollenin called the exospore or exine. The exine often bears spines or warts, or is variously sculptured, and the character of the markings is often of value for identifying genus, species, or even cultivar or individual. The spines may be less than a micron in length (spinulum, plural spinuli) referred to as spinulose (scabrate), or longer than a micron (echina, echinae) referred to as echinate. Various terms also describe the sculpturing such as reticulate, a net like appearance consisting of elements (murus, muri) separated from each other by a lumen (plural lumina).
The pollen wall protects the sperm while the pollen grain is moving from the anther to the stigma; it protects the vital genetic material from drying out and solar radiation. The pollen grain surface is covered with waxes and proteins, which are held in place by structures called sculpture elements on the surface of the grain. The outer pollen wall, which prevents the pollen grain from shrinking and crushing the genetic material during desiccation, is composed of two layers. These two layers are the tectum and the foot layer, which is just above the intine. The tectum and foot layer are separated by a region called the columella, which is composed of strengthening rods. The outer wall is constructed with a resistant biopolymer called sporopollenin.
The pollen tube passes through the pollen grain wall by way of structures called apertures. The apertures are various modifications of the wall of the pollen grain that may involve thinning, ridges and pores. They allow shrinking and swelling of the grain caused by changes in moisture content. Elongated apertures or furrows in the pollen grain are called colpi (singular: colpus) or sulci (singular: sulcus). Apertures that are more circular are called pores. Colpi, sulci and pores are major features in the identification of classes of pollen. Pollen may be referred to as inaperturate (apertures absent) or aperturate (apertures present). The aperture may have a lid (operculum), hence is described as operculate.
The orientation of furrows (relative to the original tetrad of microspores) classifies the pollen as sulcate or colpate. Sulcate pollen has a furrow across the middle of what was the outer face when the pollen grain was in its tetrad. If the pollen has only a single sulcus, it is described as monosulcate. Colpate pollen has furrows other than across the middle of the outer faces. Eudicots have pollen with three colpi (tricolpate) or with shapes that are evolutionarily derived from tricolpate pollen. The evolutionary trend in plants has been from monosulcate to polycolpate or polyporate pollen.
Pollination.
The transfer of pollen grains to the female reproductive structure (pistil in angiosperms) is called pollination. This transfer can be mediated by the wind, in which case the plant is described as anemophilous (literally wind-loving). Anemophilous plants typically produce great quantities of very lightweight pollen grains, sometimes with air-sacs. Non-flowering seed plants (e.g. pine trees) are characteristically anemophilous. Anemophilous flowering plants generally have inconspicuous flowers. Entomophilous (literally insect-loving) plants produce pollen that is relatively heavy, sticky and protein-rich, for dispersal by insect pollinators attracted to their flowers. Many insects and some mites are specialized to feed on pollen, and are called palynivores.
In non-flowering seed plants, pollen germinates in the pollen chamber, located beneath the micropyle, underneath the integuments of the ovule. A pollen tube is produced, which grows into the nucellus to provide nutrients for the developing sperm cells. Sperm cells of Pinophyta and Gnetophyta are without flagella, and are carried by the pollen tube, while those of Cycadophyta and Ginkgophyta have many flagella.
When placed on the stigma of a flowering plant, under favorable circumstances, a pollen grain puts forth a pollen tube, which grows down the tissue of the style to the ovary, and makes its way along the placenta, guided by projections or hairs, to the micropyle of an ovule. The nucleus of the tube cell has meanwhile passed into the tube, as does also the generative nucleus, which divides (if it hasn't already) to form two sperm cells. The sperm cells are carried to their destination in the tip of the pollen-tube.
Pollen in the fossil record.
Pollen's sporopollenin outer sheath affords it some resistance to the rigours of the fossilisation process that destroy weaker objects; it is also produced in huge quantities. There is an extensive fossil record of pollen grains, often disassociated from their parent plant. The discipline of palynology is devoted to the study of pollen, which can be used both for biostratigraphy and to gain information about the abundance and variety of plants alive — which can itself yield important information about paleoclimates. Pollen is first found in the fossil record in the late Devonian period and increases in abundance until the present day.
Allergy to pollen.
Nasal allergy to pollen is called pollinosis, and allergy specifically to grass pollen is called hay fever. Generally, pollens that cause allergies are those of anemophilous plants (pollen is dispersed by air currents.) Such plants produce large quantities of lightweight pollen (because wind dispersal is random and the likelihood of one pollen grain landing on another flower is small), which can be carried for great distances and are easily inhaled, bringing it into contact with the sensitive nasal passages.
In the US, people often mistakenly blame the conspicuous goldenrod flower for allergies. Since this plant is entomophilous (its pollen is dispersed by animals), its heavy, sticky pollen does not become independently airborne. Most late summer and fall pollen allergies are probably caused by ragweed, a widespread anemophilous plant.
Arizona was once regarded as a haven for people with pollen allergies, although several ragweed species grow in the desert. However, as suburbs grew and people began establishing irrigated lawns and gardens, more irritating species of ragweed gained a foothold and Arizona lost its claim of freedom from hay fever.
Anemophilous spring blooming plants such as oak, birch, hickory, pecan, and early summer grasses may also induce pollen allergies. Most cultivated plants with showy flowers are entomophilous and do not cause pollen allergies.
The percentage of people in the United States affected by hay fever varies between 10% and 20%, and such allergy has proven to be the most frequent allergic response in the nation. There are certain evidential suggestions pointing out hay fever and similar allergies to be of hereditary origin. Individuals who suffer from eczema or are asthmatic tend to be more susceptible to developing long-term hay fever.
In Denmark, decades of rising temperatures cause pollen to appear earlier and in greater numbers, as well as introduction of new species such as ragweed.
The most efficient way to handle a pollen allergy is by preventing contact with the material. Individuals carrying the ailment may at first believe that they have a simple summer cold, but hay fever becomes more evident when the apparent cold does not disappear. The confirmation of hay fever can be obtained after examination by a general physician.
Treatment.
Antihistamines are effective at treating mild cases of pollinosis, this type of non-prescribed drugs includes loratadine, cetirizine and chlorphenamine. They do not prevent the discharge of histamine, but it has been proven that they do prevent a part of the chain reaction activated by this biogenic amine, which considerably lowers hay fever symptoms.
Decongestants can be administered in different ways such as tablets and nasal sprays.
Allergy immunotherapy (AIT) treatment involves administering doses of allergens to accustom the body to pollen, thereby inducing specific long-term tolerance. Allergy immunotherapy can be administered orally (as sublingual tablets or sublingual drops), or by injections under the skin (subcutaneous). Discovered by Leonard Noon and John Freeman in 1911, allergy immunotherapy represents the only causative treatment for respiratory allergies.
Nutrition.
Most major classes of predatory and parasitic arthropods contain species that eat pollen, despite the common perception that bees are the primary pollen-consuming arthropod group. Many other Hymenoptera other than bees consume pollen as adults, though only a small number feed on pollen as larvae (including some ant larvae). Spiders are normally considered carnivores but pollen is an important source of food for several species, particularly for spiderlings, which catch pollen on their webs. It is not clear how spiderlings manage to eat pollen however, since their mouths are not large enough to consume pollen grains. Some predatory mites also feed on pollen, with some species being able to subsist solely on pollen, such as "Euseius tularensis", which feeds on the pollen of dozens of plant species. Members of some beetle families such as Mordellidae and Melyridae feed almost exclusively on pollen as adults, while various lineages within larger families such as Curculionidae, Chrysomelidae, Cerambycidae, and Scarabaeidae are pollen specialists even though most members of their families are not (e.g., only 36 of 40000 species of ground beetles, which are typically predatory, have been shown to eat pollen—but this is thought to be a severe underestimate as the feeding habits are only known for 1000 species). Similarly, Ladybird beetles mainly eat insects, but many species also eat pollen, as either part or all of their diet. Hemiptera are mostly herbivores or omnivores but pollen feeding is known (and has only been well studied in the Anthocoridae). Many adult flies, especially Syrphidae, feed on pollen, and three UK syrphid species feed strictly on pollen (syrphids, like all flies, cannot eat pollen directly due to the structure of their mouthparts, but can consume pollen contents that are dissolved in a fluid). Some species of fungus, including "Fomes fomentarius", are able to break down grains of pollen as a secondary nutrition source that is particularly high in nitrogen.
Some species of "Heliconius" butterflies consume pollen as adults, which appears to be a valuable nutrient source, and these species are more distasteful to predators than the non-pollen consuming species.
Although bats, butterflies and hummingbirds are not pollen eaters "per se", their consumption of nectar in flowers is an important aspect of the pollination process.
In humans.
A variety of producers have started selling bee pollen for human consumption, often marketed as a food (rather than a dietary supplement). The largest constituent is carbohydrates, with protein content ranging from 7 to 35 percent depending on the plant species collected by bees.
Honey produced by bees from natural sources contains pollen derived p-coumaric acid, an antioxidant.
The U.S. Food and Drug Administration (FDA) has not found any harmful effects of bee pollen consumption, except from the usual allergies. However, FDA does not allow bee pollen marketers in the United States to make health claims about their produce, as no scientific basis for these has ever been proven. Furthermore, there are possible dangers not only from allergic reactions but also from contaminants such as pesticides and from fungi and bacteria growth related to poor storage procedures. A manufacturers's claim that pollen collecting helps the bee colonies is also controversial.
Forensic palynology.
In forensic biology, pollen can tell a lot about where a person or object has been, because regions of the world, or even more particular locations such a certain set of bushes, will have a distinctive collection of pollen species. Pollen evidence can also reveal the season in which a particular object picked up the pollen. Pollen has been used to trace activity at mass graves in Bosnia, catch a burglar who brushed against a "Hypericum" bush during a crime, and has even been proposed as an additive for bullets to enable tracking them.

</doc>
<doc id="46981" url="http://en.wikipedia.org/wiki?curid=46981" title="Scutum">
Scutum

Scutum is a small constellation introduced in the seventeenth century. Its name is Latin for shield.
History.
Scutum was named in 1684 by Polish astronomer Johannes Hevelius (Jan Heweliusz), who originally named it Scutum Sobiescianum (Shield of Sobieski) to commemorate the victory of the Christian forces led by Polish King John III Sobieski (Jan III Sobieski) in the Battle of Vienna in 1683. Later, the name was shortened to Scutum.
Five bright stars of Scutum (α Sct, β Sct, δ Sct, ε Sct and η Sct) were previously known as 1, 6, 2, 3, and 9 Aquilae respectively.
Coincidentally, the Chinese also associated these stars with battle armor, incorporating them into the larger asterism known as "Tien Pien", i.e., the Heavenly Casque (or Helmet).
Notable features.
Stars.
Scutum is not a bright constellation, with the brightest star, Alpha Scuti, at magnitude 3.85. But some stars are notable in the constellation. Beta Scuti is the second brightest at magnitude 4.22, followed by Delta Scuti at magnitude 4.72. Beta Scuti is a binary system, with the primary with a spectral type similar to the Sun, although it is 1,270 times brighter. Delta Scuti is a bluish white giant star, which is now coming at the direction of the Solar System. Within 1.3 million years it will come as close to 10 light years from Earth, and will be much brighter than Sirius by that time.
Scutum is also notable for having UY Scuti, a red supergiant pulsating variable star. At 1,708 ± 192 solar radii, it is the largest star currently known.
Deep sky objects.
Although not a large constellation, Scutum contains several open clusters, as well as a globular cluster and a planetary nebula. The two best known deep sky objects in Scutum are M11 (the Wild Duck Cluster) and the open cluster M26 (NGC 6694). The globular cluster NGC 6712 and the planetary nebula IC 1295
can be found in the eastern part of the constellation, only 24 arcminutes apart.
The most prominent open cluster in Scutum is the Wild Duck Cluster, M11. It was named by William H. Smyth in 1844 for its resemblance in the eyepiece to a flock of ducks in flight. The cluster, 6200 light-years from Earth and 20 light-years in diameter, contains approximately 3000 stars, making it a particularly rich cluster. It is 220 million years old.
Space Exploration.
The Space probe Pioneer 11 is moving in the direction of this constellation, though it will not be nearing any of the stars in this constellation for many thousands of years, by which time its batteries will be long dead.
External links.
Coordinates: 

</doc>
<doc id="46982" url="http://en.wikipedia.org/wiki?curid=46982" title="Transmission system">
Transmission system

In telecommunications a transmission system is a system that transmits a signal from one place to another. The signal can be an electrical, optical or radio signal.
Some transmission systems contain multipliers, which amplify a signal prior to re-transmission, or regenerators, which attempt to reconstruct and re-shape the coded message before re-transmission.
One of the most widely used transmission system technologies in the Internet and the PSTN is SONET.
Also, transmission system is the medium through which data[information] is transmitted from one point[computer] to another. Examples of common transmission systems people use everyday are: the internet, mobile network, cordless cables, etc.

</doc>
<doc id="46984" url="http://en.wikipedia.org/wiki?curid=46984" title="Echion (painter)">
Echion (painter)

Echion (Ancient Greek: Έχίων), also known as Aetion, was a celebrated Greek painter spoken of by Lucian, who gives a description of one of his pictures, representing the marriage of Alexander and Roxana. This painting excited such admiration when exhibited at the Olympic Games, that Proxenidas, one of the judges, gave the artist his daughter in marriage. Echion seems to have excelled particularly in the art of mixing and laying on his colors. It has commonly been supposed that he lived in the time of Alexander the Great; but the words of Lucian show clearly that he must have lived about the time of Hadrian and the Antonines. Aloys Hirt supposes that the name of the painter of Alexander's marriage, whom Lucian praises so highly, as Aetion, is a corruption of Echion.

</doc>
<doc id="46986" url="http://en.wikipedia.org/wiki?curid=46986" title="Grand Canyon National Park">
Grand Canyon National Park

Grand Canyon National Park is the United States' 15th oldest national park. Named a UNESCO World Heritage Site in 1979, the park is located in Arizona. The park's central feature is the Grand Canyon, a gorge of the Colorado River, which is often considered one of the Seven Natural Wonders of the World. The park covers 1,217,262 acre of unincorporated area in Coconino and Mohave counties.
History.
Grand Canyon National Park was named as an official national park in 1919, but the landmark had been well known to Americans for over thirty years prior. In 1903, President Theodore Roosevelt visited the site and said: "The Grand Canyon fills me with awe. It is beyond comparison—beyond description; absolutely unparalleled through-out the wide world... Let this great wonder of nature remain as it now is. Do nothing to mar its grandeur, sublimity and loveliness. You cannot improve on it. But what you can do is to keep it for your children, your children's children, and all who come after you, as the one great sight which every American should see."
Despite Roosevelt's enthusiasm and his strong interest in preserving land for public use, the Grand Canyon was not immediately designated a national park. The first bill to create Grand Canyon National Park was introduced in 1882 by then-Senator Benjamin Harrison, which would have made Grand Canyon National Park the nation's second, after Yellowstone National Park. Harrison unsuccessfully reintroduced his bill in 1883 and 1886; after his election to the presidency, he established the Grand Canyon Forest Reserve in 1893. Theodore Roosevelt created the Grand Canyon Game Preserve by proclamation on 28 November 1906 and Grand Canyon National Monument in 1908. Further Senate bills to establish the site as a national park were introduced and defeated in 1910 and 1911, before the Grand Canyon National Park Act was finally signed by President Woodrow Wilson in 1919. The National Park Service, established in 1916, assumed administration of the park.
The creation of the park was an early success of the conservation movement. Its national park status may have helped thwart proposals to dam the Colorado River within its boundaries. (Later, the Glen Canyon Dam would be built upriver.) In 1975, the former Marble Canyon National Monument, which followed the Colorado River northeast from the Grand Canyon to Lee's Ferry, was made part of Grand Canyon National Park. In 1979, UNESCO declared the park a World Heritage Site.
In 2010, Grand Canyon National Park was honored with its own coin under the America the Beautiful Quarters program.
Geography.
The Grand Canyon, including its extensive system of tributary canyons, is valued for its combination of size, depth, and exposed layers of colorful rocks dating back to Precambrian times. The canyon itself was created by the incision of the Colorado River and its tributaries after the Colorado Plateau was uplifted, causing the Colorado River system to develop along its present path.
The primary public areas of the park are the North and South Rims of the Grand Canyon itself. The rest of the park is extremely rugged and remote, although many places are accessible by pack trail and backcountry roads. Only the Navajo Bridge near Page connects the rims by road in Arizona; this journey can take around five hours by car. Otherwise, the two rims of the Canyon are connected via the Mike O'Callaghan–Pat Tillman Memorial Bridge and the Hoover Dam.
The park headquarters are at Grand Canyon Village, not far from the south entrance to the park, near one of the most popular viewpoints. Park accommodations are operated by Xanterra Parks and Resorts.
North Rim.
The North Rim is a smaller, more remote area with less tourist activity. It is accessed by Arizona State Route 67.
South Rim.
The South Rim is more accessible than the North Rim; most visitors to the park come to the South Rim, arriving on Arizona State Route 64. The highway enters the park through the South Entrance, near Tusayan, Arizona, and heads eastward, leaving the park through the East Entrance. Interstate 40 provides access to the area from the south. From the north, U.S. Route 89 connects Utah, Colorado, and the North Rim to the South Rim. Overall, some thirty miles of the South Rim are accessible by road.
Services.
The Grand Canyon Village is located at the north end of U.S. Route 180, coming from Flagstaff. It is a full-service community, including lodging, fuel, food, souvenirs, a hospital, churches, and access to trails and guided walks and talks.
Lodging.
Several lodging facilities are available along the South Rim. Hotels and other lodging include: El Tovar in the village, Bright Angel Lodge, Kachina Lodge, Thunderbird Lodge, Maswik Lodge, Yavapai Lodge and Phantom Ranch, located on the canyon floor. There is also an RV Park named Trailer Village. All of these facilities are managed by Xanterra Parks & Resorts.
On the North rim there is the historic Grand Canyon Lodge managed by Forever resorts and a campground near the lodge, managed by the National Park staff.
Activities.
South Rim.
A variety of activities at the South Rim cater to park visitors. The South Rim Drive (35 mi is a driving tour split into two segments. The western drive to Hermit's Point is 8 mi with several overlooks along the way, including Mohave Point, Hopi Point, and the Powell Memorial. From March to December, access to Hermit's Rest is restricted to the free shuttle provided by the Park Service. The eastern portion to Desert View is 25 mi, and is open to private vehicles year round.
Walking tours include the Rim Trail, which runs west from the Pipe Creek viewpoint for about 8 mi of paved road, followed by 7 mi unpaved to Hermit's Rest. Hikes can begin almost anywhere along this trail, and a shuttle can return hikers to their point of origin. Mather Point, the first view most people reach when entering from the South Entrance, is a popular place to begin.
Private canyon flyovers are provided by helicopters and small airplanes out of Las Vegas, Phoenix, and the Grand Canyon National Park Airport. Due to a crash in the 1990s, scenic flights are no longer allowed to fly within 1500 ft of the rim within the Grand Canyon National Park. Several companies offer flights that land 3500 ft in the Grand Canyon in Hualapai Indian territory.
North Rim.
On the North Rim there are few roads, however, there are some notable vehicle accessible lookout points including Point Imperial, Roosevelt Point, and Cape Royal. Mule rides are also available that go to a variety of places including several thousand feet down into the canyon.
Many visitors to the North Rim choose to make use of the variety of hiking trails including the Wildforss Trail, Uncle Jim's Trail, the Transept Trail, and the North Kaibab Trail which can be followed all the way across the canyon to the Bright Angel Trail on the South side.
Development.
The US government had halted development of a 1.6 million acres area including the National Park from 1966 to 2009, during the "Bennett Freeze", because of an ownership dispute between Hopi and Navajo.
In 2007, a skywalk over the western rim was completed which has attracted" thousands of visitors a year, most from Las Vegas". That land is owned by the Hualapai tribe, which co-developed the site.
In 2014, a developer announced plans to build a multimedia complex on the canyon's rim called the Grand Canyon Escalade. On 420 acres there would be shops, an IMAX theater, hotels and an RV park. A gondola would enable easy visits to the canyon floor where a "riverwalk" of "connected walkways, an eatery, a tramway station, a seating area and a wastewater package plant" would be situated. Navajo Nation President Ben Shelly has indicated agreement; the tribe would have to invest $65 million for road, water and communication facilities for the $1 billion complex. One of the developers is Navajo and has cited an 8 to 18 percent share of the gross revenue for the tribe as an incentive.
Grand Canyon Association.
The Grand Canyon Association (GCA) is the National Park Service's official nonprofit partner. It raises private funds to benefit Grand Canyon National Park by operating retail shops and visitor centers within the park, and providing educational opportunities about the natural and cultural history of the region.

</doc>
<doc id="46988" url="http://en.wikipedia.org/wiki?curid=46988" title="Justinian (disambiguation)">
Justinian (disambiguation)

Justinian (from Latin: "Iustinianus") may refer to:

</doc>
<doc id="46989" url="http://en.wikipedia.org/wiki?curid=46989" title="Grand Canyon">
Grand Canyon

View from the South Rim
The Grand Canyon (Hopi: "Ongtupqa"; Yavapai: "Wi:kaʼi:la", Spanish: "Gran Cañón"), is a steep-sided canyon carved by the Colorado River in the state of Arizona in the United States. It is contained within and managed by Grand Canyon National Park, the Hualapai Tribal Nation, the Havasupai Tribe and the Navajo Nation. President Theodore Roosevelt was a major proponent of preservation of the Grand Canyon area, and visited it on numerous occasions to hunt and enjoy the scenery.
The Grand Canyon is 277 mi long, up to 18 mi wide and attains a depth of over a mile (6000 ft). Nearly two billion years of Earth's geological history have been exposed as the Colorado River and its tributaries cut their channels through layer after layer of rock while the Colorado Plateau was uplifted. While the specific geologic processes and timing that formed the Grand Canyon are the subject of debate by geologists, recent evidence suggests that the Colorado River established its course through the canyon at least 17 million years ago. Since that time, the Colorado River continued to erode and form the canyon to its present-day configuration.
For thousands of years, the area has been continuously inhabited by Native Americans who built settlements within the canyon and its many caves. The Pueblo people considered the Grand Canyon ("Ongtupqa" in the Hopi language) a holy site, and made pilgrimages to it. The first European known to have viewed the Grand Canyon was García López de Cárdenas from Spain, who arrived in 1540.
Geography.
The Grand Canyon is a fissure in the Colorado Plateau that exposes uplifted Proterozoic and Paleozoic strata, and is also one of the 19 distinct physiographic sections of the Colorado Plateau province. It is not the deepest canyon in the world (Kali Gandaki Gorge in Nepal is far deeper), however, the Grand Canyon is known for its visually overwhelming size and its intricate and colorful landscape. Geologically it is significant because of the thick sequence of ancient rocks that are beautifully preserved and exposed in the walls of the canyon. These rock layers record much of the early geologic history of the North American continent.
Uplift associated with mountain formation later moved these sediments thousands of feet upward and created the Colorado Plateau. The higher elevation has also resulted in greater precipitation in the Colorado River drainage area, but not enough to change the Grand Canyon area from being semi-arid. The uplift of the Colorado Plateau is uneven, and the Kaibab Plateau that Grand Canyon bisects is over a thousand feet higher at the North Rim (about 1000 ft) than at the South Rim. Almost all runoff from the North Rim (which also gets more rain and snow) flows toward the Grand Canyon, while much of the runoff on the plateau behind the South Rim flows away from the canyon (following the general tilt). The result is deeper and longer tributary washes and canyons on the north side and shorter and steeper side canyons on the south side.
Temperatures on the North Rim are generally lower than those on the South Rim because of the greater elevation (averaging 8,000 ft/2,438 m above sea level). Heavy rains are common on both rims during the summer months. Access to the North Rim via the primary route leading to the canyon (State Route 67) is limited during the winter season due to road closures.
Geology.
The Grand Canyon is part of the Colorado River basin which has developed over the past 40million years. A recent study places the origins of the canyon beginning about 17M years ago. Previous estimates had placed the age of the canyon at 5–6 million years. The study, which was published in the journal "Science" in 2008, used uranium-lead dating to analyze calcite deposits found on the walls of nine caves throughout the canyon. There is a substantial amount of controversy because this research suggests such a substantial departure from prior widely supported scientific consensus. In December 2012, a study published in the journal "Science" claimed new tests had suggested the Grand Canyon could be as old as 70M years. However, this study has been criticized as "[an] attempt to push the interpretation of their new data to their limits without consideration of the whole range of other geologic data sets."
The canyon is the result of erosion which creates one of the most complete geologic columns on the planet.
The major geologic exposures in the Grand Canyon range in age from the 2-billion-year-old Vishnu Schist at the bottom of the Inner Gorge to the 230M-year-old Kaibab Limestone on the Rim. There is a gap of about a billion years between the 500M-year-old stratum and the level below it, which dates to about 1.5billion yearsago. This large unconformity indicates a period of erosion between two periods of deposition.
Many of the formations were deposited in warm shallow seas, near-shore environments (such as beaches), and swamps as the seashore repeatedly advanced and retreated over the edge of a proto-North America. Major exceptions include the Permian Coconino Sandstone, which contains abundant geological evidence of aeolian sand dune deposition. Several parts of the Supai Group also were deposited in non–marine environments.
The great depth of the Grand Canyon and especially the height of its strata (most of which formed below sea level) can be attributed to 5–10 thousand feet (5000 to) of uplift of the Colorado Plateau, starting about 65M years ago (during the Laramide Orogeny). This uplift has steepened the stream gradient of the Colorado River and its tributaries, which in turn has increased their speed and thus their ability to cut through rock (see the elevation summary of the Colorado River for present conditions).
Weather conditions during the ice ages also increased the amount of water in the Colorado River drainage system. The ancestral Colorado River responded by cutting its channel faster and deeper.
The base level and course of the Colorado River (or its ancestral equivalent) changed 5.3M years ago when the Gulf of California opened and lowered the river's base level (its lowest point). This increased the rate of erosion and cut nearly all of the Grand Canyon's current depth by 1.2M years ago. The terraced walls of the canyon were created by differential erosion.
Between 100,000 and 3M years ago, volcanic activity deposited ash and lava over the area which at times completely obstructed the river. These volcanic rocks are the youngest in the canyon.
History.
Native Americans.
The Ancestral Puebloans were a Native American culture centered on the present-day Four Corners area of the United States. They were the first people known to live in the Grand Canyon area. The cultural group has often been referred to in archaeology as the Anasazi, although the term is not preferred by the modern Puebloan peoples. The word "Anasazi" is Navajo for "Ancient Ones" or "Ancient Enemy".
Archaeologists still debate when this distinct culture emerged. The current consensus, based on terminology defined by the Pecos Classification, suggests their emergence around 1200  during the Basketmaker II Era. Beginning with the earliest explorations and excavations, researchers have believed that the Ancient Puebloans are ancestors of the modern Pueblo peoples.
In addition to the Ancestral Puebloans, a number of distinct cultures have inhabited the Grand Canyon area. The Cohonina lived to the west of the Grand Canyon, between 500 and 1200 . The Cohonina were ancestors of the Yuman, Havasupai, and Walapai peoples who inhabit the area today.
The Sinagua were a cultural group occupying an area to the southeast of the Grand Canyon, between the Little Colorado River and the Salt River, between approximately 500 and 1425 . The Sinagua may have been ancestors of several Hopi clans.
By the time of the arrival of Europeans in the 16thcentury, newer cultures had evolved. The Hualapai inhabit a 100 mi stretch along the pine-clad southern side of the Grand Canyon. The Havasupai have been living in the area near Cataract Canyon since the beginning of the 13thcentury, occupying an area the size of Delaware. The Southern Paiutes live in what is now southern Utah and northern Arizona. The Navajo, or Diné, live in a wide area stretching from the San Francisco Peaks eastwards towards the Four Corners. Archaeological and linguistic evidence suggests the Navajo descended from the Athabaskan people near Great Slave Lake, Canada, who migrated after the 11thcentury.
European arrival and settlement.
Spanish explorers.
In September 1540, under orders from the conquistador Francisco Vásquez de Coronado to search for the fabled Seven Cities of Cibola, Captain Garcia Lopez de Cardenas, along with Hopi guides and a small group of Spanish soldiers, traveled to the South Rim of the Grand Canyon between Desert View and Moran Point. Pablo de Melgrossa, Juan Galeras, and a third soldier descended some one third of the way into the Canyon until they were forced to return because of lack of water. In their report, they noted that some of the rocks in the Canyon were "bigger than the great tower of Seville." It is speculated that their Hopi guides likely knew routes to the canyon floor, but may have been reluctant to lead the Spanish to the river. No Europeans visited the Canyon again for over two hundred years.
Fathers Francisco Atanasio Domínguez and Silvestre Vélez de Escalante were two Spanish priests who, with a group of Spanish soldiers, explored southern Utah and traveled along the North Rim of the Canyon in Glen and Marble Canyons in search of a route from Santa Fe to California in 1776. They eventually found a crossing, formerly known as the "Crossing of the Fathers," that today lies under Lake Powell.
Also in 1776, Fray Francisco Garces, a Franciscan missionary, spent a week near Havasupai, unsuccessfully attempting to convert a band of Native Americans to Christianity. He described the Canyon as "profound".
American exploration.
James Ohio Pattie, along with a group of American trappers and mountain men, may have been the next European to reach the Canyon, in 1826.
Jacob Hamblin, a Mormon missionary, was sent by Brigham Young in the 1850s to locate suitable river crossing sites in the Canyon. Building good relations with local Native Americans Hualapai Nation and white settlers, he found the Crossing of the Fathers, and the locations what would become Lee's Ferry in 1858 and Pierce Ferry (later operated by, and named for, Harrison Pierce) – only the latter two sites suitable for ferry operation. He also acted as an advisor to John Wesley Powell before his second expedition to the Grand Canyon, serving as a diplomat between Powell and the local native tribes to ensure the safety of his party.
In 1857, Edward Fitzgerald Beale was superintendent of an expedition to survey a wagon road along the 35th parallel from Fort Defiance, Arizona to the Colorado River. He led a small party of men in search of water on the Coconino Plateau near the Canyon's South Rim. On September 19, near present day National Canyon, they came upon what May Humphreys Stacey described in his journal as "...a wonderful canyon four thousand feet deep. Everyone (in the party) admitted that he never before saw anything to match or equal this astonishing natural curiosity."
Also in 1857, the U.S. War Department asked Lieutenant Joseph Ives to lead an expedition to assess the feasibility of an up-river navigation from the Gulf of California. Also in a stern wheeler steamboat "Explorer", after two months and 350 mi of difficult navigation, his party reached Black Canyon some two months after George Johnson. The "Explorer" struck a rock and was abandoned. Ives led his party east into the Canyon — they may have been the first Europeans to travel the Diamond Creek drainage and traveled eastwards along the South Rim. In his "Colorado River of the West" report to the Senate in 1861 he states that "One or two trappers profess to have seen the canon."
According to the "San Francisco Herald", in a series of articles run in 1853, Captain Joseph R. Walker in January 1851 with his nephew James T. Walker and six men, traveled up the Colorado River to a point where it joined the Virgin River and continued east into Arizona, traveling along the Grand Canyon and making short exploratory side trips along the way. Walker is reported to have said he wanted to visit the Moqui Indians, as the Hopi were then called by whites. He had met these people briefly in previous years, thought them exceptionally interesting and wanted to become better acquainted. The Herald reporter then reported: "We believe that Capt. Joe Walker is the only white man in this country that has ever visited this strange people."
In 1858, John Strong Newberry became probably the first geologist to visit the Grand Canyon.
In 1869, Major John Wesley Powell led the first expedition down the Canyon. Powell set out to explore the Colorado River and the Grand Canyon. Gathering nine men, four boats and food for 10 months, he set out from Green River, Wyoming on May 24. Passing through (or portaging around) a series dangerous rapids, the group passed down the Green River to its confluence with the Colorado River, near present-day Moab, Utah and completed the journey with many hardships through the Grand Canyon on August 13, 1869. In 1871 Powell first used the term "Grand Canyon"; previously it had been called the "Big Canyon".
In 1889, Frank M. Brown wanted to build a railroad along the Colorado River to carry coal. He, his chief engineer Robert Brewster Stanton, and 14 others started to explore the Grand Canyon in poorly designed cedar wood boats, with no life preservers. Brown drowned in an accident near Marble Canyon: Stanton made new boats and proceeded to explore the Colorado all of the way to the Gulf of California.
The Grand Canyon became an official national monument in 1908 and a national park in 1919.
Federal protection.
U.S. President Theodore Roosevelt visited the Grand Canyon in 1903. An avid outdoorsman and staunch conservationist, he established the Grand Canyon Game Preserve on November 28, 1906. Livestock grazing was reduced, but predators such as mountain lions, eagles, and wolves were eradicated. Roosevelt added adjacent national forest lands and redesignated the preserve a U.S. National Monument on January 11, 1908. Opponents such as land and mining claim holders blocked efforts to reclassify the monument as a U.S. National Park for 11 years. Grand Canyon National Park was finally established as the 17th U.S. National Park by an Act of Congress signed into law by President Woodrow Wilson on February 26, 1919.
The federal government administrators who manage park resources face many challenges. These include issues related to the recent reintroduction into the wild of the highly endangered California condor, air tour overflight noise levels, water rights disputes with various tribal reservations that border the park, and forest fire management. The Grand Canyon National Park superintendent is Steve Martin. Martin was named superintendent on February 5, 2007, to replace retiring superintendent Joe Alston. Martin was previously the National Park Service Deputy Director and superintendent of several other national parks, including Denali and Grand Teton. Federal officials started a flood in the Grand Canyon in hopes of restoring its ecosystem on March 5, 2008. The canyon's ecosystem was permanently changed after the construction of the Glen Canyon Dam in 1963.
Between 2003 and 2011, 2,215 mining claims have been requested that are adjacent to the Canyon, including claims for uranium mines. Mining has been suspended since 2009, when U.S. Interior Secretary Ken Salazar temporarily withdrew 1 e6acre from the permitting process, pending assessment of the environmental impact of mining. Critics of the mines are concerned that, once mined, the uranium will leach into the water of the Colorado River and contaminate the water supply for up to 18 million people.
South Rim buildings.
There are several historic buildings located along the South Rim with most in the vicinity of Grand Canyon Village.
Weather.
Weather in the Grand Canyon varies according to elevation. The forested rims are high enough to receive winter snowfall, but along the Colorado River in the Inner Gorge, temperatures are similar to those found in Tucson and other low elevation desert locations in Arizona. Conditions in the Grand Canyon region are generally dry, but substantial precipitation occurs twice annually, during seasonal pattern shifts in winter (when Pacific storms usually deliver widespread, moderate rain and high-elevation snow to the region from the west) and in late summer (due to the North American Monsoon, which delivers waves of moisture from the southeast, causing dramatic, localized thunderstorms fueled by the heat of the day). Average annual precipitation on the South Rim is less than 16 inches (35 cm), with 60 inches (132 cm) of snow; the higher North Rim usually receives 27 inches (59 cm) of moisture, with a typical snowfall of 144 inches (317 cm); and Phantom Ranch, far below the Canyon's rims along the Colorado River at 2,500 feet (762 m) gets just 8 inches (17.6 cm) of rain, and snow is a rarity. 
Temperatures vary wildly throughout the year, with summer highs within the Inner Gorge commonly exceeding 100 °F (37.8 °C) and winter minimum temperatures sometimes falling below zero degrees Fahrenheit (−17.8 °C) along the canyon's rims. Visitors are often surprised by these potentially extreme conditions, and this, along with the high altitude of the canyon's rims, can lead to unpleasant side effects such as dehydration, sunburn, and hypothermia.
Weather conditions can greatly affect hiking and canyon exploration, and visitors should obtain accurate forecasts because of hazards posed by exposure to extreme temperatures, winter storms and late summer monsoons. While the park service posts weather information at gates and visitor centers, this is a rough approximation only, and should not be relied upon for trip planning. For accurate weather in the Canyon, hikers should consult the National Weather Service's NOAA weather radio or the official National Weather Service website.
The National Weather Service has had a cooperative station on the South Rim since 1903. The record high temperature on the South Rim was 105 °F (41 °C) on June 26, 1974, and the record low temperature was −20 °F (−29 °C) on January 1, 1919, February 1, 1985, and December 23, 1990.
Air quality.
The Grand Canyon area has some of the cleanest air in the United States.:p.5-2
However at times the air quality can be considerably affected by events such as forest fires and dust storms in the Southwest.
What effect there is on air quality and visibility in the Canyon has been mainly from sulfates, soils, and organics. The sulfates largely result from urban emissions in southern California, borne on the prevailing westerly winds throughout much of the year, and emissions from Arizona’s copper smelter region, borne on southerly or southeasterly winds during the monsoon season. Airborne soils originate with windy conditions and road dust. Organic particles result from vehicle emissions, long-range transport from urban areas, and forest fires, as well as from VOCs emitted by vegetation in the surrounding forests. Nitrates, carried in from urban areas, stationary sources, and vehicle emissions; as well as black carbon from forest fires and vehicle emissions, also contribute to a lesser extent.
A number of actions have been taken to preserve and further improve air quality and visibility at the Canyon.
In 1990, amendments to the Clean Air Act established the Grand Canyon Visibility Transport Commission (GCVTC) to advise the US EPA on strategies for protecting visual air quality on the Colorado Plateau. The GCVTC released its final report in 1996 and initiated the Western Regional Air Partnership (WRAP), a partnership of state, tribal and federal agencies to help coordinate implementation of the Commission’s recommendations.
In 1999, the Regional Haze Rule established a goal of restoring visibility in national parks and wilderness areas (Class 1 areas), such as the Grand Canyon, to natural background levels by 2064. Subsequent revisions to the rule provide specific requirements for making reasonable progress toward that goal.
In the early 1990s, studies indicated that emissions of SO2, a sulfate precursor, from the Navajo Generating Station affected visibility in the Canyon mainly in the winter, and which if controlled would improve wintertime visibility by 2 to 7%.
As a result, scrubbers were added to the plant’s three units in 1997 through 1999, reducing SO2 emissions by more than 90%. The plant also installed low-NOx SOFA burners in 2009 -2011, reducing emissions of NOx, a nitrate precursor, by 40%.
Emissions from the Mohave Generating Station to the west were similarly found to affect visibility in the Canyon. The plant was required to have installed SO2 scrubbers, but was instead shut down in 2005, completely eliminating its emissions.
Prescribed fires are typically conducted in the spring and fall in the forests adjacent to the Canyon to reduce the potential for severe forest fires and resulting smoke conditions. Although prescribed fires also affect air quality, the controlled conditions allow the use of management techniques to minimize their impact.
Biology and ecology.
Plants.
There are approximately 1,737 known species of vascular plants, 167 species of fungi, 64 species of moss and 195 species of lichen found in Grand Canyon National Park. This variety is largely due to the 8,000 foot elevation change from the Colorado River up to the highest point on the North Rim. Grand Canyon boasts a dozen endemic plants (known only within the Park's boundaries) while only ten percent of the Park's flora is exotic. Sixty-three plants found here have been given special status by the U.S. Fish and Wildlife Service.
The Mojave Desert influences the western sections of the canyon, Sonoran Desert vegetation covers the eastern sections, and ponderosa and pinyon pine forests grow on both rims.
Natural seeps and springs percolating out of the canyon walls are home to 11% of all the plant species found in the Grand Canyon. The Canyon itself can act as a connection between the east and the west by providing corridors of appropriate habitat along its length. The canyon can also be a genetic barrier to some species, like the tassel-eared squirrel.
The aspect, or direction a slope faces, also plays a major role in adding diversity to the Grand Canyon. North-facing slopes receive about one-third the normal amount of sunlight, so plants growing there are similar to plants found at higher elevations, or in more northern latitudes. The south-facing slopes receive the full amount of sunlight and are covered in vegetation typical of the Sonoran Desert.
Animals.
Of the 34 mammal species found along the Colorado River corridor, 18 are rodents and eight are bats.
Life zones and communities.
The Park contains several major ecosystems. Its great biological diversity can be attributed to the presence of five of the seven life zones and three of the four desert types in North America. The five life zones represented are the Lower Sonoran, Upper Sonoran, Transition, Canadian, and Hudsonian. This is equivalent to traveling from Mexico to Canada. Differences in elevation and the resulting variations in climate are the major factors that form the various life zones and communities in and around the canyon. Grand Canyon National Park contains 129 vegetation communities, and the composition and distribution of plant species is influenced by climate, geomorphology and geology.
Lower Sonoran.
The Lower Sonoran life zone spans from the Colorado River up to 3500 ft. Along the Colorado River and its perennial tributaries, a riparian community exists. Coyote willow, arrowweed, seep-willow, western honey mesquite, catclaw acacia, and exotic tamarisk (saltcedar) are the predominant species. Hanging gardens, seeps and springs often contain rare plants such as the white-flowering western redbud, stream orchid, and McDougall's flaveria. Endangered fish in the river include the humpback chub and the razorback sucker.
The three most common amphibians in these riparian communities are the canyon tree frog, red-spotted toad, and Woodhouse’s Rocky Mountain toad. Leopard frogs are very rare in the Colorado River corridor, and are known to exist at only a few sites. There are 33 crustacean species found in the Colorado River and its tributaries within Grand Canyon National Park. Of these 33, 16 are considered true zooplankton organisms.
Only 48 bird species regularly nest along the river, while others use the river as a migration corridor or as overwintering habitat. The bald eagle is one species that uses the river corridor as winter habitat.
River otters may have disappeared from the park in late 20th century, and muskrats are extremely rare. Beavers cut willows, cottonwoods, and shrubs for food, and can significantly affect the riparian vegetation. Other rodents, such as antelope squirrels and pocket mice, are mostly omnivorous, using many different vegetation types. Grand Canyon bats typically roost in desert uplands, but forage on the abundance of insects along the river and its tributaries. In addition to bats, coyotes, ringtails, and spotted skunks are the most numerous riparian predators and prey on invertebrates, rodents, and reptiles.
Raccoons, weasels, bobcats, gray foxes, and mountain lions are also present, but are much more rare. Mule deer and desert bighorn sheep are the ungulates that frequent the river corridor. Since the removal of 500 feral burros in the early 1980s, bighorn sheep numbers have rebounded. Mule deer are generally not permanent residents along the river, but travel down from the rim when food and water resources there become scarce.
The insect species commonly found in the river corridor and tributaries are midges, caddis flies, mayflies, stoneflies, black flies, mites, beetles, butterflies, moths, and fire ants. Numerous species of spiders and several species of scorpions including the bark scorpion and the giant desert hairy scorpion inhabit the riparian zone.
Eleven aquatic and 26 terrestrial species of mollusks have been identified in and around Grand Canyon National Park. Of the aquatic species, two are bivalves (clams) and nine are gastropods (snails). Twenty-six species of terrestrial gastropods have been identified, primarily land snails and slugs.
There are approximately 47 reptile species in Grand Canyon National Park. Ten are considered common along the river corridor and include lizards and snakes. Lizard density tends to be highest along the stretch of land between the water's edge and the beginning of the upland desert community. The two largest lizards in the Canyon are gila monsters and chuckwallas. Many snake species, which are not directly dependent on surface water, may be found both within the inner gorge and the Colorado River corridor. Six rattlesnake species have been recorded in the park.
Above the river corridor a desert scrub community, composed of North American desert flora, thrives. Typical warm desert species such as creosote bush, white bursage, brittlebush, catclaw acacia, ocotillo, mariola, western honey mesquite, four-wing saltbush, big sagebrush, blackbrush and rubber rabbitbrush grow in this community. The mammalian fauna in the woodland scrub community consists of 50 species, mostly rodents and bats. Three of the five Park woodrat species live in the desert scrub community.
Except for the western (desert) banded gecko, which seems to be distributed only near water along the Colorado River, all of the reptiles found near the river also appear in the uplands, but in lower densities. The desert gopher tortoise, a threatened species, inhabits the desert scrublands in the western end of the park.
Some of the common insects found at elevations above 2,000 ft are orange paper wasps, honey bees, black flies, tarantula hawks, stink bugs, beetles, black ants, and monarch and swallowtail butterflies. Solifugids, wood spiders, garden spiders, black widow spiders and tarantulas can be found in the desert scrub and higher elevations.
Upper Sonoran and Transition.
The Upper Sonoran Life Zone includes most of the inner canyon and South Rim at elevations from 3500 to. This zone is generally dominated by blackbrush, sagebrush, and pinyon-juniper woodlands. Elevations of 3500 to are in the Mojave Desert Scrub community of the Upper Sonoran. This community is dominated by the four-winged saltbush and creosote bush; other important plants include Utah agave, narrowleaf mesquite, ratany, catclaw, and various cacti species.
Approximately 30 bird species breed primarily in the desert uplands and cliffs of the inner canyon. Virtually all bird species present breed in other suitable habitats throughout the Sonoran and Mohave deserts. The abundance of bats, swifts, and riparian birds provides ample food for peregrines, and suitable eyrie sites are plentiful along the steep canyon walls. Also, several critically endangered California condors that were re-introduced to the Colorado Plateau on the Arizona Strip, have made the eastern part of the Park their home.
The conifer forests provide habitat for 52 mammal species. Porcupines, shrews, red squirrels, tassel eared Kaibab and Abert's squirrels, black bear, mule deer, and elk are found at the park's higher elevations on the Kaibab Plateau.
Above the desert scrub and up to 6200 ft is a pinyon pine forest and one seed juniper woodland. Within this woodland one can find big sagebrush, snakeweed, Mormon tea, Utah agave, banana and narrowleaf Yucca, winterfat, Indian ricegrass, dropseed, and needlegrass. There are a variety of snakes and lizards here, but one species of reptile, the mountain short-horned lizard, is a particularly abundant inhabitant of the piñon-juniper and ponderosa pine forests.
Ponderosa pine forests grow at elevations between 6500 and, on both North and South rims in the Transition life zone. The South Rim includes species such as gray fox, mule deer, bighorn sheep, rock squirrels, pinyon pine and Utah juniper. Additional species such as Gambel oak, New Mexico locust, mountain mahogany, elderberry, creeping mahonia, and fescue have been identified in these forests. The Utah tiger salamander and the Great Basin spadefoot toad are two amphibians that are common in the rim forests. Of the approximately 90 bird species that breed in the coniferous forests, 51 are summer residents and at least 15 of these are known to be neotropical migrants.
Canadian and Hudsonian.
Elevations of 8,200 to are in the Canadian Life Zone, which includes the North Rim and the Kaibab Plateau. Spruce-fir forests characterized by Englemann spruce, blue spruce, Douglas fir, white fir, aspen, and mountain ash, along with several species of perennial grasses, groundsels, yarrow, cinquefoil, lupines, sedges, and asters, grow in this sub-alpine climate. Mountain lions, Kaibab squirrels, and northern goshawks are found here.
Montane meadows and subalpine grassland communities of the Hudsonian life zone are rare and located only on the North Rim. Both are typified by many grass species. Some of these grasses include blue and black grama, big galleta, Indian ricegrass and three-awns. The wettest areas support sedges and forbs.
Grand Canyon tourism.
Grand Canyon National Park is one of the world's premier natural attractions, attracting about five million visitors per year. Overall, 83% were from the United States: California (12.2%), Arizona (8.9%), Texas (4.8%), Florida (3.4%) and New York (3.2%) represented the top domestic visitors. Seventeen percent of visitors were from outside the United States; the most prominently represented nations were the United Kingdom (3.8%), Canada (3.5%), Japan (2.1%), Germany (1.9%) and The Netherlands (1.2%).
The South Rim is open all year round weather permitting. The North Rim is generally open mid-May to mid-October.
Activities.
Aside from casual sightseeing from the South Rim (averaging 7,000 feet [2,100 m] above sea level), rafting, hiking, running, and helicopter tours are popular. The Grand Canyon Ultra Marathon is a 78 mi race over 24 hours. The floor of the valley is accessible by foot, muleback, or by boat or raft from upriver. Hiking down to the river and back up to the rim in one day is discouraged by park officials because of the distance, steep and rocky trails, change in elevation, and danger of heat exhaustion from the much higher temperatures at the bottom. Rescues are required annually of unsuccessful rim-to-river-to-rim travelers. Nevertheless, hundreds of fit and experienced hikers complete the trip every year.
Camping on the North and South Rims is generally restricted to established campgrounds and reservations are highly recommended, especially at the busier South Rim. There is at large camping available along many parts of the North Rim managed by Kaibab National Forest. North Rim campsites are only open seasonally due to road closures from weather and winter snowpack. All overnight camping below the rim requires a backcountry permit from the Backcountry Office (BCO). Each year Grand Canyon National Park receives approximately 30,000 requests for backcountry permits. The park issues 13,000 permits, and close to 40,000 people camp overnight. The earliest a permit application is accepted is the first of the month, four months before the proposed start month.
Tourists wishing for a more vertical perspective can board helicopters and small airplanes in Boulder, Las Vegas, Phoenix and Grand Canyon National Park Airport (seven miles from the South Rim) for canyon flyovers. Scenic flights are no longer allowed to fly within 1500 feet of the rim within the national park because of a late 1990s crash. The last aerial video footage from below the rim was filmed in 1984. However, some helicopter flights land on the Havasupai and Hualapai Indian Reservations within Grand Canyon (outside of the park boundaries). Recently, the Hualapai Tribe opened the glass-bottomed Grand Canyon Skywalk on their property, Grand Canyon West. The Skywalk has seen mixed reviews since the site is only accessible by driving down a 10 mi dirt road, costs a minimum of $85 in total for reservation fees, a tour package and admission to the Skywalk itself and the fact that cameras or other personal equipment are not permitted on the Skywalk at any time due to the hazard of damaging the glass if dropped. The Skywalk is about 250 mi by road from Grand Canyon Village at the South Rim.
Viewing the canyon.
Lipan Point is a promontory located on the South Rim. This point is located to the east of the Grand Canyon Village along the Desert View Drive. There is a parking lot for visitors who care to drive along with the Canyon's bus service that routinely stops at the point. The trailhead to the Tanner Trail is located just before the parking lot. The view from Lipan Point shows a wide array of rock strata and the Unkar Creek area in the inner canyon.
The canyon can be seen from the Toroweap (or Tuweep) Overlook situated 3000 vertical feet above the Colorado River, about 50 miles downriver from the South Rim and 70 upriver from the Grand Canyon Skywalk. This region — "one of the most remote in the United States" according to the National Park Service — is reached only by one of three lengthy dirt tracks beginning in from St. George, Utah, Colorado City or near Pipe Spring National Monument (both in Arizona). Each road traverses wild, uninhabited land for 97, 62 and 64 miles respectively. The Park Service manages the area for primitive value with minimal improvements and services.
Grand Canyon fatalities.
About 600 deaths have occurred in the Grand Canyon since the 1870s. Some of these deaths occurred as the result of overly zealous photographic endeavors, some were the result of airplane collisions within the canyon, and some visitors drowned in the Colorado River.
Of the fatalities, 53 have resulted from falls; 65 deaths were attributable to environmental causes, including heat stroke, cardiac arrest, dehydration, and hypothermia; 7 were caught in flash floods; 79 were drowned in the Colorado River; 242 perished in airplane and helicopter crashes (128 of them in the 1956 disaster mentioned below); 25 died in freak errors and accidents, including lightning strikes and rock falls; and 23 were the victims of homicides.
1956 air disaster.
In 1956 the Grand Canyon was the site of the deadliest commercial aviation disaster in history at the time.
On the morning of June 30, 1956, a TWA Lockheed Super Constellation and a United Airlines Douglas DC-7 departed Los Angeles International Airport within three minutes of one another on eastbound transcontinental flights. Approximately 90 minutes later, the two propeller-driven airliners collided above the canyon while both were flying in unmonitored airspace.
The wreckage of both planes fell into the eastern portion of the canyon, on Temple and Chuar Buttes, near the confluence of the Colorado and Little Colorado rivers. The disaster killed all 128 passengers and crew members aboard both planes.
This accident led to the institution of high-altitude airways and positive control by en route ground controllers.
Evacuation.
Canyon tourists and residents of Supai, a town located in the bottom of the canyon, were evacuated from the Supai area on August 17–18, 2008 due to a break in the earthen Redlands Dam, located upstream of Supai, after a night of heavy rainfall. Evacuees were taken to Peach Springs, Arizona. More heavy rains were expected and a flash flood warning was put into effect, necessitating the evacuation, according to the Grand Canyon National Park Service. The floods were significant enough to attract coverage from international media.
Legal issues.
Environmental.
The Havasupai Native American tribe and a coalition of environmental groups sued the federal government in 2012 over what they argue is an outdated environmental review from 1986. The uranium will be on standby until December 2014.
See also.
South Kaibab Trail at Cedar Ridge

</doc>
<doc id="46991" url="http://en.wikipedia.org/wiki?curid=46991" title="Bryce Canyon National Park">
Bryce Canyon National Park

Bryce Canyon National Park is a National Park located in southwestern Utah in the United States. The major feature of the park is Bryce Canyon, which despite its name, is not a canyon, but a collection of giant natural amphitheaters along the eastern side of the Paunsaugunt Plateau. Bryce is distinctive due to geological structures called "hoodoos", formed by frost weathering and stream erosion of the river and lake bed sedimentary rocks. The red, orange, and white colors of the rocks provide spectacular views for park visitors. Bryce sits at a much higher elevation than nearby Zion National Park. The rim at Bryce varies from 8000 to.
The Bryce Canyon area was settled by Mormon pioneers in the 1850s and was named after Ebenezer Bryce, who homesteaded in the area in 1874. The area around Bryce Canyon became a National Monument in 1923 and was designated as a National Park in 1928. The park covers 35835 acre and receives relatively few visitors compared to Zion National Park and the Grand Canyon, largely due to its remote location.
Geography and climate.
Bryce Canyon National Park is located in southwestern Utah about 50 mi northeast of and 1000 ft higher than Zion National Park. The weather in Bryce Canyon is therefore cooler, and the park receives more precipitation: a total of 15 to per year. Yearly temperatures vary from an average minimum of 9 °F in January to an average maximum of 83 °F in July, but extreme temperatures can range from -30 to. The record high temperature in the park was 98 F on July 14, 2002. The record low temperature was -28 F on December 10, 1972.
The national park lies within the Colorado Plateau geographic province of North America and straddles the southeastern edge of the Paunsaugunt Plateau west of the Paunsaugunt Fault ("Paunsaugunt" is Paiute for "home of the beaver"). Park visitors arrive from the plateau part of the park and look over the plateau's edge toward a valley containing the fault and the Paria River just beyond it ("Paria" is Paiute for "muddy or elk water"). The edge of the Kaiparowits Plateau bounds the opposite side of the valley.
Bryce Canyon was not formed from erosion initiated from a central stream, meaning it technically is not a canyon. Instead headward erosion has excavated large amphitheater-shaped features in the Cenozoic-aged rocks of the Paunsaugunt Plateau. This erosion exposed delicate and colorful pinnacles called hoodoos that are up to 200 ft high. A series of amphitheaters extends more than 20 mi north-to-south within the park. The largest is Bryce Amphitheater, which is 12 mi long, 3 mi wide and 800 ft deep. A nearby example of amphitheaters with hoodoos in the same formation but at a higher elevation, is in Cedar Breaks National Monument, which is 25 mi to the west on the Markagunt Plateau.
Rainbow Point, the highest part of the park at 9105 ft, is at the end of the 18 mi scenic drive. From there, Aquarius Plateau, Bryce Amphitheater, the Henry Mountains, the Vermilion Cliffs and the White Cliffs can be seen. Yellow Creek, where it exits the park in the north-east section, is the lowest part of the park at 6620 ft.
History.
Native American habitation.
Little is known about early human habitation in the Bryce Canyon area. Archaeological surveys of Bryce Canyon National Park and the Paunsaugunt Plateau show that people have been in the area for at least 10,000 years. Basketmaker Anasazi artifacts several thousand years old have been found south of the park. Other artifacts from the Pueblo-period Anasazi and the Fremont culture (up to the mid-12th century) have also been found.
The Paiute Indians moved into the surrounding valleys and plateaus in the area around the same time that the other cultures left. These Native Americans hunted and gathered for most of their food, but also supplemented their diet with some cultivated products. The Paiute in the area developed a mythology surrounding the hoodoos (pinnacles) in Bryce Canyon. They believed that hoodoos were the Legend People whom the trickster Coyote turned to stone. At least one older Paiute said his culture called the hoodoos "Anka-ku-was-a-wits", which is Paiute for "red painted faces".
European American exploration and settlement.
It was not until the late 18th and the early 19th century that the first European Americans explored the remote and hard-to-reach area. Mormon scouts visited the area in the 1850s to gauge its potential for agricultural development, use for grazing, and settlement.
The first major scientific expedition to the area was led by U.S. Army Major John Wesley Powell in 1872. Powell, along with a team of mapmakers and geologists, surveyed the Sevier and Virgin River area as part of a larger survey of the Colorado Plateaus. His mapmakers kept many of the Paiute place names.
Small groups of Mormon pioneers followed and attempted to settle east of Bryce Canyon along the Paria River. In 1873, the Kanarra Cattle Company started to use the area for cattle grazing.
The Church of Jesus Christ of Latter-day Saints sent Scottish immigrant Ebenezer Bryce and his wife Mary to settle land in the Paria Valley because they thought his carpentry skills would be useful in the area. The Bryce family chose to live right below Bryce Canyon Amphitheater. Bryce grazed his cattle inside what are now park borders, and reputedly thought that the amphitheaters were a "helluva place to lose a cow." He also built a road to the plateau to retrieve firewood and timber, and a canal to irrigate his crops and water his animals. Other settlers soon started to call the unusual place "Bryce's canyon", which was later formalized into Bryce Canyon.
A combination of drought, overgrazing and flooding eventually drove the remaining Paiutes from the area and prompted the settlers to attempt construction of a water diversion channel from the Sevier River drainage. When that effort failed, most of the settlers, including the Bryce family, left the area. Bryce moved his family to Arizona in 1880. The remaining settlers dug a 10 mi ditch from the Sevier's east fork into Tropic Valley.
Creation of the park.
These scenic areas were first described for the public in magazine articles published by Union Pacific and Santa Fe railroads in 1916. People like Forest Supervisor J. W. Humphrey promoted the scenic wonders of Bryce Canyon's amphitheaters, and by 1918 nationally distributed articles also helped to spark interest. However, poor access to the remote area and the lack of accommodations kept visitation to a bare minimum.
Ruby Syrett, Harold Bowman and the Perry brothers later built modest lodging, and set up "touring services" in the area. Syrett later served as the first postmaster of Bryce Canyon. Visitation steadily increased, and by the early 1920s the Union Pacific Railroad became interested in expanding rail service into southwestern Utah to accommodate more tourists.
At the same time, conservationists became alarmed by the damage overgrazing, logging, and unregulated visitation were having on the fragile features of Bryce Canyon. A movement to have the area protected was soon started, and National Park Service Director Stephen Mather responded by proposing that Bryce Canyon be made into a state park. The governor of Utah and the Utah State Legislature, however, lobbied for national protection of the area. Mather relented and sent his recommendation to President Warren G. Harding, who on June 8, 1923 declared Bryce Canyon a national monument.
A road was built the same year on the plateau to provide easy access to outlooks over the amphitheaters. From 1924 to 1925, Bryce Canyon Lodge was built from local timber and stone.
Members of the United States Congress started work in 1924 on upgrading Bryce Canyon's protection status from a U.S. National Monument to a National Park in order to establish Utah National Park. A process led by the Utah Parks Company for transferring ownership of private and state-held land in the monument to the federal government started in 1923. The last of the land in the proposed park's borders was sold to the federal government four years later, and on February 25, 1928, the renamed Bryce Canyon National Park was established.
In 1931, President Herbert Hoover annexed an adjoining area south of the park, and in 1942 an additional 635 acres was added. This brought the park's total area to the current figure of 35835 acres. Rim Road, the scenic drive that is still used today, was completed in 1934 by the Civilian Conservation Corps. Administration of the park was conducted from neighboring Zion National Park until 1956, when Bryce Canyon's first superintendent started work.
More recent history.
The USS "Bryce Canyon" was named for the park and served as a supply and repair ship in the U.S. Pacific Fleet from September 15, 1950, to June 30, 1981.
Bryce Canyon Natural History Association (BCNHA) was established in 1961. It runs the bookstore inside the park visitor center and is a non-profit organization created to aid the interpretive, educational and scientific activities of the National Park Service at Bryce Canyon National Park. A portion of the profits from all bookstore sales are donated to public land units. 
Responding to increased visitation and traffic congestion, the National Park Service implemented a voluntary, summer-only, in-park shuttle system in June 2000. In 2004, reconstruction began on the aging and inadequate road system in the park.
Geology.
The Bryce Canyon area shows a record of deposition that spans from the last part of the Cretaceous period and the first half of the Cenozoic era. The ancient depositional environment of the region around what is now the park varied. The Dakota Sandstone and the Tropic Shale were deposited in the warm, shallow waters of the advancing and retreating Cretaceous Seaway (outcrops of these rocks are found just outside park borders). The colorful Claron Formation, from which the park's delicate hoodoos are carved, was laid down as sediments in a system of cool streams and lakes that existed from 63 to about 40 million years ago (from the Paleocene to the Eocene epochs). Different sediment types were laid down as the lakes deepened and became shallow and as the shoreline and river deltas migrated.
Several other formations were also created but were mostly eroded away following two major periods of uplift. The Laramide orogeny affected the entire western part of what would become North America starting about 70 million to 50 million years ago. This event helped to build the Rocky Mountains and in the process closed the Cretaceous Seaway. The Straight Cliffs, Wahweap, and Kaiparowits formations were victims of this uplift. The Colorado Plateaus were uplifted 16 million years ago and were segmented into different plateaus, each separated from its neighbors by faults and each having its own uplift rate. The Boat Mesa Conglomerate and the Sevier River Formation were removed by erosion following this uplift.
This uplift created vertical joints, which over time were preferentially eroded. The easily eroded Pink Cliffs of the Claron Formation responded by forming freestanding pinnacles in badlands called hoodoos, while the more resistant White Cliffs formed monoliths. The brown, pink and red colors are from hematite (iron oxide; Fe2O3); the yellows from limonite (FeO(OH)·"n"H2O); and the purples are from pyrolusite (MnO2). Also created were arches, natural bridges, walls, and windows. Hoodoos are composed of soft sedimentary rock and are topped by a piece of harder, less easily eroded stone that protects the column from the elements. Bryce Canyon has one of the highest concentrations of hoodoos of any place on Earth.
The formations exposed in the area of the park are part of the Grand Staircase. The oldest members of this supersequence of rock units are exposed in the Grand Canyon, the intermediate ones in Zion National Park, and its youngest parts are laid bare in Bryce Canyon area. A small amount of overlap occurs in and around each park.
Biology.
More than 400 native plant species live in the park. There are three life zones in the park based on elevation: The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and White Fir, along with Aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have Limber Pine and ancient Great Basin Bristlecone Pine, some more than 1,600 years old, holding on.
The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life, from birds and small mammals to foxes and occasional bobcats, cougars, and black bears. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park.
Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the Southwestern Willow Flycatcher. The Utah Prairie Dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.
About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, mountain lion, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.
Eleven species of reptiles and four species of amphibians have been found at in the park. Reptiles include the Great Basin Rattlesnake, Short-horned Lizard, Side-blotched Lizard, Striped Whipsnake, and the Tiger Salamander.
Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
Activities.
Most park visitors sightsee using the scenic drive, which provides access to 13 viewpoints over the amphitheaters. Bryce Canyon has eight marked and maintained hiking trails that can be hiked in less than a day (round trip time, trailhead): Mossy Cave (one hour, State Route 12 northwest of Tropic), Rim Trail (5–6 hours, anywhere on rim), Bristlecone Loop (one hour, Rainbow Point), and Queens Garden (1–2 hours, Sunrise Point) are easy to moderate hikes. Navajo Loop (1–2 hours, Sunset Point) and Tower Bridge (2–3 hours, north of Sunrise Point) are moderate hikes. Fairyland Loop (4–5 hours, Fairyland Point) and Peekaboo Loop (3–4 hours, Bryce Point) are strenuous hikes. Several of these trails intersect, allowing hikers to combine routes for more challenging hikes.
The park also has two trails designated for overnight hiking: the 9 mi Riggs Spring Loop Trail and the 23 mi Under-the-Rim Trail. Both require a backcountry camping permit. In total there are 50 mi of trails in the park.
More than 10 mi of marked but ungroomed skiing trails are available off of Fairyland, Paria, and Rim trails in the park. Twenty miles (32 km) of connecting groomed ski trails are in nearby Dixie National Forest and Ruby's Inn.
The air in the area is so clear that on most days from Yovimpa and Rainbow points, Navajo Mountain and the Kaibab Plateau can be seen 90 mi away in Arizona. On extremely clear days, the Black Mesas of eastern Arizona and western New Mexico can be seen some 160 mi away.
The park also has a 7.4 magnitude night sky, making it one of the darkest in North America. Stargazers can, therefore, see 7,500 stars with the naked eye, while in most places fewer than 2,000 can be seen due to light pollution, and in many large cities only a few dozen can be seen. Park rangers host public stargazing events and evening programs on astronomy, nocturnal animals, and night sky protection. The Bryce Canyon Astronomy Festival, typically held in June, attracts thousands of visitors. In honor of this astronomy festival, was named after the national park.
There are two campgrounds in the park, North Campground and Sunset Campground. Loop A in North Campground is open year-round. Additional loops and Sunset Campground are open from late spring to early autumn. The 114-room Bryce Canyon Lodge is another way to stay overnight in the park.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="46992" url="http://en.wikipedia.org/wiki?curid=46992" title="Czech literature">
Czech literature

Czech literature is the literature written by Czechs, mostly in the Czech language, although other languages like Old Church Slavonic, Latin or German have been also used, especially in the past. Non-Czech inhabitants of the Czech lands who had written in German and other languages, are usually excluded from the corpus of Czech literature, regardless of their own national self-identification. Thus Franz Kafka, for example, who wrote in German (though he was also fluent in Czech), is often considered part of Austrian or German literature.
Czech literature is divided into roughly ten main time periods: the Middle Ages; the Hussite period; the years of re-Catholicization and the baroque; the Enlightenment and Czech reawakening in the 19th century; the avantgarde of the interwar period; the years under Communism and the Prague Spring; and the literature of the post-Communist Czech Republic. Czech literature and culture played a notable role on at least two occasions when Czech society lived under oppression and little to no political activity was possible. On both of these occasions, in the early 19th century and then again in the 1960s, the Czechs used their cultural and literary effort to create political freedom and to establish a confident, politically aware nation.
Middle Ages.
Literature in the Czech lands was founded in the 8th century AD, in the kingdom of Greater Moravia. The Saints Konstantin (i.e. Cyril) and Methodius, sent by the Byzantine Emperor Michael III to complete the Christianization of the kingdom, created there the first written Slavic language, Old Church Slavonic, written in the Glagolitic alphabet. Their translations of Latin liturgy into Slavonic are the earliest surviving literature sources created in the Czech lands.
After the collapse of Greater Moravia at the end of the 9th century, the political and cultural orientation of the Bohemian lands shifted from Byzantium to Rome. Very little is known about the next two centuries of literary development - fragments of works exist, but many are simply inferred from citations in works found elsewhere. The close of the century heralded the ultimate victory of Latin over Old Church Slavonic as the official language of liturgy and culture in Moravia and Bohemia, and cultural alliance shifted from east to west. The "Legend of Christian", written in Latin verse in the latter half of the 10th century, describing the lives of Saints Ludmila and Wenceslas is the greatest surviving work; its authenticity however is under some dispute.
In the Přemyslid Bohemia of the 12th and early 13th century, all preserved literary works are written in Latin. Historical chronicles and hagiographies comprise the majority of works preserved. Bohemian hagiographies focus exclusively on Bohemian saints (Sts. Ludmila, Wenceslas, Procopius, Cyril and Methodius, and Adalbert), although numerous legends about Bohemian saints were also written by foreign authors. The most important chronicle of the period is the Chronica Boemorum (Bohemian Chronicle) by Kosmas, though it does approach its topics with then-contemporary politics in mind, and attempts to legitimize the ruling dynasty. Kosmas' work was updated and extended by several authors in the latter part of the 12th and during the 13th centuries.
During the first part of the 13th century, the Přemyslid rulers of Bohemia expanded their political and economic influence westward and came into contact with the political and cultural kingdoms of Western Europe. This cultural exchange was evident in literature through the introduction of German courtly poetry, or Minnesang, in the latter part of the 13th century. After the murder of Wenceslas III and the subsequent upheavals in the kingdom in 1306, however, the Bohemian nobles distanced themselves from German culture and looked for literature in their native language. Despite this, German remained an important literary language in Bohemia until the 19th century. This new literature in Czech consisted largely of epic poetry of two types: the legend and the knightly epic, both based on apocryphal tales from the Bible, as well as hagiographic legends of earlier periods. Prose was also first developed during this period: administrative and instructional texts, which necessitated the development of a more extensive and specialized vocabulary; the first Czech-Latin dictionaries date from this time. Extensive chronicles, of which the "Chronicle of Dalimil" and "Chronicon Aulae Regiae" (the "Zbraslav Chronicle") are the most striking examples, and artistic prose (e.g. Smil Flaška z Pardubic and Johannes von Saaz) were also written.
The Hussite Era.
The Hussite revolution of the 15th century created a definite break in the literary evolution of Czech literature and forms its own separate history within Czech literature. The main aim of this literature was to communicate and argue for a specific religious doctrine and its form was generally prose. Jan Hus' theological writing first appears at the beginning of the 15th century; he wrote first in Latin, later in Czech, and this divide remained for much of the later period: poetry and intellectual prose used primarily Latin, whereas popular prose was written in Czech or German. Hus' writings center on technical, theological questions; however, he did publish a set of his Czech sermons and created rules of orthography and grammar that would be used to create the foundations of modern Czech in the 17th and 18th centuries. Only fragments remain of the literary works of the radical Taborite faction - these were generally Latin apologia defending the Taborite doctrine (Mikuláš Biskupec z Pelhřimova, Petr Chelčický). In general, Hussite writings differed from the preceding era by their focus on social questions - their audience consisted of the lower and lower middle classes. Works defending Catholicism and attacking the Hussite utraquists were also written, one example being Jan Rokycana's works. The Hussite period for the first time also truly developed the genre of Czech religious songs as a replacement for Latin hymns and liturgy, e.g. the "Jistebnický kancionál", the Jistebnice Hymnal.
After the election of George of Poděbrady to the Czech throne following the Hussite wars, a new cultural wave swept into Bohemia. Humanism saw in the classics of antiquity an ideal for literature and culture. The main feature of the literature of this period is the competition between Catholics writing in Latin, e.g. Bohuslav Hasištejnský z Lobkovic and Jan Dubravius) and Protestants writing in Czech, e.g. Viktorin Kornel z Všehrd and Václav Hájek. New literary devices incited scholars, e.g. Veleslavín, to construct a more complex grammatical structure, based on Latin, as well as an influx of loan words. Gutenberg's printing press rendered books and pamphlets more accessible, which slowly changed literature's status in society.
Baroque.
The demise of the Czech Protestants after the Battle of the White Mountain decidedly affected Czech literary development. The forceful re-Catholicization and Germanization of Bohemia and the ensuing confiscations and expulsions virtually eliminated the Protestant middle classes and split the literature into two parts: the domestic Catholic and the émigré Protestant branches. Unlike in other European countries of the time, the nobility in Bohemia was not a part of the literary audience and thus this split of literary effort led to a certain lack of development and stagnation of Czech baroque literature in comparison to other European countries of the time, especially in genres that were written for noble courts. The largest personality of Czech evangelical baroque writing is John Comenius, who spent his youth in Bohemia but was forced into exile later in life. He was a pedagogue, theologian, reformer of education, and philosopher; his works include grammars, theoretical tracts on education, and works on theology. With his death in the late 17th century, Protestant literature in the Czech language virtually disappeared. Catholic baroque works span two types: religious poetry such as that of Adam Michna z Otradovic, Fridrich Bridel and Václav Jan Rosa, and religious prose writings (i.e. homiletic prose and hagiographies), and historical accounts (Bohuslav Balbín), as well as the Jesuit St. Wenceslas Bible.
The Enlightenment.
At the end of the 18th century, the Bohemian lands underwent a considerable change - the Habsburg emperor Josef II put an end to the feudal system and supported a new religious and ideological tolerance. Enlightened classicism emerged, which sought to apply the principles of rational science to all aspects of daily life. A national culture and literature in one's own national language began to be seen as a prerequisite for the unification of a nation. In literature, this constituted a renewed interest in prose novels (e.g. Václav Matěj Kramerius), in Czech history and in the historical development of Czech culture (e.g. Josef Dobrovský, who re-codified the grammar of the Czech language and Antonín Jaroslav Puchmayer, who systematically set out to develop a Czech poetic style). The literary audience evolved from priests and monks to the laity and general public and literature began to be seen as a vehicle of artistic expression. Bohemia and Moravia, however, remained within the sphere of Austrian and German cultural influence. The new national literature thus firstly mimicked popular German genres and would only later evolve into an independent creative effort; this was especially true for drama, e.g. Václav Kliment Klicpera.
19th century.
Pre-romanticism formed the transition between enlightened classicism and romanticism - the pre-romantics did not completely abandon the emphasis on poetic forms drawn from antiquity, but relaxed the strict separation between the genres and turned away from didactic genres toward more lyric, folk-inspired works (e.g. Ján Kollár and František Čelakovský.) It was during this period that the idea of a truly national literature and culture developed, as a rejection of Bernard Bolzano's vision of a bi-lingual and bi-cultural Czech-German state. The perhaps greatest figure of this era is Josef Jungmann, who translated many classics of world literature and spent his life establishing Czech literature as a serious, rich literature capable of great development. František Palacký and Pavel Jozef Šafárik took up the challenge of reexamining Czech history. As part of the effort to establish a pedigree for Czech literature and culture, Czech historians of the time sought evidence of heroic epics of the Middle Ages. They appeared to find such evidence in the "Rukopis královédvorský" and "Rukopis zelenohorský" (the "Dvůr Králové Manuscript" and the "Zelená Hora Manuscript", respectively), although both were later proven forgeries.
By the 1830s, the foundations of Czech literature were laid and authors now began to focus more on the artistic merits of their work and less on developing the idea of Czech literature and culture as a whole. During this time period two main types of literature were produced: Biedermeier literature, which strove to educate the readers and encourage them to be loyal to the Austro-Hungarian Empire (e.g. Karel Jaromír Erben and Božena Němcová), and romanticism, which emphasized the freedom of the individual and focused on subjectivity and the subconscious (e.g. Karel Hynek Mácha.) These authors were generally published in either newspapers or in the literary magazine "Květy" ("Blossoms") published by Josef Kajetán Tyl.
The year 1848 brought to the fore a new generation of Czech authors who followed in the footsteps of Mácha, and published their work in the new almanac "Máj" ("May") (e.g. Vítězslav Hálek, Karolina Světlá and Jan Neruda). These authors rejected the narrow ideal of a purely national culture and favored one that incorporated Czech literature into European culture and drew inspiration from the progress made outside of the Czech lands. Their work, however, also commented on the encroachment of industrialization and focused increasingly on the simple life as opposed to the unfettered romantic ideal.
The May generation was followed by the neo-romantics, who continued in the romantic tradition, but also incorporated more contemporary styles: realism, Symbolism, and decadence. Three periods are apparent: the first reacted to the disappointment due to the lack of political and social progress during the 1870s (e.g. Václav Šolc); the second was the great return to poetry, especially epic poetry (e.g. Josef Václav Sládek); and the third focused on prose (e.g. Alois Jirásek).
In conversation with the neo-romantics, the next generation of authors leaned toward realism and naturalism, the ordinary and banal. They favored contemporary subjects over historical ones, and sought to deemphasize the personal voice of the author in comparison to the often highly colored speech of the characters. Two main topics were of interest: the exploration of the Czech village and the extent to which it remained an oasis of good morals (Jan Herben, Karel Václav Rais, Alois Mrštík); and Prague, especially the life of the lower classes (Ignát Herrman, Karel Matěj Čapek Chod).
The last literary generation of the 19th century signaled a decided break with the past and the advent of modernism - after the wave of optimism in the wake of the French Revolution at the beginning of the century, the lack of progress in implementing these ideals of freedom and brotherhood led to both a skepticism toward the possibility of ever achieving these ideals, and renewed efforts to do so. The common link between authors of this generation is their adherence to a particular style over their own voices, and their often very critical perspectives on the work of the previous generations. The modernists also inaugurated the cult of the artist, and this period saw the birth of the literary critic as an independent profession, as an ally of the artist, helping to both define and present work to the public (František Xaver Šalda). Notable poets of this period drew on the works and translations of the poet Jaroslav Vrchlický and include, among others, Josef Svatopluk Machar, Antonín Sova, Otokar Březina, and Karel Hlaváček); prose authors include Vilém Mrštík, Růžena Svobodová, and Josef Karel Šlejhar.
20th century.
The turn of the 20th century marked a profound shift in Czech literature — after nearly a century of work, literature finally freed itself from the confines of needing to educate and serve the nation and spread Czech culture, and became literature simply for the sake of art. The orientation toward France, Northern Europe, and Russia intensified, and new demands were laid on the cultural knowledge of authors and their audience.
The new generation of poets distanced themselves from both the neo-romantics and the modernists: led by S. K. Neumann, their work focused on concrete reality, free of any pathos, or complicated symbolism. Many of the new poets (Karel Toman, Fráňa Šrámek, Viktor Dyk, František Gellner, Petr Bezruč) allied themselves with anarchism and the women's movement, although this influence waned throughout the decade. In prose, the work of the modernist generation was only now coming into its own, but the different stylistic waves that affected their prose are also evident in the work of the new generation — naturalism (A. M Tilšchová); impressionism (Šrámek, Gellner, Jiří Mahen, Jan Opolský, Rudolf Těsnohlídek); the Vienna Secession (Růžena Svobodová, Jan Karásek).
After their rebellious first decade, the new generation of poets (Toman, Neumann, Šrámek) turned toward nature and life in their work. This decade also marked the return of Catholic authors (Josef Florian, Jakub Deml, Jaroslav Durych, Josef Váchal) and the first entrance of the avant-garde into Czech literature, seeking to document the rapid changes in society and modernization. The first avantgarde style was neoclassicism, which soon gave way to cubism, futurism, and civilism (S. K. Neumann, the young brothers Čapek).
World War I brought with it a wave of repression of the newly emergent Czech culture, and this meant a return to the past, to traditional Czech values and history: the Hussites and the Awakening. The war, however, also precipitated a crisis of values, of faith in progress, religion, and belief, which found outlet in expressionism (Ladislav Klíma, Jakub Deml, Richard Weiner), civilism (Čapek brothers) and visions of a universal brotherhood of mankind (Ivan Olbracht, Karel Matěj Čapek Chod, F. X. Šalda).
The interwar period, coinciding with the First Republic, is one of the apogees of Czech literature — the new state brought with it a plurality of thinking, religion, and philosophy, leading to a great flowering of literature and culture. The first major theme of the interwar period was the war — the inhumanity, violence, and terror, but also the heroic actions of the Czech Legion (Rudolf Medek, Josef Kopta, František Langer, Jaroslav Hašek). A new generation of poets ushered in the return of the avantgarde: poetry of the heart (early Jiří Wolker, Zdeněk Kalista) and naivism (Čapek brothers, Josef Hora, Jaroslav Seifert, and S. K. Neumann). The avantgarde soon split, however, into the radical proletarian socialist and communist authors (Wolker, Seifert, Neumann, Karel Teige, Antonín Matěj Píša, Hora, Jindřich Hořejší), the Catholics (Durych, Deml), and to the centrists (brothers Čapek, Dyk, Fischer, Šrámek, Langer, Jan Herben). A specifically Czech literary style, poetism, was developed by the group Devětsil (Vítězslav Nezval, Jaroslav Seifert, Konstantin Biebl, Karel Teige), which argued that poetry should pervade everyday life, that poetry is inseparable from daily life, that everyone is a poet. Prose of the interwar period distanced itself even more from the traditional, single perspective prose of the previous century, in favor of multiple perspectives, subjectivity, and fractured narratives. Utopian and fantastic literature came into the forefront (Jan Weiss, Karel Čapek, Eduard Bass, Jiří Haussmann), as well as the genres of documentary prose, which sought to paint as accurate a picture of the world as possible (Karel Čapek, Egon Erwin Kisch, Jiří Weil, Rudolf Těsnohlídek, Eduard Bass, Jaromír John, Karel Poláček); lyrical, imaginative prose that allied itself with the poetic poetry of the time (Karel Konrád, Jaroslav Jan Paulík, Vladislav Vančura); and Catholically-oriented prose (Jaroslav Durych, Jan Čep, Jakub Deml). The drama of the time also followed the same stylistic evolution as poetry and prose — expressionism, followed by a return to realistic, civilian theater (František Langer, Karel Čapek). Along with avantgarde poetry, avantgarde theater also flourished, focusing on removing the barriers between actors and audience, breaking the illusion of the unity of a theatrical work (Osvobozené divadlo, Jiří Voskovec and Jan Werich).
After the heady optimism of the 1920s, the 1930s brought with them an economic crisis, which helped spur a political crisis: both the left (Communist) and right (anti-German and fascist) parties radicalized and threatened the stability of the democracy. This led the authors of the time to focus on public matters and spirituality; Catholicism gained in importance (Kalista, Karel Schulz, Halas, Vančura, Durych). Changes were apparent first in poetry: the new generation of poets (Bohuslav Reynek, Vilém Závada, František Halas, Vladimír Holan, Jan Zahradníček) began as poetists, but their work is much darker, full of images of death and fear. The older avantgarde (Teige, Nezval) also turned away from poetism to surrealism, and a third group (Hora, Seifert, František Hrubín) turned instead to lyricism, to quiet, memory-filled poetry. Prose, after the years of realistic journalism, turned to epics, existential novels, and subjective perspectives. Folk-inspired ballads (Josef Čapek, K. Čapek, Vančura, Ivan Olbracht), social-themed novels (Olbracht, Vančura, Poláček, Marie Majerová, Marie Pujmanová), and psychological novels (Jarmila Glazarová, Egon Hostovský, Jaroslav Havlíček) appeared. During this period, Karel Čapek wrote his most politically charged (and well-known) plays in response to the rise of fascist dictators. After the Munich Agreement in 1938, literature once more mirrored the current political present and called for national solidarity and a return to the past.
The German protectorate and World War II left its mark on Czech literature — many of the authors of the interwar generations did not survive or went into exile. During 1938–1940, society was still relatively free, but during 1941, most of the free newspapers, magazines, and publishers were shut down, and authors were silenced. WWII thus marks the origin of the 3-way split of literature that continued throughout the socialist years until 1989: domestic published, domestic illegal, and exile literature. As a result of the war, all forms of literature turned even more toward tradition and history: poetry became more subdued, and greater emphasis was laid upon language as an expression of national identity (Hora, Halas, Seifert, Nezval), and on spirituality and religious values (Hrubín, Závada, Zahradníček, Holan). The same occurred in prose: gone were the experimental works of the interwar period, but the social and psychological novel (Václav Řezáč, Vladimír Neff, Miloš Václav Kratochvíl) remained. The historical novel marked a new resurgence (Kratochvíl, Vančura, Durych, Schulz) as a way to write about the present while cloaking it in historical novels, as did prose inspired by folk tales and folk culture (Josef Štefan Kubín, Jan Drda, Vančura, Jaromír John, Zdeněk Jirotka). The generation of authors that debuted during the war and shortly afterwards (Jiří Orten, Group 42) all shared a similar harrowing experience of the war; their works all bear the hallmark of tragedy, existentialist thought, and the focus on the person as an isolated being.
Czech postwar literature is tightly intertwined with the political state of postwar Czechoslovakia; as during the war, literature broke apart into three main branches: domestic published, domestic illegal, and exile literature. Literature under the communist regime became the refuge of freedom and democracy, and literary works and authors were valued not only for their literary merits, but also for their struggle against the regime. The literature of the entire postwar period thus enjoyed great attention, despite its often precarious position. During the first three years after the end of the war (1945–1948), however, literature maintained a certain degree of freedom, although the strengthening of the extreme left gradually pushed out of the public sphere first the Catholic authors (Deml, Durych, Čep, Zahradníček), then the moderate Communists.
1948 brought the ultimate victory of the Communists, and the subsequent end of civil freedoms — any literature contrary to the official perspective was banned and the authors persecuted. The official literary style became socialist realism and all avantgarde leanings were suppressed. Many authors went into exile — to Germany, the U.S., the Vatican. Of those that stayed, many chose to write in secret and remain unpublished (the surrealists (Zbyněk Havlíček, Karel Hynek), Holan, Zahradníček, Jiří Kolář, Josef Jedlička, Jan Hanč, Jiřina Hauková, Josef Škvorecký, Egon Bondy, Jan Zábrana, Bohumil Hrabal). Most of their works were published only during the 1960s and 1990s.
Only at the end of the 1950s did the tight censorial control begin to ease — some poets were allowed to publish again (Hrubín, Oldřich Mikulášek, Jan Skácel) and a new literary group formed around the magazine "Květen", striving to break the hold of socialist realism (Miroslav Holub, Karel Šiktanc, Jiří Šotola). Prose lagged behind poetry for much of the period, with the exception of Edvard Valenta and Josef Škvorecký. Shorter works, such as the short story also became popular again.
The 1960s brought with them the beginnings of reform efforts in the Communist party, and the subsequent liberalization of literature and increasing prestige of authors. Beginning with 1964, literature began to broaden in scope beyond the officially approved style. In poetry, intimate lyricism became popular (Vladimír Holan), as well as epic poetry (Karel Šiktanc, Hrubín), and the realism of Group 42. In prose, new authors abandoned polemics about socialism and instead turned toward personal and civic morality (Jan Trefulka, Milan Kundera, Ivan Klíma, Pavel Kohout), the theme of war and occupation (Jiří Weil, Arnošt Lustig), especially the fate of Jews. Bohumil Hrabal became the most prominent of the contemporary prose authors, with his works full of colloquialisms and non-traditional narrative structures, and the absence of official moral frameworks. Toward the end of the decade, novels of disillusionment, skepticism, and a need to find one's place in the world and history begin to appear (Vaculík, M. Kundera, Hrubín), as do modern historical novels (Oldřich Daněk, Jiří Šotola, Vladimír Körner, Ota Filip). The 1960s also brought the debuts of a new generation of authors who grew up during the excesses of Stalinism, and thus had no ideals about world utopias — their works dealt not with changing the world, but with living in it: authenticity, responsibility both moral and literary. These included the poets Jiří Gruša, Josef Hanzlík, Antonín Brousek, Jiří Kuběna, and playwrights Ivan Vyskočil, Jiří Šlitr, Václav Havel, Milan Uhde, Josef Topol. The close of the reform years also saw a return to experiments: surrealism (Milan Nápravník, Vratislav Effenberger), nonsense poetry (Emanuel Frynta), experimental poetry (Josef Hiršal, Bohumila Grögerová, Emil Juliš), abstract poetry and dada (Ladislav Novák), gritty realistic prose (Jan Hanč, Vladimír Páral) and ornate, symbol filled fantasy (Věra Linhartová). The era of literary freedom and experiments, which reached its apogee during the Prague Spring of 1968, came to an abrupt end the same summer, with the Soviet invasion and subsequent "normalization."
Normalization reinstated the severe censorship of the 1950s, shut down most of the literary magazines and newspapers, and silenced authors who did not conform. More than ever before, literature split into the legal, illegal, and exile branches. Many authors fled to the U.S. and Canada (Josef Škvorecký), Germany (Peroutka), Austria (Kohout), France (M. Kundera), but they generally did not fare much better than their contemporaries in Czechoslovakia, largely due to the absence of a readership. Their works became better known only through translations. The work of experimental, avantgarde authors who continued to publish as "official" authors generally shrank in quality, conformed to the official dogma, although in comparison to the 1950s, the literature was less rigid, less wooden. On the border between official and unofficial literature stood authors of historical novels (Korner, Karel Michal), and well as Bohumil Hrabal and Ota Pavel. Seifert, Mikulášek, Skácel were all also barred from publishing; their work was published as samizdat, small underground presses that hand-published much of the work of the underground, illegal authors. Ludvík Vaculík, Jan Vladislav, and Václav Havel and Jan Lopatka organized the largest samizdat editions. It was many of these illegal authors who signed Charta 77 and were jailed for doing so. Samizdat literature again returned to Catholicism, to memoirs and diaries of daily life (Vaculík). Memory and history were also chief motifs of samizdat literature (Karel Šiktanc, Jiřina Hauková), as were brutally honest, factual testimonials of daily life (Ivan Martin Jirous). The new literary generation of the 1980s was marked by the need to rebel, to act outside of the bounds of society — their work draws on the war generation (Group 42), and is often brutal, aggressive, and vulgar (Jáchym Topol, Petr Placák, Zuzana Brabcová); postmodernism also influenced literature as a whole (Jiří Kratochvil, Daniela Hodrová).
The fall of communism in 1989 marked another break in Czech literature — plurality and freedom returned. The works of many of the illegal and exiled authors working under the communist regime were published for the first time (for instance Jan Křesadlo and Ivan Blatný) and many of them returned to public life and publishing. Although some critics would say that contemporary Czech literature (since 1989) is relatively marginalised in comparison with Czech film-making, writers such as Petr Šabach, Ivan Martin Jirous, Jáchym Topol, Miloš Urban, Patrik Ouředník, and Petra Hůlová are public figures and sell books in large numbers. Contemporary Czech poetry, in Petr Borkovec can boast a poet of European standing.

</doc>
<doc id="46993" url="http://en.wikipedia.org/wiki?curid=46993" title="Jaroslav Foglar">
Jaroslav Foglar

Jaroslav Foglar (6 July 1907 - 23 January 1999) was a famous Czech author who wrote many novels about youths (partly also about Boy Scouts movement) and their adventures in nature and dark city streets.
Early life.
Foglar was born and grew up in Prague, capital of Bohemia. Because his father died prematurely he was brought up in rather poor material conditions by his mother. He was strongly influenced by romantic parts of Prague. All of the fictional towns in his novels are more or less derived from Prague. During the 1920s, Foglar was strongly influenced by German independent Wandervogel movement as well as Scout movement led by Antonín Benjamin Svojsík under Czech name Junák.
Writer and editor career, prohibited writer and the end of life.
During the 1930s and 1940s, Foglar worked as a magazine editor in one of the largest Prague publishing houses, Melantrich. He edited several journals for youths:
and he wrote articles for other journals including the "Skaut", "Sluníčko", "ABC", and the "Tramp".
After the Communist coup in 1948 Foglar was kicked out of the publishing house, his magazines were liquidated and his books prohibited, as was the Scout movement and independent youth clubs. For many years he worked as a tutor in boarding schools and youth homes. During the fall of censorship at the end of the 1960s, he published some new books and re-editions of the older ones. After Soviet occupation of Czechoslovakia his books were once again banned until 1989.
Foglar lived with his mother caring for her until her death in high age and never married.
Scout versus Youth Club movement.
However Foglar worked as Scout leader, his relation to the Scout movement was not simple. He basically pictured the boy scouts only in few of his novels (especially Pod junackou vlajkou a Devadesatka pokracuje), preferring to write mostly about his own invention, the boy clubs.
Foglar's idea of independent boy clubs is basically derived from German Wandervogel movement. As editor of Mlady Hlasatel, Foglar systematically build clubbist ideology (based on friendship, good deeds, personal sacrifice, love to the nature, etc.) on some and traditions and own terminology. Clubs were small groups between 4 and 8 youths. Some of them were informally led by young men few years older than other youths, like Rikitan in novel Hosi od Bobri reky or by best of the youths - like 'exemplary youth' Mirek Dusin of Rychle Sipy Club. Having Foglar's novels and his magazine articles as a pattern, many Czech youths established such a clubs. At its high tide, there were many thousand of such independent clubs, which were basically kind of Wandervogel concurrence towards the organized Scout movement.
On the other hand, when Scouts were persecuted and forbidden during the German occupation between 1938 and 1945 and during Communism between 1948 and 1989 (with short exception of renewal of Scout during 1968 and 1969), boy clubs posed excellent informal alternative of youth life based on ideas similar to those of Scouts.
Ideal friendship and Ideal of male education.
One of the key motives of Foglar's novels is the tension between the loneliness and close friendship between young male heroes. These are especially distinctive in novels 'Přístav volá', 'Když duben přichází', 'Chata v Jezerní kotlině', 'Modrá rokle' and 'Tajemná Řásnovka'. These novels are also non-scout ones, picturing independent life of youths. On the other hand, in second large group of his novels, a 'group hero' novels, the plot is based on stories of some organized group of youths, with less individual psychology and more action and adventures. The heroes are boy scouts or independent clubbists.
Homoerotic elements in his novels.
Some critics argued that Foglar's novels are crammed with covert homoerotic desire, or that the author himself was gay. Foglar was strongly influenced by German Wandervogel romantism more than the ideas of British scout movement (which emerged in Bohemian Lands during the WWI). Wandervogel movement itself had some elements of male eroticism. It can be admitted that most of the Foglar's novels include close friendships between two youths, with some exceptions in relation to the 'group-hero' novels like 'Rychle Sipy' Club and 'Devadesatka'. Foglar novels are set in an almost exclusively male world, where women are irrelevant (old grannys or small girls, often without names). This homosociality was very common in literature of the period, regardless of the sexual orientation of the author. Foglar would probably not have considered himself gay.
Books.
Foglar is also the author of a comics serial called Rychlé šípy ("Rapid Arrows"), very successful among young readers.

</doc>
<doc id="46996" url="http://en.wikipedia.org/wiki?curid=46996" title="Marcus Licinius Crassus">
Marcus Licinius Crassus

Marcus Licinius Crassus (; Latin: ; c. 115 BC – 53 BC) was a Roman general and politician who played a key role in the transformation of the Roman Republic into the Roman Empire. Amassing an enormous fortune during his life, Crassus is considered the wealthiest man in Roman history, and among the richest men in all history.
Crassus began his public career as a military commander under Lucius Cornelius Sulla during his civil war. Following Sulla's assumption of the dictatorship, Crassus amassed an enormous fortune through real estate speculation. Crassus rose to political prominence following his victory over the slave revolt led by Spartacus, sharing the Consulship with his rival Pompey the Great.
A political and financial patron of Julius Caesar, Crassus joined Caesar and Pompey in the unofficial political alliance known as the First Triumvirate. Together the three men dominated the Roman political system. The alliance would not last indefinitely due to the ambitions, egos, and jealousies of the three men. While Caesar and Crassus were lifelong allies, Crassus and Pompey disliked each other and Pompey grew increasingly envious of Caesar's spectacular successes in the Gallic Wars. The alliance was re-stabilized at the Lucca Conference in 56 BC, after which Crassus and Pompey again served jointly as Consuls. Following his second Consulship, Crassus was appointed as the Governor of Roman Syria. Crassus used Syria as the launchpad for a military campaign against the Parthian Empire, Rome's long-time Eastern enemy. Crassus' campaign was a disastrous failure, resulting in his defeat and death at the Battle of Carrhae.
Crassus' death permanently unraveled the alliance between Caesar and Pompey. Within four years of Crassus' death, Caesar would cross the Rubicon and begin a civil war against Pompey and the Optimates.
Family and background.
Marcus Licinius Crassus was the second of three sons born to the eminent senator and "vir triumphalis" P. Licinius Crassus (consul 97, censor 89 BC). This line was not descended from the Crassi Divites, although often assumed to be. The eldest brother Publius (born c.116 BC) died shortly before the Italic War and Marcus took the brother's wife as his own. His father and the youngest brother Gaius took their own lives in Rome in winter 87–86 BC to avoid capture when he was being hunted down by the Marians following their victory in the "bellum Octavianum".
There were three main branches of the house of Licinia Crassi in the 2nd and 1st centuries BC, and many mistakes in identifications and lines have arisen owing to the uniformity of Roman nomenclature, erroneous modern suppositions, and the unevenness of information across the generations. In addition the "Dives" cognomen of the Crassi Divites means rich or wealthy, and since Marcus Crassus the subject here was renowned for his enormous wealth this has contributed to hasty assumptions that his family belonged to the Divites. But no ancient source accords him or his father the Dives cognomen, while we are explicitly informed that his great wealth was acquired rather than inherited, and that he was raised in modest circumstances.
Crassus' homonymous grandfather, M. Licinius Crassus (praetor c.126 BC), was facetiously given the Greek nickname Agelastus (the grim) by his contemporary Gaius Lucilius, the famous inventor of Roman satire, who asserted that he smiled once in his whole life. This grandfather was son of P. Licinius Crassus (consul 171 BC). The latter's brother C. Licinius Crassus (consul 168 BC) produced the third line of Licinia Crassi of the period, the most famous of whom was Lucius Licinius Crassus, the greatest Roman orator before Cicero and the latter's childhood hero and model. Marcus Crassus was also a talented orator and one of the most energetic and active advocates of his time.
History.
Youth and the First Civil War.
After the Marian purges and the sudden death subsequently of Gaius Marius, the surviving consul Lucius Cornelius Cinna (better-known as father-in-law of Julius Caesar) imposed proscriptions on those surviving Roman senators and equestrians who had supported Lucius Cornelius Sulla in his 88 BC march on Rome and overthrow of the traditional Roman political arrangements.
Cinna's proscription forced Crassus to flee to Hispania. After Cinna's death in 84 BC, Crassus went to the Roman province of Africa where adherents of Sulla were gathering. When Sulla invaded Italy after returning from partial successes in the inconclusive Second Mithridatic War, Crassus joined Sulla and Metellus Pius, Sulla's closest ally. He was given command of the right wing in the Battle of the Colline Gate when the remaining Marian adherents and the surviving Samnites marched on Rome in a last-ditch bid to oust Sulla from Rome. The Colline Gate was one of the entrances into Rome through the Servian Walls; Crassus and his troops ensured Sulla's victory, including destruction of the surviving Samnite troops and any other military opposition.
Rise to power and wealth.
Marcus Licinius Crassus' next concern was to rebuild the fortunes of his family, which had been confiscated during the Marian-Cinnan proscriptions. According to Plutarch's "Life of Crassus", Crassus made most of his fortune through "rapine and fire". Sulla's proscriptions, in which the property of his victims was cheaply auctioned off, found one of the greatest acquirers of this type of property in Crassus: indeed, Sulla was especially supportive of this because he wished to spread around the blame as much as possible, among those unscrupulous to be glad to do so. Sulla's proscriptions ensured that his survivors would recoup their lost fortunes from the fortunes of wealthy adherents to Gaius Marius or Lucius Cornelius Cinna. Proscriptions meant that their political enemies lost their fortunes and their lives; that their female relatives (notably, widows and widowed daughters) were forbidden to remarry; and that in some cases, their families' hopes of rebuilding their fortunes and political significance were destroyed. Crassus is said to have made part of his money from proscriptions, notably the proscription of one man whose name was not initially on the list of those proscribed but was added by Crassus who coveted the man's fortune. Crassus's wealth is estimated by Pliny at approximately 200 million sestertii. Plutarch says the wealth of Crassus increased from less than 300 talents at first to 7,100 talents, or close to $8.4 Billion USD today, accounted right before his Parthian expedition, most of which Plutarch declares Crassus got "by fire and rapine, making his advantage of public calamities".
Some of Crassus' wealth was acquired conventionally, through traffic in slaves, production from silver mines, and speculative real estate purchases. Crassus tended to specialize in deals involving proscribed citizens and especially and notoriously purchasing during fires or structural collapse of buildings. When buildings were burning, Crassus and his purposely-trained crew would show up, and Crassus would offer to purchase the presumably doomed property and perhaps neighboring endangered properties from their owners for speculatively low sums; if the purchase offer was accepted, Crassus would then use his army of some 500 slaves which he purchased due to their knowledge of architecture and building to put the fire out, sometimes before too much damage had been done: otherwise Crassus would use his crews to rebuild. If his purchase offers were not accepted, then Crassus would not engage in firefighting. Crassus's slaves employed the Roman method of firefighting—destroying the burning building to curtail the spread of the flames. Similar methods were used by Crassus in the common event of the collapse of the large Roman buildings known as "insulae", which were notorious for their poor construction and unsafe conditions. Crassus was happy to cheaply construct new insulae using his slave labour force, in place of the old insulae which had collapsed and/or burned; however, he was known for his raising of rents rather than for his erection of improved residential structures.
Crassus was kinsman to Licinia, a Vestal Virgin, whose valuable property he coveted. Plutarch says: "And yet when he was further on in years, he was accused of criminal intimacy with Licinia, one of the vestal virgins and Licinia was formally prosecuted by a certain Plotius. Now Licinia was the owner of a pleasant villa in the suburbs which Crassus wished to get at a low price, and it was for this reason that he was forever hovering about the woman and paying his court to her, until he fell under the abominable suspicion. And in a way it was his avarice that absolved him from the charge of corrupting the vestal, and he was acquitted by the judges. But he did not let Licinia go until he had acquired her property."
After rebuilding his fortune, Crassus' next concern was his political career. As an adherent of Sulla, and the wealthiest man in Rome, and a man who hailed from a line of consuls and praetors, Crassus' political future was apparently assured. His problem was that despite his military successes, he was eclipsed by his contemporary Pompey the Great who blackmailed the dictator Sulla into granting him a triumph for victory in Africa over a rag-tag group of dissident Romans; a first in Roman history on a couple of counts. First, Pompey was not even a praetor, on which grounds a triumph had been denied in 206 BC to the great Scipio Africanus, who had just defeated Hannibal's brother Hasdrubal in Spain and brought Rome the entire province (Hispania). Second, Pompey had defeated fellow Romans; however, a quasi-precedent had been set when the consul Lucius Julius Caesar (a relative of Gaius Julius Caesar) had been granted a triumph for a small victory over Italian (non-Roman) peoples in the Social War. Pompey's triumph was the first granted to any Roman for defeating another Roman army. Crassus' rivalry with Pompey and his envy of Pompey's triumph would influence his subsequent career.
Crassus and Spartacus.
Crassus was rising steadily up the cursus honorum, the sequence of offices held by Roman citizens seeking political power, when ordinary Roman politics were interrupted by two events – first, the Third Mithridatic War, and second, the Third Servile War, which was the organized two-year rebellion of Roman slaves under the leadership of Spartacus (from the Summer of 73 BC to the Spring, 71 BC). In response to the first threat, Rome's best general, Lucius Licinius Lucullus (consul in 74 BC), was sent to defeat Mithridates, followed shortly by his brother Varro Lucullus (consul in 73 BC, whose daughter Tertulla later became his wife). Meanwhile, Pompey was fighting in Hispania against Quintus Sertorius, the last effective Marian general, without notable advantage. Pompey succeeded only when Sertorius was assassinated by one of his own commanders. The only source to mention Crassus holding the office of "praetor "is Appian, and the date appears to be in 73 or possibly 72 BC.
The Senate did not initially take the slave rebellion seriously, until they believed Rome itself was under threat. Crassus offered to equip, train, and lead new troops, at his own expense, after several legions had been defeated and their commanders killed in battle or taken prisoner. Eventually, Crassus was sent into battle against Spartacus by the Senate. At first he had trouble both in anticipating Spartacus' moves and in inspiring his army and strengthening their morale. When a segment of his army fled from battle, abandoning their weapons, Crassus revived the ancient practice of decimation – i.e., executing one out of every ten men, with the victims selected by drawing lots. Plutarch reports that "many things horrible and dreadful to see" occurred during the infliction of punishment, which was witnessed by the rest of Crassus' army. Nevertheless, according to Appian, the troops' fighting spirit improved dramatically thereafter, since Crassus had demonstrated that "he was more dangerous to them than the enemy."
Afterwards, when Spartacus retreated to the Bruttium peninsula in the southwest of Italy, Crassus tried to pen up his armies by building a ditch and a rampart across an isthmus in Bruttium, "from sea to sea." Despite this remarkable feat, Spartacus and part of his army still managed to break out. On the night of a heavy snowstorm, they sneaked through Crassus' lines and made a bridge of dirt and tree branches over the ditch, thus escaping.
Some time later, when the Roman armies led by Pompey and Varro Lucullus were recalled to Italy in support of Crassus, Spartacus decided to fight rather than find himself and his followers trapped between three armies, two of them returning from overseas action. In this last battle, the Battle of the Siler River, Crassus gained a decisive victory, and captured six thousand slaves alive. During the fighting, Spartacus attempted to kill Crassus personally, slaughtering his way toward the general's position, but he succeeded only in killing two of the centurions guarding Crassus. Spartacus himself is believed to have been killed in the battle, although his body was never recovered. The six thousand captured slaves were crucified along the Via Appia by Crassus' orders. At his command, their bodies were not taken down afterwards but remained rotting along Rome's principal route to the South. This was intended as an object lesson to anyone who might think of rebelling against Rome in the future, particularly of slave insurrections against their owners and masters, the Roman citizens.
Crassus effectively ended the Third Servile War in 71 BC; however, his political nemesis in the other faction of the aristocratic party, Pompey, who had arrived with his veteran troops from Hispania (Spain) in time merely for a mop up operation against the disorganized and defeated fugitives who had scattered after the final battle, unfairly received credit for the final victory, writing a letter to the Senate, in which he argued that Crassus had merely defeated some slaves, while Pompey had won the war (referring also to the successfully concluded Spanish civil war, a success which Pompey also questionably claimed credit for). This caused much strife between Pompey and Crassus. Crassus was honored only with an Ovation (originally a sheep sacrifice, which was much less an honor than was the Triumph), even though the danger to Rome and the destruction to Roman lives and property merited much more, considered purely from a military viewpoint; however, as Plutarch eagerly and unhesitatingly points out, according to an ancient prejudice against slaves, even an Ovation was unseemly, according to ancient tradition: in Plutarch's opinion, it was a shameful thing for a free man to claim any honor from battling slaves; instead he retroactively recommended that if Crassus had to sully himself by performing such a duty, he should rather have done his job and then kept quiet about having done his duty, rather than wanting to brag about it, and unreasonably demanding the honor of a Triumph, something which by ancient tradition up to this point been reserved for a general whose military victories had led to significant gains of additional territory for his country. As a result of his thwarted hopes for a Triumph, together with the addition of the humiliating remarks made in the presence of the aristocratic senators, Crassus' animosity towards his political enemy Pompey increased.
Nevertheless, Crassus was elected consul for 70 BC, alongside Pompey. In that year, Crassus displayed his wealth by public sacrifices to Hercules and entertained the populace at 10,000 tables and distributing sufficient grain to last each family three months, an act which had the additional ends of performing a previously made religious vow of a tithe to the god Hercules and also to gain support among the members of the popular party.
Alliance with Pompey and Caesar.
In 65 BC, Crassus was elected censor with another conservative Quintus Lutatius Catulus (Capitolinus), himself son of a consul. During that decade, Crassus was Julius Caesar's patron in all but name, financing Caesar's successful election to become Pontifex Maximus, Caesar had formerly held the #2 post as the priest of Jupiter or flamen dialis, but had been deprived of office by Sulla. Crassus also supported Caesar's efforts to win command of military campaigns. Caesar's mediation between Crassus and Pompey led to the creation of the First Triumvirate in 60/59 BC, the coalition of Crassus, Pompey, and Caesar (by now consul in 59). This coalition would last until Crassus' own death.
In 55 BC, after the Triumvirate met at the Lucca Conference, Crassus was again consul with Pompey, and a law was passed assigning the provinces of the two Hispanias and Syria to Pompey and Crassus respectively for five years.
Syrian governorship and death.
Crassus received Syria as his province, which promised to be an inexhaustible source of wealth. It may have been had he not also sought military glory and crossed the Euphrates in an attempt to conquer Parthia. Crassus attacked Parthia not only because of its great source of riches, but because of a desire to match the military victories of his two major rivals, Pompey the Great and Julius Caesar. The king of Armenia, Artavazdes II, offered Crassus the aid of nearly forty thousand troops (ten thousand cataphracts and thirty thousand infantrymen) on the condition that Crassus invaded through Armenia so that the king could not only maintain the upkeep of his own troops but also provide a safer route for his men and Crassus'. Crassus refused, and chose the more direct route by crossing the Euphrates. His legions were defeated at Carrhae (modern Harran in Turkey) in 53 BC by a numerically inferior Parthian force. Crassus' legions were mainly infantry men and were not prepared for the type of swift, cavalry-and-arrow attack that the Parthian troops were particularly adept at. The Parthians would get within shooting range, rain a barrage of arrows down upon Crassus's troops, turn, fall back, and charge forth with another attack in the same vein. They were even able to shoot as well backwards as they could forwards, increasing the deadliness of their onslaught. Crassus refused his quaestor Gaius Cassius Longinus's plans to reconstitute the Roman battle line, and remained in the testudo formation thinking that the Parthians would eventually run out of arrows.
Subsequently Crassus' men, being near mutiny, demanded he parley with the Parthians, who had offered to meet with him. Crassus, despondent at the death of his son Publius in the battle, finally agreed to meet the Parthian general; however, when Crassus mounted a horse to ride to the Parthian camp for a peace negotiation, his junior officer Octavius suspected a Parthian trap and grabbed Crassus' horse by the bridle, instigating a sudden fight with the Parthians that left the Roman party dead, including Crassus. A story later emerged that, after Crassus' death, the Parthians poured molten gold into his mouth as a symbol of his thirst for wealth. Or, according to a popular but historically unreliable account that it was by this means that he was put to death.
The account given in Plutarch's biography of Crassus also mentions that, during the feasting and revelry in the wedding ceremony of Artavazd's sister to the Parthian king Orodes II's son and heir Pacorus in Artashat, Crassus' head was brought to Orodes II. Both kings were enjoying a performance of Euripides' Greek tragedy "The Bacchae" and a certain actor of the royal court, named Jason of Tralles, took the head and sang the following verses (also from the "Bacchae"):
 We bring from the mountain<br>A tendril fresh-cut to the palace<br>A wonderful prey.
 Crassus' head was thus used in place of a prop head representing Pentheus and carried by the heroine of the play, Agave.
Also according to Plutarch, a final mockery was made ridiculing the memory of Crassus, by dressing up a Roman prisoner, Caius Paccianus, who resembled him in appearance in women's clothing, calling him "Crassus" and "Imperator", and leading him in a spectacular show of a final, mock "triumphal procession", putting to ridiculous use the traditional symbols of Roman triumph and authority.

</doc>
<doc id="46999" url="http://en.wikipedia.org/wiki?curid=46999" title="Buffer solution">
Buffer solution

A buffer solution (more precisely, pH buffer or hydrogen ion buffer) is an aqueous solution consisting of a mixture of a weak acid and its conjugate base, or vice versa. Its pH changes very little when a small or moderate amount of strong acid or base is added to it and thus it is used to prevent changes in the pH of a solution. Buffer solutions are used as a means of keeping pH at a nearly constant value in a wide variety of chemical applications. Many life forms thrive only in a relatively small pH range so they utilize a buffer solution to maintain a constant pH. One example of a buffer solution found in nature is blood.
Principles of buffering.
Buffer solutions achieve their resistance to pH change because of the presence of an equilibrium between the acid HA and its conjugate base A−.
When some strong acid is added to an equilibrium mixture of the weak acid and its conjugate base, the equilibrium is shifted to the left, in accordance with Le Chatelier's principle. Because of this, the hydrogen ion concentration increases by less than the amount expected for the quantity of strong acid added.
Similarly, if strong alkali is added to the mixture the hydrogen ion concentration decreases by less than the amount expected for the quantity of alkali added. The effect is illustrated by the simulated titration of a weak acid with pKa = 4.7. The relative concentration of undissociated acid is shown in blue and of its conjugate base in red. The pH changes relatively slowly in the buffer region, pH = pKa ± 1, centered at pH = 4.7 where [HA] = [A−]. The hydrogen ion concentration decreases by less than the amount expected because most of the added hydroxide ion is consumed in the reaction
and only a little is consumed in the neutralization reaction which results in an increase in pH.
Once the acid is more than 95% deprotonated the pH rises rapidly because most of the added alkali is consumed in the neutralization reaction.
Applications.
Buffer solutions are necessary to keep the correct pH for enzymes in many organisms to work. Many enzymes work only under very precise conditions; if the pH moves outside of a narrow range, the enzymes slow or stop working and can denature. In many cases denaturation can permanently disable their catalytic activity.
A buffer of carbonic acid (H2CO3) and bicarbonate (HCO3−) is present in blood plasma, to maintain a pH between 7.35 and 7.45.
Industrially, buffer solutions are used in fermentation processes and in setting the correct conditions for dyes used in colouring fabrics. They are also used in chemical analysis and calibration of pH meters.
The majority of biological samples that are used in research are made in buffers, especially phosphate buffered saline (PBS) at pH 7.4.
Simple buffering agents.
For buffers in acid regions, the pH may be adjusted to a desired value by adding a strong acid such as hydrochloric acid to the buffering agent. For alkaline buffers, a strong base such as sodium hydroxide may be added. Alternatively, a buffer mixture can be made from a mixure of an acid and its conjugate base. For example, an acetate buffer can be made from a mixture of acetic acid and sodium acetate. Similarly an alkaline buffer can be made from a mixture of the base and its conjugate acid.
"Universal" buffer mixtures.
By combining substances with p"K"a values differing by only two or less and adjusting the pH, a wide range of buffers can be obtained. Citric acid is a useful component of a buffer mixture because it has three p"K"a values, separated by less than two. The buffer range can be extended by adding other buffering agents.
The following two-component mixtures (McIlvaine's buffer solutions) have a buffer range of pH 3 to 8.
A mixture containing citric acid, monopotassium phosphate, boric acid, and diethyl barbituric acid can be made to cover the pH range 2.6 to 12.
Other universal buffers are Carmody buffer and Britton-Robinson buffer, developed in 1931.
Common buffer compounds used in biology.
Biological buffers cover 1.9 to 11 pH range. 
Buffer capacity.
Buffer capacity, β, is a quantitative measure of the resistance of a buffer solution to pH change on addition of hydroxide ions. It can be defined as follows.
where dn is an infinitesimal amount of added base and d(p[H+]) is the resulting infinitesimal change in the cologarithm of the hydrogen ion concentration. With this definition the buffer capacity of a weak acid, with a dissociation constant Ka, can be expressed as
where CA is the analytical concentration of the acid. pH is defined as -log10[H+].
There are three regions of high buffer capacity.
The buffer capacity of a buffering agent is at a local maximum when p[H+] = pKa. It falls to 33% of the maximum value at p[H+] = pKa ± 1 and to 10% at p[H+] = pKa ± 1.5. For this reason the useful range is approximately pKa ± 1.
Calculating buffer pH.
Monoprotic acids.
First write down the equilibrium expression.
This shows that when the acid dissociates equal amounts of hydrogen ion and anion are produced. The equilibrium concentrations of these three components can be calculated in an ICE table.
The first row, labelled I, lists the initial conditions: the concentration of acid is C0, initially undissociated, so the concentrations of A− and H+ would be zero; y is the initial concentration of "added" strong acid, such as hydrochloric acid. If strong alkali, such as sodium hydroxide, is added y will have a negative sign because alkali removes hydrogen ions from the solution. The second row, labelled C for change, specifies the changes that occur when the acid dissociates. The acid concentration decreases by an amount "-x" and the concentrations of A− and H+ both increase by an amount "+x". This follows from the equilibrium expression. The third row, labelled E for equilibrium concentrations, adds together the first two rows and shows the concentrations at equilibrium.
To find "x", use the formula for the equilibrium constant in terms of concentrations:
Substitute the concentrations with the values found in the last row of the ICE table:
Simplify to:
With specific values for C0, Ka and y this equation can be solved for x. Assuming that pH = -log10[H+] the pH can be calculated as pH = -log10(x+y).
Polyprotic acids.
Polyprotic acids are acids that can lose more than one proton. The constant for dissociation of the first proton may be denoted as "K"a1 and the constants for dissociation of successive protons as "K"a2, etc. Citric acid, H3A, is an example of a polyprotic acid as it can lose three protons.
When the difference between successive p"K" values is less than about three there is overlap between the pH range of existence of the species in equilibrium. The smaller the difference, the more the overlap. In the case of citric acid, the overlap is extensive and solutions of citric acid are buffered over the whole range of pH 2.5 to 7.5.
Calculation of the pH with a polyprotic acid requires a speciation calculation to be performed. In the case of citric acid, this entails the solution of the two equations of mass balance
CA is the analytical concentration of the acid, CH is the analytical concentration of added hydrogen ions, βq are the cumulative association constants
Kw is the constant for Self-ionization of water. There are two non-linear simultaneous equations in two unknown quantities [A3−] and [H+]. Many computer programs are available to do this calculation. The speciation diagram for citric acid was produced with the program HySS.

</doc>
<doc id="47000" url="http://en.wikipedia.org/wiki?curid=47000" title="Colocation centre">
Colocation centre

A colocation centre or colocation center (also spelled co-location, collocation, colo, or coloc) is a type of data centre where equipment, space, and bandwidth are available for rental to retail customers. Colocation facilities provide space, power, cooling, and physical security for the server, storage, and networking equipment of other firms—and connect them to a variety of telecommunications and network service providers—with a minimum of cost and complexity.
Benefits.
Colocation has become a popular option for companies with midsize IT needs—especially those in Internet related business—because it allows the company to focus its IT staff on the actual work being done, instead of the logistical support needs which underlie the work. Significant benefits of scale (large power and mechanical systems) result in large colocation facilities, typically 4500 to 9500 square metres (roughly 50,000 to 100,000 square feet).
Claimed benefits of colocation include:
Colocation facilities provide, as a retail rental business, usually on a term contract:
They also provide redundant systems for, usually, all of these features, to mitigate the problems when each inevitably fails.
Among the economies of scale which result from grouping many small-to-midsized customers together in one facility are included:
Major types of colocation customers are:
Configuration.
“Multi-tenant [colocation] providers sell to a wide range of customers, from Fortune 1000 enterprises to small- and medium-sized organizations.” “Typically the facility provides power and cooling to the space, but the IT equipment is owned by the customer. The value proposition of retail multi-tenant is that customers can retain full control of the design and management of their servers and storage, but turn over the daily task of managing data center and facility infrastructure to their multi-tenant provider.”
Building features.
Buildings with data centres inside them are often easy to recognize due to the amount of cooling equipment located outside or on the roof.
Colocation facilities have many other special characteristics:
Colocation data centres are often audited to prove that they live up to certain standards and levels of reliability; the most commonly seen systems are SSAE 16 SOC 1 Type I and Type II (formerly SAS 70 Type I and Type II) and the tier system by the Uptime Institute. For service organizations today, SSAE 16 calls for a description of its "system". This is far more detailed and comprehensive than SAS 70's description of "controls". Other data center compliance standards include HIPAA (Health Insurance Portability and Accountability Act (HIPAA) audit) and PCI DSS Standards.
Physical security.
Most colocation centres have high levels of physical security, including on-site security guards trained for Anti-Terrorism in the most extreme cases. Others may simply be guarded continuously. They may also employ CCTV.
Some colocation facilities require that employees escort customers, especially if there are not individual locked cages or cabinets for each customer. In other facilities, a PIN code or proximity card access system may allow customers access into the building, and individual cages or cabinets have locks. Biometric security measures, such as fingerprint recognition, voice recognition and "weight matching", are also becoming more commonplace in modern facilities. 'Man-traps' are also used, where a hallway leading into the data centre has a door at each end and both cannot be open simultaneously; visitors can be seen via CCTV and are manually authorized to enter.
Power.
Colocation facilities generally have generators that start automatically when utility power fails, usually running on diesel fuel. These generators may have varying levels of redundancy, depending on how the facility is built.
Generators do not start instantaneously, so colocation facilities usually have battery backup systems. In many facilities, the operator of the facility provides large inverters to provide AC power from the batteries. In other cases, the customers may install smaller UPSes in their racks.
Some customers choose to use equipment that is powered directly by 48VDC (nominal) battery banks. This may provide better energy efficiency, and may reduce the number of parts that can fail, though the reduced voltage greatly increases necessary current, and thus the size (and cost) of power delivery wiring.
An alternative to batteries is a motor generator connected to a flywheel and diesel engine.
Many colocation facilities can provide redundant, A and B power feeds to customer equipment, and high end servers and telecommunications equipment often can have two power supplies installed.
“Redundancy in IT is a system design in which a component is duplicated so if it fails there will be a backup.”
N+1, also referred to as “parallel redundant”: “The number of UPS modules that are required to handle an adequate supply of power for essential connected systems, plus one more.”
2N+1, also referred to as “system plus system”: “2 UPS systems feeding 2 independent output distribution systems.” Offers complete redundancy between sides A and B. “2(N+1) architectures fed directly to dual-corded loads provide the highest availability by offering complete redundancy and eliminating single points of failure.”
Colocation facilities are sometimes connected to multiple sections of the utility power grid for additional reliability.
Cooling.
The operator of a colocation facility generally provides air conditioning for the computer and telecommunications equipment in the building. The cooling system generally includes some degree of redundancy.
In older facilities, the cooling system capacity often limits the amount of equipment that can operate in the building, more so than the available square footage.
Internal connections.
Colocation facility owners have differing rules regarding cross connects between their customers, some of whom may be carriers. These rules may allow customers to run such connections at no charge, or allow customers to order such connections for a significant monthly fee. They may allow customers to order cross connects to carriers, but not to other customers.
Some colocation centres feature a "meet-me-room" where the different carriers housed in the centre can efficiently exchange data.
Most peering points sit in colocation centres.
Because of the high concentration of servers inside larger colocation centres, most carriers will be interested in bringing direct connections to such buildings.
In many cases, there will be a larger Internet Exchange hosted inside a colocation centre, where customers can connect for peering.
External connections.
Colocation facilities generally have multiple locations for fibre optic cables to enter the building, to provide redundancy so that communications can continue if one bundle of cables is damaged. Some also have wireless backup connections, for example via satellite.

</doc>
<doc id="47004" url="http://en.wikipedia.org/wiki?curid=47004" title="Colo (gorilla)">
Colo (gorilla)

Colo (born December 22, 1956) is a Western Gorilla widely known as the first gorilla to be born in captivity anywhere in the world and as the oldest gorilla in captivity in the world. Colo was born at the Columbus Zoo and Aquarium to Millie Christina (mother) and Baron Macombo (father). She was briefly called Cuddles before a contest was held to officially name her. Colo's name is derived from the place of her birth, Columbus, Ohio.
Life.
Colo's mother rejected her at birth, and she was hand-raised by the zookeepers like a human child, dressed in clothing and fed from a bottle. When she was two she was introduced to Bongo, a 19-month-old male from Africa, and on February 1, 1968, their first of three offspring was born, a female named Emmy, named by the zoo after the mayor of Columbus, M. E. "Jack" Sensenbrenner. The following two offspring were similarly named after awards; Oscar, born July 18, 1969, and Toni, on December 28, 1971. 
On April 25, 1979, Columbus Zoo had its first third generation birth. The infant was named Cora, short for Central Ohio Rare Ape. On January 27, 1997, Colo's great-grandson Jontu was born. A birth at the Henry Doorly Zoo made Colo a great-great-grandmother in 2003.
Although Colo did not raise any of her own offspring, she reared her twin grandsons, Macombo II and Mosuba, from birth. Colo also acted as a guardian for her grandson, named J.J. after "Jungle" Jack Hanna with whom he shares a birthday. Since that time, there have been 12 gorilla offspring in the Columbus Zoo surrogacy program.
Colo has resided at the Columbus Zoo longer than any other animal in the zoo's collection. Colo celebrated her 50th birthday in 2006 with her keeper Gregory Moore with a chimps tea party. Colo and her progeny, four of which still reside at the Columbus Zoo, comprised almost one-third of Columbus Zoo's current gorilla collection as of 2007. 
Colo is the oldest living gorilla in captivity, following the death of 55-year-old Jenny in September 2008.
Genealogy.
Listed in order of birth (within generation)
Children (2nd generation):
Grandchildren (3rd generation):
Great-grandchildren (4th generation):
Great-great-grandchildren (5th generation):

</doc>
<doc id="47005" url="http://en.wikipedia.org/wiki?curid=47005" title="Telephone company">
Telephone company

A telephone company, also known as a telco, telephone service provider, or telecommunications operator, is a kind of communications service provider (CSP) (more precisely a telecommunications service provider or TSP) that provides telecommunications services such as telephony and data communications access. Many telephone companies were at one time government agencies or privately owned but state-regulated monopolies. The government agencies are often referred to, primarily in Europe, as PTTs (postal, telegraph and telephone services).
Telephone companies are common carriers, and in the United States are also called local exchange carriers. With the advent of mobile telephony, telephone companies now include wireless carriers, or mobile network operators.
Most telephone companies now also function as internet service providers (ISPs), and the distinction between a telephone company and an ISP may disappear completely over time, as the current trend for supplier convergence in the industry continues.

</doc>
<doc id="47008" url="http://en.wikipedia.org/wiki?curid=47008" title="Elfstedentocht">
Elfstedentocht

 
The Elfstedentocht (]; Frisian: "Alvestêdetocht" [ɑlvəˈstɛːdətɔxt], English: "Eleven cities tour"), is an almost 200 km long skating tour which is held both as a speed skating match (with 300 contestants) and a leisure tour (with 16,000 skaters). It is held in the province of Friesland in the north of the Netherlands, leading past all eleven historical cities of the province. The tour is held at most once a year, only when the natural ice along the entire course is at least 15 cm thick; sometimes on consecutive years, other times with gaps that may exceed 20 years. When the ice is suitable, the tour is announced and starts within 48 hours.
Course and rules.
The tour, almost 200 km in length, follows a route along frozen canals, rivers and lakes visiting the eleven historic Frisian cities: Leeuwarden, Sneek, IJlst, Sloten, Stavoren, Hindeloopen, Workum, Bolsward, Harlingen, Franeker, Dokkum, then returning to Leeuwarden. The tour is held only if the ice is, and remains, at least 15 centimetres thick along the entire course as about 15,000 amateur skaters will take part, putting high requirements on the quality of the ice. The last tours were held in 1985, 1986 and 1997. All skaters must be members of the Association of the Eleven Frisian Cities. A starting permit is required. Skaters must collect a stamp in each city, and at three secret check points, and must finish the course before midnight.
The finishing point of the "Elfstedentocht" is a canal near Leeuwarden, called the "Bonkevaart", close to the landmark windmill, "De Bullemolen", Lekkum.
Planning and publicity.
Since the "Elfstedentocht" is such a rare event, its declaration creates excitement all over the country - in the build-up to a possible race in 2012, Mark Rutte, the Dutch Prime Minister remarked "once every 15 years our country is not governed from The Hague but by 22 district heads in Friesland. And our country is in good hands". As soon as a few days pass with sub-zero temperatures, the media start speculating about the chances for an "Elfstedentocht". The longer the freezing temperatures stay, the more intense this "Elfstedenkoorts" (eleven cities fever) gets - culminating in a national near-frenzy when it is announced that the tour will actually be held. The day before the tour many Dutch flock to Leeuwarden to enjoy the party atmosphere that surrounds the event; that evening, called the "Nacht van Leeuwarden" (Night of Leeuwarden), becomes a giant city-wide street party (Frisians, who have a reputation for surliness, are said to thaw when it freezes).
On the day of the tour most Frisians and Dutchmen either stay home to watch it on television, or find a place along the route to cheer on the skaters, either taking the day off or calling in sick for work. In February 2012, Friesland hotels were fully booked and expecting between 1.5 and 2 million visitors in expectation of a tour before it was announced, as the weather seemed suitable.
There are often points along the route where the ice is too thin to allow mass skating; they are called "kluning points" (from West Frisian klúnje), and the skaters walk on their skates to the next stretch of good ice. In 1997 ice-transplantation was re-introduced to strengthen weak places in the ice, for instance under bridges.
History.
There has been mention of skaters visiting all eleven cities of Friesland on one day since 1760. The "Elfstedentocht" was already part of Frisian tradition when, in 1890, Pim Mulier conceived the idea of an organised tour, which was first held in 1909. After this race, the (nl) (Association of the Eleven Frisian Cities) was established to organise the tours.
The winters of 1939/40, 1940/41 and 1941/42 were particularly severe, with the race being run in each of them. The 1940 race, run three months prior to the German invasion of the Netherlands, saw over 3,000 competitors start at 05:00 on 30 January, with the first five finishing at 16:34. The event dominated the front pages of Dutch newspapers.
The Elfstedentocht of 1963 became known as "The hell of '63" when only 69 of the 10,000 participants were able to finish the race, due to the extremely low temperatures, -18 °C, and a harsh eastern wind. Conditions were so horrendous that the 1963 winner, Reinier Paping, became a national hero, and the tour itself legendary.
The next Elfstedentocht after 1963 was held in 1985; times had changed. Before, one of the best methods to stay warm during the tour was to wear newspapers underneath the clothes. In the 20 years between the tours of 1963 and 1985, clothing, training methods and skates became much more advanced, changing the nature of skating.
The tour of 1985 was terminated prematurely because of thawing; as early as 22:00 in the evening skaters were taken off the ice. In 1986 the Dutch Crown Prince Willem-Alexander completed the "Elfstedentocht" under the name "W.A. van Buren", "Van Buren" being a traditional pseudonym of the Royal House.
Elfstedentocht 2012 - the race that did not happen.
The ten-day cold spell in late January and early February 2012, the 33rd such occurrence since 1901 when temperatures as low as -22.9 °C were recorded in Lelystad, heightened the expectation of a 2012 Elfstedentocht - the expected day of the event, had it taken place, was Saturday 11 February. The preparations before the race was called off on 8 February are typical of what can be expected.
On 2 February 2012, it was reported that 95% of various locks that controlled the water flow in the canals had been adjusted to maximise the ice thickness. On the same day, the Dutch meteorological institute forecast that temperatures would not rise above freezing until Wednesday 8 February at the earliest and that the thickness of the ice would be 15 cm from Tuesday 7 February until Saturday 11 February. On 3 February, the Dutch meteorological institute forecast a probable ice-thickness of 20 cm on Saturday 11 February and on 5 February they forecast an ice thickness approaching 25 cm.
On 6 February it was announced that the committee had met the previous evening for the first time in fifteen years. Although there were areas where the ice was not thick enough for the race to be held, the forecast for continuing freezing weather meant that they were optimistic that the race would be held. A press conference was held at 09:30 CET and the committee was due to meet again on 8 February. At the press conference, it was stated that in north Friesland the ice conditions were suitable to hold the race. In south west Friesland, the conditions were not so good, Stavoren being a particular problem where the ice was only 2 cm thick in places.
On 6 February a prohibition order on navigation on many of the canals in Friesland was extended in order to facilitate the growth of the ice. The following day the Dutch meteorological institute forecast that the cold spell would break on Sunday 12 February or Monday 13 February with the temperature rising above freezing point.
Late afternoon on the 8 February Wiebe Wieling, chairman of the organising committee said that the race was off - the committee had to be realistic - safety issues had made the race impossible. A ten-day cold spell was insufficient for the event; the 1997 event had taken place at the end of a 12-day cold spell.
Winners.
The time taken for the winner to complete the course is given in hours and minutes. Although temperatures were sometimes above freezing on the day of the tour, all tours were preceded by many days of sub-zero temperatures.
Women were first allowed to take part in the tour proper in 1985; before then they had to skate with the amateurs and no award was given. The women to cross the finish line first were:
(**) After shared wins in 1933 and 1940, when the front-runners decided not to compete but join hands to cross the line together, this practice was forbidden by the organisation. Jan van der Hoorn, Aad de Koning, Jeen Nauta, Maus Wijnhout and Anton Verhoeven however ignored this rule when they crossed the finish line in unison. They were not disqualified, but no winner was declared.
The eleven cities.
The course can vary slightly from race to race, depending on the quality of the ice. The cumulative distance at each checkpoint in 1997 was:
Fietselfstedentocht (Eleven cities by bicycle).
The eleven cities cycle race was first held in 1912 and developed in parallel with its skating counterpart, but unlike the skating race, has taken place on almost every year - apart from the 2001 event which was cancelled due to foot and mouth disease, it has taken place on Whit Monday every year since 1947. The event has become immensely popular and as a safety precaution it ceased to be a race but has become a tour with a maximum average speed of 25 km/h between checkpoints.
The tour, which starts and ends in Bolsward rather than Leeuwarden, is currently limited to 15,000 entrants. Between 05:00 and 08:00, entrants leave Bolsward every eight minutes in batches of about 600 and those who complete the 240 km course before midnight receive medals.
In 2013 the organisers banned Velomobiles from the event by limiting the event to two-wheeled vehicles propelled by human power.

</doc>
<doc id="47009" url="http://en.wikipedia.org/wiki?curid=47009" title="Jakko Jan Leeuwangh">
Jakko Jan Leeuwangh

Jakko Jan Leeuwangh (born 9 September 1972 in Alkmaar) is a former speed skater from the Netherlands. He finished fourth in the 1998 Olympic 1000 m event. In January 2000 he broke the 1500 m world record in Calgary, Canada, holding the record until it was broken by Lee Kyou-hyuk in March 2001.

</doc>
<doc id="47011" url="http://en.wikipedia.org/wiki?curid=47011" title="Arrhenius equation">
Arrhenius equation

The Arrhenius equation is a formula for the temperature dependence of reaction rates. The equation was proposed by Svante Arrhenius in 1889, based on the work of Dutch chemist Jacobus Henricus van 't Hoff who had noted in 1884 that Van 't Hoff's equation for the temperature dependence of equilibrium constants suggests such a formula for the rates of both forward and reverse reactions. Arrhenius provided a physical justification and interpretation for the formula. Currently, it is best seen as an empirical relationship. It can be used to model the temperature variation of diffusion coefficients, population of crystal vacancies, creep rates, and many other thermally-induced processes/reactions. The Eyring equation, developed in 1935, also expresses the relationship between rate and energy.
A historically useful generalization supported by Arrhenius' equation is that, for many common chemical reactions at room temperature, the reaction rate doubles for every 10 degree Celsius increase in temperature.
Equation.
Arrhenius' equation gives the dependence of the rate constant formula_1 of a chemical reaction on the absolute temperature formula_2 (in kelvin), where formula_3 is the pre-exponential factor (or simply the "prefactor"), formula_4 is the activation energy, and formula_5 is the universal gas constant:
Alternatively, the equation may be expressed as
The only difference is the energy units of formula_4: the former form uses energy per mole, which is common in chemistry, while the latter form uses energy per molecule directly, which is common in physics.
The different units are accounted for in using either formula_5 = gas constant or the Boltzmann constant formula_10 as the multiplier of temperature formula_2.
The units of the pre-exponential factor formula_3 are identical to those of the rate constant and will vary depending on the order of the reaction. If the reaction is first order it has the units s−1, and for that reason it is often called the "frequency factor" or "attempt frequency" of the reaction. Most simply, formula_1 is the number of collisions that result in a reaction per second, formula_3 is the total number of collisions (leading to a reaction or not) per second and formula_15 is the probability that any given collision will result in a reaction. It can be seen that either increasing the temperature or decreasing the activation energy (for example through the use of catalysts) will result in an increase in rate of reaction.
Given the small temperature range kinetic studies occur in, it is reasonable to approximate the activation energy as being independent of the temperature. Similarly, under a wide range of practical conditions, the weak temperature dependence of the pre-exponential factor is negligible compared to the temperature dependence of the formula_16 factor; except in the case of "barrierless" diffusion-limited reactions, in which case the pre-exponential factor is dominant and is directly observable.
Arrhenius plot.
Taking the natural logarithm of Arrhenius' equation yields:
This has the same form as an equation for a straight line:
So, when a reaction has a rate constant that obeys Arrhenius' equation, a plot of ln("k") versus "T" −1 gives a straight line, whose gradient and intercept can be used to determine "E"a and "A" . This procedure has become so common in experimental chemical kinetics that practitioners have taken to using it to "define" the activation energy for a reaction. That is the activation energy is defined to be (-"R") times the slope of a plot of ln("k") vs. (1/"T" )
Modified Arrhenius' equation.
The modified Arrhenius' equation makes explicit the temperature dependence of the pre-exponential factor. The modified equation is usually of the form
where "T"0 is a reference temperature and allows "n" to be a unitless power. Clearly the original Arrhenius expression above corresponds to "n" = 0. Fitted rate constants typically lie in the range -1<"n"<1. Theoretical analyses yield various predictions for "n". It has been pointed out that "it is not feasible to establish, on the basis of temperature studies of the rate constant, whether the predicted "T"½ dependence of the pre-exponential factor is observed experimentally." However, if additional evidence is available, from theory and/or from experiment (such as density dependence), there is no obstacle to incisive tests of the Arrhenius law.
Another common modification is the stretched exponential form: 
where "β" is a unitless number of order 1. This is typically regarded as a fudge factor to make the model fit the data, but can have theoretical meaning, for example showing the presence of a range of activation energies or in special cases like the Mott variable range hopping.
Theoretical interpretation of the equation.
Arrhenius' Concept of Activation Energy.
Arrhenius argued that for reactants to transform into products, they must first acquire a minimum amount of energy, called the activation energy "E"a. At an absolute temperature "T", the fraction of molecules that have a kinetic energy greater than "E"a can be calculated from statistical mechanics. The concept of "activation energy" explains the exponential nature of the relationship, and in one way or another, it is present in all kinetic theories.
The calculations for reaction rate constants involve an energy averaging over a Maxwell–Boltzmann distribution with formula_4as lower bound and so are often of the type of incomplete gamma functions, which turn out to be proportional to formula_23.
Collision theory.
One example comes from the "collision theory" of chemical reactions, developed by Max Trautz and William Lewis in the years 1916-18. In this theory, molecules are supposed to react if they collide with a relative kinetic energy along their lines-of-center that exceeds "E"a. This leads to an expression very similar to the Arrhenius equation.
Transition state theory.
Another Arrhenius-like expression appears in the "transition state theory" of chemical reactions, formulated by Wigner, Eyring, Polanyi and Evans in the 1930s. This takes various forms, but one of the most common is
where formula_25 is the Gibbs energy of activation, formula_10 is Boltzmann's constant, and formula_27 is Planck's constant.
At first sight this looks like an exponential multiplied by a factor that is "linear" in temperature. However, one must remember that free energy is itself a temperature dependent quantity. The free energy of activation is the difference of an enthalpy term and an entropy term multiplied by the absolute temperature. When all of the details are worked out one ends up with an expression that again takes the form of an Arrhenius exponential multiplied by a slowly varying function of "T". The precise form of the temperature dependence depends upon the reaction, and can be calculated using formulas from statistical mechanics involving the partition functions of the reactants and of the activated complex.
Limitations of the idea of Arrhenius activation energy.
Both the Arrhenius activation energy and the rate constant "k" are experimentally determined, and represent macroscopic reaction-specific parameters that are not simply related to threshold energies and the success of individual collisions at the molecular level. Consider a particular collision (an elementary reaction) between molecules A and B. The collision angle, the relative translational energy, the internal (particularly vibrational) energy will all determine the chance that the collision will produce a product molecule AB. Macroscopic measurements of E and "k" are the result of many individual collisions with differing collision parameters. To probe reaction rates at molecular level, experiments are conducted under near-collisional conditions and this subject is often called molecular reaction dynamics.

</doc>
<doc id="47012" url="http://en.wikipedia.org/wiki?curid=47012" title="Peering">
Peering

In computer networking, peering is a voluntary interconnection of administratively separate Internet networks for the purpose of exchanging traffic between the users of each network. The pure definition of peering is settlement-free, "bill-and-keep," or "sender keeps all," meaning that neither party pays the other in association with the exchange of traffic; instead, each derives and retains revenue from its own customers.
An agreement by two or more networks to peer is instantiated by a physical interconnection of the networks, an exchange of routing information through the Border Gateway Protocol (BGP) routing protocol and, in one case out of every two hundred agreements, a formalized contractual document.
Occasionally the word “peering” is used to describe situations where there is some settlement involved. In the face of such ambiguity, the phrase "settlement-free peering" is sometimes used to explicitly denote pure cost-free peering.
How peering works.
The Internet is a collection of separate and distinct networks named autonomous systems, each one operating under a common framework of globally unique IP addressing and global BGP routing.
The relationships between these networks are generally described by one of the following three categories:
Furthermore, in order for a network to reach any specific other network on the Internet, it must either:
The Internet is based on the principle of "global reachability" (sometimes called "end-to-end reachability"), which means that any Internet user can reach any other Internet user as though they were on the same network. Therefore, any Internet connected network must by definition either pay another network for transit, or peer with every other network who also does not purchase transit.
Motivations for peering.
Peering involves two networks coming together to exchange traffic with each other freely, and for mutual benefit. This 'mutual benefit' is most often the motivation behind peering, which is often described solely by "reduced costs for transit services". Other less tangible motivations can include:
Physical interconnections for peering.
The physical interconnections used for peering are categorized into two types:
Public peering.
Public peering is accomplished across a Layer 2 access technology, generally called a "shared fabric". At these locations, multiple carriers interconnect with one or more other carriers across a single physical port. Historically, public peering locations were known as network access points (NAPs); today they are most often called exchange points or "Internet exchanges" ("IXP"). Many of the largest exchange points in the world can have hundreds of participants, and some span multiple buildings and colocation facilities across a city.
Since public peering allows networks interested in peering to interconnect with many other networks through a single port, it is often considered to offer "less capacity" than private peering, but to a larger number of networks. Many smaller networks, or networks who are just beginning to peer, find that public peering exchange points provide an excellent way to meet and interconnect with other networks who may be open to peering with them. Some larger networks utilize public peering as a way to aggregate a large number of "smaller peers", or as a location for conducting low-cost "trial peering" without the expense of provisioning private peering on a temporary basis, while other larger networks are not willing to participate at public exchanges at all.
A few exchange points, particularly in the United States, are operated by commercial carrier-neutral third parties, which are critical for achieving cost-effective data center connectivity.
Private peering.
Private peering is the direct interconnection between only two networks, across a Layer 1 or 2 medium that offers dedicated capacity that is not shared by any other parties. Early in the history of the Internet, many private peers occurred across 'telco' provisioned SONET circuits between individual carrier-owned facilities. Today, most private interconnections occur at carrier hotels or carrier neutral colocation facilities, where a direct crossconnect can be provisioned between participants within the same building, usually for a much lower cost than telco circuits.
Most of the traffic on the Internet, especially traffic between the largest networks, occurs via private peering. However, because of the resources required to provision each private peer, many networks are unwilling to provide private peering to "small" networks, or to "new" networks who have not yet proven that they will provide a mutual benefit.
Peering agreement.
Throughout the history of the Internet, there have been a spectrum of kinds of agreements between peers, ranging from handshake agreements to written contracts as required by one or more parties. Such agreements set forth the details of how traffic is to be exchanged, along with a list of expected activities which may be necessary to maintain the peering relationship, a list of activities which may be considered abusive and result in termination of the relationship, and details concerning how the relationship can be terminated. Detailed contracts of this type are typically used between the largest ISPs, and the ones operating in the most heavily regulated economies. As of 2011, such contracts account for less than 0.5% of all peering agreements.
History of peering.
The first Internet exchange point was the Commercial Internet Exchange (CIX), formed by Alternet/UUNET (now Verizon Business), PSI, and CERFNET to exchange traffic without regard for whether the traffic complied with the acceptable use policy (AUP) of the NSFNet or ANS' interconnection policy. The CIX infrastructure consisted of a single router, managed by PSI, and was initially located in Santa Clara, California. Paying CIX members were allowed to attach to the router directly or via leased lines. After some time, the router was also attached to the Pacific Bell SMDS cloud. The router was later moved to the Palo Alto Internet Exchange, or PAIX, which was developed and operated by Digital Equipment Corporation (DEC). Because the CIX operated at , rather than , and because it was not neutral, in the sense that it was operated by one of its participants rather than by all of them collectively, and it conducted lobbying activities supported by some of its participants and not by others, it would not today be considered an Internet exchange point. Nonetheless, it was the first thing to bear that name.
The first exchange point to resemble modern, neutral, Ethernet-based exchanges was the Metropolitan Area Ethernet, or MAE, in Tysons Corner, Virginia. When the United States government de-funded the NSFNET backbone, Internet exchange points were needed to replace its function, and initial governmental funding was used to aid the preexisting MAE and bootstrap three other exchanges, which they dubbed NAPs, or "Network Access Points," in accordance with the terminology of the National Information Infrastructure document. All four are now defunct or no longer functioning as Internet exchange points:
As the Internet grew, and traffic levels increased, these NAPs became a network bottleneck. Most of the early NAPs utilized FDDI technology, which provided only 100 Mbit/s of capacity to each participant. Some of these exchanges upgraded to ATM technology, which provided OC-3 (155 Mbit/s) and OC-12 (622 Mbit/s) of capacity.
Other prospective exchange point operators moved directly into offering Ethernet technology, such as gigabit Ethernet (1000 Mbit/s), which quickly became the predominant choice for Internet exchange points due to the reduced cost and increased capacity offered. Today, almost all significant exchange points operate solely over Ethernet, and most of the largest exchange points offer ten gigabit Ethernet (10,000 Mbit/s) service.
During the dot-com boom, many exchange point and carrier neutral colocation providers had plans to build as many as 50 locations to promote carrier interconnection in the United States alone. Essentially all of these plans were abandoned following the dot-com bust, and today it is considered both economically and technically infeasible to support this level of interconnection among even the largest of networks.
Depeering.
By definition, peering is the voluntary and free exchange of traffic between two networks, for mutual benefit. If one or both networks believes that there is no longer a mutual benefit, they may decide to cease peering: this is known as depeering. Some of the reasons why one network may wish to depeer another include:
In some situations, networks who are being depeered have been known to attempt to fight to keep the peering by intentionally breaking the connectivity between the two networks when the peer is removed, either through a deliberate act or an act of omission. The goal is to force the depeering network to have so many customer complaints that they are willing to restore peering. Examples of this include forcing traffic via a path that does not have enough capacity to handle the load, or intentionally blocking alternate routes to or from the other network. Some very notable examples of these situations have included:
Modern peering.
Donut peering model.
The "donut peering" model describes the intensive interconnection of small and medium-sized regional networks that make up much of the Internet.
Traffic between these regional networks can be modeled as a toroid, with a core "donut hole" that is poorly interconnected to the networks around it.
As detailed above, some carriers attempted to form a cartel of self-described Tier 1 networks, nominally refusing to peer with any networks outside the oligopoly.
Seeking to reduce transit costs, connections between regional networks bypass those "core" networks.
Data takes a more direct path, reducing latency and packet loss. This also improves resiliency between consumers and content providers via multiple connections in many locations around the world, in particular during business disputes between the core transit providers.
Multilateral peering.
While more attention is paid to bilateral peering, and bilateral peering agreements numerically predominate, the majority of BGP AS-AS adjacencies are more likely the product of multilateral peering agreements. In multilateral peering, an unlimited number of parties agree to exchange traffic on common terms, using a single agreement that they all accede to, and using a route server or route reflector (which differ from looking glasses in that they serve routes back out to participants, rather than just listening to inbound routes) to redistribute routes via a BGP hub-and-spoke topology, rather than a partial-mesh topology. The two primary criticisms of multilateral peering are that it breaks the shared fate of the forwarding and routing planes, since the layer-2 connection between two participants could hypothetically fail while their layer-2 connections with the route server remained up, and that they force all participants to treat each other with the same, undifferentiated, routing policy. The primary benefit of multilateral peering is that it minimizes configuration for each peer, while maximizing the efficiency with which new peers can begin contributing routes to the exchange. While optional multilateral peering agreements and route servers are now widely acknowledged to be a good practice, mandatory multilateral peering agreements (MMLPAs) have long been agreed to not be a good practice.
Peering locations.
The modern Internet operates with significantly more peering locations than at any time in the past, resulting in improved performance and better routing for the majority of the traffic on the Internet. However, in the interests of reducing costs and improving efficiency, most networks have attempted to standardize on relatively few locations within these individual regions where they will be able to quickly and efficiently interconnect with their peering partners.
Exchange points.
The largest exchange points in the world are DE-CIX in Frankfurt, AMS-IX in Amsterdam, LINX in London, the Moscow Internet Exchange, Equinix Ashburn in Washington D.C., and JPNAP in Tokyo. The United States, with a historically larger focus on private peering and commercial public peering, has much less traffic visible on public peering switch-fabrics compared to other regions that are dominated by non-profit membership exchange points. Collectively, the many exchange points operated by Equinix are generally considered to be the largest, though traffic figures are not generally published. Other important but smaller exchange points include LIPEX and LONAP in London, NYIIX in New York, and NAP of the Americas in Miami.
URLs to some public traffic statistics of exchange points include:
Peering and BGP.
A great deal of the complexity in the BGP routing protocol exists to aid the enforcement and fine-tuning of peering and transit agreements. BGP allows operators to define a policy that determines where traffic is routed. Three things commonly used to determine routing are local-preference, multi exit discriminators (MEDs) and AS-Path. Local-preference is used internally within a network to differentiate classes of networks. For example, a particular network will have a higher preference set on internal and customer advertisements. Settlement free peering is then configured to be preferred over paid IP transit.
Networks that speak BGP to each other can engage in multi exit discriminator exchange with each other, although most do not. When networks interconnect in several locations, MEDs can be used to reference that network's interior gateway protocol cost. This results in both networks sharing the burden of transporting each other's traffic on their own network (or "cold potato"). "Hot-potato" or nearest-exit routing, which is typically the normal behavior on the Internet, is where traffic destined to another network is delivered to the closest interconnection point.
Law and policy.
Internet interconnection is not regulated in the same way that public telephone network interconnection is regulated. Nevertheless, Internet interconnection has been the subject of several areas of federal policy in the United States. Perhaps the most dramatic example of this is the attempted MCI Worldcom/Sprint merger. In this case, the Department of Justice blocked the merger specifically because of the impact of the merger on the Internet backbone market (thereby requiring MCI to divest itself of its successful "internetMCI" business to gain approval). In 2001, the Federal Communications Commission's advisory committee, the Network Reliability and Interoperability Council recommended that Internet backbones publish their peering policies, something that they had been hesitant to do beforehand. The FCC has also reviewed competition in the backbone market in its Section 706 proceedings which review whether advanced telecommunications are being provided to all Americans in a reasonable and timely manner.
Finally, Internet interconnection has become an issue in the international arena under something known as the International Charging Arrangements for Internet Services (ICAIS). In the ICAIS debate, countries underserved by Internet backbones have complained that it is unfair that they must pay the full cost of connecting to an Internet exchange point in a different country, frequently the United States. These advocates argue that Internet interconnection should work like international telephone interconnection, with each party paying half of the cost. Those who argue against ICAIS point out that much of the problem would be solved by building local exchange points. A significant amount of the traffic, it is argued, that is brought to the US and exchanged then leaves the US, using US exchange points as switching offices but not terminating in the US. In some worst-case scenarios, traffic from one side of a street is brought all the way to a distant exchange point in a foreign country, exchanged, and then returned to another side of the street. Countries with liberalized telecommunications and open markets, where competition between backbone providers occurs, tend to oppose ICAIS.

</doc>
<doc id="47013" url="http://en.wikipedia.org/wiki?curid=47013" title="Free World">
Free World

The Free World is a Cold War–era term for the non-communist countries of the world. The free world included countries such as the United States, the United Kingdom, Italy, France, Canada, West Germany, Australia, and countries belonging to organizations such as the European Community and NATO. In addition, the "Free World" occasionally includes the Commonwealth realms, Japan, Israel, and India.
Origins.
During World War II, the Allied powers viewed themselves as opposing the oppression and fascism of the Axis powers, thus making them "free". Following the end of World War II, the Cold War conception of the "Free World" included only capitalist particularly anti-communist states as being "free" and having such freedoms as free speech, free press, free to protest and freedom of association.
In World War II, the term "free world" was used to refer to the nations fighting against the Axis Powers. Such use would have included the Soviet Union, contrary to the later, "Cold War" definition of the term.
During World War II the term "free countries" was used to identify the western allies. During the Cold War, the term referred to the allies of the United States. In both cases, the term was used for propaganda purposes.
Recent usage.
Although the "Free World" had its origins in the Cold War, the phrase is still occasionally used after the end of the Cold War. According to Samuel P. Huntington the term has been replaced by the concept of the World community, which, he argues, "has become the euphemistic collective noun (replacing
"the Free World") to give global legitimacy to actions reflecting the interests of the United States and other Western powers."
Organizations such as Freedom House release a Freedom in the World report every year rating countries based on a scale of free to not-free. The Freedom House think tank in the United States published reports of "state of freedom in the world," with countries classified as "free countries", "partly free" or "not free". In 2013 there were 90, 58 and 47 respectively.
Criticism.
During the Cold War, many neutral countries, namely those in what is considered the Third World, or those having no formal alliance with either the United States or the Soviet Union, viewed the claim of "Free World" leadership by the United States as somewhat grandiose and illegitimate. The phrase has also been used in a negative manner, usually in an anti-American context, by those who do not approve of either United States foreign policy, or the United States as a whole. The United States policy of putting their interests ahead of human rights lead to economic or military partnerships with countries such as China, Egypt and Saudi Arabia who are complained about for their lack of freedoms.
One of the earliest uses of the term "Free World" as a politically significant term occurs in Frank Capra's World War II propaganda film series "Why We Fight". In "Prelude to War", the first film of that series, the "free world" is portrayed as a white planet, directly contrasted with the black planet called the "slave world". The film depicts the free world as the Western Hemisphere, led by the United States and Western Europe, and the slave world as the Eastern Hemisphere, dominated by Nazi Germany and the Japanese Empire.
"Leader of the Free World".
The "Leader of the Free World" is a colloquialism, first used during the Cold War, to describe either the United States or, more commonly, the President of the United States. The term when used in this context suggests that the United States is the principal democratic superpower, and the U.S. President is by extension the leader of the world's democratic states, i.e. the "Free World". The phrase had its origin in the late 1940s, and has become more widely used since the early 1950s. It was heavily referenced in American foreign policy up until the dissolution of the Soviet Union in December 1991, and has since fallen out of use, in part due to its usage in anti-American rhetoric.
In popular culture.
Canadian singer Neil Young referenced the term in his 1989 song "Rockin' in the Free World". In the song, he points out the irony of the many injustices in the "Free World", especially the United States.
British alternative rock band Elbow refer to the term in both their 2005 album name 'Leaders of the Free World' and their single of the same name. The song criticises Western foreign leaders in the early 21st century, in particular George W. Bush.

</doc>
<doc id="47014" url="http://en.wikipedia.org/wiki?curid=47014" title="Metric time">
Metric time

Metric time is the measure of time interval using the metric system, which defines the second as the base unit of time, and multiple and submultiple units formed with metric prefixes, such as kiloseconds and milliseconds. It does not define the time of day, as this is defined by various time scales, which may be based upon the metric definition of the second. Other units of time, the minute, hour, and day, are accepted for use with the modern metric system, but are not part of it. 
History.
Although part of the decimal metric system, the second derives its name from the sexagesimal system, which originated with the Sumerians and Babylonians, and divides a base unit into sixty minutes, minutes into sixty seconds, seconds into sixty thirds, etc. The word "minute" comes from the Latin "pars minuta prima", meaning first small part, and "second" from "pars minuta secunda" or second small part. Coincidentally, angular measure also uses these sexagesimal units; in that field, it is the degree that is subdivided into minutes and seconds, while in time, it is the hour.
When the metric system was introduced in France in 1795, it included units for length, area, dry volume, liquid capacity, weight or mass, and even currency, but not for time. Decimal time of day had been introduced in France two years earlier, but was set aside at the same time the metric system was inaugurated, and did not follow the metric pattern of a base unit and prefixed units. James Clerk Maxwell and Elihu Thomson (through the British Association for the Advancement of Science - BAAS) introduced the Centimetre gram second system of units (cgs) in 1874, in order to derive electric and magnetic metric units, following the recommendation of Carl Friedrich Gauss in 1832.
The ephemeris second (defined as 1/86400 of a mean solar day) was made one of the original base units of the modern metric system, or International System of Units (SI), at the 10th General Conference on Weights and Measures (CGPM) in 1954. The SI second was later redefined more precisely as 
the duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium-133 atom.
Alternative units.
Numerous proposals have been made for alternative base units of metric time. On March 28, 1794, the president of the commission which developed the metric system, Joseph Louis Lagrange, proposed in a report to the commission the names "déci-jour" and "centi-jour" (deciday and centiday in English). Base units equivalent to decimal divisions of the day, such as 1/10, 1/100, 1/1000, or 1/100,000 day, or other divisions of the day, such as 1/20 or 1/40 day, have also been proposed, with names such as tick, meck, chi, chron, moment, etc., and multiple and submultiple units formed with metric prefixes. Such alternative units have not gained any notable acceptance.
A decimal second = 1/100 000 of a day = 0.864 s is an obvious alternative. Any redefinition of the second, however, creates conflicts with anything based on its precise current definition. Another unit for time, more familiar than some other suggestions, could be 14.4 minutes, i.e. a shorter "quarter of an hour", or a "centiday", as proposed by Lagrange. The centiday was used in China (called "ke" in Chinese) for thousands of years.
In the 19th century, Joseph Charles François de Rey-Pailhade proposed using the centiday, abbreviated "cé", divided into 10 "decicés", 100 "centicés", 1000 "millicés". and 10000 "dimicés".
In 1897, the "Commission de décimalisation du temps" was created by the French Bureau of Longitude, with the mathematician Henri Poincaré as secretary. The commission proposed making the standard hour the base unit of metric time, but the proposal did not gain acceptance and was eventually abandoned.
Swatch Internet Time introduced a time unit called .beat, equal to the French decimal minute or 1/1000 day. However, there are no other units with metric prefixes.
Alternative meaning.
Metric time is sometimes used to mean decimal time. Metric time properly refers to measurement of time interval, while decimal time refers to the time of day. Standard time of day is usually measured by the 24-hour clock or its closely related derivative, the 12-hour clock. These measurement systems are now based on the metric base unit of time, the second. Some proposals for alternative units of metric time are accompanied by decimal time scales for telling the time of day. Other proposals called "metric time" refer only to decimal time and are therefore not truly metric. 
French decimal time is sometimes called "metric time" because it was introduced around the same time as the metric system and both were decimal. The April 7, 1795 decree creating the original metric units and prefixes actually suspended decimal time, which had named its units the (decimal) hour, minute and second instead of using metric prefixes.
In computing.
In computing, at least internally, metric time gained widespread use for ease of computation. Unix time gives date and time as the number of seconds since January 1, 1970, and Microsoft's FILETIME as multiples of 100ns since January 1, 1601. VAX/VMS uses the number of 100ns since November 17, 1858 and RISC OS the number of centiseconds since January 1, 1900.
None of these systems is strictly linear, as they each have discontinuities at leap seconds.
Fractions of a second.
While larger metric prefixes of a second are rarely used, subdivision prefixes are well established within science and technology. Milliseconds, microseconds and smaller units (but not centiseconds) are well established.
Problems.
The main problem lies in the size of time units created using metric prefixes. The International System of Units has developed 10 prefixes to express units as exponential multiples or sub-multiples. The first three multiples would be viable for use within a metric time system; they are:
However, the next multiples are:
Such lengths of time are relatively impractical for measuring human activities.
For decimal multiples of the metric time unit to be practical, standard units and prefixes would need to be developed or revived for the 4th and 5th exponents so that it becomes viable for measuring human life. Historically, the metric system included a "myria-" prefix to represent 104, but its use was abrogated when the SI was introduced in 1960. 105 was represented using various ad hoc solutions; 100 000 meters composed a grade, 100 000 pascals composed a bar, and 100 000 dynes composed a newton; most of these conversions stem from compatible units that were used in the SI (and its predecessor, MKS) and the CGS system of units. Meteorologists currently use a unit of 100 000 seconds (a quantity on the order of a day) to quantify atmospheric vorticity at the synoptic scale; the unit, however, does not have a name.
In popular culture.
Metric time appears in science fiction writing on occasion. In the novel "A Deepness in the Sky" by Vernor Vinge, the spacefaring Qeng Ho culture is depicted as using metric units such as the kilosecond and megasecond for all time interval measurements.
The novels "Accelerando" and "Glasshouse" by Charles Stross have several uses of metric time, including kiloseconds, megaseconds, and megayears. In the second case, humans are living in habitats circling brown dwarfs, with no lingering connection to the Solar System that gave birth to the earlier time scale. In "Glasshouse" characters use kiloseconds, "megs" etc. to refer to different periods of time, any earlier use of megayears has been dropped.
"The Risen Empire" by Scott Westerfeld uses metric time to measure time within an empire that extends over many star systems.
In television, the "Simpsons" episode "They Saved Lisa's Brain" has Principal Skinner saying that, thanks to the city being under Mensa's control, the city's trains are not only running on time, but they are running on metric time, while looking at an analog clock with numbers 1–10 (which indicates decimal time).
An opening scene of Fritz Lang's 1927 film "Metropolis" shows a metric clock with ten numbers instead of twelve, illustrating the improved efficiency of future industrial society.
A song on the 2000 album "Veni Vidi Vicious" by the Swedish band The Hives is titled "The Hives - Introduce the Metric System in Time".
Battlestar Galactica uses "" as a unit of time, analogous to a minute. While not strictly linked to seconds, it is part of a decimal-based time keeping system.
In 2014, WestJet Airlines created an April Fool's Day joke stating they were switching to metric time, including how to calculate. 

</doc>
<doc id="47016" url="http://en.wikipedia.org/wiki?curid=47016" title="Commentarii de Bello Gallico">
Commentarii de Bello Gallico

Commentarii de Bello Gallico (English: Commentaries on the Gallic War), also simply Bellum Gallicum (English: Gallic War), is Julius Caesar's firsthand account of the Gallic Wars, written as a third-person narrative. In it Caesar describes the battles and intrigues that took place in the nine years he spent fighting local armies in Gaul that opposed Roman domination.
The "Gaul" that Caesar refers to is sometimes all of Gaul except for the Roman province of "Gallia Narbonensis" (modern-day Provence), encompassing the rest of modern France, Belgium and some of Switzerland. On other occasions, he refers only to that territory inhabited by the Celtic peoples known to the Romans as Gauls, from the English Channel to Lugdunum (Lyon).
The work has been a mainstay in Latin instruction because of its simple, direct prose. It begins with the frequently quoted phrase "Gallia est omnis divisa in partes tres", meaning "All of Gaul is divided into three parts". The full work is split into eight sections, Book 1 to Book 8, each varying in size from approximately 5,000 to 15,000 words. Book 8 was written by Aulus Hirtius, after Caesar's death.
Title.
The Latin title, literally "Commentaries on the Gallic War", is often retained in English translations of the book, and the title is also translated to "About the Gallic War", "Of the Gallic War", "On the Gallic War", "The Conquest of Gaul", and "The Gallic War".
Contents.
"De Bello Gallico" consists of eight books: seven written by Caesar himself, and the eighth book added later by Aulus Hirtius, one of Caesar's generals.
Book I.
Caesar describes a conflict with the Gallic tribe known as the Helvetii. The wealthiest man of the Helvetii, Orgetorix, convinced his countrymen that they should leave their homeland (modern-day Switzerland) because they are too constricted by the surrounding rivers and mountains (the Jura Mountains, the Rhône and Lake Geneva, and the Rhine). They concoct a plan to burn their homes and then make an exodus out of their homeland. When Caesar learns of their plan to cross over a Roman province, a conflict arises, pitting the Romans against the Helvetii. The Helvetii are defeated by the Romans, and return to Helvetia, their numbers greatly reduced.
Later, more problems arise surrounding a tribal conflict in Gaul. The Aedui (allies of Rome) are engaged in a power struggle with two other tribes called the Arverni and the Sequani. The Arverni and Sequani decide to get help in their struggle from Germanic mercenaries from across the Rhine, led by a king named Ariovistus. At first the plan works because the Germanic mercenaries help the Arverni and Sequani to oppress their enemy, the Aedui. But, the plan backfires as Ariovistus begins a violent takeover of the territory of the Arverni and Sequani, exercising a cruel tyranny over them. Caesar then takes military action against Ariovistus, both because the Aedui are allies of Rome, and because he wants to stop the flow of Germanics from across the Rhine into Gaul, which neighbors upon Rome. Caesar moves his troops to Vesontio (modern Besançon), the capital city of the Sequani. Ariovistus and Caesar conduct many negotiations, including an unusual face-to-face conference. Two days later, when Ariovistus requests to speak with Caesar again, Caesar instead sends C. Valerius Procillus who is ambushed and taken captive by Ariovistus. When battle between the Germanics and the Romans finally commences, there is fierce hand-to-hand combat. The Romans defeat Ariovistus, and Caesar is overjoyed to find and rescue C. Valerius Procillus, the envoy who had earlier been captured by Ariovistus.
Book II.
The Belgae, fearing the expansion of Roman military power in Gaul, begin to form a huge army to fight the Romans. The Remi, one of the Belgic tribes, breaks away from the Belgic conspiracy and offers its help to the Romans. This help from the Remi proves useful, as does aid from the Aedui (allies of Rome) in the form of cavalrymen. The military conflict that follows culminates in a huge battle with the toughest of all the Belgae, the Nervii. After bloody fighting, the Romans are victorious.
Book III.
Some Roman troops under Servius Galba are wintering peacefully in the Alps between Italy and Gaul. Suddenly, the occupied tribes strike at the vulnerable Romans, posing a grave threat. But the Romans fight back, defeating the tribesmen. Later, the Veneti, a seafaring tribe on the western coast of Gaul, begin a rebellion against Rome. Breaking an important tradition, the Veneti capture and imprison the Roman envoys sent to them. This infuriates Caesar. The Veneti, because of their great experience as sailors, exercise great advantage over the Romans in naval battle. But the Romans eventually adapt and defeat the Veneti. Caesar, wishing to make an example of the Veneti for violating the customs of diplomacy by imprisoning the envoys, executes their chief nobles and sells the rest of them into slavery.
Book IV.
The Suebi, an enormous Germanic tribe, have a reputation for physical toughness and military prowess. Two other smaller tribes, the German Usipetes and Tencteri, are tired of being oppressed by the Suebi so they decide to go west, crossing the Rhine and moving into the territory of the Belgae (more specifically the Menapii). Caesar quickly arrives with Roman troops. The Suebi try repeatedly to bargain with Caesar but to no avail. After settling on a temporary truce, the Germans unexpectedly attack the Romans, causing casualties. Caesar considers this to be an act of treachery, and decides to break off any further negotiations with the Germans. The next day, the Germans send a large number of their leaders and elders to negotiate with Caesar. Caesar, expecting further treachery, takes these Germans prisoner. He attacks and defeats the remaining Germans who are disadvantaged by the absence of their leadership. As a show of force, Caesar constructs a sturdy wooden bridge across the Rhine and crosses into Germania, alarming the Germans.
Later, Caesar makes arrangements for an assault on Britain. Although the Roman infantry successfully cross the English channel, the cavalry fail to cross, putting the Romans at a disadvantage. After being routed by the Romans, the Britons agree to submit to Caesar and disperse back into the countryside. But after a storm damages the Romans' ships and leaves them stranded in Britain without adequate supplies, the Britons decide to attack the Romans in their vulnerable state. The Britons initially catch the Romans off guard, but the Romans regroup and rout the Britons. When the Roman troops begin to cross back over to the continent, they are attacked by the Morini, who are then defeated by the Romans.
Book V.
Caesar commands that as many boats as possible be prepared during the winter for a campaign against Britain in the Spring. He orders all boats to assemble at Portus Itius (near modern-day Boulogne-sur-Mer). Caesar’s attention is momentarily diverted to Gallia Belgica as Indutiomarus and Cingetorix struggle for power over the Treveri. Back at Port Itius, Caesar prepares to take many Gallic hostages with him to his British campaign so as to prevent revolts while he is gone. Among them is Dumnorix, a rebellious noble of the Aedui. Dumnorix violently opposes being taken over to Britain, and flees back to his homeland. Caesar stops all preparations while Dumnorix is hunted down and slain.
The Romans sail to Britain to begin their campaign. There are some skirmishes between the Romans and the Britons, and a storm destroys many of the Roman boats. The British tribes, although previously at war with one another, band together to face the Roman threat with Cassivellaunus as their commander in chief. Caesar discovers the stronghold of Cassivellaunus near the Thames river and routs the Britons there. The Trinovantes, a powerful tribe, offer to become Rome's allies, and several other British tribes follow suit. From these tribes Caesar learns the location of Cassivellaunus and successfully attacks him there. Cassivellaunus orders the tribes in Kent to attack the British ships, but they are defeated. Cassivellaunus surrenders to Caesar, enabling Caesar to quickly return to the continent before Winter arrives.
Because a drought has affected the grain supply, Caesar's troops must winter among the rebellious Belgic tribes. Roman troops, led by Q. Titurius Sabinus and L. Aurunculeius Cotta are wintering among the Eburones when they are attacked by the Eburones, led by Ambiorix and Cativolcus. Ambiorix deceives the Romans by saying that the attack was made without his consent, and furthermore advises them to flee because a huge Germanic army is coming from across the Rhine. After much discussion and disagreement, the Romans decide to trust Ambiorix and leave the next morning. As the Romans are marching away the next morning, the Eburones ambush them, killing most of the Romans. A few Roman survivors make it back to their winter quarters where they commit suicide that night.
Other Roman troops are wintering among the Nervii under Quintus Tullius Cicero (brother of the famous orator). Ambiorix convinces the other Belgic tribes to immediately attack Cicero's camp. Cicero's troops are trapped, outnumbered, and blocked from help as their messengers are intercepted. The situation gets progressively more desperate for the Romans, but finally they are able to get a message to Caesar. Caesar summons the other Roman legions and rushes to Cicero's aid. As Caesar approaches Cicero’s camp, the Belgae abandon their siege and maneuver toward Caesar’s troops. Caesar, vastly outnumbered, creates a ruse, ordering his troops to appear confused and frightened. The ruse works and entices the Belgae to attack on ground favorable to the Romans. Caesar's troops counterattack and put the Belgae to flight. That same day Caesar’s troops reach Cicero's camp finding most of the men wounded. Meanwhile, Indutiomarus, a leader of the Treveri, begins to harass the camp of Labienus daily, until Labienus sends out cavalry for the express purpose of killing Indutiomarus. After killing Indutiomarus, the Roman cavalry routs the rest of Indutiomarus' army. Caesar personally stays in Gaul all winter due to the risk of unrest among the Gallic tribes.
Book VI.
Caesar enlists a large number of new troops to replace the ones lost the previous year when the forces of Q. Titurius Sabinus and L. Aurunculeius Cotta were slain by the forces of Ambiorix. Meanwhile, the Treveri, angry over the death of Indutiomarus, fervently try to arouse the Belgic and Germanic peoples around them to fight the Romans. In response to this, Caesar lays waste to the territory of the Nervii, neighbors of the Treveri. Caesar holds a council of Gaul, but the Senones, Carnutes, and Treveri do not send representatives. Caesar quickly marches to the territory of the Senones. Acco, their leader, is forced to ask for pardon and give hostages to Caesar as collateral. This arrangement is acceptable to Caesar because he does not wish to pursue war with the Senones. Instead, he turns his attention to the Treveri and Ambiorix. Caesar lays waste to the territory of the Menapii, and they sue for peace. After warning them not to help Ambiorix, he heads toward the Treveri. The Treveri prepare to attack the camp of Labienus. Labienus, wishing to tempt the Treveri to attack in a situation favorable to the Romans, orders his men to break camp as though they are retreating. The ruse works, and the Treveri are drawn into battle and routed by the Romans.
Caesar decides to cross the Rhine again so that Ambiorix might not be able to receive help or quarter there. At this point in his commentary, Caesar gives a thorough description of Gallic culture and customs, along with Germanic culture, customs, and animals. After crossing the Rhine back into Gaul, Caesar sends troops ahead of him to surprise Ambiorix, but Ambiorix escapes. Ambiorix warns all the Eburones to flee to safety. Cativolcus, the aged king of one half of the Eburones curses Ambiorix and commits suicide because he wishes neither to engage in war nor flee from his home.
Caesar establishes fortifications in several places within the territory of the Eburones. Engaging the Eburones is difficult because the terrain of the area provides the Eburones with numerous opportunities to hide from the Romans. Caesar invites neighboring tribes to pillage and plunder the Eburones. The Sigambri, a Germanic tribe, come and take a large amount of cattle. Learning that all the Roman supplies and equipment are stored at Atuatuca, the Sigambri head there to try to capture it. Cicero, in charge of Atuatuca, is not supposed to let anybody out of the camp for the sake of security. But on the seventh day, thinking that it would not be dangerous, Cicero allows some troops to exit the camp to forage for food. At that very moment, the Sigambri arrive, throwing the Roman camp into a panic. Hampered by fear and surprise, the Romans suffer losses but eventually mount a defense. The Sigambri retreat, and Caesar arrives, complaining that the troops had not been kept within the camp, as he had ordered.
Caesar marches out again, gathering a large number of auxiliaries. They exert much effort to find Ambiorix, but he evades their grasp. After devastating the territory of the Eburones, Caesar holds a council of Gaul in which he investigates the rebellion of the Senones and Carnutes mentioned at the beginning of book six. Acco, the leader of the rebellion, is found guilty and put to death by Caesar.
Book VII.
While Caesar is in Italy, he decides to enlist more troops because of political disturbances in Rome. When the Gauls hear of this, they conspire to rebel and prevent Caesar from returning to his troops stationed in Gaul. The Carnutes take the lead in this endeavor, slaughtering and plundering the Roman citizens at Cenabum. Vercingetorix, a young nobleman of the Arverni, gathers troops, and with the support of neighbouring tribes is given supreme command of the Gallic armies. At first the Bituriges resist, but then join forces with the Arverni. As more and more Gallic tribes join the rebellion, Caesar reaches his armies in Narbo and begins to move them toward the territory of the Arverni. Caesar surprises the Arverni by arriving in their territory unexpectedly after clearing a path through deep snow. Caesar moves quickly to rendezvous with his legions wintering among the Lingones before Vercingetorix can realise what is happening. Vercingetorix decides to attack Gergovia, a town of the Boii who are allies of Rome. Caesar pillages and burns Cenabum, the town attacked earlier by the Carnutes.
Vercingetorix tells his troops that they must adopt a new strategy: burn all the towns and crops in the area so as to starve the Romans. The Gauls burn all the towns of the Bituriges except Avaricum, which they decide to defend. Near Avaricum, Vercingetorix camps near Caesar and inflicts heavy losses on Romans as they forage for food. The Romans ask their Gallic allies for food, but this accomplishes little and the Romans suffer from lack of food. The Romans lay siege to Avaricum. After twenty-five grueling days of constructing siege works under great stress, the Romans attack and trap the Gauls in the town. The Romans slaughter almost all of the Gauls within the town, causing a huge loss for the Gauls.
Caesar then sets out for the town of Gergovia. He builds elaborate siege works with the intention of completely encircling and starving the Gauls inside. Caesar is distracted from the siege by betrayal from his Gallic allies the Aedui, led by Litaviccus whom he fought and defeated after a desperate struggle. Caesar then went back to Gergovia and realised that his siege would fail. He creates a false retreat to lure Vercingetorix into battle, but this fails and the Romans are defeated by Vercingetorix.
Labienus battles the Parisii, and the entire Gallic war becomes more dangerous as the Aedui, Rome's longtime allies, revolt and try to induce other tribes to revolt as well. After Vercingetorix's cavalry is defeated in a skirmish with the Romans, he moves his forces to the town of Alesia. Caesar builds a circumvallation around the town to lay siege to it but Vercingetorix had summoned his Gallic allies to attack the besieging Romans, so Caesar built another outer fortification, a contravallation, against the expected relief armies (resulting in a doughnut-shaped fortification). Since Vercingetorix was inside Alesia, the Gauls attacking from the outside were without his leadership. However, the attacks did reveal a weak point in the fortifications and the combined forces on the inside and the outside almost made a breakthrough. Only when Caesar personally led the last reserves into battle did he finally manage to prevail, leading to the surrender of Vercingetorix.
Book VIII.
While in winter quarters in Bibracte, Caesar learns that several tribes are planning a revolt. After quelling a rebellion among the Bituriges, Caesar rewards his troops with large monetary bonuses. The Romans stop the Carnutes from waging war against the Bituriges. The Bellovaci, a Belgic tribe, raise an army and begin to wage war against the Suessiones who are under the patronage of the Remi, allies of Rome. The Bellovaci try to get other tribes to join in their rebellion. Caesar campaigns against them and defeats them. Caesar lays waste to the territory of the Eburones. Labienus wages war against the Treviri. The Romans quell a revolt among the Pictones.
Drapes, a Senonian, gathers troops, joins with Luterius, a Cadurcian, and tries to attack the Roman province in the southern part of Gaul called Gallia Narbonensis. Drapes and Luterius stop at a town called Uxellodunum which is strongly fortified by its geography. After a bloody conflict, the Romans defeat them by cutting off the supply of water to the town.
Labienus defeats the Treveri and captures their leaders. Caesar goes to Aquitania to assure the loyalty of the tribes there. Caesar puts his legions in winter quarters among various tribes of Gaul. While Caesar is in winter quarters among the Belgae, Commius the Atrebatian begins to harass the Romans by attacking their convoys. The Romans defeat Commius.
Since Caesar's term of office in Gaul is almost over, he does not wish to get involved in another war just as he is finishing his term. Therefore he tries to ensure peace by treating all the Gauls respectfully and giving gifts to the leading men of Gaul. Caesar travels to Italy in order to encourage the people to vote for Mark Antony, his close associate, for the office of augur. Some of Caesar's political opponents want to prevent Mark Antony from being elected simply as a way to oppose Caesar. While en route to Italy, Caesar hears that Mark Antony has already been elected as augur, and the townspeople give Caesar a generous and celebratory welcome.
Within the senate there is division between the supporters of Caesar and supporters of Pompey. Some worry about Caesar and Pompey having such powerful armies. The Senate passes a decree that Pompey and Caesar should each contribute one legion to the Parthian war in the East. But in reality Caesar actually ends up contributing both of them. The legion that Pompey contributes is his in name only, because it was enlisted in Caesar's territory, and then Caesar contributed another legion that had been with him previously in Gaul. So the levy of troops in reality took away two legions from Caesar. When these two legions were delivered to Italy, instead of being sent to fight against the Parthians, they remained in Italy and were handed over to Pompey. This act made it obvious that Caesar's opponents were planning to wage war against him.
Motivations.
The victories in Gaul won by Caesar had increased the alarm and hostility of his enemies at Rome, and his aristocratic enemies, the boni, were spreading rumors about his intentions once he returned from Gaul. The boni intended to prosecute Caesar for abuse of his authority upon his return, when he would lay down his imperium. Such prosecution would not only see Caesar stripped of his wealth and citizenship, but also negate all of the laws he enacted during his term as Consul and his dispositions as pro-consul of Gaul. To defend himself against these threats, Caesar knew he needed the support of the plebeians, particularly the Tribunes of the Plebs, on whom he chiefly relied for help in carrying out his agenda. The Commentaries were an effort by Caesar to directly communicate with the plebeians - thereby circumventing the usual channels of communication that passed through the Senate - to propagandize his activities as efforts to increase the glory and influence of Rome. By winning the support of the people, Caesar sought to make himself unassailable from the boni.
Truthfulness.
The work was most likely made for propaganda purposes. The work is a paradigm of proper reporting and stylistic clarity.
Influence.
Educational use.
It is often lauded for its polished, clear Latin. This book is traditionally the first authentic text assigned to students of Latin, as Xenophon's "Anabasis" is for students of Ancient Greek; they are both autobiographical tales of military adventure told in the third person, though a translation in 1985 by Peter and Anne Wiseman has Caesar speaking in the first person, which makes for a clearer read. The style is simple and elegant, essential and not rhetorical. It contains many details and employs many stylistic devices to promote Caesar's political interests.
Also, the books are valuable for the many geographical and historical claims that can be retrieved from the work. Notable chapters describe Gaulish custom (VI, 13), their religion (VI, 17), a comparison between Gauls and Germanic peoples (VI, 24) and other curious notes such as the lack of Germanic interest in agriculture (VI, 22).
Astérix.
Since Caesar is one of the characters in the Astérix and Obélix albums, René Goscinny included gags for French schoolchildren who had the "Commentarii" as a textbook. One example is having Caesar talk about himself in the third person as in the book.
Some English editions state that Astérix's village of indomitable Gauls is the "fourth part" of Gaul, not yet having been conquered by Caesar.
Vorenus and Pullo.
In Book 5, Chapter 44 the "Commentarii de Bello Gallico" notably mentions Lucius Vorenus and Titus Pullo, two Roman centurions of the 11th Legion. The 2005 television series "Rome" gives a fictionalized account of Caesar's rise and fall, featuring Kevin McKidd as the character of Lucius Vorenus and Ray Stevenson as the character of Titus Pullo of 13th Legion .
Vincent d'Indy.
During World War I the French composer Vincent d'Indy wrote his Third Symphony, which bears the title "De Bello Gallico". D'Indy was adapting Caesar's title to the situation of the current struggle in France against the German army, in which he had a son and nephew fighting, and which the music illustrates to some extent.
References.
</dl>

</doc>
<doc id="47017" url="http://en.wikipedia.org/wiki?curid=47017" title="Package manager">
Package manager

A package manager or package management system is a collection of software tools that automates the process of installing, upgrading, configuring, and removing software packages for a computer's operating system in a consistent manner. It typically maintains a database of software dependencies and version information to prevent software mismatches and missing prerequisites.
Packages are distributions of software, applications and data. Packages also contain metadata, such as the software's name, description of its purpose, version number, vendor, checksum, and a list of dependencies necessary for the software to run properly. Upon installation, metadata is stored in a local package database.
Package managers are designed to save organizations time and money through remote administration and software distribution technology that eliminate the need for manual installs and updates. This can be particularly useful for large enterprises whose operating systems are based on Linux and other Unix-like systems, typically consisting of hundreds or even thousands of distinct software packages; in the former case, a package manager is a convenience, in the latter case it becomes essential.
Impact.
Ian Murdock has commented that package management is "the single biggest advancement Linux has brought to the industry", that it blurs the boundaries between operating system and applications, and that it makes it "easier to push new innovations [...] into the marketplace and [...] evolve the OS".
Terminology.
A package manager is often called an "install manager". This can lead to confusion between a package manager and an installer. The differences include:
A package, for package managers, denotes a specific set of files bundled with the appropriate metadata for use by a package manager. This can be confusing, as some programming languages often use the word "package" as a specific form of software library. Furthermore, that software library can be distributed in a package of files bundled for either an OS-level package manager, or one specific to the programming language.
Package managers are analogous to the app store concept from commercial operating systems, except almost all packages are available at no charge and app stores may not perform dependency resolution, as all dependencies are typically bundled with the main executable on these systems.
Functions.
Package managers are charged with the task of organizing all of the packages installed on a system. Typical functions of a package management system include:
Some additional challenges are met by only a few package managers.
Challenges with shared libraries.
Computer systems which rely on dynamic library linking, instead of static library linking, share executable libraries of machine instructions across packages and applications. In these systems, complex relationships between different packages requiring different versions of libraries results in a challenge colloquially known as "dependency hell". On Microsoft Windows systems, this is also called "DLL hell" when working with dynamically linked libraries. Good package management is vital on these systems. The Framework system from OPENSTEP was an attempt at solving this issue, by allowing multiple versions of libraries to be installed simultaneously, and for software packages to specify which version they were linked against.
Front-ends for locally compiled packages.
System administrators may install and maintain software using tools other than package management software. For example, a local administrator may download unpackaged source code, compile it, and install it. This may cause the state of the local system to fall out of synchronization with the state of the package manager's database. The local administrator will be required to take additional measures, such as manually managing some dependencies or integrating the changes into the package manager.
There are tools available to ensure that locally compiled packages are integrated with the package management. For distributions based on .deb and .rpm files as well as Slackware Linux, there is CheckInstall, and for recipe-based systems such as Gentoo Linux and hybrid systems such as Arch Linux, it is possible to write a recipe first, which then ensures that the package fits into the local package database.
Maintenance of configuration.
Particularly troublesome with software upgrades are upgrades of configuration files. Since package managers, at least on Unix systems, originated as extensions of file archiving utilities, they can usually only either overwrite or retain configuration files, rather than applying rules to them. There are exceptions to this that usually apply to kernel configuration (which, if broken, will render the computer unusable after a restart). Problems can be caused if the format of configuration files changes. For instance, if the old configuration file does not explicitly disable new options that should be disabled. Some package managers, such as Debian's dpkg, allow configuration during installation. In other situations, it is desirable to install packages with the default configuration and then overwrite this configuration, for instance, in headless installations to a large number of computers. This kind of pre-configured installation is also supported by dpkg.
Repositories.
To give users more control over the kinds of software that they are allowing to be installed on their system (and sometimes due to legal or convenience reasons on the distributors' side), software is often downloaded from a number of software repositories.
Upgrade suppression.
When a user interacts with the package management software to bring about an upgrade, it is customary to present the user with the list of things to be done (usually the list of packages to be upgraded, and possibly giving the old and new version numbers), and allow the user to either accept the upgrade in bulk, or select individual packages for upgrades. Many package managers can be configured to never upgrade certain packages, or to upgrade them only when critical vulnerabilities or instabilities are found in the previous version, as defined by the packager of the software. This process is sometimes called "version pinning".
For instance:
Cascading package removal.
Some of the more advanced package management features offer "cascading package removal", in which all packages that depend on the target package and all packages that only the target package depends on, are also removed.
Common package managers and formats.
Package formats.
Each package manager relies on the format and metadata of the packages it can manage. That is, package managers need groups of files to be bundled for the specific package manager along with appropriate metadata, such as dependencies. Often, a core set of utilities manages the basic installation from these packages and multiple package managers use these utilities to provide additional functionality.
For example, yum relies on rpm as a backend. Yum extends the functionality of the backend by adding features such as simple configuration for maintaining a network of systems. As another example, the Synaptic Package Manager provides a graphical user interface by using the Advanced Packaging Tool (apt) library, which, in turn, relies on dpkg for core functionality.
Alien is a program that converts between different Linux package formats. It supports conversion between Linux Standard Base (LSB), LSB-compliant .rpm packages, .deb, Stampede (.slp), Solaris (.pkg) and Slackware (.tgz, .txz, .tbz, .tlz) packages.
Free and open source software systems.
By the nature of free and open source software, packages under similar and compatible licenses are available for use on a number of operating systems. These packages can be combined and distributed using configurable and internally complex packaging systems to handle many permutations of software and manage version-specific dependencies and conflicts. Some packaging systems of free and open source software are also themselves released as free and open source software. One typical difference between package management in proprietary operating systems, such as Mac OS X and Windows, and those in free and open source software, such as Linux, is that free and open source software systems permit third-party packages to also be installed and upgraded through the same mechanism, whereas the package managers of Mac OS X and Windows will only upgrade software provided by Apple and Microsoft, respectively (with the exception of some third party drivers in Windows). The ability to continuously upgrade third party software is typically added by adding the URL of the corresponding repository to the package management's configuration file.
Application-level package managers.
Beside the system-level application managers, there are some add-on package managers for operating systems with limited capabilities and for programming languages in which developers need the latest libraries.
In contrast to system-level application managers, application-level package managers focus on a small part of the software system. They typically reside within a directory tree that is not maintained by the system-level package manager, such as c:\cygwin or /usr/local/fink. However, this might not be the case for the package managers that deal with programming libraries, leading to a possible conflict as both package managers may claim to "own" a file and might break upgrades.

</doc>
<doc id="47019" url="http://en.wikipedia.org/wiki?curid=47019" title="OBL">
OBL

OBL may refer to:
ÖBL may refer to:

</doc>
<doc id="47022" url="http://en.wikipedia.org/wiki?curid=47022" title="Osama (disambiguation)">
Osama (disambiguation)

Osama most commonly refers to Osama bin Laden (1957–2011), the founder of al-Qaeda.
Osama or Usama may also refer to:

</doc>
<doc id="47023" url="http://en.wikipedia.org/wiki?curid=47023" title="London Internet Exchange">
London Internet Exchange

The London Internet Exchange ("LINX") is a mutually governed Internet exchange point (IXP) that provides peering services and public policy representation to over 500 Internet Service Providers (ISPs) and other network operators. LINX operates IXPs in London, Manchester (IXManchester), Edinburgh (IXScotland), Cardiff, and North Virginia, USA (LINX NoVA).
LINX was founded in 1994 by a group of ISPs and educational networks and is a founder member of Euro-IX, a Europe-wide alliance of Internet Exchanges. It is currently one of the largest neutral IXPs in Europe in terms of average throughput.
LINX is a not-for-profit organisation (Company Limited by Guarantee). Internet service providers join LINX as members and sign a memorandum of understanding. Members collectively 'own' the company and all members have a single vote at AGMs and EGMs in matters relating to finances, constitution, and what activities LINX may carry out. Members also periodically elect the LINX non-executive board of directors. Members meet at regular LINX meetings to discuss technical, corporate governance, and regulatory matters. LINX has a mandate to not actively compete with its members.
History.
Back in November 1994, using a donated piece of equipment no bigger than a video recorder and without any legal contracts, five UK-based Internet service providers (ISPs) linked their networks in order to exchange data and avoid paying high transatlantic bandwidth costs.
LINX effectively began when two ISPs (PIPEX and UKnet) linked their networks via a 64-kilobit serial link to save the cost and time delay involved in routing data across the Atlantic to US Internet exchanges.
When Demon Internet, UKERNA - the UK academic network - and other ISPs showed interest in establishing similar serial links, Keith Mitchell, then chief technical officer of PIPEX, initiated a meeting with BT to discuss the creation of a London-based Internet exchange.
PIPEX provided the LINX founders with a Cisco Catalyst 1200 switch with eight 10-megabit ports. Rack space was leased at a then virtually empty data centre operated by Telehouse International Corporation of Europe Ltd at Coriander Avenue in London's Docklands.
Switching the first data through the Telehouse hub was a momentous event that was accomplished by primarily technical specialists who were unconcerned about the formalities of legal contracts. However, while PIPEX continued to provide administrative and technical oversight, the need for a formal constitution was eventually recognised.
The solution was to form a company limited by guarantee. Lawyers produced a draft memorandum and constitution which was extensively modified by members. The company was formed in 1995 and a board of five non-executive directors was elected, with Keith Mitchell as the initial chairman.
Not for profit organisation.
From the beginning it was agreed that LINX would be a non-profit organisation run for the benefit of members and governed collectively through regular member meetings, a practice which continues to this day.
While PIPEX continued to provide administrative oversight - charged at cost to LINX - member meetings were held every two months (now every three months) to decide strategic direction. Eventually, it was accepted that the purely co-operative system of operation at LINX was becoming overly demanding and in late 1996 Keith Mitchell accepted the role of full-time chairman, working with a personal assistant from offices in Peterborough.
In the summer of 1996 LINX became the first Internet exchange in the world to deploy a 100-megabit switch - a Cisco Catalyst 5000. In January 1999 it pioneered the implementation of a Metropolitan Area Network (MAN) running over gigabit Ethernet connections.
As the Internet grew in popularity, legislators and law enforcement agencies inevitably decided it must be regulated. LINX increasingly found itself taking on non-core activities, such as providing expert advice on behalf of its members (and, therefore, the whole industry) to a wide range of official agencies.
In 1999 LINX appointed its first full-time regulation officer in Roland Perry to advise organisations such as the Home Office Internet Task Force, the National Hi-Tech Crime Unit, the Department of Trade and Industry and relevant departments of the EU Commission. In 2003 Roland was succeeded by Malcolm Hutty when Roland left to form his own advisory group.
LINX also actively and financially supports the Internet Watch Foundation which, since 1996, has worked to eradicate child abuse images from the UK Internet. Initial funding from LINX was instrumental in enabling the IWF to launch and the decision to create the IWF and define its charter was taken at a LINX member meeting in Heathrow.
LINX in the twenty-first century.
2000 - a training manager was appointed to introduce an accredited training programme for Internet engineers and technicians.
2001 - LINX amended its corporate structure to make the post of chairman non-executive and appointed its first chief executive officer, John Souter, previously UK managing director of German-owned Varetis Communications.
2002 - LINX was the first exchange to introduce 10G Ethernet operation, using equipment from Foundry Networks - in fact the second of their world-wide customers to deploy their technology.
2003 - This year saw the launch of the 'LINX from Anywhere' service, a facility that permits smaller ISPs to piggyback on the networks of existing members to obtain a secure, virtual presence on the LINX exchange without incurring the manpower and rackspace costs of having their own installation in London.
2004 - LINX considerably expanded its footprint, with four new points of presence (PoPs) - all in the Docklands area of London.
2005 - LINX members vote to make Public Affairs one of the company's "core functions", placing it on an equal footing with peering and interconnection.
2006 - LINX membership reached 200.
2008 - LINX expanded again opening three new PoPs, this time adding considerably to the geographical diversity by doing so in the City of London, North Acton and Slough. It also heralded new relationships with data centre operators, with the addition of Interxion and Equinix (to the pre-existing ones of Telehouse and Telecity).
2011 - The LINX primary LAN was redesigned to a VPLS-based infrastructure with a new vendor, Juniper Networks.
2012 - The ConneXions reseller programme launched. Also this year LINX became the first Internet Exchange in the world to install a Juniper PTX5000 in a live network and the exchange opened its first site outside London in Manchester (IXManchester).
2013 - In the Autumn two further local exchanges were launched. IXScotland in Edinburgh and LINX NoVA in North Virginia, USA, the exchange's first sites outside England.
2014 - In the Autumn a local exchange was launched, IXCardiff in Wales.
Technology.
In London LINX operates two physically separate networks or switching platforms on different architectures using equipment from different manufacturers (Extreme Networks and Juniper Networks). These networks are deployed over ten locations, each connected by multiple 100 Gigabit Ethernet or 10 Gigabit Ethernet connections over fibre networks.
The ten London locations are:
Connecting is also possible from remote locations via the 'LINX from Anywhere' scheme via a range of layer 2 service/MPLS carriers.
Redundancy of the network is managed using rapid-failover protection mechanisms such a VPLS service over an MPLS core (MPLS/VPLS) from Juniper and Extreme's Ethernet Automatic Protection System (EAPS). These restore connectivity within tenths of a second in the event of the loss of a network segment.
In addition to the main peering infrastructure, LINX provides managed private interconnections (via Single Mode Fibre) between LINX members, and works with a number of fibre carriers to provide rapid connection between LINX members for private circuits. These PI circuits may be used for any purpose but are mainly intended for Private Peering arrangements between members. Private Peering is sometimes more appropriate for managing large flows of traffic between ISPs and/or Content Providers.
Public Affairs.
One of LINX's core functions is to represent its members in matters of public policy. LINX's Public Affairs department works to obtain advance warning of public policy developments that could affect LINX members, to inform members about important policy developments at a time when it is still possible to influence them, and to educate, inform and influence regulators and legislators.
At the UK level, LINX liaises directly with policy makers on matters such as content regulation, telecoms regulation, privacy and data protection, e-mail spam, online fraud/phishing, law enforcement, counter-terrorism, and any other topics that affect LINX members.
At the European level, LINX is an active member of EuroISPA, and works with EuroISPA's other national associations to influence European policy.
LINX also works actively with partner organisations such as the Internet Watch Foundation (IWF) and the Internet Society (ISOC). LINX was instrumental in the creation of the Internet Watch Foundation, the UK hotline for reporting and removing child sexual abuse material from the Internet, and continues to advise the IWF as a member of its Funding Council.
As a mutual, membership based organisation, all of LINX's Public Affairs activity is informed and guided by extensive and ongoing consultation with LINX members.
Local Exchanges.
IXManchester.
Launched in April 2012 IXManchester was initially a single-site platform based on a standalone Brocade BigIron RX-16 switch, repurposed from the Primary LAN in London. A new design extends the network to two new sites - M247's IceColo & Joule House and brings in completely new hardware from Extreme Networks.
The new network will be configured as an EAPS protected ring, topology currently used for the secondary LINX LAN in London. All sites will use a Summit X670v switch, capable of connecting 48x10Gb member ports. Telecity Williams will also have a smaller Summit X460 switch for terminating slower ports.
The links between sites will be provisioned on a dark fibre provided by NTL and lit by LINX with the use of SFP+ DWDM transceivers and passive DWDM MUX units. Initial ISL capacity will be configured at 20Gbit/s but easily upgradeable to 80Gbit/s if required.
IXScotland.
Located in Pulsant’s South Gyle data centre in Edinburgh (formerly Scolocate), IXScotland was officially launched in November 2013. The site will use the latest hardware from Extreme Networks enabling all the advanced features expected from a modern exchange.
IXCardiff.
Located in BT’s Stadium House data centre in Cardiff. IXCardiff was officially launched on 15 October 2014 in an effort to decentralise UK internet traffic.
LINX NoVA.
Following discussions with the US network community, and its own members, LINX decided to launch an open peering exchange in North America in late 2013. Formally to be known as LINX NoVA, the exchange will be closely aligned with, and endorsed by, the newly formed Open-IX, using systems and processes that have proved successful in Europe and the wider world for nearly 20 years.
Based in the North Virginia/Washington DC area LINX NoVA initially spanned three data centres - EvoSwitch in Manassas, DuPont Fabros in Ashburn and Coresite in Reston - and will be connected using dark fibre lit by LINX. An agreement for a fourth colo site, also in Ashburn, is expected shortly.
The LINX NoVA exchange will use cutting edge MX960 router equipment from Juniper Networks capable of delivering 100G member ports from launch with the three PoPs all connected by diverse dark fiber lit by LINX.
LINX NoVA will be developed in the same way as the LINX primary LAN in London (VPLS, separated core and edge, etc.) but as a stand-alone site not connected back to the LANs operated in the UK.
Other activities.
LINX also carries on a number of non-core activities (NCAPs) from time to time, for example the LINX Accredited Internet Technician (LAIT) training programme.
Other services provided include a time service using atomic clocks, as well as hosting for other "Good of the Internet" services including various Internet statistics projects, numerous secondary name servers for various ccTLD domains, and instances of the F, K and I root nameservers.
See also.
Some other Internet exchange points in London:
LINX hosts a number of meetings where network operators can exchange information: see List of Internet Network Operators' Groups for a list of other organizations holding similar meetings.

</doc>
<doc id="47027" url="http://en.wikipedia.org/wiki?curid=47027" title="Cinna">
Cinna

Cinna was a "cognomen" that distinguished a patrician branch of the "gens Cornelia", particularly in the late Roman Republic.
Prominent members of this family include:

</doc>
<doc id="47028" url="http://en.wikipedia.org/wiki?curid=47028" title="Painter's algorithm">
Painter's algorithm

The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics. When projecting a 3D scene onto a 2D plane, it is necessary at some point to decide which polygons are visible, and which are hidden.
The name "painter's algorithm" refers to the technique employed by many painters of painting distant parts of a scene before parts which are nearer thereby covering some areas of distant parts. The painter's algorithm sorts all the polygons in a scene by their depth and then paints them in this order, farthest to closest. It will paint over the parts that are normally not visible — thus solving the visibility problem — at the cost of having painted invisible areas of distant objects. The ordering used by the algorithm is called a "'depth order'", and does not have to respect the numerical distances to the parts of the scene: the essential property of this ordering is, rather, that if one object obscures part of another then the first object is painted after the object that it obscures. Thus, a valid ordering can be described as a topological ordering of a directed acyclic graph representing occlusions between objects.
The distant mountains are painted first, followed by the closer meadows; finally, the trees, are painted. Although some trees are more distant from the viewpoint than some parts of the meadows, the ordering (mountains, meadows, trees) forms a valid depth order, because no object in the ordering obscures any part of a later object.
The algorithm can fail in some cases, including cyclic overlap or piercing polygons. In the case of cyclic overlap, as shown in the figure to the right, Polygons A, B, and C overlap each other in such a way that it is impossible to determine which polygon is above the others. In this case, the offending polygons must be cut to allow sorting. Newell's algorithm, proposed in 1972, provides a method for cutting such polygons. Numerous methods have also been proposed in the field of computational geometry.
The case of piercing polygons arises when one polygon intersects another. As with cyclic overlap, this problem may be resolved by cutting the offending polygons.
In basic implementations, the painter's algorithm can be inefficient. It forces the system to render each point on every polygon in the visible set, even if that polygon is occluded in the finished scene. This means that, for detailed scenes, the painter's algorithm can overly tax the computer hardware.
A reverse painter's algorithm is sometimes used, in which objects nearest to the viewer are painted first — with the rule that paint must never be applied to parts of the image that are already painted (unless they are partially transparent). In a computer graphic system, this can be very efficient, since it is not necessary to calculate the colors (using lighting, texturing and such) for parts of the more distant scene that are hidden by nearby objects. However, the reverse algorithm suffers from many of the same problems as the standard version.
These and other flaws with the algorithm led to the development of Z-buffer techniques, which can be viewed as a development of the painter's algorithm, by resolving depth conflicts on a pixel-by-pixel basis, reducing the need for a depth-based rendering order. Even in such systems, a variant of the painter's algorithm is sometimes employed. As Z-buffer implementations generally rely on fixed-precision depth-buffer registers implemented in hardware, there is scope for visibility problems due to rounding error. These are overlaps or gaps at joins between polygons. To avoid this, some graphics engine implementations "overrender", drawing the affected edges of both polygons in the order given by painter's algorithm. This means that some pixels are actually drawn twice (as in the full painter's algorithm) but this happens on only small parts of the image and has a negligible performance effect.

</doc>
<doc id="47029" url="http://en.wikipedia.org/wiki?curid=47029" title="Scanline rendering">
Scanline rendering

Scanline rendering is an algorithm for visible surface determination, in 3D computer graphics,
that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis. All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, then each row or scanline of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.
The main advantage of this method is that sorting vertices along the normal of the scanning plane reduces the number of comparisons between edges. Another advantage is that it is not necessary to translate the coordinates of all vertices from the main memory into the working memory—only vertices defining edges that intersect the current scan line need to be in active memory, and each vertex is read in only once. The main memory is often very slow compared to the link between the central processing unit and cache memory, and thus avoiding re-accessing vertices in main memory can provide a substantial speedup.
This kind of algorithm can be easily integrated with many other graphics techniques, such as the Phong reflection model or the Z-buffer algorithm.
Algorithm.
The usual method starts with edges of projected polygons inserted into buckets, one per scanline; the rasterizer maintains an active edge table ("AET"). Entries maintain sort links, X coordinates, gradients, and references to the polygons they bound. To rasterize the next scanline, the edges no longer relevant are removed; new edges from the current scanlines' Y-bucket are added, inserted sorted by X coordinate. The active edge table entries have X and other parameter information incremented.
Active edge table entries are maintained in an X-sorted list by bubble sort, effecting a change when 2 edges cross.
After updating edges, the active edge table is traversed in X order to emit only the visible spans, maintaining a Z-sorted active Span table, inserting and deleting the surfaces when edges are crossed.
Variants.
A hybrid between this and Z-buffering does away with the active edge table sorting, and instead rasterizes one scanline at a time into a Z-buffer, maintaining active polygon spans from one scanline to the next.
In another variant, an ID buffer is rasterized in an intermediate step, allowing deferred shading of the resulting visible pixels.
History.
The first publication of the scanline rendering technique was probably by Wylie, Romney, Evans, and Erdahl in 1967.
Other early developments of the scanline rendering method were by Bouknight in 1969, and Newell, Newell, and Sancha in 1972. Much of the early work on these methods was done in Ivan Sutherland's graphics group at the University of Utah, and at the Evans & Sutherland company in Salt Lake City.
Use in realtime rendering.
The early Evans & Sutherland ESIG line of image-generators (IGs) employed the technique in hardware 'on the fly', to generate images one raster-line at a time without a framebuffer, saving the need for then costly memory. Later variants used a hybrid approach.
The Nintendo DS is the latest hardware to render 3D scenes in this manner, with the option of caching the rasterized images into VRAM.
The sprite hardware prevalent in 1980s games machines can be considered a simple 2D form of scanline rendering.
The technique was used in the first Quake engine for software rendering of environments (but moving objects were Z-buffered over the top). Static scenery used BSP-derived sorting for priority. It proved better than Z-buffer/painter's type algorithms at handling scenes of high depth complexity with costly pixel operations (i.e. perspective-correct texture mapping without hardware assist). This use preceded the widespread adoption of Z-buffer-based GPUs now common in PCs.
Sony experimented with software scanline renderers on a second Cell processor during the development of the PlayStation 3, before settling on a conventional CPU/GPU arrangement.
Similar techniques.
A similar principle is employed in tiled rendering (most famously the PowerVR 3D chip); that is, primitives are sorted into screen space, then rendered in fast on-chip memory, one tile at a time. The Dreamcast provided a mode for rasterizing one row of tiles at a time for direct raster scanout, saving the need for a complete framebuffer, somewhat in the spirit of hardware scanline rendering.
Some software rasterizers use 'span buffering' (or 'coverage buffering'), in which a list of sorted, clipped spans are stored in scanline buckets. Primitives would be successively added to this datastructure, before rasterizing only the visible pixels in a final stage.
Comparison with Z-buffer algorithm.
The main advantage of scanline rendering over Z-buffering is that the number of times visible pixels are processed is kept to the absolute minimum which is always one time if no transparency effects are used—a benefit for the case of high resolution or expensive shading computations.
In modern Z-buffer systems, similar benefits can be gained through rough front-to-back sorting (approaching the 'reverse painters algorithm'), early Z-reject (in conjunction with hierarchical Z), and less common deferred rendering techniques possible on programmable GPUs.
Scanline techniques working on the raster have the drawback that overload is not handled gracefully.
The technique is not considered to scale well as the number of primitives increases. This is because of the size of the intermediate datastructures required during rendering—which can exceed the size of a Z-buffer for a complex scene.
Consequently, in contemporary interactive graphics applications, the Z-buffer has become ubiquitous. The Z-buffer allows larger volumes of primitives to be traversed linearly, in parallel, in a manner friendly to modern hardware. Transformed coordinates, attribute gradients, etc., need never leave the graphics chip; only the visible pixels and depth values are stored.

</doc>
<doc id="47030" url="http://en.wikipedia.org/wiki?curid=47030" title="Lucius Cornelius Cinna">
Lucius Cornelius Cinna

Lucius Cornelius Cinna (died 84 BC) was a four-time consul of the Roman Republic, serving four consecutive terms from 87 to 84 BC, and a member of the ancient Roman Cinna family of the Cornelii gens.
Cinna's influence in Rome exacerbated the tensions which existed between Gaius Marius and Lucius Cornelius Sulla. After the death of Marius, he became the leading power in Rome until his own death. His main impact upon Roman politics was his ability to veil his tyranny and make it appear that he was working under a constitutional government. His policies also impinged on Julius Caesar, who married his daughter.
Rise to power.
Not much is known about Cinna before his bid for the consulship of 87 BC. He had praetorian rank in the Social War (91–88 BC), and had most likely also been praetor previous to this time. Cinna was elected as Roman consul in 87 BC, but historians disagree about who supported his election and what his own original political goals and causes were. All seem to agree on a basic chain of events, however. Cinna was elected at a time when Sulla (the current consul) was very unpopular with the lower classes and the Latin allies, because he had sided with the Roman Senate, blocking the advancement of their rights as citizens. The people had intentionally elected candidates (probably for Tribune) who were not supported by Sulla. Sulla had a little more control over the election for consul, or at least, had enough power to be certain no one who supported his rival, Marius could be elected.
Sulla seems to have supported Cinna as a compromise candidate, but clearly did not trust him, as seen from an anecdote from Plutarch. Immediately after Cinna's election, Sulla made Cinna swear loyalty to him by taking a stone up to the Capitol and casting it down, "praying that, if he failed to preserve his goodwill for Sulla, he might be thrown out of Rome as the stone was thrown out of his hand". Somehow then, Cinna had enough support to be elected. Various theories on who supported him and why are postulated based on what he did while in office, but all agree that Sulla was correct in his distrust. Gnaeus Octavius was elected as Cinna's colleague under relatively similar circumstances, though Octavius probably had more support from Sulla.
First consulship and exile.
One of Cinna's first decisions as consul was not to let his oath to Sulla influence his decisions as consul. Cinna argued that the oath should not prevent him from helping the people of Rome. Soon after this, Cinna sought to remove Sulla from the city. He brought some sort of charge against Sulla soon after coming to power. Sulla, rather than facing the charge, escaped with his army and led them to fight the army of Mithridates VI of Pontus in Boeotia. This left only Octavius and the Senate to defend the causes of Sulla in Rome. Cinna eventually supported many causes, which leads to some debate concerning his original goals and to accusations that he chose his issues based on bribes.
Two causes predominated, that of the exiles and that of the Italians. Marius and his supporters, as well as many prominent supporters of Publius Sulpicius Rufus, had been exiled from Rome under Sulla's rule, but were still very popular amongst the people. It is clear that there were later connections between Cinna and this group (see "Preparations while in exile"), but it is not clear at what point he took up this cause. The other cause, to which Cinna can be more clearly connected, is that of the “novus homo” or “new citizen”. These were members of Italian tribes who had been promised citizenship as a condition of peace in the Social War. Technically they had been given citizenship, but in such a way that they had no real power. Cinna, even before his election, seems to have favored this cause. Certainly after his election, he worked to increase their rights, fighting against Octavius, who tried to maintain the status quo. This feud ended in one of the largest street fights ever to occur in Rome, between the supporters of Octavius and the supporters of Cinna. Although Appian states that Cinna had no support from the “old citizens” in anything, including the street fight, this is highly unlikely, as none of his laws would have been a threat without at least some support from this quarter. Why the “old citizens” supported him, and how many of them supported him, is entirely unknown. Octavius used the street fight to justify exiling Cinna immediately, deposing him of his office and citizenship, an accusation that seems to have stuck with many historians, who accused Cinna of acting as a dictator. The deposition of Cinna was unconstitutional and the only instance of its kind in the history of the Roman Republic.
Preparations while in exile.
Cinna then began to raise an army from the Italian countryside. His connections with the Italian groups seem to have been quite strong, as they quickly joined his forces (although accusations of bribery abound among the ancient historians). At this point, the connections between Marius and Cinna become quite clear. Because they shared the support of the Italians, Cinna was willing to join forces with Marius. Together they planned to retake the city. Cinna and Marius’ army moved through the countryside, cutting off supply routes and cities used for food storage from the city.
Invasion and slaughter of Rome.
The first major battle of the conflict occurred at the Janiculum, where Octavius’ forces prevailed, but with heavy losses, including the general Pompeius Strabo. This demoralized Octavius’ army, but did not hinder the siege of Cinna and Marius, further weakening Rome. Eventually, after various skirmishes around the outskirts of Rome, negotiators secured Cinna’s assurance that he would not “willingly cause anyone’s death on reentering Rome”. Thus, in late 87 BC, Cinna was reinstated as consul and the armies reentered the city. As Cinna and his bodyguard entered, however, Marius refused to enter Rome until his exile was officially repealed. The Senate quickly began to vote to approve this, but before it finished, Marius had given up all pretense and entered the city with his bodyguard, the Bardyiae. This unit consisted of Marius’ slaves who killed at Marius’ orders. Marius, according to the ancient historians, filled the city with blood, slaughtering anyone who remotely supported Sulla, had a lot of property, or was a personal enemy of Marius. These claims are most likely exaggerated, as they do not appear in Sulla’s memoirs, a source that would seem biased against Marius. These seem to appear later, but all agree that Cinna distanced himself from the indiscriminate slaughter, ordering only the deaths of Octavius and others who were direct political threats.
Eventually, “Cinna had had enough of murder”, and he and Quintus Sertorius, a general who supported Marius and later governed Spain, had their troops ambush the sleeping Bardyiae, ending their reign of terror. Soon after this, in 86 BC, Marius and Cinna were reelected for consulship. Seventeen days after attaining his much sought seventh consulship, Marius died. This began the era many historians have termed the “Dominatio Cinnae” (Domination of Cinna).
Dominatio Cinnae.
What occurred during this period is not as well documented as other parts of Cinna’s life. After the death of Marius, Lucius Valerius Flaccus (suffect consul 86 BC) succeeded him. Flaccus’ major contribution was the submission of a bill attempting to solve a financial crisis. The Social War had caused a financial depression, resulting in exorbitant interest on loans and collapsing financial confidence in Rome after the start of the Mithridatic War. Counterfeiting became rampant, forcing Cinna and the government to develop testing stations to discover the false coins and replace them with good ones.
In 85 BC, Cinna attempted to revive Sulpicius' bill to solidify the citizenship of the Italian groups, but it was not in practice quickly as the census the next year lists 463,000 citizens. This is not a large enough increase from 115/114 BC, where the total was 394,336 to have included the Italians. Much of what Cinna’s attention while ruling Rome was focused on was dealing with Sulla. Flaccus soon took over the war against Mithridates, which Sulla interpreted as a threat; Sulla then moved to intercept Flaccus.
Flaccus was disliked by his soldiers and many deserted to Sulla. That any remained was due to the legate Fimbria, who used his popularity and influence with the troops to convince them to stay. This did not benefit Flaccus for long though, as Fimbria later had the army rebel against Flaccus and continue against Mithridates under his own leadership. Fimbria tried to offer peace with Sulla, but Sulla and Mithridates were already in negotiations which were favorable to both parties, therefore negating any necessity for Fimbria’s offer to Mithridates. After confirming peace with Mithridates, Sulla went to negotiate with Fimbria, at which point Fimbria’s army deserted to Sulla and Fimbria committed suicide.
After finishing his war, Sulla returned to Italy. He sent out letters to the Italians in order to soothe the fear that he would take away their citizenship. Sulla also sent a letter to the Senate regaling them of his victories over Mithridates and assuring them that he had received those exiled by Cinna and that he would provide swift retribution to those who were guilty of causing himself and the Senate to suffer. Cinna and his colleague, Carbo, prepared for war. They postponed the elections of that year, declaring themselves re-elected so that they would not have to return to Rome early to participate in an election. It is unlikely that this was contested because Cinna and his allies had enough power that no one dared to run in opposition to them. While complying with the constitution, this allowed Cinna to act as monarch while still appearing to follow the will of the population. As Cinna and Carbo doubled their efforts for war with the looming threat of Sulla, Cinna was unaware that it would not be battle, but his preparations for war, which would cost him his life.
Death.
Cinna was murdered in a mutiny of his own soldiers in 84 BC. He had been working to transport his troops across the Adriatic in order to meet Sulla on foreign soil. The troops were not eager for the upcoming fight, which promised no booty. Their dissatisfaction increased when they heard that the second convoy of troops, still in transit, had been shipwrecked in a storm. The survivors had returned to their homes. Cinna ordered an assembly in order to frighten the troops into obedience. One of his lictors struck a soldier who had been standing in the way as Cinna entered the gathering, and when the soldier hit back, Cinna ordered his arrest. This caused another soldier to throw a stone at Cinna, which struck him. The spirit of the mob then took hold as more missiles were thrown and the nearest soldiers stabbed Cinna to death.
Plutarch tells a slightly different story, stating that Pompey visited Cinna’s camp and escaped, after having been accused of doing some wrongdoing. The soldiers assumed that Cinna had helped Pompey escape and killed Cinna for this breach of their trust. In both accounts, Cinna was murdered not due to his politics, but as more of a brief flare up of the mob spirit within his troops. Christoph Bulst argues that Cinna was killed in “an absolutely un-political mutiny,” pointing out that there is no mention of specific opposition to Cinna, and that he did not even feel the need to travel with a bodyguard.
Family.
Cinna was married to Annia, who was the daughter of Annius (unidentifiable). They had two daughters and a son. One of his daughters married one of Cinna’s supporters named Gnaeus Domitius Ahenobarbus. His other daughter, Cornelia, married Julius Caesar around 84 BC and died in 69 BC after bearing a daughter Julia.
Cinna’s son, the younger Lucius Cornelius Cinna, fled Italy when Sulla returned, most likely to Spain. He returned briefly in 78 BC to help in the rebellion of Lepidus, then again fled to Spain after the plot fell through. He was able to return to Rome in 78 BC due to Lex Plautia, which extended an amnesty to all exiles of the civil war era. The son of this Cinna was Gnaeus Cornelius Cinna Magnus, who was pardoned twice, once after his support for Marc Antony, then again later for conspiracy against the emperor Augustus. Surprisingly he was then honored as a consul in AD 5 with the Emperor.
Cinna's legacy.
Lucius Cornelius Cinna was important within Roman history. He played an important role in the dispute between Gaius Marius and Lucius Sulla, allowing Marius to return to Rome for his seventh consulship. Cinna’s rule was not well documented and many argue that his only goal was his own advancement. His alliance with Marius was to better his interests rather than as a statement of his politics. He attempted to become a tyrant behind a veiled disguise of a republic under a strict constitution. His only real cause was that of the equalization of the Italian groups. Although he was not as well documented as his contemporaries, Cinna was still an essential player in the fall of the system of the Roman Republic, ushering in a thinly veiled form of tyranny.

</doc>
<doc id="47037" url="http://en.wikipedia.org/wiki?curid=47037" title="Offa of Mercia">
Offa of Mercia

Offa was King of Mercia, a kingdom of Anglo-Saxon England, from 757 until his death in July 796. The son of Thingfrith and a descendant of Eowa, Offa came to the throne after a period of civil war following the assassination of Æthelbald. Offa defeated the other claimant, Beornred. In the early years of Offa's reign, it is likely that he consolidated his control of midland peoples such as the Hwicce and the Magonsæte. Taking advantage of instability in the kingdom of Kent to establish himself as overlord, Offa also controlled Sussex by 771, though his authority did not remain unchallenged in either territory. In the 780s he extended Mercian Supremacy over most of southern England, allying with Beorhtric of Wessex, who married Offa's daughter Eadburh, and regained complete control of the southeast. He also became the overlord of East Anglia and had King Æthelberht II of East Anglia beheaded in 794, perhaps for rebelling against him.
Offa was a Christian king who came into conflict with the Church, particularly with Jaenberht, the Archbishop of Canterbury. Offa managed to persuade Pope Adrian I to divide the archdiocese of Canterbury in two, creating a new archdiocese of Lichfield. This reduction in the power of Canterbury may have been motivated by Offa's desire to have an archbishop consecrate his son Ecgfrith of Mercia as king, since it is possible Jaenberht refused to perform the ceremony, which took place in 787. Offa had a dispute with the Bishop of Worcester, which was settled in the Council of Brentford in 781.
Many surviving coins from Offa's reign carry elegant depictions of him, and the artistic quality of these images exceeds that of the contemporary Frankish coinage. Some of his coins carry images of his wife, Cynethryth – the only Anglo-Saxon queen ever depicted on a coin. Only three gold coins of Offa's have survived: one is a copy of an Abbasid dinar of 774 and carries Arabic text on one side, with "Offa Rex" on the other side. The gold coins are of uncertain use but may have been struck to be used as alms or for gifts to Rome.
Many historians regard Offa as the most powerful Anglo-Saxon king before Alfred the Great. His dominance never extended to Northumbria, though he gave his daughter Ælfflæd in marriage to the Northumbrian king Æthelred I in 792. Historians once saw his reign as part of a process leading to a unified England, but this is no longer the majority view. In the words of a recent historian: "Offa was driven by a lust for power, not a vision of English unity; and what he left was a reputation, not a legacy." Offa died in 796; his son, Ecgfrith, succeeded him, but reigned for less than five months before Coenwulf of Mercia became king.
Background and sources.
In the first half of the eighth century, the dominant Anglo-Saxon ruler was King Æthelbald of Mercia, who by 731 had become the overlord of all the provinces south of the river Humber. Æthelbald was one of a number of strong Mercian kings who ruled from the mid-seventh century to the early ninth, and it was not until the reign of Egbert of Wessex in the ninth century that Mercian power began to wane.
The power and prestige that Offa attained made him one of the most significant rulers in Early Medieval Britain, though no contemporary biography of him survives. A key source for the period is the "Anglo-Saxon Chronicle", a collection of annals in Old English narrating the history of the Anglo-Saxons. The "Chronicle" was a West Saxon production, however, and is sometimes thought to be biased in favour of Wessex; hence it may not accurately convey the extent of power achieved by Offa, a Mercian. That power can be seen at work in charters dating from Offa's reign. Charters were documents which granted land to followers or to churchmen and were witnessed by the kings who had the authority to grant the land. A charter might record the names of both a subject king and his overlord on the witness list appended to the grant. Such a witness list can be seen on the Ismere Diploma, for example, where Æthelric, son of king Oshere of the Hwicce, is described as a "subregulus", or subking, of Æthelbald's. The eighth-century monk and chronicler the Venerable Bede wrote a history of the English church called "Historia Ecclesiastica Gentis Anglorum"; the history only covers events up to 731, but as one of the major sources for Anglo-Saxon history it provides important background information for Offa's reign.
Offa's Dyke, most of which was probably built in his reign, is a testimony to the extensive resources Offa had at his command and his ability to organise them. Other surviving sources include a problematic document known as the Tribal Hidage, which may provide further evidence of Offa's scope as a ruler, though its attribution to his reign is disputed. A significant corpus of letters dates from the period, especially from Alcuin, an English deacon and scholar who spent over a decade at Charlemagne's court as one of his chief advisors, and corresponded with kings, nobles and ecclesiastics throughout England. These letters in particular reveal Offa's relations with the continent, as does his coinage, which was based on Carolingian examples.
Ancestry and family.
Offa's ancestry is given in the Anglian collection, a set of genealogies that include lines of descent for four Mercian kings. All four lines descend from Pybba, who ruled Mercia early in the seventh century. Offa's line descends through Pybba's son Eowa and then through three more generations: Osmod, Eanwulf and Offa's father, Thingfrith. Æthelbald, who ruled Mercia for most of the forty years before Offa, was also descended from Eowa according to the genealogies: Offa's grandfather, Eanwulf, was Æthelbald's second cousin. Æthelbald granted land to Eanwulf in the territory of the Hwicce, and it is possible that Offa and Æthelbald were from the same branch of the family. In one charter Offa refers to Æthelbald as his kinsman, and Headbert, Æthelbald's brother, continued to witness charters after Offa rise to power.
Offa's wife was Cynethryth, whose ancestry is unknown. The couple had a son, Ecgfrith, and four daughters: Ælfflæd, Eadburh, Æthelburh and Æthelswith. It has been speculated that Æthelburh was the abbess who was a kinswoman of King Ealdred of the Hwicce, but there are other prominent women named Æthelburh during that period.
Early reign, the midland territories, and the Middle and East Saxons.
Æthelbald, who had ruled Mercia since 716, was assassinated in 757. According to a later continuation of Bede's "Historia Ecclesiastica" (written anonymously after Bede's death) the king was "treacherously murdered at night by his own bodyguards," though the reason why is unrecorded. Æthelbald was initially succeeded by Beornred, about whom little is known. The continuation of Bede comments that Beornred "ruled for a little while, and unhappily", and adds that "the same year, Offa, having put Beornred to flight, sought to gain the kingdom of the Mercians by bloodshed." It is possible that Offa did not gain the throne until 758, however, since a charter of 789 describes Offa as being in the thirty-first year of his reign.
The conflict over the succession suggests that Offa needed to re-establish control over Mercia's traditional dependencies, such as the Hwicce and the Magonsæte. Charters dating from the first two years of Offa's reign show the Hwiccan kings as "reguli", or kinglets, under his authority; and it is likely that he was also quick to gain control over the Magonsæte, for whom there is no record of an independent ruler after 740. Offa was probably able to exert control over the kingdom of Lindsey at an early date, as it appears that the independent dynasty of Lindsey had disappeared by this time.
Little is known about the history of the East Saxons during the eighth century, but what evidence there is indicates that both London and Middlesex, which had been part of the kingdom of Essex, were finally brought under Mercian control during the reign of Æthelbald. Both Æthelbald and Offa granted land in Middlesex and London as they wished; in 767 a charter of Offa's disposed of land in Harrow without a local ruler as witness. It is likely that both London and Middlesex were quickly under Offa's control at the start of his reign. The East Saxon royal house survived the eighth century, so it is probable that the kingdom of Essex retained its native rulers, but under strong Mercian influence, for most or all of the eighth century.
It is unlikely that Offa had significant influence in the early years of his reign outside the traditional Mercian heartland. The overlordship of the southern English which had been exerted by Æthelbald appears to have collapsed during the civil strife over the succession, and it is not until 764, when evidence emerges of Offa's influence in Kent, that Mercian power can be seen expanding again.
Kent and Sussex.
Offa appears to have exploited an unstable situation in Kent after 762. Kent had a long tradition of joint kingship, with east and west Kent under separate kings, though one king was typically dominant. Prior to 762 Kent was ruled by Æthelberht II and Eadberht I; Eadberht's son Eardwulf is also recorded as a king. Æthelberht died in 762, and Eadberht and Eardwulf are last mentioned in that same year. Charters from the next two years mention other kings of Kent, including Sigered, Eanmund and Heahberht. In 764, Offa granted land at Rochester in his own name, with Heahberht on the witness list as king of Kent. Another king of Kent, Egbert, appears on a charter in 765 along with Heahberht; the charter was subsequently confirmed by Offa. Offa's influence in Kent at this time is clear, and it has been suggested that Heahberht was installed by Offa as his client. There is less agreement among historians on whether Offa had general overlordship of Kent thereafter. He is known to have revoked a charter of Egbert's on the grounds that "it was wrong that his thegn should have presumed to give land allotted to him by his lord into the power of another without his witness", but the date of Egbert's original grant is unknown, as is the date of Offa's revocation of it. It may be that Offa was the effective overlord of Kent from 764 until at least 776. The limited evidence for Offa's direct involvement in the kingdom between 765 and 776 includes two charters of 774 in which he grants land in Kent; but there are doubts about their authenticity, so Offa's intervention in Kent prior to 776 may have been limited to the years 764–765.
The "Anglo-Saxon Chronicle" records that "the Mercians and the inhabitants of Kent fought at Otford" in 776, but does not give the outcome of the battle. It has traditionally been interpreted as a Mercian victory, but there is no evidence for Offa's authority over Kent until 785: a charter from 784 mentions only a Kentish king named Ealhmund, which may indicate that the Mercians were in fact defeated at Otford. The cause of the conflict is also unknown: if Offa was ruling Kent before 776, the battle of Otford was probably a rebellion against Mercian control. However, Ealhmund does not appear again in the historical record, and a sequence of charters by Offa from the years 785–789 makes his authority clear. During these years he treated Kent "as an ordinary province of the Mercian kingdom", and his actions have been seen as going beyond the normal relation of overlordship and extending to the annexation of Kent and the elimination of a local royal line. After 785, in the words of one historian, "Offa was the rival, not the overlord, of Kentish kings". Mercian control lasted until 796, the year of Offa's death, when Eadberht Praen was temporarily successful in regaining Kentish independence.
Ealhmund was probably the father of Egbert of Wessex, and it is possible that Offa's interventions in Kent in the mid-780s are connected to the subsequent exile of Egbert to Francia. The "Chronicle" claims that when Egbert invaded Kent in 825, the men of the southeast turned to him "because earlier they were wrongly forced away from his relatives". This is likely to be an allusion to Ealhmund, and may imply that Ealhmund had a local overlordship of the southeastern kingdoms. If so, Offa's intervention was probably intended to gain control of this relationship and take over the dominance of the associated kingdoms.
The evidence for Offa's involvement in the kingdom of Sussex comes from charters, and as with Kent there is no clear consensus among historians on the course of events. What little evidence survives that bears on Sussex's kings indicates that several kings ruled at once, and it may never have formed a single kingdom. It has been argued that Offa's authority was recognised early in his reign by local kings in western Sussex, but that eastern Sussex (the area around Hastings) submitted to him less readily. Simeon of Durham, a twelfth-century chronicler, records that in 771 Offa defeated "the people of Hastings", which may record the extension of Offa's dominion over the entire kingdom. However, doubts have been expressed about the authenticity of the charters which support this version of events, and it is possible that Offa's direct involvement in Sussex was limited to a short period around 770–771. After 772, there is no further evidence of Mercian involvement in Sussex until c. 790, and it may be that Offa gained control of Sussex in the late 780s, as he did in Kent.
East Anglia, Wessex and Northumbria.
In East Anglia, Beonna probably became king in about 758. Beonna's first coinage predates Offa's own, and implies independence from Mercia. Subsequent East Anglian history is quite obscure, but in 779 Æthelberht II became king, and was independent long enough to issue coins of his own. In 794, according to the "Anglo-Saxon Chronicle", "King Offa ordered King Æthelberht's head to be struck off". Offa minted pennies in East Anglia in the early 790s, so it is likely that Æthelberht rebelled against Offa and was beheaded as a result. Accounts of the event have survived in which Aethelberht is killed through the machinations of Offa's wife Cynethryth, but the earliest manuscripts in which these possibly legendary accounts are found date from the eleventh and twelfth centuries, and recent historians do not regard them with confidence. The legend also claims that Æthelberht was killed at Sutton St. Michael and buried four miles (6 km) to the south at Hereford, where his cult flourished, becoming at one time second only to Canterbury as a pilgrimage destination.
To the south of Mercia, Cynewulf came to the throne of Wessex in 757 and recovered much of the border territory that Æthelbald had conquered from the West Saxons. Offa won an important victory over Cynewulf at the Battle of Bensington (in Oxfordshire) in 779, reconquering some of the land along the Thames. No indisputably authentic charters from before this date show Cynewulf in Offa's entourage, and there is no evidence that Offa ever became Cynewulf's overlord. In 786, after the murder of Cynewulf, Offa may have intervened to place Beorhtric on the West Saxon throne. Even if Offa did not assist Beorhtric's claim, it seems likely that Beorhtric to some extent recognised Offa as his overlord shortly thereafter. Offa's currency was used across the West Saxon kingdom, and Beorhtric had his own coins minted only after Offa's death. In 789, Beorhtric married Eadburh, a daughter of Offa; the "Chronicle" records that the two kings combined to exile Egbert to Francia for "three years", adding that "Beorhtric helped Offa because he had his daughter as his queen". Some historians believe that the "Chronicle"'s "three years" is an error, and should read "thirteen years", which would mean Egbert's exile lasted from 789 to 802, but this reading is disputed. Eadburh is mentioned by Asser, a ninth-century monk who wrote a biography of Alfred the Great: Asser says that Eadburh had "power throughout almost the entire kingdom", and that she "began to behave like a tyrant after the manner of her father". Whatever power she had in Wessex was no doubt connected with her father's overlordship.
If Offa did not gain the advantage in Wessex until defeating Cynewulf in 779, it may be that his successes south of the river were a necessary prerequisite to his interventions in the southeast. In this view, Egbert of Kent's death in about 784 and Cynewulf's death in 786 were the events that allowed Offa to gain control of Kent and bring Beorhtric into his sphere of influence. This version of events also assumes that Offa did not have control of Kent after 764–765, as some historians believe.
Offa's marital alliances extended to Northumbria when his daughter Ælfflæd married Æthelred I of Northumbria at Catterick in 792. However, there is no evidence that Northumbria was ever under Mercian control during Offa's reign.
Wales and Offa's Dyke.
Offa was frequently in conflict with the various Welsh kingdoms. There was a battle between the Mercians and the Welsh at Hereford in 760, and Offa is recorded as campaigning against the Welsh in 778, 784 and 796 in the tenth-century "Annales Cambriae".
The best known relic associated with Offa's time is Offa's Dyke, a great earthen barrier that runs approximately along the border between England and Wales. It is mentioned by the monk Asser in his biography of Alfred the Great: "a certain vigorous king called Offa ... had a great dyke built between Wales and Mercia from sea to sea". The dyke has not been dated by archaeological methods, but most historians find no reason to doubt Asser's attribution. Early names for the dyke in both Welsh and English also support the attribution to Offa. Despite Asser's comment that the dyke ran "from sea to sea", it is now thought that the original structure only covered about two-thirds of the length of the border: in the north it ends near Llanfynydd, less than five miles (8 km) from the coast, while in the south it stops at Rushock Hill, near Kington in Herefordshire, less than fifty miles (80 km) from the Bristol Channel. The total length of this section is about sixty-four miles (103 km). Other earthworks exist along the Welsh border, of which Wat's Dyke is one of the largest, but it is not possible to date them relative to each other and so it cannot be determined whether Offa's Dyke was a copy of or the inspiration for Wat's Dyke.
The construction of the dyke suggests that it was built to create an effective barrier and to command views into Wales. This implies that the Mercians who built it were free to choose the best location for the dyke. There are settlements to the west of the dyke that have names that imply they were English by the eighth century, so it may be that in choosing the location of the barrier the Mercians were consciously surrendering some territory to the native Britons. Alternatively it may be that these settlements had already been retaken by the Welsh, implying a defensive role for the barrier. The effort and expense that must have gone into building the dyke are impressive, and suggest that the king who had it built (whether Offa or someone else) had considerable resources at his disposal. Other substantial construction projects of a similar date do exist, however, such as Wat's Dyke and the Danevirke, in what is now Denmark, as well as such sites as Stonehenge from millennia earlier. The dyke can be regarded in the light of these counterparts as the largest and most recent great construction of the preliterate inhabitants of Britain.
Church.
Offa ruled as a Christian king, but despite being praised by Charlemagne's advisor, Alcuin, for his piety and efforts to "instruct [his people] in the precepts of God", he came into conflict with Jaenberht, the Archbishop of Canterbury. Jaenberht had been a supporter of Egbert II of Kent, which may have led to conflict in the 760s when Offa is known to have intervened in Kent. Offa rescinded grants made to Canterbury by Egbert, and it is also known that Jaenberht claimed the monastery of Cookham, which was in Offa's possession.
In 786 Pope Adrian I sent papal legates to England to assess the state of the church and provide canons (ecclesiastical decrees) for the guidance of the English kings, nobles and clergy. This was the first papal mission to England since Augustine had been sent by Pope Gregory the Great in 597 to convert the Anglo-Saxons. The legates were George, the Bishop of Ostia, and Theophylact, the Bishop of Todi. They visited Canterbury first, and then were received by Offa at his court. Both Offa and Cynewulf, king of the West Saxons, attended a council where the goals of the mission were discussed. George then went to Northumbria, while Theophylact visited Mercia and "parts of Britain". A report on the mission, sent by the legates to Pope Adrian, gives details of a council held by George in Northumbria, and the canons issued there, but little detail survives of Theophylact's mission. After the northern council George returned to the south and another council was held, attended by both Offa and Jaenberht, at which further canons were issued.
In 787, Offa succeeded in reducing the power of Canterbury through the establishment of a rival archdiocese at Lichfield. The issue must have been discussed with the papal legates in 786, although it is not mentioned in the accounts that have survived. The "Anglo-Saxon Chronicle" reports a "contentious synod" in 787 at Chelsea, which approved the creation of the new archbishopric. It has been suggested that this synod was the same gathering as the second council held by the legates, but historians are divided on this issue. Hygeberht, already Bishop of Lichfield, became the new archdiocese's first and only archbishop, and by the end of 788 he received the pallium, a symbol of his authority, from Rome. The new archdiocese included the sees of Worcester, Hereford, Leicester, Lindsey, Dommoc and Elmham; these were essentially the midland Anglian territories. Canterbury retained the sees in the south and southeast.
The few accounts of the creation of the new archbishopric date from after the end of Offa's reign. Two versions of the events appear in the form of an exchange of letters between Coenwulf, who became king of Mercia shortly after Offa's death, and Pope Leo III, in 798. Coenwulf asserts in his letter that Offa wanted the new archdiocese created out of enmity for Jaenberht; but Leo responds that the only reason the papacy agreed to the creation was because of the size of the kingdom of Mercia. Both Coenwulf and Leo had their own reasons for representing the situation as they did: Coenwulf was entreating Leo to make London the sole southern archdiocese, while Leo was concerned to avoid the appearance of complicity with the unworthy motives Coenwulf imputed to Offa. These are therefore partisan comments. However, both the size of Offa's territory and his relationship with Jaenberht and Kent are indeed likely to have been factors in Offa's request for the creation of the new archdiocese. Coenwulf's version has independent support, with a letter from Alcuin to Archbishop Æthelheard giving his opinion that Canterbury's archdiocese had been divided "not, as it seems, by reasonable consideration, but by a certain desire for power". Æthelheard himself later said that the award of a pallium to Lichfield depended on "deception and misleading suggestion".
Another possible reason for the creation of an archbishopric at Lichfield relates to Offa's son, Ecgfrith. After Hygeberht became archbishop, he consecrated Ecgfrith as king; the ceremony took place within a year of Hygeberht's elevation. It is possible that Jaenberht refused to perform the ceremony, and that Offa needed an alternative archbishop for that purpose. The ceremony itself is noteworthy for two reasons: it is the first recorded consecration of any English king, and it is unusual in that it asserted Ecgfrith's royal status while his father was still alive. Offa would have been aware that Charlemagne's sons, Pippin and Louis, had been consecrated as kings by Pope Adrian, and probably wished to emulate the impressive dignity of the Frankish court. Other precedents did exist: Æthelred of Mercia is said to have nominated his son Coenred as king during his lifetime, and Offa may have known of Byzantine examples of royal consecration.
Despite the creation of the new archdiocese, Jaenberht retained his position as the senior cleric in the land, with Hygeberht conceding his precedence. When Jaenberht died in 792, he was replaced by Æthelheard, who was consecrated by Hygeberht, now senior in his turn. Subsequently Æthelheard appears as a witness on charters and presides at synods without Hygeberht, so it appears that Offa continued to respect Canterbury's authority.
A letter from Pope Adrian to Charlemagne survives which makes reference to Offa, but the date is uncertain; it may be as early as 784 or as late as 791. In it Adrian recounts a rumour that had reached him: Offa had reportedly proposed to Charlemagne that Adrian should be deposed, and replaced by a Frankish pope. Adrian disclaims all belief in the rumour, but it is clear it had been a concern to him. The enemies of Offa and Charlemagne, described by Adrian as the source of the rumour, are not named. It is unclear whether this letter is related to the legatine mission of 786; if it predates it, then the mission might have been partly one of reconciliation, but the letter might well have been written after the mission.
Offa was a generous patron of the church, founding several churches and monasteries, often dedicated to St Peter. Among these was St Albans Abbey, which he probably founded in the early 790s. He also promised a yearly gift of 365 mancuses to Rome; a mancus was a term of account equivalent to thirty silver pennies, derived from Abbasid gold coins that were circulating in Francia at the time. Control of religious houses was one way in which a ruler of the day could provide for his family, and to this end Offa ensured (by acquiring papal privileges) that many of them would remain the property of his wife or children after his death. This policy of treating religious houses as worldly possessions represents a change from the early eighth century, when many charters showed the foundation and endowment of small minsters, rather than the assignment of those lands to laypeople. In the 770s, an abbess named Æthelburh (who may have been the same person as Offa's daughter of that name) held multiple leases on religious houses in the territory of the Hwicce; her acquisitions have been described as looking "like a speculator assembling a portfolio". Æthelburh's possession of these lands foreshadows Cynethryth's control of religious lands, and the pattern was continued in the early ninth century by Cwoenthryth, the daughter of King Coenwulf.
Either Offa or Ine of Wessex is traditionally supposed to have founded the Schola Saxonum in Rome, in what is today the Roman rione, or district, of Borgo. The Schola Saxonum took its name from the militias of Saxons who served in Rome, but it eventually developed into a hostelry for English visitors to the city.
European connections.
Offa's diplomatic relations with Europe are well documented, but appear to belong only to the last dozen years of his reign. In letters dating from the late 780s or early 790s, Alcuin congratulates Offa for encouraging education and greets Offa's wife and son, Cynethryth and Ecgfrith. In about 789, or shortly before, Charlemagne proposed that his son Charles marry one of Offa's daughters, most likely Ælfflæd. Offa countered with a request that his son Ecgfrith should also marry Charlemagne's daughter Bertha: Charlemagne was outraged by the request, and broke off contact with Britain, forbidding English ships from landing in his ports. Alcuin's letters make it clear that by the end of 790 the dispute was still not resolved, and that Alcuin was hoping to be sent to help make peace. In the end diplomatic relations were restored, at least partly by the agency of Gervold, the abbot of St Wandrille.
Charlemagne sought support from the English church at the council of Frankfurt in 794, where the canons passed in 787 at the Second Council of Nicaea were repudiated, and the heresies of two Spanish bishops, Felix and Elipandus, were condemned. In 796 Charlemagne wrote to Offa; the letter survives and refers to a previous letter of Offa's to Charlemagne. This correspondence between the two kings produced the first surviving documents in English diplomatic history. The letter is primarily concerned with the status of English pilgrims on the continent and with diplomatic gifts, but it reveals much about the relations between the English and the Franks. Charlemagne refers to Offa as his "brother", and mentions trade in black stones, sent from the continent to England, and cloaks (or possibly cloths), traded from England to the Franks. Charlemagne's letter also refers to exiles from England, naming Odberht, who was almost certainly the same person as Eadberht Praen, among them. Egbert of Wessex was another refugee from Offa who took shelter at the Frankish court. It is clear that Charlemagne's policy included support for elements opposed to Offa; in addition to sheltering Egbert and Eadberht he also sent gifts to Æthelred I of Northumbria.
Events in southern Britain to 796 have sometimes been portrayed as a struggle between Offa and Charlemagne, but the disparity in their power was enormous. By 796 Charlemagne had become master of an empire which stretched from the Atlantic Ocean to the Great Hungarian Plain, and Offa and then Coenwulf were clearly minor figures by comparison.
Government.
The nature of Mercian kingship is not clear from the limited surviving sources. There are two main theories regarding the ancestry of Mercian kings of this period. One is that descendants of different lines of the royal family competed for the throne. In the mid-seventh century, for example, Penda had placed royal kinsmen in control of conquered provinces. Alternatively, it may be that a number of kin-groups with local power-bases may have competed for the succession. The sub-kingdoms of the Hwicce, the Tomsæte, and the unidentified Gaini are examples of such power-bases. Marriage alliances could also have played a part. Competing magnates, those called in charters "dux" or "princeps" (that is, leaders), may have brought the kings to power. In this model, the Mercian kings are little more than leading noblemen. Offa seems to have attempted to increase the stability of Mercian kingship, both by the elimination of dynastic rivals to his son Ecgfrith, and the reduction in status of his subject kings, sometimes to the rank of ealdorman. He was ultimately unsuccessful, however; Ecgfrith only survived in power for a few months, and ninth-century Mercia continued to draw its kings from multiple dynastic lines.
There is evidence that Offa constructed a series of defensive "burhs", or fortified towns; the locations are not generally agreed on but may include Bedford, Hereford, Northampton, Oxford and Stamford. In addition to their defensive uses, these "burhs" are thought to have been administrative centres, serving as regional markets and indicating a transformation of the Mercian economy away from its origins as a grouping of midland peoples. The "burhs" are forerunners of the defensive network successfully implemented by Alfred the Great a century later to deal with the Danish invasions. However, Offa did not necessarily understand the economic changes that came with the "burhs", so it is not safe to assume he envisioned all their benefits. In 749, Æthelbald of Mercia had issued a charter that freed ecclesiastical lands from all obligations except the requirement to build forts and bridges – obligations which lay upon everyone, as part of the trinoda necessitas. Offa's Kentish charters show him laying these same burdens on the recipients of his grants there, and this may be a sign that the obligations were being spread outside Mercia. These burdens were part of Offa's response to the threat of "pagan seaman".
Offa issued laws in his name, but no details of them have survived. They are known only from a mention by Alfred the Great, in the preface to Alfred's own law code. Alfred says that he has included in his code those laws of Offa, Ine of Wessex and Æthelberht of Kent which he found "most just". The laws may have been an independent lawcode, but it is also possible that Alfred is referring to the report of the legatine mission in 786, which issued statutes that the Mercians undertook to obey.
Coinage.
At the start of the eighth century, sceattas were the primary circulating coinage. These were small silver pennies, which often did not bear the name of either the moneyer or the king for whom they were produced. To contemporaries these were probably known as pennies, and are the coins referred to in the laws of Ine of Wessex. This light coinage (in contrast to the heavier coins minted later in Offa's reign) can probably be dated to the late 760s and early 770s. A second, medium-weight coinage can be identified before the early 790s. These new medium-weight coins were heavier, broader and thinner than the pennies they replaced, and were prompted by the contemporary Carolingian currency reforms. The new pennies almost invariably carried both Offa's name and the name of the moneyer from whose mint the coins came. The reform in the coinage appears to have extended beyond Offa's own mints: the kings of East Anglia, Kent and Wessex all produced coins of the new heavier weight in this period.
Some coins from Offa's reign bear the names of the archbishops of Canterbury, Jaenberht and, after 792, Æthelheard. Jaenberht's coins all belong to the light coinage, rather than the later medium coinage. There is also evidence that coins were issued by Eadberht, who was bishop of London in the 780s and possibly before. Offa's dispute with Jaenberht may have led him to allow Eadberht coining rights, which may then have been revoked when the see of Lichfield was elevated to an archbishopric.
The medium-weight coins often carry designs of high artistic quality, exceeding that of the contemporary Frankish currency. Coin portraits of Offa have been described as "showing a delicacy of execution which is unique in the whole history of the Anglo-Saxon coinage". The depictions of Offa on the coins include a "striking and elegant" portrait showing him with his hair in voluminous curls, and another where he wears a fringe and tight curls. Some coins show him wearing a necklace with a pendant. The variety of these depictions implies that Offa's die-cutters were able to draw on varied artistic sources for their inspiration.
Offa's wife Cynethryth was the only Anglo-Saxon queen ever named or portrayed on coinage – in a remarkable series of pennies struck by the moneyer Eoba. These were probably derived from contemporary coins from the reign of the Byzantine emperor Constantine VI, who minted a series showing a portrait of his mother, the later Empress Irene, though the Byzantine coins show a frontal bust of Irene rather than a profile, and so cannot have been a direct model.
Around the time of Jaenberht's death and replacement with Æthelheard in 792-3, the silver currency was reformed a second time: in this "heavy coinage" the weight of the pennies was increased again, and a standardised non-portrait design was introduced at all mints. None of Jaenberht's or Cynethryth's coins occur in this coinage, whereas all of Æthelheard's coins are of the new, heavier weight.
There are also surviving gold coins from Offa's reign. One is a copy of an Abbasid dinar struck in 774 by Caliph Al-Mansur, with "Offa Rex" centred on the reverse. It is clear that the moneyer had no understanding of Arabic as the Arabic text contains many errors. The coin may have been produced to trade with Islamic Spain; or it may be part of the annual payment of 365 mancuses that Offa promised to Rome. There are other Western copies of Abbasid dinars of the period, but it is not known whether they are English or Frankish. Two other English gold coins of the period survive, from two moneyers, Pendraed and Ciolheard: the former is thought to be from Offa's reign but the latter may belong either to Offa's reign or to that of Coenwulf, who came to the throne in 796. Nothing definite is known about their use, but they may have been struck to be used as alms.
Although many of the coins bear the name of a moneyer, there is no indication of the mint where each coin was struck. As a result the number and location of mints used by Offa is uncertain. Current opinion is that there were four mints, in Canterbury, Rochester, East Anglia and London.
Stature.
The title Offa used on most of his charters was "rex Merciorium", or "king of the Mercians", though this was occasionally extended to "king of the Mercians and surrounding nations". Some of his charters use the title "Rex Anglorum," or "King of the English," and this has been seen as a sweeping statement of his power. There is debate on this point, however, as several of the charters in which Offa is named "Rex Anglorum" are of doubtful authenticity. They may represent later forgeries of the tenth century, when this title was standard for kings of England. The best evidence for Offa's use of this title comes from coins, not charters: there are some pennies with "Of ℞ A" inscribed, but it is not regarded as definite that this stood for "Offa Rex Anglorum."
In "Anglo-Saxon England", Stenton argued that Offa was perhaps the greatest king of the English kingdoms, commenting that "no other Anglo-Saxon king ever regarded the world at large with so ... acute a political sense". Many historians regard Offa's achievements as second only to Alfred the Great among the Anglo-Saxon kings. Offa's reign has sometimes been regarded as a key stage in the transition to a unified England, but this is no longer the general view among historians in the field. In the words of Simon Keynes, "Offa was driven by a lust for power, not a vision of English unity; and what he left was a reputation, not a legacy." It is now believed that Offa thought of himself as "King of the Mercians," and that his military successes were part of the transformation of Mercia from an overlordship of midland peoples into a powerful and aggressive kingdom.
Death and succession.
Offa died on 29 July 796, and may be buried in Bedford, though it is not clear that the "Bedeford" named in that charter was actually Bedford. He was succeeded by his son, Ecgfrith, but according to the "Anglo-Saxon Chronicle" Ecgfrith died after a reign of only 141 days. A letter written by Alcuin in 797 to a Mercian ealdorman named Osbert makes it apparent that Offa had gone to great lengths to ensure that his son Ecgfrith would succeed him. Alcuin's opinion is that Ecgfrith "has not died for his own sins; but the vengeance for the blood his father shed to secure the kingdom has reached the son. For you know very well how much blood his father shed to secure the kingdom on his son." It is apparent that in addition to Ecgfrith's consecration in 787, Offa had eliminated dynastic rivals. This seems to have backfired, from the dynastic point of view, as no close male relatives of Offa or Ecgfrith are recorded, and Coenwulf, Ecgfrith's successor, was only distantly related to Offa's line.
Bibliography.
</dl>

</doc>
<doc id="47047" url="http://en.wikipedia.org/wiki?curid=47047" title="Long Valley, California">
Long Valley, California

Long Valley, California may refer to:

</doc>
<doc id="47062" url="http://en.wikipedia.org/wiki?curid=47062" title="Queen Elizabeth's Grammar School for Boys">
Queen Elizabeth's Grammar School for Boys

Queen Elizabeth's School, Barnet is a boys' grammar school in Barnet, North London, which was founded in 1573 by Robert Dudley, 1st Earl of Leicester and others, in the name of Queen Elizabeth I.
It is one of the most academically successful secondary schools in England and was chosen as The Sunday Times State School of the Year 2007. The school was the subject of some controversy in the 1990s, but an Ofsted report published in January 2008 stated: "It is held in very high regard by the vast majority of students and their parents, and rightly so." It has a specialism in Music and also from April 2009 is a Training School
The school is also known as Queen Elizabeth's School or simply QE Boys.
History.
Foundation and location.
The school was founded in 1573 by Queen Elizabeth I, petitioned by Robert Dudley, Earl of Leicester, and assisted by local alderman Edward Underne. Elizabeth I's charter of 1573 describes the school's purpose thus: "a grammar school which shall be called The Free Grammar School of Queen Elizabeth for the education, bringing up and instruction of boys and youth, to be brought up in grammar and other learning, and the same to continue for ever, and the said School for one Master and one Usher for ever to continue and remain and that there shall be for ever four-and-twenty discreet, honest governors of the said Free Grammar School." 
The original Tudor building, known as Tudor Hall, was erected in 1577 opposite the Church of St John The Baptist on Wood Street, with money raised by the first governors of the school and by collections in London churches. It was repaired in 1597 and again in 1637. During the 17th century, further extensive repairs were carried out, in spite of a poor financial situation following the Civil War. Financial conditions became progressively more comfortable during the 18th century.
The trustees of Elizabeth Allen’s Charity, which had been established by her will dated 10 February 1725, gave financial assistance to save it from a state "very ruinous and unfit for habitation". It then became a private boarding school. It was closed in 1872 and restored in 1874 with many additions. In 1885 a governor, H E Chetwynd Stapleton, bought a plot of land behind the Jesus Hospital, a building in Wood Street dating back to 1679; today the Stapylton field stands in front of the main School building and is used for rugby and cricket. As the number of pupils outgrew the capacity of Tudor Hall, so the school was transferred in 1932 to a new site in Queen’s Road, which backed on to the Stapylton field. It was administered by the South Herts Division of Hertfordshire County Council, until 1965 when it became part of the borough of Barnet. In the 1960s, there were around 550 boys with 150 in the sixth form. Tudor Hall was completely restored in 1968 by the London Borough of Barnet, and is now part of Barnet College.
Two plaques are located on the walls of the original school building, Tudor Hall. Inscribed on the stone plaque is: "This is to commemorate the original school founded here by Queen Elizabeth and built in 1573. The school was removed in 1932 to new building in Queens Road, Barnet. This plaque was erected by the Visitors of Jesus Hospital Charity, the present owners in 1952." A more recent blue plaque was erected by the London borough of Barnet which dictates: "This Tudor Hall housed the free grammar school of Queen Elizabeth I who granted its charter in 1573."
Grammar school reinstatement.
It returned to its previous selective grammar school status in August 1994, having opted out of the London borough and become a grant-maintained school in 1989. Other schools in London (outer London) did this, and many became partially selective (up to 50%) at this time. In the 1990s it went on to become England's top state school for A-Levels. The girls' school remained a comprehensive.
Since 1999, the Headmaster has been Dr John Marincowitz, who commissioned the new Martin swimming pool, opened in 2006, Shearly Hall, opened in 2009, and a digital library which is currently in construction. In 2011, Neil Enright became Headmaster.
Jeremy Corbyn.
After the school became selective again, many parents flocked to the school, and not just from the borough of Barnet. It was offering what many schools in London could not. One parent was Claudia Bracchitta, the wife of staunchly left-wing Jeremy Corbyn (who himself attended Adams' Grammar School in Shropshire). She wanted their son to go to the school and not a comprehensive, the Holloway School. This led to the left-wing MP and his wife separating. They had been married for 12 years. She said "I could not compromise my son's future for my husband's career".
Culture and sports.
Queen Elizabeth's School is divided into six houses, named after famous old boys, patrons and former teachers. They are Broughton, Harrisons', Leicester, Pearce, Stapylton and Underne. There are many inter-house competitions, from rugby to creative writing. The inter-house debating tournaments, for all years, take place at the end of the year and are probably the most fiercely contested non-physical inter-house competition.
Rugby union, played during the Winter and Spring terms, is compulsory for boys in their first four years at the school, as are cross-country running and most other school sports, which include orienteering, swimming, basketball, tennis, cricket, Eton Fives and athletics. QE is well known for not practising football in lessons or playing football competitively against other schools. A particularly boggy part of the cross-country route, suitably nicknamed the 'Elephant Dip', owing to its depth, links Barnet Rugby Club and the north-west gate of the bottom fields.
There is much competition in the Summer term when frequent competitions between houses are held before the summer examinations begin in June, including the QE Sevens Tournament which takes place in the school for the U14's and U16's it is normally held at the end of the Spring Term.
Sixth form.
Boys usually choose four subjects which will be studied for both AS and A-level, although provision can be made for five to be taken. These subjects can only be chosen after receiving recommendations from that subject teacher. Entry to the Sixth Form is dependent on gaining enough such recommendations. All boys in the Sixth Form are made to wear a suit.
One of the main focuses of sixth form is preparation for entry into higher education. As a result the school focuses on career advice, the UCAS application process, personal statements, finance and other things related to university entry. The school also encourages that students partake in a wide range of extra curricular actives both outside of and during school time.
Founder's Day Fête.
The Founder's Day Fête, and the preceding service of celebration at St John the Baptist's Church, Barnet, is the largest cultural event in the school calendar. Steeped in tradition, it is held every year, regardless of weather, on the third Saturday in June, and celebrates the founding of the school in 1573.
All Year 7 boys must attend the church service. The governors also attend, as do most teachers, in academic dress. The head boys, past and present, are readers at the service, and the school choir sings. The boys then walk back to the school along Wood Street and prepare for the roll call on Staplyton Field. This is again compulsory for all of Year 7, with five boys from each house attending from all other senior years. In the past, this was compulsory for the whole school. The boys troop in from the two wings of the main building and form three lines stretching across the School Field. The boys of each house sit together, although traditionally they had to stand. 
The Fête itself is attended by some three thousand people every year, and is a source of funds for the school.
The School Chronicle is read out each year at Founder's Day during the Role Call, with minor additions as necessary. It was originally prepared in 1930 by Ernest H. Jenkins, the headmaster, Cecil Tripp, Secretary of the Old Elizabethan Association and a Governor of the school for twenty-four years. The most up to date school chronicle is listed on the school website and was read out during the Roll Call at Founder's Day 2013.
School Chronicle
BE IT KNOWN that on the 24th March in the year 1573 Her Most Gracious Majesty Elizabeth, by the grace of God of England, France and Ireland Queen, Defender of the faith, did at the humble request of Robert Dudley, Earl of Leicester, grant on behalf of her faithful subjects of the town of Barnet, in the County of Hertford and Middlesex, a Charter for the erection and establishment of a common Grammar School which should be called the Free Grammar School of Queen Elizabeth, for the education, bringing up and instruction of boys and youth, in Grammar and other learning: and the same to continue for ever.
AND that funds for the building and maintenance of the said School were raised by Edward Underne, Rector of East and Chipping Barnet, and that the control and government of the said School was vested in twenty-four discreet and honest men who were to be a body corporate and to have perpetual succession, and that when any of them should die, the surviving Governors should elect other fit persons in their places, and that they should appoint the Master according to their discretion.
AND be it known that in or about the year 1587 John Lonison, Citizen and Goldsmith of London, did add to the foundation the gift of one hundred pounds for the good of the School, from the interest upon which a large proportion of its expenses were for many years borne.
AND that in and from the year 1594 John Boyle, who was subsequently Bishop of Cork, was Master, and that in or about the year 1599 his cousin Richard Boyle, who subsequently became Archbishop of Tuam, was Master.
AND that by their diligence and good discretion the School flourished, and it was ordered by the Governors that children of townsmen should be admitted upon the payment of twelve pence, and that children of others should be admitted upon the payment of two shillings, and that each scholar should be present in his appointed place in the Church upon the Sabbath day under pain of six lashes, and that the Scholars should attend the Master to the Church in an orderly manner, and not negligently nor in uncomely sort; in memory of which the Scholars of this School do to this day each year upon Founder's Day attend Church in like manner.
AND be it known that in the year 1633 Matthias Millward, Rector of Barnet, was Master, and that under his successor, George Smallwood, additional buildings were added to the School, and that in his Mastership the Governors, after careful consideration, drew up a fresh code of regulations for the government of the School, to the end that youth there placed should receive good education as well in religion and good manners as in nurture for learning.
AND that in the year 1637 the Governors made decisions to admit the first free pupils, these being four children, toties quoties, towardly and docible, of poor parents not able to disburse the set stipend, which practice of admitting free pupils is therefore continued by the School unto the present day.
AND be it known that during the great Rebellion in the reign of His Majesty King Charles I the School was noticeably loyal, and this it continued even to the extent of appointing as Master in 1654 William Sclater who had served as Cornet of Horse in the Army of His Majesty, and who upon the King's execution had suffered imprisonment and trial for his life for his steadfast adherence to the Royal Family.
AND that John Owen, Citizen and Fishmonger of London, Governor of the School, gave to the School the sum of six pounds annually, and be it also known that during the political difficulties of the reigns of James II, William and Mary, and George I, the School declined noticeably from its former prosperity, through thirty of which years from the year 1689 to the year 1719, James Barcock was Master.
AND that in the year 1754 the Reverend Humphrey Hall gave to the School the sum of one hundred pounds for the better support and maintenance of the School and for no other use or purpose whatsoever.
AND that throughout the Eighteenth Century the School failed noticeably to prosper, and that the majority of its pupils were boarders at the School for the greater profit of the Master, wherefore to them he devoted the greater part of his attention.
AND that in the year 1740 John Gray was appointed Master, he holding the position longer than any of his predecessors or successors, being Master for forty-seven years, until the year 1787, and that during this long Mastership the pupils admitted privately for the Master's profit continued to out-number the public pupils, and did so continue until the remodelling of the Foundation in the year 1873.
AND be it known that in the year 1828 William Grant Broughton, Old Elizabethan, was appointed first Archdeacon of New South Wales, subsequently to become first Bishop and Metropolitan of Australia, in commemoration of which distinguished Old Boy a yearly prize for Divinity is to this day awarded.
AND be it known that in the year 1853 the income of the Foundation was found inadequate to carry on the School and maintain the buildings, wherefore a public subscription which amounted to more than four hundred pounds was raised in Barnet, and be it known that following upon the Schools Enquiry Commission of the year 1866, and the passing of the Endowed Schools Act in the year 1869, the Scheme for the reconstruction of the School was prepared, and received the assent of Her Most Gracious Majesty Queen Victoria in the year 1873, whereby the endowment was increased by a portion of the surplus income of the local charity of Jesus Hospital and new land obtained round about the School whereon new buildings and a house for the Headmaster were erected, and that in the year 1875 the School with its new buildings and under its new Scheme of Management the Headmaster was the Reverend John Bond Lee, Master in Arts of Oxford University, he being the Headmaster in honour of whom an annual prize in Classics has been endowed and is awarded to this day.
AND be it known that during his Headmastership, in the year 1885, H.E. Chetwynd Stapylton, Chairman of the Governors, purchased for the School the Stapylton field, whereon the XI and the XV do play to this day, and that upon his retirement in the year 1906 William Lattimer, Master of Arts of Cambridge University, was appointed Headmaster, under whom, as under his predecessor, the School continued to grow in numbers and to flourish, and did so in peace and prosperity until the outbreak of the European War of 1914, and, this in perpetual honour of those who fell in this War, their contemporaries have endowed a prize to be awarded on each year to the boy who, like them most unselfishly serves the School, and that the manner in which its Scholars bore themselves during that War was testified by the memorial tablet placed in the School Hall. And that, when new buildings were erected and were opened by H.R.H. The Prince George in Michaelmas 1932, this tablet was given the place of honour in the Entrance Hall.
AND be it known that in January 1930, Ernest Harold Jenkins, Master of Arts of Oxford University, became Headmaster with the particular charge of arranging to move the School to the new buildings and with the able and enthusiastic support of the Chairman of Governors, Alderman Harold Fern, the School greatly increased, not only in numbers, but in the scope both of its learning and of its interests, so that many more University Scholarships were won than before and many successes were gained in other fields, among them the winning of the Public Schools' Athletic Cup upon several occasions.
AND be it known that in 1938 the Governors and parents provided the School with a Swimming Bath.
AND be it further known that, during the Second Great War, the School continued its work without intermission despite difficulties that were grave, since it was in an area subject to enemy attack and did in fact suffer great damage from several bombs that struck it and fell nearby in January, 1941.
AND be it known that in this War, as in the first Great War, former Scholars of the School served their country with devotion and sacrifice, and that in memory of those who fell, a further tablet was dedicated in 1948, being placed as near to the former tablet as might suitably be, both tablets being placed in the Entrance Hall to the end that each present Scholar may bear it in mind so always honourably to do.
AND be it known that in the year 1961 Timothy Edwards, Master of Arts of Oxford University was Headmaster and guided the School through the difficult period of reorganisation. He it was who caused to be built the Fern Building and under his care the numbers of boys doubled.
AND be it known that during the Headmastership of Eamonn Harris from 1984 the School flourished. Following the Education Reform Act of 1988 the Board of Governors under the leadership of Luxton Robert Heard, Old Elizabethan, successfully petitioned the Secretary of State to empower the Governors to conduct the affairs of the School in accordance with Grant Maintained Status. And that in the year 1994, the Secretary of State granted the Governors' petition for the admission of pupils by virtue of their ability and aptitude. And so by these Orders the School was restored to that independence and status spoken in its ancient title - the Free Grammar School of Elizabeth I. And that in the 425th year of the School, new buildings were erected: the Heard Building to accommodate the enlarged Sixth Form; the Friends' Music Rooms to serve the many musicians; the Clark Laboratories to provide for the growth of the sciences. And all of this was made possible by the labours and donations of parents, by a grant from the Wolfson Foundation and by further grants from Her Majesty's Government.
AND be it known that in the year 1999, John Marincowitz, Doctor of Philosophy of London University, succeeded as the 39th Headmaster. And that the Board of Governors, in accordance with the School Standards and Framework Act of 1999, and under the guidance of the Chairman Barrie Martin, secured the School’s Foundation Status with an Instrument of Government that increased representation of the Friends of Queen Elizabeth’s, the parents, Foundation Trustees and Old Elizabethans. Their unity of purpose enabled considerable development of the School: new buildings included the Martin Swimming Pool and Shearly Hall; and many refinements to teaching and learning facilitated the School’s emergence as a centre of national excellence in the education of young men. And in 2010, the Governors with the steadfast leadership of the Chairman, Barrie Martin, further consolidated the School’s autonomy by converting to Academy status on the express invitation of the Secretary of State in terms of the Academies Act of 2010.
AND be it known that in the year 2011, Neil Enright, Master of Arts of Oxford University, succeeded as the School’s 40th Headmaster.
"AND so the efforts and generosity of many: the Governors, Trustees, parents, teachers, boys and old boys, all have made the School renowned and all have made the School to flourish - may it always flourish."
Kerala partnership.
QE Boys has formed a long-term successful partnership with a school in Kerala called the Shri Sathya Sai School, funded by the '. In addition, the school has strong links with charities through the house system, and each house holds at least one event a year to generate funds for its associated charity.
House System.
There are six houses in QE: Broughton, Harrisons, Leicester, Pearce, Stapylton, Underne. The story behind the foundation and naming of each house is listed on the school's website: 
House competitions are run yearly in:
Academic performance.
In 2007, QE came first in the A-Level league table for state schools, and twelfth in the GCSE league table for state schools. In 2008 QE again topped the league table in A-level results and a record number of 37 pupils gained a place in Oxbridge Universities. In 2009 QE topped the league table in A-level results for the third consecutive year. Students are only allowed to progress to the Sixth Form if subject teachers feel they will be capable of obtaining the highest grades, regardless of performance in other subjects or participation in the extracurricular life of the school. This has led to the accusation that the school has been putting their position in league tables above the interests of pupils. 
In January 2014, it was announced that Barrie Martin, Chairman of the QE Governing Body and the Friends of Queen Elizabeth's was awarded an MBE for 'Services to Education'. Current Headmaster Neil Enright described Martin as: 'an asset to QE in so many ways, combining a tireless work ethic with tremendous focus and an utterly reliable good nature'
Notable former pupils.
During the Second World War the famous athletics coach Franz Stampfl taught physical education at the school until his internment in 1940 as an enemy alien. The future headmaster of Eton John Lewis briefly taught Latin in the early 1970s.
Criticisms.
Admissions procedure.
Parents of boys not admitted to the school have protested and appealed against the school's selective admissions policy. Some have come about because the prospective boy's parents have moved to the area assuming a place will be guaranteed, when this is not the case. In addition there have been calls throughout the United Kingdom for the end of selective grammar schools in favour of selection by distance to school or lottery. The school was also on a list of schools breaching admissions laws in England.
Academic league tables.
Students and parents have been concerned by the school's other actions to keep up its position in academic league tables.In the 1990s the school frequently gave leave-or-be-expelled ultimatums to boys in trouble, which were allegedly aimed at passing lower-performing students on to other schools. This led in the mid-1990s to QE Boys becoming national news, with this practice dubbed ‘Eamonn’s Hit List’, referring to the head teacher, Eamonn Harris. Most notably, The Times Education supplement ran a front page with a sizeable cartoon of Mr Harris. . The school has been criticised for its attitude to Sixth Form admissions by many parents and commentators.

</doc>
<doc id="47063" url="http://en.wikipedia.org/wiki?curid=47063" title="Monarchy of the United Kingdom">
Monarchy of the United Kingdom

The monarchy of the United Kingdom, commonly referred to as the British monarchy, is the constitutional monarchy of the United Kingdom and its overseas territories. The monarch's title is "King" (male) or "Queen" (female). The current monarch, Queen Elizabeth II, ascended the throne on the death of her father, King George VI, on 6 February 1952.
The monarch and his or her immediate family undertake various official, ceremonial, diplomatic and representational duties. As the monarchy is constitutional, the monarch is limited to non-partisan functions such as bestowing honours and appointing the Prime Minister. The monarch is, by tradition, commander-in-chief of the British Armed Forces. Though the ultimate formal executive authority over the government of the United Kingdom is still by and through the monarch's royal prerogative, these powers may only be used according to laws enacted in Parliament and, in practice, within the constraints of convention and precedent.
The British monarchy traces its origins from the petty kingdoms of early medieval Scotland and Anglo-Saxon England, which consolidated into the kingdoms of England and Scotland by the 10th century AD. In 1066, the last crowned Anglo-Saxon monarch, Harold II, was defeated and killed during the Norman conquest of England and the English monarchy passed to the Normans' victorious leader, William the Conqueror, and his descendants.
In the 13th century, Wales, as a principality, became a client state of the English kingdom, while the "Magna Carta" began a process of reducing the English monarch's political powers.
From 1603, when the Scottish monarch King James VI inherited the English throne as James I, both the English and Scottish kingdoms were ruled by a single sovereign. From 1649 to 1660, the tradition of monarchy was broken by the republican Commonwealth of England, which followed the War of the Three Kingdoms. The Act of Settlement 1701, which is still in force, excluded Roman Catholics, or those who marry Catholics, from succession to the English throne. In 1707, the kingdoms of England and Scotland were merged to create the Kingdom of Great Britain, and in 1801, the Kingdom of Ireland joined to create the United Kingdom of Great Britain and Ireland. The British monarch became nominal head of the vast British Empire, which covered a quarter of the world's surface at its greatest extent in 1921.
In the 1920s, five-sixths of Ireland seceded from the Union as the Irish Free State, and the Balfour Declaration recognised the evolution of the dominions of the empire into separate, self-governing countries within a Commonwealth of Nations. After the Second World War, the vast majority of British colonies and territories became independent, effectively bringing the empire to an end. George VI and his successor, Elizabeth II, adopted the title Head of the Commonwealth as a symbol of the free association of its independent member states.
The United Kingdom and fifteen other Commonwealth monarchies that share the same person as their monarch are called Commonwealth realms. The terms "British monarchy" and "British monarch" are frequently still employed in reference to the shared individual and institution; however, each country is sovereign and independent of the others, and the monarch has a different, specific, and official national title and style for each realm.
Constitutional role.
In the uncodified Constitution of the United Kingdom, the Monarch (otherwise referred to as the Sovereign or "His/Her Majesty", abbreviated H.M.) is the Head of State. Oaths of allegiance are made to the Queen and her lawful successors. "God Save the Queen" (or "God Save the King") is the British national anthem, and the monarch appears on postage stamps, coins and banknotes.
The Monarch takes little direct part in Government. The decisions to exercise sovereign powers are delegated from the Monarch, either by statute or by convention, to Ministers or officers of the Crown, or other public bodies, exclusive of the Monarch personally. Thus the acts of state done in the name of the Crown, such as Crown Appointments, even if personally performed by the Monarch, such as the Queen's Speech and the State Opening of Parliament, depend upon decisions made elsewhere:
The Sovereign's role as a constitutional monarch is largely limited to non-partisan functions, such as granting honours. This role has been recognised since the 19th century; the constitutional writer Walter Bagehot identified the monarchy in 1867 as the "dignified part" rather than the "efficient part" of government.
Appointment of the Prime Minister.
Whenever necessary, the Monarch is responsible for appointing a new Prime Minister (who by convention appoints and may dismiss every other Minister of the Crown, and thereby constitutes and controls the government). In accordance with unwritten constitutional conventions, the Sovereign must appoint an individual who commands the support of the House of Commons, usually the leader of the party or coalition that has a majority in that House. The Prime Minister takes office by attending the Monarch in private audience, and after "kissing hands" that appointment is immediately effective without any other formality or instrument.
In a hung parliament where no party or coalition holds a majority, the monarch has an increased degree of latitude in choosing the individual likely to command the most support, though it would usually be the leader of the largest party. Since 1945, there have only been two hung parliaments. The first followed the February 1974 general election when Harold Wilson was appointed Prime Minister after Edward Heath resigned following his failure to form a coalition. Although Wilson's Labour Party did not have a majority, they were the largest party. The second followed the May 2010 general election, in which the Conservatives (the largest party) and Liberal Democrats (the third largest party) agreed to form the first coalition government since World War II.
Dissolution of Parliament.
In 1950 the King's Private Secretary writing pseudonymously to "The Times" newspaper asserted a constitutional convention: according to the Lascelles Principles, if a minority government asked to dissolve Parliament to call an early election to strengthen its position, the monarch could refuse, and would do so under three conditions. When Prime Minister Wilson requested a dissolution late in 1974, the Queen granted his request as Heath had already failed to form a coalition. The resulting general election gave Wilson a small majority. The monarch could in theory unilaterally dismiss a Prime Minister, but a Prime Minister's term now comes to an end only by electoral defeat, death, or resignation. The last monarch to remove a Prime Minister was William IV, who dismissed Lord Melbourne in 1834. The Fixed-term Parliaments Act 2011 removed the monarch's authority to dissolve Parliament.
Royal Prerogative.
Some of the government's executive authority is theoretically and nominally vested in the Sovereign and is known as the royal prerogative. The monarch acts within the constraints of convention and precedent, exercising prerogative only on the advice of ministers responsible to Parliament, often through the Prime Minister or Privy Council. In practice, prerogative powers are exercised only on the Prime Minister's advice—the Prime Minister, and not the Sovereign, has control. The monarch holds a weekly audience with the Prime Minister. No records of these audiences are taken and the proceedings remain fully confidential. The monarch may express his or her views, but, as a constitutional ruler, must ultimately accept the decisions of the Prime Minister and the Cabinet (providing they command the support of the House). In Bagehot's words: "the Sovereign has, under a constitutional monarchy ... three rights—the right to be consulted, the right to encourage, the right to warn."
Although the Royal Prerogative is extensive and parliamentary approval is not formally required for its exercise, it is limited. Many Crown prerogatives have fallen out of use or have been permanently transferred to Parliament. For example, the monarch cannot impose and collect new taxes; such an action requires the authorisation of an Act of Parliament. According to a parliamentary report, "The Crown cannot invent new prerogative powers", and Parliament can override any prerogative power by passing legislation.
The Royal Prerogative includes the powers to appoint and dismiss ministers, regulate the civil service, issue passports, declare war, make peace, direct the actions of the military, and negotiate and ratify treaties, alliances, and international agreements. However, a treaty cannot alter the domestic laws of the United Kingdom; an Act of Parliament is necessary in such cases. The monarch is commander-in-chief of the Armed Forces (the Royal Navy, the British Army, and the Royal Air Force), accredits British High Commissioners and ambassadors, and receives diplomats from foreign states.
It is the prerogative of the monarch to summon and prorogue Parliament. Each parliamentary session begins with the monarch's summons. The new parliamentary session is marked by the State Opening of Parliament, during which the Sovereign reads the Speech from the throne in the Chamber of the House of Lords, outlining the Government's legislative agenda. Prorogation usually occurs about one year after a session begins, and formally concludes the session. Dissolution ends a parliamentary term, and is followed by a general election for all seats in the House of Commons. A general election is normally held five years after the previous one under the Fixed-term Parliaments Act 2011, but can be held sooner if the Prime Minister loses a motion of confidence, or if two-thirds of the members of the House of Commons vote to hold an early election.
Before a bill passed by the legislative Houses can become law, the Royal Assent (the monarch's approval) is required. In theory, assent can either be granted (making the bill law) or withheld (vetoing the bill), but since 1707 assent has always been granted.
The monarch has a similar relationship with the devolved governments of Scotland, Wales, and Northern Ireland. The Sovereign appoints the First Minister of Scotland on the nomination of the Scottish Parliament, and the First Minister of Wales on the nomination of the National Assembly for Wales. In Scottish matters, the Sovereign acts on the advice of the Scottish Government. However, as devolution is more limited in Wales, in Welsh matters the Sovereign acts on the advice of the Prime Minister and Cabinet of the United Kingdom. The Sovereign can veto any law passed by the Northern Ireland Assembly, if it is deemed unconstitutional by the Secretary of State for Northern Ireland.
The Sovereign is deemed the "fount of justice"; although the Sovereign does not personally rule in judicial cases, judicial functions are performed in his or her name. For instance, prosecutions are brought on the monarch's behalf, and courts derive their authority from the Crown. The common law holds that the Sovereign "can do no wrong"; the monarch cannot be prosecuted for criminal offences. The Crown Proceedings Act 1947 allows civil lawsuits against the Crown in its public capacity (that is, lawsuits against the government), but not lawsuits against the monarch personally. The Sovereign exercises the "prerogative of mercy", which is used to pardon convicted offenders or reduce sentences.
The monarch is the "fount of honour", the source of all honours and dignities in the United Kingdom. The Crown creates all peerages, appoints members of the orders of chivalry, grants knighthoods and awards other honours. Although peerages and most other honours are granted on the advice of the Prime Minister, some honours are within the personal gift of the Sovereign, and are not granted on ministerial advice. The monarch alone appoints members of the Order of the Garter, the Order of the Thistle, the Royal Victorian Order and the Order of Merit.
History.
English monarchy.
Following Viking raids and settlement in the ninth century, the Anglo-Saxon kingdom of Wessex emerged as the dominant English kingdom. Alfred the Great secured Wessex, achieved dominance over western Mercia, and assumed the title "King of the English". His grandson Athelstan was the first king to rule over a unitary kingdom roughly corresponding to the present borders of England, though its constituent parts retained strong regional identities. The 11th century saw England become more stable, despite a number of wars with the Danes, which resulted in a Danish monarchy for one generation. William, Duke of Normandy's conquest of England in 1066 was crucial in terms of both political and social change. The new monarch continued the centralisation of power begun in the Anglo-Saxon period, while the Feudal System continued to develop.
William I was succeeded by two of his sons: William II, then Henry I. Henry made a controversial decision to name his daughter Matilda (his only surviving child) as his heir. Following Henry's death in 1135, one of William I's grandsons, Stephen, laid claim to the throne and took power with the support of most of the barons. Matilda challenged his reign; as a result, England descended into a period of disorder known as the Anarchy. Stephen maintained a precarious hold on power, but agreed to a compromise under which Matilda's son Henry would succeed him. Henry accordingly became the first Angevin king of England and the first monarch of the Plantagenet dynasty as Henry II in 1154.
The reigns of most of the Angevin monarchs were marred by civil strife and conflicts between the monarch and the nobility. Henry II faced rebellions from his own sons, the future monarchs Richard I and John. Nevertheless, Henry managed to expand his kingdom, forming what is retrospectively known as the Angevin Empire. Upon Henry's death, his elder son Richard succeeded to the throne; he was absent from England for most of his reign, as he left to fight in the Crusades. He was killed besieging a castle, and John succeeded him.
John's reign was marked by conflict with the barons, particularly over the limits of royal power. In 1215, the barons coerced the king into issuing the Magna Carta (Latin for "Great Charter") to guarantee the rights and liberties of the nobility. Soon afterwards, further disagreements plunged England into a civil war known as the First Barons' War. The war came to an abrupt end after John died in 1216, leaving the Crown to his nine-year-old son Henry III. Later in Henry's reign, Simon de Montfort led the barons in another rebellion, beginning the Second Barons' War. The war ended in a clear royalist victory and in the death of many rebels, but not before the king had agreed to summon a parliament in 1265.
The next monarch, Edward I ("Edward Longshanks"), was far more successful in maintaining royal power and responsible for the conquest of Wales. He attempted to establish English domination of Scotland. However, gains in Scotland were reversed during the reign of his successor, Edward II, who also faced conflict with the nobility. In 1311, Edward II was forced to relinquish many of his powers to a committee of baronial "ordainers"; however, military victories helped him regain control in 1322. Nevertheless, in 1327, Edward was deposed and then murdered by his wife Isabella. His 14-year-old son became Edward III. Edward III claimed the French Crown, setting off the Hundred Years' War between England and France.
His campaigns conquered much French territory, but by 1374, all the gains had been lost. Edward's reign was also marked by the further development of Parliament, which came to be divided into two Houses. In 1377, Edward III died, leaving the Crown to his 10-year-old grandson Richard II. Like many of his predecessors, Richard II conflicted with the nobles by attempting to concentrate power in his own hands. In 1399, while he was campaigning in Ireland, his cousin Henry Bolingbroke seized power. Richard was deposed, imprisoned, and eventually murdered, probably by starvation, and Henry became king as Henry IV.
Henry IV was the grandson of Edward III and the son of John of Gaunt, Duke of Lancaster; hence, his dynasty was known as the House of Lancaster. For most of his reign, Henry IV was forced to fight off plots and rebellions; his success was partly due to the military skill of his son, the future Henry V. Henry V's own reign, which began in 1413, was largely free from domestic strife, leaving the king free to pursue the Hundred Years' War in France. Although he was victorious, his sudden death in 1422 left his infant son Henry VI on the throne and gave the French an opportunity to overthrow English rule.
The unpopularity of Henry VI's counsellors and his belligerent consort, Margaret of Anjou, as well as his own ineffectual leadership, led to the weakening of the House of Lancaster. The Lancastrians faced a challenge from the House of York, so called because its head, a descendant of Edward III, was Richard, Duke of York. Although the Duke of York died in battle in 1460, his eldest son, Edward IV, led the Yorkists to victory in 1461. The Wars of the Roses, nevertheless, continued intermittently during his reign and those of his son Edward V and brother Richard III. Edward V disappeared, presumably murdered by Richard. Ultimately, the conflict culminated in success for the Lancastrian branch led by Henry Tudor, in 1485, when Richard III was killed in the Battle of Bosworth Field.
Now King Henry VII, he neutralised the remaining Yorkist forces, partly by marrying Elizabeth of York, a Yorkist heir. Through skill and ability, Henry re-established absolute supremacy in the realm, and the conflicts with the nobility that had plagued previous monarchs came to an end. The reign of the second Tudor king, Henry VIII, was one of great political change. Religious upheaval and disputes with the Pope led the monarch to break from the Roman Catholic Church and to establish the Church of England (the Anglican Church).
Wales – which had been conquered centuries earlier, but had remained a separate dominion – was annexed to England under the Laws in Wales Acts 1535–1542. Henry VIII's son and successor, the young Edward VI, continued with further religious reforms, but his early death in 1553 precipitated a succession crisis. He was wary of allowing his Catholic elder half-sister Mary to succeed, and therefore drew up a will designating Lady Jane Grey as his heiress. Jane's reign, however, lasted only nine days; with tremendous popular support, Mary deposed her and declared herself the lawful sovereign. Mary I married Philip of Spain, who was declared king and co-ruler, pursued disastrous wars in France, and attempted to return England to Roman Catholicism, burning Protestants at the stake as heretics in the process. Upon her death in 1558, the pair were succeeded by her Protestant half-sister Elizabeth I. England returned to Protestantism and continued its growth into a major world power by building its navy and exploring the New World.
Scottish monarchy.
In Scotland, as in England, monarchies emerged after the withdrawal of the Roman empire from Britain in the early fifth century. The three groups that lived in Scotland at this time were the Picts in the north east, the Britons in the south, including the Kingdom of Strathclyde, and the Gaels or Scotti (who would later give their name to Scotland), of the Irish petty kingdom of Dál Riata in the west. Kenneth MacAlpin is traditionally viewed as the first king of a united Scotland (known as Scotia to writers in Latin, or Alba to the Scots). The expansion of Scottish dominions continued over the next two centuries, as other territories such as Strathclyde were absorbed.
Early Scottish monarchs did not inherit the Crown directly; instead the custom of tanistry was followed, where the monarchy alternated between different branches of the House of Alpin. As a result, however, the rival dynastic lines clashed, often violently. From 942 to 1005, seven consecutive monarchs were either murdered or killed in battle. In 1005, Malcolm II ascended the throne having killed many rivals. He continued to ruthlessly eliminate opposition, and when he died in 1034 he was succeeded by his grandson, Duncan I, instead of a cousin, as had been usual. In 1040, Duncan suffered defeat in battle at the hands of Macbeth, who was killed himself in 1057 by Duncan's son Malcolm. The following year, after killing Macbeth's stepson Lulach, Malcolm ascended the throne as Malcolm III.
With a further series of battles and deposings, five of Malcolm's sons as well as one of his brothers successively became king. Eventually, the Crown came to his youngest son, David. David was succeeded by his grandsons Malcolm IV, and then by William the Lion, the longest-reigning King of Scots before the Union of the Crowns. William participated in a rebellion against King Henry II of England but when the rebellion failed, William was captured by the English. In exchange for his release, William was forced to acknowledge Henry as his feudal overlord. The English King Richard I agreed to terminate the arrangement in 1189, in return for a large sum of money needed for the Crusades. William died in 1214, and was succeeded by his son Alexander II. Alexander II, as well as his successor Alexander III, attempted to take over the Western Isles, which were still under the overlordship of Norway. During the reign of Alexander III, Norway launched an unsuccessful invasion of Scotland; the ensuing Treaty of Perth recognised Scottish control of the Western Isles and other disputed areas.
Alexander III's unexpected death in a riding accident in 1286 precipitated a major succession crisis. Scottish leaders appealed to King Edward I of England for help in determining who was the rightful heir. Edward chose Alexander's three-year-old Norwegian granddaughter, Margaret. On her way to Scotland in 1290, however, Margaret died at sea, and Edward was again asked to adjudicate between 13 rival claimants to the throne. A court was set up and after two years of deliberation, it pronounced John Balliol to be king. However, Edward proceeded to treat Balliol as a vassal, and tried to exert influence over Scotland. In 1295, when Balliol renounced his allegiance to England, Edward I invaded. During the first ten years of the ensuing Wars of Scottish Independence, Scotland had no monarch, until Robert the Bruce declared himself king in 1306.
Robert's efforts to control Scotland culminated in success, and Scottish independence was acknowledged in 1328. However, only one year later, Robert died and was succeeded by his five-year-old son, David II. On the pretext of restoring John Balliol's rightful heir, Edward Balliol, the English again invaded in 1332. During the next four years, Balliol was crowned, deposed, restored, deposed, restored, and deposed until he eventually settled in England, and David remained king for the next 35 years.
David II died childless in 1371 and was succeeded by his nephew Robert II of the House of Stuart. The reigns of both Robert II and his successor, Robert III, were marked by a general decline in royal power. When Robert III died in 1406, regents had to rule the country; the monarch, Robert III's son James I, had been taken captive by the English. Having paid a large ransom, James returned to Scotland in 1424; to restore his authority, he used ruthless measures, including the execution of several of his enemies. He was assassinated by a group of nobles. James II continued his father's policies by subduing influential noblemen but he was killed in an accident at the age of thirty, and a council of regents again assumed power. James III was defeated in a battle against rebellious Scottish earls in 1488, leading to another boy-king: James IV.
In 1513 James IV launched an invasion of England, attempting to take advantage of the absence of the English King Henry VIII. His forces met with disaster at Flodden Field; the King, many senior noblemen, and hundreds of soldiers were killed. As his son and successor, James V, was an infant, the government was again taken over by regents. James V led another disastrous war with the English in 1542, and his death in the same year left the Crown in the hands of his six-day-old daughter, Mary. Once again, a regency was established.
Mary, a Roman Catholic, reigned during a period of great religious upheaval in Scotland. As a result of the efforts of reformers such as John Knox, a Protestant ascendancy was established. Mary caused alarm by marrying her Catholic cousin, Lord Darnley, in 1565. After Lord Darnley's assassination in 1567, Mary contracted an even more unpopular marriage with the Earl of Bothwell, who was widely suspected of Darnley's murder. The nobility rebelled against the Queen, forcing her to abdicate. She fled to England, and the Crown went to her infant son James VI, who was brought up as a Protestant. Mary was imprisoned and later executed by the English Queen Elizabeth I.
Personal union and republican phase.
Elizabeth's death in 1603 ended Tudor rule in England. Since she had no children, she was succeeded by the Scottish monarch James VI, who was the great-grandson of Henry VIII's older sister and hence Elizabeth's first cousin twice removed. James VI ruled in England as James I after what was known as the "Union of the Crowns". Although England and Scotland were in personal union under one monarch—James I became the first monarch to style himself "King of Great Britain and Ireland" in 1604—they remained separate kingdoms. James I's successor, Charles I, experienced frequent conflicts with the English Parliament related to the issue of royal and parliamentary powers, especially the power to impose taxes. He provoked opposition by ruling without Parliament from 1629 to 1640, unilaterally levying taxes and adopting controversial religious policies (many of which were offensive to the Scottish Presbyterians and the English Puritans). His attempt to enforce Anglicanism led to organised rebellion in Scotland (the "Bishops' Wars") and ignited the Wars of the Three Kingdoms. In 1642, the conflict between the King and English Parliament reached its climax and the English Civil War began.
The Civil War culminated in the execution of the king in 1649, the overthrow of the English monarchy, and the establishment of the Commonwealth of England. Charles I's son, Charles II, was proclaimed King of Great Britain in Scotland, but he was forced to flee abroad after he invaded England and was defeated at the Battle of Worcester. In 1653, Oliver Cromwell, the most prominent military and political leader in the nation, seized power and declared himself Lord Protector (effectively becoming a military dictator, but refusing the title of king). Cromwell ruled until his death in 1658, when he was succeeded by his son Richard. The new Lord Protector had little interest in governing; he soon resigned. The lack of clear leadership led to civil and military unrest, and for a popular desire to restore the monarchy. In 1660, the monarchy was restored and Charles II returned to Britain.
Charles II's reign was marked by the development of the first modern political parties in England. Charles had no legitimate children, and was due to be succeeded by his Roman Catholic brother, James, Duke of York. A parliamentary effort to exclude James from the line of succession arose; the "Petitioners", who supported exclusion, became the Whig Party, whereas the "Abhorrers", who opposed exclusion, became the Tory Party. The Exclusion Bill failed; on several occasions, Charles II dissolved Parliament because he feared that the bill might pass. After the dissolution of the Parliament of 1681, Charles ruled without a Parliament until his death in 1685. When James succeeded Charles, he pursued a policy of offering religious tolerance to Roman Catholics, thereby drawing the ire of many of his Protestant subjects. Many opposed James's decisions to maintain a large standing army, to appoint Roman Catholics to high political and military offices, and to imprison Church of England clerics who challenged his policies. As a result, a group of Protestants known as the Immortal Seven invited James II's daughter Mary and her husband William of Orange to depose the king. William obliged, arriving in England on 5 November 1688 to great public support. Faced with the defection of many of his Protestant officials, James fled the realm and William and Mary (rather than James II's Catholic son) were declared joint Sovereigns of England, Scotland and Ireland.
James's overthrow, known as the Glorious Revolution, was one of the most important events in the long evolution of parliamentary power. The Bill of Rights 1689 affirmed parliamentary supremacy, and declared that the English people held certain rights, including the freedom from taxes imposed without parliamentary consent. The Bill of Rights required future monarchs to be Protestants, and provided that, after any children of William and Mary, Mary's sister Anne would inherit the Crown. Mary died childless in 1694, leaving William as the sole monarch. By 1700, a political crisis arose, as all of Anne's children had died, leaving her as the only individual left in the line of succession. Parliament was afraid that the former James II or his supporters, known as Jacobites, might attempt to reclaim the throne. Parliament passed the Act of Settlement 1701, which excluded James and his Catholic relations from the succession and made William's nearest Protestant relations, the family of Sophia, Electress of Hanover, next in line to the throne after his sister-in-law Anne. Soon after the passage of the Act, William III died, leaving the Crown to Anne.
After the 1707 Acts of Union.
After Anne's accession, the problem of the succession re-emerged. The Scottish Parliament, infuriated that the English Parliament did not consult them on the choice of Sophia's family as the next heirs, passed the Act of Security, threatening to end the personal union between England and Scotland. The Parliament of England retaliated with the Alien Act 1705, threatening to devastate the Scottish economy by restricting trade. The Scottish and English parliaments negotiated the Act of Union 1707, under which England and Scotland were united into a single Kingdom of Great Britain, with succession under the rules prescribed by the Act of Settlement.
In 1714, Queen Anne was succeeded by her second cousin, and Sophia's son, George I, Elector of Hanover, who consolidated his position by defeating Jacobite rebellions in 1715 and 1719. The new monarch was less active in government than many of his British predecessors, but retained control over his German kingdoms, with which Britain was now in personal union. Power shifted towards George's ministers, especially to Sir Robert Walpole, who is often considered the first British prime minister, although the title was not then in use. The next monarch, George II, witnessed the final end of the Jacobite threat in 1746, when the Catholic Stuarts were completely defeated. During the long reign of his grandson, George III, Britain's American colonies were lost, the former colonies having formed the United States of America, but British influence elsewhere in the world continued to grow, and the United Kingdom of Great Britain and Ireland was created by the Act of Union 1800.
From 1811 to 1820, George III suffered a severe bout of what is now believed to be porphyria, an illness rendering him incapable of ruling. His son, the future George IV, ruled in his stead as Prince Regent. During the Regency and his own reign, the power of the monarchy declined, and by the time of his successor, William IV, the monarch was no longer able to effectively interfere with parliamentary power. In 1834, William dismissed the Whig Prime Minister, William Lamb, 2nd Viscount Melbourne, and appointed a Tory, Sir Robert Peel. In the ensuing elections, however, Peel lost. The king had no choice but to recall Lord Melbourne. During William IV's reign, the Reform Act 1832, which reformed parliamentary representation, was passed. Together with others passed later in the century, the Act led to an expansion of the electoral franchise and the rise of the House of Commons as the most important branch of Parliament.
The final transition to a constitutional monarchy was made during the long reign of William IV's successor, Victoria. As a woman, Victoria could not rule Hanover, which only permitted succession in the male line, so the personal union of the United Kingdom and Hanover came to an end. The Victorian era was marked by great cultural change, technological progress, and the establishment of the United Kingdom as one of the world's foremost powers. In recognition of British rule over India, Victoria was declared Empress of India in 1876. However, her reign was also marked by increased support for the republican movement, due in part to Victoria's permanent mourning and lengthy period of seclusion following the death of her husband in 1861.
Victoria's son, Edward VII, became the first monarch of the House of Saxe-Coburg and Gotha in 1901. In 1917, the next monarch, George V, changed "Saxe-Coburg and Gotha" to "Windsor" in response to the anti-German sympathies aroused by the First World War. George V's reign was marked by the separation of Ireland into Northern Ireland, which remained a part of the United Kingdom, and the Irish Free State, an independent nation, in 1922.
Shared monarchy.
During the twentieth century, the Commonwealth of Nations evolved from the British Empire. Prior to 1926, the British Crown reigned over the British Empire collectively; the Dominions and Crown colonies were subordinate to the United Kingdom. The Balfour Declaration of 1926 gave complete self-government to the Dominions, effectively creating a system whereby a single monarch operated independently in each separate Dominion. The concept was solidified by the Statute of Westminster 1931, which has been likened to "a treaty among the Commonwealth countries".
The monarchy thus ceased to be an exclusively British institution, although it is often still referred to as "British" for legal and historical reasons and for convenience. The monarch became separately monarch of the United Kingdom, monarch of Canada, monarch of Australia, and so forth. The independent states within the Commonwealth would share the same monarch in a relationship likened to a personal union.
George V's death in 1936 was followed by the accession of Edward VIII, who caused a public scandal by announcing his desire to marry the divorced American Wallis Simpson, even though the Church of England opposed the remarriage of divorcées. Accordingly, Edward announced his intention to abdicate; the Parliaments of the United Kingdom and of other Commonwealth countries granted his request. Edward VIII and any children by his new wife were excluded from the line of succession, and the Crown went to his brother, George VI. George served as a rallying figure for the British people during World War II, making morale-boosting visits to the troops as well as to munitions factories and to areas bombed by Nazi Germany. In June 1948 George VI relinquished the title "Emperor of India", and became King of India, instead.
At first, every member of the Commonwealth retained the same monarch as the United Kingdom, but when India became a republic in 1950, it would no longer share in a common monarchy. Instead, the British monarch was acknowledged as "Head of the Commonwealth" in all Commonwealth member states, whether monarchies or not. The position is purely ceremonial, and is not inherited by the British monarch as of right but is vested in an individual chosen by the Commonwealth Heads of Government. Members of the Commonwealth that share the same person as monarch are known as Commonwealth realms.
Monarchy in Ireland.
In the 12th century the only English pope, Adrian IV, authorised King Henry II of England to take possession of Ireland as a feudal territory nominally under papal overlordship. Celtic Christianity was not closely following Roman Catholic practices, and was accused of heretical beliefs. The pope wanted the English monarch to annex Ireland and bring the Irish church into line with Rome. Around 1170, King Dermot MacMurrough of Leinster was deposed by his arch-enemy King Rory O'Connor of Connacht. Dermot escaped to England and asked Henry for help. Henry let him use a group of Anglo-Norman aristocrats and adventurers, led by Richard de Clare, 2nd Earl of Pembroke, to help him regain his throne. Dermot and his Anglo-Norman allies succeeded and he became King of Leinster again. De Clare married Dermot's daughter, and when Dermot died in 1171, de Clare became King of Leinster. Henry was afraid that de Clare would make Ireland a rival Norman state or a place of refuge for Anglo-Saxons, so he took advantage of the papal bull and invaded, forcing de Clare and the other Anglo-Norman aristocrats in Ireland and some Gaelic Irish chieftains to recognise him as their overlord.
By 1541, King Henry VIII of England had broken with the Church of Rome and declared himself Supreme Head of the Church of England. The pope's grant of Ireland to the English monarch became invalid, so Henry summoned a meeting of the Irish Parliament to change his title from Lord of Ireland to King of Ireland.
In 1800, the Act of Union merged the kingdom of Great Britain and the kingdom of Ireland into the United Kingdom of Great Britain and Ireland. The whole island of Ireland continued to be a part of the United Kingdom until 1922, when 26 counties seceded from the United Kingdom to form the Irish Free State, a separate Dominion within the Commonwealth. Six counties in Northern Ireland remained within the Union. The Irish Free State was renamed Ireland in 1937, and in 1949 declared itself a republic. It ceased to be a member of the Commonwealth and severed all ties with the monarchy. In 1927, the United Kingdom changed its name to the United Kingdom of Great Britain and Northern Ireland, while the monarch's style for the next twenty years became "of Great Britain, Ireland and the British Dominions beyond the Seas, King, Defender of the Faith, Emperor of India".
Modern status.
In the 1990s, Republicanism in the United Kingdom grew, partly on account of negative publicity associated with the Royal Family (for instance, immediately following the death of Diana, Princess of Wales). However, recent polls show that around 70–80% of the British public support the continuation of the monarchy.
Religious role.
The sovereign is the Supreme Governor of the established Church of England. Archbishops and bishops are appointed by the monarch, on the advice of the Prime Minister, who chooses the appointee from a list of nominees prepared by a Church Commission. The Crown's role in the Church of England is titular; the most senior clergyman, the Archbishop of Canterbury, is the spiritual leader of the Church and of the worldwide Anglican Communion. The monarch takes an oath to preserve Church of Scotland and he or she holds the power to appoint the Lord High Commissioner to the Church's General Assembly, but otherwise plays no part in its governance, and enjoys no powers over it. The Sovereign plays no formal role in the disestablished Church in Wales or Church of Ireland.
Succession.
The relationship between the Commonwealth realms is such that any change to the laws governing succession to the shared throne requires the unanimous consent of all the realms. Succession is governed by statutes such as the Bill of Rights 1689, the Act of Settlement 1701 and the Acts of Union 1707. The rules of succession may only be changed by an Act of Parliament; it is not possible for an individual to renounce his or her right of succession. The Act of Settlement restricts the succession to the legitimate Protestant descendants of Sophia of Hanover (1630–1714), a granddaughter of James I.
Upon the death of the Sovereign, his or her heir immediately and automatically succeeds (hence the phrase "The King is dead. Long live the King!"), and the accession of the sovereign is publicly proclaimed by an Accession Council that meets at St. James's Palace. The monarch is crowned in Westminster Abbey, normally by the Archbishop of Canterbury. A coronation is not necessary for a sovereign to reign; indeed, the ceremony usually takes place many months after accession to allow sufficient time for its preparation and for a period of mourning.
After an individual ascends the throne, he or she reigns until death. The only voluntary abdication, that of Edward VIII, had to be authorised by a special Act of Parliament, His Majesty's Declaration of Abdication Act 1936. The last monarch involuntarily removed from power was James VII and II, who fled into exile in 1688 during the Glorious Revolution.
Restrictions by gender and religion.
Succession was largely governed by male-preference cognatic primogeniture, under which sons inherit before daughters, and elder children inherit before younger ones of the same gender. The Prime Minister of the United Kingdom, David Cameron, announced at the Commonwealth Heads of Government Meeting 2011 that all 16 Commonwealth realms, including the United Kingdom, had agreed to abolish the gender-preference rule for anyone born after the date of the meeting, 28 October 2011. They also agreed that future monarchs would no longer be prohibited from marrying a Roman Catholic – a law which dated from the Act of Settlement 1701. However, since the monarch is also the Supreme Governor of the Church of England, the law which prohibits a Roman Catholic from acceding to the throne remains. The necessary UK legislation making the changes received the Royal Assent on 25 April 2013 and was brought into force in March 2015 after the equivalent legislation was approved in all the other Commonwealth realms.
Only individuals who are Protestants may inherit the Crown. Roman Catholics are prohibited from succeeding. An individual thus disabled from inheriting the Crown is deemed "naturally dead" for succession purposes, and the disqualification does not extend to the individual's legitimate descendants.
Regency.
The Regency Acts allow for regencies in the event of a monarch who is a minor or who is physically or mentally incapacitated. When a regency is necessary, the next qualified individual in the line of succession automatically becomes regent, unless they themselves are a minor or incapacitated. Special provisions were made for Queen Elizabeth II by the Regency Act 1953, which stated that the Duke of Edinburgh (the Queen's husband) could act as regent in these circumstances.
During a temporary physical infirmity or an absence from the kingdom, the sovereign may temporarily delegate some of his or her functions to Counsellors of State, the monarch's spouse and the first four adults in the line of succession. The present Counsellors of State are: The Duke of Edinburgh, The Prince of Wales, The Duke of Cambridge, Prince Harry and The Duke of York.
Finances.
Until 1760 the monarch met all official expenses from hereditary revenues, which included the profits of the Crown Estate (the royal property portfolio). King George III agreed to surrender the hereditary revenues of the Crown in return for the Civil List, and this arrangement persisted until 2012. An annual Property Services Grant-in-aid paid for the upkeep of the royal residences, and an annual Royal Travel Grant-in-Aid paid for travel. The Civil List covered most expenses, including those for staffing, state visits, public engagements, and official entertainment. Its size was fixed by Parliament every 10 years; any money saved was carried forward to the next 10-year period. From 2012 until 2020, the Civil List and Grants-in-Aid are to be replaced with a single Sovereign Grant, which will be set at 15% of the revenues generated by the Crown Estate.
The Crown Estate is one of the largest property owners in the United Kingdom, with holdings of £7.3 billion in 2011. It is held in trust, and cannot be sold or owned by the Sovereign in a private capacity. In modern times, the profits surrendered from the Crown Estate to the Treasury have exceeded the Civil List and Grants-in-Aid. For example, the Crown Estate produced £200 million in the financial year 2007–8, whereas reported parliamentary funding for the monarch was £40 million during the same period. Republicans estimate that the real cost of the monarchy including security is between £134 and 184 million a year.
Like the Crown Estate, the land and assets of the Duchy of Lancaster, a property portfolio valued at £383 million in 2011, are held in trust. The revenues of the Duchy form part of the Privy Purse, and are used for expenses not borne by the parliamentary grants. The Duchy of Cornwall is a similar estate held in trust to meet the expenses of the monarch's eldest son. The Royal Collection, which includes artworks and the Crown Jewels, is not owned by the Sovereign personally and is held in trust, as are the occupied palaces in the United Kingdom such as Buckingham Palace and Windsor Castle.
The sovereign is subject to indirect taxes such as value added tax, and since 1993 the Queen has paid income tax and capital gains tax on personal income. Parliamentary grants to the Sovereign are not treated as income as they are solely for official expenditure.
Estimates of the Queen's wealth vary, depending on whether assets owned by her personally or held in trust for the nation are included. "Forbes" magazine estimated her wealth at US$450 million in 2010, but no official figure is available. In 1993, the Lord Chamberlain said estimates of £100 million were "grossly overstated". Jock Colville, who was her former private secretary and a director of her bank, Coutts, estimated her wealth in 1971 at £2 million (the equivalent of about £ today).
Residences.
The Sovereign's official residence in London is Buckingham Palace. It is the site of most state banquets, investitures, royal christenings and other ceremonies. Another official residence is Windsor Castle, the largest occupied castle in the world, which is used principally at weekends, Easter and during Royal Ascot, an annual race meeting that is part of the social calendar. The Sovereign's official residence in Scotland is the Palace of Holyroodhouse in Edinburgh. The monarch stays at Holyrood for at least one week each year, and when visiting Scotland on state occasions.
Historically, the Palace of Westminster and the Tower of London were the main residences of the English Sovereign until Henry VIII acquired the Palace of Whitehall. Whitehall was destroyed by fire in 1698, leading to a shift to St James's Palace. Although replaced as the monarch's primary London residence by Buckingham Palace in 1837, St James's is still the senior palace and remains the ceremonial Royal residence. For example, foreign ambassadors are accredited to the Court of St. James's, and the Palace is the site of the meeting of the Accession Council. It is also used by other members of the Royal Family.
Other residences include Clarence House and Kensington Palace. The palaces belong to the Crown; they are held in trust for future rulers, and cannot be sold by the monarch. Sandringham House in Norfolk and Balmoral Castle in Aberdeenshire are privately owned by the Queen.
Style.
The present Sovereign's full style and title is "Elizabeth the Second, by the Grace of God, of the United Kingdom of Great Britain and Northern Ireland and of Her other Realms and Territories Queen, Head of the Commonwealth, Defender of the Faith". The title "Head of the Commonwealth" is held by the Queen personally, and is not vested in the British Crown. Pope Leo X first granted the title "Defender of the Faith" to King Henry VIII in 1521, rewarding him for his support of the Papacy during the early years of the Protestant Reformation, particularly for his book the Defence of the Seven Sacraments. After Henry broke from the Roman Church, Pope Paul III revoked the grant, but Parliament passed a law authorising its continued use.
The Sovereign is known as "His Majesty" or "Her Majesty". The form "Britannic Majesty" appears in international treaties and on passports to differentiate the British monarch from foreign rulers. The monarch chooses his or her regnal name, not necessarily his or her first name—King George VI, King Edward VII and Queen Victoria did not use their first names.
If only one monarch has used a particular name, no ordinal is used; for example, Queen Victoria is not known as "Victoria I", and ordinals are not used for English monarchs who reigned before the Norman conquest of England. The question of whether numbering for British monarchs is based on previous English or Scottish monarchs was raised in 1953 when Scottish nationalists challenged the Queen's use of "Elizabeth II", on the grounds that there had never been an "Elizabeth I" in Scotland. In "MacCormick v. Lord Advocate", the Scottish Court of Session ruled against the plaintiffs, finding that the Queen's title was a matter of her own choice and prerogative. The Home Secretary told the House of Commons that monarchs since the Acts of Union had consistently used the higher of the English and Scottish ordinals, which in the applicable four cases has been the English ordinal. The Prime Minister confirmed this practice, but noted that "neither The Queen nor her advisers could seek to bind their successors". Future monarchs will apply this policy.
Traditionally, the signature of the monarch includes their regnal name but not ordinal, followed by the letter R, which stands for "rex" or "regina" (Latin for "king" and "queen", respectively). The present monarch's signature is "Elizabeth R". From 1877 until 1948 reigning monarchs added the letter I to their signatures, for "imperator" or "imperatrix" ("emperor" or "empress" in Latin), from their status as Emperor or Empress of India. For example, Queen Victoria signed as "Victoria RI" from 1877.
Arms.
The Royal coat of arms of the United Kingdom are "Quarterly, I and IV Gules three lions passant guardant in pale Or [for England]; II Or a lion rampant within a double tressure flory-counter-flory Gules [for Scotland]; III Azure a harp Or stringed Argent [for Ireland]". The supporters are the lion and the unicorn; the motto is "Dieu et mon droit" (French: "God and my Right"). Surrounding the shield is a representation of a Garter bearing the motto of the Chivalric order of the same name; "Honi soit qui mal y pense". (Old French: "Shame be to him who thinks evil of it"). In Scotland, the monarch uses an alternative form of the arms in which quarters I and IV represent Scotland, II England, and III Ireland. The mottoes are "In Defens" (an abbreviated form of the Scots "In My Defens God Me Defend") and the motto of the Order of the Thistle; "Nemo me impune lacessit". (Latin: "No-one provokes me with impunity"); the supporters are the unicorn and lion, who support both the escutcheon and lances, from which fly the flags of Scotland and England.
The monarch's official flag in the United Kingdom is the Royal Standard, which depicts the Royal Arms. It is flown only from buildings, vessels and vehicles in which the Sovereign is present. The Royal Standard is never flown at half-mast because there is always a sovereign: when one dies, his or her successor becomes the sovereign instantly.
When the monarch is not in residence, the Union Flag is flown at Buckingham Palace, Windsor Castle and Sandringham House, whereas in Scotland the Royal Standard of Scotland is flown at Holyrood Palace and Balmoral Castle.

</doc>
