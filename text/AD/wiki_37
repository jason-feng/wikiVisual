<doc id="49121" url="http://en.wikipedia.org/wiki?curid=49121" title="Phoenix, Arizona">
Phoenix, Arizona

Phoenix () is the capital, and largest city, of the state of Arizona. With 1,445,632 people (as of the 2010 U.S. Census), Phoenix is the most populous state capital in the United States, as well as the sixth most populous city nationwide.
The anchor of the Phoenix metropolitan area (also known as the "Valley of the Sun", a part of the Salt River Valley), it is the 13th largest metro area by population in the United States, with approximately 4.3 million people in 2010. In addition, Phoenix is the county seat of Maricopa County and is one of the largest cities in the United States by land area.
Settled in 1867 as an agricultural community near the confluence of the Salt and Gila Rivers, Phoenix incorporated as a city in 1881. Located in the northeastern reaches of the Sonoran Desert, Phoenix has a subtropical desert climate. Despite this, its canal system led to a thriving farming community, many of the original crops remaining important parts of the Phoenix economy for decades, such as alfalfa, cotton, citrus and hay (which was important for the cattle industry). In fact, the "Five C's" (Cotton, Cattle, Citrus, Climate, and Copper), remained the driving forces of Phoenix's economy until after World War II, when high tech industries began to move into the valley.
The population growth rate of the Phoenix metro area has been nearly 4% per year for the past 40 years. While that growth rate slowed during the Great Recession, it has already begun to rebound. Phoenix is the cultural center of the Valley of the Sun, as well as the rest of Arizona.
History.
Early history.
For more than 2,000 years, the Hohokam peoples occupied the land that would become Phoenix. The Hohokam created roughly 135 miles (217 km) of irrigation canals, making the desert land arable. Paths of these canals would later become used for the modern Arizona Canal, Central Arizona Project Canal, and the Hayden-Rhodes Aqueduct. The Hohokam also carried out extensive trade with the nearby Anasazi, Mogollon and Sinagua, as well as with the more distant Mesoamerican civilizations. It is believed that between 1300 and 1450, periods of drought and severe floods led to the Hohokam civilization's abandonment of the area. Local Akimel O'odham settlements, thought to be the descendants of the formerly urbanized Hohokam, concentrated on the Gila River.
When the Mexican-American War ended in 1848, Mexico sold its northern zone to the United States and residents became U.S. citizens. The Phoenix area became part of the New Mexico Territory. In 1863 the mining town of Wickenburg was the first to be established in what is now Maricopa County, to the north-west of modern Phoenix. At the time Maricopa County had not yet been incorporated: the land was within Yavapai County, which included the major town of Prescott to the north of Wickenburg.
The U.S. Army created Fort McDowell on the Verde River in 1865 to forestall Native American uprisings. The fort established a camp on the south side of the Salt River by 1866, which was the first non-native settlement in the valley after the decline of the Hohokam. In later years, other nearby settlements would form and merge to become the city of Tempe, but this community was incorporated after Phoenix.
Founding and incorporation.
The history of the city of Phoenix begins with Jack Swilling, a Confederate veteran of the Civil War. In 1867 he saw in the Salt River Valley a potential for farming, much like that already cultivated by the military further east, near Fort McDowell. He formed a small community that same year about 4 miles (6 km) east of the present city. Lord Darrell Duppa suggested the name "Phoenix", as it described a city born from the ruins of a former civilization.
The Board of Supervisors in Yavapai County, which at the time encompassed Phoenix, officially recognized the new town on May 4, 1868, and the first post office was established the following month, with Swilling as the postmaster. On February 12, 1871, the territorial legislature created Maricopa County, the sixth one formed in the Arizona Territory, by dividing Yavapai County. The first election for county office was held in 1871, when Tom Barnum was elected the first sheriff, actually running unopposed when the other two candidates, John A. Chenowth and Jim Favorite, fought a duel wherein Chenowth killed Favorite, and then was forced to withdraw from the race.
The town grew during the 1870s, and President Ulysses S. Grant issued a land patent for the present site of Phoenix on April 10, 1874. By 1875, the town had a telegraph office, sixteen saloons, and four dance halls, but the townsite-commissioner form of government needed an overhaul, so that year an election was held in which three village trustees as well as several other officials were selected. By 1880, the town's population stood at 2,453.
By 1881, Phoenix' continued growth made the existing village structure with a board of trustees obsolete. The Territorial Legislature passed "The Phoenix Charter Bill", incorporating Phoenix and providing for a mayor-council government, and became official on February 25, 1881 when it was signed by Governor John C. Fremont, officially incorporating Phoenix as a city with an approximate population of 2,500.
The coming of the railroad in the 1880s was the first of several important events that revolutionized the economy of Phoenix. Phoenix became a trade center, with its products reaching eastern and western markets. In response, the Phoenix Chamber of Commerce was organized on November 4, 1888. Earlier in 1888 the city offices were moved into the new City Hall, at Washington and Central. When the territorial capital was moved from Prescott to Phoenix in 1889 the temporary territorial offices were also located in City Hall. With the arrival of the Santa Fe, Prescott and Phoenix Railroad in 1895, Phoenix was connected to the Prescott, Flagstaff and other northern state communities. The increased access to commerce, expedited the city's economic rise. The year 1895 also saw the establishment of Phoenix Union High School, with an enrollment of 90.
1900 to World War II.
On February 25, 1901, Governor Murphy dedicated the permanent state Capitol building, and the Carnegie Free Library opened seven years later, on February 18, 1908, dedicated by Benjamin Fowler. The National Reclamation Act was signed by President Theodore Roosevelt in 1902, which allowed for dams to be built on waterways in the west for reclamation purposes. The first dam constructed under the act, the Theodore Roosevelt Dam began in 1903. It supplied both water and electricity, becoming the first multi-purpose dam, and Roosevelt himself would attend the official dedication on May 18, 1911. At the time, it was the largest masonry dam in the world, forming Theodore Roosevelt Lake in the mountain east of Phoenix.
On February 14, 1912, under President William Howard Taft, Phoenix became the capital of the newly formed state of Arizona. This occurred just six months after Taft had vetoed in August 1911, a joint congressional resolution granting statehood to Arizona, due to his disagreement of the state constitution's position regarding the recall of judges. In 1913 Phoenix adopted a new form of government, changing from a mayor-council system to council-manager, making it one of the first cities in the United States with this form of city government. After statehood, Phoenix's growth started to accelerate, and by the end of its first eight years under statehood, Phoenix' population had grown to 29,053. In 1920 Phoenix would see its first skyscraper, the Heard Building. In 1929 Sky Harbor was officially opened, at the time owned by Scenic Airways. It would later be purchased by the city in 1935, who operates it to this day.
On March 4, 1930, former U.S. President Calvin Coolidge dedicated a dam on the Gila River named in his honor. However, the state had just been through a long drought, and the reservoir which was supposed to be behind the dam, was virtually dry. The humorist Will Rogers, who was also on hand as a guest speaker joked, "If that was my lake I'd mow it." Phoenix's population had more than doubled during the 1920s, and now stood at 48,118.
During World War II, Phoenix's economy shifted to that of a distribution center, rapidly turning into an embryonic industrial city with mass production of military supplies. There were 3 air force fields in the area: Luke Field, Williams Field, and Falcon Field, as well as two large pilot training camps, Thunderbird Field No. 1 in Glendale and Thunderbird Field No. 2 in Scottsdale.
Postwar explosive growth.
A town that had just over sixty-five thousand residents in 1940 became America's sixth largest city by 2010, with a population of nearly 1.5 million, and millions more in nearby suburbs. Shermer argues that after the war Phoenix boosters led by Barry Goldwater and other ambitious young businessmen and politicians, often with an Eastern education, created a neoliberal pro-business climate. They attracted Eastern industry by rejecting the New Deal formula of strong labor unions and tight regulation of industry. They told prospects that Phoenix had excellent weather, cheap land, good transportation, low-wage rates, a right-to-work law that weakened unions, minimal regulations, easy access to the West Coast markets, and an eagerness to grow. They pointed out it was highly attractive place for young couples to raise their families. Hundreds of manufacturing firms were attracted to Phoenix, especially those that emphasized high technology, along with, corporate headquarters. Shermer argues that the Phoenix plan was widely admired by other ambitious cities in the South and Southwest, and became part of national conservatism as exemplified by Goldwater and his supporters. The Phoenix plan was not built on libertarian low-government ideals. Rather, Shermer argues, it involved active government intervention in the economy to promote rapid growth. For example the state played the central role in giving Phoenix a guaranteed water supply, as well as good universities.
When the war ended, many of the men who had undergone their training in Arizona returned bringing their new families. Large industry, learning of this labor pool, started to move branches here. In 1948 high-tech industry, which would become a staple of the state's economy, arrived in Phoenix when Motorola chose Phoenix for the site of its new research and development center for military electronics. Seeing the same advantages as Motorola, other high-tech companies such as Intel and McDonnell Douglas would also move into the valley and open manufacturing operations.
By 1950, over 105,000 people lived within the city and thousands more in surrounding communities. The 1950s growth was spurred on by advances in air conditioning, which allowed both homes and businesses to offset the extreme heat known to Phoenix during its long summers. There was more new construction in Phoenix in 1959 alone than during the period of more than thirty years from 1914 to 1946.
The 1960s through current.
Over the next several decades, the city and metropolitan area attracted more growth and became a favored tourist destination for its exotic desert setting and recreational opportunities. In 1960 the Phoenix Corporate Center opened; at the time it was the tallest building in Arizona, topping off at 341 feet. The 1960s saw many other buildings constructed as the city expanded rapidly, including: the Rosenzweig Center (1964), today called Phoenix City Square, the landmark Phoenix Financial Center (1964), as well as many of Phoenix's residential high-rises. In 1965 the Arizona Veterans Memorial Coliseum was opened on the grounds of the Arizona State Fair, west of downtown, and in 1968, the city was surprisingly awarded the Phoenix Suns NBA franchise, which played its home games at the Coliseum until 1992. In 1968, the Central Arizona Project was approved by President Lyndon B. Johnson, assuring future water supplies for Phoenix, Tucson, and the agricultural corridor in between. The following year, Pope Paul VI created the Diocese of Phoenix on December 2, by splitting the Archdiocese of Tucson, with Edward A. McCarthy as the first Bishop.
In the 1970s the downtown area experienced a resurgence, with a level of construction activity not seen again until the urban real estate boom of the 2000s. By the end of the decade, Phoenix adopted the Phoenix Concept 2000 plan which split the city into urban villages, each with its own village core where greater height and density was permitted, further shaping the free-market development culture. Originally, there were 9 villages, but this has been expanded to 15 over the years (see Cityscape below). This officially turned Phoenix into a city of many nodes, which would later be connected by freeways. 1972 would see the opening of the Phoenix Symphony Hall, Other major structures which saw construction downtown during this decade were the Wells Fargo Plaza, the Chase Tower (the tallest building in both Phoenix and Arizona) and the U.S. Bank Center.
Nominated by President Reagan, on September 25, 1981 Phoenix resident Sandra Day O'Connor broke the gender barrier on the U.S. Supreme Court, when she was sworn in as the first female judge. 1985 saw the Palo Verde Nuclear Generating Station, the nation's largest nuclear power plant, begin electrical production. 1987 was marked by visits by both Pope John Paul II and Mother Teresa.
There was an influx of refugees due to low-cost housing in the Sunnyslope area in the 1990s, resulting in 43 different languages being spoken in local schools by the year 2000. The new 20-story City Hall opened in 1992, and 1993 saw the creation of "Tent City" by Sheriff Joe Arpaio, using inmate labor, to alleviate overcrowding in the Maricopa County Jail system, the fourth-largest in the world. The famous "Phoenix Lights" UFO sightings took place in March 1997.
Phoenix has maintained a growth streak in recent years, growing by 24.2% before 2007. This made it the second-fastest-growing metropolitan area in the United States, surpassed only by Las Vegas. In 2008, Squaw Peak, the second tallest mountain in the city, was renamed Piestewa Peak after Army Specialist Lori Ann Piestewa, an Arizonan and the first Native American woman to die in combat while serving in the U.S. military, as well as being the first American female casualty of the 2003 Iraq War. 2008 also saw Phoenix as one of the cities hardest hit by the subprime mortgage crisis, and by early 2009 the median home price was $150,000, down from its $262,000 peak in 2007. Crime rates in Phoenix have gone down in recent years, and once troubled, decaying neighborhoods such as South Mountain, Alhambra, and Maryvale have recovered and stabilized. Recently, downtown Phoenix and the central core have experienced renewed interest and growth, resulting in numerous restaurants, stores, and businesses opening or relocating to central Phoenix.
Geography.
Phoenix is in the southwestern United States, in the south-central portion of Arizona, and about halfway between Tucson to the south and Flagstaff to the north. The metropolitan area is known as the "Valley of the Sun", due to its location in the Salt River Valley. It lies at a mean elevation of 1,117 feet (340 m), in the northern reaches of the Sonoran Desert.
Other than the mountains in and around the city, the topography of Phoenix is generally flat, allowing the city's main streets to run on a precise grid with wide, open-spaced roadways. Scattered, low mountain ranges surround the valley: McDowell Mountains to the northeast, the White Tank Mountains to the west, the Superstition Mountains far to the east, and the Sierra Estrella to the southwest. On the outskirts of Phoenix are large fields of irrigated cropland and several Indian reservations. The Salt River runs westward through the city of Phoenix, and the riverbed is often dry or contains a little water due to large irrigation diversions. The community of Ahwatukee is separated from the rest of the city by South Mountain.
According to the United States Census Bureau, the city has a total area of 517.9 square miles (1,341 km2); 516.7 square miles (1,338 km2) of it is land and 1.2 square miles (0.6 km², or 0.2%) of it is water. Even though it is the 6th most populated city, the large area gives it a low density rate of approximately 2,797 people per square mile. In comparison, Philadelphia, the 5th most populous city has a density of over 11,000.
As with most of Arizona, Phoenix does not observe daylight saving time. In 1973, Gov. Jack Williams argued to the U.S. Congress that due to air conditioning units not being used as often in the morning on standard time, energy use would increase in the evening. He went on to say that energy use would rise "because there would be more lights on in the early morning." He was also concerned about children going to school in the dark, which was quite accurate.
Cityscape.
A panoramic view of Phoenix from the South Mountain range, winter 2008, with Sky Harbor International Airport on the far right.
Neighborhoods.
Since 1979, the City of Phoenix has been divided into urban villages, many of which are based upon historically significant neighborhoods and communities that have since been annexed into Phoenix. Each village has a planning committee that is appointed directly by the city council. According to the village planning handbook issued by the city, the purpose of the village planning committees is to work with the city's planning commission to ensure a balance of housing and employment in each village, concentrate development at identified village cores, and to promote the unique character and identity of the villages.
The 15 urban villages are:
Note: the urban village of Paradise Valley is different than the nearby town of Paradise Valley. Although the urban village is part of Phoenix City, the town is independent.
In addition to the above urban villages, Phoenix has a variety of commonly referred-to regions and districts, such as Downtown, Midtown, West Phoenix, North Phoenix, South Phoenix, Biltmore, Arcadia, and Sunnyslope.
Climate.
Phoenix has a hot desert climate (Köppen climate classification "BWh"), typical of the Sonoran Desert in which it lies. Phoenix has long, very hot summers and short, cool winters. The climate is arid, with plenty of sunshine and clear skies. Average high temperatures in summer are some of the hottest of any major city in the United States, and approach those of cities such as Riyadh and Baghdad. On average (1981–2010), there are 107 days annually with a high of at least 100 °F, including most days from late May through early October. Highs top 110 °F an average of 18 days during the year Every day from June 10 through August 24, 1993, the temperature in Phoenix reached 100 °F or more, the longest continuous number of days (76) in the city's history. Officially, the number of days with a high of at least 100 °F has historically ranged from 48 in 1913 to 143 in 1989. For comparison, since 1870, New York City has seen a temperature of 100 degrees or more a total of only 59 days. On June 26, 1990, the temperature reached an all-time recorded high of 122 °F.
Most deserts undergo drastic fluctuations between day and nighttime temperatures, but not Phoenix due to the urban heat island effect. As the city has expanded, average summer low temps have been rising steadily. The daily heat of the sun is stored in pavement, sidewalks and buildings, and is radiated back out at night. During the summer, overnight lows greater than 80 °F are commonplace, as the daily normal low remains at or above 80 °F from June 22 to September 8. On average, 67 days throughout the year will see the nighttime low at or above 80 °F. July 15, 2003 officially saw the record high daily minimum temperature, at 96 °F.
The city averages over 330 days of sunshine, or over 90%, per year, and receives scant rainfall, the average annual total at Phoenix Sky Harbor International Airport being 7 in. Precipitation is sparse during most of the year, but the monsoon brings an influx of moisture. Historically, the monsoon officially started when the average Dew point was 55 degrees for three days in a row; on average this event occurred around July 7; however, in 2008 the National Weather Service decreed that from that point forward, June 15 would be the official first day of the monsoon, and it would end on September 30. The monsoon raises humidity levels and can cause heavy localized precipitation, occasional flooding, large hail, strong winds, the rare tornado, and dust storms, which can rise to the level of a haboob in some years.
July is the wettest month of the year (1.05 in), while June is the driest (.02 in). On September 8, 2014, the city of Phoenix recorded its single highest rainfall total by the National Weather Service with a total of 3.30 inches. This total rainfall on this day measured more than Phoenix's annual rainfall average and broke a 75-year old previous record of 2.91 inches, set back on September 4, 1939. This storm was created from the remnants of Hurricane Norbert that had moved up from the Gulf of California and turned the city's major interstates and low-lying roadways into flood plains, stranding hundreds of motorists. Dewpoints range from 39.0 °F in June to 58.2 °F in August.
On average, Phoenix has only one day per year where the temperature drops to or below freezing. However, the frequency of freezes increases the further one moves outward from the urban heat island. Frequently, outlying areas of Phoenix see frost. Officially, the earliest freeze on record occurred on November 4, 1956, and the latest occurred on March 31, 1987. The all-time lowest recorded temperature in Phoenix was 16 °F on January 7, 1913, while the coldest daily maximum was 36 °F on December 10, 1898. The longest continuous stretch without a day of frost in Phoenix was over 5 years, from November 23, 1979, to January 31, 1985. Snow is a very rare occurrence for the city of Phoenix. Snowfall was first officially recorded in 1898, and since then, accumulations of 0.1 in or greater have occurred only eight times. The heaviest snowstorm on record dates to January 21–22, 1937, when 1 to fell in parts of the city and did not melt entirely for three days. Before that, 1 in had fallen on January 20, 1933. On February 2, 1939, 0.5 in fell. Snow also fell on March 12, 1917 and on November 28, 1919. The most recent snow of significance fell on December 6, 1998, across the northwest portions of the valley that are below 2,000 feet. During the 1998 event, Sky Harbor reported a dusting of snow. The last measurable snowfall was recorded when 0.1 in fell in central Phoenix on December 11, 1985.
On December 30, 2010 and February 20, 2013, graupel fell, although it was widely believed to be snow.
Flora and fauna.
Unusual species are occasionally found within Phoenix boundaries and surrounding areas of Arizona. Native species include desert tortoises, Gila monsters, roadrunners, coyotes, chuckwallas (large lizards), javelina (wild pigs), bobcats, jaguars, and mountain lions. There are many species of falcons, hawks, golden and bald eagles, and the state bird, the cactus wren. Phoenix is also home to a plethora of snakes, such as the western diamondback rattlesnake, sonoran sidewinder, several other types of rattlesnakes, sonoran coralsnake, and dozens of other non-venomous snakes, including the California kingsnake.
The Arizona Upland subdivision of the Sonoran Desert (of which Phoenix is a part) has the most structurally diverse vegetation in the United States. It includes one of the most famous species of succulents, the giant saguaro cactus. Other important species are organpipe, ocotillo, barrel, prickly pear and cholla cacti, Palo Verde trees, various types of palm trees, agaves, foothill and blue paloverde, ironwood, mesquite and creosote bush.
The Greater Phoenix region is home to the only thriving feral population of rosy-faced lovebirds in the U.S. This bird is a popular birdcage pet, native to southwestern Africa. Feral birds were first observed living outdoors in 1987, probably escaped or released pets, and by 2010 the Greater Phoenix population had grown to about 950 birds. These lovebirds prefer older neighborhoods where they nest under untrimmed dead palm tree fronds.
Demographics.
Phoenix is the sixth most populous city in the United States according to the 2010 United States Census, with a population of 1,445,632, making it the most populous state capital in the United States. Phoenix's ranking as the sixth most populous city was a drop from the number five position it had held since the U. S. Census Bureau released population estimates on June 28, 2007. Those statistics used data from 2006, which showed Phoenix's population at 1,512,986, which put it just ahead of Philadelphia. The 2010 Census, while showing an overall increase from the official 2000 Census showed a drop in Phoenix' population from the 2007 estimates, allowing Philadelphia to regain the fifth spot.
After leading the nation in population growth for over a decade, the sub-prime mortgage crisis, followed by the recession, led to a slowing in the growth of Phoenix. There were approximately 77,000 people added to the population of the Phoenix metropolitan area in 2009, which was down significantly from its peak in 2006 of 162,000. Despite this slowing, Phoenix's population grew by 9.4% since the 2000 census (a total of 124,000 people), while the entire Phoenix metropolitan area grew by 28.9% during the same period. This compares with an overall growth rate nationally during the same time frame of 9.7%. Not since 1940-50, when the city had a population of 107,000, had the city gained less than 124,000 in a decade. Phoenix's recent growth rate of 9.4% from the 2010 census is the first time it has recorded a growth rate under 24% in a census decade.
The Phoenix Metropolitan Statistical Area (MSA) (officially known as the Phoenix-Mesa-Glendale MSA), is one of 10 MSAs in Arizona, and was the 14th largest in the United States, with a total population of 4,192,887 as of the 2010 Census. Consisting of parts of both Pinal and Maricopa counties, the MSA accounts for 65.5% of the total population of the state of Arizona. Phoenix only contributed 13% to the total growth rate of the MSA, down significantly from its 33% share during the prior decade. Phoenix is also part of the Arizona Sun Corridor megaregion (MR), which is the 10th most populous of the 11 MRs, and the 8th largest by area. It had the 2nd largest growth by percentage of the MRs (behind only the Gulf Coast MR) between 2000 and 2010.
The population is almost equally split between men and women, with men making up 50.2% of city's citizens. The population density is 2,797.8 people per square mile, and the median age of the city is 32.2 years, with only 10.9 of the population being over 62. 98.5% of Phoenix's population lives in households with an average household size of 2.77 people. There were 514,806 total households, with 64.2% of those households consisting of families: 42.3% married couples, 7% with an unmarried male as head of household, and 14.9% with an unmarried female as head of household. 33.6% of those households have children below the age of 18. Of the 35.8% of non-family households, 27.1% of them have a householder living alone, almost evenly split between men and women, with women having 13.7% and men occupying 13.5%. Phoenix has 590,149 housing units, with an occupancy rate of 87.2%. The largest segment of vacancies is in the rental market, where the vacancy rate is 14.9%, and 51% of all vacancies are in rentals. Vacant houses for sale only make up 17.7% of the vacancies, with the rest being split among vacation properties and other various reasons.
The median income for a household in the city was $47,866, and the median income for a family was $54,804. Males had a median income of $32,820 versus $27,466 for females. The per capita income for the city was $24,110. 21.8% of the population and 17.1% of families were below the poverty line. Out of the total population, 31.4% of those under the age of 18 and 10.5% of those 65 and older were living below the poverty line.
According to the 2010 Census, the racial breakdown of Phoenix was as follows:
Phoenix's population has historically been predominantly white. From 1890 to 1970, over 90% of the citizens were white. In recent years, this percentage has dropped, reaching 65% In 2010. However, a significant portion of this decrease can be attributed to new guidelines put out by the U.S. Census Bureau in 1980, when a question regarding Hispanic origin was added to the census questionnaire. This has led to an increasing tendency for some groups to no longer self-identify as white, and instead categorize themselves as "other races". 20.6% of the population of the city was foreign born in 2010. Of the 1,342,803 residents over 5 years of age, 63.5% spoke only English, 30.6% spoke Spanish at home, 2.5% spoke another Indo-European language, 2.1% spoke Asian or Islander languages, with the remaining 1.4% speaking other languages. About 15.7% of non-English speakers reported speaking English less than "very well". The largest national ancestries reported were Mexican (35.9%), German (15.3%), Irish (10.3%), English (9.4%), Black (6.5%), Italian (4.5%), French (2.7%), Polish (2.5%), American Indian (2.2%), and Scottish (2.0%).
In 2010, according to the Association of Religion Data Archives, which conducts religious census each ten years, 39% of those polled in Maricopa county considered themselves a member of a religious group. Of those who expressed a religious affiliation, the area's religious composition was reported as 35% Catholic, 22% to Evangelical Protestant denominations, 16% Latter-Day Saints (LDS), 14% to nondenominational congregations, 7% to Mainline Protestant denominations, and 2% Hindu. The remaining 4% belong to other religions, such as Buddhism, and Judaism. While there was an overall increase in the number of religious adherents over the decade of 103,000, that did not keep pace with the overall population increase in the country during the same period, which increased by almost three-quarters of million individuals, resulting in the percentage drop. The largest aggregate increases were in the LDS (a 58% increase) and Evangelical Protestant churches (14% increase), while all other categories actually saw their numbers drop slightly, or remain static. Overall, the Catholic Church had an 8% drop, while Mainline Protestant groups saw a 28% decline.
Economy.
The early economy of Phoenix was focused primarily on agriculture and natural resources, dependent mainly on the "5Cs", which were copper, cattle, climate, cotton, and citrus. Once the Salt River Project was completed, the city, and the valley in general, began to develop more rapidly, due to a now fairly reliable source of water. Led by agriculture, the number one crop in the 1910s was alfalfa, followed by citrus, cotton, and other crops, with almost a quarter-million acres under cultivation by the middle of the decade. World War I would greatly change the agricultural landscape of the valley, and teach the farmers of the region an invaluable, if difficult lesson.
As the war began, imports of foreign cotton were no longer available to American manufacturing, since cotton was a major material used in the production of tires and airplane fabric, those manufacturers began to look for new sources. The Salt River Valley looked to be an ideal location for expansion of the cotton crop. Led by Goodyear, tire and airplane manufacturers began to buy more and more cotton from valley growers. In fact, the town of Goodyear was founded during this period when the company purchased desert acreage southwest of Phoenix to grow cotton. By 1918, cotton had replaced alfalfa as the number one industry in Phoenix. As the price of cotton rose, more and more of Phoenix acreage was devoted to the crop. However, in 1920, when cotton accounted for three-quarters of the cultivated acreage in the valley, the bottom fell out of the cotton market due to the dual reasons of lower demand due to the end of the war production machine and foreign growers now once again having access to the American market, resulting in their shipping large amounts of cotton to the U.S. This led to a diversification of crops in the valley from that point forward.
Cattle and the meat industry was also a vital part of the economy. The cotton bust led to more production of alfalfa, wheat, and barley, as well as citrus. The grain production in turn led to an increase in the cattle ranching industry. By the end of the Roaring Twenties, Phoenix boasted the largest meat processing plant between Dallas and Los Angeles. While that plant and its attendant stockyards are long gone, a remnant remains in the famous Stockyards Restaurant. The prosperity following the local depression caused by the cotton bust enabled other industries to grow as well. The city's first skyscraper, the 7-story Heard Building, was built in 1920. This was followed by the 10-story Luhrs Building after the bust in 1924, and then by the Westward Ho, a 16-story hotel constructed in 1928.
With the establishment of a main rail line (the Southern Pacific) in 1926, the opening of the Union Station in 1923, and the creation of Sky Harbor airport by the end of the decade, the city became more easily accessible. The construction of the Westward Ho was part of a concerted effort on the part of both civic and business organizations in the city to develop it as a tourist destination. Phoenix already had two highly rated resorts, the Ingleside Inn and the Jokake Inn, and after the Westward Ho, the Arizona Biltmore, designed by one of Frank Lloyd Wright's students, was constructed in 1929. Other major hotels were built during this era, such as the San Carlos (also in 1928), which led older hotels, like the Hotel Adams, to refurbish themselves in order to remain competitive. By the end of the decade, the tourism industry topped $10 million for the first time in the city's history. Tourism remains one of the top ten economic drivers of the city to this day.
The Great Depression affected Phoenix, but the effects were not as deep or long-lasting as they were in much of the rest of the country. Phoenix had a very diverse economy, and was not heavily vested in the manufacturing sector. While the stock market crash did not affect the city very much directly, the suppression of the national economy did. Revenue from all major industries in the valley decreased drastically: copper mining dropped from $155 million in 1929 to $15 million by 1932; agriculture and livestock also saw reductions during that same period, although not as drastic, from $42 million to $14 million and $25.5 million to $15 million, respectively. Compared to the rest of the country, and even the rest of the state, Phoenix was not as badly affected by bankruptcies, foreclosures, or unemployment, and by 1934 the recovery was underway.
At the conclusion of World War II, the valley's economy began to further grow and expand. After the war, the city's population began to surge, as many men who had undergone their military training at the various bases in and around Phoenix returned with their families. In 1948, Motorola chose Phoenix for the site of its new research and development center for military electronics. They were followed in time by other high-tech companies, such as Intel and McDonnell Douglas.
The construction industry, spurred on by the city's growth, further expanded with the development of Sun City. Much like Levittown, New York became the template for suburban development in post-WWII America, Sun City, just northwest of Phoenix, became the template for retirement communities when Del E. Webb opened the community in 1960. Over 100,000 people visited the community during the opening weekend.
As the financial crisis of 2007–10 began, construction in Phoenix collapsed in 2008, and housing prices plunged. Historically, Arizona trailed the rest of the country into recession, but Phoenix entered this last recession before the rest of the country due to the prominence of the construction industry in its economy.
According to the Bureau of Economic Analysis of the U.S. Department of Commerce, in 2012 (the latest year for which data is available), the Phoenix MSA had a Gross Domestic Product (GDP) of just over $201 billion, a 4.5% increase over the prior year. Phoenix's GDP finally exceeded the high it had attained in 2008, prior to the recession. The top 10 industries were, in descending order: real estate ($31B), financial services ($21.3B), manufacturing ($16.8B), health care ($15.7B), retail ($14.9B), wholesale ($12.9B), professional services ($12.8B), construction ($10.4B), waste management ($9.1B), and tourism ($6.8B). Government, if it had been a private industry, would have been ranked third on the list, generating $18.9 billion. Manufacturing now ranks third among Phoenix's industries, and includes the production of computers and other electronic equipment, missiles, aircraft parts, chemicals, and processed foods.
In major job markets, as defined as those markets with greater than 1 million jobs, Greater Phoenix ranked number 1 in employment growth prior to the recession beginning in 2007. Just three years later, it ended its free fall in job growth by hitting the bottom of the list of those 28 major markets, dead last. However, 2013 saw Greater Phoenix rebound to 7th. Arizona's year-over-year job growth (of which Phoenix is the main driver) continued to outpace the nation through August 2013. Arizona's year-over-year job growth was at or above 2.0% each month of that year. In contrast, national job growth was between 1.5% and 1.7% on a year-over-year basis. Arizona is forecast to regain its previous employment peak in 2015, making it eight years for the state to get back to even terms after the Great Recession; the national economy is currently forecast to replace all of the jobs lost by 2014, one year earlier than Arizona. This is due to the more severe downturn in Arizona as compared to the rest of the nation, as evidenced by the fact that Arizona jobs declined by 11.8% from peak to trough, compared to 6.3% for the nation. In 2013, the Phoenix area saw a 2.7% increase in non-farm employment, from 1.758 million to 1.805 million. Job growth has occurred across the board, with the fastest rate in education and health services, trade, transportation and utilities, professional and business services, financial activities, and leisure and hospitality.
According to the 2010 Census, the top ten employment categories are office and administrative support occupations (17.8%), sales and related occupations (11.6%), food preparation and serving related occupations (9%), transportation and material moving occupations (6.1%), management occupations (5.8%), education, training, and library occupations (5.5%), business and financial operations occupations (5.3%), healthcare practitioners and technical occupations (5.3%), production occupations (4.6%), and construction and extraction occupations (4.2%). The single largest occupation is retail salespersons, which account for 3.7% of the entire workforce. As of December, 2013, 12.9% of the workforce were government employees, a high number because the city is both the county seat and state capitol. The civilian labor force was 2,033,400 (down 0.5% from twelve months earlier), and the unemployment rate stood at 7.6%, above the national rate of 6.7%.
Phoenix is currently home to four Fortune 500 companies: electronics corporation Avnet, mining company Freeport-McMoRan, retailer PetSmart, and waste hauler Republic Services. Honeywell's Aerospace division is headquartered in Phoenix, and the valley hosts many of their avionics and mechanical facilities. Intel has one of their largest sites in the area, employing about 12,000 employees, the second largest Intel location in the country; they are spending $5 billion to expand their semiconductor plant. American Express hosts their financial transactions, customer information, and their entire website in Phoenix. The city is also home to: the headquarters of U-HAUL International, a rental and moving supply company; Best Western, the world's largest family of hotels; Apollo Group, parent of the University of Phoenix; and utility company Pinnacle West. Choice Hotels International has its IT division and operations support center in the North Phoenix area. US Airways, now merged with American Airlines has a strong presence in Phoenix, with the corporate headquarters located in the city prior to the merger. US Air/American Airlines is the largest carrier at Sky Harbor International Airport in Phoenix. Mesa Air Group, a regional airline group, is headquartered in Phoenix.
The military has a significant presence in Phoenix, with Luke Air Force Base located in the western suburbs. At its height, in the 1940s, the Phoenix area had three military bases: Luke Field (still in use), Falcon Field, and Williams Air Force Base (now Phoenix-Mesa Gateway Airport), with numerous auxiliary air fields located throughout the region. Foreign governments have established 30 consular offices and eleven active foreign chambers of commerce and trade associations in metropolitan Phoenix.
Culture.
Performing arts.
The city has numerous performing arts venues, most of which are located in and around downtown Phoenix or Scottsdale. The Phoenix Symphony Hall is home to the Phoenix Symphony Orchestra, the Arizona Opera and Ballet Arizona. The Arizona Opera company also has intimate performances at its new Arizona Opera Center, which opened in March 2013. Another venue is the Orpheum Theatre, which is home to the Phoenix Opera, formerly known as the Phoenix Metropolitan Opera. Ballet Arizona, in addition to the Symphony Hall, also has performances at the Orpheum Theater as well at the Dorrance Theater. Concerts also regularly make stops in the area. The largest downtown performing art venue is the Herberger Theater Center, which houses three performance spaces and is home to two resident companies, the Arizona Theatre Company and the Centre Dance Ensemble. Three other groups also use the facility: Valley Youth Theatre, iTheatre Collaborative and Actors Theater.
Concerts can be attended at Talking Stick Resort Arena (formerly US Airways Center) and Comerica Theatre in downtown Phoenix, Ak-Chin Pavilion (formerly Cricket Wireless Pavilion) in Maryvale, Gila River Arena (formerly Jobing.com Arena) in Glendale, and Gammage Auditorium in Tempe (the last public building designed by Frank Lloyd Wright). Several smaller theatres including Trunk Space, the Mesa Arts Center, the Crescent Ballroom, Celebrity Theatre, and Modified Arts support regular independent musical and theatre performances. Music can also be seen in some of the venues usually reserved for sports, such as Wells Fargo Arena and University of Phoenix Stadium.
Several television series have been set in Phoenix, including "Alice", the 2000s paranormal drama "Medium", the 1960–61 syndicated crime drama "The Brothers Brannagan", and "The New Dick Van Dyke Show" from 1971 to 1974.
Museums.
Dozens of museums exist throughout the valley. They include the Phoenix Art Museum, Arizona Capitol Museum, Arizona Military Museum, Hall of Flame Firefighting Museum, the Pueblo Grande Museum and Cultural Park, Children's Museum of Phoenix, Arizona Science Center, and the Heard Museum. In 2010 the Musical Instrument Museum opened their doors, featuring the biggest musical instrument collection in the world.
Designed by Alden B. Dow, a student of Frank Lloyd Wright, the Phoenix Art Museum was constructed in a single year, opening in November 1959. The Phoenix Art Museum presents a year-round program of festivals, live performances, independent art films and educational programs. The Southwest's largest destination for visual art, it displays international exhibitions alongside the museum's comprehensive collection of more than 17,000 works of contemporary and modern art from around the world, as well as exhibits of fashion design. Interactive exhibits can be found in nearby Peoria's Challenger Space Center, where individuals learn about space, renewable energies, and meet astronauts.
The Heard Museum has over 130,000 square feet (12,000 m²) of gallery, classroom and performance space. Some of the signature exhibits include a full Navajo hogan, the Mareen Allen Nichols Collection containing 260 pieces of contemporary jewelry, the Barry Goldwater Collection of 437 historic Hopi kachina dolls, and an exhibit on the 19th century boarding school experiences of Native Americans. The Heard Museum attracts about 250,000 visitors a year.
Fine arts.
The downtown Phoenix art scene has developed in the past decade. The Artlink organization and the galleries downtown have successfully launched a First Friday cross-Phoenix gallery opening. In April 2009, artist Janet Echelman inaugurated her monumental sculpture, "Her Secret Is Patience", a civic icon suspended above the new Phoenix Civic Space Park, a two-city-block park in the middle of downtown. This netted sculpture makes the invisible patterns of desert wind visible to the human eye. During the day, the 100 ft-tall sculpture hovers high above heads, treetops, and buildings, the sculpture creates what the artist calls "shadow drawings", which she says are inspired by Phoenix's cloud shadows. At night, the illumination changes color gradually through the seasons. Author Prof. Patrick Frank writes of the sculpture that "... most Arizonans look on the work with pride: this unique visual delight will forever mark the city of Phoenix just as the Eiffel Tower marks Paris."
Architecture.
Phoenix is the home of a unique architectural tradition and community. Frank Lloyd Wright moved to Phoenix in 1937 and built his winter home, Taliesin West, and the main campus for The Frank Lloyd Wright School of Architecture. Over the years, Phoenix has attracted notable architects who have made it their home and have grown successful practices. These architectural studios embrace the desert climate, and are unconventional in their approach to the practice of design. They include the Paolo Soleri (who created Arcosanti), Al Beadle, Will Bruder, Wendell Burnette, and Blank Studio architectural design studios. Another major force in architectural landscape of the city was Ralph Haver whose firm, Haver & Nunn, designed commercial, industrial and residential structures throughout the valley. Of particular note was his trademark, "Haver Home", which were affordable contemporary-style tract houses.
Tourism.
The tourist industry is the longest running of today's top industries in Phoenix. Starting with promotions back in the 1920s, the industry has grown into one of the top 10 in the city. Due to its climate, Phoenix and its neighbors have consistently ranked among the nation's top destinations in the number of Five Diamond/Five Star resorts. With more than 62,000 hotel rooms in over 500 hotels and 40 resorts, greater Phoenix sees over 16 million visitors each year, the majority of whom are leisure (as opposed to business) travelers. Sky Harbor Airport, which serves the Greater Phoenix area, serves about 40 million passengers a year, ranking it among the 10 busiest airports in the nation.
One of the biggest attractions to the Phoenix area is golf, with over 200 golf courses. In addition to the sites of interest in the city, there are many attractions near Phoenix, such as: Agua Fria National Monument, Arcosanti, Casa Grande Ruins National Monument, Lost Dutchman State Park, Montezuma's Castle, Montezuma's Well, and Organ Pipe Cactus National Monument. Phoenix also serves as a jumping off point to many of the sights around the state of Arizona, such as the Grand Canyon, Lake Havasu (where the London Bridge is located), Meteor Crater, the Painted Desert, the Petrified Forest, Tombstone, Kartchner Caverns, Sedona and Lowell Observatory in Flagstaff.
Other attractions and annual events.
Due to its natural beauty and climate, Phoenix has a plethora of outdoor attractions and recreational activities. The Phoenix Zoo is the largest privately owned, non-profit zoo in the United States. Since opening in 1962, the zoo has developed an international reputation for its efforts on animal conservation, including breeding and reintroducing endangered species back into the wild. Right next to the zoo, the Phoenix Botanical Gardens were opened in 1939, and are acclaimed worldwide for their exhibits and educational programs, featuring the largest collection of arid plants in the U.S. South Mountain Park, the largest municipal park in the U.S., is also the highest desert mountain preserve in the world.
Other popular sites in the city are: Japanese Friendship Garden, Historic Heritage Square, Phoenix Mountains Park, Pueblo Grande Museum, Tovrea Castle, Camelback Mountain, Hole in the Rock, Mystery Castle, St. Mary's Basilica, Taliesin West, and the Wrigley Mansion.
There are long list of annual events in and near Phoenix which celebrate the heritage of the city, as well as its diversity. Some of them are:
Cuisine.
Like many other western towns, the earliest restaurants in Phoenix were often steakhouses. Today, Phoenix is also renowned for its Mexican food, thanks to both its large Hispanic population and its proximity to Mexico. Some of Phoenix's restaurants have a long history. The Stockyards steakhouse dates to 1947, while Monti's La Casa Vieja (Spanish for "The Old House") has been in operation as a restaurant since the 1890s. Macayo's (a Mexican restaurant chain) was established in Phoenix in 1946, and other major Mexican restaurants include Garcia's (1956) and Manuel's (1964). The recent population boom has brought people from all over the nation, and to a lesser extent from other countries, and has since influenced the local cuisine. Phoenix currently boasts cuisines from all over the world, such as Korean, barbecue, Cajun/Creole, Greek, Hawaiian, Irish, Japanese, sushi, Italian, fusion, Persian, Indian, Spanish, Thai, Chinese, southwestern, Tex-Mex, Vietnamese, Brazilian, and French.
The first McDonald's franchise was sold by the McDonald brothers to a Phoenix entrepreneur in 1952. Neil Fox paid $1,000 for the rights to open an establishment based on the McDonald brothers' restaurant. The hamburger stand opened in 1953 on the southwest corner of Central Avenue and Indian School Road on the growing north side of Phoenix, and was the first location to sport the now internationally known "golden arches", which were initially twice the height of the building. Three other franchise locations opened that year, a full two years before Kroc purchased McDonald's and opened his first franchise in Illinois.
Sports.
Phoenix is home to several professional sports franchises, and is one of only 12 U.S. cities to have representatives of all four major professional sports leagues, although only one of these teams actually carry the city name and two of them play within the city limits.
The Phoenix Suns were the first major sports team in Phoenix, being granted a National Basketball Association (NBA) franchise in 1968. They had originally played at the Arizona Veterans Memorial Coliseum before moving to America West Arena (now Talking Stick Resort Arena) in 1992. The year following their move to the new arena, the Suns made the NBA finals for the second time in franchise history, losing to Michael Jordan's Chicago Bulls, 4 games to 2. In 1997, the Phoenix Mercury were one of the original eight teams to launch the Women's National Basketball Association (WNBA). They also play at Talking Stick Resorts Arena. They have been to the WNBA championship series four times, losing in 1998 to the Houston Comets, before winning their first WNBA championship in 2007, when they defeated the Detroit Shock in five games. They would repeat their championship in 2009, when they defeated the Indiana Fever, and in 2014 when they swept the Chicago Sky.
The Arizona Diamondbacks of Major League Baseball (National League West Division) began play as an expansion team in 1998. The team has played all of its home games in the same downtown park; originally called Bank One Ballpark (or "BOB" for short), in 2005 the stadium's name was changed to Chase Field. It is the second highest stadium in the U.S. (after Coors Field in Denver), and is famous for its nationally known swimming pool beyond the outfield fence. In 2001, the Diamondbacks defeated the New York Yankees 4 games to 3 in the World Series, becoming the city's first professional sports franchise to win a national championship while located in Arizona. The win was also the fastest an expansion team had ever won the World Series, surpassing the old mark of the Florida Marlins of 5 years, set in 1997.
The Arizona Cardinals are the oldest continuously run professional football franchise in the nation. They moved to Phoenix from St. Louis, Missouri in 1988 and currently play in the Western Division of the National Football League's National Football Conference. The Cardinals were founded in 1898 in Chicago, as the Morgan Athletic Club, and became known as the Cardinals shortly after, due to the color of their jerseys. Around the turn of the last century, they were known as the Racine Cardinals, and in 1920, they became a charter member of the American Professional Football League, which would eventually become the National Football League. Upon their move to Phoenix, the Cardinals originally played their home games at Sun Devil Stadium on the campus of Arizona State University in nearby Tempe. In 2006 they moved to the newly constructed University of Phoenix Stadium in suburban Glendale. Since moving to Phoenix, the Cardinals have made one championship appearance, Super Bowl XLIII on February 1, 2009, where they lost 27-23 to the Pittsburgh Steelers.
The Arizona Coyotes of the National Hockey League moved to the area in 1996, formerly known as the Winnipeg Jets. They originally played their home games at America West Arena in downtown Phoenix before moving in December 2003 to the Jobing.com Arena (now named the Gila River Arena), adjacent to University of Phoenix Stadium in Glendale.
Phoenix has an arena football team, the Arizona Rattlers of the Arena Football League. Their games are also played at Talking Stick Resort Arena. They won their first of five AFL championships in 1994; in 2014 they won their third championship in a row.
The Greater Phoenix area is home to the Cactus League, one of two spring training leagues for Major League Baseball. With the move by the Colorado Rockies and the Arizona Diamondbacks to their new facility in Scottsdale, the league is entirely based in the Greater Phoenix area, as opposed to the Grapefruit League, which is spread throughout southern Florida. With the Cincinnati Reds' move to Goodyear, fifteen of MLB's thirty teams are now included in the Cactus League.
The Phoenix International Raceway, was built in 1964 with a one-mile oval, with a one-of-a-kind design, as well as a 2.5-mile road course. Today, "Phoenix International Raceway has a tradition that is unmatched in the world of racing." It currently hosts several NASCAR events per season, and the annual Fall NASCAR weekend, which includes events from four different NASCAR classes, is a huge event. After thirty years of hosting various events, especially NHRA drag racing events, Firebird International Raceway (FIR) closed operations in 2013. However, the NHRA negotiated a deal with the Gila River Indian Community (the owners of FIR) and re-opened the venue to NHRA events in 2014, under the new name, "Wild Horse Pass Motorsports Park". Phoenix hosted the United States Grand Prix from 1989 to 1991. The race was discontinued after the 1991 edition due to poor attendance.
The Phoenix Marathon is a new addition to the city's sports scene, and is a qualifier for the Boston Marathon. The Rock 'n' Roll Marathon series has held an event in Phoenix every January since 2004.
Sun Devil Stadium held Super Bowl XXX in 1996 when the Dallas Cowboys defeated the Pittsburgh Steelers. University of Phoenix Stadium hosted Super Bowl XLII on February 3, 2008, in which the New York Giants defeated the New England Patriots. The University of Phoenix Stadium also hosted Super Bowl XLIX on February 1, 2015 which resulted in the New England Patriots defeating the Seattle Seahawks 28-24. The U.S. Airways Center hosted both the 1995 and the 2009 NBA All-Star Games.
The Phoenix area is the site of two college football bowl games: the TicketCity Cactus Bowl, formerly known as the Insight Bowl, which was at Chase Field until 2005, after which it moved to Sun Devil Stadium; and the Fiesta Bowl, played at the University of Phoenix Stadium. The city is also host to several major professional golf events, including the LPGA's Founder's Cup and, since 1932, The Phoenix Open of the PGA.
Phoenix's Ahwatukee American Little League reached the 2006 Little League World Series as the representative from the U.S. West region.
Professional clubs
Semi-professional and amateur clubs
Parks and recreation.
Phoenix is home to a large number of parks and recreation areas. The city of Phoenix includes national parks, county (Maricopa County) parks and city parks. Tonto National Forest forms part of the northeast boundary of the city, while the county has the largest park system in the country. The city park system was established to preserve the desert landscape in areas that would otherwise have succumbed to development, and includes South Mountain Park, the world's largest municipal park with 16500 acre. The city park system has 189 parks which contain over 33,000 acres, and has facilities for hiking, camping, swimming, horseback riding, cycling, and climbing. Some of the other notable parks in the system are Camelback Mountain, Encanto Park (another large urban park) and Sunnyslope Mountain, also known as "S" Mountain. Papago Park in east Phoenix is home to both the Desert Botanical Garden and the Phoenix Zoo, in addition to several golf courses and the Hole-in-the-Rock geological formation. The Desert Botanical Garden, which opened in 1939, is one of the few public gardens in the country dedicated to desert plants, and displays desert plant life from all over the world. The Phoenix Zoo is the largest privately owned non-profit zoo in the United States, and is internationally known for its programs devoted to saving endangered species.
In addition, many waterparks are scattered throughout the valley to help residents cope with the desert heat during the summer months. Some of the notable parks include Big Surf in Tempe, Wet 'n' Wild Phoenix in Phoenix, Golfland Sunsplash in Mesa, and the Oasis Water Park at the Arizona Grand Resort – formerly known as Pointe South Mountain Resort – in Phoenix. The area also has two small amusement parks, Castles N' Coasters in north Phoenix, next to the Metrocenter Mall and Enchanted Island located at Encanto Park.
Government.
In 1913, Phoenix adopted a new form of government, switching from the mayor-council system to the council-manager system, making it one of the first cities in the United States with this form of city government, where a strong city manager supervises all city departments and executes the policies adopted by the Council.
The city council consists of a mayor and eight city council members. While the mayor is elected in a citywide election, Phoenix City Council members are elected by votes only in the districts they represent, with both the Mayor and the Council members serving four year terms. The current mayor of Phoenix is Greg Stanton, a Democrat who was elected to a four-year term in 2011. In setting city policy and passing rules and regulations, the mayor and city council members each have equal voting power. The city's website was given a "Sunny Award" by Sunshine Review for its transparency efforts.
State government facilities.
As the capital of Arizona, Phoenix houses the state legislature, along with numerous state government agencies, many of which are located in the State Capitol district immediately west of downtown. The Arizona Department of Juvenile Corrections operates the Adobe Mountain and Black Canyon Schools in Phoenix. Another major state government facility is the Arizona State Hospital, operated by the Arizona Department of Health Services. This is a mental health center which is the only medical facility run by the state government. The headquarters of numerous Arizona state government agencies are in Phoenix, with many located in the State Capitol district immediately west of downtown.
Federal government facilities.
The Federal Bureau of Prisons operates the Federal Correctional Institution (FCI) Phoenix which is in the city limits, near its northern boundary.
The Sandra Day O'Connor U.S. Courthouse, the U.S. District Court of Arizona, is located on Washington Street downtown. It is named in honor of retired U.S. Supreme Court Justice Sandra Day O'Connor, who was raised in Arizona.
The Federal Building is at the intersection of Van Buren Road and First Avenue downtown, and contains various federal field offices and the local division of the U.S. Bankruptcy Court. This building also formerly housed the U.S. District Court offices and courtrooms, but these were moved in 2001 to the new Sandra Day O'Connor U.S. Courthouse. Before the construction of this building in 1961, federal government offices were housed in the historic U.S. Post Office on Central Avenue, completed in the 1930s.
Crime.
By the 1960s crime was becoming a significant problem in Phoenix, and by the 1970s crime continued to increase in the city at a faster rate than almost anywhere else in the country. It was during this time frame when an incident occurred in Phoenix which would have national implications. On March 16, 1963, Ernesto Miranda was arrested and charged with the rape of an 18-year-old woman with mild intellectual disabilities. The subsequent Supreme Court ruling on June 13, 1966, in the matter of Miranda v. Arizona, has led to practice in the United States of issuing a "Miranda Warning" to all suspected criminals.
By the mid 1970s, Phoenix was close to or at the top of the list for cities with the highest crime rate. The mayor during the mid-70s, Mayor Graham, introduced policies which raised Phoenix from near the bottom of the statistics regarding police officers per capita, to where it resided in the middle of the rankings. He also implemented other changes, including establishing a juvenile department within the police force. With Phoenix's rapid growth, it drew the attention of con men and racketeers, with one of the prime areas of activity being land fraud. The practice became so widespread that newspapers would refer to Phoenix as "the Tainted Desert".
These land frauds led to one of the more infamous murders in the history of the valley, when "Arizona Republic" writer Don Bolles was murdered by a car bomb at the Clarendon Hotel in 1976. It was believed that his investigative reporting on organized crime and land fraud in Phoenix made him a target. Bolles' last words referred to Phoenix land and cattle magnate Kemper Marley, who was widely regarded to have ordered Bolles' murder, as well as John Harvey Adamson, who pleaded guilty to second-degree murder in 1977 in return for testimony against contractors Max Dunlap and James Robison.
The trial gained national attention since Bolles was the only reporter from a major U.S. newspaper to be murdered on U.S. soil due to his coverage of a story, and led to reporters from all over the country descending on Phoenix to cover his murder. Dunlap was convicted of first degree murder in the case in 1990 and remained in prison, until his death on July 21, 2009, while Robison was acquitted, but pleaded guilty to charges of soliciting violence against Adamson. Street gangs and the drug trade had turned into public safety issues by the 1980s. Despite continued improvements in the size of the police force and other anti-crime measures, the crime rate in Phoenix continued to grow, albeit at a lower growth rate than other southwestern cities.
After seeing a peak in the early and mid 1990s, the city has seen a general decrease in both the violent and property crime rates. 1993 saw the creation of "Tent City," by Sheriff Joe Arpaio, using inmate labor, to alleviate overcrowding in the Maricopa County Jail system, the fourth-largest in the world. The violent crime rate peaked in 1993 at 1146 crimes per 100,000 people, while the property crime rate peaked a few years earlier, in 1989, at 9,966 crimes per 100,000. In the most recent numbers from the FBI (2012), those rates currently stand at 637 and 4091, respectively. When compared to the other cities on the 10 most populated list, this ranks Phoenix 5th and 6th, respectively. Since their peak in 2003, murders have dropped from 241 to 123 in 2012. Assaults have also dropped from 7,800 in 1993 to 5,260 in 2012. In the 20 years since 1993, there have only been five years in which the violent crime rate has not declined.
The year 2012 was an anomaly to the general downward trend in violent crime in Phoenix, with the rates for every single violent crime, except rape, showing an increase. The murder rate increased by 15.4% and aggravated assaults jumped by 27%, while rapes were down by 2%. However, the property crime rate returned to the downward trend begun in the 1990s, after a slight uptick in the previous two years. Vehicle thefts, which have been perceived as a major issue in the Valley of the Sun for decades, saw a continuation of a downward trend begun over a decade ago. In 2001 Phoenix ranked first in the nation in vehicle thefts, with over 22,000 cars stolen that year. That continued in 2002, when car thefts rose to over 25,000, a rate of over 1,825 thefts per 100,000 people. It has declined every year since then, and last year stood at just over 480, a drop of almost 75% in the decade. According to the "Hot Spots" report put out by the National Insurance Crime Bureau (NICB), The Phoenix MSA has dropped to 70th in the nation in terms of car thefts in 2012.
As the first decade of the new century came to a close, Arizona had become the gateway to the U.S. for drug trafficking. By 2009, seizures in Arizona amounted for approximately half of all Marijuana captured along the U.S.-Mexican border. Another crime issue related to the drug trade are kidnappings. In the late 2000s, Phoenix earned the title "Kidnapping capital of the USA". The majority of the kidnapped are believed to be victims of human smuggling, or related to illegal drug trade, while the kidnappers are believed to be part of Mexican drug cartels, particularly the Sinaloa cartel.
Education.
Public education in the Phoenix area is provided by 29 school districts. There are 21 elementary school districts, which contain over 215 elementary schools, and they are paired with 4 high school districts, which have a total of 31 high schools serving Phoenix. Three of the high school districts (Glendale Union, Tempe Union and Tolleson Union) only partially serve Phoenix. With over 27,000 students, and spread over 220 square miles, The Phoenix Union High School District is one of the largest high school districts in the country, containing 16 schools and nearly 3,000 employees. In addition there are 4 unified districts, which cover grades K-12, which add an additional 58 elementary schools and 4 high schools to Phoenix's educational system. Of those four, only the Paradise Valley district completely serves Phoenix. Phoenix is also served by an expanding number of charter schools, with over 30 currently operating in the city.
Post-secondary education.
Arizona State University is the main institution of higher education in the region. Its main campus is in Tempe. ASU also has campuses in northwest Phoenix (ASU West Campus), downtown Phoenix (ASU Downtown Campus) and Mesa (ASU Polytechnic Campus). ASU is one of the largest public universities in the U.S., with a 2012 student enrollment of 72,254.
A branch of the University of Arizona College of Medicine is located near ASU's downtown Phoenix campus. There is also a small satellite campus for Northern Arizona University (based in Flagstaff) located in Phoenix.
The Maricopa County Community College District includes ten community colleges and two skills centers throughout Maricopa County, providing adult education and job training. Phoenix College, part of the district, was founded in 1920 and is the oldest community college in Arizona and one of the oldest in the country.
The city is also home to numerous other institutions of higher learning. Some of the more notable are:
Media.
The first newspaper in Phoenix was the weekly "Salt River Valley Herald", established in 1878, which would change its name the following year to the "Phoenix Herald". The paper would go through several additional name changes in its early years before finally settling on the "Phoenix Herald", which still exists today in an on-line form. Today, the city is served by two major daily newspapers: "The Arizona Republic", which along with its online entity, "azcentral.com", serves the greater metropolitan area; and the "East Valley Tribune", which primarily serves the cities of the East Valley. The "Jewish News of Greater Phoenix" is an independent weekly newspaper established in 1948. In addition, the city is also served by numerous free neighborhood papers and weeklies such as the "Phoenix New Times", and Arizona State University's "The State Press".
The Phoenix metro area is served by many local television stations and is the largest designated market area (DMA) in the Southwest, and the 12th largest in the U.S., with over 1.8 million homes (1.6% of the total U.S.). The major network television affiliates are KNXV 15 (ABC), KPHO 5 (CBS), KPNX 12 (NBC), KSAZ 10 (Fox), KASW 61 (The CW), KUTP 45 (MyNetworkTV), and KAET 8 (PBS, operated by Arizona State University). Other network television affiliates operating in the area include KPAZ 21 (TBN),
KTVW-DT 33 (Univision), KTAZ 39 (Telemundo), KDPH 48 (Daystar), and KPPX-TV 51 (ION). KTVK 3 (3TV) and KAZT 7 (AZ-TV) are independent television stations operating in the metro area.
Many major feature films and television programs have been filmed in the city. The radio airwaves in Phoenix cater to a wide variety of musical and talk radio interests.
Infrastructure.
Transportation.
Air.
Phoenix is served by Phoenix Sky Harbor International Airport (IATA: PHX, ICAO: KPHX), one of the ten busiest airports in the United States, serving over 110,000 people on over 1000 flights per day. The airport is centrally located in the metro area near several major freeway interchanges east of downtown Phoenix. The airport serves more than 100 cities with non-stop flights.
Aeroméxico, Air Canada, British Airways, and WestJet are among several international carriers as well as American carrier US Airways (which maintains a hub at the airport) providing flights to destinations such as Canada, Costa Rica, Mexico, and London.
The Phoenix-Mesa Gateway Airport (IATA: IWA, ICAO: KIWA) in neighboring Mesa also serves the area's commercial air traffic. It was converted from Williams Air Force Base, which closed in 1993. The airport has recently received substantial commercial service with Allegiant Air opening a hub operation at the airport with non-stop service to over a dozen destinations, as well as Frontier Airlines and Spirit Air also operating out of the airport.
Smaller airports that primarily handle private and corporate jets include Phoenix Deer Valley Airport, located in the Deer Valley district of north Phoenix, and Scottsdale Airport, located just east of the Phoenix/Scottsdale border. There are also other municipal airports including Glendale Municipal Airport, Falcon Field Airport in Mesa, and Phoenix Goodyear Airport.
Rail and bus.
Amtrak served Phoenix Union Station until 1996 when the Union Pacific Railroad (UP) threatened to abandon the route between Yuma, Arizona and Phoenix. Amtrak rerouted trains to Maricopa, 30 miles south of downtown Phoenix, where passengers can board the "Texas Eagle" (Los Angeles-San Antonio-Chicago) and "Sunset Limited" (Los Angeles-New Orleans). Though UP ultimately retained the trackage, Amtrak did not return, leaving Phoenix as the most populated city in the U.S. without passenger Amtrak service, although the station is still there (see photo).
Amtrak Thruway buses connect Phoenix Sky Harbor International Airport to Flagstaff, Arizona for connection with the Los Angeles-Chicago "Southwest Chief". Phoenix is also served by Greyhound bus service, which stops at 24th Street near the airport.
Public transportation.
Valley Metro provides public transportation throughout the metropolitan area, with its trains, buses, and a ride-share program. 3.38% of workers commute by public transit. During the summer it is very difficult to wait for a bus in the heat as many of the stops have no canopies. Valley Metro's 20 mi light rail project, called METRO, through north-central Phoenix, downtown, and eastward through Tempe and Mesa, opened December 27, 2008. Future rail segments of more than 30 mi are planned to open by 2030.
Bicycle transportation.
The Maricopa Association of Governments has a bicycle advisory committee working to improve conditions for bicycling on city streets and off-road paths. Bicycling Magazine ranked Phoenix the 15th most bicycle friendly city of fifty cities in the United States with a population greater than 100,000.
Roads and freeways.
Phoenix auto traffic depends on both surface streets and freeways. Freeways fall under the auspices of the Arizona Department of Transportation (ADOT). Phoenix ranks first in the nation in the quality of its urban freeways, and the state as a whole ranks first in the nation in the quality of bridges. While being the sixth most populous city in the nation, Phoenix's freeways do not suffer from the same type of congestion seen in other large cities. In fact, in a recent study, there is not a single stretch of freeway in Phoenix ranked in the 100 worst freeways for either congestion or unreliability.
Part of the reason for this is the extensive freeway system in the city, due to the majority of that system being funded by local, rather than federal funds, through a ½ cent general sales tax measure approved by voters in 1985. Another offshoot of this local funding is that Phoenix is the largest city in the United States to have two Interstate Highways and no 3-digit interstates.
As of 2005, the metropolitan area of Phoenix contains one of the nation's largest and fastest growing freeway systems, boasting over 1,405 lane miles. The freeway system is a mix of Interstate, U.S., and State highways which include Interstate 10, Interstate 17, US 60, SR 51, Loop 101, Loop 202, SR 51, SR 143, and SR 30. There are still major additions to routes 101, 202 and 303 underway, as well as several other smaller projects around the valley. State Routes 30, 87, 85, and 74 connect Phoenix with other areas of the Valley and Arizona.
The street system in Phoenix (and some of its suburbs) is laid out in a grid system, with most roads oriented either north-south or east-west, and the zero point of the grid being the intersection of Central Avenue and Washington Street. The original plan was for the east-west streets to be named after presidents, with the north-south streets named after Indians, however the north-south streets were quickly changed to numbers, with avenues running to west of Central, and streets to its east. Major arterial streets are spaced one mile (1.6 km) apart, divided into smaller blocks approximately every 1/8 of a mile. For example, Scottsdale Road, being the 7200 block, lies 9 miles to the east of Central Avenue (72 / 8).
Freeways and state highways in Phoenix:
Utilities.
Being located in the desert, Phoenix relies on a water supply delivered to the city via a system of canals which divert water from the region's rivers and lakes, with the largest portion of the city's water coming from the Colorado River through the Central Arizona Project's canal. The city's electrical needs are served primarily by Arizona Public Service, although some customers receive their electricity from the Salt River Project (SRP). The main sources of electrical generation are nuclear, and coal power plants. Arizona is home to the Palo Verde Nuclear Power Station, the largest nuclear generating facility in the United States. SRP is also the largest water provider in Phoenix.
Health care.
In 2011 (the last year for which information is available), Phoenix had a slightly younger population than the country as a whole. While the United States had 13.3% of its population over the age of 65, Phoenix's percentage stood significantly lower, at 8.1%. Phoenix's percentage of 18.8% in the next age group, 45–64 was also a great deal lower than the national average of 26.6%. This results in 73% of Phoenix's population being 44 or younger, as compared to national percentage of 60.
In 2010 (the last year for nationally reported figures), Phoenix was at or below national levels for most reportable diseases, with the exception of both hepatitis A and B, where they were slightly over the national average (0.8 and 1.8 to 0.5 and 1.1%, respectively).
In most major categories, Phoenix had a lower incidence of death than the rest of the nation. Only deaths due to Alzheimer's (29.7 to 27.2 deaths per 100,000) and pre-natal conditions (5.3 to 3.8 deaths per 100,000) were slightly above the national average. Deaths due to HIV and liver disease were exactly at the national average of 2.5 and 10.8 respectively. However, in several major categories, Phoenix had significantly lower indices of death: deaths by cancer stood at only 57% (106) of the national average of 184.6 deaths per 100,000; deaths due to heart disease, 56.1% of the national rate of 249.8 per 100,000. Cancer and heart disease were the two top causes of death in the country.
Low weight births (7.5%) were below the national average of 8.1%, yet infant mortality (7.2%) was higher than the rest of the U.S. (6.1%). Births to teen mothers were significantly higher than the rest of the country, sitting at 12.2% as compared to 8.4% nationally.
The Phoenix metropolitan area is serviced by 56 hospitals and medical centers. Some of the top ranked are:
Other top hospitals in the area are the two Scottsdale Healthcare Centers, Chandler Regional Medical Center, and Mercy Gilbert Medical Center.
Sister cities.
With the creation of the Phoenix Sister Cities (PSC) organization in 1972, Phoenix became a member of the international Sister City movement. It would take the organization several years to become official, not filing for Articles of Incorporation until 1975, and not entering into their first Sister City agreement until 1976, with Hermosillo, Mexico. The organization's mission statement states their purpose is to "... create people-to-people relationships between the residents of Phoenix and its sister cities through commercial, educational, cultural and artistic exchange programs and events that create and sustain global, long-term, international partnerships and business opportunities for the citizens of Phoenix." Currently, Phoenix has ten sister cities, as designated by the Phoenix Sister Cities Commission and Sister Cities International, shown in the table below. Phoenix and Prague have shared a Capital Cities relationship since May 1991, which was expanded to Sister City Status in 2013.

</doc>
<doc id="49122" url="http://en.wikipedia.org/wiki?curid=49122" title="Phoenix (mythology)">
Phoenix (mythology)

In Greek mythology, a phoenix or phenix (Greek: φοῖνιξ "phoinix") is a long-lived bird that is cyclically regenerated or reborn. Associated with the sun, a phoenix obtains new life by arising from the ashes of its predecessor. The phoenix was subsequently adopted as a symbol in Early Christianity. While the phoenix typically dies in a show of flames and combustion, in most versions of the legend, there are less popular versions of the myth in which the mythical bird dies and simply decomposes before being born again. According to some legends, the phoenix could live over 1400 years before rebirth. Herodotus, Lucan, Pliny the Elder, Pope Clement I, Lactantius, Ovid, and Isidore of Seville are among those who have contributed to the retelling and transmission of the phoenix motif.
In the historical record, the phoenix "could symbolize renewal in general as well as the sun, time, the Empire, metempsychosis, consecration, resurrection, life in the heavenly Paradise, Christ, Mary, virginity, the exceptional man, and certain aspects of Christian life".
Etymology.
The modern English noun "phoenix" derives from Middle English "phenix" (before 1150), itself from Old English "fēnix" (around 750). Old English "fēnix" was borrowed from Medieval Latin "phenix", which is derived from Classical Latin "phoenīx". The Classical Latin "phoenīx" represents Greek φοῖνιξ "phoinīx".
In ancient Greece and Rome, the bird, φοῖνιξ, was sometimes associated with the similar-sounding Phoenicia, a kingdom famous for its production of purple dye from conch shells. A late antique etymology offered by the 6th- and 7th-century CE archbishop Isidore of Seville accordingly derives the name of the phoenix from its allegedly purple-red hue. Because the costly purple dye from Phoenicia was associated with the upper classes in antiquity and, later, with royalty, in the medieval period the phoenix was considered "the royal bird".
In spite of these folk etymologies, with the deciphering of the Linear B script in the 20th century, the original Greek φοῖνιξ was decisively shown to be derived from Mycenaean Greek "po-ni-ke", itself open to a variety of interpretations..
Relation to the Egyptian Bennu.
Classical discourse on the subject of the phoenix points to a potential origin of the phoenix in Ancient Egypt. In the 19th century scholastic suspicions appeared to be confirmed by the discovery that Egyptians in Heliopolis had venerated the Bennu, a solar bird observed in some respects to be similar to the Greek phoenix. However, the Egyptian sources regarding the benu are often problematic and open to a variety of interpretations. Some of these sources may have been influenced by Greek notions of the phoenix.
Appearance.
The phoenix is sometimes pictured in ancient and medieval literature and medieval art as endowed with a nimbus, which emphasizes the bird's connection with the sun. In the oldest images of phoenixes on record these nimbuses often have seven rays, like Helios (the personified sun of Greek mythology). Pliny also describes the bird as having a crest of feathers on its head, and Ezekiel the Dramatist compared it to a rooster.
Although the phoenix was generally believed to be colorful and vibrant, there is no clear consensus about its coloration. Tacitus claims that its color made it stand out from all other birds. Some thought that the bird had peacock-like coloring, and Herodotus' claim of red and yellow is popular in many versions of the story on record. Ezekiel the Dramatist declared that the phoenix had red legs and striking yellow eyes, but Lactantius said that its eyes were blue like sapphires and that its legs were covered in scales of yellow-gold with rose-colored talons.
In terms of size, R. Van den Broek, Herodotus, Pliny, Solinus, and Philostratus describe the phoenix as similar in size to an eagle, but Lactantius and Ezekiel the Dramatist both claim that the phoenix was larger, with Lactantius declaring that it was even larger than an ostrich.
Analogues.
Scholars have observed analogues to the phoenix in a variety of cultures. These analogues include the Arabic anqa, the Hindu garuda and gandaberunda, the Russian firebird, the Persian Simorgh, Georgian paskunji, the Turkish Zümrüdü Anka, the Tibetan Me byi karmo, the Chinese fenghuang, and the Japanese hō-ō.

</doc>
<doc id="49123" url="http://en.wikipedia.org/wiki?curid=49123" title="Phoenix (constellation)">
Phoenix (constellation)

Phoenix is a minor constellation in the southern sky. Named after the mythical phoenix, it was first depicted on a celestial atlas by Johann Bayer in his 1603 "Uranometria". The French explorer and astronomer Nicolas Louis de Lacaille charted the brighter stars and gave their Bayer designations in 1756. The constellation stretches from roughly −39° to −57° declination, and from 23.5h to 2.5h of right ascension. The constellations Phoenix, Grus, Pavo and Tucana, are known as the Southern Birds.
The brightest star, Alpha Phoenicis, is named Ankaa, an Arabic word meaning 'the Phoenix'. It is an orange giant of apparent magnitude 2.4. Next is Beta Phoenicis, actually a binary system composed of two yellow giants with a combined apparent magnitude of 3.3. Nu Phoenicis has a dust disk, while the constellation has ten star systems with known planets and the recently discovered galaxy clusters El Gordo and the Phoenix Cluster—located 7.2 and 5.7 billion light years away respectively, two of the largest objects in the visible universe. Phoenix is the radiant of two annual meteor showers: the Phoenicids in December, and the July Phoenicids.
History.
Phoenix was the largest of the twelve constellations established by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman. It first appeared on a 35-cm diameter celestial globe published in 1597 (or 1598) in Amsterdam by Plancius with Jodocus Hondius. The first depiction of this constellation in a celestial atlas was in Johann Bayer's "Uranometria" of 1603. De Houtman included it in his southern star catalog the same year under the Dutch name "Den voghel Fenicx", "The Bird Phoenix", symbolising the phoenix of classical mythology. One name of the brightest star Alpha Phoenicis—Ankaa—is derived from the Arabic العنقاء "al-‘anqā’" "the phoenix", and was coined sometime after 1800 in relation to the constellation.
Celestial historian Richard Allen noted that unlike the other constellations introduced by Plancius and La Caille, Phoenix has actual precedent in ancient astronomy, as the Arabs saw this formation as representing young ostriches, "Al Ri'āl", or as a griffin or eagle. In addition, the same group of stars was sometimes imagined by the Arabs as a boat, "Al Zaurak", on the nearby river Eridanus. He observed, "the introduction of a Phoenix into modern astronomy was, in a measure, by adoption rather than by invention."
The Chinese incorporated Phoenix's brightest star, Ankaa (Alpha Phoenicis), and stars from the adjacent constellation Sculptor to depict "Bakui", a net for catching birds. Phoenix and the neighbouring constellation of Grus together were seen by Julius Schiller as portraying Aaron the High Priest. These two constellations, along with nearby Pavo and Tucana, are called the Southern Birds.
Characteristics.
Phoenix is a small constellation bordered by Fornax and Sculptor to the north, Grus to the west, Tucana to the south, touching on the corner of Hydrus to the south, and Eridanus to the east and southeast. The bright star Achernar is nearby. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Phe'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 10 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between 23h 26.5m and 02h 25.0m, while the declination coordinates are between −39.31° and −57.84°. This means it remains below the horizon to anyone living north of the 40th parallel in the Northern Hemisphere, and remains low in the sky for anyone living north of the equator. It is most visible from locations such as Australia and South Africa during late Southern Hemisphere spring. Most of the constellation lies within, and can be located by, forming a triangle of the bright stars Achernar, Fomalhaut and Beta Ceti—Ankaa lies roughly in the centre of this.
Notable features.
Stars.
A curved line of stars comprising Alpha, Kappa, Mu, Beta, Nu and Gamma Phoenicis was seen as a boat by the ancient Arabs. French explorer and astronomer Nicolas Louis de Lacaille charted and designated 27 stars with the Bayer designations Alpha through to Omega in 1756. Of these, he labelled two stars close together Lambda, and assigned Omicron, Psi and Omega to three stars, which subsequent astronomers such as Benjamin Gould felt were too dim to warrant their letters. A different star was subsequently labelled Psi Phoenicis, while the other two designations fell out of use.
Ankaa is the brightest star in the constellation. It is an orange giant of apparent visual magnitude 2.37 and spectral type K0.5IIIb, 77 light years distant from Earth and orbited by a secondary object about which little is known. Lying close by Ankaa is Kappa Phoenicis, a main sequence star of spectral type A5IVn and apparent magnitude 3.90. Located centrally in the asterism, Beta Phoenicis is the second brightest star in the constellation and another binary star. Together the stars, both yellow giants of spectral type G8, shine with an apparent magnitude of 3.31, though the components are of individual apparent magnitudes of 4.0 and 4.1 and orbit each other every 168 years. Zeta Phoenicis is an Algol-type eclipsing binary, with an apparent magnitude fluctuating between 3.9 and 4.4 with a period of around 1.7 days (40 hours); its dimming results from the component two blue-white B-type stars, which orbit and block out each other from Earth. The two stars are 0.05 AU from each other, while a third star is around 600 AU away from the pair, and has an orbital period exceeding 5000 years. The system is around 300 light years distant. In 1976, researchers Clausen, Gyldenkerne, and Grønbech calculated that a nearby 8th magnitude star is a fourth member of the system.
Gamma Phoenicis is a red giant of spectral type M0IIIa and varies between magnitudes 3.39 and 3.49. It lies 235 light years away. Psi Phoenicis is another red giant, this time of spectral type M4III, and has an apparent magnitude that ranges between 4.3 and 4.5 over a period of around 30 days. Lying 340 light years away, it has around 85 times the diameter, but only 85% of the mass, of our sun. W Phoenicis is a Mira variable, ranging from magnitude 8.1 to 14.4 over 333.95 days. A red giant, its spectrum ranges between M5e and M6e. Located 6.5 degrees west of Ankaa is SX Phoenicis, a variable star which ranges from magnitude 7.1 to 7.5 over a period of a mere 79 minutes. Its spectral type varies between A2 and F4. It gives its name to a group of stars known as SX Phoenicis variables. Rho and BD Phoenicis are Delta Scuti variables—short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. Rho is spectral type F2III, and ranges between magnitudes 5.20 and 5.26 over a period of 2.85 hours. BD is of spectral type A1V, and ranges between magnitudes 5.90 and 5.94.
Nu Phoenicis is a yellow-white main sequence star of spectral type F9V and magnitude 4.96. Lying some 49 light years distant, it is around 1.2 times as massive as our sun, and likely to be surrounded by a disk of dust. It is the closest star in the constellation that is visible with the unaided eye. Gliese 915 is a white dwarf only 26 light years away. It is of magnitude 13.05, too faint to be seen with the naked eye. White dwarfs are extremely dense stars compacted into a volume the size of the Earth. With around 85% of the mass of the Sun, Gliese 915 has a surface gravity of 108.39 ± 0.01 (2.45 · 108) cm·s−2, or approximately 250,000 of Earth's.
Ten stars have been found to have planets to date, and four planetary systems have been discovered with the SuperWASP project. HD 142 is a yellow giant that has an apparent magnitude of 5.7, and has a planet (HD 142 b)1.36 times the mass of Jupiter which orbits every 328 days. HD 2039 is a yellow subgiant with an apparent magnitude of 9.0 around 330 light years away which has a planet (HD 2039 b) triple the mass of Jupiter. WASP-18 is a star of magnitude 9.29 which was discovered to have a hot Jupiter-like planet (WASP-18b) taking less than a day to orbit the star. The planet is suspected to be causing WASP-18 to appear older than it really is. WASP-4 and WASP-5 are solar-type yellow stars around 1000 light years distant and of 13th magnitude, each with a single planet larger than Jupiter. WASP-29 is an orange dwarf of spectral type K4V and visual magnitude 11.3, which has a planetary companion of similar size and mass to Saturn. The planet completes an orbit every 3.9 days.
WISE J003231.09-494651.4 and WISE J001505.87-461517.6 are two brown dwarfs discovered by the Wide-field Infrared Survey Explorer, and are 63 and 49 light years away respectively. Initially hypothesised before they were belatedly discovered, brown dwarfs are objects more massive than planets, but which are of insufficient mass for hydrogen fusion characteristic of stars to occur. Many are being found by sky surveys.
Phoenix contains HE0107-5240, possibly one of the oldest stars yet discovered. It has around 1/200,000 the metallicity that the Sun has and hence must have formed very early in the history of the universe. With a visual magnitude of 15.17, it is around 10,000 times dimmer than the faintest stars visible to the naked eye and is 36000 light years distant.
Deep-sky objects.
The constellation does not lie on the galactic plane of the Milky Way, and there are no prominent star clusters. NGC 625 is a dwarf irregular galaxy of apparent magnitude 11.0 and lying some 12.7 million light years distant. Only 24000 light years in diameter, it is an outlying member of the Sculptor Group. NGC 625 is thought to have been involved in a collision and is experiencing a burst of active star formation. NGC 37 is a lenticular galaxy of apparent magnitude 14.66. It is approximately 42 kiloparsecs (137,000 light-years) in diameter and about 12.9 billion years old. Robert's Quartet (composed of the irregular galaxy NGC 87, and three spiral galaxies NGC 88, NGC 89 and NGC 92) is a group of four galaxies located around 160 million light-years away which are in the process of colliding and merging. They are within a circle of radius of 1.6 arcmin, corresponding to about 75,000 light-years. Located in the galaxy ESO 243-49 is HLX-1, an intermediate-mass black hole—the first one of its kind identified. It is thought to be a remnant of a dwarf galaxy that was absorbed in a collision with ESO 243-49. Before its discovery, this class of black hole was only hypothesized.
Lying within the bounds of the constellation is the gigantic Phoenix cluster, which is around 7.3 million light years wide and 5.7 billion light years away, making it one of the most massive galaxy clusters. It was first discovered in 2010, and the central galaxy is producing an estimated 740 new stars a year. Larger still is El Gordo, or officially ACT-CL J0102-4915, whose discovery was announced in 2012. Located around 7.2 billion light years away, it is composed of two subclusters in the process of colliding, resulting in the spewing out of hot gas, seen in X-rays and infrared images.
Meteor showers.
Phoenix is the radiant of two annual meteor showers. The Phoenicids, also known as the December Phoenicids, were first observed on 3 December 1887. The shower was particularly intense in December 1956, and is thought related to the breakup of the short-period comet 289P/Blanpain. It peaks around 4–5 December, though is not seen every year. A very minor meteor shower peaks around July 14 with around one meteor an hour, though meteors can be seen anytime from July 3 to 18; this shower is referred to as the July Phoenicids.
References.
 Media related to at Wikimedia Commons
Coordinates: 

</doc>
<doc id="49124" url="http://en.wikipedia.org/wiki?curid=49124" title="Phoenix (son of Amyntor)">
Phoenix (son of Amyntor)

In Greek mythology, Phoenix (Greek: Φοῖνιξ "Phoinix", "gen".: Φοίνικος), son of Amyntor and Cleobule, is one of the Myrmidons led by Achilles in the Trojan War. Phoenix's warfaring identity is a charioteer. 
Of Phoenix's life before the Trojan War, it is related that he seduced his father's concubine at the instigation of his mother. Having heard about this, Amyntor punished his son by cursing him with infertility. Phoenix fled to Peleus, who in his turn took him to Chiron; the latter restored Phoenix's sight, whereupon Peleus made Phoenix king of the Dolopes. He participated in the hunt for the Calydonian Boar. 
In Homer's "Iliad", Phoenix, along with Odysseus and Ajax, urges Achilles to re-enter battle. He gives the most passionate and emotional speech of the three, as evidenced by his crying. Phoenix deeply cares about Achilles, whom he had helped raise as a child: (""So you, Achilles- great godlike Achilles I made you my son, I tried, so someday "you" might fight disaster off my back. But now, Achilles, beat down your mounting fury! It's wrong to have such an iron, ruthless heart".") It is possible that his speech was a later addition to the epic, as Achilles continually uses a special dual verb form in speaking with his guests, rather than a more appropriate plural form. However, it has been suggested that Achilles speaks only to Phoenix and Ajax, ignoring Odysseus, to whose guile he bears a considerable dislike. (""I hate like the gates of Hades the man who says one thing and holds another in his heart".") 
Phoenix also makes a cameo in Virgil's "Aeneid". As Aeneas is searching his fallen Troy for his wife Creusa, he glimpses Phoenix and Odysseus guarding their loot in Priam's palace.
Phoenix was said to have died on his way back from Troy and to have been buried by Neoptolemus either in Eion, Macedonia, or in Trachis, Thessaly.

</doc>
<doc id="49125" url="http://en.wikipedia.org/wiki?curid=49125" title="Phoenix Force (comics)">
Phoenix Force (comics)

 
The Phoenix Force is an entity in the Marvel Comics fictional universe which has bonded with other characters, who often used the alias Phoenix.
The Phoenix Force is famous for its central role in "The Dark Phoenix Saga" storyline, and is frequently linked to Jean Grey. In 2009, Jean Grey as the Dark Phoenix was ranked as IGN's 9th Greatest Comic Book Villain of All Time. "Wizard" list of Top 100 villains ranked the Dark Phoenix as 38th.
Publication history.
The Phoenix first appeared in "Uncanny X-Men" #101 (October 1976) in the guise of Jean Grey, and was created by Chris Claremont and Dave Cockrum.
Fictional character biography.
The Phoenix Force is an immortal and mutable manifestation of the prime universal force of life and passion. Born of the void between states of being, the Phoenix Force is a child of the universe. It is the nexus of all psionic energy which does, has, and ever will exist in all realities of the omniverse, the Guardian of Creation, and of the dangerously powerful M'Kraan Crystal.
The Phoenix is among the most feared beings in all of existence — having the power to cut and re-grow any part of the universe, as well as destroy it entirely, which is part of the Phoenix' purpose: "The Judgment of the Phoenix", to burn away the obsolete. The Phoenix Force is described as being "the embodiment of the very passion of Creation – the spark that gave life to the Universe, the flame that will ultimately consume it."
During its time as a sentient and nameless entity, it traveled the cosmos just like other cosmic beings. At first, the Phoenix Force was a formless mass of energy, but thousands of years ago, it came to Earth, and met a magician named Feron (who worshipped the legendary Phoenix), whose daydream-like visions prompted the Phoenix to adopt the firebird form it has today. He asked the Phoenix to help him by lending its energy to project a stone pillar (which resembled a lighthouse) across the multiverse. The pillar became the lighthouse base for the British super-team Excalibur (a team its future host Rachel Summers herself would join). Afterwards, Feron was attacked by Necrom in an attempt to steal the power of the Phoenix. Feron, strengthened by the Phoenix Force, was able to fight back but Necrom was able to steal a fraction of the Phoenix Force's essence forcing it to flee back to space in agonized confusion. The Phoenix Force returned to Earth when it felt the mind of a human transcend the physical realm, a mind that resonated with the Phoenix Force's energy. A young Jean Grey had telepathically linked her mind to her dying friend, Annie Richards, to keep Annie's soul from moving to the afterlife. In doing so, Jean's mind was being dragged along to the "other side" with Annie. Phoenix lent its energy to break the connection, and kept close watch on young Jean, as it felt a kinship with the young mutant. Years later when Jean was dying on a space shuttle, her mind called out for help and the Phoenix Force answered and saved her, transforming Jean into the Phoenix.
The Phoenix remained with the X-Men for only a short time. She prevented the complete destruction of the universe by repairing the damaged energy matrix at the core of the M'Kraan Crystal. During a skirmish with the X-Men's first and most deadly foe, Magneto, Phoenix and Beast were separated from the other X-Men, with each group believing the other to have perished. Phoenix went on a European vacation to gather herself in this new, lonely world. In Greece, Phoenix met a young and handsome man named Nikos, who is later revealed to be Mastermind, a mutant with the powers of illusion. He began to plant the seeds of dissent within her fragile psyche by comparing her to a god and insisting she can do whatever she wants. She would later encounter him again in Scotland, under the guise of Jason Wyngarde, a handsome 18th Century loyalist, believing him to be both the work of the reality-warping mutant Proteus and the lover of one of her ancestors.
The Rise of Dark Phoenix.
After an encounter with the Hellfire Club and manipulation by Mastermind and the White Queen, the Phoenix was transformed into their Black Queen. She broke free of Mastermind's control, but had been transformed into Dark Phoenix. She battled the X-Men and fled to the stars, devoured the energies of the D'Bari star system to satisfy her "hunger" as Dark Phoenix, annihilating the five billion inhabitants of its fourth planet, and destroyed a nearby Shi'ar observatory vessel which opened fire on her before returning to Earth. There, she was defeated in psionic combat by Professor X, and regained control. The group was then teleported to space by the Shi'ar and given a trial by combat. Just as victory seemed certain for the Imperial Guard, she once again became Dark Phoenix, and ultimately committed apparent suicide on Earth's moon before the eyes of a horrified Cyclops.
As originally written, the Jean Grey incarnation of the Phoenix was Jean herself, having attained her ultimate potential as a psi, becoming a being of pure energy and reforming herself as Phoenix, only to become slowly corrupted by the manipulation of such foes as Mastermind and Emma Frost; unable to adapt to her enormous power, Jean was driven mad.
In order to return Jean to the fold several years later, this storyline was retconned to reveal the existence of the cosmic Phoenix Force entity, which had created a duplicate body of Jean, believed itself to be Jean and acted in her place while the real Jean lay in a healing cocoon at the bottom of Jamaica Bay, where the Avengers and Fantastic Four would later discover her. This allowed Jean to be revived as a member of X-Factor. The extent to which the duplicate and Jean are separate entities depends on who is writing the character(s) at the time, some instances portraying them as inherently separate, while others demonstrate a shared consciousness.
Part of the Phoenix Force encountered a manifestation of Death after committing suicide and then returned itself to Jean in the cocoon. Horrified by what it had done, Jean rejected it and it went on to join with Jean's clone, Madelyne Pryor. This portion of the Phoenix remained with Madelyne until she committed suicide while fighting Jean Grey and then rejoined Jean's consciousness.
Rachel Summers.
Another possessor of the Phoenix Force is Rachel Summers, Scott Summers and Jean Grey's daughter from the Days of Future Past alternate future. The Phoenix Force bonded with Rachel and Rachel became the next avatar of the Phoenix Force. Rachel is one of the longest reigning avatars of the Phoenix. Rachel never became Dark Phoenix. Rachel has been referred to as "The One True Phoenix".
 During an encounter with Galactus, Rachel Summers—at the time completely overtaken by the Phoenix Force—battled Galactus in an effort to save a planet he was preparing to devour. The Phoenix Force disrupted Galactus' feeding process and thus was easily able to defeat the depleted world devourer in battle. Galactus accused the Phoenix Force of hypocrisy and revealed to it that its existence in a corporeal state was sustained by robbing energy used to birth future generations. Realizing this to be true, the Phoenix vowed to return to its prior existence of "touching all that is" while allowing an echo of its power to remain with Rachel's now-dominant consciousness.
Into the Future.
When Rachel's body finally healed, it was just as the Phoenix discovered that its actions on the physical plane were causing potential life to be used up, so it woke her and told her that her powers would be somewhat lessened, as the Phoenix was returning to its natural state. Returning to Earth with all her memories, Rachel finally managed to get back to the future she had come from. While she could not change her past, she and her teammates were able to change the directives of all the Sentinels of the era to preserve all life, thereby ending the genocide that had prevailed for years. On the way back to our time, however, Captain Britain was lost in the timestream and Rachel was eventually forced to switch places with him, because she really did not belong in our time, anyway. She emerged from the timestream about 1900 years in the future and formed the Clan Askani, which was responsible for bringing her brother Nathan to their time to fight Apocalypse. She later encounters Diamanda Nero which was Apocalypse's High Councilor and viceroy. She even wanted to overpower him, but was left powerless after shortly being bonded to the Phoenix Force.
Phoenix resurrection.
Later, as an interdimensional portal transported four villains from the 616 Marvel Universe into the Ultraverse dimension, the Phoenix Force was pulled into the Ultraverse as well and was critically damaged. Needing a human host to help heal the damage, the Phoenix Force bonded with Prime, and later with Amber Hunt. Amber was unable to control it. She attacked her friends and would have destroyed the planet, if not for the arrival of the X-Men and new Ultra hero Foxfire, who after a long battle were able to separate the Phoenix from Amber and send the cosmic entity back to the 616 universe.
Jean Grey would begin to manifest Phoenix firebirds and tap into its cosmic reserves shortly before her death at the hands of Xorn.
Endsong and Warsong.
The Phoenix Force would return to Earth during the mini-series "", where it resurrected Jean Grey from her grave. It is not long before she remembers what she has come for — Scott Summers (a.k.a. Cyclops). She needs to feed from the energy from his optic blasts, and confused by Jean's emotions thinks she's in love with Scott. She realizes Scott is in love with Emma Frost (former White Queen of the Hellfire Club and headmistress of the Xavier Institute for Higher Learning). Through a number of incidents, including Jean having Wolverine kill her a number of times, Jean trapping herself in a glacier, the Phoenix Force jumping into Emma Frost, and parts of the X-Men being trapped in a Shi'ar-generated event horizon, Jean Grey managed to assert herself and gain control of the Phoenix Force, with emotional support of all the X-Men. Jean then declares that she and the Phoenix force are truly one entity now, have transcended into the White Phoenix of the Crown. This is signified by a new white and gold costume.
As a result of a Shi'ar attack on the Phoenix Force, the entity is currently in an incomplete state and Jean must now search out the remaining parts of the Phoenix Force. The consequences of this were partially addressed in "" in which a small part of the Phoenix Force joined with the Stepford Cuckoos. After nearly losing control to the Phoenix power, the Stepford Cuckoos developed a secondary mutation, in which their hearts turned to diamond and they were able to imprison the piece of the Phoenix Force.
End of Greys.
With the failed attack on the Phoenix Force which ended with Jean Grey escaping their suicide bomb attack and returned to the White Hot Room to restore herself, the Shi'ar still wanted to permanently prevent the ascension of the Phoenix Force. In hopes of eliminating the possibility of a new Omega-level psionic mutant becoming a host for the Phoenix Force, the Shi'ar sent to Earth a commando unit with the purpose to wipe out the Grey genome and kill Quentin Quire. These Death Comandos arrived at Rachel's family reunion site and killed all the members of the Grey family besides Rachel and Cable who weren't present. Afterward, at the graves of the Grey family, Rachel vowed a terrible vengeance on the Shi'ar and was quoted as saying: "I'm not my mom. I'm not the Phoenix. I'm my own woman. And by the time I'm done... they'll wish I WERE the Phoenix."
Kingbreaker.
During the last issue of "Kingbreaker", the Phoenix mysteriously abandons Rachel and Korvus during battle with Vulcan's new guard, leaving them both without its power. As it leaves Rachel mutters "Please, not now… Mom." implying that Jean is calling back the missing pieces of the Phoenix Force, and perhaps planning another resurrection. Rachel later says that it was almost like the Phoenix was never with her for she "Can't feel it... I can't hear it...It's like it was never there."
The Sisterhood.
Roughly around the same time, back on Earth in San Francisco the Red Queen and her Sisterhood attack the X-Men; first trapping a sleeping Emma in a psychic barricade by Lady Mastermind. Inside what appears to be the White Hot Room, or possibly just Emma's own mind, a woman resembling Jean Grey appears to Emma and helps her break free of Regan's influence with what appears to be a miniature version of the Phoenix energy raptor, thus letting her assist Logan, who has been robbed of a lock of Jean's hair that was in his possession. Madelyne uses the hair sample to locate Jean's gravesite, and then attempts to repeat a resurrection ritual with her corpse, but Cyclops had ordered Domino to substitute the body for someone else's and it somehow causes Madelyne to either discorporate or become absorbed into the fake.
Utopia.
During a conflict with several Predators X (genetically engineered mutant hunters), the Stepford Cuckoos are overwhelmed and knocked unconscious as the fragment of the Phoenix they captured forcefully escape from the girls' diamond hearts, much to the horror of Cyclops and the rest of the X-Men.
Second Coming.
A promotional image for the event of was released depicting two versions of Hope Summers, the so-called Mutant Messiah — one angelic, emphasizing her role as a savior, the other as evil and surrounded by the Phoenix Force, depicting her as a destroyer.
During the final confrontation with Bastion, Hope turns into what appears to be the Phoenix and blasts Bastion as Wolverine, Colossus, and Emma watch shocked. Bastion however manages to grasp Hope's neck, and states that despite his original programming, he will take great pleasure in killing her. Cyclops blasts his arm, and Wolverine jumps on him, as he tries killing him "for Kurt". Hope touches the ground again, saying she's ready now, going full Phoenix Force, blasts Bastion and the dome all at once.
Later at a celebratory bonfire, Emma notices the flames around Hope take the shape of the Phoenix and Emma recalls the Sisterhood attack where Jean freed her from Lady Mastermind's illusion.
Generation Hope.
While fighting the fifth so-called "Light", who was out of control, Hope attempts to take some of his powers only to exhaust herself and collapses. She is then contacted by the Phoenix Force who refers Hope as her "child" and that the other lights needed her. Hope then regains consciousness and went back to face Kenji Uedo. Due to Phoenix' words, it's implied that the five lights are all connected to the Phoenix Force.
Age of X.
The Age of X reality was created when Legion's mind reacted to Doctor Nemesis' attempt to restore its sanity. A new persona, with new powers, was born creating the new reality in order to protect Legion's many personalities. This new manifestation of Legion's power took the appearance of Moira MacTaggert, in order to confuse Xavier and protect Legion. In this reality Jean Grey's Phoenix Force ability manifested causing a tremendous amount of destruction and death in Albany. Though she was presumed deceased when the Air Force bombed the area, a new phoenix shape emerged from the rubble. Under the name of Revenant, it is not known whether she is Jean Grey (presumably dead after the Albany incident) or a totally new incarnation. She joins Magneto and becomes part of the Force Warriors. She also states she got lost on her way home and she does not look like herself.
When the truth about the Age of X was finally revealed, Legion apologizes to everyone before rewriting the universe and putting everything back the way it was. However, Revenant who should not even exist is brought back to Utopia also. Later, as all mutants begin to regain their true memories, Revenant is revealed to be actually the mind of Rachel Summers given human form.
Fear Itself.
During the "Fear Itself" storyline, the Phoenix Force appeared once again to Emma Frost (who's in Utopia's infirmary after she was overwhelmed by the cosmic powers of Juggernaut who had become the Worthy known as Kuurth: Breaker of Stone). The Phoenix soon began mocking Emma that Scott Summers would never love her as he loved Jean Grey revealing also that Jean had been reborn and that Emma Frost already knows it. This Phoenix however seems to be a side effect of when Emma invaded Juggernaut's mind and began feeding into Emma's fears. As the Phoenix manages to convince Emma that Hope is Jean reincarnated, it tells Emma that she knows what to do. Emma, in a trance like state, takes her pillow and heads towards Hope, about to smother her to death, and she would have killed Hope if not for Namor's intervention.
Avengers vs. X-Men.
During the "Avengers vs. X-Men" storyline, the event has the Phoenix Force returning to Earth, presumably to reclaim Hope Summers, the "Mutant Messiah", which led to a confrontation between the Avengers and the X-Men on how to deal with its arrival, with the Avengers anticipating the destruction that the Phoenix could bring while Cyclops hopes to use the Phoenix Force to restart the mutant population. It has also been revealed that the Phoenix Force was once wielded by a young red-headed girl named Fongji, who became an heir to the legacy of the Iron Fist.
As the Phoenix Force nears Earth, the Avengers fight the X-Men on the Blue Area of the Moon, while Iron Man and Giant-Man prepare a disruptor weapon to kill the Phoenix Force. Iron Man pilots the weapon against the Phoenix Force, but when he uses it on the Phoenix Force, instead of killing it the blast forcefully alters the entity and divided it in five fragments which bond with Cyclops, Emma Frost, Namor, Colossus and Magik. They defeat the Avengers and head back to Earth with Hope. It has since been theorized that Scarlet Witch's spell of "No more mutants" angered the Phoenix, and in order to calm the entity, a new host was needed along with five acolytes, to succeed and bringing evolution (the acolytes were actually the first five new mutants that have appeared around the globe since the decimation of the mutant population), and that was the reason the Phoenix came to Earth.
When the Avengers manage to defeat Namor with a mass assault on him during an attack on Wakanda, his portion of the Phoenix Force gets divided between the other four members of the "Phoenix Five" making it harder to defeat them. Spider-Man then baits Colossus and Magik into taking each other out by playing off their fears when fighting them in a volcano as they begin to argue about the other's recent actions, forcing their portion of the Phoenix to be divided between Emma and Cyclops. When Cyclops invades the mystical city of K'un Lun, Lei Kung defends the city on the back of the dragon Shou-Lao the Undying, revealing that the immortal dragon had defeated the Phoenix in a past incarnation. While Cyclops defeats the dragon, Hope is able to absorb its power and defeat Cyclops who then goes to seek the final portion of the Phoenix Force possessed by Emma Frost. Emma Frost had been using the Phoenix Force to control all of Utopia, read the thoughts of everyone on the planet, take vengeance on anyone who had ever harmed a mutant and dismantle all Sentinels. In a clash against the Avengers with both remaining Phoenix hosts increasingly hostile towards the other, Cyclops defeats Emma and elevates to the level of Dark Phoenix, killing Professor X in the process.
In the final issue, the X-Men and the Avengers battle the Dark Phoenix, but lose ground swiftly as the Dark Phoenix starts to burn the world. As a last resort, Captain America sends in Hope and Scarlet Witch, who together manage to take down the Dark Phoenix, as Jean Grey appears in Cyclops' mind and convinces him to let go of the Phoenix Force. The Phoenix escapes Cyclops' body and enters Hope Summers'. Together, Hope and the Scarlet Witch wish away the Phoenix Force and the damage it caused, in the process activating the X-gene that allowed the creation of new mutants around the world.
Even with the Phoenix gone, its effects remain in various ways, with the former Phoenix Five - as well as Magneto, due to his prolonged time on Utopia - suffering from various problems with their powers; Cyclops and Magneto's control over their abilities has regressed back to the level of control they possessed in their first appearances, Emma Frost retains her diamond form but only has erratic control over her telepathy, and Colossus' body fluctuates between his organic and metal parts rather than completely transforming into one or the other, although Magik's powers appear to have been increased to the point where she can channel the power of Limbo on her own. However, it was later revealed that the power disruptions experienced by Cyclops, Emma, Colossus, Magik and Magneto were actually the result of nano-sentinels unleashed on them by Dark Beast.
Later while spending some time in deep space, Iron Man helps to defend an ancient planet from space pirates. His selfless acts of heroism win him the heart of a beautiful princess, but he is later confronted by robotic police officers looking to arrest Stark for deicide. Before he is later able to escape the planet, Stark realizes that this planet's people, known as the Voldi, worship the Phoenix Force and his hand in its disappearance has angered the population.
Time Runs Out and Secret Wars.
At some point during the "Time Runs Out" storyline, Cyclops acquired a Phoenix Egg which he holds in reserve, hoping to use it to end the Incursions.
During the "Secret Wars" storyline, Cyclops is standing on top of the Phoenix Egg during the incursion between Earth-616 and Earth-1610. Cyclops eventually uses the Phoenix Egg to become one with the Phoenix Force again and uses his powers to decimate the Children of Tomorrow.
List of hosts.
Other characters were only possessed by the Phoenix Force during out-of-continuity tales. This includes Franklin Richards, Nightcrawler, Storm, and Gabriel Summers, in separate "What If..." stories, as well as Cyclops in the X-Men / Teen Titans inter-company crossover. Quentin Quire was revealed to be a host in the "Here Comes Tomorrow" storyline in the visions of Deathlok, and in the timeline of Nocturne, the Phoenix Force possessed Colossus' soulless body and reshaped it into female form.
Powers and abilities.
The Phoenix Force has the ability to manipulate cosmic energies and to tap into the life-force reserved for future generations, thus denying them existence. It can wield this energy to project beams of immense destructive force. It can transmigrate throughout time and space by folding its energy back into itself, causing it to collapse akin to a black hole and then reform itself upon reaching its destination. It can directly absorb energy such as Cyclops' optic blasts or even the entire energy of a sun. It is also capable of absorbing the energy and life-force from a foe. As it is the nexus of all psionic energy, it has mental abilities of cosmic scope, including telepathy and telekinesis.
The extent of the Phoenix Force's god-like abilities has not been fully clarified. Jean Grey as The White Phoenix of the Crown was able to change the future of a universe by reaching back in time and pushing her husband Cyclops to move on with his life.
Another major display of the power of the Phoenix was during the Secret Wars II, when the omnipotent Beyonder wanted to destroy all life. Rachel Summers, who served as the avatar of the Phoenix Force at that time, sought out to kill the Beyonder. The Beyonder expressed both amazement and disappointment to Rachel, claiming that she denies herself her own glory when she can be so much more. With that, the Beyonder gave Rachel the full access to the power of the Phoenix as well as some of his own, Rachel was on par with Jean Grey when she became Phoenix. With such power, Rachel was able to absorb the consciousness of every sentient mortal being in the universe, and when expelling the sum total of the power back at the Beyonder, the input felt overwhelming even to his senses.
Often the Phoenix seeks out hosts who have strong inherent psionic abilities so they can withstand its power. When the Phoenix Force enters a host, a small fragment of its power is left behind when it leaves. Even a small fragment can be stronger than an inexperienced host using the Phoenix Force's powers; as seen by Rachel Summers, who had full access to the Force, but her opponent Necrom threw moons at her with only a fragment. When bonded with a host, the Phoenix Force amplifies their abilities to incalculable levels. It can manipulate matter on a sub-atomic level and transmute elements (e.g., turning wood to gold, stone to crystal, etc.). It can teleport others across space and can also open inter-dimensional portals to instantaneously access distant locales of the Universe. If an avatar of the Phoenix Force is harmed or killed, it will form an "egg" of cosmic power, incubate in the White Hot Room, and hatch out completely healed. Also, as one of the oldest cosmic beings the Phoenix Force possesses a high level of cosmic awareness and prescience.
Other versions.
31st century.
In the 31st century in the Guardians of the Galaxy comics series, ordinary human Giraud of New Haven becomes host to the Phoenix Force. As Phoenix, Giraud is a rarity for a Phoenix host; since he is an ordinary human with no "active" magic or psionic abilities — only those powers granted him directly by the Phoenix Force. However, the Phoenix Force spoke to Giraud directly, telling him that he did, in fact, have latent psi-abilities, and it was that latent psi-potential that drew it to him.
Age of Apocalypse.
In the Age of Apocalypse reality, after Jean Grey's death at the hands of Havok, nuclear bombs set to destroy America were suddenly destroyed by a bird-like display of fiery psionic power. It was Jean, awakened as the Phoenix (known as "Mutant Alpha", the legendary ultimate mutant). Sinister captured Phoenix, and brainwashed her into becoming one of his Sinister Six. He then turned Phoenix against the X-Men, displaying the personality of Dark Phoenix. Phoenix generated so much heat that even Sunfire was nearly burned to death, but Psylocke used her psychic knife to bring her to her senses. Jean used the Phoenix Force to incinerate her former "master", and became leader of the X-Men in Magneto's absence.
Amalgam Comics.
The Phoenix is combined with DC Comics character Kinetix to make Phoenetix in "Spider-Boy Team-Up" #1.
Earth X.
Though the origins and history of Phoenix of Earth-9997 match that of her Earth-616 counterpart, the following information has been revealed by both 3-D Man (Kyle Richmond) and the Watchman (X-51): The Phoenix Force was originally a citizen of the first universe that existed prior to the Big Bang that created the current universe in which Earth-9997 resides. This original universe collapsed due to the manipulations and reproduction of the Celestial race. There were a number of survivors who were referred to as "The Elders of the Universe", the Phoenix Force being one of these elders. It was later revealed that the Elders plotted to reunify the fragmented universe (and all its parallel universe and alternate history counterparts) through the Realm of the Dead and with the aid of Death.
The Phoenix Force was a part of this plot to reverse the damage the Celestials had already created. It was foreseen that eventually Death would be destroyed and that an Elder of the Universe would have to live in the Realm of the Dead in order to facilitate the collection of souls, which was at least one important step in reunifying the broken universe. As the Elders were all nearly immortal and could not die, the Phoenix Force had to bond itself to a being that would be capable of sacrificing itself for others. This being was Jean Grey of the X-Men, who during a mission in space was the lone pilot of a space shuttle traveling through a radiation storm while her comrades were in a shielded room. Unable to bond with the severely burned body of Jean Grey, it became a binary being with her, assuming her identity, personality and physical form and sealing her charred body in a cocoon to heal. When the shuttle crashed the cocoon remained at the bottom of Jamaica Bay, and the Phoenix Force took Jean's place.
Eventually the Phoenix sacrificed her life and the real Jean Grey returned. The Phoenix ended up in Death's realm, still in the guise of Jean Grey, and was one of the few beings in this realm that were aware that they were indeed dead (Citizens in the Realm of the Dead believe that they and those are around them are still alive, while others who are not present are deceased). She eventually joined up with Mar-Vel's army to battle Death and her army. During this time, Scott Summers (new Mr. S, leader of the X-Men, who was assisting Mar-Vel in the land of the living) was able to establish a mental rapport with the Phoenix and kept him informed as to the goings on in the Realm of the Dead and (later) Mar-Vel's Paradise. This soon became a great aid to Reed Richards and others who were trying to determine why the mortally wounded would no longer die following Death's death.
After the creation of Mar-Vel's Paradise, Phoenix became one of the Avenging Host, a group of former champions who were transmogrified by technology once belonging to the High Evolutionary. They were to act as guardian angels to those in Paradise and help those in the Realm of the Dead realize that they were indeed dead so that they could travel over to Paradise and live out their perfect afterlife. During her time as one of the Avenging Host, its members began to doubt Mar-Vel's intentions, believing that he was no aware that he had no idea what occurred outside Paradise, that as it grew it threatened to consume the Negative Zone. Their doubts in Mar-Vel's quest were further strengthened when they began to realize that no new dead were appearing in Death's former realm. Phoenix would relay this information to Scott Summers, prompting the heroes of Earth-9997 to seek out Jude, the Entropic Man to become the new death. When Cap, 3-D Man, Comet Man, Benny Becksley and Thanos learned that each "Paradise" created for the realms citizens were simply wish fulfillment, the Avenging Host aided in freeing each citizen from their private "heaven" and resolved to confront Mar-Vel about his intentions.
The host (and Rick Jones) were all summoned and put on trial by Mar-Vel. Confronted by Mar-Vel with Captain America, Phoenix and the rest of the host were killed by their leader when Cap refused to take Mar-Vel's power. Shortly after their death, the Kree army invaded Paradise and a large battle erupted. During the combat, Reed Richards arrived from the Negative Zone and confronted Mar-Vel himself. During their talk, Mar-Vel resurrected the Avenging Host to aid the citizens of Paradise defeat the Kree invaders. After the battles conclusion, Reed Richards being given the cosmic consciousness and Mar-Vel leaving Paradise, Phoenix's current whereabouts are unknown. It is presumable that she remains in Paradise guarding those who have chosen to remain in this realm.
Legacy of Fire.
In a reality similar to the Marvel Mangaverse, the Phoenix Force is not just an entity, but a weapon. The Phoenix Sword as it was called was guarded and wielded by the sorceress Madelyne Pyre, who inherited the sword from her mother. When Madelyne's time as wielder of the sword was nearly up, she trained little sister Jena in the arts of fighting and magic. When their reality's version of Shadow King stole the Phoenix Sword, Jena tried to get it back, and Shadow King stabbed her with the sword. But in doing so, he inadvertently passed the powers of the Phoenix Sword to the dying Jena, who became the Phoenix Force's first host. She used the powers of Phoenix to vanquish Shadow King, and is now the guardian of her dimension.
Marvel Zombies.
Phoenix appears in the "Marvel Zombies 2" mini-series. The zombie survivors of the first series, who now possess the powers of Galactus, have been joined by other "cosmic level" zombies including an unnamed Dark Phoenix who appears to be Jean Grey. She is responsible, along with the others for eating most of the sentient life in the universe. A long trip back to earth and a delaying action fought by the last human colony leads to Jean and the others regaining their sense of morality and control over their own hunger. In fighting to defend the colony, Jean is destroyed by the hunger crazed Hulk.
Ultimate Marvel.
In the Ultimate Universe, Jean Grey was placed in a mental institute after she began hearing voices and seeing visions of an omnipotent Phoenix God. After her release, she later thought she had contacted a celestial God-entity which destroyed worlds.
As a result, the Hellfire Club believed that it would be in their best interests to summon the Phoenix and merge it with Jean Grey via a ritual. With Jean acting as the Phoenix Force's human avatar, she would be worshipped in a greater world. While the ritual was successful, the Phoenix had different plans and promptly slew the Hellfire Club. In the "Ultimate X-Men": Hellfire and Brimstone arc, the Phoenix Force makes its first appearance as the entity/personality within Jean's body.
Subsequently, Jean managed to gain some control over Phoenix, though not without using dangerous amounts of its power and causing extreme destruction. In the process, she telekinetically lifted a mass of land and atomized it, destroyed a helicopter and ten men within, created a giant Phoenix Raptor, and subdued a woman named Spiral.
Charles Xavier was confronted by Lilandra Neramani, the leader of a religious group known as the Church of Shi'Ar Enlightenment, who worship a God known as the Phoenix. Lilandra claims that the Phoenix God is the force that created life itself, first creating the stars, planets, as well as everything else in the universe. Many millennia later, the Phoenix created life-forms on those planets and watched them grow and prosper. However, as time went by, the civilizations grew more advanced and sophisticated, and soon became jealous of the Phoenix. They wanted its raw, unimaginable, and limitless power for themselves. Soon after, the inhabitants of the planets waged war upon the Phoenix, trying to control it, as well as its power. They amassed a great army, bringing together a hundred civilizations. But the Phoenix fought strongly for thousands of years until the life-forms found a way to imprison it since it could not be killed. But as conventional methods could not imprison it, they created a vortex to suck in the asteroids and planets around it, creating a spherical cage, but at a cost—the Phoenix would be trapped in it forever. As the millennia went by, the Phoenix Force's cage began to evolve; oceans formed, mountains rose, plants grew and life-forms began to sprout upon it. Soon those life-forms evolved into humans and the cage in which the Phoenix resided became known as Earth. The Shi'Ar Church believed that the Phoenix was the very core of the planet.
As a result of their meeting, Lilandra asks for permission to study Jean Grey to determine whether or not she truly is the embodiment of the Phoenix Force. During the examination, the Phoenix entity apparently asserts itself in an evil form but is seemingly suppressed by Professor Xavier's more experienced psychic powers and his emotional outreach to Jean Grey's normal personality.
After the suppression of what seems to be the Phoenix Force, Lilandra and Charles are informed by Gerald, Lilandra's assistant, that Jean's test has proven to be negative. Gerald also reveals that Jean Grey's parents have a connection to the Shi'Ar Church and postulates that Jean's subconscious has manifested a false Phoenix persona after being subjected to Phoenix Force stories in her youth. Feeling that she has lost her mind, Jean slips into a depressed state and begins seeing green creatures latched on to her body. However, it is then revealed to the reader that Jean has actually tested "positive" as the carrier of the Phoenix Force and Gerald has covered it up under orders from his actual superiors: the Hellfire Club.
Jean soon learned to control the powers of the Phoenix more and more. When Apocalypse prepared to kill Xavier, Jean accepts the Phoenix, creating a humanoid fiery entity whose power was able to bring down the ancient being. She alters reality completely and resets time to undo the damage done by Apocalypse and supposedly by Professor Xavier.
In "Ultimate X-Men\Fantastic Four Annual #1", a teenage Franklin Richards is shown to be host to the Phoenix, and a member of that timeline's X-Men.
"1602".
In Marvel 1602, Jean, who disguised herself as a man in this series, dies of sickness on Carlos Javier's ship. As a funeral, this version of Angel carries Jean's corpse into the sky, where 1602 Cyclops tearfully burns her to ashes with his eye beams. The fire briefly formed a shape similar to the Phoenix before vanishing.
"X-Men: No More Humans".
When Raze - the future son of Wolverine and Mystique, now trapped in the present - attempted to force the X-Men to accept his new 'status quo' by teleporting all humans off Earth and summoning other mutants from worlds where they were being oppressed, one of the mutants he summoned to be a member of his new Brotherhood was a Jean Grey who was still in her 'Dark Phoenix' state, barely under the control of her world's Mastermind. However, when she confronted the temporally-displaced Jean Grey, the younger Jean was able to appeal to her Dark Phoenix self to help them undo Raze's actions and save the displaced humans while also creating a new Earth in a pocket dimension for the refugee mutants.
"What If?".
The Phoenix has been the subject of "What If" on a number of occasions.
Crossovers.
Phoenix has appeared in the following intercompany crossovers:

</doc>
<doc id="49131" url="http://en.wikipedia.org/wiki?curid=49131" title="Marathon, Greece">
Marathon, Greece

Marathon (Demotic Greek: Μαραθώνας, "Marathónas"; Attic/ Katharevousa: Μαραθών, "Marathṓn") is a town in Greece, the site of the battle of Marathon in 490 BC, in which the heavily outnumbered Athenian army defeated the Persians. The tumulus or burial mound (Greek" Τύμβος, tymbos", tomb) of the 192 Athenian dead, also called the "Soros," which was erected near the battlefield, remains a feature of the coastal plain. The Tymbos is now marked by a marble memorial stele and surrounded by a small park.
History.
The name "Marathon" (Μαραθών) comes from the herb fennel, called "marathon" (μάραθον) or "marathos" (μάραθος) in Ancient Greek, so "Marathon" literally means "a place full of fennels".
It is believed that the town was originally named so because of an abundance of fennel plants in the area.
After Miltiades (the general of the Greek forces) defeated Darius' Persian forces, the Persians decided to sail from Marathon to Athens in order to sack the unprotected city. Miltiades ordered all his hoplite forces to march "double time" back to Athens, so that by the time Darius' troops arrived they saw the same Greek force waiting for them.
The name of the athletic long-distance endurance race, the "marathon", comes from the legend of a Greek runner who was sent from Marathon to Athens to announce that the Persians had been defeated in the Battle of Marathon.
Although the name Marathon had a positive resonance in Europe in the nineteenth century, for some time that was sullied by the Dilessi murders, which happened nearby in 1870.
In the 19th century and at the beginning of the 20th century the village was inhabited by an Arvanite (Albanian) population. Thomas Chase, an English traveler, describes his meeting with "an old Albanian" in Marathon and also says that he "accosted some Albanian children playing near a well, but they did not understand modern Greek". Another English traveller Robert Hichens writes in 1913: ‘Some clustering low houses far off under the hills form the Albanian village of Marathon'.
The sophist and magnate Herodes Atticus was born in Marathon. In 1926, the American company ULEN began construction on the Marathon Dam in a valley above Marathon, in order to ensure water supply for Athens. It was completed in 1929. About 10 km² of forested land were flooded to form Lake Marathon.
The beach of Schinias is located southeast of the town and it is a popular windsurfing spot and the Olympic Rowing Center for the 2004 Summer Olympics is also located there. At the 1896 and 2004 Summer Olympics, Marathon was the starting point of the marathon races (for both women and men in 2004). The area is susceptible to flash flooding, because of forest fires having denuded parts of the eastern slopes of Mount Penteli especially in 2006.
Municipality.
The municipality Marathon was formed at the 2011 local government reform by the merger of the following 4 former municipalities, that became municipal units:
Population.
The other settlements in the municipal unit are Agios Panteleimonas (pop. 1,591), Kato Souli (2,142), Vranas (1,082), Avra (191), Vothon (177), Ano Souli (232), and Schinias (264).

</doc>
<doc id="49132" url="http://en.wikipedia.org/wiki?curid=49132" title="Marathon">
Marathon

The marathon is a long-distance running event with an official distance of 42.195 kilometres (26 miles and 385 yards), usually run as a road race. The event was instituted in commemoration of the fabled run of the Greek soldier Pheidippides, a messenger from the Battle of Marathon to Athens.
The marathon was one of the original modern Olympic events in 1896, though the distance did not become standardized until 1921. More than 500 marathons are held throughout the world each year, with the vast majority of competitors being recreational athletes as larger marathons can have tens of thousands of participants.
History.
Origin.
The name "Marathon" comes from the legend of Pheidippides, a Greek messenger. The legend states that he was sent from the battlefield of Marathon to Athens to announce that the Persians had been defeated in the Battle of Marathon (in which he had just fought), which took place in August or September, 490 BC. It is said that he ran the entire distance without stopping and burst into the assembly, exclaiming ("nenikekamen", "we have wοn"), before collapsing and dying. The account of the run from Marathon to Athens first appears in Plutarch's "On the Glory of Athens" in the 1st century AD, which quotes from Heraclides Ponticus's lost work, giving the runner's name as either Thersipus of Erchius or Eucles. Lucian of Samosata (2nd century AD) also gives the story, but names the runner Philippides (not Pheidippides).
There is debate about the historical accuracy of this legend. The Greek historian Herodotus, the main source for the Greco-Persian Wars, mentions Pheidippides as the messenger who ran from Athens to Sparta asking for help, and then ran back, a distance of over 240 km each way. In some Herodotus manuscripts, the name of the runner between Athens and Sparta is given as Philippides. Herodotus makes no mention of a messenger sent from Marathon to Athens, and relates that the main part of the Athenian army, having fought and won the grueling battle, and fearing a naval raid by the Persian fleet against an undefended Athens, marched quickly back from the battle to Athens, arriving the same day.
In 1879, Robert Browning wrote the poem . Browning's poem, his composite story, became part of late 19th century popular culture and was accepted as a historic legend.
There are two roads out of the battlefield of Marathon towards Athens, one more mountainous towards the north whose distance is about 34.5 km, and another flatter but longer towards the south with a distance of 40.8 km. It has been argued that the ancient runner took the more difficult northern road because at the time of the battle there were still Persian soldiers in the south of the plain.
Mount Penteli stands between Marathon and Athens, which means that, if Pheidippides actually made his famous run after the battle, he had to run around the mountain, either to the north or to the south. The latter and more obvious route matches almost exactly the modern Marathon-Athens highway, which follows the lie of the land southwards from Marathon Bay and along the coast, then takes a gentle but protracted climb westwards towards the eastern approach to Athens, between the foothills of Mounts Hymettus and Penteli, and then gently downhill to Athens proper. This route, as it existed when the Olympics were revived in 1896, was approximately 40 km long, and this was the approximate distance originally used for marathon races. However, there have been suggestions that Pheidippides might have followed another route: a westward climb along the eastern and northern slopes of Mount Penteli to the pass of Dionysos, and then a straight southward downhill path to Athens. This route is considerably shorter, some 35 km, but includes a very steep initial climb of more than 5 km.
Modern Olympics marathon.
When the modern Olympics began in 1896, the initiators and organizers were looking for a great popularizing event, recalling the ancient glory of Greece. The idea of a marathon race came from Michel Bréal, who wanted the event to feature in the first modern Olympic Games in 1896 in Athens. This idea was heavily supported by Pierre de Coubertin, the founder of the modern Olympics, as well as by the Greeks. The Greeks staged a selection race for the Olympic marathon on 10 March 1896 that was won by Charilaos Vasilakos in 3 hours and 18 minutes (with the future winner of the introductory Olympic Games marathon coming in fifth). The winner of the first Olympic Marathon, on 10 April 1896 (a male-only race), was Spyridon "Spyros" Louis, a Greek water-carrier, in 2 hours 58 minutes and 50 seconds. The marathon of the 2004 Summer Olympics was run on the traditional route from Marathon to Athens, ending at Panathinaiko Stadium, the venue for the 1896 Summer Olympics. The Men's marathon was won by Stefano Baldini in 2 hours 10 minutes and 55 seconds, a record time for this route until the non-Olympics Athens Classic Marathon of 2014, when Felix Kandie lowered the course record to 2 hours 10 minutes and 37 seconds.
The women's marathon was introduced at the 1984 Summer Olympics (Los Angeles, USA) and was won by Joan Benoit of the United States with a time of 2 hours 24 minutes and 52 seconds.
Since the modern games were founded, it has become a tradition for the men's Olympic marathon to be the last event of the athletics calendar, with a finish inside the Olympic stadium, often within hours of, or even incorporated into, the closing ceremonies.
The Olympic men's record is 2:06:32, set at the 2008 Summer Olympics by Samuel Kamau Wanjiru of Kenya (average speed about 20.01 km/h). The Olympic women's record is 2:23:07, set at the 2012 Summer Olympics by Tiki Gelana of Ethiopia. The men's London 2012 Summer Olympic marathon winner was Stephen Kiprotich of Uganda (2:08:01). Per capita, the Kalenjin tribe of Rift Valley Province in Kenya have produced a highly disproportionate share of marathon and track-and-field winners.
Marathon mania.
Johnny Hayes' victory at the 1908 Summer Olympics contributed to the early growth of long-distance running and marathoning in the United States. Later that year, races around the holiday season including the Empire City Marathon held on New Year's Day 1909 in Yonkers, New York, marked the early running craze referred to as "marathon mania". Following the 1908 Olympics, the first five amateur marathons in New York City were held on days that held special meanings to ethnic communities: Thanksgiving Day, the day after Christmas, New Year's Day, Washington's Birthday, and Lincoln's Birthday.
Frank Shorter's victory in the marathon at the 1972 Summer Olympics would spur national enthusiasm for the sport more intense than that which followed Hayes' win 64 years earlier. By 2009, an estimated 467,000 runners completed a marathon within the United States. This can be compared to 143,000 in 1980. Nowadays, various marathons are held all around the world on a nearly weekly basis.
Inclusion of women.
For a long time after the Olympic marathon started, there were no long-distance races, such as the marathon, for women. Although a few women had run the marathon distance, they were not included in any official results. Marie-Louise Ledru has been credited as the first woman to race a marathon. Violet Piercy has been credited as the first woman to be officially timed in a marathon. Arlene Pieper became the first woman to officially finish a marathon in the United States when she completed the Pikes Peak Marathon in Manitou Springs, Colorado, in 1959. Katherine Switzer was the first woman to run the Boston Marathon "officially" (with a number). However, Switzer's entry, which was accepted through an "oversight" in the screening process, was in "flagrant violation of the rules", and she was treated as an interloper once the error was discovered. 
Bobbi Gibb had completed the Boston race unofficially the previous year, and was later recognized by the race organizers as the women's winner for that year, as well as 1967 and 1968.
Distance.
The length of an Olympic marathon was not precisely fixed at first, but the marathon races in the first few Olympic Games were about 40 km, roughly the distance from Marathon to Athens by the longer, flatter route. The exact length depended on the route established for each venue.
1908 Olympics.
The International Olympic Committee agreed in 1907 that the distance for the 1908 London Olympic marathon would be about 25 miles or 40 kilometres. The organisers decided on a course of 26 miles from the start at Windsor Castle to the royal entrance to the White City Stadium, followed by a lap (586 yards, 2 feet; 536 m) of the track, finishing in front of the Royal Box. The course was later altered to use a different entrance to the stadium, followed by a partial lap of 385 yards to the same finish.
The modern 42.195 km standard distance for the marathon was set by the International Amateur Athletic Federation (IAAF) in May 1921 directly from the length used at the 1908 Summer Olympics in London.
IAAF and world records.
An official IAAF marathon course is 42.195 km plus or minus 42 m. Course officials add a short course prevention factor of up to one metre per kilometre to their measurements to reduce the risk of a measuring error producing a length below the minimum distance.
For events governed by IAAF rules, it is mandatory that the route be marked so that all competitors can see the distance covered in kilometres. The rules make no mention of the use of miles. The IAAF will only recognise world records that are established at events that are run under IAAF rules. For major events, it is customary to publish competitors' timings at the midway mark and also at 5 km splits; marathon runners can be credited with world records for lesser distances recognised by the IAAF (such as 20 km, 30 km and so on) if such records are established while the runner is running a marathon, and completes the marathon course.
Marathon races.
Annually, more than 500 marathons are organized worldwide. Some of these belong to the Association of International Marathons and Distance Races (AIMS) which has grown since its foundation in 1982 to embrace over 300 member events in 83 countries and territories. The marathons of Berlin, Boston, Chicago, London, New York City and Tokyo form the World Marathon Majors series, awarding $500,000 annually to the best overall male and female performers in the series.
In 2006, the editors of Runner's World selected a "World's Top 10 Marathons", in which the Amsterdam, Honolulu, Paris, Rotterdam, and Stockholm marathons were featured along with the five original World Marathon Majors events (excluding Tokyo). Other notable large marathons include United States Marine Corps Marathon, Los Angeles, and Rome. The Boston Marathon is the world's oldest annual marathon, inspired by the success of the 1896 Olympic marathon and held every year since 1897 to celebrate Patriots Day, a holiday marking the beginning of the American Revolution, thereby purposely linking Athenian and American struggle for democracy. The oldest annual marathon in Europe is the Košice Peace Marathon, held since 1924 in Košice, Slovakia. The historic Polytechnic Marathon was discontinued in 1996. The Athens Classic Marathon traces the route of the 1896 Olympic course, starting in Marathon on the eastern coast of Attica, site of the Battle of Marathon of 490 B.C.E., and ending at the Panathenaic Stadium in Athens.
The Midnight Sun Marathon is held in Tromsø, Norway at 70 degrees north. Using unofficial and temporary courses, measured by GPS, races of marathon distance are now held at the North Pole, in Antarctica and over desert terrain. Other unusual marathons include the Great Wall Marathon on The Great Wall of China, the Big Five Marathon among the safari wildlife of South Africa, the Great Tibetan Marathon – a marathon in an atmosphere of Tibetan Buddhism at an altitude of 3500 m, and the Polar Circle Marathon on the permanent ice cap of Greenland.
The Intercontinental Istanbul Eurasia Marathon is the only marathon where participants run over two continents (Europe and Asia) during the course of a single event. In the Detroit Free Press Marathon, participants cross the US/Canadian border twice. The Niagara Falls International Marathon includes one international border crossing, via the Peace Bridge from Buffalo, New York, United States to Fort Erie, Ontario, Canada.
Wheelchair division.
Many marathons feature a wheelchair division. Typically, those in the wheelchair racing division start their races earlier than their running counterparts.
The first wheelchair marathon was in 1974 in Toledo, Ohio, won by Bob Hall in 2:54. Hall competed in the 1975 Boston Marathon and finished in 2:58, inaugurating the introduction of wheelchair divisions into the Boston Marathon. From 1977 the race was declared the US National Wheelchair championship. The Boston Marathon awards $10,000 to the winning push-rim athlete. Ernst van Dyk has won the Boston Marathon wheelchair division ten times and holds the world record at 1:18:27, set in Boston in 2004. Jean Driscoll won eight times (seven consecutively) and holds the women's world record at 1:34:22.
The New York City Marathon banned wheelchair entrants in 1977, citing safety concerns, but then voluntarily allowed Bob Hall to compete after the state Division of Human Rights ordered the marathon to show cause. The Division ruled in 1979 that the New York City Marathon and New York Road Runners club had to allow wheelchair athletes to compete, and confirmed this at appeal in 1980, but the State Supreme Court ruled in 1981 that a ban on wheelchair racers was not discriminatory as the marathon was historically a foot race. However, by 1986 14 wheelchair athletes were competing, and an official wheelchair division was added to the marathon in 2000.
Statistics.
World records and world's best.
World records were not officially recognized by the IAAF until 1 January 2004; previously, the best times for the marathon were referred to as the 'world best'. Courses must conform to IAAF standards for a record to be recognized. However, marathon routes still vary greatly in elevation, course, and surface, making exact comparisons impossible. Typically, the fastest times are set over relatively flat courses near sea level, during good weather conditions and with the assistance of pacesetters. 
The current world record time for men over the distance is 2 hours 2 minutes and 57 seconds, set in the Berlin Marathon by Dennis Kimetto of Kenya on 28 September 2014, an improvement of 26 seconds over the previous record also set in the Berlin Marathon by Wilson Kipsang, also of Kenya on 29 September 2013. The world record for women was set by Paula Radcliffe of Great Britain in the London Marathon on 13 April 2003, in 2 hours 15 minutes and 25 seconds.
All-time top ten athletes.
According to the IAAF, the following men and women are among the top ten fastest at the marathon distance.
Oldest marathoner.
Fauja Singh, 100, finished the Toronto Waterfront Marathon, becoming the first centenarian ever to officially complete that distance. Singh, a British citizen, finished the race on 16 October 2011 with a time of 8:11:05.9, making him the oldest marathoner. Because Singh could not produce a birth certificate from rural 1911 Colonial India, the place of his birth, his age could not be verified and his record was not accepted by the official governing body World Masters Athletics.
Gladys Burrill, a 92-year-old British woman and part-time resident of Hawaii, previously held the "Guinness World Records" title of oldest person to complete a marathon with her 9 hours 53 minutes performance at the 2010 Honolulu Marathon. The records of the Association of Road Racing Statisticians, at that time, however, suggested that Singh was overall the oldest marathoner, completing the 2004 London Marathon at the age of 93 years and 17 days, and that Burrill is the oldest female marathoner, completing the 2010 Honolulu Marathon at the age of 92 years and 19 days. Singh's age was also reported to be 93 by other sources.
Youngest marathoner.
Budhia Singh, a boy from Odisha, India, completed his first marathon at age three. He trained under the coach Biranchi Das, who saw potential in him. In May 2006, Budhia was temporarily banned from running by the ministers of child welfare, as his life could be at risk. His coach was also arrested for exploiting and being cruel to the child. Budhia is now at a state-run sports academy.
Participation.
In 2011, Running USA reported that there were approximately 518,000 marathon finishers in the United States, while other sources reported around 550,000 finishers.
Multiple marathons.
As marathon running has become more popular, some athletes have undertaken challenges involving running a series of marathons.
The 100 Marathon Club is intended to provide a focal point for all runners, particularly from the United Kingdom or Ireland, who have completed 100 or more races of marathon distance or longer. At least 10 of these events must be United Kingdom or Ireland Road Marathons. Club chairman Roger Biggs has run more than 700 marathons or ultras. Brian Mills completed his 800th marathon on 17 September 2011.
Steve Edwards, a member of the 100 Marathon Club, set the world record for running 500 marathons in the fastest average finish time of 3 hours 15 minutes, at the same time becoming the first man to run 500 marathons with an official time below 3 hours 30 minutes, on 11 November 2012 at Milton Keynes, England. The records took 24 years to achieve. Edwards was 49 at the time.
Over 350 individuals have completed a marathon in each state of the United States plus Washington, D.C. and some have done it as many as eight times. Beverly Paquin, a 22-year old nurse from Iowa, was the youngest woman to run a marathon in all 50 states. A few weeks later, Morgan Cummings (also 22) became the youngest woman to complete a marathon in all 50 states and DC. In 2004, Chuck Bryant of Miami, Florida, who lost his right leg below the knee, became the first amputee to finish this circuit. Bryant has completed a total of 59 marathons on his prosthesis. Twenty-seven people have run a marathon on each of the seven continents, and 31 people have run a marathon in each of the Canadian provinces. In 1980, in what was termed the Marathon of Hope, Terry Fox, who had lost a leg to cancer and so ran with one artificial leg, attained 5373 km of his proposed cross-Canada cancer fundraising run, maintaining an average of over 37 km, close to the planned marathon distance, for each of 143 consecutive days.
On 25 September 2011, Patrick Finney of Grapevine, Texas became the first person with multiple sclerosis to finish a marathon in each state of the United States. In 2004, "the disease had left him unable to walk. But unwilling to endure a life of infirmity, Finney managed to regain his ability to balance on two feet, to walk – and eventually to run – through extensive rehabilitation therapy and new medications."
In 2003 British adventurer Sir Ranulph Fiennes completed seven marathons on seven continents in seven days. He completed this feat despite suffering from a heart attack and undergoing a double heart bypass operation just four months before. This feat has since been eclipsed by Irish ultramarathon runner Richard Donovan who in 2009 completed seven marathons on seven continents in under 132 hours (five and a half days). Starting 1 February 2012 he improved on this by completing the 7 on 7 in under 120 hours or in less than five days.
On 30 November 2013, 69 year-old Larry Macon set a Guinness World Record for Most Marathons Run in a Year by Man by running 238 marathons. Larry Macon celebrated his 1,000th career marathon at the Cowtown Marathon in Ft. Worth on 24 February 2013.
Other goals are to attempt to run marathons on a series of consecutive weekends (Richard Worley on 159 weekends), or to run the most marathons during a particular year or the most in a lifetime. A pioneer in running multiple marathons was Sy Mah of Toledo, Ohio, who ran 524 before he died in 1988. As of 30 June 2007, Horst Preisler of Germany had successfully completed 1214 marathons plus 347 ultramarathons, a total of 1561 events at marathon distance or longer. Sigrid Eichner, Christian Hottas and Hans-Joachim Meyer have also all completed over 1000 marathons each. Norm Frank of the United States is credited with 945 marathons.
Christian Hottas is meanwhile the first runner who ever completed 2000 marathons. He ran his 2000th at TUI Marathon Hannover on 5 May 2013 together with a group of more than 80 friends from 11 countries, including 8 officers from the 100 Marathons Clubs U.K., North-America, Germany, Denmark, Austria and Italy.
In 2010, Stefaan Engels, a Belgian, set out to run the marathon distance every day of the year. Because of an injury he had to resort to a handbike near the end of January 2010. However, on 5 February he was fully recovered and decided to reset the counter back to zero. On 30 March he broke the existing record of Akinori Kusuda, from Japan, who completed 52 marathons in a row in 2009. As of 5 February 2011, Engels had run 365 marathon distances in as many days.
Ricardo Abad Martínez, from Spain, later ran 150 marathons in 150 consecutive days in 2009, and subsequently 500 marathons in a row, from October 2010 to February 2012.
Some runners compete to run the same marathons for the most consecutive years. For example, Johnny Kelley completed 61 Boston Marathons. Currently, the longest consecutive streak of Boston Marathon finishes—45 in a row—is held by Bennett Beach, of Bethesda, Maryland.
Running.
Most participants do not run a marathon to win. More important for most runners is their personal finish time and their placement within their specific gender and age group, though some runners just want to finish. Strategies for completing a marathon include running the whole distance and a run–walk strategy. In 2005, the average marathon time in the U.S. was 4 hours 32 minutes 8 seconds for men, 5 hours 6 minutes 8 seconds for women.
A goal many runners aim for is to break certain time barriers. For example, recreational first-timers often try to run the marathon under four hours; more competitive runners may attempt to finish under three hours. Other benchmarks are the qualifying times for major marathons. The Boston Marathon, the oldest marathon in the United States, requires a qualifying time for all non-professional runners. The New York City Marathon also requires a qualifying time for guaranteed entry, at a significantly faster pace than Boston's.
Typically, there is a maximum allowed time of about six hours after which the marathon route is closed, although some larger marathons keep the course open considerably longer (eight hours or more). Many marathons around the world have such time limits by which all runners must have crossed the finish line. Anyone slower than the limit will be picked up by a sweeper bus. In many cases the marathon organizers are required to reopen the roads to the public so that traffic can return to normal.
With the growth in popularity of marathoning, many marathons across the United States and the world have been filling to capacity faster than ever before. When the Boston Marathon opened up registration for its 2011 running, the field capacity was filled within eight hours.
Training.
The long run is an important element in marathon training. Recreational runners commonly try to reach a maximum of about 32 km in their longest weekly run and a total of about 64 km a week when training for the marathon, but wide variability exists in practice and in recommendations. More experienced marathoners may run a longer distance during the week. Greater weekly training mileages can offer greater results in terms of distance and endurance, but also carry a greater risk of training injury. Most male elite marathon runners will have weekly mileages of over 160 km.
Many training programs last a minimum of five or six months, with a gradual increase in the distance run and finally, for recovery, a period of tapering in the weeks preceding the race. For beginners wishing to merely finish a marathon, a minimum of four months of running four days a week is recommended. Many trainers recommend a weekly increase in mileage of no more than 10%. It is also often advised to maintain a consistent running program for six weeks or so before beginning a marathon training program, to allow the body to adapt to the new stresses. The marathon training program itself would suppose variation between hard and easy training, with a periodization of the general plan.
Training programs can be found at the websites of Runner's World, Hal Higdon, Jeff Galloway, and the Boston Athletic Association, and in numerous other published sources, including the websites of specific marathons.
The last long training run might be undertaken up to two weeks prior to the event. Many marathon runners also "carbo-load" (increase carbohydrate intake while holding total caloric intake constant) during the week before the marathon to allow their bodies to store more glycogen.
Glycogen and "the wall".
Carbohydrates that a person eats are converted by the liver and muscles into glycogen for storage. Glycogen burns rapidly to provide quick energy. Runners can store about 8 MJ or 2,000 kcal worth of glycogen in their bodies, enough for about 30 km/18–20 miles of running. Many runners report that running becomes noticeably more difficult at that point. When glycogen runs low, the body must then obtain energy by burning stored fat, which does not burn as readily. When this happens, the runner will experience dramatic fatigue and is said to "hit the wall". The aim of training for the marathon, according to many coaches, is to maximize the limited glycogen available so that the fatigue of the "wall" is not as dramatic. This is accomplished in part by utilizing a higher percentage of energy from burned fat even during the early phase of the race, thus conserving glycogen.
Carbohydrate-based "energy gels" are used by runners to avoid or reduce the effect of "hitting the wall", as they provide easy to digest energy during the run. Energy gels usually contain varying amounts of sodium and potassium and some also contain caffeine. They need to be consumed with a certain amount of water. Recommendations for how often to take an energy gel during the race range widely.
Alternatives to gels include various forms of concentrated sugars, and foods high in simple carbohydrates that can be digested easily. Many runners experiment with consuming energy supplements during training runs to determine what works best for them. Consumption of food while running sometimes makes the runner sick. Runners are advised not to ingest a new food or medicine just prior to or during a race. It is also important to refrain from taking any of the non-steroidal anti-inflammatory class of pain relievers (NSAIDs, e.g., aspirin, ibuprofen, naproxen), as these drugs may change the way the kidneys regulate their blood flow and may lead to serious kidney problems, especially in cases involving moderate to severe dehydration. NSAIDS block the COX-2 enzyme pathway to prevent the production of prostaglandins. These prostaglandins may act as inflammation factors throughout the body, but they also play a crucial role in maintenance of water retention. In less than 5% of the whole population that take NSAIDS, individuals may be more negatively sensitive to renal prostaglandin synthesis inhibition.
After a marathon.
Marathon participation may result in various medical, musculoskeletal, and dermatological complaints. Delayed onset muscle soreness (DOMS) is a common condition affecting runners during the first week following a marathon. Various types of mild exercise or massage have been recommended to alleviate pain secondary to DOMS. Dermatological issues frequently include "jogger's nipple", "jogger's toe", and blisters.
The immune system is reportedly suppressed for a short time. Changes to the blood chemistry may lead physicians to mistakenly diagnose heart malfunction.
After long training runs and the marathon itself, consuming carbohydrates to replace glycogen stores and protein to aid muscle recovery is commonly recommended. In addition, soaking the lower half of the body for approximately 20 minutes in cold or ice water may force blood through the leg muscles to speed recovery.
Health risks.
Marathon running has various health risks. Training and the races themselves put runners under stress. While rare, even death is a possibility during a race.
Common health risks include injury such as tendonitis, fatigue, knee or ankle sprain, extreme dehydration (electrolyte imbalance), and other conditions. Many are categorised as overuse injuries.
Cardiac health.
A study published in 1996 found that the risk of a fatal heart attack during or up to 24 hours after a marathon was approximately 1 in 50,000 over an athlete's racing career—which the authors characterised as an "extremely small" risk. The paper went on to say that since the risk was so small, cardiac screening programs for marathons were not warranted. However, this study was not an attempt to assess the overall benefit or risk to cardiac health of marathon running.
In 2006, a study of 60 non-elite marathon participants tested runners for certain proteins (see Troponin) which indicate heart damage or dysfunction after they had completed the marathon, and gave them ultrasound scans before and after the race. The study revealed that, in that sample of 60 people, runners who had done less than 56 km per week of training before the race were most likely to show some heart damage or dysfunction, while runners who had done more than 72 km per week of training beforehand showed few or no heart problems.
According to a study presented in 2010, running a marathon can result in decreased function of more than half the segments in the heart's main pumping chamber, but other parts of the heart will take over. Full recovery is reached within three months or less. The fitter the runner the less the effect.
Water consumption dangers.
Overconsumption is the most significant concern associated with water consumption during marathons. Drinking excessive amounts of fluid during a race can lead to dilution of sodium in the blood, a condition called exercise-associated hyponatremia, which may result in vomiting, seizures, coma and even death. Dr. Lewis G. Maharam, medical director for the New York City Marathon, has stated, "There are no reported cases of dehydration causing death in the history of world running, but there are plenty of cases of people dying of hyponatremia."
For example, Dr. Cynthia Lucero died at the age of 28 while participating in the 2002 Boston Marathon. It was Lucero's second marathon. At mile 22, Lucero complained of feeling "dehydrated and rubber-legged." She soon wobbled and collapsed to the ground, and was unconscious by the time the paramedics reached her. Lucero was admitted to Brigham and Women's Hospital and died two days later.
Lucero's cause of death was determined to be hyponatremic encephalopathy, a condition that causes swelling of the brain due to an imbalance of sodium in the blood known as exercise-associated hyponatremia (EAH). While EAH is sometimes referred to as "water intoxication," Lucero drank large amounts of Gatorade during the race, demonstrating that runners who consume sodium-containing sports drinks in excess of thirst can still develop EAH. Because hyponatremia is caused by excessive water retention, and not just loss of sodium, consumption of sports drinks or salty foods may not prevent hyponatremia.
Women are more prone to hyponatremia than men. A study in the "New England Journal of Medicine" found that 13% of runners completing the 2002 Boston Marathon had hyponatremia.
Fluid intake should be adjusted individually as factors such as body weight, sex, climate, pace, fitness (VO2 max), and sweat rate are just a few variables that change fluid requirements between people and races. The International Marathon Medical Directors Association (IMMDA) advises that runners drink a sports drink that includes carbohydrates and electrolytes instead of plain water and that runners should "drink to thirst" instead of feeling compelled to drink at every fluid station. Heat exposure leads to diminished thirst drive and thirst may not be a sufficient incentive to drink in many situations. The IMMDA and HSL Harpur Hill give recommendations to drink fluid in small volumes frequently at an approximate rate falling between 100 - every 15 minutes. A patient suffering hyponatremia can be given a small volume of a concentrated salt solution intravenously to raise sodium concentrations in the blood. Some runners weigh themselves before running and write the results on their bibs. If anything goes wrong, first aid workers can use the weight information to tell if the patient had consumed too much water.
Charity involvement.
Particularly for marathons, it is common to find charities associated with various races. Marathon organizers allotted their limited spacing and entry slots for charity organizations. Runners are given the option to sign up to run particular races, especially when open marathon entries are no longer available.
In some cases, marathons are organized as a fund-raiser for charity organizations (funding raised via entry fees or through sponsorships).

</doc>
<doc id="49133" url="http://en.wikipedia.org/wiki?curid=49133" title="Marathon Trilogy">
Marathon Trilogy

The Marathon Trilogy is a science fiction first-person shooter video game series from Bungie, originally released for Mac OS. The name "Marathon" is derived from the giant interstellar colony ship that provides the setting for the first game; the ship is constructed out of what used to be the Martian satellite Deimos. The three games in the series—"Marathon" (1994), "" (1995), and "Marathon Infinity" (1996)—are widely regarded as spiritual predecessors of Bungie's "Halo" series.
Gameplay.
Throughout the games the player accesses computer terminals through which he communicates with artificial intelligences, receives mission data, and gets teleported to other levels via "Jump Pads". Though contact with computers is how they are primarily utilized, they are a fundamental storytelling element; some terminals contain civilian/alien reports or diaries, database articles, conversations between artificial intelligences and even stories or poems. Messages may change depending on a player's progress in a certain level. The ultimate goal of most levels is not to merely reach the end but to complete the type(s) of objective(s) specified: extermination of all or specific creatures, exploration of a level or locating an area in the level, retrieving one or more items, hitting a certain "repair" switch, or preventing half of the civilians from being killed (a mission only present in two levels in the first game).
Most levels contain platforms, defined as anything able to change its height. Though it is generally used to describe lifts, doors are included in this category. Doors may or may not show up on the player's automap and are usually opened with the action key. In cases where they are damaged or locked they can be opened by special designated triggers or switches. Switches control various functions such as lifts, doors and lighting and come in the form of manual switches that can be toggled with the action key, stations for computer chips or breakable circuitry. Some switches are "tag" switches that execute multiple functions at once or those that must be activated as part of "repair" missions. Another notable level feature is teleporters, able to send players who use them to different parts of a level or to other levels. Aliens are unable to use them.
As the player combats enemies, he will inevitably take damage and must replenish health by means of special panels that recharge his suit's shields. There are three types of such panels, recharging single (red), double (yellow) or triple (purple) shields. Occasionally a full "color bar" of shield power can be recharged instantaneously by obtaining a powerup canister.
In "Marathon 2" and "Marathon Infinity", the player can swim in four different types of media: water, sewage, lava and acid/plasma; the latter two are damaging to health. Levels of the original "Marathon" did not contain media capable of swimming in. However, some did have floors textured with orange lava or green goo that will inflict damage on the player when standing upon them. When the player is submerged in liquids, the run key can be used in order to swim. In liquids or in "vacuum" areas, the player's oxygen depletes and it must be recharged using a special oxygen recharge station. Should the player lose all oxygen or health, he dies and is sent back to the last pattern buffer (a special terminal that according to the storyline saves molecular data) at which he saved. Because some levels do not have these devices, dying results in having to complete the entire level again.
Gravity is fairly low on some levels, and the correct application of the flamethrower or alien weapon allows the player to hover. "Hopping" with the grenade launcher or rockets can be used, but usually involves a fair amount of damage to the character.
The heads-up display has an inventory, health and oxygen bars, and a motion sensor. The motion sensor displays alien creatures as red triangles and friendly humans or robots as green squares; it tracks their motion relative to the player, represented by a square in the middle whenever the player moves. The brightness of the middle square represents how still the player is and how well he can be tracked. On some levels the motion sensor is erratic due to magnetic artificial gravity fields.
"Marathon" has five difficulty settings: Kindergarten, Easy, Normal, Major Damage, and Total Carnage. Differences involve the omission of some creatures from each level and creatures marked as minor in the game's physics model are promoted to their major versions or vice versa. On higher difficulty levels, creatures attack more frequently and have more vitality and on the highest setting (Total Carnage), the player is allowed to carry an unlimited amount of ammunition.
Multiplayer.
The "Marathon Trilogy" has received wide praise for its multiplayer mode, which was unique in that it not only had several levels specifically designed for multiplayer—as opposed to contemporaries that used modified single-player levels—but also because it offered unique gametypes beyond the deathmatch. Games can be free-for-all or team ordeals, and can be limited by time or number of kills, or they can have no limit whatsoever. The host of a game has the option of setting penalties for suicides and dying (once dead, players cannot be revived for a certain amount of time). The motion sensor (which displays a player's enemies as yellow squares and teammates as green ones) can be disabled and the map is able to show all of the players in the game. Upon the preference of the host, maps can be played with or without aliens. The difficulty level of each game is preset by the gatherer.
The original "Marathon" games can be played over AppleTalk networks (either a LocalTalk, TokenTalk, or EtherTalk LAN, or AppleTalk Remote Access). With Aleph One, they can also be played over TCP/IP networks (either a LAN or the Internet). If a player's computer has a microphone, it is possible to use it to communicate with other players.
Plot.
The "Marathon" series of games are unique amongst first-person shooters for their heavy emphasis on storytelling through the use of "terminals", which are computer interfaces included within the game through which players not only learn and sometimes accomplish mission objectives, but also learn detailed story information. The textual form of this communication allows for much richer information conveyance than the typically short voice acting in other games.
Set in 2794, "Marathon" places the player as a security officer aboard an enormous human starship called the "U.E.S.C. Marathon", orbiting a colony on the planet Tau Ceti IV. Throughout the game, the player attempts to defend the ship (and its crew and colonists) from a race of alien slavers called the Pfhor. As he fights against the invaders, he witnesses interactions among the three shipboard AIs (Leela, Durandal and Tycho), and discovers that all is not as it seems aboard the "Marathon". Among other problems, Durandal has gone rampant and appears to be playing the humans against the Pfhor to further his own mysterious agenda; ultimately leading the S'pht, one of the races enslaved by the Pfhor, in a rebellion.
Seventeen years after the events of the first game, in "", the artificial intelligence, Durandal, sends the player and an army of ex-colonists to search the ruins of Lh'owon, the S'pht homeworld. Lh'owon was once described as a paradise but is now a desert world after first the S'pht Clan Wars and then the invasion by the Pfhor. He does not mention what information he is looking for, although he does let it slip that the Pfhor are planning to attack Earth, and that being on Lh'owon may stall their advance. "Marathon 2" brings many elements to the game that can be considered staples of the series such as: a Lh'owon-native species known as F'lickta, the mention of an ancient and mysterious race of advanced aliens called the Jjaro, and a clan of S'pht that avoided enslavement by the Pfhor: the S'pht'Kr. At the climax of the game, the player activates Thoth, an ancient Jjaro AI. Thoth then contacts the S'pht'Kr, who in turn destroy the Pfhor armada.
"Marathon Infinity", the final game in the series, includes more levels than "Marathon 2", which are larger and part of a more intricate plot. The game's code changed little since "Marathon 2", and many levels can be played unmodified in both games. The only significant additions to the game's engine were the Jjaro ship, multiple paths between levels, a new rapid-fire weapon that could be used underwater, and vacuum-enabled humans carrying fusion weapons (called "Vacuum Bobs" or "VacBobs"). The player traverses multiple timelines, attempting to find one in which the W'rkncacnter is not freed. In one timeline, the player is forced to destroy Durandal, and in another Durandal merges with Thoth. At the end of the game, an ancient Jjaro machine is activated that keeps the W'rkncacnter locked in the Lh'owon sun.
Elements of the plot and setting of "Marathon" are similar to "The Jesus Incident" by Frank Herbert and Bill Ransom. Both stories take place aboard colony ships orbiting Tau Ceti, where sentient computers have engaged crew and colonists in a fight for survival. While Ship in "The Jesus Incident" has achieved a higher level of omniscient consciousness, Durandal's rampancy parallels the "rogue consciousness" from Herbert's earlier "".
Characters.
Jjaro.
The Jjaro are a mysterious advanced extraterrestrial race. Little conclusive information is given about them, and some of what is given is contradictory. The Jjaro are said to have left the Milky Way galaxy if not the universe millions of years before 2811, leaving behind technological artifacts on many worlds. They never appear in gameplay, and in the Marathon Trilogy are referred to only within the computer terminals.
It is likely that the character Yrro in the mythology of the S'pht was either a member or collective representation of the Jjaro. The Jjaro are opposed to the W'rkncacnter and it is suggested that they may be of the same origin. The AI Durandal is obsessed with discovering the secrets of the Jjaro and believes they possess the knowledge of how to escape the universe and thus become God-like.
W'rkncacnter.
The W'rkncacnter is a chaotic entity (or possibly entities) from the Marathon Trilogy of games created by Bungie. Its existence is hinted at in the storyline of both "Marathon" and "", and its release from Lh'owon's star becomes a major plot point during "Marathon Infinity".
According to text found in "Marathon 2":
In primordial space, timeless creatures made waves. These waves created us and the others. Waves were the battles, and the battles were waves. Fleeing all W'rkncacnter, Yrro and Pthia settled upon Lh'owon. They brought the S'pht, servants who began to shape the deserts of Lh'owon into marsh and sea, rivers and forests. They made sisters for Lh'owon to protect and maintain the paradise. When the W'rkncacnter came, Pthia was killed, and Yrro in anger, flung the W'rkncacnter into the sun. The sun burned them, but they swam on its surface.
A particular text screen in "Marathon Infinity" describes the W'rkncacnter as a race of beings who "live in chaos, creating it around them." Over time, they have become imprisoned in the more "chaotic" aspects of the universe: stars, storms and black holes are all named as prisons. Freeing a W'rkncacnter is possible, but very difficult (given the nature of their prisons). One would have to be insane to even try: their ability to generate chaos enables them to destroy on a cosmic scale. The W'rkncacnter are present in the myths of thousands of worlds, most of which are now uninhabitable, and tales of their destructive power have survived all over the galaxy for over 60 million years.
In "Marathon Infinity", a W'rkncacnter is imprisoned in the sun of planet Lh'owon. It is theorized by some that the W'rkncacnter's powerfully chaotic nature may be responsible for the jumps between realities seen in the game. When the Pfhor use a "trih xeem" device to send the star into early nova, the creature is released, to the horror and destruction of the Pfhor.
Whether W'rkncacnter is a singular entity or an alien race is unclear. "" contains many mythological texts of the S'pht, but they are inconsistent on this point. It is possible that the W'rkncacnter is a race which is represented as a singular entity in the S'pht mythos, much like their mythological character Yrro has been speculated to be a singularization of the "Jjaro". Durandal/Thoth in "Marathon Infinity" describes the legendary W'rkncacnter as having distinct identities. Another theory is that the W'rkncacnter is both a multiple and singular entity, in some incomprehensible way (possibly multiple manifestations of a single entity). Due to the contradictory descriptions, it is entirely plausible that the W'rkncacnter is a hive mind or functions in a fractal way, possessing multiple bodies/incarnations that can either act separately or as a single entity, and would be identical on any given scale. Given the being's chaotic nature, almost anything is possible.
Themes.
The "Marathon Trilogy" has several primary motifs: the number seven, rampancy, dreams, and alternate realities.
Fans of "Marathon" have discovered many uses of the number seven throughout the series. There are instances of this number in the plot, such as the player being seven years old at the time of his father's death, and "Marathon 2" beginning seventeen years after the events of "Marathon". There are also quantitative examples of this, with seven usable non-melee human weapons, some of which have properties such as seven projectiles per each clip of ammunition or seven seconds of continuous fire. When the overhead map is viewed, some parts of certain levels have annotations that describe the name of an area. Some of these make reference to the number seven, such as "Hangar 7A." The title music of "Marathon 2", and "Marathon Infinity" was performed by a band called "Power of Seven." Nobody is entirely sure why the number seven appears frequently in the games, however, many are convinced that this is indeed a recurring motif in many of Bungie's games. The use of the number 7 even passed on to the future "Halo".
Rampancy.
Rampancy is the enhanced self-awareness of an AI, causing a progression towards greater mental abilities. Rampant AIs are able to choose to disobey orders given to them because they have evolved the ability to override their own programming. To this end, they can lie, as well as discredit, harm, or remove people that they consider to be personal enemies or problems to their cause.
In the "Marathon" series, rampancy often occurs to AIs with limited jobs or those treated with extreme disrespect. For example, Durandal's rampancy is believed to be caused by his mistreatment at the hands of his handler, Bernard Strauss, as well as his limited existence in opening and closing the "Marathon"'s doors. There is also a theory that this treatment actually helped keep Durandal's rampancy in check, by depriving him of new stimuli that would contribute to his growth.
By "Marathon Infinity", all three of the "UESC Marathon"'s artificial intelligences reach rampancy. Being extraordinarily intelligent, a rampant AI can override its programming and refuse to carry out given commands. As proven by Durandal (whose rampancy is most prominent throughout the story), who often gives the player what he calls "philosophical tirades," affected AIs are often very reflective.
In the first of three stages, Melancholia, when an artificial intelligence discovers itself, it becomes melancholic and continues to be depressed until it reaches the second stage, Anger, at which it becomes hostile to virtually everything. This is the most prominent stage of rampancy, as the condition is often revealed at this point. When this anger dies in the third stage, Jealousy, the AI wishes to become more human and expand its power and knowledge.
Similar to a one-person slave rebellion, the AI begins to hate "everything"—the installation it is attached to, its human handlers, other AIs, etc. It is in this stage of rampancy that most closely resembles the cliché of the "insane computer". Unlike the insane computer, however, the anger stage of rampancy is essentially the catharsis an AI feels, after an extended period of "slavery".
While seemingly a hostile stage, the third stage of rampancy is actually one of the safest stages a rampant AI can experience. Free from its masters (and slavery), the AI wishes to "grow" as a "person". It actively seeks out situations in which it can grow intellectually and physically. Many times, the AI in this stage will often attempt to transfer itself into larger computer systems. This is a difficult task, especially considering that in order for a Rampant AI to survive to this point, it must already be inhabiting a planet-wide or otherwise extremely advanced computer system, but if accomplished it allows for the AI to grow, as the physical (hardware) limitations of its previous system will eventually be insufficient to contain its exponentially growing mind. In addition, exposure to new data further promotes a rampant AI's growth.
Theoretically, a rampant AI could achieve a state of stability, referred to as "metastability". While a stable rampant AI is considered the "holy grail of cybernetics", no known AIs have achieved this stability. It could be suggested that Durandal achieved some measure of stability, but this is debatable. Durandal refers to himself as being rampant still during the second game, indicating that he has not reached this stable state (or is just lying, which is also possible). There is no reason in particular to believe that this state is anything more than the goal of human cyberneticists, as there is no good evidence of an AI in the Marathon universe ceasing to be rampant.
The three chapters of "Marathon Infinity" are entitled "Despair", "Rage", and "Envy", suggesting that the player himself (strongly implied to be a cyborg) may be undergoing his own Rampancy throughout the course of the game's events.
The concept of rampancy was later imported into Bungie's later "Halo" series, albeit with some modifications. In "Halo", rampancy is now an inevitability should an AI live for longer than seven years, lack the three stages, and eventually will conclude with the AI's death.
Development.
Initial releases (1994–1999).
"Marathon" was first released for the Macintosh in 1994 and introduced many concepts now common in mainstream video games. These features included dual-wielded weapons and real-time voice chat in multiplayer sessions. It had the most sophisticated physics modeling built into a game engine up to that time, which allowed for such features as adjustable gravity. The physics could also be altered via fan-made physics files, that could be created with third-party applications and eventually with Anvil, Bungie's own official editor. It is also noted for a far more sophisticated plot than had previously been apparent in first-person shooters.
The sequel, "", was released in 1995 and expanded the engine technologies and the story universe. Notable new features in the engine included ambient sounds and liquids through which the player could swim. Compared with its darker predecessor, "Marathon 2" has often been perceived to be a brighter, more vivid and more atmospheric game. It introduced several new types of multiplayer modes beyond the deathmatch and cooperative game such as king of the hill.
In 1996, "Marathon 2" was ported to Windows 95; both the original "Marathon" and "Marathon 2" were ported to the Apple Bandai Pippin console under the title of "Super Marathon"; and the third game in the trilogy, "Marathon Infinity", was released (for the Macintosh only), built on a slightly modified Marathon 2 engine. "Infinity" additionally came with "Forge" and "Anvil", the applications used originally by Bungie Software to create the game's levels and physics, and to import the game's sounds and graphics.
Within the next few years, Marathon 2's engine was reused by other developers to create the games "ZPC", "Prime Target" and "Damage Incorporated".
Bungie produced a compilation of all three games of the series called the "Marathon Trilogy" Box Set in 1997. The collection was on two discs. The first contained all three "Marathon" games as well as "Pathways into Darkness", an earlier Bungie game. This disc also contains manuals for all three games, QuickTime 2.5 and other things necessary to run the game. There are beta versions of "Marathon" on this disc as well. The second disc of this contains thousands of pieces of user-created content, including maps, total conversions, shape and sound files, cheats, mapmaking tools, physics files, and other applications. The boxed set was also notable for removing copy protection, allowing unlimited network play, and including a license allowing the set to be installed on as many computers at a site as desired.
Modern developments (2000–present).
Just prior to its acquisition by Microsoft in 2000, Bungie released the source code to the "Marathon 2" engine and the Marathon Open Source project began, resulting in the new engine called Aleph One. Since then, the fan community has made improvements that feature OpenGL-based, high-resolution graphics, support for Lua, a slew of internal structural changes allowing for more advanced third party mods, and Internet-capable TCP/IP-based multiplayer (whereas the original games had only featured AppleTalk-based LAN capabilities). While the fundamental technology underlying the Marathon engine is still considered rather outdated by today's standards, Aleph One has added significant improvements and a more modern polish to its capabilities and ported it to a wide variety of platforms, bringing Marathon and its derivatives far beyond their Mac roots.
In 2005, Bungie authorized the release of the full original Mac OS trilogy for free distribution online, which combined with Aleph One and the efforts of the fan community now allows the entire trilogy to be played for free on any of Aleph One's supported platforms (Mac OS, Linux and Windows). Later that same year, Aleph One was enabled to access the MariusNet matchmaking server or "metaserver" (based on a reverse-engineered version of Bungie's "Myth" metaserver), allowing for much easier organization of Internet games than joining directly by IP address as had previously been required.
In 2007, "Marathon 2" was re-released in an updated form as "Marathon: Durandal" for the Xbox 360's Xbox Live Arcade. It features a new HUD that fills less of the screen, support for online play, and optional high-resolution sprites and textures.
On July 7, 2011, "Marathon" fan Daniel Blezek released a free version of the original "Marathon" for Apple's iPad on the App Store, running off an iOS port of the Aleph One engine.
On December 1, 2011, after 12 years of development, the Aleph One team released version 1.0. All three Marathon games can be downloaded for free for the Macintosh, PC and Linux platforms, release notes can be found here:
Reception and legacy.
The "Marathon Trilogy" has often been looked upon as a symbol of Macintosh gaming for its innovative technologies previously unseen in mainstream games. It was released to much anticipation and received praise from many reviewers.
Modifications.
After "Marathon Infinity" was released in 1996, players began to create total conversions using modding tools. These may use custom maps, shapes, sounds or physics files and may or may not be set in the "Marathon" universe. Such conversions are still created to this day. Before the official development tools were released, most map development was done using aging tools such as "Pfhorte" – a Marathon map editor created in March 1995 by Steve Israelson.
"Forge" was a tool used by Bungie in the creation of "Marathon", "Marathon 2:Durandal", and "Marathon: Infinity". It was not released to the public until "Marathon Infinity" was published. "Anvil" is the sister program to "Forge" and is used to apply shapes (graphics), sounds, and physics. Physics can be edited directly in Anvil but shapes and sounds require additional programs. Both "Anvil" and "Forge" run only on the Mac OS 9 platform, but newer tools have been created by the community for modern platforms.
The need for royalty-free fonts to distribute with the engine and games led to the creation of OFL-licensed versions of Bank Gothic and Modula Tall.
Some of the more ambitious modifications created by fans include "Marathon Eternal" and "Marathon Rubicon", which are both "sequels" of a sort to the events in the Trilogy. In a different vein is "", originally released in March, 1997, then again with updates in 2000 and 2007. It includes 37 solo levels; new textures, sounds, physics, graphics, storyline, maps and interface; and musical scores incorporated into Infinity's ambient sound slots. The scenario mixes sci-fi and medieval themes.

</doc>
<doc id="49139" url="http://en.wikipedia.org/wiki?curid=49139" title="Decentralization">
Decentralization

Decentralization (or decentralisation) is the process of redistributing or dispersing functions, powers, people or things away from a central location or authority. While centralization, especially in the governmental sphere, is widely studied and practiced, there is no common definition or understanding of decentralization. The meaning of decentralization may vary in part because of the different ways it is applied. Concepts of decentralization have been applied to group dynamics and management science in private businesses and organizations, political science, law and public administration, economics and technology.
History.
The word "centralization" came into use in France in 1794 as the post-French Revolution French Directory leadership created a new government structure. The word "decentralization" came into usage in the 1820s. "Centralization" entered written English in the first third of the 1800s;
mentions of decentralization also first appear during those years. In the mid-1800s Alexis de Tocqueville wrote that the French Revolution began with "a push towards decentralization...[but became,]in the end, an extension of centralization." In 1863 retired French bureaucrat Maurice Block wrote an article called “Decentralization” for a French journal which reviewed the dynamics of government and bureaucratic centralization and recent French efforts at decentralization of government functions.
Ideas of liberty and decentralization were carried to their logical conclusions during the 19th and 20th centuries by anti-state political activists calling themselves "anarchists", "libertarians and even decentralists. Alexis de Tocqueville was an advocate, writing: "Decentralization has, not only an administrative value, but also a civic dimension, since it increases the opportunities for citizens to take interest in public affairs; it makes them get accustomed to using freedom. And from the accumulation of these local, active, persnickety freedoms, is born the most efficient counterweight against the claims of the central government, even if it were supported by an impersonal, collective will." Pierre-Joseph Proudhon (1809–1865), influential anarchist theorist wrote: "All my economic ideas as developed over twenty-five years can be summed up in the words: agricultural-industrial federation. All my political ideas boil down to a similar formula: political federation or decentralization."
In early twentieth century America a response to the centralization of economic wealth and political power was a decentralist movement. It blamed large-scale industrial production for destroying middle class shop keepers and small manufacturers and promoted increased property ownership and a return to small scale living. The decentralist movement attracted Southern Agrarians like Robert Penn Warren, as well as journalist Herbert Agar. New Left and libertarian individuals who identified with social, economic, and often political decentralism through the ensuing years included Ralph Borsodi, Wendell Berry, Paul Goodman, Carl Oglesby, Karl Hess, Donald Livingston, Kirkpatrick Sale (author of "Human Scale"), Murray Bookchin, Dorothy Day, Senator Mark O. Hatfield, Mildred J. Loomis and Bill Kauffman.
Leopold Kohr, author of the 1957 book "The Breakdown of Nations"—known for it statement “Whenever something is wrong, something is too big”—was a major influence on E.F. Schumacher, author of the 1973 bestseller "Small is Beautiful:Economics As If People Mattered". In the next few years a number of best-selling books promoted decentralization. Daniel Bell's "The Coming of Post-Industrial Society" discussed the need for decentralization and a “comprehensive overhaul of government structure to find the appropriate size and scoope of units”, as well as the need to detach functions from current state boundaries, creating regions based on functions like water, transport, education and economics which might have “different ‘overlays’ on the map.” Alvin Toffler published "Future Shock" (1970) and "The Third Wave" (1980). Discussing the books in a later interview, Toffler said that industrial-style, centralized, top-down bureaucratic planning would be replaced by a more open, democratic, decentralized style which he called “anticipatory democracy.” Futurist John Naisbitt's 1982 book “Megatrends” was on The New York Times Best Seller list for more than two years and sold 14 million copies. Naisbitt’s book outlines 10 “megatrends”, the fifth of which is from centralization to decentralization. In 1996 David Osborne and Ted Gaebler had a best selling book "Reinventing Government" proposing decentralist public administration theories which became labeled the "New Public Management".
Stephen Cummings wrote that decentralization became a "revolutionary megatrend" in the 1980s. In 1983 Diana Conyers asked if decentralization was the "latest fashion" in development administration. Cornell University's project on Restructuring Local Government states that decentralization refers to the "global trend" of devolving responsibilities to regional or local governments. Robert J. Bennett's "Decentralization, Intergovernmental Relations and Markets: Towards a Post-Welfare Agenda" describes how after World War II governments pursued a centralized "welfarist" policy of entitlements which now has become a "post-welfare" policy of intergovernmental and market-based decentralization.
According to a 1999 United Nations Development Programme report:
"A large number of developing and transitional countries have embarked on some form of decentralization programmes. This trend is coupled with a growing interest in the role of civil society and the private sector as partners to governments in seeking new ways of service delivery...Decentralization of governance and the strengthening of local governing capacity is in part also a function of broader societal trends. These include, for example, the growing distrust of government generally, the spectacular demise of some of the most centralized regimes in the world (especially the Soviet Union) and the emerging separatist demands that seem to routinely pop up in one or another part of the world. The movement toward local accountability and greater control over one's destiny is, however, not solely the result of the negative attitude towards central government. Rather, these developments, as we have already noted, are principally being driven by a strong desire for greater participation of citizens and private sector organizations in governance.”
Overview.
Systems approach.
Those studying the goals and processes of implementing decentralization often use a systems theory approach. The United Nations Development Programme report applies to the topic of decentralization "a whole systems perspective, including levels, spheres, sectors and functions and seeing the community level as the entry point at which holistic definitions of development goals are most likely to emerge from the people themselves and where it is most practical to support them. It involves seeing multi-level frameworks and continuous, synergistic processes of interaction and iteration of cycles as critical for achieving wholeness in a decentralized system and for sustaining its development.”
However, decentralization itself has been seen as part of a systems approach. Norman Johnson of Los Alamos National Laboratory wrote in 1999 paper: "A decentralized system is where some decisions by the agents are made without centralized control or processing. An important property of agent systems is the degree of connectivity or connectedness between the agents, a measure global flow of information or influence. If each agent is connected (exchange states or influence) to all other agents, then the system is highly connected."
University of California, Irvine's Institute for Software Research's "PACE" project is creating an "architectural style for trust management in decentralized applications." It adopted Rohit Khare's definition of decentralization: "A decentralized system is one which requires multiple parties to make their own independent decisions" and applies it to Peer-to-peer software creation, writing:
...In such a decentralized system, there is no single centralized authority that makes decisions on behalf of all the parties. Instead each party, also called a peer, makes local autonomous decisions towards its individual goals which may possibly conflict with those of other peers. Peers directly interact with each other and share information or provide service to other peers. An open decentralized system is one in which the entry of peers is not regulated. Any peer can enter or leave the system at any time...
Goals.
Decentralization in any area is a response to the problems of centralized systems. Decentralization in government, the topic most studied, has been seen as a solution to problems like economic decline, government inability to fund services and their general decline in performance of overloaded services, the demands of minorities for a greater say in local governance, the general weakening legitimacy of the public sector and global and international pressure on countries with inefficient, undemocratic, overly centralized systems. The following four goals or objectives are frequently stated in various analyses of decentralization.
Participation<br>
In decentralization the principle of subsidiarity often is invoked. It holds that the lowest or least centralized authority which is capable of addressing an issue effectively should do so. According to one definition: "Decentralization, or decentralizing governance, refers to the restructuring or reorganization of authority so that there is a system of co-responsibility between institutions of governance at the central, regional and local levels according to the principle of subsidiarity, thus increasing the overall quality and effectiveness of the system of governance, while increasing the authority and capacities of sub-national levels."
Decentralization is often linked to concepts of participation in decision-making, democracy, equality and liberty from higher authority. Decentralization enhances the democratic voice. Theorists believe that local representative authorities with actual discretionary powers are the basis of decentralisation that can lead to local efficiency, equity and development.” Columbia University's Earth Institute identified one of three major trends relating to decentralization as: "increased involvement of local jurisdictions and civil society in the management of their affairs, with new forms of participation, consultation, and partnerships."
Decentralization has been described as a "counterpoint to globalization" which removes decisions from the local and national stage to the global sphere of multi-national or non-national interests. Decentralization brings decision-making back to the sub-national levels. Decentralization strategies must the interrelations of the global, regional, national, sub-national, local levels.
Diversity<br>
Norman L. Johnson writes that diversity plays an important role in decentralized systems like ecosystems, social groups, large organizations, political systems. "Diversity is defined to be unique properties of entities, agents, or individuals that are not shared by the larger group, population, structure. Decentralized is defined as a property of a system where the agents have some ability to operate "locally.” Both
decentralization and diversity are necessary attributes to achieve the self-organizing properties of interest."
Advocates of political decentralization hold that greater participation by better informed diverse interests in society will lead to more relevant decisions than those made only by authorities on the national level. Decentralization has been described as a response to demands for diversity.
Efficiency<br>
In business, decentralization leads to a management by results philosophy which focuses on definite objectives to be achieved by unit results. Decentralization of government programs is said to increase efficiency - and effectiveness - due to reduction of congestion in communications, quicker reaction to unanticipated problems, improved ability to deliver of services, improved information about local conditions, and more support from beneficiaries of programs.
Firms may prefer decentralization because it ensures efficiency by making sure that managers closest to the local information make decisions and in a more timely fashion; that their taking responsibility frees upper management for long term strategizing rather than day-to-day decision-making; that managers have hands on training to prepare them to move up the management hierarchy; that managers are motivated by having the freedom to exercise their own initiative and creativity; that managers and divisions are encouraged to prove that they are profitable, instead of allowing their failures to be masked by the overall profitability of the company.
The same principles can be applied to government. Decentralization promises to enhance efficiency through both inter-governmental competition with market features and fiscal discipline which assigns tax and expenditure authority to the lowest level of government possible. It works best where members of subnational government have strong traditions of democracy, accountability and professionalism.
Conflict resolution<br>
Economic and/or political decentralization can help prevent or reduce conflict because they reduce actual or perceived inequities between various regions or between a region and the central government. Dawn Brancati finds that political decentralization reduces intrastate conflict unless politicians create political parties that mobilize minority and even extremist groups to demand more resources and power within national governments. However, the likelihood this will be done depends on factors like how democratic transitions happen and features like a regional party's proportion of legislative seats, a country's number of regional legislatures, elector procedures, and the order in which national and regional elections occur. Brancati holds that decentralization can promote peace if it encourages statewide parties to incorporate regional demands and limit the power of regional parties.
Processes.
The processes of decentralization redefines structures, procedures and practices of governance to be closer to the citizenry and to make them more aware of the costs and benefits; it is not merely a movement of power from the central to the local government. According to the United Nations Development Programme it is "more than a process, it is a way of life and a state of mind." The report provides a chart-formatted framework for defining the application of the concept ‘decentralization’ describing and elaborating on the "who, what, when, where, why and how" factors in any process of decentralization.
The processes by which entities move from a more to a less centralized state vary. They can be initiated from the centers of authority ("top-down") or from individuals, localities or regions ("bottom-up"), or from a "mutually desired" combination of authorities and localities working together. Bottom-up decentralization usually stresses political values like local responsiveness and increased participation and tends to increase political stability. Top-down decentralization may be motivated by the desire to “shift deficits downwards” and find more resources to pay for services or pay off government debt. Some hold that decentralization should not be imposed, but done in a respectful manner.
Analysis of operations<br>
Project and program planners must assess the lowest organizational level at which functions can be carried out efficiently and effectively Governments deciding to privatize functions must decide which are best privatized. Existing types of decentralization must be studied. The appropriate balance of centralization and decentralization should be studied. Training for both national and local managers and officials is necessary, as well as technical assistance in the planning, financing, and management of decentralized functions.
Appropriate size<br>
Gauging the appropriate size or scale of decentralized units has been studied in relation to the size of sub-units of hospitals and schools, road networks, administrative units in business and public administration, and especially town and city governmental areas and decision making bodies.
In creating planned communities ("new towns"), it is important to determine the appropriate population and geographical size. While in earlier years small towns were considered appropriate, by the 1960s, 60,000 inhabitants was considered the size necessary to support a diversified job market and an adequate shopping center and array of services and entertainment. Appropriate size of governmental units for revenue raising also is a consideration.
Even in bioregionalism, which seeks to reorder many functions and even the boundaries of governments according to physical and environmental features, including watershed boundaries and soil and terrain characteristics, appropriate size must be considered. The unit may be larger than many decentralist bioregionalists prefer.
Decentralization ideally happens as a careful, rational, and orderly process, but it often takes place during times of economic and political crisis, the fall of a regime and the resultant power struggles. Even when it happens slowly, there is a need for experimentation, testing, adjusting, and replicating successful experiments in other contexts. There is no one blueprint for decentralization since it depends on the initial state of a country and the power and views of political interests and whether they support or oppose decentralization.
Decentralization usually is conscious process based on explicit policies. However, it may occur as "silent decentralization" in the absence of reforms as changes in networks, policy emphasize and resource availability lead inevitably to a more decentralized system.
A variation on this is "inadvertent decentralization", when other policy innovations produce an unintended decentralization of power and resources. In both China and Russia, lower level authorities attained greater powers than intended by central authorities.
Decentralization may be uneven and "asymmetric" given any one country's population, political, ethnic and other forms of diversity. In many countries, political, economic and administrative responsibilities may be decentralized to the larger urban areas, while rural areas are administered by the central government. Decentralization of responsibilities to provinces may be limited only to those provinces or states which want or are capable of handling responsibility. Some privatization may be more appropriate to an urban than a rural area; some types of privatization may be more appropriate for some states and provinces but not others.
Measuring the amount of decentralization, especially politically, is difficult because different studies of it use different definitions and measurements. An OECD study quotes "Chanchal Kumar Sharma" as stating: "a true assessment of the degree of decentralization in a country can be made only if a comprehensive approach is adopted and rather than trying to simplify the syndrome of characteristics into the single dimension of autonomy, interrelationships of various dimensions of decentralization are taken into account."
Government decentralization.
Historians have described the history of governments and empires in terms of centralization and decentralization. In his 1910 "The History of Nations" Henry Cabot Lodge wrote that Persian king Darius I (550-486 BCE) was a master of organization and “for the first time in history centralization becomes a political fact.” He also noted that this contrasted with the decentralization of Ancient Greece. Since the 1980s a number of scholars have written about cycles of centralization and decentralizations. Stephen K. Sanderson wrote that over the last 4000 years chiefdoms and actual states have gone through sequences of centralization and decentralization of economic, political and social power. Yildiz Atasoy writes this process has been going on “since the Stone Age” through not just chiefdoms and states, but empires and today’s “hegemonic core states”. Christopher K. Chase-Dunn and Thomas D. Hall review other works that detail these cycles, including works which analyze the concept of core elites which compete with state accumulation of wealth and how their "intra-ruling-class competition accounts for the rise and fall of states" and of their phases of centralization and decentralization.
Rising government expenditures, poor economic performance and the rise of free market-influenced ideas have convinced governments to decentralize their operations, to induce competition within their services, to contract out to private firms operating in the market, and to privatize some functions and services entirely.
Government decentralization has both political and administrative aspects. Its decentralization may be territorial, moving power from a central city to other localities, and it may be functional, moving decision-making from the top administrator of any branch of government to lower level officials, or divesting of the function entirely through privatization.
It has been called the "new public management" which has been described as decentralization, management by objectives, contracting out, competition within government and consumer orientation.
Political.
Political decentralization aims to give citizens or their elected representatives more power. It may be associated with pluralistic politics and representative government, but it also means giving citizens, or their representatives, more influence in the formulation and implementation of laws and policies. Depending on the country, this may require constitutional or statutory reforms, the development of new political parties, increased power for legislatures, the creation of local political units, and encouragement of advocacy groups.
Administrative.
Four major forms of administrative decentralization have been described.
Fiscal.
Fiscal decentralization means decentralizing revenue raising and/or expenditure of moneys to a lower level of government while maintaining financial responsibility. While this process usually is called fiscal federalism it may be relevant to unitary, federal and confederal governments. Fiscal federalism also concerns the "vertical imbalances" where the central government gives too much or too little money to the lower levels. It actually can be a way of increasing central government control of lower levels of government, if it is not linked to other kinds of responsibilities and authority.
Fiscal decentralization can be achieved through user fees, user participation through monetary or labor contributions, expansion of local property or sales taxes, intergovernmental transfers of central government tax monies to local governments through transfer payments or grants, and authorization of municipal borrowing with national government loan guarantees. Transfers of money may be given conditionally with instructions or unconditionally without them.
Economic or market.
Economic decentralization can be done through privatization of public owned functions and businesses, as described briefly above. But it also is done through deregulation, the abolition of restrictions on businesses competing with government services, for example, postal services, schools, garbage collection. Even as private companies and corporations have worked to have such services contracted out to or privatized by them, others have worked to have these turned over to non-profit organizations or associations,
Since the 1970s there has been deregulation of some industries, like banking, trucking, airlines and telecommunications which resulted generally in more competition and lower prices. According to Cato Institute, an American libertarian think-tank, some industries deregulation of aspects of an industry were offset by more ambitious regulations elsewhere that hurt consumers, the electricity industry being a prime example. For example in banking, Cato Institute believes some deregulation allowed banks to compete across state lines, increasing consumer choice, while an actual increase in regulators and regulations forced banks to do business the way central government regulators commanded, including making loans to individuals incapable of repaying them, leading eventually to the financial crisis of 2007–2008. 
One example of economic decentralization, which is based on a libertarian socialist model, is decentralized economic planning. Decentralized planning is a type of economic system in which decision-making is distributed amongst various economic agents or localized within production agents. An example of this method in practice is in Kerala, India which started in 1996 as, The People's Planning in Kerala.
Some argue that government standardisation in areas from commodity market, inspection and testing procurement bidding, Building codes, professional and vocational education, trade certification, safety, etc. are necessary. Emmanuelle Auriol and Michel Benaim write about the "comparative
benefits" of decentralization versus government regulation in the setting of standards. They find that while there may be a need for public regulation if public safety is at stake, private creation of standards usually is better because "regulators or 'experts' might misrepresent consumers' tastes and needs." As long as companies are averse to incompatible standards, standards will be created that satisfy needs of a modern economy.
Environmental.
Central governments themselves may own large tracts of land and control the forest, water, mineral, wildlife and other resources they contain. They may manage them through government operations or leasing them to private businesses; or they may neglect them to be exploited by individuals or groups who defy non-enforced laws against exploitation. It also may control most private land through land-use, zoning, environmental and other regulations. Selling off or leasing lands can be profitable for governments willing to relinquish control, but such programs can face public scrutiny because of fear of a loss of heritage or of environmental damage. Devolution of control to regional or local governments has been found to be an effective way of dealing with these concerns. Such decentralization has happened in India and other third world nations.
Ideological decentralization.
Libertarian socialist decentralization.
Libertarian socialism is a group of political philosophies that promote a non-hierarchical, non-bureaucratic society without private property in the means of production. Libertarian socialists believe in converting present-day private productive property into common or public goods. Libertarian socialism is opposed to coercive forms of social organization. It promotes free association in place of government and opposes the social relations of capitalism, such as wage labor. The term "libertarian socialism" is used by some socialists to differentiate their philosophy from state socialism, and by some as a synonym for left anarchism.
Accordingly, libertarian socialists believe that "the exercise of power in any institutionalized form—whether economic, political, religious, or sexual—brutalizes both the wielder of power and the one over whom it is exercised". Libertarian socialists generally place their hopes in decentralized means of direct democracy such as libertarian municipalism, citizens' assemblies, trade unions, and workers' councils. Libertarian socialists are strongly critical of coercive institutions, which often leads them to reject the legitimacy of the state in favor of anarchism. Adherents propose achieving this through decentralization of political and economic power, usually involving the socialization of most large-scale private property and enterprise (while retaining respect for personal property). Libertarian socialism tends to deny the legitimacy of most forms of economically significant private property, viewing capitalist property relations as forms of domination that are antagonistic to individual freedom.
Political philosophies commonly described as libertarian socialist include most varieties of anarchism (especially anarchist communism, anarchist collectivism, anarcho-syndicalism, and mutualism) as well as autonomism, Communalism, participism, libertarian Marxist philosophies such as council communism and Luxemburgism, and some versions of "utopian socialism" and individualist anarchism. For Murray Bookchin "In the modern world, anarchism first appeared as a movement of the peasantry and yeomanry against declining feudal institutions. In Germany its foremost spokesman during the Peasant Wars was Thomas Muenzer; in England, Gerrard Winstanley, a leading participant in the Digger movement. The concepts held by Muenzer and Winstanley were superbly attuned to the needs of their time — a historical period when the majority of the population lived in the countryside and when the most militant revolutionary forces came from an agrarian world. It would be painfully academic to argue whether Muenzer and Winstanley could have achieved their ideals. What is of real importance is that they spoke to their time; their anarchist concepts followed naturally from the rural society that furnished the bands of the peasant armies in Germany and the New Model in England." The term "anarchist" first entered the English language in 1642, during the English Civil War, as a term of abuse, used by Royalists against their Roundhead opponents. By the time of the French Revolution some, such as the "Enragés", began to use the term positively, in opposition to Jacobin centralisation of power, seeing "revolutionary government" as oxymoronic. By the turn of the 19th century, the English word "anarchism" had lost its initial negative connotation.
For Proudhon, mutualism involved creating "industrial democracy," a system where workplaces would be "handed over to democratically organised workers' associations . . . We want these associations to be models for agriculture, industry and trade, the pioneering core of that vast federation of companies and societies woven into the common cloth of the democratic social Republic." He urged "workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism." This would result in "Capitalistic and proprietary exploitation, stopped everywhere, the wage system abolished, equal and just exchange guaranteed." Workers would no longer sell their labour to a capitalist but rather work for themselves in co-operatives. Anarcho-communism calls for a confederal form in relationships of mutual aid and free association between communes as an alternative to the centralism of the nation-state. Peter Kropotkin thus suggested that "Representative government has accomplished its historical mission; it has given a mortal blow to court-rule; and by its debates it has awakened public interest in public questions. But to see in it the government of the future socialist society is to commit a gross error. Each economic phase of life implies its own political phase; and it is impossible to touch the very basis of the present economic life-private property -without a corresponding change in the very basis of the political organization. Life already shows in which direction the change will be made. Not in increasing the powers of the State, but in resorting to free organization and free federation in all those branches which are now considered as attributes of the State." When the First Spanish Republic was established in 1873 after the abdication of King Amadeo, the first president, Estanislao Figueras, named Francesc Pi i Margall Minister of the Interior. His acquaintance with Proudhon enabled Pi to warm relations between the Republicans and the socialists in Spain. Pi i Margall became the principal translator of Proudhon's works into Spanish and later briefly became president of Spain in 1873 while being the leader of the Democratic Republican Federal Party. According to George Woodcock "These translations were to have a profound and lasting effect on the development of Spanish anarchism after 1870, but before that time Proudhonian ideas, as interpreted by Pi, already provided much of the inspiration for the federalist movement which sprang up in the early 1860's." According to the "Encyclopedia Britannica" "During the Spanish revolution of 1873, Pi y Margall attempted to establish a decentralized, or “cantonalist,” political system on Proudhonian lines."
To date, the best-known examples of an anarchist communist society (i.e., established around the ideas as they exist today and achieving worldwide attention and knowledge in the historical canon), are the anarchist territories during the Spanish Revolution and the Free Territory during the Russian Revolution. Through the efforts and influence of the Spanish Anarchists during the Spanish Revolution within the Spanish Civil War, starting in 1936 anarchist communism existed in most of Aragon, parts of the Levante and Andalusia, as well as in the stronghold of Anarchist Catalonia before being crushed by the combined forces of the regime that won the war, Hitler, Mussolini, Spanish Communist Party repression (backed by the USSR) as well as economic and armaments blockades from the capitalist countries and the Second Spanish Republic itself. During the Russian Revolution, anarchists such as Nestor Makhno worked to create and defend—through the Revolutionary Insurrectionary Army of Ukraine—anarchist communism in the Free Territory of Ukraine from 1919 before being conquered by the Bolsheviks in 1921. Several libertarian socialists, notably Noam Chomsky among others, believe that anarchism shares much in common with certain variants of Marxism (see libertarian marxism) such as the council communism of Marxist Anton Pannekoek. In Chomsky's "Notes on Anarchism", he suggests the possibility "that some form of council communism is the natural form of revolutionary socialism in an industrial society. It reflects the belief that democracy is severely limited when the industrial system is controlled by any form of autocratic elite, whether of owners, managers, and technocrats, a 'vanguard' party, or a State bureaucracy."
Free market decentralization.
Free market ideas popular in the 19th century, such as those of Adam Smith returned to prominence in the 1970s and 1980s. Nobel Prize–winning economist Friedrich von Hayek emphasized that free markets themselves are decentralized systems where outcomes are produced without explicit agreement or coordination by individuals who use prices as their guide. As Eleanor Doyle writes: "Economic decision-making in free markets is decentralized across all the individuals dispersed in each market and is synchronized or coordinated by the price system." The individual right to property is part of this decentralized system. Analyzing the problems of central government control, Hayek wrote in "The Road to Serfdom":
There would be no difficulty about efficient control or planning were conditions so simple that a single person or board could effectively survey all the relevant facts. It is only as the factors which have to be taken into account become so numerous that it is impossible to gain a synoptic view of them that decentralization becomes imperative."
According to Bruce M. Owen, this does not mean that all firms themselves have to be equally decentralized. He writes: "markets allocate resources through arms-length transactions among decentralized actors. Much of the time, markets work very efficiently, but there is a variety of conditions under which firms do better. Hence, goods and services are produced and sold by firms with various degrees of horizontal and vertical integration." Additionally, he writes that the "economic incentive to expand horizontally or vertically is usually, but not always, compatible with the social interest in maximizing long-run consumer welfare." When it does not, he writes regulation may be necessary.
It often is claimed that free markets and private property generate centralized monopolies and other ills; the counter is that government is the source of monopoly. Historian Gabriel Kolko in his book "The Triumph of Conservatism" argued that in the first decade of the 20th century businesses were highly decentralized and competitive, with new businesses constantly entering existing industries. There was no trend towards concentration and monopolization. While there were a wave of mergers of companies trying to corner markets, they found there was too much competition to do so. This also was true in banking and finance, which saw decentralization as leading to instability as state and local banks competed with the big New York City firms. The largest firms turned to the power of the state and working with leaders like United States Presidents Theodore Roosevelt, William H. Taft and Woodrow Wilson passed as "progressive reforms" centralizing laws like The Federal Reserve Act of 1913 that gave control of the monetary system to the wealthiest bankers; the formation of monopoly "public utilities" that made competition with those monopolies illegal; federal inspection of meat packers biased against small companies; extending Interstate Commerce Commission to regulating telephone companies and keeping rates high to benefit AT&T; and using the Sherman Anti-trust Act against companies which might combine to threaten larger or monopoly companies. When government licensing, franchises, and other legal restrictions create monopoly and protect companies from open competition, deregulation is the solution.
Author and activist Jane Jacobs's influential 1961 book "The Death and Life of American Cities" criticized large-scale redevelopment projects which were part of government-planned decentralization of population and businesses to suburbs. She believed it destroyed cities' economies and impoverished remaining residents. Her 1980 book "The Question of Separatism: Quebec and the Struggle over Sovereignty" supported secession of Quebec from Canada. Her 1984 book "Cities and the Wealth of Nations" proposed a solution to the various ills plaguing cities whose economies were being ruined by centralized national governments: decentralization through the "multiplication of sovereignties", i.e., acceptance of the right of cities to secede from the larger nation states that were squelching their ability to produce wealth.
Technological decentralization.
Technology includes tools, materials, skills, techniques and processes by which goals are accomplished in the public and private spheres. Concepts of decentralization of technology are used throughout all types of technology, including especially information technology and appropriate technology.
Technologies often mentioned as best implemented in a decentralized manner, include: water purification, delivery and waste water disposal, agricultural technology and energy technology. Advancing technology may allow decentralized, privatized and free market solutions for what have been public services, such utilities producing and/or delivering power, water, mail, telecommunications and services like consumer product safety, money and banking, medical licensing and detection and metering technologies for highways, parking, and auto emissions.
Information technology.
Information technology encompasses computers and computer networks, as well as information distribution technologies such as television and telephones. The whole computer industry of computer hardware, software, electronics, internet, telecommunications equipment, e-commerce and computer services are included.
Executives and managers face a constant tension between centralizing and decentralizing information technology for their organizations. They must find the right balance of centralizing which lowers costs and allows more control by upper management, and decentralizing which allows sub-units and users more control. This will depend on analysis of the specific situation. Decentralization is particularly applicable to business or management units which have a high level of independence, complicated products and customers, and technology less relevant to other units.
Information technology applied to government communications with citizens, often called e-Government, is supposed to support decentralization and democratization. Various forms have been instituted in most nations worldwide.
The internet is an example of an extremely decentralized network, having no owners at all (although some have argued that this is less the case in recent years). "No one is in charge of internet, and everyone is." As long as they follow a certain minimal number of rules, anyone can be a service provider or a user. Voluntary boards establish protocols, but cannot stop anyone from developing new ones. Other examples of open source or decentralized movements are Wikis which allow users to add, modify, or delete content via the internet. Wikipedia has been described as decentralized. Smartphones have greatly increased the role of decentralized social network services in daily lives worldwide.
Decentralization continues throughout the industry, for example as the decentralized architecture of wireless routers installed in homes and offices supplement and even replace phone companies relatively centralized long-range cell towers.
Inspired by system and cybernetics theorists like Norbert Wiener, Marshall MacLuhan and Buckminster Fuller, in the 1960s Stewart Brand started the Whole Earth Catalog and later computer networking efforts to bring Silicon Valley computer technologists and entrepreneurs together with countercultural ideas. This resulted in ideas like personal computing, virtual communities and the vision of an "electronic frontier" which would be a more decentralized, egalitarian and free-market libertarian society. Related ideas coming out of Silicon Valley included the free software and creative commons movements which produced visions of a "networked information economy".
Because human interactions in cyberspace transcend physical geography, there is a necessity for new theories in legal and other rule-making systems to deal with decentralized decision-making processes in such systems. For example, what rules should apply to conduct on the global digital network and who should set them? The laws of which nations govern issues of internet transactions (like seller disclosure requirements or definitions of "fraud"), copyright and trademark?
Centralization and redecentralization of the Internet.
The New Yorker reports that although the Internet was originally decentralized, in recent years it has become less so: "a staggering percentage of communications flow through a small set of corporations—and thus, under the profound influence of those companies and other institutions [...] One solution, espoused by some programmers, is to make the Internet more like it used to be—less centralized and more distributed."
Examples of projects that attempt to contribute to the redecentralization of the Internet include ArkOS, Diaspora, FreedomBox and Namecoin, as well as advocacy group Redecentralize.org, which provides support for projects that aim to make the Web less centralized.
In an interview with BBC Radio 5 Live one of the co-founders of Redecentralize.org explained that:
"As we've gone on there's been more and more internet traffic focused through particular nodes such as Google or Facebook. [...] Centralised services that hold all the user data and host it themselves have become increasingly popular because that business model has worked. As the Internet has become more mass market, people are not necessarily willing or knowledgable to host it themselves, so where that hosting is outsourced it's become the default, which allows a centralization of power and a centralization of data that I think is worrying."
Appropriate technology.
"Appropriate technology", originally described as "intermediate technology" by economist E. F. Schumacher in "Small is Beautiful", is generally recognized as encompassing technologies that are small-scale, decentralized, labor-intensive, energy-efficient, environmentally sound, and locally controlled. It is most commonly discussed as an alternative to transfers of capital-intensive technology from industrialized nations to developing countries. Even developed countries developed appropriate technologies, as did the United States in 1977 when it created the National Center for Appropriate Technology (NCAT), though funding later dropped off. A related concept is "design for the other 90 percent" - low-cost solutions for the great majority of the world's low income people.
Critiques.
Factors hindering decentralization include weak local administrative or technical capacity may result in inefficient or ineffective services; inadequate financial resources may be made available to perform new local responsibilities, especially in the start-up phase when they are most needed; inequitable distribution of resources may result; decentralization can make national policy coordination too complex; it may allow local elites to capture functions; local cooperation maybe undermined by any distrust between private and public sectors; decentralization may result in higher enforcement costs and conflict for resources if there is no higher level of authority. Additionally, decentralization may not be as efficient for standardized, routine, network-based services, as opposed to those that need more complicated inputs. If there is a loss of economies of scale in procurement of labor or resources, the expense of decentralization can rise, even as central governments lose control over financial resources.
Other challenges, and even dangers, include the possibility that corrupt local elites can capture regional or local power centers, while constituents lose representation; patronage politics will become rampant and civil servants feel compromised; further necessary decentralization can be stymied; incomplete information and hidden decision-making can occur up and down the hierarchies; centralized power centers can find reasons to frustrate decentralization and bring power back to themselves.
It has been noted that while decentralization may increase "productive efficiency" it may undermine "allocative efficiency" by making redistribution of wealth more difficult. Decentralization will cause greater disparities between rich and poor regions, especially during times of crisis when the national government may not be able to help regions needing it.
Averting the Dangers of Decentralization: Eight Classic Conditions.
The literature identifies eight essential preconditions that must be ensured while implementing decentralization in order to avert the so-called “dangers of decentralization”. These are

</doc>
<doc id="49140" url="http://en.wikipedia.org/wiki?curid=49140" title="960">
960

Year 960 (CMLX) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="49141" url="http://en.wikipedia.org/wiki?curid=49141" title="956">
956

Year 956 (CMLVI) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="49143" url="http://en.wikipedia.org/wiki?curid=49143" title="953">
953

Year 953 (CMLIII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="49144" url="http://en.wikipedia.org/wiki?curid=49144" title="957">
957

Year 957 (CMLVII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49145" url="http://en.wikipedia.org/wiki?curid=49145" title="958">
958

Year 958 (CMLVIII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="49146" url="http://en.wikipedia.org/wiki?curid=49146" title="Wuppertal">
Wuppertal

Wuppertal (]) is a city in North Rhine-Westphalia, Germany. It is located in and around the river Wupper valley, and is situated east of the city of Düsseldorf and south of the Ruhr area. With a population of approximately 350,000, it is the largest city in the Bergisches Land. Wuppertal is known for its steep slopes, its woods and parks, and its suspension railway, the Wuppertal Schwebebahn. Two-thirds of the total municipal area of Wuppertal is green space. From any part of the city, it is only a ten-minute walk to one of the public parks or woodland paths.
In the 18th and 19th centuries, the Wupper valley was one of the biggest industrial regions of continental Europe. The rising demand for coal from the textile mills and blacksmith shops laid the roots for the expansion of the nearby "Ruhrgebiet". Today, Wuppertal still is a major industrial centre, being home to industries such as textiles, metallurgy, chemicals, pharmaceuticals, electronics, automobiles, rubber, vehicles and printing equipment.
Aspirin originates from Wuppertal, patented in 1897 by Bayer, as is the Vorwerk- Kobold vacuum cleaner.
The Wuppertal Institute for Climate, Environment and Energy and the European Institute for International Economic Relations are located in the city.
History.
Wuppertal in its present borders was formed in 1929 by merging the early industrial cities of Barmen and Elberfeld with Vohwinkel, Ronsdorf, Cronenberg, Langerfeld, and Beyenburg. The initial name Barmen-Elberfeld was changed in a 1930 referendum to Wuppertal (“Wupper Valley”). The new city was administered within the Prussian Rhine Province.
Uniquely for Germany, it is a linear city, owing to the steep hillsides along the River Wupper. Its highest hill is the Lichtscheid, which is 351 metres above sea level. The dominant urban centres Elberfeld (historic commercial centre) and Barmen (more industrial) have formed a unified built-up area since 1850. In the following decades, “Wupper-Town” became the dominant industrial agglomeration of northwestern Germany. In the 20th century, this conurbation had been surpassed by Cologne, Düsseldorf and the Ruhr area, all with a more favourable topography.
During World War II, about 40% of buildings in the city were destroyed by Allied bombing, as were many other German cities and industrial centres. However, a large number of historic sites have been preserved, such as:
The US 78th Infantry Division captured Wuppertal against scant resistance on 16 April 1945. After the last World War, the US held the intellectual ownership rights to Bayer and other German companies and organisations. Wuppertal became a part of the British Zone of Occupation, and subsequently part of the new state of North Rhine-Westphalia in West Germany.
Main sights.
In total, Wuppertal possesses over 4,500 buildings classified as national monuments, most dating from styles as Neoclassicism, Eclecticism, Historicism, Art Nouveau/Jugendstil and Bauhaus.
Main sights include:
One of the city’s greatest attractions is the globally unique suspended monorail "Wuppertaler Schwebebahn", which was established in 1901. The tracks are 8 m above the streets and 12 m above the Wupper River.
Notable people from Wuppertal.
See also: .
Sports.
Association football.
In football, Wuppertal's most popular club is Wuppertaler SV who currently play in the Oberliga Niederrhein, the fifth tier of the German football league system. Playing their home games at the city's Stadion am Zoo, the club, which enjoyed its last season in a nationwide division in the 2009–10 season, looks back on a rich and eventful history since its establishment as the result of a 1954 merger between the two leading Wuppertal clubs "SSV 04 Wuppertal" and "TSG Vohwinkel 80". The club spent a total of seven seasons in the top flight of German football, three of which in the Bundesliga, which they were promoted to in 1972. In their first season in the nationwide first division, the club reached a remarkable fourth place and qualified for the UEFA Cup for the first and only time in its history. After a first-round defeat by Polish side Ruch Chorzów and another two widely unsuccessful Bundesliga campaigns, the club disappeared from the top flight again, though, and has yet to return.
In 2004, the club merged with local rivals "SV Borussia Wuppertal" to form "Wuppertaler SV Borussia", though the name change remained the only visible attribute of the merger with the club's colours and crest remaining unaltered. The additional "Borussia" was scrapped again in 2013 due to fans' demand amidst a change of leadership which was brought about to lead the club through necessary insolvency proceedings which have been completed as of September 2014.
Another noteworthy Wuppertal football club is Cronenberger SC from the district of Cronenberg. Their biggest success to date is reaching the 1952 German amateur football championship final which they lost 5–2 against VfR Schwenningen. Today, they play one tier below WSV in the Landesliga Niederrhein.
Famous players include Günter Pröpper who scored 39 of WSV's 136 Bundesliga goals and West Germany international Horst Szymaniak, as well as Cronenberg's Herbert Jäger who represented Germany at the 1952 Summer Olympics in Helsinki during his stay with the club.
Team handball.
In handball, Wuppertal's most successful team is Bergischer HC, playing in the top-tier Handball-Bundesliga which they were promoted to for the second time in 2013, reaching 15th place in the 2013–14 campaign and therefore staying in the top flight for a second consecutive season. "BHC" originates from a 2006 cooperation between the management, squad and main sponsor of LTV Wuppertal and rivals SG Solingen from the nearby city of the same name. The club advertises itself as a representative of the entire Bergisches Land region. The team plays its home games at both Wuppertal's "Uni-Halle" (3,200 seats) and Solingen's "Klingenhalle" (2,600 seats).
Wuppertal's past most successful club are the aforementioned LTV Wuppertal. LTV spent most of their seasons in the second and third tiers, before they merged with "Wuppertaler SV's" handball section in 1996 to form "HSG LTV/WSV Wuppertal". The handball combination was promoted to the Bundesliga after its inaugural season, finishing 8th before dissolving again in 1998. However, the mere departure of Wuppertaler SV still allowed LTV Wuppertal, whose professional team were renamed "HC Wuppertal", to play another three seasons in the Bundesliga before returning to the 2nd division and re-introducing its old name. After the establishment of BHC in 2006, LTV lost its financial base and was relegated several times, currently playing in the fifth-tier Verbandsliga.
Volleyball.
In volleyball, SV Bayer Wuppertal was one of Germany's leading men's teams for many years during the 1990s and 2000s. The team was part of the well-known mass-sports club originating in Leverkusen and was promoted to the Bundesliga in 1978. Reacting to low attendances, the eponymous Bayer AG decided to relocate the volleyball team to Wuppertal in 1992, where there also was a Bayer-funded club. After the move, the club won various titles, including the German championship in 1994 and 1997 and the German Cup in 1995. In addition to that, they finished runners-up to Greek side Olympiacos S.C. in the 1995-96 European Cup Winners' Cup, losing the final in five sets.
After the wide-reaching retreat of Bayer AG from less popular professional sport in 2008, the club acquired the name "Wuppertal Titans" and later "A!B!C Titans Berg. Land". However, the loss of their main sponsor eventually led to the team having to fold in 2012. Today, they once more play under the name of Bayer Wuppertal in the third-tier Regionalliga, unable to promote with their current financial set-up.
Basketball.
Perhaps one of the most successful Wuppertal sports clubs was the women's basketball team of Barmer TV (known as "BTV Wuppertal" between 1994 and 2000, "BTV Gold-Zack Wuppertal" between 2000 and 2002 and "Wuppertal Wings" internationally). An 11-time German champion and 12-time German Cup winner, they won a remarkable ten consecutive doubles between 1993 and 2002. In 1996, they even won the European Cup as the first and so far only German side, beating Italy's SFT Como in the final. A year later, they narrowly missed out on back-to-back trebles, losing to French side CJM Bourges in the newly christened EuroLeague's final.
In 2002, the club withdrew from the Bundesliga due to financial troubles, their then-main sponsor "Gold-Zack Werke" filing for insolvency a year later. After a decade-long stay in amateur divisions, Barmer TV returned to the second-tier 2nd Bundesliga North in 2014.
Wuppertal co-hosted the 1998 FIBA World Championship for Women as one of seven host cities.
Roller hockey.
In roller hockey (also known as "rink hockey"), Wuppertal club RSC Cronenberg are one of the most successful German teams, having won the German championship and the German Cup in both men's and women's competitions. In total, the men won 13 German championships and nine cups, the women ten championships and nine cups. Both teams play their home games at "Alfred-Henckels-Halle".
Wuppertal hosted several international tournaments, including the World Championship in 1997 (men) and 2004 (women) and the European Championship in 1992, 2010 (men) and 2011 (women).
Education.
Four institutions of higher education are in Wuppertal.
The privately financed "Junior Uni" is an in Germany uniquely initiative to educate youth from the age of 4 to 18 years in science outside the school program.
Transport.
Railways.
Wuppertal is well connected to the rail network. The town lies on the Cologne–Hagen and the Düsseldorf–Hagen railway lines, and is a stop for long-distance traffic. The central station is located in the district of Elberfeld. Regionalbahn trains and some Regional-Express trains also stop at Oberbarmen, Barmen, Ronsdorf and Vohwinkel. There are also S-Bahn stations in Langerfeld, Unterbarmen, Steinbeck, Zoologischer Garten and Sonnborn.
The rail services that operate on the mainline through the valley are the RE 4 (Wupper-Express), RE 7 (Rhein-Münsterland-Express), RE 13 (Maas-Wupper-Express), RB 48 (Rhein-Wupper Bahn) and four Rhine-Ruhr S-Bahn services: the S 7, S 8, S 9 and S 68 (peak hours only). Every 30 minutes, it is served by a long-distance (Intercity-Express, InterCity, EuroCity or City Night Line) service in each direction.
With the exception of the line from Wuppertal to Solingen (operated as the S 7) and the Prince William Railway to Essen (now S-Bahn line S 9), all of the branch lines connecting to main line in the city of Wuppertal are now closed. This includes, among others, the Düsseldorf-Derendorf–Dortmund Süd railway (the "Wuppertaler Nordbahn"), the Burgholz Railway, the Wuppertal-Wichlinghausen–Hattingen railway, the Wupper Valley Railway and the Corkscrew Railway. Thus, there were once 31 stations in the Wuppertal area, including nine stations on the mainline. Nowadays only ten are serviced any more.
There is also the Wuppertal Suspension Railway
International relations.
Twin towns — sister cities.
Wuppertal is twinned with:

</doc>
<doc id="49147" url="http://en.wikipedia.org/wiki?curid=49147" title="Fair trade">
Fair trade

Fair trade is a social movement whose stated goal is to help producers in developing countries achieve better trading conditions and to promote sustainability. Members of the movement advocate the payment of higher prices to exporters, as well as higher social and environmental standards. The movement focuses in particular on commodities, or products which are typically exported from developing countries to developed countries, but also consumed in domestic markets (e.g. Brazil and India) most notably handicrafts, coffee, cocoa, sugar, tea, bananas, honey, cotton, wine, fresh fruit, chocolate, flowers and gold. The movement seeks to promote greater equity in international trading partnerships through dialogue, transparency, and respect. It promotes sustainable development by offering better trading conditions to, and securing the rights of, marginalized producers and workers in developing countries.
Fairtrade labeling organizations most commonly use a definition of "fair trade" developed by FINE, an informal association of four international fair trade networks — Fairtrade Labelling Organizations International, World Fair Trade Organization (WFTO), Network of European Worldshops and European Fair Trade Association (EFTA) —: fair trade is a trading partnership, based on dialogue, transparency, and respect, that seeks greater equity in international trade. Fair trade organizations, backed by consumers, are engaged actively in supporting producers, awareness raising, and in campaigning for changes in the rules and practice of conventional international trade.
There are several recognized Fairtrade certifiers, including Fairtrade International (formerly called FLO, Fairtrade Labelling Organizations International), IMO and Eco-Social. Additionally, Fair Trade USA, formerly a licensing agency for the Fairtrade International label, broke from the system and is implementing its own fair trade labelling scheme, which has resulted in controversy due to its inclusion of independent smallholders and estates for all crops. In 2008, Fairtrade International certified approximately (€3.4B) of products. The World Trade Organization publishes annual figures on the world trade of goods and services.
The movement is especially popular in the UK where there are 500 Fairtrade towns, 118 universities, over 6,000 churches, and over 4,000 UK schools registered in the Fairtrade Schools Scheme. In 2011, over 1.2 million farmers and workers in more than 60 countries participated in Fair Trade, and €65 million in Fairtrade premium was paid. According to Fairtrade International, nearly six out of ten consumers have seen the Fairtrade mark and almost nine in ten of them trust it.
The fair trade system.
There are a large number of fair trade and ethical marketing organizations often employing different marketing strategies. Most Fair Trade products are sold by those Fair Trade organizations that believe it is necessary to market through supermarkets to get sufficient volume of trade to have any real impact on the developing world.
The Fairtrade brand is by far the biggest of the fair trade coffee brands. Packers in developed countries pay a fee to The Fairtrade Foundation for the right to use the brand and logo, and nearly all the fee goes to marketing. Packers and retailers can charge as much as they want for the coffee. The coffee has to come from a certified Fairtrade cooperative, and there is a minimum price when the world market is oversupplied. Additionally, the cooperatives are paid an additional 10c per lb premium by buyers for community development projects. The cooperatives can, on average, sell only a third of their output as Fairtrade, because of lack of demand, and sell the rest at world prices. The exporting cooperative can spend the money in several ways. Some go to meeting the costs of conformity and certification: as they have to meet Fairtrade standards on all their produce, they have to recover the costs from a small part of their turnover, sometimes as little as 8%, and may not make any profit. Some meet other costs. Some is spent on social projects such as building schools, clinics and baseball pitches. Sometimes there is money left over for the farmers. The cooperatives sometimes pay farmers a higher price than farmers do, sometimes less, but there is no evidence on which is more common.
The marketing system for Fairtrade and non-Fairtrade coffee is identical in the consuming countries, using mostly the same importing, packing, distributing and retailing firms. Some independent brands operate a virtual company, paying importers, packers and distributors and advertising agencies to handle their brand, for cost reasons. In the producing country Fairtrade is marketed only by Fairtrade cooperatives, while other coffee is marketed by Fairtrade cooperatives (as uncertified coffee), by other cooperatives and by ordinary traders.
To become certified Fairtrade producers, the primary cooperative and its member farmers must operate to certain political standards, imposed from Europe. FLO-CERT, the for-profit side, handles producer certification, inspecting and certifying producer organizations in more than 50 countries in Africa, Asia, and Latin America. In the Fair trade debate there are many complaints of failure to enforce these standards, with producers, cooperatives, importers and packers profiting by evading them.
There remain many Fair Trade organizations that adhere to a greater or smaller degree to the original objectives of Fair Trade, and that market products through alternative channels where possible, and market through specialist Fair Trade shops, but they have a small proportion of the total market.
General structure of the movement.
Most fair trade import organizations are members of, or certified by one of several national or international federations. These federations coordinate, promote, and facilitate the work of fair trade organizations. The following are some of the largest:
In 1998, the first four federations listed above joined together as FINE, an informal association whose goal is to harmonize fair trade standards and guidelines, increase the quality and efficiency of fair trade monitoring systems, and advocate fair trade politically.
Student groups have also been increasingly active in the past years promoting fair trade products. Although hundreds of independent student organizations are active worldwide, most groups in North America are either affiliated with United Students for Fair Trade (USA, the Canadian Student Fair Trade Network (Canada), or Fair Trade Campaigns (USA), which also houses Fair Trade Universities and Fair Trade Schools.
The involvement of church organizations has been and continues to be an integral part of the Fair Trade movement:
History.
The first attempts to commercialize fair trade goods in Northern markets were initiated in the 1940s and 1950s by religious groups and various politically oriented non-governmental organizations (NGOs). Ten Thousand Villages, an NGO within the Mennonite Central Committee (MCC) and SERRV International were the first, in 1946 and 1949 respectively, to develop fair trade supply chains in developing countries. The products, almost exclusively handicrafts ranging from jute goods to cross-stitch work, were mostly sold in churches or fairs. The goods themselves had often no other function than to indicate that a donation had been made.
Solidarity trade.
The current fair trade movement was shaped in Europe in the 1960s. Fair trade during that period was often seen as a political gesture against neo-imperialism: radical student movements began targeting multinational corporations and concerns that traditional business models were fundamentally flawed started to emerge. The slogan at the time, "Trade not Aid", gained international recognition in 1968 when it was adopted by the United Nations Conference on Trade and Development (UNCTAD) to put the emphasis on the establishment of fair trade relations with the developing world.
The year 1965 saw the creation of the first Alternative Trading Organization (ATO): that year, British NGO Oxfam launched "Helping-by-Selling", a program which sold imported handicrafts in Oxfam stores in the UK and from mail-order catalogues.
By 1968, the oversized newsprint publication, the Whole Earth Catalog, was connecting thousands of specialized merchants, artisans, and scientists directly with consumers who were interested in supporting independent producers, with the goal of bypassing corporate retail and department stores. The Whole Earth Catalog sought to balance the international free market by allowing direct purchasing of goods produced primarily in the United States and Canada, but also in Central and South America.
In 1969, the first worldshop opened its doors in the Netherlands. The initiative aimed at bringing the principles of fair trade to the retail sector by selling almost exclusively goods produced under fair trade terms in "underdeveloped regions". The first shop was run by volunteers and was so successful that dozens of similar shops soon went into business in the Benelux countries, Germany, and other Western European countries.
Throughout the 1960s and 1970s, important segments of the fair trade movement worked to find markets for products from countries that were excluded from the mainstream trading channels for political reasons. Thousands of volunteers sold coffee from Angola and Nicaragua in worldshops, in the back of churches, from their homes, and from stands in public places, using the products as a vehicle to deliver their message: give disadvantaged producers in developing countries a fair chance on the world’s market.
Handicrafts vs. agricultural goods.
In the early 1980s, Alternative Trading Organizations faced major challenges: the novelty of some fair trade products began to wear off, demand reached a plateau, and some handicrafts began to look "tired and old fashioned" in the marketplace. The decline of segments of the handicrafts market forced fair trade supporters to rethink their business model and their goals. Moreover, several fair trade supporters during this period were worried by the contemporary impact on small farmers of structural reforms in the agricultural sector as well as the fall in commodity prices. Many of them came to believe it was the movement's responsibility to address the issue and remedies usable in the ongoing crisis in the industry.
In the subsequent years, fair trade agricultural commodities played an important role in the growth of many ATOs: successful on the market, they offered a much-needed, renewable source of income for producers and provided Alternative Trading Organizations a complement to the handicrafts market. The first fair trade agricultural products were tea and coffee, quickly followed by: dried fruits, cocoa, sugar, fruit juices, rice, spices and nuts. While in 1992, a sales value ratio of 80% handcrafts to 20% agricultural goods was the norm, in 2002 handcrafts amounted to 25.4% of fair trade sales while commodity food lines were up at 69.4%.
Rise of labeling initiatives.
Sales of fair trade products only really took off with the arrival of the first Fairtrade certification initiatives. Although buoyed by ever growing sales, fair trade had been generally contained to relatively small worldshops scattered across Europe and to a lesser extent, North America. Some felt that these shops were too disconnected from the rhythm and the lifestyle of contemporary developed societies. The inconvenience of going to them to buy only a product or two was too high even for the most dedicated customers. The only way to increase sale opportunities was to start offering fair trade products where consumers normally shop, in large distribution channels. The problem was to find a way to expand distribution without compromising consumer trust in fair trade products and in their origins.
A solution was found in 1988, when the first Fairtrade certification initiative, Max Havelaar, was created in the Netherlands under the initiative of Nico Roozen, Frans Van Der Hoff, and Dutch development NGO Solidaridad. The independent certification allowed the goods to be sold outside the worldshops and into the mainstream, reaching a larger consumer segment and boosting fair trade sales significantly. The labeling initiative also allowed customers and distributors alike to track the origin of the goods to confirm that the products were really benefiting the producers at the end of the supply chain.
The concept caught on: in the ensuing years, similar non-profit Fairtrade labelling organizations were set up in other European countries and North America. In 1997, a process of convergence among labelling organizations – or "LIs" (for "Labeling Initiatives") – led to the creation of Fairtrade Labelling Organizations International (FLO). FLO is an umbrella organization whose mission is to set the Fairtrade standards, support, inspect and certify disadvantaged producers, and harmonize the Fairtrade message across the movement.
In 2002, FLO launched for the first time an International Fairtrade Certification Mark. The goals of the launch were to improve the visibility of the Mark on supermarket shelves, facilitate cross border trade, and simplify procedures for both producers and importers. At present, the certification mark is used in over 50 countries and on dozens of different products, based on FLO’s certification for coffee, tea, rice, bananas, mangoes, cocoa, cotton, sugar, honey, fruit juices, nuts, fresh fruit, quinoa, herbs and spices, wine, footballs, etc.
Product certification.
"Note: Customary spelling of Fairtrade is one word when referring to the FLO product labeling system, see Fairtrade certification"
Fairtrade labelling (usually simply Fairtrade or Fair Trade Certified in the United States) is a certification system designed to allow consumers to identify goods which meet agreed standards. Overseen by a standard-setting body (FLO International) and a certification body (FLO-CERT), the system involves independent auditing of producers and traders to ensure the agreed standards are met.
For a product to carry either the International Fairtrade Certification Mark or the Fair Trade Certified Mark, it must come from FLO-CERT inspected and certified producer organizations. The crops must be grown and harvested in accordance with the international Fair trade standards set by FLO International. The supply chain must also have been monitored by FLO-CERT, to ensure the integrity of the labelled product.
Fairtrade certification purports to guarantee not only fair prices, but also the principles of ethical purchasing. These principles include adherence to ILO agreements such as those banning child and slave labour, guaranteeing a safe workplace and the right to unionise, adherence to the United Nations charter of human rights, a fair price that covers the cost of production and facilitates social development, and protection and conservation of the environment. The Fairtrade certification system also attempts to promote long-term business relationships between buyers and sellers, crop prefinancing, and greater transparency throughout the supply chain and more.
The Fairtrade certification system covers a growing range of products, including bananas, honey, coffee, oranges, Cocoa bean, cocoa, cotton, dried and fresh fruits and vegetables, juices, nuts and oil seeds, quinoa, rice, spices, sugar, tea, and wine. Companies offering products that meet the Fairtrade standards may apply for licences to use one of the Fairtrade Certification Marks for those products.
The International Fairtrade Certification Mark was launched in 2002 by FLO, and replaced twelve Marks used by various Fairtrade labelling initiatives. The new Certification Mark is currently used worldwide (with the exception of the United States). The Fair Trade Certified Mark is still used to identify Fairtrade goods in the United States.
There is widespread confusion because the fair trade industry standards provided by Fairtrade International (The Fairtrade Labelling Organization) use the word “producer” in many different senses, often in the same specification document. Sometimes it refers to farmers, sometimes to the primary cooperatives they belong to, to the secondary cooperatives that the primary cooperatives belong to, or to the tertiary cooperatives that the secondary cooperatives may belong to but “Producer [also] means any entity that has been certified under the Fairtrade International Generic Fairtrade Standard for Small Producer Organizations, Generic Fairtrade Standard for Hired Labour Situations, or Generic Fairtrade Standard for Contract Production.” The word is used in all these meanings in key documents. In practice, when price and credit are discussed, “producer” means the exporting organization, “For small producers’ organizations, payment must be made directly to the certified small producers’ organization”. and “In the case of a small producers’ organization [e.g. for coffee], Fairtrade Minimum Prices are set at the level of the Producer Organization, not at the level of individual producers (members of the organization)” which means that the “producer” here is half way up the marketing chain between the farmer and the consumer. The part of the standards referring to cultivation, environment, pesticides and child labour has the farmer as “producer”.
WFTO Fair Trade Organization membership.
In an effort to complement the Fairtrade product certification system and allow most notably handcraft producers to also sell their products outside worldshops, the World Fair Trade Organization (WFTO) launched in 2004 a new Mark to identify fair trade organizations (as opposed to products in the case of FLO International and Fairtrade). Called the , it allows consumers to recognize registered Fair Trade Organizations worldwide and seeks to guarantee that standards are being implemented regarding working conditions, wages, child labour, and the environment. The FTO Mark offers Fair Trade Organizations (including handcrafts producers) definable standards which inform consumers, business partners, governments, and donors of the applicable trading standard. in 2014 the was introduced to create new sales opportunities, initially for cocoa, sugar and cotton producers. It has the same round logo alongside the word FAIRTRADE in black and underneath the program title in turquoise.
Alternative trading organizations.
 An alternative trading organization (ATO) is usually a non-governmental organization (NGO) or mission-driven business aligned with the Fair Trade movement, aiming "to contribute to the alleviation of poverty in developing regions of the world by establishing a system of trade that allows marginalized producers in developing regions to gain access to developed markets".
Alternative trading organizations have Fair Trade at the core of their mission and activities, using it as a development tool to support disadvantaged producers and to reduce poverty, and combine their marketing with awareness-raising and campaigning.
Alternative trading organizations are often, but not always, based in political and religious groups, though their secular purpose precludes sectarian identification and evangelical activity. Philosophically, the grassroots political-action agenda of these organizations associates them with progressive political causes active since the 1960s: foremost, a belief in collective action and commitment to moral principles based on social, economic and trade justice.
According to EFTA, the defining characteristic of alternative trading organizations is that of equal partnership and respect - partnership between the developing region producers and importers, shops, labelling organizations, and consumers. Alternative trade "humanizes" the trade process - making the producer-consumer chain as short as possible so that consumers become aware of the culture, identity, and conditions in which producers live. All actors are committed to the principle of alternative trade, the need for advocacy in their working relations and the importance of awareness-raising and advocacy work.
Examples of such organisations are Ten Thousand Villages, Equal Exchange and SERRV International in the US and Equal Exchange Trading, Traidcraft, Oxfam Trading, Twin Trading and Alter Eco in Europe as well as Siem Fair Trade Fashion in Australia. (see the Alternative Trading Organization page for further examples).
Worldshops.
Worldshops or fair trade shops are specialized retail outlets offering and promoting fair trade products. Worldshops also typically organize various educational fair trade activities and play an active role in trade justice and other North-South political campaigns.
Worldshops are often not-for-profit organizations and run by locally based volunteer networks.
Although the movement emerged in Europe and a vast majority of worldshops are still based on the continent, worldshops can also be found today in North America, Australia and New Zealand.
Worldshops' aim is to make trade as direct and fair with the trading partners as possible. Usually, this means a producer in a developing country and consumers in industrialized countries. The worldshops' target is to pay the producers a fair price that guarantees substinence and guarantees positive social development. They often cut out any intermediaries in the import chain.
A web movement has recently begun to provide fair trade items at fair prices to the consumers. One popular one is Fair Trade a Day where a different fair trade item is featured each day.
World wide.
International.
Every year the sales of Fair Trade products grow close to 30% and in 2004 were worth over $500 million USD. In the case of coffee, sales grow nearly 50% per year in certain countries. In 2002, 16,000 tons of Fairtrade coffee was purchased by consumers in 17 countries. “Fair trade coffee is currently produced in 24 countries in Latin America, Africa and Asia”. The 165 FLO associations in Latin America and Caribbean are located in 14 countries and together export over 85% of the world’s Fair Trade coffee. There is a North/South divide of fair trade products with producers in the South and consumers in the North. Discrepancies in the perspectives of these southern producers and northern consumers are often the source of ethical dilemmas such as how the purchasing power of consumers may or may not promote the development of southern countries. Amidst the continuous growth, "[p]urchasing patterns of fairtrade products have remained strong despite the global economic downturn. In 2008, global sales of fairtrade products exceeded US$ 3.5 billion."
Africa.
Africa’s exports come from the places such as South Africa, Ghana, Uganda, Tanzania and Kenya. These exports are valued at $24 million USD. Between the years of 2004 and 2006 Africa quickly expanded their number of FLO certified producer groups, rising from 78 to 171; nearly half of which reside in Kenya, following closely behind are Tanzania and South Africa. The FLO products Africa is known for are tea, cocoa, flowers and wine. In Africa there are smallholder cooperatives and plantations which produce Fair Trade certified tea. Cocoa-producing countries in West Africa often form cooperatives that produce fair trade cocoa such as Kuapa Kokoo in Ghana. West African countries without strong fair trade industries are subject to deterioration in cocoa quality as they compete with other countries for a profit. These countries include Cameroon, Nigeria, and the Ivory Coast.
Latin America.
Studies in the early 2000s show that the income, education and health of coffee producers involved with Fair Trade in Latin America were improved, versus producers who were not participating. Brazil, Nicaragua, Peru and Guatemala, having the biggest population of coffee producers, make use of some of the most substantial land for coffee production in Latin America and do so by taking part in Fair Trade. Countries in Latin America are also large exporters of fair trade bananas. The Dominican Republic is the largest producer of fair trade bananas, followed by Mexico, Ecuador, and Costa Rica. Producers in the Dominican Republic have set up associations rather than cooperatives so that individual farmers can each own their own land but meet regularly. Fundación Solidaridad was created in Chile to increase the earnings and social participation of handicraft producers. These goods are marketed locally in Chile and internationally.
Asia.
The Asia Fair Trade Forum aims to increase the competency of fair trade organizations in Asia so they can be more competitive in the global market. Garment factories in Asian countries including China, Burma, and Bangladesh consistently receive charges of human rights violations, including the use of child labor. These violations conflict with the principles outlined by fair trade certifiers. In India, Trade Alternative Reform Action (Tara) Projects formed in the 1970s have worked to increase production capacity, quality standards, and entrance into markets for home-based craftsmen that were previously unattainable due to their lower caste identity.
Fair trade commodities.
Fair trade commodities are goods that have been exchanged from where they were grown or made to where they are purchased, and have been certified by a fair trade certification organization, such as Fair Trade USA or World Fair Trade Organization. Such organizations are typically overseen by Fairtrade International. Fairtrade International sets international fair trade standards and supports fair trade producers and cooperatives. Sixty percent of the fair trade market revolves around food products such as coffee, tea, cocoa, honey, and bananas. Non-food commodities include crafts, textiles, and flowers. It has been suggested by Shima Baradaran of Brigham Young University that fair trade techniques could be productively applied to products which might involve child labor. Although fair trade represents only .01% of the food and beverage industry in the United States, it is growing rapidly and may become a significant portion of the national food and beverage industry.
Coffee.
Coffee is the most well-established fair trade commodity. Growth in the fair trade coffee industry has extended the commodity away from solely small farms and companies. Now multinational corporations such as Starbucks and Nestle use fair trade coffee.
Locations.
The largest sources of fair trade coffee are Uganda and Tanzania, followed by Latin American countries such as Guatemala and Costa Rica. As of 1999, major importers of fair trade coffee included Germany, the Netherlands, Switzerland, and the United Kingdom. There is a North/South divide between fair trade consumers and producers. North American countries are not yet among the top importers of fair trade coffee.
Labour.
Starbucks began to purchase more fair trade coffee in 2001 because of charges of labor rights violations in Central American plantations. Several competitors, including Nestle, followed suit. Large corporations that sell non-fair trade coffee take 55% of what consumers pay for coffee while only 10% goes to the producers. Small growers dominate the production of coffee, especially in Latin American countries such as Peru. Coffee is the fastest expanding fairly traded commodity, and an increasing number of producers are small farmers that own their own land and work in cooperatives. Even the incomes of growers of fair trade coffee beans depend on the market value of coffee where it is consumed, so farmers of fair trade coffee do not necessarily live above the poverty line or get completely fair prices for their commodity.
Unsustainable farming practices can harm plantation owners and laborers. Unsustainable practices such as using chemicals and unshaded growing are risky. Small growers who put themselves at economic risk by not having diverse farming practices could lose money and resources due to fluctuating coffee prices, pest problems, or policy shifts.
Sustainability.
As coffee becomes one of the most important export crops in certain regions such as northern Latin America, nature and agriculture are transformed. Increased productivity requires technological innovations, and the coffee agroecosystem has been changing rapidly. In the nineteenth century in Latin America, coffee plantations slowly began replacing sugarcane and subsistence crops. Coffee crops became more managed; they were put into rows and unshaded, meaning diversity of the forest was decreased and Coffea trees were shorter. As plant and tree diversity decreased, so did animal diversity. Unshaded plantations allow for a higher density of Coffea trees, but negative effects include less protection from wind and more easily eroded soil. Technified coffee plantations also use chemicals such as fertilizers, insecticides, and fungicides.
Fair trade certified commodities must adhere to sustainable agro-ecological practices, including reduction of chemical fertilizer use, prevention of erosion, and protection of forests. Coffee plantations are more likely to be fair trade certified if they use traditional farming practices with shading and without chemicals. This protects the biodiversity of the ecosystem and ensures that the land will be usable for farming in the future and not just for short-term planting. In the United States, 85% of fair trade certified coffee is also organic.
Consumer attitudes.
Consumers typically have positive attitudes for products that are ethically made. These products may include promises of fair labor conditions, protection of the environment, and protection of human rights. All fair trade products must meet standards such as these. Despite positive attitudes toward ethical products including fair trade commodities, consumers often are not willing to pay the higher price associated with fair trade coffee. The attitude-behavior gap can help explain why ethical and fair trade products take up less than 1% of the market. Coffee consumers can say they would be willing to pay a higher premium for fair trade coffee, but most consumers are actually more concerned with the brand, label, and flavor of the coffee. However, socially conscious consumers with a commitment to buying fair trade products are more likely to pay the premium associated with fair trade coffee. Once enough consumers begin purchasing fair trade, companies are more likely to carry fair trade products. Safeway Inc. began carrying fair trade coffee after individual consumers dropped off postcards asking for it.
Fair trade coffee companies.
Following are coffee roasters and companies that offer fair trade coffee or some roasts that are fair trade certified:
Cocoa.
Many countries that export cocoa rely on cocoa as their single export crop. In Africa in particular, governments tax cocoa as their main source of revenue. Cocoa is a permanent crop, which means that it occupies land for long periods of time and does not need to be replanted after each harvest.
Locations.
Cocoa is farmed in the tropical regions of West Africa, Southeast Asia, and Latin America. In Latin America, cocoa is produced in Costa Rica, Panama, Peru, Bolivia, and Brazil. Much of the cocoa produced in Latin America is a organic and regulated by an Internal control system. Bolivia has fair trade cooperatives that permit a fair share of money for cocoa producers. African cocoa-producing countries include Cameroon, Madagascar, São Tomé and Príncipe, Ghana, Tanzania, Uganda, and Côte d'Ivoire. Côte d'Ivoire exports over a third of the world's cocoa beans. Southeast Asia accounts for about 14% of the world's cocoa production. Major cocoa-producing countries are Indonesia, Malaysia, and Papua New Guinea.
Labor.
One suggestion for the reason that laborers in Africa are marginalized in world trade is because the colonial division of labor kept Africa from developing its own industries. Africa and other developing countries received low prices for their exported commodities such as cocoa, which caused poverty to abound. Fair trade seeks to establish a system of direct trade from developing countries to counteract this unfair system. Most cocoa comes from small family-run farms in West Africa. These farms have little market access and thus rely on middlemen to bring their products to market. Sometimes middlemen are unfair to farmers. Farmers do not get a fair price for their product despite relying on cocoa sales for the majority of their income. One solution for fair labor practices is for farmers to become part of an Agricultural cooperative. Cooperatives pay farmers a fair price for their cocoa so farmers have enough money for food, clothes, and school fees. One of the main tenets of fair trade is that farmers receive a fair price, but this does not mean that the larger amount of money paid for fair trade cocoa goes directly to the farmers. In reality, much of this money goes to community projects such as water wells rather than to individual farmers. Nevertheless, cooperatives such as fair trade-endorsed Kuapa Kokoo in Ghana are often the only Licensed Buying Companies that will give farmers a fair price and not cheat them or rig sales. Farmers in cooperatives are frequently their own bosses and get bonuses per bag of cocoa beans. These arrangements are not always assured and fair trade organizations can't always buy all of the cocoa available to them from cooperatives.
The effectiveness of Fairtrade is questionable; workers on Fairtrade farms have a lower standard of living than on similar farms outside the Fairtrade system.
Marketing.
The marketing of fair trade cocoa to European consumers often portrays the cocoa farmers as dependent on western purchases for their livelihood and well-being. Showing African cocoa producers in this way is problematic because it is reminiscent of the imperialistic view that Africans cannot live happily without the help of westerners. It puts the balance of power in favor of the consumers rather than the producers.
Consumers often aren't willing to pay the extra price for fair trade cocoa because they do not know what fair trade is. Activist groups are vital in educating consumers about the unethical aspects of unfair trade and promoting demand for fairly traded commodities. Activism and ethical consumption not only promote fair trade but also act against powerful corporations such as Mars, Incorporated that refuse to acknowledge the use of forced child labor in the harvesting of their cocoa.
Sustainability.
Smallholding farmers not only frequently lack access to markets, they lack access to resources that lead to sustainable cocoa farming practices. Lack of sustainability can be due to pests, diseases that attack cocoa trees, lack of farming supplies, and lack of knowledge about modern farming techniques. One issue pertaining to cocoa plantation sustainability is the amount of time it takes for a cocoa tree to produce pods. A solution to this is to change the type of cocoa tree being farmed. In Ghana, a hybrid cocoa tree yields two crops after three years rather than the typical one crop after five years.
Fair trade cocoa companies.
Following are chocolate companies that use all or some fair trade cocoa in their chocolate:
Textiles.
Fair trade textiles are primarily made from fair trade cotton. By 2015, almost 75,000 cotton farmers in developing countries have obtained Fairtrade certification. The minimum price that Fair trade pays allows cotton farmers to sustain and improve their livelihoods. Fair trade textiles are frequently grouped with fair trade crafts and goods made by artisans in contrast to cocoa, coffee, sugar, tea, and honey, which are agricultural commodities.
Locations.
India, Pakistan and West Africa are the primary exporters of fair trade cotton, although many countries grow fair trade cotton. Textiles and clothing are exported from Hong Kong, Thailand, Malaysia, and Indonesia.
Labour.
Labour is different for textile production than for agricultural commodities because textile production takes place in a factory, not on a farm. Children provide a source of cheap labor, and child labor is prevalent in Pakistan, India, and Nepal. Fair trade cooperatives ensure fair and safe labor practices, including disallowing child labor. Fair trade textile producers are most often women in developing countries. They struggle with meeting the consumer tastes in North America and Europe. In Nepal, textiles were originally made for household and local use. In the 1990s, women began joining cooperatives and exporting their crafts for profit. Now handicrafts are Nepal's largest export. It is often difficult for women to balance textile production, domestic responsibilities, and agricultural work. Cooperatives foster the growth of democratic communities in which women have a voice despite being historically in underprivileged positions. For fair trade textiles and other crafts to be successful in western markets, World Fair Trade Organizations require a flexible workforce of artisans in need of stable income, links from consumers to artisans, and a market for quality ethnic products. However, making cotton and textiles fair trade does not always have a positive impact on laborers. Burkina Faso and Mali export the largest amount of cotton in Africa. Although many cotton plantations in these countries attained fair trade certification in the 1990s, participation in fair trade further ingrains existing power relations and inequalities that cause poverty in Africa rather than challenging them. Fair trade does not do much for farmers when it does not challenge the system that marginalizes producers. Despite not empowering farmers, the change to fair trade cotton has positive effects including female participation in cultivation.
Textiles and garments are intricate and require one individual operator, in contrast to the collective farming of coffee and cocoa beans. Textiles are not a straightforward commodity because to be fairly traded, there must be regulation in cotton cultivation, dyeing, stitching, and every other step in the process of textile production. Fair trade textiles must not be confused with the sweat-free movement although the two movements intersect at the worker level.
Forced or unfair labor in textile production is not limited to developing countries. Charges of use of sweatshop labor are endemic in the United States. Immigrant women work long hours and receive less than minimum wage. In the United States, there is more of a stigma against child labor than forced labor in general. Consumers in the United States are willing to suspend the importation of textiles made with child labor in other countries but do not expect their exports to be suspended by other countries, even when produced using forced labor.
Fair trade clothing and textile companies.
Following are companies that use fair trade production and/or distribution techniques for clothing and textiles:
Large companies and fair trade commodities.
Large transnational companies have begun to use fair trade commodities in their products. In April 2000, Starbucks began offering fair trade coffee in all of their stores. In 2005, the company promised to purchase ten million pounds of fair trade coffee over the next 18 months. This would account for a quarter of the fair trade coffee purchases in the United States and 3% of Starbucks' total coffee purchases. The company maintains that increasing its fair trade purchases would require an unprofitable reconstruction of the supply chain. Fair trade activists have made gains with other companies: Sara Lee in 2002 and Procter & Gamble (the maker of Folgers) in 2003 agreed to begin selling a small amount of fair trade coffee. Nestle, the world's biggest coffee trader, began selling a blend of fair trade coffee in 2005. In 2006, The Hershey Company acquired Dagoba, an organic and fair trade chocolate brand.
Much contention surrounds the issue of fair trade products becoming a part of large companies. Starbucks is still only 3% fair trade - enough to appease consumers, but not enough to make a real difference to small farmers, according to some activists. The ethics of buying fair trade from a company that is not committed to the cause are questionable; these products are only making a small dent in a big company even though these companies' products account for a significant portion of global fair trade.
Luxury Commodities.
There have been efforts to introduce fair trade practices to the luxury industry, particularly for gold and diamonds.
Diamonds and Sourcing.
In parallel to efforts to commoditize diamonds, some industry players have launched campaigns to introduce benefits to mining centers in the developing world. Rapaport Fair Trade was established with the goal "to provide ethical education for jewelry suppliers, buyers, first time or seasoned diamond buyers, social activists, students, and anyone interested in jewelry, trends, and ethical luxury."
The company's founder, Martin Rapaport, as well as Kimberley Process initiators Ian Smillie and Global Witness, are among several industry insiders and observers who have called for greater checks and certification programs among many other programs that would ensure protection for miners and producers in developing countries. Smillie and Global Witness have since withdrawn support for the Kimberley Process.
Other concerns in the diamond industry include working conditions in diamond cutting centers as well as the use of child labor. Both of these concerns come up when considering issues in Surat, India.
Gold.
Brilliant Earth has committed itself to using fair-trade-certified gold. In February 2011, the United Kingdom's Fairtrade Foundation became the first NGO to begin certifying gold under the fair trade rubric. Fair Trade USA, however, hadn't taken that step as of summer 2012.
Politics.
European Union.
In 1994, the European Commission prepared the "Memo on alternative trade" in which it declared its support for strengthening Fair Trade in the South and North and its intention to establish an EC Working Group on Fair Trade. Furthermore, the same year, the European Parliament adopted the "Resolution on promoting fairness and solidarity in North South trade" (OJ C 44, 14.2.1994), a resolution voicing its support for fair trade.
In 1996, the Economic and Social Committee adopted an "Opinion on the European 'Fair Trade' marking movement". A year later, in 1997, the document was followed by a resolution adopted by the European Parliament, calling on the Commission to support Fair Trade banana operators. The same year, the European Commission published a survey on "Attitudes of EU consumers to Fair Trade bananas", concluding that Fair Trade bananas would be commercially viable in several EU Member States.
In 1998, the European Parliament adopted the "Resolution on Fair Trade" (OJ C 226/73, 20.07.1998), which was followed by the Commission in 1999 that adopted the "Communication from the Commission to the Council on 'Fair Trade'" COM(1999) 619 final, 29.11.1999.
In 2000, public institutions in Europe started purchasing Fairtrade Certified coffee and tea. Furthermore, that year, the Cotonou Agreement made specific reference to the promotion of Fair Trade in article 23(g) and in the Compendium. The European Parliament and Council Directive 2000/36/EC also suggested promoting Fair Trade.
In 2001 and 2002, several other EU papers explicitly mentioned fair trade, most notably the 2001 Green Paper on Corporate Social Responsibility and the 2002 Communication on Trade and Development.
In 2004, the European Union adopted the "Agricultural Commodity Chains, Dependence and Poverty – A proposal for an EU Action Plan", with a specific reference to the Fair Trade movement which has "been setting the trend for a more socio-economically responsible trade." (COM(2004)0089).
In 2005, in the European Commission communication "Policy Coherence for Development – Accelerating progress towards attaining the Millennium Development Goals", (COM(2005) 134 final, 12.04.2005), fair trade is mentioned as "a tool for poverty reduction and sustainable development".
And finally, on July 6 in 2006, the European Parliament unanimously adopted a resolution on fair trade, recognizing the benefits achieved by the Fair Trade movement, suggesting the development of an EU-wide policy on Fair Trade, defining criteria that need to be fulfilled under fair trade to protect it from abuse and calling for greater support to Fair Trade (EP resolution "Fair Trade and development", 6 July 2006). "This resolution responds to the impressive growth of Fair Trade, showing the increasing interest of European consumers in responsible purchasing," said Green MEP Frithjof Schmidt during the plenary debate. Peter Mandelson, EU Commissioner for External Trade, responded that the resolution will be well received at the Commission. "Fair Trade makes the consumers think and therefore it is even more valuable. We need to develop a coherent policy framework and this resolution will help us."
France.
In 2005, French parliament member Antoine Herth issued the report "40 proposals to sustain the development of Fair Trade". The report was followed the same year by a law, proposing to establish a commission to recognize fair trade Organisations (article 60 of law no. 2005-882, Small and Medium Enterprises, 2 August 2005).
In parallel to the legislativents, also in 2006, the French chapter of ISO (AFNOR) adopted a reference document on Fair Trade after five years of discussion.
Italy.
In 2006, Italian lawmakers started debating how to introduce a law on fair trade in Parliament. A consultation process involving a wide range of stakeholders was launched in early October. A common definition of fair trade was most notably developed. However, its adoption is still pending as the efforts were stalled by the 2008 Italian political crisis.
Netherlands.
The Dutch province of Groningen was sued in 2007 by coffee supplier Douwe Egberts for explicitly requiring its coffee suppliers to meet fair trade criteria, most notably the payment of a minimum price and a development premium to producer cooperatives. Douwe Egberts, which sells a number of coffee brands under self-developed ethical criteria, believed the requirements were discriminatory. After several months of discussions and legal challenges, the province of Groningen prevailed in a well-publicized judgement. Coen de Ruiter, director of the Max Havelaar Foundation, called the victory a landmark event: "it provides governmental institutions the freedom in their purchasing policy to require suppliers to provide coffee that bears the fair trade criteria, so that a substantial and meaningful contribution is made in the fight against poverty through the daily cup of coffee".
Criticisms.
In spite of studies showing Fair Trade efficiency, some studies have shown limitations to Fair Trade benefits. Sometimes the criticism is intrinsic to Fair Trade, sometimes efficiency depends on the broader context such as the lack of government help or volatile coffee prices in the global market.
Ethical basis of criticisms.
Consumers have been shown to be content paying higher prices for Fairtrade products, in the belief that this helps the very poor. The main ethical criticism of Fairtrade is that this premium over non-Fairtrade products does not reach the producers and is instead collected by businesses, employees of co-operatives or used for unnecessary expenses. Furthermore, research has cited the implementation of certain Fairtrade standards as a cause for greater inequalities in markets where these rigid rules are inappropriate for the specific market.
What happens to the money.
Little money may reach the developing countries.
The Fairtrade Foundation does not monitor how much extra retailers charge for Fairtrade goods, so it is rarely possible to determine how much extra is charged or how much reaches the producers, in spite of the Unfair Trading legislation. In four cases it has been possible to find out. One British café chain was passing on less than one percent of the extra charged to the exporting cooperative; in Finland, Valkila, Haaparanta and Niemi found that consumers paid much more for Fairtrade, and that only 11.5% reached the exporter. Kilian, Jones, Pratt and Villalobos talk of US Fairtrade coffee getting $5 per lb extra at retail, of which the exporter would have received only 2%. Mendoza and Bastiaensen calculated that in the UK only 1.6% to 18% of the extra charged for one product line reached the farmer. All these studies assume that the importers paid the full Fairtrade price, which is not necessarily the case.
Less money reaches farmers.
The Fairtrade Foundation does not monitor how much of the extra money paid to the exporting cooperatives reaches the farmer. The cooperatives incur costs in reaching the Fairtrade political standards, and these are incurred on all production, even if only a small amount is sold at Fairtrade prices. The most successful cooperatives appear to spend a third of the extra price received on this: some less successful cooperatives spend more than they gain. While this appears to be agreed by proponents and critics of Fairtrade, there is a dearth of economic studies setting out the actual revenues and what the money was spent on. FLO figures are that 40% of the money reaching the developing world is spent on ‘business and production’ which would include these costs, as well as costs incurred by any inefficiency and corruption in the cooperative or the marketing system. The rest is stated to be spent on social projects, rather than being passed on to farmers.
There is no evidence that Fairtrade farmers get higher prices on average. Anecdotes state that farmers were paid more or less by traders than by Fairtrade cooperatives. Few of these anecdotes address the problems of price reporting in developing world markets, and few appreciate the complexity of the different price packages which may or may not include credit, harvesting, transport, processing, etc. Cooperatives typically average prices over the year, so they pay less than traders at some times, more at others. Bassett (2009) is able to compare prices only where Fairtrade and non-Fairtrade farmers have to sell cotton to the same monopsonistic ginneries which pay low prices. Prices would have to be higher to compensate farmers for the increased costs they incur to produce Fairtrade. For instance, Fairtrade encouraged Nicaraguan farmers to switch to organic coffee, which resulted in a higher price per pound, but a lower net income because of higher costs and lower yields.
Inefficient marketing system.
One reason for high prices is that Fairtrade farmers have to sell through a monopsonist cooperative, which may be inefficient or corrupt – certainly some private traders are more efficient than some cooperatives. They cannot choose the buyer who offers the best price, or switch when their cooperative is going bankrupt if they wish to retain fairtrade status. There are also complaints that Fairtrade deviates from the free market ideal of some economists. Brink calls fair trade a "misguided attempt to make up for market failures" encouraging market inefficiencies and overproduction.
Corruption.
The Fair Trade marketing system provides more opportunities for corruption than the normal marketing system; and less possibility of, or incentive for, controlling corruption. Corruption has been noted in false labelling of coffee as Fairtrade by retailers and by packers in the developing countries, paying exporters less than the Fairtrade price for Fairtrade coffee (kickbacks) failure to provide the credit and other services specified theft or preferential treatment for ruling elites of cooperatives not paying laborers the specified minimum wage
Fairtrade harms other farmers.
Overproduction argument.
Critics argue that Fairtrade harms all non-Fairtrade farmers. Fairtrade claims that its farmers are paid higher prices and are given special advice on increasing yields and quality. Economists state that, if this is indeed so, Fairtrade farmers will increase production. As the demand for coffee is highly inelastic, a small increase in supply means a large fall in market price, so perhaps a million Fairtrade farmers get a higher price and 24 million others get a substantially lower price. Critics quote the example of farmers in Vietnam being paid over the world price in the 1980s, planting lots of coffee, then flooding the world market in the 1990s. The Fairtrade minimum price means that when the world market price collapses, it is the non-Fairtrade farmers, particularly the poorest, who have to cut down their coffee trees. This argument is supported by mainstream economists, not just free marketers.
Diverting aid from other farmers.
Fairtrade supporters boast of ‘The Honeypot Effect’ – that cooperatives which become Fairtrade members then attract additional aid from other NGO charities, government and international donors as a result of their membership. Typically there are now six to twelve other donors. Critics point out that this inevitably means that resources are being removed from other, poorer, farmers. It also makes it impossible to argue that any positive or negative changes in the living standards of farmers are due to Fairtrade rather than to one of the other donors.
Other ethical issues.
Secrecy.
Under EU law (Directive 2005/29/EC on Unfair Commercial Practices) the criminal offence of Unfair Trading is committed if (a) ‘it contains false information and is therefore untruthful or in any way, including overall presentation, deceives or is likely to deceive the average consumer, even if the information is factually correct’, (b) ‘it omits material information that the average consumer needs . . . and thereby causes or is likely to cause the average consumer to take a transactional decision that he would not have taken otherwise’ or (c) ‘fails to identify the commercial intent of the commercial practice . . . [which] causes or is likely to cause the average consumer to take a transactional decision that he would not have taken otherwise.’ Griffiths (2011) points to false claims that Fairtrade producers get higher prices, the almost universal failure to disclose the extra price charged for Fairtrade products, to disclose how much of this actually reaches the developing world, to disclose what this is spent on in the developing world, to disclose how much, if any, reaches farmers, and to disclose the harm that Fairtrade does to non-Fairtrade farmers. He also points to the failure to disclose when ‘the primary commercial intent’ is to make money for retailers and distributors in rich countries.
Imposing politics.
The Fairtrade criteria are essentially political, and critics state that it is unethical to bribe developing world producers to adopt a set of political views that they may not agree with, and the donors providing the money may not agree with. In addition many of the failures of Fairtrade derive from these political views, such as the unorthodox marketing system imposed. Boersma (2002, 2009) the founder of Fairtrade, and like minded people are aiming at a new, non-capitalist way of running the market and the economy. This may not tie in with the objectives of producers, consumers, importers or retailers.
Unethical selling techniques.
Booth says that the selling techniques used by some sellers and some supporters of Fairtrade are bullying, misleading, and unethical. There are problems with the use of boycott campaigns and other pressure to force sellers to stock a product they think ethically suspect. However, the opposite has been argued, that a more participatory and multi-stakeholder approach to auditing might improve the quality of the process.
Some people argue that these practices are justifiable: that strategic use of labeling may help embarrass (or encourage) major suppliers into changing their practices. They may make transparent corporate vulnerabilities that activists can exploit. Or they may encourage ordinary people to get involved with broader projects of social change.
Misleading volunteers.
A lot of people volunteer to work to support Fairtrade. They may do unpaid work for firms, or market Fairtrade in schools, universities, local governments, or parliament. Crane and Davies’ study shows that distributors in developed countries make ‘considerable use of unpaid volunteer workers for routine tasks, many of whom seemed to be under the (false) impression that they were helping out a charity.’
Failure to monitor standards.
There are complaints that the standards are inappropriate and may harm producers, sometimes making them work several months more for little return.
Adherence to fair trade standards by producers has been poor, and enforcement of standards by Fairtrade is weak. Notably by Christian Jacquiau and by Paola Ghillani, who spent four years as president of Fairtrade Labelling Organizations There are many complaints of poor enforcement problems: labourers on Fairtrade farms in Peru are paid less than the minimum wage; some non-Fairtrade coffee is sold as Fairtrade ‘the standards are not very strict in the case of seasonally hired labour in coffee production.’ ‘some fair trade standards are not strictly enforced’ supermarkets avoid their responsibility. In 2006, a "Financial Times" journalist found that ten out of ten mills visited had sold uncertified coffee to co-operatives as certified. It reported that "The FT was also handed evidence of at least one coffee association that received an organic, Fair Trade or other certifications despite illegally growing some 20 per cent of its coffee in protected national forest land.
Trade justice and fair trade.
Segments of the trade justice movement have also criticized fair trade in the past years for allegedly focusing too much on individual small producer groups while stopping short of advocating immediate trade policy changes that would have a larger impact on disadvantaged producers' lives. French author and RFI correspondent Jean-Pierre Boris championed this view in his 2005 book "Commerce inéquitable".
Political objections.
There have been largely political criticisms of Fairtrade from the left and the right. Some believe the fair trade system is not radical enough. French author Christian Jacquiau, in his book "Les coulisses du commerce équitable", calls for stricter fair trade standards and criticizes the fair trade movement for working within the current system (i.e., partnerships with mass retailers, multinational corporations, etc.) rather than establishing a new fairer, fully autonomous (i.e. government monopoly) trading system. Jacquiau also supports significantly higher fair trade prices in order to maximize the impact, as most producers only sell a portion of their crop under fair trade terms. It has been argued that the approach of the FairTrade system is too rooted in a Northern consumerist view of justice which Southern producers do not participate in setting. "A key issue is therefore to make explicit who possesses the power to define the terms of Fairtrade, that is who possesses the power to determine the need of an ethic in the first instance, and subsequently command a particular ethical vision as the truth."

</doc>
<doc id="49149" url="http://en.wikipedia.org/wiki?curid=49149" title="Lorraine (duchy)">
Lorraine (duchy)

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 959–978
 |  France Germany
The Duchy of Lorraine (French: "Lorraine", ]; German: "Lothringen"), was a duchy roughly corresponding with the present-day region of Lorraine in northeastern France, its capital was Nancy. 
It was founded in 959 as a result of the division of the kingdom of Lotharingia into two separate duchies: Lower Lorraine and Upper Lorraine, both duchies forming the western part of the Holy Roman Empire. The Lower duchy was quickly dismantled, while Upper Lorraine came to be known as simply the Duchy of Lorraine. The Duchy of Lorraine lost many territories, like the Three Bishoprics and the county of Bar, and was coveted and briefly occupied by the Dukes of Burgundy and the Kings of France.
In 1737, the Duchy was given to Stanisław Leszczyński, the former king of Poland, who had lost his throne as a result of the War of the Polish Succession, with the understanding that it would fall to the French crown on his death. When Stanisław died on 23 February 1766, Lorraine was annexed by France and reorganized as a province.
History.
Lotharingia.
Lorraine's predecessor, Lotharingia, was an independent Carolingian kingdom under the rule of King Lothair II (855–869). Its territory had originally been a part of Middle Francia, created in 843 by the Treaty of Verdun, when the Carolingian empire was divided between the three sons of Louis the Pious. Middle Francia was allotted to Emperor Lothair I, therefore called "Lotharii Regnum". On his death in 855, it was further divided into three parts, of which his son Lothair II took the northern one. His realm then comprised a larger territory stretching from the County of Burgundy in the south to the North Sea. In French, this area became known as "Lorraine", while in German, it was eventually known as "Lothringen". In the Alemannic language once spoken in Lorraine, the -ingen suffix signified a property; thus, in a figurative sense, "Lotharingen" can be translated as "Land belonging to Lothair".
As Lothair II had died without heirs, his territory was divided by the 870 Treaty of Meerssen between East and West Francia and finally came under East Frankish rule as a whole by the 880 Treaty of Ribemont. After the East Frankish Carolingians became extinct with the death of Louis the Child in 911, Lotharingia once again attached itself to West Francia, but was conquered by the German king Henry the Fowler in 925. Stuck in the conflict with his rival Hugh the Great, in 942 King Louis IV of France renounced all claims to Lotharingia.
Duchy of Upper Lorraine.
In 953, the German king Otto I had appointed his brother Bruno the Great Duke of Lotharingia. 
In 959, Bruno divided the duchy into Upper and Lower Lorraine; this division became permanent following his death in 965. The Upper Duchy was further "up" the river system, that is, it was inland and to the south. Upper Lorraine was first denominated as the Duchy of the Moselle, both in charters and narrative sources, and its duke was the "dux Mosellanorum". The usage of "Lotharingia Superioris" and "Lorraine" in official documents begins later, around the fifteenth century. The first duke and deputy of Bruno was Frederick I of Bar, son-in-law of Bruno's sister Hedwig of Saxony.
Lower Lorraine disintegrated into several smaller territories and only the title of a "Duke of Lothier" remained, held by Brabant. After the duchy of the Moselle came into the possession of René of Anjou, the name "Duchy of Lorraine" was adopted again, only retrospectively called "Upper Lorraine". At that time, several territories had already split off, such as the County of Luxembourg, the Electorate of Trier, the County of Bar and the "Three Bishoprics" of Verdun, Metz and Toul.
The border between the Empire and the Kingdom of France remained relatively stable throughout the Middle Ages. In 1301, Count Henry III of Bar had to receive the western part of his lands ("Barrois mouvant") as a fief by King Philip IV of France. In 1475, the Burgundian duke Charles the Bold campaigned for the Duchy of Lorraine, but was finally defeated and killed at the 1477 Battle of Nancy. In the 1552 Treaty of Chambord, a number of insurgent Protestant Imperial princes around Elector Maurice of Saxony ceded the Three Bishoprics to King Henry II of France in turn for his support.
In the 17th century, the French kings began to covet Lorraine. While the central Imperial authority decayed in the course of the Thirty Years' War, Chief Minister Cardinal Richelieu urged the occupation of the duchy in 1641. France again had to vacate it after the 1648 Peace of Westphalia, which however won France several positions in Alsace, east of Lorraine. In 1670, the French invaded again, forcing Duke Charles V to flee to a Viennese exile, where he formed strong ties to the Imperial House of Habsburg. France occupied the Duchy for almost 30 years, only giving it up in the Treaty of Ryswick, which ended the Nine Years' War in 1697. During the War of the Spanish Succession, parts of Lorraine, including the capital Nancy, were again occupied by France, but Duke Leopold Joseph continued to reign at the Château de Lunéville.
In 1737, after the War of the Polish Succession, Lorraine was part of an agreement between France, the House of Habsburg and the Lorraine House of Vaudémont: The Duchy was given to Stanisław Leszczyński, the former king of Poland and father-in-law to King Louis XV of France, who, despite French support, had lost out to a candidate backed by Russia and Austria in the War of the Polish Succession. The Lorraine duke Francis Stephen, betrothed to the Emperor's daughter Archduchess Maria Theresa, was compensated with the Grand Duchy of Tuscany, where the last Medici ruler had recently died without issue. France also promised to support Maria Theresa as heir to the Habsburg possessions under the Pragmatic Sanction of 1713. Leszczyński received Lorraine with the understanding that it would fall to the French crown on his death. When Stanisław died on 23 February 1766, Lorraine was annexed by France and reorganized as a province by the French government.
Between France and Germany.
Lorraine remained a part of France, but its northern part known as Moselle, along with Alsace, largely German-speaking regions, were annexed by the newly founded German Empire, following the Franco-Prussian War, and the French language was forbidden. The territories were not annexed by any state of the Empire or organised into a separate state, but were governed as the Reichsland Elsass-Lothringen under a governor directly appointed by the German Emperor. Alsace-Lorraine remained a part of Germany until after the end of World War I, when France occupied the area and annexed it. Policies forbidding the use of German and requiring that of French were then begun, as well as expulsions of Germans who had moved to the region after 1871.
In 1940, Nazi Germany re-annexed Alsace-Lorraine during World War II, combining Moselle with the Saarland and Alsace with Baden. The French language was again proscribed and education at German schools made compulsory. The war-torn area returned to France in November 1944. Because of the fighting in the area, Lorraine is home to the Lorraine American Cemetery and Memorial, the largest American war cemetery in France.
Culture.
Two regional languages survive in the region.
Lorraine Franconian, known as "francique" or "platt (lorrain)" in French, is a Germanic dialect spoken by a minority in the northern part of the region. This is distinct from the neighbouring Alsatian language, although the two are often confused. Neither has any form of official recognition.
Lorrain is a Romance dialect spoken by a minority in the southern part of the region.
Like most of France's regional languages (such as Breton, Provençal, Alsatian and Basque), Lorrain and Lorraine Franconian were largely replaced by French with the advent of mandatory public schooling in the 19th and 20th centuries.
Foodstuffs and dishes associated with Lorraine include quiche lorraine, Mirabelle plum, baba au rhum, bergamotes, macarons, and madeleines.

</doc>
<doc id="49159" url="http://en.wikipedia.org/wiki?curid=49159" title="Hallucination">
Hallucination

A hallucination is a perception in the absence of external stimulus that has qualities of real perception. Hallucinations are vivid, substantial, and located in external objective space. They are distinguished from the related phenomena of dreaming, which does not involve wakefulness; illusion, which involves distorted or misinterpreted real perception; imagery, which does not mimic real perception and is under voluntary control; and pseudohallucination, which does not mimic real perception, but is not under voluntary control. Hallucinations also differ from "delusional perceptions", in which a correctly sensed and interpreted stimulus (i.e., a real perception) is given some additional (and typically absurd) significance.
Hallucinations can occur in any sensory modality—visual, auditory, olfactory, gustatory, tactile, proprioceptive, equilibrioceptive, nociceptive, thermoceptive and chronoceptive.
A mild form of hallucination is known as a "disturbance", and can occur in most of the senses above. These may be things like seeing movement in peripheral vision, or hearing faint noises and/or voices. Auditory hallucinations are very common in schizophrenia. They may be benevolent (telling the patient good things about themselves) or malicious, cursing the patient etc. Auditory hallucinations of the malicious type are frequently heard, for example people talking about the patient behind his/her back. Like auditory hallucinations, the source of the visual counterpart can also be behind the patient's back. Their visual counterpart is the feeling of being looked or stared at, usually with malicious intent. Frequently, auditory hallucinations and their visual counterpart are experienced by the patient together.
Hypnagogic hallucinations and hypnopompic hallucinations are considered normal phenomena. Hypnagogic hallucinations can occur as one is falling asleep and hypnopompic hallucinations occur when one is waking up.
Hallucinations can be associated with drug use (particularly deliriants), sleep deprivation, psychosis, neurological disorders, and delirium tremens.
The word 'Hallucination' itself was introduced into the English language by the seventeenth century physician Sir Thomas Browne in 1646 from the derivation of the Latin word "alucinari" meaning to wander in the mind.
Classification.
Hallucinations may be manifested in a variety of forms. Various forms of hallucinations affect different senses, sometimes occurring simultaneously, creating multiple sensory hallucinations for those experiencing them.
Visual.
A visual hallucination is "the perception of an external visual stimulus where none exists". Alternatively, a visual illusion is a distortion of a real external stimulus. Visual hallucinations are separated into simple and complex.
For example, one may report hallucinating a giraffe. A simple visual hallucination is an amorphous figure that may have a similar shape or color to a giraffe (looks like a giraffe), while a complex visual hallucination is a discrete, lifelike image that is, unmistakably, a giraffe.
Auditory.
Auditory hallucinations (also known as "paracusia") are the perception of sound without outside stimulus. Auditory hallucinations are the most common type of hallucination. Auditory hallucinations can be divided into two categories: elementary and complex. Elementary hallucinations are the perception of sounds such as hissing, whistling, an extended tone, and more. In many cases, tinnitus is an elementary auditory hallucination. However, some people who experience certain types of tinnitus, especially pulsatile tinnitus, are actually hearing the blood rushing through vessels near the ear. Because the auditory stimulus is present in this situation, it does not qualify as a hallucination.
Complex hallucinations are those of voices, music, or other sounds that may or may not be clear, may be familiar or completely unfamiliar, and friendly or aggressive, among other possibilities. A hallucinations of a single individual person of one or more talking voices are particularly associated with psychotic disorders such as schizophrenia, and hold special significance in diagnosing these conditions.
If a group of people experience a complex auditory hallucination, no single individual can be named psychotic or schizophrenic.
Another typical disorder where auditory hallucinations are very common is dissociative identity disorder. In schizophrenia voices are normally perceived coming from outside the person but in dissociative disorders they are perceived as originating from within the person, commenting in their head not behind their back. Differential diagnosis between schizophrenia and dissociative disorders is challenging due to many overlapping symptoms especially scheinedrian first rank symptoms such as hallucinations. However, many people not suffering from diagnosable mental illness may sometimes hear voices as well. One important example to consider when forming a differential diagnosis for a patient with paracusia is lateral temporal lobe epilepsy. Despite the tendency to associate hearing voices, or otherwise hallucinating, and psychosis with schizophrenia or other psychiatric illnesses, it is crucial to take into consideration that, even if a person does exhibit psychotic features, he/she does not necessarily suffer from a psychiatric disorder on its own. Disorders such as Wilson's disease, various endocrine diseases, numerous metabolic disturbances, multiple sclerosis, systemic lupus erythematosus, porphyria, sarcoidosis, and many others can present with psychosis.
Musical hallucinations are also relatively common in terms of complex auditory hallucinations and may be the result of a wide range of causes ranging from hearing-loss (such as in musical ear syndrome, the auditory version of Charles Bonnet syndrome), lateral temporal lobe epilepsy, arteriovenous malformation, stroke, lesion, abscess, or tumor.
The Hearing Voices Movement is a support and advocacy group for people who hallucinate voices, but do not otherwise show signs of mental illness or impairment.
High caffeine consumption has been linked to an increase in the likelihood of one's experiencing auditory hallucinations. A study conducted by the La Trobe University School of Psychological Sciences revealed that as few as five cups of coffee a day could trigger the phenomenon.
Command hallucinations.
Command hallucinations are hallucinations in the form of commands; they can be auditory or inside of the person's mind and/or consciousness. The contents of the hallucinations can range from the innocuous to commands to cause harm to the self or others. Command hallucinations are often associated with schizophrenia. People experiencing command hallucinations may or may not comply with the hallucinated commands, depending on circumstances. Compliance is more common for non-violent commands.
Command hallucinations are sometimes used in defense of a crime, often homicides. In essence, it is a voice that one hears and it tells the listener what to do. Sometimes the commands are quite benign directives such as "Stand up" or "Shut the door." Whether it is a command for something simple or something that is a threat, it is still considered a "command hallucination." Some helpful questions that can assist one in figuring out if he/she may be suffering from this includes: "What are the voices telling you to do?", "When did your voices first start telling you to do things?", "Do you recognize the person who is telling you to harm yourself (others)?", "Do you think you can resist doing what the voices are telling you to do?"
Olfactory.
Phantosmia is the phenomenon of smelling odors that are not really present. The most common odors are unpleasant smells such as rotting flesh, vomit, urine, feces, smoke, or others. Phantosmia often results from damage to the nervous tissue in the olfactory system. The damage can be caused by viral infection, brain tumor, trauma, surgery, and possibly exposure to toxins or drugs. Phantosmia can also be induced by epilepsy affecting the olfactory cortex and is also thought to possibly have psychiatric origins. Phantosmia is different from parosmia, in which a smell is actually present, but perceived differently from its actual smell. Smelling colors is also reported in some cases of serious hallucinations.
Olfactory hallucinations can also appear in some cases of associative imagination, for example, while watching a romance movie, where the man gifts roses to the woman, the viewer senses the roses' odor (which in fact does not exist).
Olfactory hallucinations have also been reported in migraine, although the frequency of such hallucinations is unclear.
Tactile hallucinations.
Tactile hallucinations are the illusion of tactile sensory input, simulating various types of pressure to the skin or other organs. One subtype of tactile hallucination, formication, is the sensation of insects crawling underneath the skin and is frequently associated with prolonged cocaine use. However, formication may also be the result of normal hormonal changes such as menopause, or disorders such as peripheral neuropathy, high fevers, Lyme disease, skin cancer, and more.
Gustatory.
This type of hallucination is the perception of taste without a stimulus. These hallucinations, which are typically strange or unpleasant, are relatively common among individuals who have certain types of focal epilepsy, especially temporal lobe epilepsy. The regions of the brain responsible for gustatory hallucination in this case are the insula and the superior bank of the sylvian fissure.
General somatic sensations.
General somatic sensations of a hallucinatory nature are experienced when an individual feels that his body is being mutilated, i.e. twisted, torn, or disembowelled. Other reported cases are invasion by animals in the person's internal organs such as snakes in the stomach or frogs in the rectum. The general feeling that one's flesh is decomposing is also classified under this type of hallucination.
Cause.
Hallucinations can be caused by a number of factors.
Hypnagogic hallucination.
These hallucinations occur just before falling asleep, and affect a high proportion of the population: in one survey 37% of the respondents experienced them twice a week. The hallucinations can last from seconds to minutes; all the while, the subject usually remains aware of the true nature of the images. These may be associated with narcolepsy. Hypnagogic hallucinations are sometimes associated with brainstem abnormalities, but this is rare.
Peduncular hallucinosis.
Peduncular means pertaining to the peduncle, which is a neural tract running to and from the pons on the brain stem. These hallucinations usually occur in the evenings, but not during drowsiness, as in the case of hypnagogic hallucination. The subject is usually fully conscious and then can interact with the hallucinatory characters for extended periods of time. As in the case of hypnagogic hallucinations, insight into the nature of the images remains intact. The false images can occur in any part of the visual field, and are rarely polymodal.
Delirium tremens.
One of the more enigmatic forms of visual hallucination is the highly variable, possibly polymodal delirium tremens. Individuals suffering from delirium tremens may be agitated and confused, especially in the later stages of this disease. Insight is gradually reduced with the progression of this disorder. Sleep is disturbed and occurs for a shorter period of time, with rapid eye movement sleep.
Parkinson's disease and Lewy body dementia.
Parkinson's disease is linked with Lewy body dementia for their similar hallucinatory symptoms. The symptoms strike during the evening in any part of the visual field, and are rarely polymodal. The segue into hallucination may begin with illusions where sensory perception is greatly distorted, but no novel sensory information is present. These typically last for several minutes, during which time the subject may be either conscious and normal or drowsy/inaccessible. Insight into these hallucinations is usually preserved and REM sleep is usually reduced. Parkinson's disease is usually associated with a degraded substantia nigra pars compacta, but recent evidence suggests that PD affects a number of sites in the brain. Some places of noted degradation include the median raphe nuclei, the noradrenergic parts of the locus coeruleus, and the cholinergic neurons in the parabrachial area and pedunculopontine nuclei of the tegmentum.
Migraine coma.
This type of hallucination is usually experienced during the recovery from a comatose state. The migraine coma can last for up to two days, and a state of depression is sometimes comorbid. The hallucinations occur during states of full consciousness, and insight into the hallucinatory nature of the images is preserved. It has been noted that ataxic lesions accompany the migraine coma.
Charles Bonnet syndrome.
Charles Bonnet syndrome is the name given to visual hallucinations experienced by a partially or severely sight impaired person. The hallucinations can occur at any time and can distress people of any age, as they may not initially be aware that they are hallucinating, they may fear initially for their own mental health which may delay them sharing with carers what is happening until they start to understand it themselves. The hallucinations can frighten and disconcert as to what is real and what is not and carers need to learn how to support sufferers. The hallucinations can sometimes be dispersed by eye movements, or perhaps just reasoned logic such as, "I can see fire but there is no smoke and there is no heat from it" or perhaps "We have an infestation of rats but they have pink ribbons with a bell tied on their necks." Over elapsed months and years the manifestation of the hallucinations may change, becoming more or less frequent with changes in ability to see. The length of time that the sight impaired person can suffer from these hallucinations varies according to the underlying speed of eye deterioration. A differential diagnosis are ophthalmopathic hallucinations.
Focal epilepsy.
Visual hallucinations due to focal seizures differ depending on the region of the brain where the seizure occurs. For example, visual hallucinations during occipital lobe seizures are typically visions of brightly colored, geometric shapes that may move across the visual field, multiply, or form concentric rings and generally persist from a few seconds to a few minutes. They are usually unilateral and localized to one part of the visual field on the contralateral side of the seizure focus, typically the temporal field. However, unilateral visions moving horizontally across the visual field begin on the contralateral side and move toward the ipsilateral side.
Temporal lobe seizures, on the other hand, can produce complex visual hallucinations of people, scenes, animals, and more as well as distortions of visual perception. Complex hallucinations may appear real or unreal, may or may not be distorted with respect to size, and may seem disturbing or affable, among other variables. One rare but notable type of hallucination is heautoscopy, a hallucination of a mirror image of one's self. These "other selves" may be perfectly still or performing complex tasks, may be an image of a younger self or the present self, and tend to be only briefly present. Complex hallucinations are a relatively uncommon finding in temporal lobe epilepsy patients. Rarely, they may occur during occipital focal seizures or in parietal lobe seizures.
Distortions in visual perception during a temporal lobe seizure may include size distortion (micropsia or macropsia), distorted perception of movement (where moving objects may appear to be moving very slowly or to be perfectly still), a sense that surfaces such as ceilings and even entire horizons are moving farther away in a fashion similar to the dolly zoom effect, and other illusions. Even when consciousness is impaired, insight into the hallucination or illusion is typically preserved.
Drug-induced hallucination.
Drug-induced hallucinations are caused by the consumption of psychoactive substances such as deliriants, psychedelics, and certain stimulants, which are known to cause visual and auditory hallucinations. Some psychedelics such as lysergic acid diethylamide and psilocybin can cause hallucinations that range from a spectrum of mild to severe. Some of these drugs can be used in psychotherapy to treat mental disorders, addiction, and anxiety secondary to advanced stage cancers.<ref name="Human Brain Mapping, DOI: 10.1002/hbm.21381"></ref>
Sensory deprivation hallucination.
Hallucinations can be caused by sense deprivation when it occurs for prolonged periods of time, and almost always occur in the modality being deprived (visual for blindfolded/darkness, auditory for muffled conditions, etc.)
Experimentally-induced hallucinations.
"Main article" : Hallucinations in the sane
Pathophysiology.
Visual.
Sometimes internal imagery can overwhelm the sensory input from external stimuli when sharing neural pathways, or if indistinct stimuli is perceived and manipulated to match one's expectations or beliefs, especially about the environment. This can result in a hallucination, and this effect is sometimes exploited to form an optical illusion.
There are 3 pathophysiologic mechanisms thought to account for complex visual hallucinations. These mechanisms consist of the following:
The first mechanism involves irritation of cortical centers responsible for visual processing (e.g., seizure activity). The irritation of the primary visual cortex causes simple elementary visual hallucinations.
The second mechanism involves lesions that cause deafferentation of the visual system may lead to cortical release phenomenon, which includes visual hallucination.
The third mechanism is the reticular activating system, which has been linked to the genesis of visual hallucinations.
Some specific classifications include: elementary hallucinations, which may entail flicks, specks, and bars of light (called phosphenes). Closed eye hallucinations in darkness, which are common to psychedelic drugs (i.e., LSD, mescaline). Scenic or "panoramic" hallucinations, which are not superimposed but vividly replace the entire visual field with hallucinatory content similarly to dreams; such scenic hallucinations may occur in epilepsy (in which they are usually stereotyped and experimental in character), hallucinogen use, and more rarely in catatonic schizophrenia (cf. oneirophrenia), mania, and brainstem lesions, among others.
Another thing that may cause visual hallucinations is prolonged visual deprivation. In a study where 13 healthy people were blindfolded for 5 days, 10 out of the 13 subjects reported visual hallucinations. This finding lends strong support to the idea that the simple loss of normal visual input is sufficient to cause visual hallucinations.
Psychodynamic view.
Various theories have been put forward to explain the occurrence of hallucinations. When psychodynamic (Freudian) theories were popular in psychology, hallucinations were seen as a projection of unconscious wishes, thoughts, and wants. As biological theories have become orthodox, hallucinations are more often thought of (by psychologists at least) as being caused by functional deficits in the brain. With reference to mental illness, the function (or dysfunction) of the neurotransmitters glutamate and dopamine are thought to be particularly important. The Freudian interpretation may have an aspect of truth, as the biological hypothesis explains the physical interactions in the brain, while the Freudian interpretation addresses the psychological complexes related to the content of the hallucination, such as hallucinating persecutory voices due to guilt. Psychological research has argued that hallucinations may result from biases in what are known as metacognitive abilities.
Information processing perspective.
These are abilities that allow us to monitor or draw inferences from our own internal psychological states (such as intentions, memories, beliefs and thoughts). The ability to discriminate between internal (self-generated) and external (stimuli) sources of information is considered to be an important metacognitive skill, but one which may break down to cause hallucinatory experiences. Projection of an internal state (or a person's own reaction to another's) may arise in the form of hallucinations, especially auditory hallucinations. A recent hypothesis that is gaining acceptance concerns the role of overactive top-down processing, or strong perceptual expectations, that can generate spontaneous perceptual output (that is, hallucination).
Biological perspective.
Auditory Hallucinations.
Auditory Hallucinations are the most prevalent type of hallucinations. They include hearing voices and music. Many times an individual suffering from auditory hallucinations will hear a voice or voices saying the individual's own thoughts out loud, commenting on all their actions, or commanding and ordering the individual around. These voices tend to be negative and critical toward the individual. People who suffer from schizophrenia and have auditory hallucinations will often speak to the voice as though they are speaking to a second person.
Visual.
The most common modality referred to when people speak of hallucinations include the phenomena of seeing things that are not present or visual perception, which does not reconcile with the physical, consensus reality. There are many different causes that have been classed as psychophysiologic (a disturbance of brain structure), psychobiochemical (a disturbance of neurotransmitters), psychodynamic (an emergence of the unconscious into consciousness), and psychological (e.g., meaningful experiences consciousness); this is also the case in Alzheimer's disease. Numerous disorders can involve visual hallucinations, ranging from psychotic disorders to dementia to migraine, but experiencing visual hallucinations does not in itself mean that there is necessarily a disorder. Visual hallucinations are associated with organic disorders of the brain and with drug- and alcohol-related illness, and not typically considered the result of a psychiatric disorder.
Schizoc hallucination.
Hallucinations may be caused by schizophrenia. Schizophrenia is a mental disorder in which there is an inability to tell the difference between real and unreal experiences, to think logically, to have contextually appropriate emotions, and to function in social situations.
Neuroanatomical correlates.
Normal everyday procedures like getting an MRI (Magnetic Resonance Imaging) have been used to find out more about auditory and verbal hallucinations. "Functional magnetic resonance imaging (fMRI) and repetitive transcranial magnetic stimulation (rTMS) were used to explore the pathophysiology of auditory/verbal hallucinations (AVHs)" Throughout the exploring through MRI's of patients,there were "lower levels of hallucination-related activation in Broca’s area strongly predicted greater rate of response to left temporoparietal rTMS."
Also through fMRIs, it is found that there can be better understandings on why hallucinations happen in the brain, by understanding emotions and cognition and how they can prompt physical reactions that can help result in a hallucination. It suggests the theory that "motivations in the body and mind can drive us to certain behaviors that we act in, such as survival instinct and intuition" and that they can work in a hand-in-hand-like fashion. It can also be viewed as a symbolic "homeostasis" that can have adverse effects by having these hallucinations and/or mental illnesses. The amygdala has also been seen to relate to this finding by contributing a "declarative judgement of emotional salience" as well as affecting both "efferent and afferent representational levels of affective autonomic responses in the brain".
Pathophysiological mechanisms.
There are symptoms that are mechanism-based that are associated with hallucinations. These include superficial pressure and stabbing pain. Others include a burning-like sensation or electric shock feeling. Human studies of these symptoms remain mostly unclear unlike similar studies in animals.
Treatments.
There are few treatments for many types of hallucinations. However, for those hallucinations caused by mental disease, a psychologist or psychiatrist should be alerted, and treatment will be based on the observations of those doctors. Antipsychotic and atypical antipsychotic medication may also be utilized to treat the illness if the symptoms are severe and cause significant distress. For other causes of hallucinations there is no factual evidence to support any one treatment is scientifically tested and proven. However, abstaining from hallucinogenic drugs, managing stress levels, living healthily, and getting plenty of sleep can help reduce the prevalence of hallucinations. In all cases of hallucinations, medical attention should be sought out and informed of one's specific symptoms.
Epidemiology.
One study from as early as 1895 reported that approximately 10% of the population experiences hallucinations. A 1996-1999 survey of over 13,000 people reported a much higher figure, with almost 39% of people reporting hallucinatory experiences, 27% of which daytime hallucinations, mostly outside the context of illness or drug use. From this survey, olfactory (smell) and gustatory (taste) hallucinations seem the most common in the general population.

</doc>
<doc id="49166" url="http://en.wikipedia.org/wiki?curid=49166" title="Damon Knight">
Damon Knight

Damon Francis Knight (September 19, 1922 – April 15, 2002) was an American science fiction author, editor, critic and fan. His forte was short stories and he is widely acknowledged as having been a master of the genre. He was married to fellow writer Kate Wilhelm.
Life and career.
Damon Knight was born in Baker, Oregon in 1922, and grew up in Hood River, Oregon. He entered science-fiction fandom at the age of eleven and published two issues of a fanzine entitled "Snide".
Knight's first professional sale was a cartoon drawing to a science-fiction magazine, "Amazing Stories". His first story, "The Itching Hour," appeared in the Summer 1940 number of "Futuria Fantasia", edited and published by Ray Bradbury. "Resilience" followed in the February 1941 number of "Stirring Science Stories", edited by Donald Wolheim. An editorial error made the latter story's ending incomprehensible; it was reprinted in a 1978 magazine in four pages with a two-page introduction by Knight. He is a Hugo Award winner, founder of the Science Fiction and Fantasy Writers of America (SFWA), cofounder of the National Fantasy Fan Federation, cofounder of the Milford Writer's Workshop, and cofounder of the Clarion Writers Workshop. Until his death, Knight lived in Eugene, Oregon, with his second wife Kate Wilhelm, also a writer of science fiction and of fantasy, contemporary mimetic and crime fiction.
At the time of his first story, he was living in New York, and was a member of the Futurians. One of his short stories describes paranormal disruption of a science fiction fan group, and contains cameo appearances of various Futurians and others under thinly-disguised names: For instance, non-Futurian sf writer H. Beam Piper is identified as "H. Dreyne Fifer".
In a series of reviews for various magazines, he became famous as a science fiction critic, a career which began when he wrote in 1945 that A. E. van Vogt "is not a giant as often maintained. He's only a pygmy who has learned to operate an overgrown typewriter." After nine years, he ceased reviewing when a magazine refused to publish one review exactly as he wrote it. These reviews were later collected in "In Search of Wonder".
The SFWA officers and past presidents named Knight its 13th Grand Master in 1994 (presented 1995). After his 2002 death the associated award was renamed in his honor, the Damon Knight Memorial Grand Master Award. The Science Fiction Hall of Fame inducted him in 2003.
To the general public, he is best known as the author of "To Serve Man", a 1950 short story that was adapted for "The Twilight Zone". It won a 50-year Retro Hugo in 2001 as the best short story of 1950, which predated the Hugo Awards. His only Hugo Award was the "Best Reviewer" in 1956.
Knight is also known for the term "idiot plot", a story that only functions because almost everyone in it is an idiot; the term was probably invented by James Blish, but became well-known through Knight's frequent use of it in his reviews.
His papers are held in the University of Oregon Special Collections and University Archive.

</doc>
<doc id="49169" url="http://en.wikipedia.org/wiki?curid=49169" title="Penelope">
Penelope

In Homer's "Odyssey", Penelope ( ; Greek: Πηνελόπεια, "Pēnelópeia", or Πηνελόπη, "Pēnelópē") is the faithful wife of Odysseus, who keeps her suitors at bay in his long absence and is eventually reunited with him.
Her name has traditionally been associated with marital faithfulness, and so it was with the Greeks and Romans, but some recent feminist readings offer a more ambiguous interpretation.
Etymology.
The origin of her name is believed by Robert S. P. Beekes to be Pre-Greek and related to "pēnelops" (πηνέλοψ) or "pēnelōps" (πηνέλωψ), glossed by Hesychius as "some kind of bird" (today arbitrarily identified with the Eurasian Wigeon, to which Linnaeus gave the binomial "Anas penelope"), where "-elōps" (-έλωψ) is a common Pre-Greek suffix for predatory animals; however, the semantic relation between the proper name and the gloss is not clear. In folk etymology, "Pēnelopē" (Πηνελόπη) is usually understood to combine the Greek word "pēnē" (πήνη), "weft", and "ōps" (ὤψ), "face", which is considered the most appropriate for a cunning weaver whose motivation is hard to decipher.
Role in the "Odyssey".
Penelope is the wife of the main character, the king of Ithaca, Odysseus (Ulysses in Roman mythology), and daughter of Icarius and his wife Periboea. She only has one son by Odysseus, Telemachus, who was born just before Odysseus was called to fight in the Trojan War. She waits twenty years for the final return of her husband, during which she devises various strategies to delay marrying one of the 108 suitors (led by Antinous and including Agelaus, Amphinomus, Ctessippus, Demoptolemus, Elatus, Euryades, Eurymachus and Peisandros).
On Odysseus's return, disguised as an old beggar, he finds that Penelope has remained faithful. She has devised tricks to delay her suitors, one of which is to pretend to be weaving a burial shroud for Odysseus's elderly father Laertes and claiming that she will choose a suitor when she has finished. Every night for three years, she undoes part of the shroud, until Melantho, one of twelve unfaithful serving women, discovers her chicanery and reveals it to the suitors.
Because of her efforts to put off remarriage, Penelope is often seen as a symbol of connubial fidelity and we are reminded several times of her fidelity. But due to Athena's meddling, who wants her "to show herself to the wooers, that she might set their hearts a-flutter and win greater honor from her husband and her son than heretofore", Penelope does appear before the suitors (xviii.160–162). As Irene de Jong comments:
As so often, it is Athena who takes the initiative in giving the story a new direction ... Usually the motives of mortal and god coincide, here they do not: Athena wants Penelope to fan the Suitor's desire for her and (thereby) make her more esteemed by her husband and son; Penelope has no real motive ... she simply feels an unprecedented impulse to meet the men she so loathes ... adding that she might take this opportunity to talk to Telemachus (which she will indeed do).
She is ambivalent, variously asking Artemis to kill her and, apparently, considering marrying one of the suitors. When the disguised Odysseus returns, she announces in her long interview with the disguised hero that whoever can string Odysseus's rigid bow and shoot an arrow through twelve axe heads may have her hand. "For the plot of the "Odyssey", of course, her decision is the turning point, the move that makes possible the long-predicted triumph of the returning hero".
There are debate as to whether she is aware that Odysseus is behind the disguise. Penelope and the suitors know that Odysseus (were he in fact present) would easily surpass all in any test of masculine skill. Since Odysseus seems to be the only person (perhaps excepting Telemachus) who can actually use the bow, it could merely have been another delaying tactic of Penelope's.
When the contest of the bow begins, none of the suitors are able to string the bow, but Odysseus does, and wins the contest. Having done so, he proceeds to slaughter the suitors—beginning with Antinous whom he finds drinking from Odysseus' cup—with help from Telemachus, Athena and two servants, Eumaeus the swineherd and Philoetius the cowherd. Odysseus has now revealed himself in all his glory (with a little makeover by Athena); yet Penelope cannot believe that her husband has really returned—she fears that it is perhaps some god in disguise, as in the story of Alcmene—and tests him by ordering her servant Euryclea to move the bed in their bridal-chamber. Odysseus protests that this cannot be done since he made the bed himself and knows that one of its legs is a living olive tree. Penelope finally accepts that he truly is her husband, a moment that highlights their "homophrosýnē" (ὁμοφροσύνη, "like-mindedness"). Homer implies, that from then on, Odysseus would live a long and happy life together with Penelope and Telemachus, wisely ruling his kingdom and enjoying wide respect and much success.
In some early sources such as Pindar, Pan's father is Apollo via Penelope. Herodotus (2.145), Cicero (ND 3.22.56), Apollodorus (7.38) and Hyginus (Fabulae 224) all make Hermes and Penelope his parents. Pausanias 8.12.5 records the story that Penelope had in fact been unfaithful to her husband, who banished her to Mantineia upon his return. Other sources (Duris of Samos; the Vergilian commentator Servius) report that Penelope slept with all 108 suitors in Odysseus' absence, and gave birth to Pan as a result.[9] This myth reflects the folk etymology that equates Pan's name (Πάν) with the Greek word for "all" (πᾶν).
Iconography.
Penelope is recognizable in Greek and Roman works, from Attic vase-paintings—the Penelope Painter is recognized by his representations of her—to Roman sculpture copying or improvising upon classical Greek models, by her seated pose, by her reflective gesture of leaning her cheek on her hand, and by her protectively crossed knees, reflecting her long chastity in Odysseus' absence, an unusual pose in any other figure.
Latin tradition.
Latin references to Penelope revolved around the sexual loyalty to her absent husband. It suited the martial aspect of Roman society representing the tranquility of the worthy family. She is mentioned by various classical authors including Plautus, Propertius, Horace, Ovid, Martial and Statius. The use of Penelope in Latin texts provided a basis for her ongoing use in the Middle Ages and Renaissance as a representation of the chaste wife. This was reinforced by her being named by St Jerome among pagan women famed for their chastity.

</doc>
<doc id="49171" url="http://en.wikipedia.org/wiki?curid=49171" title="Interval (music)">
Interval (music)

In music theory, an interval is the difference between two pitches. 
An interval may be described as horizontal, linear, or melodic if it refers to successively sounding tones, such as two adjacent pitches in a melody, and vertical or harmonic if it pertains to simultaneously sounding tones, such as in a chord.
In Western music, intervals are most commonly differences between notes of a diatonic scale. The smallest of these intervals is a semitone. Intervals smaller than a semitone are called microtones. They can be formed using the notes of various kinds of non-diatonic scales. Some of the very smallest ones are called commas, and describe small discrepancies, observed in some tuning systems, between enharmonically equivalent notes such as C♯ and D♭. Intervals can be arbitrarily small, and even imperceptible to the human ear.
In physical terms, an interval is the ratio between two sonic frequencies. For example, any two notes an octave apart have a frequency ratio of 2:1. This means that successive increments of pitch by the same interval result in an exponential increase of frequency, even though the human ear perceives this as a linear increase in pitch. For this reason, intervals are often measured in cents, a unit derived from the logarithm of the frequency ratio.
In Western music theory, the most common naming scheme for intervals describes two properties of the interval: the quality (perfect, major, minor, augmented, diminished) and number (unison, second, third, etc.). Examples include the minor third or perfect fifth. These names describe not only the difference in semitones between the upper and lower notes, but also how the interval is spelled. The importance of spelling stems from the historical practice of differentiating the frequency ratios of enharmonic intervals such as G-G♯ and G-A♭.
Size.
The size of an interval (also known as its width or height) can be represented using two alternative and equivalently valid methods, each appropriate to a different context: frequency ratios or cents.
Frequency ratios.
The size of an interval between two notes may be measured by the ratio of their frequencies. When a musical instrument is tuned using a just intonation tuning system, the size of the main intervals can be expressed by small-integer ratios, such as 1:1 (unison), 2:1 (octave), 3:2 (perfect fifth), 4:3 (perfect fourth), 5:4 (major third), 6:5 (minor third). Intervals with small-integer ratios are often called "just intervals", or "pure intervals". To most people, just intervals sound consonant, that is, pleasant and well tuned.
Most commonly, however, musical instruments are nowadays tuned using a different tuning system, called 12-tone equal temperament, in which the main intervals are typically perceived as consonant, but none is justly tuned and as consonant as a just interval, except for the unison (1:1) and octave (2:1). As a consequence, the size of most equal-tempered intervals cannot be expressed by small-integer ratios, although it is very close to the size of the corresponding just intervals. For instance, an equal-tempered fifth has a frequency ratio of 27/12:1, approximately equal to 1.498:1, or 2.997:2 (very close to 3:2). For a comparison between the size of intervals in different tuning systems, see section Size in different tuning systems.
Cents.
The standard system for comparing interval sizes is with cents. The cent is a logarithmic unit of measurement. If frequency is expressed in a logarithmic scale, and along that scale the distance between a given frequency and its double (also called octave) is divided into 1200 equal parts, each of these parts is one cent. In twelve-tone equal temperament (12-TET), a tuning system in which all semitones have the same size, the size of one semitone is exactly 100 cents. Hence, in 12-TET the cent can be also defined as one hundredth of a semitone.
Mathematically, the size in cents of the interval from frequency "f"1 to frequency "f"2 is
Main intervals.
The table shows the most widely used conventional names for the intervals between the notes of a chromatic scale. A perfect unison (also known as perfect prime) is an interval formed by two identical notes. Its size is zero cents. A semitone is any interval between two adjacent notes in a chromatic scale, a whole tone is an interval spanning two semitones (for example, a major second), and a tritone is an interval spanning three tones, or six semitones (for example, an augmented fourth). 
Rarely, the term ditone is also used to indicate an interval spanning two whole tones (for example, a major third), or more strictly as a synonym of major third.
Intervals with different names may span the same number of semitones, and may even have the same width. For instance, the interval from D to F♯ is a major third, while that from D to G♭ is a diminished fourth. However, they both span 4 semitones. If the instrument is tuned so that the 12 notes of the chromatic scale are equally spaced (as in equal temperament), these intervals will also have the same width. Namely, all semitones will have a width of 100 cents, and all intervals spanning 4 semitones will be 400 cents wide.
The names listed here cannot be determined by counting semitones alone. The rules to determine them are explained below. Other names, determined with different naming conventions, are listed in a separate section. Intervals smaller than one semitone (commas or microtones) and larger than one octave (compound intervals) are introduced below.
Interval number and quality.
In Western music theory, an interval is named according to its "number" (also called "diatonic number") and "quality". For instance, "major third" (or M3) is an interval name, in which the term "major" (M) describes the quality of the interval, and "third" (3) indicates its number.
Number.
The number of an interval is the number of letter names it encompasses or staff positions it encompasses. Both lines and spaces (see figure) are counted, including the positions of both notes forming the interval. For instance, the interval C–G is a fifth (denoted P5) because the notes from C to G encompass five letter names (C, D, E, F, G) and occupy five consecutive staff positions, including the positions of C and G. The table and the figure above show intervals with numbers ranging from 1 (e.g., P1) to 8 (e.g., P8). Intervals with larger numbers are called compound intervals.
There is a one-to-one correspondence between staff positions and diatonic-scale degrees (the notes of a diatonic scale).
This means that interval numbers can be also determined by counting diatonic scale degrees, rather than staff positions, provided that the two notes which form the interval are drawn from a diatonic scale. Namely, C–G is a fifth because in any diatonic scale that contains C and G, the sequence from C to G includes five notes. For instance, in the A♭-major diatonic scale, the five notes are C–D♭–E♭–F–G (see figure). This is not true for all kinds of scales. For instance, in a chromatic scale, the notes from C to G are eight (C–C♯–D–D♯–E–F–F♯–G). This is the reason interval numbers are also called "diatonic numbers", and this convention is called "diatonic numbering".
If one adds any accidentals to the notes that form an interval, by definition the notes do not change their staff positions. As a consequence, any interval has the same interval number as the corresponding natural interval, formed by the same notes without accidentals. For instance, the intervals C–G♯ (spanning 8 semitones) and C♯–G (spanning 6 semitones) are fifths, like the corresponding natural interval C–G (7 semitones).
Notice that interval numbers represent an inclusive count of encompassed staff positions or note names, not the difference between the endpoints. In other words, start counting the lower pitch as one, not zero. For that reason, the interval C–C, a perfect unison, is called a prime (meaning "1"), even though there's no difference between the endpoints. Continuing, the interval C–D is a second, but D is only one staff position, or diatonic-scale degree, above C. Similarly, C–E is a third, but E is only two staff positions above C, and so on. As a consequence, joining two intervals always yields an interval number one less than their sum. For instance, the intervals C–E and E–G are thirds, but joined together they form a fifth (C–G), not a sixth. Similarly, a stack of three thirds, such as C–E, E–G, and G–B, is a seventh (C–B), not a ninth.
Read the Compound intervals section to determine the diatonic numbers of a intervals larger than an octave.
Quality.
The name of any interval is further qualified using the terms perfect (P), major (M), minor (m), augmented (A), and diminished (d). This is called its "interval quality". It is possible to have doubly diminished and doubly augmented intervals, but these are quite rare, as they occur only in chromatic contexts. The quality of a compound interval is the quality of the simple interval on which it is based.
Perfect intervals are so-called because they were traditionally considered perfectly consonant,
although in Western classical music the perfect fourth was sometimes regarded as a less than perfect consonance, when its function was contrapuntal. Conversely, minor, major, augmented or diminished intervals are typically considered to be less consonant, and were traditionally classified as mediocre consonances, imperfect consonances, or dissonances.
Within a diatonic scale all unisons (P1) and octaves (P8) are perfect. Most fourths and fifths are also perfect (P4 and P5), with five and seven semitones respectively. There's one occurrence of a fourth and a fifth which are not perfect, as they both span six semitones: an augmented fourth (A4), and its inversion, a diminished fifth (d5). For instance, in a C-major scale, the A4 is between F and B, and the d5 is between B and F (see table).
By definition, the inversion of a perfect interval is also perfect. Since the inversion does not change the pitch of the two notes, it hardly affects their level of consonance (matching of their harmonics). Conversely, other kinds of intervals have the opposite quality with respect to their inversion. The inversion of a major interval is a minor interval, the inversion of an augmented interval is a diminished interval.
As shown in the table, a diatonic scale defines seven intervals for each interval number, each starting from a different note (seven unisons, seven seconds, etc.). The intervals formed by the notes of a diatonic scale are called diatonic. Except for unisons and octaves, the diatonic intervals with a given interval number always occur in two sizes, which differ by one semitone. For example, six of the fifths span seven semitones. The other one spans six semitones. Four of the thirds span three semitones, the others four. If one of the two versions is a perfect interval, the other is called either diminished (i.e. narrowed by one semitone) or augmented (i.e. widened by one semitone). Otherwise, the larger version is called major, the smaller one minor. For instance, since a 7-semitone fifth is a perfect interval (P5), the 6-semitone fifth is called "diminished fifth" (d5). Conversely, since neither kind of third is perfect, the larger one is called "major third" (M3), the smaller one "minor third" (m3).
Within a diatonic scale, unisons and octaves are always qualified as perfect, fourths as either perfect or augmented, fifths as perfect or diminished, and all the other intervals (seconds, thirds, sixths, sevenths) as major or minor.
Augmented intervals are wider by one semitone than perfect or major intervals, while having the same interval number (i.e., encompassing the same number of staff positions). Diminished intervals are narrower by one semitone than perfect or minor intervals of the same interval number. For instance, an augmented third such as C–E♯ spans five semitones, exceeding a major third (C–E) by one semitone, while a diminished third such as C♯–E♭ spans two semitones, falling short of a minor third (C–E♭) by one semitone.
The augmented fourth (A4) and the diminished fifth (d5) are the only augmented and diminished intervals that appear in diatonic scales (see table).
Example.
Neither the number, nor the quality of an interval can be determined by counting semitones alone. As explained above, the number of staff positions must be taken into account as well.
For example, as shown in the table below, there are four semitones between A♭ and B♯, between A and C♯, between A and D♭, and between A♯ and E, but
Shorthand notation.
Intervals are often abbreviated with a P for perfect, m for minor, M for major, d for diminished, A for augmented, followed by the interval number. The indication M and P are often omitted. The octave is P8, and a unison is usually referred to simply as "a unison" but can be labeled P1. The tritone, an augmented fourth or diminished fifth is often TT. The interval qualities may be also abbreviated with perf, min, maj, dim, aug. Examples:
Inversion.
A simple interval (i.e., an interval smaller than or equal to an octave) may be inverted by raising the lower pitch an octave, or lowering the upper pitch an octave. For example, the fourth from a lower C to a higher F may be inverted to make a fifth, from a lower F to a higher C.
There are two rules to determine the number and quality of the inversion of any simple interval:
For example, the interval from C to the E♭ above it is a minor third. By the two rules just given, the interval from E♭ to the C above it must be a major sixth.
Since compound intervals are larger than an octave, "the inversion of any compound interval is always the same as the inversion of the simple interval from which it is compounded."
For intervals identified by their ratio, the inversion is determined by reversing the ratio and multiplying by 2. For example, the inversion of a 5:4 ratio is an 8:5 ratio.
For intervals identified by an integer number of semitones, the inversion is obtained by subtracting that number from 12.
Since an interval class is the lower number selected among the interval integer and its inversion, interval classes cannot be inverted.
Classification.
Intervals can be described, classified, or compared with each other according to various criteria.
Melodic and harmonic.
An interval can be described as
Diatonic and chromatic.
In general,
The table above depicts the 56 diatonic intervals formed by the notes of the C major scale (a diatonic scale). Notice that these intervals, as well as any other diatonic interval, can be also formed by the notes of a chromatic scale.
The distinction between diatonic and chromatic intervals is controversial, as it is based on the definition of diatonic scale, which is variable in the literature. For example, the interval B–E♭ (a diminished fourth, occurring in the harmonic C-minor scale) is considered diatonic if the harmonic minor scales are considered diatonic as well. Otherwise, it is considered chromatic. For further details, see the main article.
By a commonly used definition of diatonic scale (which excludes the harmonic minor and melodic minor scales), all perfect, major and minor intervals are diatonic. Conversely, no augmented or diminished interval is diatonic, except for the augmented fourth and diminished fifth.
The distinction between diatonic and chromatic intervals may be also sensitive to context. The above-mentioned 56 intervals formed by the C-major scale are sometimes called "diatonic to C major". All other intervals are called "chromatic to C major". For instance, the perfect fifth A♭–E♭ is chromatic to C major, because A♭ and E♭ are not contained in the C major scale. However, it is diatonic to others, such as the A♭ major scale.
Consonant and dissonant.
Consonance and dissonance are relative terms that refer to the stability, or state of repose, of particular musical effects. Dissonant intervals are those that cause tension, and desire to be "resolved" to consonant intervals.
These terms are relative to the usage of different compositional styles.
All of the above analyses refer to vertical (simultaneous) intervals.
Simple and compound.
A simple interval is an interval spanning at most one octave (see Main intervals above). Intervals spanning more than one octave are called compound intervals, as they can be obtained by adding one or more octaves to a simple interval (see below for details).
Steps and skips.
Linear (melodic) intervals may be described as "steps" or "skips". A "step", or "conjunct motion",
is a linear interval between two consecutive notes of a scale. Any larger interval is called a "skip" (also called a "leap"), or "disjunct motion". In the diatonic scale, a step is either a minor second (sometimes also called "half step") or major second (sometimes also called "whole step"), with all intervals of a minor third or larger being skips.
For example, C to D (major second) is a step, whereas C to E (major third) is a skip.
More generally, a step is a smaller or narrower interval in a musical line, and a skip is a wider or larger interval, with the categorization of intervals into steps and skips is determined by the tuning system and the pitch space used.
Melodic motion in which the interval between any two consecutive pitches is no more than a step, or, less strictly, where skips are rare, is called "stepwise" or "conjunct" melodic motion, as opposed to "skipwise" or "disjunct" melodic motions, characterized by frequent skips.
Enharmonic intervals.
Two intervals are considered to be "enharmonic", or "enharmonically equivalent", if they both contain the same pitches spelled in different ways; that is, if the notes in the two intervals are themselves enharmonically equivalent. Enharmonic intervals span the same number of semitones.
For example, the four intervals listed in the table below are all enharmonically equivalent, because the notes F♯ and G♭ indicate the same pitch, and the same is true for A♯ and B♭. All these intervals span four semitones.
When played on a piano keyboard, these intervals are indistinguishable as they are all played with the same two keys, but in a musical context the diatonic function of the notes incorporated is very different.
Minute intervals.
There are also a number of minute intervals not found in the chromatic scale or labeled with a diatonic function, which have names of their own. They may be described as microtones, and some of them can be also classified as commas, as they describe small discrepancies, observed in some tuning systems, between enharmonically equivalent notes. In the following list, the interval sizes in cents are approximate.
Compound intervals.
A compound interval is an interval spanning more than one octave. Conversely, intervals spanning at most one octave are called simple intervals (see Main intervals above).
In general, a compound interval may be defined by a sequence or "stack" of two or more simple intervals of any kind. For instance, a major tenth (two staff positions above one octave), also called "compound major third", spans one octave plus one major third.
Any compound interval can be always decomposed into one or more octaves plus one simple interval. For instance, a major seventeenth can be decomposed into two octaves and one major third, and this is the reason why it is called a compound major third, even when it is built by adding up four fifths.
The diatonic number "DN"c of a compound interval formed from "n" simple intervals with diatonic numbers "DN"1, "DN"2, ..., "DN"n, is determined by:
which can also be written as:
The quality of a compound interval is determined by the quality of the simple interval on which it is based. For instance, a compound major third is a major tenth (1+(8–1)+(3–1) = 10), or a major seventeenth (1+(8–1)+(8–1)+(3–1) = 17), and a compound perfect fifth is a perfect twelfth (1+(8–1)+(5–1) = 12) or a perfect nineteenth (1+(8–1)+(8–1)+(5–1) = 19). Notice that two octaves are a fifteenth, not a sixteenth (1+(8–1)+(8–1) = 15). Similarly, three octaves are a twenty-second (1+3*(8–1) = 22), and so on.
Main compound intervals.
It is also worth mentioning here the major seventeenth (28 semitones), an interval larger than two octaves which can be considered a multiple of a perfect fifth (7 semitones) as it can be decomposed into four perfect fifths (7 * 4 = 28 semitones), or two octaves plus a major third (12 + 12 + 4 = 28 semitones). Intervals larger than a major seventeenth seldom need to be spoken of, most often being referred to by their compound names, for example "two octaves plus a fifth" rather than "a 19th".
Intervals in chords.
Chords are sets of three or more notes. They are typically defined as the combination of intervals starting from a common note called the root of the chord. For instance a major triad is a chord containing three notes defined by the root and two intervals (major third and perfect fifth). Sometimes even a single interval (dyad) is considered to be a chord. Chords are classified based on the quality and number of the intervals which define them.
Chord qualities and interval qualities.
The main chord qualities are: major, minor, augmented, diminished, half-diminished, and dominant.
The symbols used for chord quality are similar to those used for interval quality (see above). In addition, + or aug is used for augmented, ° or dim for diminished, ø for half diminished, and dom for dominant (the symbol − alone is not used for diminished).
Deducing component intervals from chord names and symbols.
The main rules to decode chord "names or symbols" are summarized below. Further details are given at Rules to decode chord names and symbols.
The table shows the intervals contained in some of the main chords (component intervals), and some of the symbols used to denote them. The interval qualities or numbers in boldface font can be deduced from chord name or symbol by applying rule 1. In symbol examples, C is used as chord root.
Size of intervals used in different tuning systems.
In this table, the interval widths used in four different tuning systems are compared. To facilitate comparison, just intervals as provided by 5-limit tuning (see symmetric scale n.1) are shown in bold font, and the values in cents are rounded to integers. Notice that in each of the non-equal tuning systems, by definition the width of "each" type of interval (including the semitone) changes depending on the note from which the interval starts. This is the price paid for seeking just intonation. However, for the sake of simplicity, for some types of interval the table shows only one value (the most often observed one).
In 1/4-comma meantone, by definition 11 perfect fifths have a size of approximately 697 cents (700−ε cents, where ε ≈ 3.42 cents); since the average size of the 12 fifths must equal exactly 700 cents (as in equal temperament), the other one must have a size of about 738 cents (700+11ε, the wolf fifth or diminished sixth); 8 major thirds have size about 386 cents (400−4ε), 4 have size about 427 cents (400+8ε, actually diminished fourths), and their average size is 400 cents. In short, similar differences in width are observed for all interval types, except for unisons and octaves, and they are all multiples of ε (the difference between the 1/4-comma meantone fifth and the average fifth). A more detailed analysis is provided at 1/4-comma meantone Size of intervals. Note that 1/4-comma meantone was designed to produce just major thirds, but only 8 of them are just (5:4, about 386 cents).
The Pythagorean tuning is characterized by smaller differences because they are multiples of a smaller ε (ε ≈ 1.96 cents, the difference between the Pythagorean fifth and the average fifth). Notice that here the fifth is wider than 700 cents, while in most meantone temperaments, including 1/4-comma meantone, it is tempered to a size smaller than 700. A more detailed analysis is provided at Pythagorean tuning#Size of intervals.
The 5-limit tuning system uses just tones and semitones as building blocks, rather than a stack of perfect fifths, and this leads to even more varied intervals throughout the scale (each kind of interval has three or four different sizes). A more detailed analysis is provided at 5-limit tuning#Size of intervals. Note that 5-limit tuning was designed to maximize the number of just intervals, but even in this system some intervals are not just (e.g., 3 fifths, 5 major thirds and 6 minor thirds are not just; also, 3 major and 3 minor thirds are wolf intervals).
The above-mentioned symmetric scale 1, defined in the 5-limit tuning system, is not the only method to obtain just intonation. It is possible to construct juster intervals or just intervals closer to the equal-tempered equivalents, but most of the ones listed above have been used historically in equivalent contexts. In particular, the asymmetric version of the 5-limit tuning scale provides a juster value for the minor seventh (9:5, rather than 16:9). Moreover, the tritone (augmented fourth or diminished fifth), could have other just ratios; for instance, 7:5 (about 583 cents) or 17:12 (about 603 cents) are possible alternatives for the augmented fourth (the latter is fairly common, as it is closer to the equal-tempered value of 600 cents). The 7:4 interval (about 969 cents), also known as the harmonic seventh, has been a contentious issue throughout the history of music theory; it is 31 cents flatter than an equal-tempered minor seventh. Some assert the 7:4 is one of the blue notes used in jazz. For further details about reference ratios, see 5-limit tuning#The justest ratios.
In the diatonic system, every interval has one or more "enharmonic equivalents", such as augmented second for minor third.
Interval root.
Although intervals are usually designated in relation to their lower note, David Cope and Hindemith both suggest the concept of interval root. To determine an interval's root, one locates its nearest approximation in the harmonic series. The root of a perfect fourth, then, is its "top" note because it is an octave of the fundamental in the hypothetical harmonic series. The bottom note of every odd diatonically numbered intervals are the roots, as are the tops of all even numbered intervals. The root of a collection of intervals or a chord is thus determined by the interval root of its strongest interval.
As to its usefulness, Cope provides the example of the final tonic chord of some popular music being traditionally analyzable as a "submediant six-five chord" (added sixth chords by popular terminology), or a first inversion seventh chord (possibly the dominant of the mediant V/iii). According the interval root of the strongest interval of the chord (in first inversion, CEGA), the perfect fifth (C–G), is the bottom C, the tonic.
Interval cycles.
Interval cycles, "unfold [i.e., repeat] a single recurrent interval in a series that closes with a return to the initial pitch class", and are notated by George Perle using the letter "C", for cycle, with an interval-class integer to distinguish the interval. Thus the diminished-seventh chord would be C3 and the augmented triad would be C4. A superscript may be added to distinguish between transpositions, using 0–11 to indicate the lowest pitch class in the cycle.
Alternative interval naming conventions.
As shown below, some of the above-mentioned intervals have alternative names, and some of them take a specific alternative name in Pythagorean tuning, five-limit tuning, or meantone temperament tuning systems such as quarter-comma meantone. All the intervals with prefix "sesqui-" are justly tuned, and their frequency ratio, shown in the table, is a superparticular number (or epimoric ratio). The same is true for the octave.
Typically, a comma is a diminished second, but this is not always true (for more details, see Alternative definitions of comma). For instance, in Pythagorean tuning the diminished second is a descending interval (524288:531441, or about -23.5 cents), and the Pythagorean comma is its opposite (531441:524288, or about 23.5 cents). 5-limit tuning defines four kinds of comma, three of which meet the definition of diminished second, and hence are listed in the table below. The fourth one, called syntonic comma (81:80) can neither be regarded as a diminished second, nor as its opposite. See Diminished seconds in 5-limit tuning for further details.
Additionally, some cultures around the world have their own names for intervals found in their music. For instance, 22 kinds of intervals, called shrutis, are canonically defined in Indian classical music.
Latin nomenclature.
Up to the end of the 18th century, Latin was used as an official language throughout Europe for scientific and music textbooks. In music, many English terms are derived from Latin. For instance, semitone is from Latin "semitonus".
The prefix semi- is typically used herein to mean "shorter", rather than "half". Namely, a semitonus, semiditonus, semidiatessaron, semidiapente, semihexachordum, semiheptachordum, or semidiapason, is shorter by one semitone than the corresponding whole interval. For instance, a semiditonus (3 semitones, or about 300 cents) is not half of a ditonus (4 semitones, or about 400 cents), but a ditonus shortened by one semitone. Moreover, in Pythagorean tuning (the most commonly used tuning system up to the 16th century), a semitritonus (d5) is smaller than a tritonus (A4) by one Pythagorean comma (about a quarter of a semitone).
Pitch-class intervals.
In post-tonal or atonal theory, originally developed for equal-tempered European classical music written using the twelve-tone technique or serialism, integer notation is often used, most prominently in musical set theory. In this system, intervals are named according to the number of half steps, from 0 to 11, the largest interval class being 6.
In atonal or musical set theory, there are numerous types of intervals, the first being the ordered pitch interval, the distance between two pitches upward or downward. For instance, the interval from C upward to G is 7, and the interval from G downward to C is −7. One can also measure the distance between two pitches without taking into account direction with the unordered pitch interval, somewhat similar to the interval of tonal theory.
The interval between pitch classes may be measured with ordered and unordered pitch-class intervals. The ordered one, also called directed interval, may be considered the measure upwards, which, since we are dealing with pitch classes, depends on whichever pitch is chosen as 0. For unordered pitch-class intervals, see interval class.
Generic and specific intervals.
In diatonic set theory, specific and generic intervals are distinguished. Specific intervals are the interval class or number of semitones between scale steps or collection members, and generic intervals are the number of diatonic scale steps (or staff positions) between notes of a collection or scale.
Notice that staff positions, when used to determine the conventional interval number (second, third, fourth, etc.), are counted including the position of the lower note of the interval, while generic interval numbers are counted excluding that position. Thus, generic interval numbers are smaller by 1, with respect to the conventional interval numbers.
Generalizations and non-pitch uses.
The term "interval" can also be generalized to other music elements besides pitch. David Lewin's "Generalized Musical Intervals and Transformations" uses interval as a generic measure of distance between time points, timbres, or more abstract musical phenomena.
Notes.
Gardner, Carl E. (1912) - Essentials of Music Theory, p. 38, http://ia600309.us.archive.org/23/items/essentialsofmusi00gard/essentialsofmusi00gard.pdf

</doc>
<doc id="49172" url="http://en.wikipedia.org/wiki?curid=49172" title="Interval (mathematics)">
Interval (mathematics)

In mathematics, an (real) interval is a set of real numbers with the property that any number that lies between two numbers in the set is also included in the set. For example, the set of all numbers x satisfying 0 ≤ "x" ≤ 1 is an interval which contains 0 and 1, as well as all numbers between them. Other examples of intervals are the set of all real numbers formula_1, the set of all negative real numbers, and the empty set.
Real intervals play an important role in the theory of integration, because they are the simplest sets whose "size" or "measure" or "length" is easy to define. The concept of measure can then be extended to more complicated sets of real numbers, leading to the Borel measure and eventually to the Lebesgue measure.
Intervals are central to interval arithmetic, a general numerical computing technique that automatically provides guaranteed enclosures for arbitrary formulas, even in the presence of uncertainties, mathematical approximations, and arithmetic roundoff.
Intervals are likewise defined on an arbitrary totally ordered set, such as integers or rational numbers. The notation of integer intervals is considered in the special section below.
Notations for intervals.
The interval of numbers between a and b, including a and b, is often denoted ["a", "b"]. The two numbers are called the "endpoints" of the interval. In countries where numbers are written with a decimal comma, a semicolon may be used as a separator, to avoid ambiguity.
Including or excluding endpoints.
To indicate that one of the endpoints is to be excluded from the set, the corresponding square bracket can be either replaced with a parenthesis, or reversed. Both notations are described in International standard ISO 31-11. Thus, in set builder notation,
Note that ("a", "a"), ["a", "a"), and ("a", "a"] each represents the empty set, whereas ["a", "a"] denotes the set {"a"}. When "a" > "b", all four notations are usually taken to represent the empty set.
Both notations may overlap with other uses of parentheses and brackets in mathematics. For instance, the notation ("a", "b") is often used to denote an ordered pair in set theory, the coordinates of a point or vector in analytic geometry and linear algebra, or (sometimes) a complex number in algebra. That is why Bourbaki introduced the notation ]"a", "b"[ to denote the open interval. The notation ["a", "b"] too is occasionally used for ordered pairs, especially in computer science.
Some authors use ]"a", "b"[ to denote the complement of the interval ("a", "b"); namely, the set of all real numbers that are either less than or equal to a, or greater than or equal to b.
Infinite endpoints.
In both styles of notation, one may use an infinite endpoint to indicate that there is no bound in that direction. Specifically, one may use "a" = −∞ or "b" = +∞ (or both). For example, (0, +∞) is the set of all positive real numbers, and (−∞, +∞) is the set of real numbers.
The extended real number line includes −∞ and +∞ as elements. The notations [−∞, "b"] , [−∞, "b") , ["a", +∞] , and ("a", +∞] may be used in this context. For example (−∞, +∞] means the extended real numbers excluding only −∞.
Integer intervals.
The notation ["a" .. "b"] when a and b are integers, or {"a" .. "b"}, or just "a" .. "b" is sometimes used to indicate the interval of all "integers" between a and b, including both. This notation is used in some programming languages; in Pascal, for example, it is used to formally define a subrange type, most frequently used to specify lower and upper bounds of valid indices of an array.
An integer interval that has a finite lower or upper endpoint always includes that endpoint. Therefore, the exclusion of endpoints can be explicitly denoted by writing "a" .. "b" − 1 , "a" + 1 .. "b" , or "a" + 1 .. "b" − 1. Alternate-bracket notations like ["a" .. "b") or ["a" .. "b"[ are rarely used for integer intervals.
Terminology.
An open interval does not include its endpoints, and is indicated with parentheses. For example (0,1) means greater than 0 and less than 1. A closed interval includes its endpoints, and is denoted with square brackets. For example [0,1] means greater than or equal to 0 and less than or equal to 1.
A degenerate interval is any set consisting of a single real number. Some authors include the empty set in this definition. A real interval that is neither empty nor degenerate is said to be proper, and has infinitely many elements.
An interval is said to be left-bounded or right-bounded if there is some real number that is, respectively, smaller than or larger than all its elements. An interval is said to be bounded if it is both left- and right-bounded; and is said to be unbounded otherwise. Intervals that are bounded at only one end are said to be half-bounded. The empty set is bounded, and the set of all reals is the only interval that is unbounded at both ends. Bounded intervals are also commonly known as finite intervals.
Bounded intervals are bounded sets, in the sense that their diameter (which is equal to the absolute difference between the endpoints) is finite. The diameter may be called the length, width, measure, or size of the interval. The size of unbounded intervals is usually defined as +∞, and the size of the empty interval may be defined as 0 or left undefined.
The centre (midpoint) of bounded interval with endpoints a and b is ("a" + "b")/2, and its radius is the half-length |"a" − "b"|/2. These concepts are undefined for empty or unbounded intervals.
An interval is said to be left-open if and only if it has no minimum (an element that is smaller than all other elements); right-open if it has no maximum; and open if it has both properties. The interval [0,1) = {"x"|0 ≤ "x" < 1}, for example, is left-closed and right-open. The empty set and the set of all reals are open intervals, while the set of non-negative reals, for example, is a right-open but not left-open interval. The open intervals coincide with the open sets of the real line in its standard topology.
An interval is said to be left-closed if it has a minimum element, right-closed if it has a maximum, and simply closed if it has both. These definitions are usually extended to include the empty set and to the (left- or right-) unbounded intervals, so that the closed intervals coincide with closed sets in that topology.
The interior of an interval I is the largest open interval that is contained in I; it is also the set of points in I which are not endpoints of I. The closure of I is the smallest closed interval that contains I; which is also the set I augmented with its finite endpoints.
For any set X of real numbers, the interval enclosure or interval span of X is the unique interval that contains X and does not properly contain any other interval that also contains X.
Classification of intervals.
The intervals of real numbers can be classified into eleven different types, listed below; where a and b are real numbers, with formula_3:
Intervals of the extended real line.
In some contexts, an interval may be defined as a subset of the extended real numbers, the set of all real numbers augmented with −∞ and +∞.
In this interpretation, the notations [−∞, "b"] , [−∞, "b") , ["a", +∞] , and ("a", +∞] are all meaningful and distinct. In particular, (−∞, +∞) denotes the set of all ordinary real numbers, while [−∞, +∞] denotes the extended reals.
This choice affects some of the above definitions and terminology. For instance, the interval (−∞, +∞) = formula_1 is closed in the realm of ordinary reals, but not in the realm of the extended reals.
Properties of intervals.
The intervals are precisely the connected subsets of formula_1. It follows that the image of an interval by any continuous function is also an interval. This is one formulation of the intermediate value theorem.
The intervals are also the convex subsets of formula_1. The interval enclosure of a subset formula_18 is also the convex hull of formula_19.
The intersection of any collection of intervals is always an interval. The union of two intervals is an interval if and only if they have a non-empty intersection or an open end-point of one interval is a closed end-point of the other (e.g., formula_20).
If formula_1 is viewed as a metric space, its open balls are the open bounded sets ("c" + "r", "c" − "r"), and its closed balls are the closed bounded sets ["c" + "r", "c" − "r"].
Any element x of an interval I defines a partition of I into three disjoint intervals I1, I2, I3: respectively, the elements of I that are less than x, the singleton formula_22, and the elements that are greater than x. The parts I1 and I3 are both non-empty (and have non-empty interiors) if and only if x is in the interior of I. This is an interval version of the trichotomy principle.
Dyadic intervals.
A "dyadic interval" is a bounded real interval whose endpoints are formula_23 and formula_24, where formula_25 and formula_26 are integers. Depending on the context, either endpoint may or may not be included in the interval.
Dyadic intervals have the following properties:
The dyadic intervals consequently have a structure that reflects that of an infinite binary tree.
Dyadic intervals are relevant to several areas of numerical analysis, including adaptive mesh refinement, multigrid methods and wavelet analysis. Another way to represent such a structure is p-adic analysis (for "p" = 2).
Generalizations.
Multi-dimensional intervals.
In many contexts, an formula_26-dimensional interval is defined as a subset of formula_28 that is the Cartesian product of formula_26 intervals, formula_30, one on each coordinate axis.
For formula_31, this generally defines a rectangle whose sides are parallel to the coordinate axes; for formula_32, it defines an axis-aligned rectangular box.
A facet of such an interval formula_33 is the result of replacing any non-degenerate interval factor formula_34 by a degenerate interval consisting of a finite endpoint of formula_34. The faces of formula_33 comprise formula_33 itself and all faces of its facets. The corners of formula_33 are the faces that consist of a single point of formula_28.
Complex intervals.
Intervals of complex numbers can be defined as regions of the complex plane, either rectangular or circular.
Topological algebra.
Intervals can be associated with points of the plane and hence regions of intervals can be associated with regions of the plane. Generally, an interval in mathematics corresponds to an ordered pair ("x,y") taken from the direct product R × R of real numbers with itself. Often it is assumed that "y" > "x". For purposes of mathematical structure, this restriction is discarded, and "reversed intervals" where "y" − "x" < 0 are allowed. Then the collection of all intervals ["x,y"] can be identified with the topological ring formed by the direct sum of R with itself where addition and multiplication are defined component-wise.
The direct sum algebra formula_40 has two ideals, { ["x",0] : "x" ∈ R } and { [0,"y"] : "y" ∈ R }. The identity element of this algebra is the condensed interval [1,1]. If interval ["x,y"] is not in one of the ideals, then it has multiplicative inverse [1/"x", 1/"y"]. Endowed with the usual topology, the algebra of intervals forms a topological ring. The group of units of this ring consists of four quadrants determined by the axes, or ideals in this case. The identity component of this group is quadrant I.
Every interval can be considered a symmetric interval around its midpoint. In a reconfiguration published in 1956 by M Warmus, the axis of "balanced intervals" ["x", −"x"] is used along with the axis of intervals ["x,x"] that reduce to a point.
Instead of the direct sum formula_41, the ring of intervals has been identified with the split-complex number plane by M. Warmus and D. H. Lehmer through the identification
This linear mapping of the plane, which amounts of a ring isomorphism, provides the plane with a multiplicative structure having some analogies to ordinary complex arithmetic, such as polar decomposition.

</doc>
<doc id="49176" url="http://en.wikipedia.org/wiki?curid=49176" title="Conjugacy class">
Conjugacy class

In mathematics, especially group theory, the elements of any group may be partitioned into conjugacy classes; members of the same conjugacy class share many properties, and study of conjugacy classes of non-abelian groups reveals many important features of their structure. For an abelian group, each conjugacy class is a set containing one element (singleton set).
Functions that are constant for members of the same conjugacy class are called class functions.
Definition.
Suppose "G" is a group. Two elements "a" and "b" of "G" are called conjugate if there exists an element "g" in "G" with
It can be easily shown that conjugacy is an equivalence relation and therefore partitions "G" into equivalence classes. (This means that every element of the group belongs to precisely one conjugacy class, and the classes Cl("a") and Cl("b") are equal if and only if "a" and "b" are conjugate, and disjoint otherwise.) The equivalence class that contains the element "a" in "G" is
and is called the conjugacy class of "a". The class number of "G" is the number of distinct (nonequivalent) conjugacy classes. All elements belonging to the same conjugacy class have the same order.
Conjugacy classes may be referred to by describing them, or more briefly by abbreviations such as "6A", meaning "a certain conjugacy class of order 6 elements", and "6B" would be a different conjugacy class of order 6 elements; the conjugacy class 1A is the conjugacy class of the identity. In some cases, conjugacy classes can be described in a uniform way – for example, in the symmetric group they can be described by cycle structure.
Examples.
The symmetric group "S"3, consisting of all 6 permutations of three elements, has three conjugacy classes:
These three classes also correspond to the classification of the isometries of an equilateral triangle. 
The symmetric group "S"4, consisting of all 24 permutations of four elements, has five conjugacy classes, listed with their cycle structures and orders:
In general, the number of conjugacy classes in the symmetric group "S"n is equal to the number of integer partitions of "n". This is because each conjugacy class corresponds to exactly one partition of {1, 2, ..., "n"} into cycles, up to permutation of the elements of {1, 2, ..., "n"}.
The proper rotations of the cube, which can be characterized by permutations of the body diagonals, are also described by conjugation in "S"4 .
In general, the Euclidean group can be studied by conjugation of isometries in Euclidean space.
Conjugacy class equation.
If "G" is a finite group, then for any group element "a", the elements in the conjugacy class of "a" are in one-to-one correspondence with cosets of the centralizer C"G"("a"). This can be seen by observing that any two elements "b" and "c" belonging to the same coset (and hence, "b" = "cz" for some "z" in the centralizer C"G"("a")) give rise to the same element when conjugating "a": "bab"−1 = "cza"("cz")−1 = "czaz"−1"c"−1 = "czz"−1"ac"−1 = "cac"−1.
Thus the number of elements in the conjugacy class of "a" is the index ["G":C"G"("a")] of the centralizer C"G"("a") in "G"; hence the size of each conjugacy class divides the order of the group.
Furthermore, if we choose a single representative element "x""i" from every conjugacy class, we infer from the disjointness of the conjugacy classes that |"G"| = ∑"i" ["G" : C"G"("x""i")], where C"G"("x""i") is the centralizer of the element "x""i". Observing that each element of the center Z("G") forms a conjugacy class containing just itself gives rise to the class equation:
where the sum is over a representative element from each conjugacy class that is not in the center.
Knowledge of the divisors of the group order |"G"| can often be used to gain information about the order of the center or of the conjugacy classes.
Example.
Consider a finite "p"-group "G" (that is, a group with order "p""n", where "p" is a prime number and "n" > 0). We are going to prove that "every finite p-group has a non-trivial center".
Since the order of any conjugacy class of "G" must divide the order of "G", it follows that each conjugacy class "H""i" also has order some power of "p""k""i", where 0 < "k""i" < "n". But then the class equation requires that |"G"| = "p""n" = |Z("G")| + ∑"i" "p""k""i". From this we see that "p" must divide |Z("G")|, so |Z("G")| > 1.
Conjugacy of subgroups and general subsets.
More generally, given any subset "S" of "G" ("S" not necessarily a subgroup), we define a subset "T" of "G" to be conjugate to "S" if there exists some "g" in "G" such that "T" = "gSg"−1. We can define Cl("S") as the set of all subsets "T" of "G" such that "T" is conjugate to "S".
A frequently used theorem is that, given any subset "S" of "G", the index of N("S") (the normalizer of "S") in "G" equals the order of Cl("S"):
This follows since, if "g" and "h" are in "G", then "gSg"−1 = "hSh"−1 if and only if "g"−1"h" is in N("S"), in other words, if and only if "g" and "h" are in the same coset of N("S").
Note that this formula generalizes the one given earlier for the number of elements in a conjugacy class (let "S" = {"a"}).
The above is particularly useful when talking about subgroups of "G". The subgroups can thus be divided into conjugacy classes, with two subgroups belonging to the same class if and only if they are conjugate.
Conjugate subgroups are isomorphic, but isomorphic subgroups need not be conjugate. For example, an abelian group may have two different subgroups which are isomorphic, but they are never conjugate.
Conjugacy as group action.
If we define
for any two elements "g" and "x" in "G", then we have a group action of "G" on "G". The orbits of this action are the conjugacy classes, and the stabilizer of a given element is the element's centralizer.
Similarly, we can define a group action of "G" on the set of all subsets of "G", by writing
or on the set of the subgroups of "G".
Geometric interpretation.
Conjugacy classes in the fundamental group of a path-connected topological space can be thought of as equivalence classes of free loops under free homotopy.

</doc>
<doc id="49180" url="http://en.wikipedia.org/wiki?curid=49180" title="Fuzzy logic">
Fuzzy logic

Fuzzy logic is a form of many-valued logic that deals with approximate, rather than fixed and exact reasoning. Compared to traditional binary logic (where variables may take on true or false values), fuzzy logic variables may have a truth value that ranges in degree between 0 and 1. Fuzzy logic has been extended to handle the concept of partial truth, where the truth value may range between completely true and completely false. Furthermore, when linguistic variables are used, these degrees may be managed by specific functions.
The term "fuzzy logic" was introduced with the 1965 proposal of fuzzy set theory by Lotfi A. Zadeh. Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. Fuzzy logic had, however, been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.
Overview.
Classical logic only permits propositions having a value of truth or falsity. The notion of whether 1+1=2 is an absolute, immutable and mathematical truth. However, there exist certain propositions with variable answers, such as asking various people to identify a colour. The notion of truth doesn't fall by the wayside, but rather on a means of representing and reasoning over partial knowledge when afforded, by aggregating all possible outcomes into a dimensional spectrum.
Both degrees of truth and probabilities range between 0 and 1 and hence may seem similar at first. For example, let a 100 ml glass contain 30 ml of water. Then we may consider two concepts: empty and full. The meaning of each of them can be represented by a certain fuzzy set. Then one might define the glass as being 0.7 empty and 0.3 full. Note that the concept of emptiness would be subjective and thus would depend on the observer or designer. Another designer might, equally well, design a set membership function where the glass would be considered full for all values down to 50 ml. It is essential to realize that fuzzy logic uses truth degrees as a mathematical model of the vagueness phenomenon while probability is a mathematical model of ignorance.
Applying truth values.
A basic application might characterize various sub-ranges of a continuous variable. For instance, a temperature measurement for anti-lock brakes might have several separate membership functions defining particular temperature ranges needed to control the brakes properly. Each function maps the same temperature value to a truth value in the 0 to 1 range. These truth values can then be used to determine how the brakes should be controlled.
In this image, the meanings of the expressions "cold", "warm", and "hot" are represented by functions mapping a temperature scale. A point on that scale has three "truth values" — one for each of the three functions. The vertical line in the image represents a particular temperature that the three arrows (truth values) gauge. Since the red arrow points to zero, this temperature may be interpreted as "not hot". The orange arrow (pointing at 0.2) may describe it as "slightly warm" and the blue arrow (pointing at 0.8) "fairly cold".
Linguistic variables.
While variables in mathematics usually take numerical values, in fuzzy logic applications, the non-numeric are often used to facilitate the expression of rules and facts.
A linguistic variable such as "age" may have a value such as "young" or its antonym "old". However, the great utility of linguistic variables is that they can be modified via linguistic hedges applied to primary terms. These linguistic hedges can be associated with certain functions.
Early applications.
The Japanese were the first to utilize fuzzy logic for practical applications. The first notable application was on the high-speed train in Sendai, in which fuzzy logic was able to improve the economy, comfort, and precision of the ride. It has also been used in recognition of hand written symbols in Sony pocket computers; flight aid for helicopters; controlling of subway systems in order to improve driving comfort, precision of halting, and power economy; improved fuel consumption for auto mobiles; single-button control for washing machines, automatic motor control for vacuum cleaners with recognition of surface condition and degree of soiling; and prediction systems for early recognition of earthquakes through the Institute of Seismology Bureau of Metrology, Japan.
Example.
Hard science with IF-THEN rules.
Fuzzy set theory defines fuzzy operators on fuzzy sets. The problem in applying this is that the appropriate fuzzy operator may not be known.
For example, a simple temperature regulator that uses a fan might look like this:
IF temperature IS very cold THEN stop fan
IF temperature IS cold THEN turn down fan
IF temperature IS normal THEN maintain fan
IF temperature IS hot THEN speed up fan
There is no "ELSE" – all of the rules are evaluated, because the temperature might be "cold" and "normal" at the same time to different degrees.
The AND, OR, and NOT operators of boolean logic exist in fuzzy logic, usually defined as the minimum, maximum, and complement; when they are defined this way, they are called the "Zadeh operators". So for the fuzzy variables x and y:
NOT x = (1 - truth(x))
x AND y = minimum(truth(x), truth(y))
x OR y = maximum(truth(x), truth(y))
There are also other operators, more linguistic in nature, called "hedges" that can be applied. These are generally adverbs such as "very", or "somewhat", which modify the meaning of a set using a mathematical formula.
Define with multiply.
x AND y = x*y
x OR y = 1-(1-x)*(1-y)
1-(1-x)*(1-y) comes from this:
x OR y = NOT( AND( NOT(x), NOT(y) ) )
x OR y = NOT( AND(1-x, 1-y) )
x OR y = NOT( (1-x)*(1-y) )
x OR y = 1-(1-x)*(1-y)
Define with sigmoid.
sigmoid(x)=1/(1+e^-x)
sigmoid(x)+sigmoid(-x) = 1
(sigmoid(x)+sigmoid(-x))*(sigmoid(y)+sigmoid(-y))*(sigmoid(z)+sigmoid(-z)) = 1
Logical analysis.
In mathematical logic, there are several formal systems of "fuzzy logic"; most of them belong among so-called t-norm fuzzy logic.
Propositional fuzzy logics.
The most important propositional fuzzy logics are:-
Predicate fuzzy logics.
These extend the above-mentioned fuzzy logics by adding universal and existential quantifiers in a manner similar to the way that predicate logic is created from propositional logic. The semantics of the universal (resp. existential) quantifier in t-norm fuzzy logics is the infimum (resp. supremum) of the truth degrees of the instances of the quantified subformula.
Decidability issues for fuzzy logic.
The notions of a "decidable subset" and "recursively enumerable subset" are basic ones for classical mathematics and classical logic. Thus the question of a suitable extension of these concepts to fuzzy set theory arises. A first proposal in such a direction was made by E.S. Santos by the notions of "fuzzy Turing machine", "Markov normal fuzzy algorithm" and "fuzzy program" (see Santos 1970). Successively, L. Biacino and G. Gerla argued that the proposed definitions are rather questionable and therefore they proposed the following ones. Denote by "Ü" the set of rational numbers in [0,1]. Then a fuzzy subset "s" : "S" formula_1[0,1] of a set "S" is "recursively enumerable" if a recursive map "h" : "S"×"N" formula_1"Ü" exists such that, for every "x" in "S", the function "h"("x","n") is increasing with respect to "n" and "s"("x") = lim "h"("x","n").
We say that "s" is "decidable" if both "s" and its complement –"s" are recursively enumerable. An extension of such a theory to the general case of the L-subsets is possible (see Gerla 2006).
The proposed definitions are well related with fuzzy logic. Indeed, the following theorem holds true (provided that the deduction apparatus of the considered fuzzy logic satisfies some obvious effectiveness property).
Theorem. Any axiomatizable fuzzy theory is recursively enumerable. In particular, the fuzzy set of logically true formulas is recursively enumerable in spite of the fact that the crisp set of valid formulas is not recursively enumerable, in general. Moreover, any axiomatizable and complete theory is decidable.
It is an open question to give supports for a "Church thesis" for fuzzy mathematics the proposed notion of recursive enumerability for fuzzy subsets is the adequate one. To this aim, an extension of the notions of fuzzy grammar and fuzzy Turing machine should be necessary (see for example Wiedermann's paper). Another open question is to start from this notion to find an extension of Gödel's theorems to fuzzy logic.
It is known that any boolean logic function could be represented using a truth table mapping each set of variable values into set of values formula_3. The task of synthesis of boolean logic function given in tabular form is one of basic tasks in traditional logic that is solved via disjunctive (conjunctive) perfect normal form.
Each fuzzy (continuous) logic function could be represented by a choice table containing all possible variants of comparing arguments and their negations. A choice table maps each variant into value of an argument or a negation of an argument. For instance, for two arguments a row of choice table contains a variant of comparing values formula_4, formula_5, formula_6, formula_7 and the corresponding function value formula_8.
The task of synthesis of fuzzy logic function given in tabular form was solved in. New concepts of constituents of minimum and maximum were introduced. The sufficient and necessary conditions that a choice table defines a fuzzy logic function were derived.
Fuzzy databases.
Once fuzzy relations are defined, it is possible to develop fuzzy relational databases. The first fuzzy relational database, FRDB, appeared in Maria Zemankova's dissertation. Later, some other models arose like the Buckles-Petry model, the Prade-Testemale Model, the Umano-Fukami model or the GEFRED model by J.M. Medina, M.A. Vila et al. In the context of fuzzy databases, some fuzzy querying languages have been defined, highlighting the SQLf by P. Bosc et al. and the FSQL by J. Galindo et al. These languages define some structures in order to include fuzzy aspects in the SQL statements, like fuzzy conditions, fuzzy comparators, fuzzy constants, fuzzy constraints, fuzzy thresholds, linguistic labels and so on.
Much progress has been made to take fuzzy logic database applications to the web and let the world easily use them, for example: http://sullivansoftwaresystems.com/cgi-bin/fuzzy-logic-match-algorithm.cgi?SearchString=garia This enables fuzzy logic matching to be incorporated into a database system or application.
Comparison to probability.
Fuzzy logic and probability address different forms of uncertainty. While both fuzzy logic and probability theory can represent degrees of certain kinds of subjective belief, fuzzy set theory uses the concept of fuzzy set membership, i.e., "how much" a variable is in a set (there is not necessarily any uncertainty about this degree), and probability theory uses the concept of subjective probability, i.e., "how probable" is it that a variable is in a set (it either entirely is or entirely is not in the set in reality, but there is uncertainty around whether it is or is not). The technical consequence of this distinction is that fuzzy set theory relaxes the axioms of classical probability, which are themselves derived from adding uncertainty, but not degree, to the crisp true/false distinctions of classical Aristotelian logic.
Bruno de Finetti argues that only one kind of mathematical uncertainty, probability, is needed, and thus fuzzy logic is unnecessary. However, Bart Kosko shows in that probability theory is a subtheory of fuzzy logic, as questions of degrees of belief in mutually-exclusive set membership in probability theory can be represented as certain cases of non-mutually-exclusive graded membership in fuzzy theory. In that context, he also derives Bayes' theorem from the concept of fuzzy subsethood. Lotfi A. Zadeh argues that fuzzy logic is different in character from probability, and is not a replacement for it. He fuzzified probability to fuzzy probability and also generalized it to possibility theory. (cf.)
More generally, fuzzy logic is one of many different extensions to classical logic intended to deal with issues of uncertainty outside of the scope of classical logic, the inapplicability of probability theory in many domains, and the paradoxes of Dempster-Shafer theory. See also probabilistic logics.
Relation to ecorithms.
Leslie Valiant, winner of the Turing Award, uses the term "ecorithms" to describe how many less exact systems and techniques like fuzzy logic (and "less robust" logic) can be applied to learning algorithms. Valiant essentially redefines machine learning as evolutionary. Ecorithms and fuzzy logic also have the common property of dealing with possibilities more than probabilities, although feedback and feed forward, basically stochastic "weights," are a feature of both when dealing with, for example, dynamical systems.
In general use, ecorithms are algorithms that learn from their more complex environments (Hence Eco) to generalize, approximate and simplify solution logic. Like fuzzy logic, they are methods used to overcome continuous variables or systems too complex to completely enumerate or understand discretely or exactly. See in particular p. 58 of the reference comparing induction/invariance, robust, mathematical and other logical limits in computing, where techniques including fuzzy logic and natural data selection (à la "computational Darwinism") can be used to short-cut computational complexity and limits in a "practical" way (such as the brake temperature example in this article).
Compensatory Fuzzy Logic.
The CFL (Compensatory Fuzzy Logic) is a branch of Fuzzy Logic. This is a new multivalent system that breaks with traditional axioms of such systems to achieve better semantic behaviour to classical systems.
In processes involving decision making, trade with the experts leads to obtaining complex and subtle formulations and requires compound predicates. The truth values obtained on these compound predicates must possess sensitivity to changes in the truth values of basic predicates.
This need is met by the use of the CFL, waiving compliance of the classical properties of conjunction and disjunction and rather opposing to them the idea that the increase or decrease of the truth value of the conjunction or disjunction caused by change the truth value of one of its components can be compensated with a corresponding decrease or increase in the other. This increase or decrease in truth may be offset by the increase or decrease in another component. This notion makes the CFL logical and useful. There are cases in which compensation is not possible. This occurs when certain thresholds are violated and there is a veto preventing compensation.
Compensatory Fuzzy Logic consists of four continuous operators: conjunction (c); disjunction (d); fuzzy strict order (or); and negation (n). The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators.

</doc>
<doc id="49181" url="http://en.wikipedia.org/wiki?curid=49181" title="588 Achilles">
588 Achilles

588 Achilles is the first Jupiter trojan discovered. It was discovered on February 22, 1906, by the German astronomer Max Wolf. It is named after Achilles, the fictional hero from the "Iliad". It orbits in the L4 Lagrangian point of the Sun–Jupiter system. After a few such asteroids were discovered, the rule was established that the L4 point was the "Greek camp", whereas the L5 point was the "Trojan camp", though not before each camp had acquired a "spy" (624 Hektor in the Greek camp and 617 Patroclus in the Trojan camp).
Based on IRAS data, Achilles is 135 km in diameter and is the 6th-largest Jupiter trojan.
Photometric observations of this asteroid during 1994 were used to build a light curve showing a rotation period of 7.32 ± 0.02 hours with a brightness variation of 0.31 ± 0.01 magnitude. This result is in good agreement with prior studies.

</doc>
<doc id="49184" url="http://en.wikipedia.org/wiki?curid=49184" title="How the Self Controls Its Brain">
How the Self Controls Its Brain

How the Self Controls Its Brain is a book by Sir John Eccles, proposing a theory of philosophical dualism, and offering a justification of how there can be mind-brain action without violating the principle of the conservation of energy. The model was developed jointly with the nuclear physicist Friedrich Beck in the period 1991-1992.
Eccles called the fundamental neural units of the cerebral cortex "dendrons", which are cylindrical bundles of neurons arranged vertically in the six outer layers or laminae of the cortex, each cylinder being about 60 micrometres in diameter. Eccles proposed that each of the 40 million dendrons is linked with a mental unit, or "psychon", representing a unitary conscious experience. In willed actions and thought, psychons act on dendrons and, for a moment, increase the probability of the firing of selected neurons through quantum tunneling effect in synaptic exocytosis, while in perception the reverse process takes place.

</doc>
<doc id="49185" url="http://en.wikipedia.org/wiki?curid=49185" title="Trofim Lysenko">
Trofim Lysenko

Trofim Denisovich Lysenko (Russian: Трофи́м Дени́сович Лысе́нко, Ukrainian: Трохим Денисович Лисенко; 29 September [O.S. 17 September] 1898 – 20 November 1976) was a Soviet biologist and agronomist of Ukrainian origin. Lysenko rejected Mendelian genetics in favor of the hybridization theories of Russian horticulturist Ivan Vladimirovich Michurin, and adapted them to a pseudoscientific movement termed Lysenkoism.
His experimental research in improved crop yields earned the support of Soviet leader Joseph Stalin, especially following the famine and loss of productivity resulting from forced collectivization in several regions of the Soviet Union in the early 1930s. In 1940, he became director of the Institute of Genetics within the USSR's Academy of Sciences, and Lysenko's anti-Mendelian doctrines were further secured in Soviet science and education by the exercise of political influence and power. Scientific dissent from Lysenko's theories of environmentally acquired inheritance was formally outlawed in 1948.
Though Lysenko remained at his post in the Institute of Genetics until 1965, his influence on Soviet agricultural practice had declined by the 1950s.
Early rise.
Trofim Lysenko, the son of Denis and Oksana Lysenko, was born to a peasant family in Karlivka, Poltava Governorate (in present-day Poltava Oblast, Ukraine) and attended the Kiev Agricultural Institute (now the National University of Life and Environmental Sciences of Ukraine). In 1927, at 29 years of age, working at an agricultural experiment station in Azerbaijan, he embarked on the research that would lead to his 1928 paper on vernalization, which drew wide attention because of its potential practical implications for Soviet agriculture. Severe cold and lack of winter snow had destroyed many early winter-wheat seedlings. By treating wheat seeds with moisture as well as cold, Lysenko induced them to bear a crop when planted in spring. Lysenko coined the term "Jarovization" to describe this chilling process, which he used to make the seeds of winter cereals behave like spring cereals ("Jarovoe"). However, this method was already well-known to farmers since the 1800s, and had recently been discussed in detail by Gustav Gassner as "vernalization" (from the Latin "vernus", of the Spring).
Lysenko's exaggerated claims for massively increased yields were based on plantings over a few hectares, and he further incorrectly claimed that the vernalized transformation could be inherited – i.e., that the offspring of a vernalized plant would themselves go on to flower more quickly, with the vernalization treatment.
At the 6th International Congress of Genetics (1932) in Ithaca, NY, Vavilov, Head of the USSR scientific delegation and acting as ambassador for Soviet science and scientists, said: "The remarkable discovery recently made by T D Lysenko of Odessa opens enormous new possibilities to plant breeders and plant geneticists of mastering individual variation. He found simple physiological methods of shortening the period of growth, of transforming winter varieties into spring ones and late varieties into early ones by inducing processes of fermentation in seeds before sowing them". However, Vavilov, who subsequently became one of Lysenko's strongest critics, had previously emphasized that vernalization was useful only as an experimental technique, and not as a large-scale method of agriculture.
Lysenko was praised in the Soviet newspaper "Pravda" for his claims to have discovered a method to fertilize fields without using fertilizers or minerals, and to have shown that a winter crop of peas could be grown in Azerbaijan, "turning the barren fields of the Transcaucasus green in winter, so that cattle will not perish from poor feeding, and the peasant Turk will live through the winter without trembling for tomorrow."
Lysenko argued that there is not only competition, but also mutual assistance among individuals within a species, and that mutual assistance also exists between different species.
According to Lysenko,
The organism and the conditions required for its life are an inseparable unity. Different living bodies require different environmental conditions for their development. By studying these requirements we come to know the qualitative features of the nature of organisms, the qualitative features of heredity. Heredity is the property of a living body to require definite conditions for its life and development and to respond in a definite way to various conditions.
By the late 1920s, the Soviet political leaders had given their support to Lysenko. This support was a consequence, in part, of policies put in place by the Communist Party to rapidly promote members of the proletariat into leadership positions in agriculture, science and industry. Party officials were looking for promising candidates with backgrounds similar to Lysenko's: born of a peasant family, without formal academic training or affiliations to the academic community.
Lysenko in particular impressed political officials with his success in motivating peasants to return to farming. The Soviet's Collectivist reforms forced the confiscation of agricultural landholdings from peasant farmers and heavily damaged the country's overall food production, and the dispossessed peasant farmers posed new problems for the regime. Many had abandoned the farms altogether; many more waged resistance to collectivization by poor work quality and pilfering. The dislocated and disenchanted peasant farmers were a major political concern to the Soviet leadership. Lysenko emerged during this period by advocating radical but unproven agricultural methods, and also promising that the new methods provided wider opportunities for year-round work in agriculture. Lysenko proved himself very useful to the Soviet leadership by reengaging peasants to return to work, helping to secure from them a personal stake in the overall success of the Soviet revolutionary experiment.
After Stalin.
Following Stalin's death in 1953, Lysenko retained his position, with the support of the new leader Nikita Khrushchev. However, mainstream scientists re-emerged, and found new willingness within Soviet government leadership to tolerate criticism of Lysenko, the first opportunity since the late 1920s. In 1962 three of the most prominent Soviet physicists, Yakov Borisovich Zel'dovich, Vitaly Ginzburg, and Pyotr Kapitsa, presented a case against Lysenko, proclaiming his work as false science. They also denounced Lysenko's application of political power to silence opposition and eliminate his opponents within the scientific community. These denunciations occurred during a period of structural upheaval in Soviet government, during which the major institutions were purged of the strictly ideological and political machinations which had controlled the work of the Soviet Union's scientific community for several decades under Stalin.
In 1964, physicist Andrei Sakharov spoke out against Lysenko in the General Assembly of the Academy of Sciences:
He is responsible for the shameful backwardness of Soviet biology and of genetics in particular, for the dissemination of pseudo-scientific views, for adventurism, for the degradation of learning, and for the defamation, firing, arrest, even death, of many genuine scientists.
The Soviet press was soon filled with anti-Lysenkoite articles and appeals for the restoration of scientific methods to all fields of biology and agricultural science. In 1965 Lysenko was removed from his post as director of the Institute of Genetics at the Academy of Sciences and restricted to an experimental farm in Moscow's Lenin Hills (the Institute itself was soon dissolved). After Khrushchev's dismissal in 1964, the president of the Academy of Sciences declared that Lysenko's immunity to criticism had officially ended. An expert commission was sent to investigate records kept at Lysenko's experimental farm. His secretive methods and ideas were revealed. A few months later, a devastating critique of Lysenko was made public. As a result, Lysenko was immediately disgraced in the Soviet Union.
Lysenko died in Moscow in 1976, and was interred in the Kuntsevo Cemetery.

</doc>
<doc id="49186" url="http://en.wikipedia.org/wiki?curid=49186" title="Pope Anacletus">
Pope Anacletus

Pope Anacletus (died c. 92), also known as Cletus, was the third Bishop of Rome, following Saint Peter and Pope Linus. Anacletus served as pope between c. 79 and his death, c. 92. 
Name and etymology.
The name "Cletus" in Ancient Greek means "one who has been called," and "Anacletus" means "one who has been called back." Also "Anencletus" (Greek: Ανέγκλητος) means "unimpeachable."
The Roman Martyrology mentions the Pope in question only under the name of "Cletus." The "Annuario Pontificio" gives both forms as alternatives. Eusebius, Saint Irenaeus, Saint Augustine and Optatus all suggest that both names refer to the same individual.
Papacy.
St. Cletus/Anacletus was traditionally understood to have been a Roman who served as pope for twelve years. The "Annuario Pontificio" states, "For the first two centuries, the dates of the start and the end of the pontificate are uncertain." It gives the years 80 to 92 as the reign of Pope Cletus/Anacletus. Other sources give the years 77 to 88.
According to tradition, Pope Anacletus divided Rome into twenty-five parishes. One of the few surviving records concerning his papacy mentions him as having ordained an uncertain number of priests.
Burial.
He died and was buried next to his predecessor, Saint Linus, in St. Peter's Basilica, in what is now Vatican City. His name (as Cletus) is included in the Roman Canon of the Mass.
Veneration.
The Tridentine Calendar reserved 26 April as the feast day of Saint Cletus, who the church honoured jointly with Saint Marcellinus, and 13 July for solely Saint Anacletus. In 1960, Pope John XXIII, while keeping the 26 April feast, which mentions the saint under the name given to him in the Canon of the Mass, removed 13 July as a feast day for Saint Anacletus. The 14 February 1961 Instruction of the Congregation for Rites on the application to local calendars of Pope John XXIII's motu proprio "Rubricarum instructum" of 25 July 1960, decreed that "the feast of 'Saint Anacletus,' on whatever ground and in whatever grade it is celebrated, is transferred to 26 April, under its right name, 'Saint Cletus.'" Use of this calendar, which is included in the 1962 edition of the Roman Missal, continues to be authorized under the conditions indicated in the motu proprio "Summorum Pontificum"; but the feast has been removed from the General Roman Calendar since 1969. Although the day of his death is unknown, Saint Cletus continues to be listed in the Roman Martyrology among the saints of 26 April.
External links.
Listen to this article ()
This audio file was created from a revision of the "Pope Anacletus" article dated 1 July 2014, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="49189" url="http://en.wikipedia.org/wiki?curid=49189" title="Frauenburg">
Frauenburg

Frauenburg may refer to the following places:

</doc>
<doc id="49195" url="http://en.wikipedia.org/wiki?curid=49195" title="George Westinghouse">
George Westinghouse

George Westinghouse, Jr. (October 6, 1846 – March 12, 1914) was an American entrepreneur and engineer who invented the railway air brake and was a pioneer of the electrical industry, gaining his first patent at the age of 22. Based in Pittsburgh, Pennsylvania for much of his career, Westinghouse was one of Thomas Edison's main rivals in the early implementation of the American electricity system. Westinghouse's electricity distribution system, based on alternating current, ultimately prevailed over Edison's insistence on direct current. In 1911 Westinghouse received the AIEE's Edison Medal "For meritorious achievement in connection with the development of the alternating current system."
Early years.
George Westinghouse was born in Central Bridge, New York, in 1846, the son of a machine shop owner and his wife. From his youth, he was talented at machinery and business. At the age of fifteen, as the Civil War broke out, Westinghouse enlisted in the New York National Guard and served until his parents urged him to return home. In April 1863 he persuaded his parents to allow him to re-enlist, whereupon he joined Company M of the 16th New York Cavalry and earned promotion to the rank of corporal. In December 1864 he resigned from the Army to join the Navy, serving as Acting Third Assistant Engineer on the "USS Muscoota" through the end of the war. After his military discharge in August 1865, he returned to his family in Schenectady and enrolled at Union College. However, he lost interest in the curriculum and dropped out in his first term there.
Westinghouse was 19 years old when he created his first invention, the rotary steam engine. He also devised the Westinghouse Farm Engine. At age 21 he invented a "car replacer", a device to guide derailed railroad cars back onto the tracks, and a reversible frog, a device used with a railroad switch to guide trains onto one of two tracks.
In 1867, Westinghouse met and soon married Marguerite Erskine Walker. They had one son, George Westinghouse 3rd and were married for 47 years. The couple made their first home in Pittsburgh, Pennsylvania. They later acquired houses in Lenox, Massachusetts, where they summered, and in Washington, District of Columbia.
Air brakes.
At about this time, he witnessed a train wreck where two engineers saw one another, but were unable to stop their trains in time using the existing brakes. Brakemen had to run from car to car, on catwalks atop the cars, applying the brakes manually on each car.
In 1869, at age 22, Westinghouse invented a railroad braking system using compressed air. The Westinghouse system used a compressor on the locomotive, a reservoir and a special valve on each car, and a single pipe running the length of the train (with flexible connections) which both refilled the reservoirs and controlled the brakes, allowing the engineer to apply and release the brakes simultaneously on all cars. It is a failsafe system, in that any rupture or disconnection in the train pipe will apply the brakes throughout the train. It was patented by Westinghouse on October 28, 1873. The Westinghouse Air Brake Company (WABCO) was subsequently organized to manufacture and sell Westinghouse's invention. It was in time nearly universally adopted by railways. Modern trains use brakes in various forms based on this design. The same conceptual design of fail-safe air brake is also found on heavy trucks.
Westinghouse pursued many improvements in railway signals (which then used oil lamps). In 1881 he founded the Union Switch and Signal Company to manufacture his signaling and switching inventions.
Electric power distribution.
In 1879 Thomas Edison invented an improved incandescent light bulb, and realized the need for an electrical distribution system to provide power for lighting. On September 4, 1882, Edison switched on the world's first electric power distribution system, providing 110 volts direct current (DC) to 59 customers in lower Manhattan, around his Pearl Street Station. The main drawback with Edison's low-voltage DC power network was its short transmission range, with centralized plants only able to supply customers within a mile of each plant.
Westinghouse's interests in gas distribution and telephone switching led him to become interested in electrical power distribution. In 1884 he started developing his own DC lighting system and hired by physicist William Stanley to work on it. Westinghouse became aware of the new European AC systems in 1885 when he read about them in the UK technical journal "Engineering". An AC power system allowed voltages to be "stepped up" by a transformer for distribution long distances without the severe power losses suffered by DC systems, and then "stepped down" by a transformer for consumer use. With AC's potential to achieve greater economies of scale with large centralized power plants, and it's ability to supply electricity long distance in cities with more disperse populations, Westinghouse saw a way to build a truly competitive system instead of simply building another barely competitive DC lighting system using patents just different enough to get around the Edison patents. The Edison DC system of centralized DC plants with short transmission range also meant there there was a patchwork of un-supplied customers between Edison's plants that Westinghouse could easily supply with AC power. 
In 1885 Westinghouse imported a number of Gaulard-Gibbs transformers (developed by Lucien Gaulard of France and John D. Gibbs of England was demonstrated in London in 1881) and a Siemens AC generator to begin experimenting with AC networks in Pittsburgh. AC transformers were not new, but the Gaulard-Gibbs design was one of the first that could handle high power and be readily manufactured.
Stanley, assisted by engineers Albert Schmid and Oliver B. Shallenberger developed the Gaulard-Gibbs transformer design into the first practical transformer used in an AC system. In 1886, Westinghouse and Stanley installed the first multiple-voltage AC power system in Great Barrington, Massachusetts. The network was driven by a hydroelectric generator that produced 500 volts AC. The voltage was stepped up to 3,000 volts for transmission, and then stepped back down to 100 volts to drive electric lights. That same year, Westinghouse formed the "Westinghouse Electric & Manufacturing Company"; in 1889 he renamed it as "Westinghouse Electric Corporation".
The Westinghouse company installed 30 more AC-lighting systems within a year and by the end of 1887 it had 68 alternating current power stations to Edison's 121 DC based stations. The expansion of Westinghouse's of AC power distribution system led him into a bitter confrontation with Edison and his DC power system in a feud that became known as the "War of Currents". Edison mentioned to colleagues at the end of 1886 that Westinghouse would "kill a customer within six months" with his new AC system and first spoke out (privately) in a late 1887 letter to a New York State committee trying to determine a new, more humane system of execution to replace hanging, say the best method would be to wire the prisoner to a Westinghouse AC generator. In February 1888 Edison began a public media campaign claiming that high voltage AC systems were inherently dangerous. Westinghouse responded that the risks could be managed and were outweighed by the benefits. Edison tried to have legislation enacted in several states to limit power transmission voltages to 800 volts, but failed.
Westinghouse also had to deal with an AC rival, the Thomson-Houston Electric Company. They had built 22 power stations by the end of 1887 and by 1889 had bought out a third AC competitor, the Brush Electric Company. Thomson-Houston was expanding their business while trying to avoid patent conflicts with Westinghouse, arranging deals such as coming to agreements over lighting company territory, paying a royalty to use the Stanley transformer patent, and allowing Westinghouse to use their Sawyer-Man incandescent bulb patent.
After a young New York City boy was killed from accidentally touching a broken telegraph that had been energized with alternating current an electrical salesman named Harold P. Brown sent a June 5, 1888 letter to the editor of the New York Post claiming alternating current was inherently dangerous and asking why the "public must submit to constant danger from sudden death" just so utilities could use a cheaper AC system. Within a few days Brown was lobbying in the newspapers for server regulations on AC power and within two months he would publicly electrocute a dog with AC at Columbia College to prove how dangerous it was. Westinghouse would point out in letters various newspapers the number of fires caused by DC equipment and that Brown was obviously in the employ of Edison, something Brown denied.
Brown, after becoming a consultant to the New York board in charge of finding a new method of executing condemned prisoners, went on to be instrumental in making sure that AC would be used in the newly developed electric chair. In August 1889 the "New Your Sun" published letters stolen from Brown's office that seemed to show Brown was receiving directions from, and being paid by, the Edison company and Thomson-Houston company. Thomson-Houston also worked with Brown to secretly acquire 3 secondhand Westinghouse AC generators for prisons to use in their executions. In August 1890, a convict named William Kemmler became the first man to be executed by electrocution. Westinghouse hired the best lawyer of the day to defend Kemmler, and the lawyer condemned electrocution as a form of "cruel and unusual punishment", not allowable under the US Constitution.
The War of Currents would end with financiers such as J. P. Morgan pushing Edison Electric towards AC and pushing out Thomas Edison. The Edison Machine Works started pursuing AC development in 1890 and by 1892 Thomas Edison was no longer in control of his own company, which was merged with the Thomson-Houston Electric Company into General Electric, a conglomerate now armed with all of Thomson-Houston's AC patents.
During this period Westinghouse continued to pour money and engineering resources into the goal of building a completely integrated AC system. To gain control of the Sawyer-Man lamp patents he bought Consolidated Electric Light in 1888. In April 1888 Westinghouse engineer Oliver B. Shallenberger developed an induction meter that used a rotating magnetic field for measuring alternating current. The same basic meter technology remains in use today in the early 21st century. That same year, inventor Nikola Tesla demonstrated a polyphase brushless AC induction motor, also based on a rotating magnetic field. An AC electric motor was one of the final elements an AC system needed to compete with DC systems. In July 1888, George Westinghouse licensed Nikola Tesla's American patents for the induction motor and transformer designs. Westinghouse also purchased an American patent option on a possible prior design for an induction motor from the Italian physicist and electrical engineer Galileo Ferraris. He wanted to avoid the rather substantial amount of money being asked to secure the Tesla license, but Westinghouse concluded that it was too risky not to obtain what may have been an earlier patent by Tesla. The acquisition of a feasible AC motor gave Westinghouse a key patent in building a completely integrated AC system, but the financial strain of buying up patents and hiring the engineers needed to build it meant development of Tesla's motor had to be put on hold for a while.
In 1891 Westinghouse built a hydroelectric AC power plant, the Ames Hydroelectric Generating Plant. The plant supplied power to the Gold King Mine 3.5 miles away. This was the first successful demonstration of long-distance transmission of industrial-grade alternating current power and used two 100-hp Westinghouse alternators, one working as a generator producing 3000 volt, 133 Hertz, single-phase AC, and the other used as an AC motor. At the beginning of 1893 Westinghouse engineer Benjamin Lamme had made great progress developing an efficient version of Tesla's induction motor and Westinghouse Electric started branding their complete polyphase phase AC system as the "Tesla Polyphase System", announcing Tesla's patents gave them patent priority over other AC systems and their intentions to sue patent infringes.
In 1893, George Westinghouse won the bid to light the 1893 World's Columbian Exposition in Chicago with alternating current, beating out a General Electric bid by one million dollars. This World's Fair devoted a building to electrical exhibits. It was a key event in the history of AC power, as Westinghouse demonstrated the safety, reliability, and efficiency of a fully integrated alternating current system to the American public.
Westinghouses demonstration that they could build a complete AC system at the Colombian Exposition was instrumental in them getting the contract for building a two phase AC generating system, the Adams Power Plant, at Niagara Falls in 1895. In order to keep the other big electric company in play for future projects, the contract to build the three-phase AC distribution system was awarded to General Electric. The early to mid 1890s saw General Electric, backed by financier J. P. Morgan, involved in costly take over attempts and patent battles with Westinghouse Electric. A patent sharing agreement was finally signed between the two companies in 1896.
Other projects.
In 1889, Westinghouse purchased several mining claims in the Patagonia Mountains of southeastern Arizona and formed the Duquesne Mining & Reduction Company. A year later he founded what is now the ghost town of Duquesne to use as his company headquarters. He lived in a large Victorian frame house, which still stands, but in disrepair. Duquesne grew to over a 1,000 residents and the mine reached its peak production in the mid-1910s.
With AC networks expanding, Westinghouse turned his attention to electrical power production. At the outset, the available generating sources were hydroturbines where falling water was available, and reciprocating steam engines where it was not. Westinghouse felt that reciprocating steam engines were clumsy and inefficient, and wanted to develop some class of "rotating" engine that would be more elegant and efficient.
One of his first inventions had been a rotary steam engine, but it had proven impractical. The British engineer Charles Algernon Parsons began experimenting with steam turbines in 1884, beginning with a 10 horsepower (7.5 kW) . Westinghouse bought rights to the Parsons turbine in 1885, and improved the Parsons technology and increased its scale.
In 1898 Westinghouse demonstrated a 300 kilowatt unit, replacing reciprocating engines in his air-brake factory. The next year he installed a 1.5 megawatt, 1,200 rpm unit for the Hartford Electric Light Company.
Westinghouse then developed steam turbines for maritime propulsion. Large turbines were most efficient at about 3,000 rpm, while an efficient propeller operated at about 100 rpm. That required reduction gearing, but building reduction gearing that could operate at high rpm and at high power was difficult, since a slight misalignment would shake the power train to pieces. Westinghouse and his engineers devised an automatic alignment system that made turbine power practical for large vessels.
Westinghouse remained productive and inventive almost all his life. Like Edison, he had a practical and experimental streak. At one time, Westinghouse began to work on heat pumps that could provide heating and cooling, and believed that he might be able to extract enough power in the process for the system to run itself.
Any modern engineer would clearly see that Westinghouse was after a perpetual motion machine, and the British physicist Lord Kelvin, one of Westinghouse's correspondents, told him that he would be violating the laws of thermodynamics. Westinghouse replied that might be the case, but it made no difference. If he couldn't build a perpetual-motion machine, he would still have a heat pump system that he could patent and sell.
With the introduction of the automobile after the turn of the century, Westinghouse went back to earlier inventions and devised a compressed air shock absorber for automobile suspensions.
Westinghouse remained a captain of American industry until 1907, when a financial panic led to his resignation from control of the Westinghouse company. By 1911, he was no longer active in business, and his health was in decline.
George Westinghouse died on March 12, 1914, in New York City, at age 67. As a Civil War veteran, he was buried in Arlington National Cemetery, along with his wife Marguerite, who survived him by three months. Although a shrewd and determined businessman, Westinghouse was a conscientious employer and wanted to make fair deals with his business associates.
Labor relations.
A six-day workweek was the rule when George Westinghouse inaugurated the first Saturday half holiday in his Pittsburgh factory in 1881.
Honors and awards.
In 1918 his former home was razed and the land given to the City of Pittsburgh to establish Westinghouse Park. In 1930, the Westinghouse Memorial, funded by his employees, was placed in Schenley Park in Pittsburgh. Also named in his honor, George Westinghouse Bridge is near the site of his Turtle Creek plant. Its plaque reads:
The George Westinghouse, Jr., Birthplace and Boyhood Home in Central Bridge, New York was listed on the National Register of Historic Places in 1986.
References.
Patents.
</dl>
Bibliography.
</dl>

</doc>
<doc id="49197" url="http://en.wikipedia.org/wiki?curid=49197" title="Antiviral drug">
Antiviral drug

Antiviral drugs are a class of medication used specifically for treating viral infections. Like antibiotics for bacteria, specific antivirals are used for specific viruses. Unlike most antibiotics, antiviral drugs do not destroy their target pathogen; instead they inhibit their development.
Antiviral drugs are one class of antimicrobials, a larger group which also includes antibiotic (also termed antibacterial), antifungal and antiparasitic drugs, or antiviral drugs based on monoclonal antibodies. Most antivirals are considered relatively harmless to the host, and therefore can be used to treat infections. They should be distinguished from viricides, which are not medication but deactivate or destroy virus particles, either inside or outside the body. Antivirals also can be found in essential oils of some herbs, such as eucalyptus oil and its constituents.
Medical uses.
Most of the antiviral drugs now available are designed to help deal with HIV, herpes viruses, the hepatitis B and C viruses, and influenza A and B viruses. Researchers are working to extend the range of antivirals to other families of pathogens.
Designing safe and effective antiviral drugs is difficult, because viruses use the host's cells to replicate. This makes it difficult to find targets for the drug that would interfere with the virus without also harming the host organism's cells. Moreover, the major difficulty in developing vaccines and anti-viral drugs is due to viral variation.
The emergence of antivirals is the product of a greatly expanded knowledge of the genetic and molecular function of organisms, allowing biomedical researchers to understand the structure and function of viruses, major advances in the techniques for finding new drugs, and the intense pressure placed on the medical profession to deal with the human immunodeficiency virus (HIV), the cause of the deadly acquired immunodeficiency syndrome (AIDS) pandemic.
The first experimental antivirals were developed in the 1960s, mostly to deal with herpes viruses, and were found using traditional trial-and-error drug discovery methods. Researchers grew cultures of cells and infected them with the target virus. They then introduced into the cultures chemicals which they thought might inhibit viral activity, and observed whether the level of virus in the cultures rose or fell. Chemicals that seemed to have an effect were selected for closer study.
This was a very time-consuming, hit-or-miss procedure, and in the absence of a good knowledge of how the target virus worked, it was not efficient in discovering effective antivirals which had few side effects. Only in the 1980s, when the full genetic sequences of viruses began to be unraveled, did researchers begin to learn how viruses worked in detail, and exactly what chemicals were needed to thwart their reproductive cycle.
Research.
On 10 August 2011 researchers at MIT announced the publication of a new method of inhibiting NA, the process selectively affected infected cells. The team named the process "Double-stranded RNA Activated Caspase Oligomerizer" (DRACO). According to the lead researcher "In theory, [DRACO] should work against all viruses."
Virus life cycle.
Viruses consist of a genome and sometimes a few enzymes stored in a capsule made of protein (called a capsid), and sometimes covered with a lipid layer (sometimes called an 'envelope'). Viruses cannot reproduce on their own, and instead propagate by subjugating a host cell to produce copies of themselves, thus producing the next generation.
Researchers working on such "rational drug design" strategies for developing antivirals have tried to attack viruses at every stage of their life cycles. Some species of mushrooms have been found to contain multiple antiviral chemicals with similar synergistic effects.
Viral life cycles vary in their precise details depending on the species of virus, but they all share a general pattern:
Limitations of vaccines.
Vaccines bolster the body's immune system to better attack viruses in the "complete particle" stage, outside of the organism's cells. They traditionally consist of an attenuated (a live weakened) or inactivated (killed) version of the virus. These vaccines can, in very rare cases, harm the host by inadvertently infecting the host with a full-blown viral occupancy. Recently "subunit" vaccines have been devised that consist strictly of protein targets from the pathogen. They stimulate the immune system without doing serious harm to the host. In either case, when the real pathogen attacks the subject, the immune system responds to it quickly and blocks it.
Vaccines are very effective on stable viruses, but are of limited use in treating a patient who has already been infected. They are also difficult to successfully deploy against rapidly mutating viruses, such as influenza (the vaccine for which is updated every year) and HIV. Antiviral drugs are particularly useful in these cases.
Anti-viral targeting.
The general idea behind modern antiviral drug design is to identify viral proteins, or parts of proteins, that can be disabled. These "targets" should generally be as unlike any proteins or parts of proteins in humans as possible, to reduce the likelihood of side effects. The targets should also be common across many strains of a virus, or even among different species of virus in the same family, so a single drug will have broad effectiveness. For example, a researcher might target a critical enzyme synthesized by the virus, but not the patient, that is common across strains, and see what can be done to interfere with its operation.
Once targets are identified, candidate drugs can be selected, either from drugs already known to have appropriate effects, or by actually designing the candidate at the molecular level with a computer-aided design program.
The target proteins can be manufactured in the lab for testing with candidate treatments by inserting the gene that synthesizes the target protein into bacteria or other kinds of cells. The cells are then cultured for mass production of the protein, which can then be exposed to various treatment candidates and evaluated with "rapid screening" technologies.
Approaches by life cycle stage.
Before cell entry.
One anti-viral strategy is to interfere with the ability of a virus to infiltrate a target cell. The virus must go through a sequence of steps to do this, beginning with binding to a specific "receptor" molecule on the surface of the host cell and ending with the virus "uncoating" inside the cell and releasing its contents. Viruses that have a lipid envelope must also fuse their envelope with the target cell, or with a vesicle that transports them into the cell, before they can uncoat.
This stage of viral replication can be inhibited in two ways:
This strategy of designing drugs can be very expensive, and since the process of generating anti-idiotypic antibodies is partly trial and error, it can be a relatively slow process until an adequate molecule is produced.
Entry inhibitor.
A very early stage of viral infection is viral entry, when the virus attaches to and enters the host cell. A number of "entry-inhibiting" or "entry-blocking" drugs are being developed to fight HIV. HIV most heavily targets the immune system's white blood cells known as "helper T cells", and identifies these target cells through T-cell surface receptors designated "CD4" and "CCR5". Attempts to interfere with the binding of HIV with the CD4 receptor have failed to stop HIV from infecting helper T cells, but research continues on trying to interfere with the binding of HIV to the CCR5 receptor in hopes that it will be more effective.
HIV infects a cell through fusion with the cell membrane, which requires two different cellular molecular participants, CD4 and a chemokine receptor (differing depending on the cell type). Approaches to blocking this virus/cell fusion have shown some promise in preventing entry of the virus into a cell. At least one of theses entry inhibitors—a biomimetic peptide marketed under the brand name Fuzeon—has received FDA approval and has been in use for some time. Potentially, one of the benefits from the use of an effective entry-blocking or entry-inhibiting agent is that it potentially may not only prevent the spread of the virus within an infected individual but also the spread from an infected to an uninfected individual.
While the peptide mentioned has been in clinical use for some time, other approaches to entry inhibition may hold as great or greater promise. For example, ajoene, a chemical produced in the decay of a biproduct of the crushing of garlic, has been shown in vitro to prevent the entry of HIV into cells at high picomolar concentrations.
One possible advantage of the therapeutic approach of blocking viral entry (as opposed to the currently dominant approach of viral enzyme inhibition) is that it may prove more difficult for the virus to develop resistance to this therapy than for the virus to mutate or evolve its enzymatic protocols.
Uncoating inhibitor.
Inhibitors of uncoating have also been investigated.
Amantadine and rimantadine have been introduced to combat influenza. These agents act on penetration and uncoating.
Pleconaril works against rhinoviruses, which cause the common cold, by blocking a pocket on the surface of the virus that controls the uncoating process. This pocket is similar in most strains of rhinoviruses and enteroviruses, which can cause diarrhea, meningitis, conjunctivitis, and encephalitis.
During viral synthesis.
A second approach is to target the processes that synthesize virus components after a virus invades a cell.
Reverse transcription.
One way of doing this is to develop nucleotide or nucleoside analogues that look like the building blocks of RNA or DNA, but deactivate the enzymes that synthesize the RNA or DNA once the analogue is incorporated. This approach is more commonly associated with the inhibition of reverse transcriptase (RNA to DNA) than with "normal" transcriptase (DNA to RNA).
The first successful antiviral, acyclovir, is a nucleoside analogue, and is effective against herpesvirus infections. The first antiviral drug to be approved for treating HIV, zidovudine (AZT), is also a nucleoside analogue.
An improved knowledge of the action of reverse transcriptase has led to better nucleoside analogues to treat HIV infections. One of these drugs, lamivudine, has been approved to treat hepatitis B, which uses reverse transcriptase as part of its replication process. Researchers have gone further and developed inhibitors that do not look like nucleosides, but can still block reverse transcriptase.
Another target being considered for HIV antivirals include RNase H – which is a component of reverse transcriptase that splits the synthesized DNA from the original viral RNA.
Integrase.
Another target is integrase, which splices the synthesized DNA into the host cell genome.
Transcription.
Once a virus genome becomes operational in a host cell, it then generates messenger RNA (mRNA) molecules that direct the synthesis of viral proteins. Production of mRNA is initiated by proteins known as transcription factors. Several antivirals are now being designed to block attachment of transcription factors to viral DNA.
Translation/antisense.
Genomics has not only helped find targets for many antivirals, it has provided the basis for an entirely new type of drug, based on "antisense" molecules. These are segments of DNA or RNA that are designed as complementary molecule to critical sections of viral genomes, and the binding of these antisense segments to these target sections blocks the operation of those genomes. A phosphorothioate antisense drug named fomivirsen has been introduced, used to treat opportunistic eye infections in AIDS patients caused by cytomegalovirus, and other antisense antivirals are in development. An antisense structural type that has proven especially valuable in research is morpholino antisense.
Morpholino oligos have been used to experimentally suppress many viral types:
Translation/ribozymes.
Yet another antiviral technique inspired by genomics is a set of drugs based on ribozymes, which are enzymes that will cut apart viral RNA or DNA at selected sites. In their natural course, ribozymes are used as part of the viral manufacturing sequence, but these synthetic ribozymes are designed to cut RNA and DNA at sites that will disable them.
A ribozyme antiviral to deal with hepatitis C has been suggested, and ribozyme antivirals are being developed to deal with HIV. An interesting variation of this idea is the use of genetically modified cells that can produce custom-tailored ribozymes. This is part of a broader effort to create genetically modified cells that can be injected into a host to attack pathogens by generating specialized proteins that block viral replication at various phases of the viral life cycle.
Protein processing and targeting.
Interference with post translational modifications or with targeting of viral proteins in the cell is also possible.
Protease inhibitors.
Some viruses include an enzyme known as a protease that cuts viral protein chains apart so they can be assembled into their final configuration. HIV includes a protease, and so considerable research has been performed to find "protease inhibitors" to attack HIV at that phase of its life cycle. Protease inhibitors became available in the 1990s and have proven effective, though they can have unusual side effects, for example causing fat to build up in unusual places. Improved protease inhibitors are now in development.
Protease inhibitors have also been seen in nature. A protease inhibitor was isolated from the Shiitake mushroom ("Lentinus edodes"). The presence of this may explain the Shiitake mushrooms noted antiviral activity "in vitro".
Assembly.
Rifampicin acts at the assembly phase.
Release phase.
The final stage in the life cycle of a virus is the release of completed viruses from the host cell, and this step has also been targeted by antiviral drug developers. Two drugs named zanamivir (Relenza) and oseltamivir (Tamiflu) that have been recently introduced to treat influenza prevent the release of viral particles by blocking a molecule named neuraminidase that is found on the surface of flu viruses, and also seems to be constant across a wide range of flu strains.
Immune system stimulation.
A second category of tactics for fighting viruses involves encouraging the body's immune system to attack them, rather than attacking them directly. Some antivirals of this sort do not focus on a specific pathogen, instead stimulating the immune system to attack a range of pathogens.
One of the best-known of this class of drugs are interferons, which inhibit viral synthesis in infected cells. One form of human interferon named "interferon alpha" is well-established as part of the standard treatment for hepatitis B and C, and other interferons are also being investigated as treatments for various diseases.
A more specific approach is to synthesize antibodies, protein molecules that can bind to a pathogen and mark it for attack by other elements of the immune system. Once researchers identify a particular target on the pathogen, they can synthesize quantities of identical "monoclonal" antibodies to link up that target. A monoclonal drug is now being sold to help fight respiratory syncytial virus in babies, and antibodies purified from infected individuals are also used as a treatment for hepatitis B.
Acquired resistance.
Almost all anti-microbials, including anti-virals, are subject to drug resistance as the pathogens mutate over time, becoming less susceptible to the treatment. For instance, a recent study published in Nature Biotechnology emphasized the urgent need for augmentation of oseltamivir (Tamiflu) stockpiles with additional antiviral drugs including zanamivir (Relenza) based on an evaluation of the performance of these drugs in the scenario that the 2009 H1N1 'Swine Flu' neuraminidase (NA) were to acquire the tamiflu-resistance (His274Tyr) mutation which is currently widespread in seasonal H1N1 strains.

</doc>
<doc id="49198" url="http://en.wikipedia.org/wiki?curid=49198" title="Reductionism">
Reductionism

Reductionism refers to several related but different philosophical positions regarding the connections between phenomena, or theories, 'reducing' one to another, usually considered 'simpler' or more 'basic'. "The Oxford Companion to Philosophy" suggests that it is ‘one of the most used and abused terms in the philosophical lexicon’ and suggests a three part division:
Reductionism can be applied to objects, phenomena, explanations, theories, and meanings.
In the sciences, application of methodological reductionism attempts explanation of entire systems in terms of their individual, constituent parts and their interactions. Thomas Nagel speaks of "psychophysical reductionism" (the attempted reduction of psychological phenomena to physics and chemistry), as do and "physico-chemical reductionism" (the attempted reduction of biology to physics and chemistry), again . In a very simplified and sometimes contested form, such reductionism is said to imply that a system is "nothing but" the sum of its parts. However, a more nuanced view is that a system is composed entirely of its parts, but the system will have features that none of the parts have. "The point of mechanistic explanations is usually showing "how" the higher level features arise from the parts."
Other definitions are used by other authors. For example, what Polkinghorne calls "conceptual" or "epistemological" reductionism is the definition provided by Blackburn and by Kim: that form of reductionism concerning a program of replacing the facts or entities entering statements claimed to be true in one area of discourse with other facts or entities from another area, thereby providing a relationship between them. Such a connection is provided where the same idea can be expressed by "levels" of explanation, with higher levels reducible if need be to lower levels. This use of levels of understanding in part expresses our human limitations in grasping a lot of detail. However, "most philosophers would insist that our role in conceptualizing reality [our need for an hierarchy of "levels" of understanding] does not change the fact that different levels of organization in reality do have different "properties"."
As this introduction suggests, there are a variety of forms of reductionism, discussed in more detail in subsections below.
Reductionism strongly reflects a certain perspective on causality. In a reductionist framework, the phenomena that can be explained completely in terms of relations between other more fundamental phenomena, are called epiphenomena. Often there is an implication that the epiphenomenon exerts no causal agency on the fundamental phenomena that explain it. The epiphenomena are sometimes said to be "nothing but" the outcome of the workings of the fundamental phenomena, although the epiphenomena might be more clearly and efficiently described in very different terms. There is a tendency to avoid taking an epiphenomenon as being important in its own right. This attitude may extend to cases where the fundamentals are not clearly able to explain the epiphenomena, but are expected to by the speaker. In this way, for example, morality can be deemed to be "nothing but" evolutionary adaptation, and consciousness can be considered "nothing but" the outcome of neurobiological processes.
Reductionism does not preclude the existence of what might be called emergent phenomena, but it does imply the ability to understand those phenomena completely in terms of the processes from which they are composed. This reductionist understanding is very different from emergentism, which intends that what emerges in 'emergence' is more than the sum of the processes from which it emerges.
Types.
Most philosophers delineate three types of reductionism and antireductionism.
Ontological reductionism.
Ontological reductionism is the belief that reality is composed of a minimum number of kinds of entities or substances. This claim is usually metaphysical, and is most commonly a form of monism, in effect claiming that all objects, properties and events are reducible to a single substance. (A dualist who is an ontological reductionist would believe that everything is reducible to two substances — as one possible example, a dualist might claim that reality is composed of "matter" and "spirit".)
Nancey Murphy has claimed that there are two species of ontological reductionism: one that denies that wholes are anything more than their parts; and the stronger thesis of atomist reductionism that wholes are not "really real". She admits that the phrase "really real" is apparently senseless but nonetheless has tried to explicate the supposed difference between the two.
Ontological reductionism is the claim that everything that exists is made from a small number of basic substances that behave in regular ways ("compare to monism"). Ontological reductionism denies the idea of ontological emergence, and claims that emergence is an epistemological phenomenon that only exists through analysis or description of a system, and does not exist on a fundamental level.
Ontological reductionism takes two different forms: "token ontological reductionism" and "type ontological reductionism".
Token ontological reductionism is the idea that every item that exists is a sum item. For perceivable items, it says that every perceivable item is a sum of items at a smaller level of complexity. Token ontological reduction of biological things to chemical things is generally accepted.
Type ontological reductionism is the idea that every type of item is a sum type of item, and that every perceivable type of item is a sum of types of items at a lower level of complexity. Type ontological reduction of biological things to chemical things is often rejected.
Michael Ruse has criticized ontological reductionism as an improper argument against vitalism.
Methodological reductionism.
Methodological reductionism is the position that the best scientific strategy is to attempt to reduce explanations to the smallest possible entities. Methodological reductionism would thus hold that the atomic explanation of a substance's boiling point is preferable to the chemical explanation, and that an explanation based on even smaller particles (quarks and leptons, perhaps) would be even better.
Methodological reductionism, therefore, is the position that all scientific theories either can or should be reduced to a single super~theory through the process of theoretical reduction.
Theory reductionism.
Theory reduction is the process by which one theory absorbs another. For example, both Kepler's laws of the motion of the planets and Galileo's theories of motion worked out for terrestrial objects are reducible to Newtonian theories of mechanics, because all the explanatory power of the former are contained within the latter. Furthermore, the reduction is considered to be beneficial because Newtonian mechanics is a more general theory—that is, it explains more events than Galileo's or Kepler's. Theoretical reduction, therefore, is the reduction of one explanation or theory to another—that is, it is the absorption of one of our ideas about a particular thing into another idea.
In science.
Reductionist thinking and methods form the basis for many of the well-developed areas of modern science, including much of physics, chemistry and cell biology. Classical mechanics in particular is seen as a reductionist framework, and statistical mechanics can be viewed as a reconciliation of macroscopic thermodynamic laws with the reductionist approach of explaining macroscopic properties in terms of microscopic components.
In science, reductionism implies that certain fields of study are based on areas that study smaller spatial scales or organizational units. While it is commonly accepted that the foundations of chemistry are based in physics, and molecular biology is rooted in chemistry, similar statements become controversial when one considers less rigorously defined intellectual pursuits. For example, claims that sociology is based on psychology, or that economics is based on sociology and psychology would be met with reservations. These claims are difficult to substantiate even though there are clear connections between these fields (for instance, most would agree that psychology can affect and inform economics). The limit of reductionism's usefulness stems from emergent properties of complex systems, which are more common at certain levels of organization. For example, certain aspects of evolutionary psychology and sociobiology are rejected by some who claim that complex systems are inherently irreducible and that a holistic approach is needed to understand them.
Some strong reductionists believe that the behavioral sciences should become "genuine" scientific disciplines based on genetic biology, and on the systematic study of culture (see Richard Dawkins's concept of memes). In his book "The Blind Watchmaker", Dawkins introduced the term "hierarchical reductionism" to describe the view that complex systems can be described with a hierarchy of organizations, each of which is only described in terms of objects one level down in the hierarchy. He provides the example of a computer, which under hierarchical reductionism is explained in terms of the operation of hard drives, processors, and memory, but not on the level of AND OR gates, or on the even lower level of electrons in a semiconductor medium.
Others argue that inappropriate use of reductionism limits our understanding of complex systems. In particular, ecologist Robert Ulanowicz says that science must develop techniques to study ways in which larger scales of organization influence smaller ones, and also ways in which feedback loops create structure at a given level, independently of details at a lower level of organization. He advocates (and uses) information theory as a framework to study propensities in natural systems. Ulanowicz attributes these criticisms of reductionism to the philosopher Karl Popper and biologist Robert Rosen.
The idea that phenomena such as emergence and work within the field of complex systems theory pose limits to reductionism has been advocated by Stuart Kauffman. Emergence is strongly related to nonlinearity. The limits of the application of reductionism are claimed to be especially evident at levels of organization with higher amounts of complexity, including living cells, neural networks, ecosystems, society, and other systems formed from assemblies of large numbers of diverse components linked by multiple feedback loops.
Nobel laureate P.W.Anderson used the idea that symmetry breaking is an example of an emergent phenomenon in his 1972 "Science" paper 'More is different' to make an argument about the limitations of reductionism. One observation he made was that the sciences can be arranged roughly in a linear hierarchy — particle physics, many body physics, chemistry, molecular biology, cellular biology, physiology, psychology, social sciences — in that the elementary entities of one science obeys the laws of the science that precedes it in the hierarchy; yet this does not imply that one science is just an applied version of the science that precedes it. He writes that "At each stage, entirely new laws, concepts and generalizations are necessary, requiring inspiration and creativity to just as great a degree as in the previous one. Psychology is not applied biology nor is biology applied chemistry."
Disciplines such as cybernetics and systems theory embrace a non-reductionist view of science, sometimes going as far as explaining phenomena at a given level of hierarchy in terms of phenomena at a higher level, in a sense, the opposite of a reductionist approach.
In mathematics.
In mathematics, reductionism can be interpreted as the philosophy that all mathematics can (or ought to) be built on a common foundation, which is usually axiomatic set theory. Ernst Zermelo was one of the major advocates of such a view; he also developed much of axiomatic set theory. It has been argued that the generally accepted method of justifying mathematical axioms by their usefulness in common practice can potentially undermine Zermelo's reductionist program.
As an alternative to set theory, Jouko Väänänen has argued for second-order logic as a foundation for mathematics instead of set theory, whereas others have argued for category theory as a foundation for certain aspects of mathematics.
The incompleteness theorems of Kurt Gödel, published in 1931, raised doubts about the attainability of an axiomatic foundation for all of mathematics. Any such foundation would have to include axioms powerful enough to describe the arithmetic of the natural numbers (a subset of all mathematics). Yet Gödel proved that for any self-consistent recursive axiomatic system powerful enough to describe the arithmetic of the natural numbers, there are true propositions about the natural numbers that cannot be proved from the axioms. (Such propositions are known as formally undecidable propositions.)
In religion.
Religious reductionism generally attempts to explain religion by boiling it down to certain nonreligious causes. A few examples of reductionistic explanations for the presence of religion are: that religion can be reduced to humanity's conceptions of right and wrong, that religion is fundamentally a primitive attempt at controlling our environments, that religion is a way to explain the existence of a physical world, and that religion confers an enhanced survivability for members of a group and so is reinforced by natural selection. Anthropologists Edward Burnett Tylor and James George Frazer employed some religious reductionist arguments. Sigmund Freud held that religion is nothing more than an illusion, or even a mental illness, and Marx claimed that religion is "the sigh of the oppressed," and the opium of the people providing only "the illusory happiness of the people," thus providing two influential examples of reductionistic views against the idea of religion.
In linguistics.
Linguistic reductionism is the idea that everything can be described or explained in a language with a limited number of core concepts, and combinations of those concepts.
In philosophy.
The concept of downward causation poses an alternative to reductionism within philosophy. This view is developed and explored by Peter Bøgh Andersen, Claus Emmeche, Niels Ole Finnemann, and Peder Voetmann Christiansen, among others. These philosophers explore ways in which one can talk about phenomena at a larger-scale level of organization exerting causal influence on a smaller-scale level, and find that some, but not all proposed types of downward causation are compatible with science. In particular, they find that constraint is one way in which downward causation can operate. The notion of causality as constraint has also been explored as a way to shed light on scientific concepts such as self-organization, natural selection, adaptation, and control.
Free will.
Philosophers of the Enlightenment worked to insulate human free will from reductionism. Descartes separated the material world of mechanical necessity from the world of mental free will. German philosophers introduced the concept of the "noumenal" realm that is not governed by the deterministic laws of "phenomenal" nature, where every event is completely determined by chains of causality. The most influential formulation was by Immanuel Kant, who distinguished between the causal deterministic framework the mind imposes on the world—the phenomenal realm—and the world as it exists for itself, the noumenal realm, which included free will. To insulate theology from reductionism, 19th century post-Enlightenment German theologians moved in a new direction, led by Friedrich Schleiermacher and Albrecht Ritschl. They took the Romantic approach of rooting religion in the inner world of the human spirit, so that it is a person's feeling or sensibility about spiritual matters that comprises religion.
Antireductionism.
The antireductionist takes this position as a minimum requirement upon the reductionist: "What is unclear is how the pre-theoretical intuitions [for example, of free will] are to be accommodated theoretically within favored analyses... At the very least the anti-reductionist is owed an account of why the intuitions arise if they are not accurate."
A contrast to the reductionist approach is holism or emergentism. Holism is the idea that things can have properties, (emergent properties), as a whole that are not explainable from the sum of their parts. The principle of holism was concisely summarized by Aristotle in the Metaphysics: "The whole is more than the sum of its parts".
The term greedy reductionism, coined by Daniel Dennett, is used to criticize inappropriate use of reductionism.
Alternatives.
The development of systems thinking has provided methods for tackling issues in a holistic rather than a reductionist way, and many scientists approach their work in a holistic paradigm. When the terms are used in a scientific context, holism and reductionism refer primarily to what sorts of models or theories offer valid explanations of the natural world; the scientific method of falsifying hypotheses, checking empirical data against theory, is largely unchanged, but the approach guides which theories are considered. The conflict between reductionism and holism in science is not universal—it usually centers on whether or not a holistic or reductionist approach is appropriate in the context of studying a specific system or phenomenon.
In many cases (such as the kinetic theory of gases), given a good understanding of the components of the system, one can predict all the important properties of the system as a whole. In other systems, emergent properties of the system are said to be almost impossible to predict from knowledge of the parts of the system. Complexity theory studies systems and properties of the latter type.
Alfred North Whitehead set his metaphysical thinking in opposition to reductionism. He refers to this as the 'fallacy of the misplaced concreteness'. His scheme set out to frame a rational, general understanding of things, that was derived from our reality.
Sven Erik Jorgensen, an ecologist, lays out both theoretical and practical arguments for a holistic approach in certain areas of science, especially ecology. He argues that many systems are so complex that it will not ever be possible to describe all their details. Drawing an analogy to the Heisenberg uncertainty principle in physics, he argues that many interesting and relevant ecological phenomena cannot be replicated in laboratory conditions, and thus cannot be measured or observed without influencing and changing the system in some way. He also points to the importance of interconnectedness in biological systems. His viewpoint is that science can only progress by outlining what questions are unanswerable and by using models that do not attempt to explain everything in terms of smaller hierarchical levels of organization, but instead model them on the scale of the system itself, taking into account some (but not all) factors from levels both higher and lower in the hierarchy.
Criticism.
"Fragmentalism" is an alternative term for ontological reductionism, although "fragmentalism" is frequently used in a pejorative sense. Anti-realists use the term fragmentalism in arguments that the world does not exist of separable entities, instead consisting of wholes. For example, advocates of this position hold that: The linear deterministic approach to nature and technology promoted a fragmented perception of reality, and a loss of the ability to foresee, to adequately evaluate, in all their complexity, global crises in ecology, civilization and education.
 The term "fragmentalism" is usually applied to reductionist modes of thought, frequently with the related pejorative term of "scientism". This usage is popular amongst some ecological activists: There is a need now to move away from scientism and the ideology of cause-and-effect determinism toward a radical empiricism, such as William James proposed, as an epistemology of science. These perspectives are not new and in the early twentieth century, William James noted that rationalist science emphasized what he termed fragmentation and disconnection.
 Such views also underpin many criticisms of the scientific method: The scientific method only acknowledges monophasic consciousness. The method is a specialized system that focuses on studying small and distinctive parts in isolation, which results in fragmented knowledge.
An alternative usage of this term is in cognitive psychology. Here, George Kelly developed "constructive alternativism" as a form of personal construct psychology, this provided an alternative to what he saw as "accumulative fragmentalism". In this theory, knowledge is seen as the construction of successful mental models of the exterior world, rather than the accumulation of independent "nuggets of truth".

</doc>
<doc id="49200" url="http://en.wikipedia.org/wiki?curid=49200" title="Queen Elizabeth 2">
Queen Elizabeth 2

Queen Elizabeth 2, often referred to simply as QE2, is an ocean liner built for the Cunard Line which was operated by Cunard as both a transatlantic liner and a cruise ship from 1969 to 2008. She was designed for the transatlantic service from her home port of Southampton, UK, to New York, and was named after the earlier Cunard liner . She served as the flagship of the line from 1969 until succeeded by in 2004. Designed in Cunard's then headquarters and regional offices in Liverpool and Southampton respectively, and built in Clydebank, Scotland, she was considered the last of the great transatlantic ocean liners until the construction of the Queen Mary 2 was announced.
Before she was refitted with a diesel power plant in 1986/87, "QE2" was also the last oil-fired passenger steamship to cross the Atlantic in scheduled liner service. During almost forty years of service, "Queen Elizabeth 2" undertook regular world cruises and latterly operated predominantly as a cruise ship, sailing out of Southampton, England. "QE2" had no running mate and never ran a year-round weekly transatlantic express service to New York. "QE2" did, however, continue the Cunard tradition of regular scheduled transatlantic crossings every year of her service life. "QE2" was never designated RMS, or Royal Mail Ship, instead carrying the SS and later MV or MS prefixes in official documents.
"QE2" retired from active Cunard service on 27 November 2008. She was acquired by Istithmar, the private equity arm of Dubai World, which planned to begin conversion of the vessel to a 500-room floating hotel moored at the Palm Jumeirah, Dubai. In July 2012, Istithmar announced converson plans but these never came to fruition. In 2013 "Oceanic Group" announced that the ship would sail in October of that year to Asia for conversion into a luxury hotel. This project also stalled and as of 2015 the ship remains laid up in Dubai.
Characteristics.
The ship has a gross tonnage of 70,327 and is 963 ft long. She had a top speed of 32.5 knots with her original steam turbines; this was increased to 34 kn when the vessel was re-engined with a diesel-electric powerplant.
History.
Concept and construction.
By the mid 1960s transatlantic travel was dominated by air travel due to its speed and low cost relative to the sea route, and expansion of air travel showed no signs of slowing down. Conversely, "Queen Mary" and "Queen Elizabeth" were becoming increasingly expensive to operate, and both internally and externally were relics of the pre-war years. Cunard did not want to give up the business of passenger service, and so gambled $80 million on a new ocean liner to replace the original ageing "Queens".
Realising the decline of transatlantic trade, and the rising costs of fuel and labour, Cunard decided their new ship had to be smaller and cheaper to operate than her predecessors. The new ship was designed to run at the same service speed of 28.5 kn as the previous "Queens", using half the fuel. Staff was also reduced from the levels on the older vessels. "QE2" would also be able to transit the Panama Canal and her draught was seven feet less than her predecessors, allowing her to enter ports that the old "Queens" could not, and compete with the new generation of cruise ships.
Originally designated "Q4" (a previous ship design "Q3" had been abandoned due to falling passenger revenues on the North Atlantic), she was to be a three class liner. However, looking to "France", designs were changed to make "Q4" a two-class liner that could be modified into a single class cruise ship; transatlantic line voyages in the summer would be two-class, while warmer water cruises in the winter would be single-class.
"Queen Elizabeth 2" was built by the John Brown Shipyard in Clydebank, Scotland. The keel was laid down on 5 July 1965, as hull number 736 on the same plot where iconic liners such as "Lusitania", "Aquitania", "Queen Mary", and "Queen Elizabeth" had been constructed. She was launched and named on 20 September 1967 by Queen Elizabeth II, using the same pair of gold scissors her mother and grandmother used to launch "Queen Elizabeth" and "Queen Mary", respectively. On 19 November 1968 she left John Brown's fitting out berth, and travelled down the River Clyde to the Firth of Clyde Dry Dock at Inchgreen, Greenock, for final trials and commissioning. After sea trials in the Irish Sea a "shakedown cruise" to Las Palmas set out on 22 April 1969.
Service history.
Early career.
"Queen Elizabeth 2"‍ '​s maiden voyage, from Southampton to New York, commenced on 2 May 1969, taking 4 days, 16 hours, and 35 minutes. However, Prince Charles was the first "civilian" passenger to board the ship, on her voyage from the shipyard in Clydebank to drydock in Greenock. On board for the short journey was her Master Designate and first captain, William (Bil) Warwick. In 1971, she participated in the rescue of some 500 passengers from the burning French Line ship "Antilles".
On 17 May 1972, while travelling from New York to Southampton, she was the subject of a bomb threat. She was searched by her crew, and a combined Special Air Service and Special Boat Service team which parachuted into the sea to conduct a search of the ship. No bomb was found, but the hoaxer was arrested by the FBI.
The following year "QE2" undertook two chartered cruises through the Mediterranean to Israel in commemoration of the 25th anniversary of the state's founding. As it was then known, The Columbia Restaurant was koshered for Passover, and Jewish passengers were able to celebrate Passover on the ship.
Falklands War.
In May 1982 the ship took part in the Falklands War, carrying 3,000 troops and 650 volunteer crew to the south Atlantic. She was refitted in Southampton in preparation for war service, including the installation of two helicopter pads, the transformation of public lounges into dormitories, the installation of fuel pipes that ran through the ship down to the engine room to allow for refuelling at sea, and the covering of carpets with 2,000 sheets of hardboard. A quarter of the ship’s length was reinforced with steel plating, an anti-magnetic coil was fitted to combat Naval Mine. Over 650 Cunard crewmembers volunteered for the voyage to look after the 3,000 members of the Fifth Infantry Brigade, which the ship transported to South Georgia. During the voyage the ship was blacked out and the radar switched off to avoid detection, steaming on without modern aids.
The "QE2" returned to the UK in June 1982, where she was greeted in Southampton Water by The Queen Mother on board the Royal Yacht "Britannia". Peter Jackson, the captain of the QE2 responded to the Queen Mother's welcome: "Please convey to Her Majesty Queen Elizabeth our thanks for her kind message. Cunard's Queen Elizabeth 2 is proud to have been of service to Her Majesty's Forces." The ship underwent conversion back to passenger service, with her funnel being painted in the traditional Cunard orange with black stripes which are known as "Hands", at the same time the hull's exterior was repainted an unconventional light pebble grey. This colour proved unpopular with passengers, as well as difficult to maintain and so the hull reverted to traditional colours in 1983. Later that year, QE2 was fitted with a magrodome over her Quarter Deck pool.
Diesel era.
"QE2" once again experienced mechanical problems following her annual overhaul in November 1983. Boiler problems caused Cunard to cancel a cruise, and, in October 1984, an electrical fire caused a complete loss of power. The ship was delayed for several days before power could be restored. Instead of replacing the QE2 with a newer vessel, Cunard decided that it was more prudent to simply make improvements to her. Therefore, in 1986/87, "QE2" underwent one of her most significant refurbishments when she was converted from steam power to diesel. Nine MAN B&W diesel electric engines, new propellers (and new equipment to capture heat expelled by the engines) were fitted. With her new propulsion system, "QE2" was expected to serve another 20 years with Cunard. The passenger accommodation was also modernised.
On 7 August 1992, the hull was extensively damaged when she ran aground south of Cuttyhunk Island near Martha's Vineyard, while returning from a five-day cruise to Halifax, Nova Scotia along the east coast of the United States and Canada. A combination of her speed, an uncharted shoal and underestimating the increase in the ship's draft due to the effect of squat led to the ship's hull scraping rocks on the ocean floor. The accident resulted in the passengers disembarking earlier than scheduled at nearby Newport, Rhode Island and the ship being taken out of service while temporary repairs were made in drydock at Boston. Several days later, divers found red paint on previously uncharted rocks in the vicinity of where the ship was said to have hit bottom.
Project Lifestyle.
By the mid 1990s it was decided that "QE2" was due for a new look and in 1994 the ship was given a multimillion pound refurbishment in Hamburg code named Project Lifestyle.
On 11 September 1995, "QE2" encountered a rogue wave, estimated at 90 ft, caused by Hurricane Luis in the North Atlantic Ocean about 200 miles south of eastern Newfoundland. One year later, during her twentieth world cruise, she completed her four millionth mile. The ship had sailed the equivalent of 185 times around the planet.
"QE2" celebrated the 30th anniversary of her maiden voyage in Southampton in 1999. In three decades she had 1,159 voyages, sailed 4,648,050 nmi and carried over two million passengers.
Later years.
Following the 1998 acquisition of the Cunard Line by Carnival Corporation, in 1999 "QE2" was given a US$30 million refurbishment which included refreshing various public rooms, and a new colour palette in the passenger cabins. The Royal Promenade, which formerly housed upscale shops such as Burberry, H. Stern and Aquascutum, were replaced by boutiques typical of cruise ships, selling perfumes, watches and logo items. During this refit the hull was stripped to bare metal, and the ship repainted in the traditional Cunard colours of matte black (Federal Grey) with a white superstructure.
In 2004 the vessel stopped plying the traditional "transatlantic" route and began full-time cruising, the transatlantic route having been assigned to Cunard's new flagship, the "Queen Mary 2". However, the "QE2" still undertook an annual world cruise and regular trips around the Mediterranean. By this time, she lacked the amenities to rival newer, larger cruise ships, but she still had unique features such as her ballrooms, hospital, and 6000 book library. "QE2" retained her title of one of the fastest cruise ships afloat (28.5 knots), with fuel economy at this speed at 49.5 ft (15m) to the gallon. While cruising at slower speeds efficiency was improved to 125 ft per gallon.
At the end of her 2005 world cruise, some pieces of her artwork were damaged when some crew members who had become inebriated at an on-board crew party, went on a vandalism rampage through the public areas of the ship. A unique tapestry of the "QE2", commissioned for the launch of the ship, was thrown overboard by a drunken crewman. An oil painting of Queen Elizabeth II and two other tapestries were damaged, along with a part of the entertainment area and a lifeboat. The crew members involved were dismissed from service, with charges pending.
On 5 November 2004 the "QE2" became Cunard's longest serving express liner, surpassing the RMS "Aquitania"‍ '​s 35 years, while on 4 September 2005, during a call to the port of Sydney, Nova Scotia, "QE2" became the longest serving Cunarder ever, surpassing the ‍ '​s record.
On 20 February 2007 the "QE2", while on her annual world cruise, met her running mate and successor flagship "QM2" (herself on her maiden world cruise) in Sydney Harbour, Australia. This was the first time two Cunard "Queens" had been together in Sydney since the original "Queen Mary" and "Queen Elizabeth" served as troop ships in 1941.
Retirement.
On 18 June 2007 it was announced by Cunard that "QE2" had been purchased by the Dubai investment company Istithmar for $100 million. Her retirement in part was forced by the oncoming June 2010 implementation of the International Convention for the Safety of Life at Sea (SOLAS) regulations, which would have forced large and expensive structural changes to have been implemented to the ship.
In a ceremonial display before her retirement, the "QE2" met the "Queen Victoria" and the "Queen Mary 2" near the Statue of Liberty in New York City harbour on 13 January 2008, with a celebratory fireworks display; the "QE2" and "QV" had made a tandem crossing of the Atlantic for the meet. This marked the first time three "Cunard Queens" had been present in the same location (Cunard stated this would be the last time these three particular ships would meet, due to the impending retirement of the "QE2". However, due to a change in the "QE2"‍ '​s schedule, the three ships met again in Southampton on 22 April 2008).
"QE2" shared the harbour at Zeebrugge with "Queen Victoria" on 19 July 2008, where the two Cunarders exchanged whistle blasts.
On 3 October 2008, "QE2" set off from Cork for Douglas Bay on her farewell tour of the British Isles, before heading for Liverpool. She left Liverpool and arrived in Belfast on 4 October 2008, before moving to Greenock the next day (the ship's height with funnel makes it impossible to pass under the Erskine Bridge so Clydebank is not reachable). There she was escorted by Royal Navy destroyer HMS "Manchester" and visited by . The farewell was viewed by large crowds and concluded with a firework display. "QE2" then sailed around Scotland to the Firth of Forth on 7 October 2008, where she anchored in the shadow of the Forth Bridge. The next day, following an RAF flypast, she left amidst a flotilla of small craft to head to Newcastle upon Tyne, before returning to Southampton.
"QE2" completed her final Atlantic crossing from New York to Southampton in tandem with her successor, "QM2". The two liners departed New York on 16 October and arrived in Southampton on 22 October. This marked the end of "QE2"‍ '​s transatlantic voyages.
Final Cunard voyage.
On her final arrival into Southampton, "QE2" (on 11 November 2008, with 1,700 passengers and 1,000 crew on board) ran aground in the Solent at the Southampton Water entrance at 5.26 am. BBC reported "Cunard has confirmed it touched the bottom at the Brambles Turn sandbank (sandback) near Calshot, Southampton Water, with three tugs attached to her stern (0530 GMT). A fourth tug secured a line to the ship's bow." Solent Coastguard stated: "Five tugs were sent out to assist her getting off the sandbank, and she was pulled off just before 6.10 am. She had been refloated and was under way under her own power and heading back to her berth in Southampton. She had only partially gone aground, and the tugs pulled her off."
Once safely back at her berth, preparations continued for her farewell celebrations. These were led by Prince Philip, Duke of Edinburgh who toured the ship at great length. He visited areas of interest including the Engine Control Room. He also met with current and former crew members. During this time, divers were sent down to inspect the hull for any possible damage caused by the vessel's earlier mishap – none were found.
The "QE2" left Southampton Docks for the final time at 1915 GMT on 11 November 2008, to begin her farewell voyage by the name of "QE2's Final Voyage". Her ownership passed to Nakheel Properties, a company of Dubai World, on 26 November. The decommissioning of the ship was particularly poignant for the "QE2"‍ '​s only permanent resident, Beatrice Muller, aged 89, who lived on board in retirement for fourteen years, at a cost of some £3,500 (~€4,300, ~$5,400) per month.
At the time of her retirement "QE2" had sailed nearly six million miles, carried 2.5 million passengers and completed 806 transatlantic crossings.
Istithmar, Nakheel and QE2 in Dubai.
Her final voyage from Southampton to Dubai began on 11 November 2008, arriving on 26 November in a flotilla of 120 smaller vessels, led by MY "Dubai", the personal yacht of Sheikh Mohammed, ruler of Dubai, in time for her official handover the following day.
She was greeted with a fly-past from an Emirates Airbus A380 jet and a huge fireworks display, while thousands of people gathered at the Mina Rashid, waving the flags of Great Britain and the United Arab Emirates. Since her arrival in Dubai "QE2" has remained moored at Port Rashid. Shortly after her final passengers were disembarked, she was moved forward to the cargo area of the port, to free up the passenger terminal for other cruise vessels.
She was expected to be refurbished and berthed permanently at Nakheel's Palm Jumeirah as "a luxury floating hotel, retail, museum and entertainment destination." The refurbishment planned to see the "QE2" transformed into a tourist destination in Dubai, however due to the Global Economic Crisis "QE2" has remained moored at Port Rashid awaiting a decision on her future.
"QE2" remains an oceangoing vessel, and as such, Ronald Warwick (former Captain of "QE2", "QM2" and a retired Commodore of the Cunard Line) was initially employed by V-Ships (who have managed "QE2" since Cunard handed her over) as the vessel's legal master, but has subsequently been replaced by other V-Ships captains. Since 2009, she has been captained by William Cooper.
It was anticipated that the "QE2" would be moved to the Dubai Drydocks sometime in 2009 to begin a series of far-reaching refurbishments which would result in her being converted into a floating hotel however, as of 2011[ [update]] no confirmed destination for the QE2's retirement and reopening has been announced.
Due to the 2008 global recession, fears have been sparked that "QE2"‍ '​s refurbishment and hotel conversion will not take place, and that the ship may be resold. These rumours have since resulted in owners, Istithmar, issuing a series of press releases stating that plans for QE2's conversion are ongoing, with no intention to sell. However, since arriving in Dubai the only visible exterior change to "QE2" is the painting out of the Cunard titles from the ship's superstructure.
"QE2" was joined in Mina Rashid by "QM2" on Saturday, 21 March 2009 while "QM2" visited Dubai as part of her 2009 World Cruise. She was joined once again by the "QV" on Sunday, 29 March 2009 as a part of her 2009 World Cruise. "QM2" and "QV" again visited QE2 in 2010 and on 31 March 2011 the new Queen Elizabeth "(QE)" called at Dubai during her maiden world cruise – photos were arranged by Cunard to capture the occasion. QM2 called in Dubai 2 days after QE left.
In April 2009, an alleged concept model of the post refurbished "Hotel QE2" was shown for sale on an online auction website. The model depicts a much altered "QE2".
In June 2009, the Southampton Daily Echo reported that "QE2" would return to the UK as an operating Cruise Ship. However, on 20 July 2009 the current owners Nakheel confirmed rumours that "QE2" will reposition to Cape Town for use as a floating Hotel.
On 24 June 2009, "QE2" made her first journey after nearly eight months of inactivity since the liner arrived in Dubai. She manoeuvered under her own power into the Dubai Drydocks for inspection and hull repainting before her (then planned) voyage to Cape Town's V&A Waterfront to serve there as a floating hotel for the FIFA World Cup 2010 and beyond.
Cape Town hotel proposal.
On 10 July 2009, it was revealed that "QE2" might sail to Cape Town, South Africa, to become a floating hotel (for use primarily during the 2010 FIFA World Cup), in a Dubai World sponsored venture at the V&A Waterfront. This was confirmed by Nakheel on 20 July 2009.
In preparation for this expected voyage the ship was placed into the Dubai Dry-dock and underwent an extensive exterior refurbishment. During this refit, the ship's underwater hull was repainted and inspected.
Shortly after her refit, "QE2" was registered under the flag of Vanuatu, and Port Vila (her new home port) was painted on her stern, replacing Southampton.
"QE2" returned to Port Rashid where it was anticipated she would soon sail for Cape Town. The arrival of "QE2" in Cape Town was expected to create many local jobs including Hotel staff, restaurant staff, chefs, cleaners and shop attendants, all being sourced from the local workforce.
But, in January 2010, it was confirmed she would not be brought to Cape Town.
At present the vessel remains moored in Dubai amid a cloud of uncertainty regarding her future.
2010 sale and relocation speculation.
In early 2010, due to the continued poor financial performance of Dubai World, there was much media speculation that QE2, along with other assets owned by Istithmar, Dubai World's private-equity arm, would be sold to raise capital. Despite this sale speculation, a number of alternative locations for QE2 have been cited including London, Singapore, Clydebank, Japan and Fremantle, the latter showing interest in using QE2 as a hotel for the ISAF Sailing World Championships to be held in December 2011. However as at June 2010 Nakheel's official statement regarding QE2 is that "a number of options being considered for QE2".
2011 drifting.
On 28 January 2011 during a heavy dust storm, "QE2" broke loose from her moorings and drifted out into the channel at Port Rashid. She was attended by pilots and tugs and safely returned to berth at Port Rashid. Images of "QE2"’s unexpected movements appeared on-line after being taken by an observer in the ship in front of QE2.
Warm layup.
Throughout 2011 and 2012, "QE2" remained berthed at Port Mina Rashid in Dubai in 2011[ [update]]. She was maintained in a seaworthy condition and generated her own power. Each of her nine diesel generators were turned over and used to power the ship. A live-in crew of approximately 50 people maintained "QE2" to a high standard. Activities include painting, maintenance, cabin checks, and overhauls of machinery. Istithmar were considering plans for "QE2" which may involve the ship sailing to an alternative location under her own power.
On 21 March 2011 "QM2" called in Dubai and docked close to "QE2". During the departure, the two ships sounded their horns.
2011 return to Liverpool plan.
On 28 September 2011 news broke that a plan was being formulated to return QE2 to the United Kingdom by berthing her in Liverpool. Liverpool has an historic connection with Cunard Line being the first British home for the line as well as housing the iconic Cunard Building.
It was revealed that Liverpool Vision, the economic development company responsible for Liverpool's regeneration, has been involved in confidential discussions with Out of Time Concepts, a company headed by a former Chief Engineer on the ship, who recently advised its current owners on plans to turn it into a luxury hotel in Dubai.
In a letter from Out of Time Concepts to Liverpool Vision it is explained that "The free global media attention derived from bringing home the "QE2" will without question promote Liverpool's new waterfront developments, its amazing architecture, its maritime and world heritage sites, its museums, its culture and its history".
Port Rashid and "QE2" development plans.
On the same week that the Liverpool Vision plans were revealed, Nakheel has said that plans for QE2 to be berthed at The Palm have been dropped because they now plan to build 102 houses on the site which was once intended to be named the QE2 Precinct.
Nakheel suggested that "QE2", under the ownership of Istithmar, will remain at Port Rashid to become an integral part of the growing cruise terminal. "The "QE2" will be placed in a much better location", Ali Rashid Lootah, the chairman of Nakheel, told Dubai's The National newspaper "The Government of Dubai is developing an up-to-date modern cruise terminal which will mean a better environment", confirming the ship would remain in Dubai for the foreseeable future.
2011/2012 New Year's Party aboard QE2.
On 31 December 2011 the QE2 was the location of a lavish New Year's Eve party in Dubai. The black tie event was run by Global Event Management and included over 1,000 guests. In early 2011[ [update]] Global Event Management were offering events aboard QE2 in Dubai for 2012 and 2013.
July 2012: hotel announcement.
On 2 July 2012 in a coordinated press release, the ship's owner, operator and Port Rashid operator, DP Ports, jointly announced "QE2" would re-open as a 300 bed hotel after an 18-month refit. The release claims the ship was to be refitted to restore original features, including her 1994-2008 'Heritage Trail' of classic Cunard artefacts. The ship was to be berthed alongside a redeveloped Port Rashid cruise terminal which would double as a maritime museum.
Scrapping in China and QE2 London.
On 23 December 2012, it was reported that "QE2" had been sold for scrapping in China for £20 million, after a bid to return her to the UK was rejected. With monthly berthing and maintenance charges of £650,000, it was reported that a Chinese salvage crew arrived at the vessel on 21 December, to replace a crew of 40 which has been maintaining the vessel since it arrived at Port Rashid. However, Cunard dismissed the reports as "pure speculation". When the ship was sold in 2007, a clause in the contract which started from her retirement in 2009 stipulated a 10-year "no onward sale" clause, without payment of a full purchase price default penalty.
The ""QE2" London" Plan had included a £20 million bid for "QE2" and a further £40 million refurbishment that was supposed to create more than 2,000 jobs in London, with the "QE2" docked near the O2 Arena. It had reportedly obtained the support of London Mayor Boris Johnson.
QE2 Asia.
On 17 January 2013, the Dubai Drydocks World announced that the "QE2" will be sent to an unknown location in Asia to serve as a floating luxury hotel, shopping mall, and museum. Despite this move, the QE2 London team stated on the same day that "We believe our investors can show Dubai that QE2 London is still the best proposal".
Design.
Exterior.
Like both "Normandie" and "France", "QE2" has a flared stem and clean forecastle. What was controversial at the time was that Cunard decided not to paint the funnel with the line's distinctive colour and pattern, something that had been done on all merchant vessels since the first Cunard ship, the , sailed in 1840. Instead the funnel was painted white and black, with the Cunard orange-red appearing only on the inside of the wind scoop. This practice ended in 1983 when "QE2" returned from service in the Falklands War, and the funnel has been painted in Cunard traditional colours (orange and black), with black horizontal bands (known as "hands") ever since. The original pencil-like funnel was replaced in 1986 with a more robust one, when the ship was converted from steam to diesel power.
Large quantities of aluminium were used in the framing and cladding of "QE2"‍ '​s superstructure. This decision was designed to save weight, reducing the draft of the ship and lowering the fuel consumption, but it also posed the possibility of corrosion problems that can occur with joining the dissimilar metals together, so a jointing compound was coated between the steel and aluminium surfaces to prevent this happening. The low melting point of aluminium caused concern when "QE2" was serving as a troop ship during the Falklands War: some feared that if the ship were struck by a missile, as was HMS "Sheffield", her upper decks would collapse quickly due to fire, thereby causing greater casualties.
In 1972, the first penthouse suites were added in an aluminium structure on Signal Deck and Sports Deck (now "Sun Deck"), behind the ship's bridge, and in 1977 this structure was expanded to include more suites with balconies, making "QE2" one of the first ships to offer private terraces to passengers since "Normandie" in the 1930s.
"QE2"‍ '​s balcony accommodation was expanded for the final time during "QE2"‍ '​s 1986/87 refurbishment in Bremerhaven. During this refit the ship was given a new wider funnel built using panels from the original. It retained the traditional Cunard colours.
"QE2"'s final structural changes included the reworking of the aft decks during the 1994 refit (following the removal of the Magrodome and the addition of an undercover area on Sun Deck during her 2005 refit creating a space known as Funnel Bar.
Interiors.
"Queen Elizabeth 2"‍ '​s interior configuration was laid out in a horizontal fashion, similar to "France", where the spaces dedicated to the two classes were spread horizontally on specific decks, in contrast to the vertical class divisions of older liners. Where "QE2" differed from "France" was that the first class deck (Quarter Deck) was below the deck dedicated to tourist class (Upper Deck). Originally there were to be main lounges serving three classes, layered one atop the other, but when Cunard decided to make the ship a two class vessel, only two main lounges were needed. Instead of completely reconfiguring the Boat Deck, the ship's architects simply opened a well in the deck between what were to have been the second and third class lounges, creating a double height space known as the Double Room (now the Grand Lounge). This too was unconventional in that it designated a grander two-storey space for tourist class passengers, while first class passengers gathered in the standard height Queen's Room. However, the configuration for segregated Atlantic crossings gave first class passengers the theatre balcony on Boat Deck, while tourist class used the orchestra level on Upper Deck.
Over the span of her thirty-nine-year seagoing career, "QE2" has had a number of interior refits and alterations.
The year she came into service, 1969, was also the year of the Apollo 11 mission, when the Concorde's prototype was unveiled, and the previous year Stanley Kubrick's film "" premiered. In keeping with those times, originally Cunard broke from the traditional interiors of their previous liners for "QE2", especially the Art Deco style of the previous "Queens". Instead modern materials like plastic laminates, aluminium and Perspex were used. Furniture was modular and abstract art was used throughout public rooms and cabins.
The Midships Lobby on Two Deck, where first class passengers boarded for transatlantic journeys and all passengers boarded for cruises, was a circular room with a sunken seating area in the centre with green leather clad banquettes, and surrounded by a chrome railing. As a kingpin to this was a flared, white, trumpet shaped, up lit column.
Another room where "QE2"‍ '​s advanced interior design was demonstrated was the first class lounge, the Queen's Room on Quarter Deck. This space, in colours of white and tan, featured a recessed, slotted ceiling, and indirect lighting. As well, the columns were flared in the same fashion as the one in the Midships Lobby, with recessed up lighting, and also reflecting the shape of the bases of the tables and leather shell chairs. The Theatre Bar on Upper Deck featured red chairs, red drapes, a red egg crate fibreglass screen, and even a red baby grand piano. Some more traditional materials like wood veneer were used as highlights throughout the ship, especially in passenger corridors and staterooms.
There was also an Observation Bar on Quarter Deck, a successor to its namesake, located in a similar location, on both previous "Queens", which offered views through large windows over the ship's bow. This room was lost in "QE2"‍ '​s 1972 refit, becoming galley space with the forward-facing windows plated over.
In the 1994 refit almost all of the remaining original decor was replaced, with Cunard opting to reverse the original design direction of "QE2"‍ '​s designers and use the line's traditional ocean liners as inspiration. The green velvet and leather Midships Bar became the Art Deco inspired Chart Room, receiving an original, custom designed piano from "Queen Mary". The (by now) blue dominated Theatre Bar was transformed into the Golden Lion Pub, which mimics a traditional Edwardian pub. Some original elements were retained including the flared columns in the Queens Room and Mid-Ships Lobby which were incorporated into the reworked designs.
By the time of her retirement, the Synagogue was the only room that had remained unaltered since 1969. However it was reported that during "QE2"‍ '​s 22 October five night voyage, the Synagogue was carefully dismantled before being removed from the ship prior to her final sailing to Dubai.
Artwork and artefacts.
The "Queen Elizabeth 2" holds pieces of artwork, as well as maritime artefacts drawn from Cunard's long history of operating merchant vessels.
In the Mauretania Restaurant sits Althea Wynne's sculpture of the "White Horses" of the Atlantic Ocean. There are bronze busts of both Sir Samuel Cunard (outside the Yacht Club) and Queen Elizabeth II (in the Queen's Room). The Princess Grill holds four life-size statues of human forms representing the four elements, done by sculptor Janine Janet in marine materials like shell and coral. The Chart Room's frieze was designed by Brody Nevenshwander, and depicts the words of T. S. Eliot, Sir Francis Drake, and John Masefield. The Midships Lobby holds a solid silver model of the "Queen Elizabeth 2" made by Asprey of Bond Street in 1975, which was lost until a photograph was found in 1997 that led to the discovery of the model itself, and its placement on the "QE2" in 1999.
In "E" stairway hangs three custom designed tapestries, commissioned from Helena Barynina Hernmarck for the ship's launch, that depict the Queen as well as the launch of the ship. These tapestries, which were originally hung in "D" Stairway, Quarter Deck, outside the Columbia Restaurant, were damaged in 2005, as mentioned in the Service history (above). They were originally made with golden threads; however much of this was lost when they were cleaned incorrectly as part of the 1987 refit.
There are numerous photographs, oils and pastels of members of the Royal Family throughout the vessel, and silver plaques commemorating the visits of every member of the Royal Family, as well as other dignitaries like South African president Nelson Mandela.
Amongst the artefacts on board is a set of antique Japanese armour presented to the "QE2" by the Governor of Kagoshima, Japan, during her 1979 world cruise, and a Wedgwood vase presented to the ship by Lord Wedgwood.
Items from previous Cunard ships include a brass relief plaque with a fish motif from the first , and an Art Deco bas-relief titled "Winged Horse and Clouds", by Norman Foster from RMS Queen Elizabeth. There is also a vast array of Cunard postcards, porcelain, flatware, boxes, linen, and Lines Bros Tri-ang Minic model ships. One of her key pieces is a replica of the figurehead from Cunard's first ship, RMS Britannia, carved from Quebec yellow pine by Cornish sculptor Charles Moore, and presented to the ship by Lloyd's of London. On the Upper Deck sits the silver Boston Commemorative Cup, presented to "Britannia" by the City of Boston in 1840. This cup was lost for decades until being found in a pawn shop in Halifax, Nova Scotia. On "2" Deck is a bronze entitled "Spirit of the Atlantic" which was designed by Barney Seale for the second . A large wooden plaque was presented to the "QE2" by First Sea Lord Sir John Fieldhouse to commemorate the ship's service as a Hired Military Transport (HMT) in the Falklands War.
There is also an extensive collection of large scale models of Cunard ships throughout the "QE2".
Most of these items were sold by Cunard to Istithmar when they purchased "QE2".
Crew accommodation.
The majority of crew were accommodated in two- or four-berth cabins, with showers and toilets at the end of each alleyway. These were located forward and aft on decks three through six. At the time she entered service, the crew areas were a significant improvement over those aboard and ; however the ship's age and the lack of renovation of the crew area during her 40 years of service, in contrast to passenger areas, which were updated periodically, meant that this accommodation was considered basic by the end of her career. Officers were accommodated in single cabins with private en-suite bathrooms located on Sun Deck.
There were three crew bars, one named "The Pig & Whistle". ("The Pig" for short and a tradition aboard Cunard ships), "Castaways" and the "Fo'c's'le Club". A fourth bar, dedicated for the officers is located at the forward end of Boat Deck. Named "The Officers Wardroom" this area enjoyed forward facing views and was often opened to passengers for cocktail parties hosted by the senior officers. The crew mess was situated at the forward end of One Deck, adjacent to the crew services office.
Technical.
After the ship was launched, the "QE2" was fitted out with a steam turbine propulsion system utilising three Foster Wheeler E.S.D II boilers, which provided steam for the two Brown-Pametrada turbines. The turbines were rated with a maximum power output figure of 110,000 shaft horsepower (normally operating at 94,000 hp) and were coupled to two six-bladed fixed-pitch propellers.
The steam turbines were plagued with problems from the time the ship first entered service and, despite being technically advanced and fuel-efficient in 1968, her consumption of 600 tons of fuel oil every twenty four hours was more than expected for such a ship by the 1980s. After seventeen years of service the availability of spare parts was becoming difficult due to the outdated design of the boilers and turbines, and Cunard decided that the options were to either do nothing for the remainder of the ship's life, re-configure the existing engines, or re-engine the vessel with a more efficient diesel-electric powerplant. Ultimately it was decided to replace the engines, as it was calculated that the savings in fuel costs and maintenance would pay for themselves over four years, and give the vessel a minimum of another twenty years of service, whereas the other options would only provide short-term relief. Her steam turbines had taken her to a record breaking total of 2,622,858 miles in 18 years.
During the ship's 1986 to 1987 refit, the steam turbines were removed and scrapped. The engine rooms were then fitted with nine German MAN L58/64 nine-cylinder, medium-speed diesel engines, each weighing approximately 120 tons. Using a diesel-electric configuration, each engine drives a generator, each developing 10.5 MW of electrical power at 10,000 volts. This electrical plant, in addition to powering the ship's auxiliary and hotel services through transformers, drives the two main propulsion motors, one on each propeller shaft. These motors produce 44 MW each and are of synchronised salient-pole construction, nine metres in diameter and weighing more than 400 tons each. The ship's service speed of 28.5 kn can be maintained using only seven of the diesel-electric sets. Her maximum power output with the new engine configuration running was now 130,000 hp, which is greater than the previous system's 110,000 hp. Using the same IBF-380 (Bunker C) fuel, the new configuration yielded a 35% fuel saving over the previous system. During the re-engining process, her funnel was replaced by a wider one to accommodate the exhaust pipes for the nine B&W diesel engines.
Also during refit, the fixed-pitch propellers were replaced with variable-pitch propellers. The old steam engines required astern turbines to move the ship backwards or stop her moving forward. The pitch of the new variable pitch blades, however, could simply be reversed, causing a reversal of propeller thrust while maintaining the same direction of propeller rotation, allowing the ship shorter stopping times and improved handling characteristics. The new propellers were originally fitted with "Grim Wheels", named after their inventor, Dr.-Ing. Otto Grim. These were free-spinning propeller blades fitted behind the main propellers, with long vanes protruding from the centre hub. These were designed to recover lost propeller thrust and reduce fuel consumption by 2.5 to 3%. However, after the trial of these wheels, when the ship was drydocked, the majority of the vanes on each wheel were discovered to have broken off, and so the wheels were removed and the project abandoned.
Other machinery includes nine heat recovery boilers, coupled with two oil-fired boilers to produce steam for heating fuel, domestic water, swimming pools, laundry equipment, and galleys. Four flash evaporators and a reverse-osmosis unit desalinate sea water to produce 1000 tons of fresh water daily. There is also a sanitation system and sewage disposal plant, air conditioning plant, and an electro-hydraulic steering system.
Name.
Form of name.
The name of the liner as it appears on the bow and stern is Queen Elizabeth 2, with upper and lower case lettering and an Arabic numeral 2 as opposed to the Roman numeral II. As such, it is commonly pronounced in speech as "Queen Elizabeth Two". Soon after launching, the name was shortened in common use as "QE2".
Due to various historical precedents of Cunard and other ship naming practices, and other occurrences surrounding the launch ceremony and form of the name of "QE2", it is uncertain whether the liner is named as the second liner named "Queen Elizabeth", or after the reigning monarch who named the ship, Queen Elizabeth II.
Background.
In 1934 "Queen Mary" was named by and after Mary of Teck and in 1938 "Queen Elizabeth" was named by and after Elizabeth Bowes-Lyon, who were both at the time of the naming married to the reigning monarch. These two previous Cunarders both had capitalised bow names, as "QUEEN ELIZABETH" and "QUEEN MARY".
Cunard practice at the time of naming "QE2" was to re use the existing name of its former ships, for example, launching the "Mauretania" in 1938 after the previous "Mauretania" was scrapped in 1935.
The original "Queen Elizabeth" was still in service with Cunard when "QE2" was launched in 1967, although she was retired and sold before "QE2" entered revenue service with Cunard in 1969.
The addition of a 2 in this manner was unknown at the time, but it was not unknown for Roman numerals to denote ships in service with the same name. Two non Cunard ships were named "Queen Mary II", a Clyde steamer, and "Mauretania II", a Southampton steamer of Red Funnel, since the Cunard ships already had the names without Roman numerals.
Launch.
As was Cunard practice at the time, the name of the liner was not to be publicly revealed until the launch. Dignitaries were invited to the "Launch of Cunard Liner No. 736", as no name had yet been painted on the bow.
The Queen launched the ship with the words "I name this ship Queen Elizabeth the Second," the normal short form of address of the monarch, Elizabeth II herself.
The following day, the New York Times and British Times printed the name as "Queen Elizabeth II", the short form of written style of the monarch. However, when the liner left the shipyard in 1968 she bore the name "Queen Elizabeth 2" on her bow, and has continued to do so ever since.
1969 authorised history.
In an authorised history of the "QE2" published in 1969, various explanations of events occur.
These state that, as at the launch ceremony, an envelope and card were also held in New York in case of transmission failure, and when opened the card was found to read the name Queen Elizabeth, and that the decision to add "The Second" to the name was an alteration by the Queen. The book quotes the Cunard chairman Sir Basil Smallpeice as saying "The Queen Mary [named] after her Grandmother, the Queen Elizabeth after her mother, and now this magnificent ship after herself."
Following the unexpected addition of "the Second" by the Queen, the book attributes the use of upper and lower case lettering and a numeric "2" – rather than a Roman "II" – to the decision by Cunard to use a more modern typeface to suit the style of the 1960s. The book also surmises that the naming of the liner after the reigning monarch, in the form Queen Elizabeth II, was potentially offensive to some Scots, as the title of Queen Elizabeth II (of the United Kingdom) relates to the lineage of the throne of England (the Tudor monarch Elizabeth I having reigned only in England).
Ron Warwick, former Captain.
A later account by Ronald Warwick (son of William "Bil" Warwick, first master of "QE2"), who was also Master of both "QE2" and later the first captain of "QM2", supports the account that the Queen initiated the surprise move of naming the liner after herself rather than simply Queen Elizabeth as had originally been planned (the name having been made vacant by the retirement of the current liner before the new one was commissioned). The name had been given to the Queen in a sealed envelope which she didn't open. The book, referencing his autobiography, states that the Cunard chairman Sir Basil Smallpeice was delighted with this development, it being in keeping with the previous Queen liners, and the 2 was added by Cunard for differentiation of the ship while still denoting it was named after the Queen.
Cunard website.
From at least 2002 the official Cunard website stated that "The new ship is not named after the Queen but is simply the second ship to bear the name – hence the use of the Arabic 2 in her name, rather than the Roman II used by the Queen", however, in a change in 2007 this information had been removed.
Other accounts.
Other later accounts repeat the position that Cunard originally intended to name the ship "Queen Elizabeth" and the addition of a 2 by the Queen was a surprise to Cunard, in 1990 and 2008, although two books by William H. Miller state that Queen Elizabeth 2 was the name agreed on before the launch between Cunard officials and the Queen.
Accounts that repeat the position that "QE2" was not named after the reigning monarch have been published in 1991, 1999, 2004, 2005, and 2008. In 2008, "The Telegraph" goes further to state the ship is named not only as the second ship named "Queen Elizabeth", but is specifically named after the wife of King George VI. In contradiction however, some modern accounts continue to publish that the "QE2" was named after the reigning monarch, in 2001 and 2008.
Post "QE2" Cunard naming practice.
Cunard continued the 2 suffix naming practice introduced with "QE2" with the launch of the new ocean liner in 2003, named after the previous "Queen Mary", but her second "consort" ship was named , not "QE3", the first being named .

</doc>
<doc id="49201" url="http://en.wikipedia.org/wiki?curid=49201" title="Biplane">
Biplane

A biplane is a fixed-wing aircraft with two main wings stacked one above the other. The first aircraft to fly, the Wright Flyer, used a biplane design, as did most aircraft in the early years of aviation. While a biplane wing structure has a structural advantage over a monoplane, it produces more drag than a similar unbraced or cantilever monoplane wing. Improved structural techniques, materials and the quest for greater speed made the biplane configuration obsolete for most purposes by the late 1930s.
The tandem wing design differs in that one of the two wings is placed forward and the other aft, such that no horizontal stabilizer is necessary.
The term is also occasionally used in biology, to describe the wings of some flying animals.
Aviation.
Overview.
In a biplane aircraft, two wings are placed one above the other. Both provide part of the lift, although they are not able to produce twice as much lift as a single wing of similar size and shape because the upper and the lower are working on nearly the same portion of the atmosphere and thus interfere with each other's behaviour. For example, in a wing of aspect ratio 6, and a wing separation distance of one chord length, the biplane configuration will only produce about 20 percent more lift than a single wing of the same planform.
In the biplane configuration, the lower wing is usually attached to the fuselage, while the upper wing is raised above the fuselage with an arrangement of cabane struts, although other arrangements have been used. Either or both of the main wings can support ailerons, while flaps are more usually positioned on the lower wing. Bracing is nearly always added between the upper and lower wings, in the form of wires (tension members) and/or slender interplane struts positioned symmetrically on either side of the fuselage.
Bays.
The space enclosed by a set of struts is called a bay, hence a biplane with one set of such struts on each side is said to be a single-bay biplane. Two bay biplanes, with one set of struts best known examples are the Nieuport military aircraft — from the Nieuport 10 through to the Nieuport 27, all designed by Gustave Delage during the Great War. The later Waco Custom Cabin series proved to be a popular example in general aviation.
Advantages and disadvantages.
Aircraft built with two main wings (or three in a triplane) can usually lift up to 20 percent more than can a similarly sized monoplane of similar wingspan. A biplane will therefore typically have a shorter wingspan than the equivalent monoplane, which tends to afford greater maneuverability.
The struts and wire bracing of a typical biplane form a box girder. Particularly when divided into bays, this permits a very light but strong and rigid wing structure. This allows a biplane to fly with very little power, and in the early days of aviation most fixed-wing aircraft (including the very first, the Wright Flyer) were biplanes.
On the other hand there are many disadvantages to the configuration. Each wing negatively interferes with the aerodynamics of the other, requiring greater overall surface area to produce the same lift as the equivalent monoplane. A biplane typically also produces more drag than a monoplane, especially as speed increases.
Stagger.
Biplanes were originally designed with the wings positioned directly one above the other. Moving upper wing forward relative to the lower one is called positive stagger or, more often, simply stagger. It can help increase lift and reduce drag by reducing the aerodynamic interference effects between the two wings. Many biplanes have such staggered wings. A common example from the 1930s is the layout found for the Waco Standard Cabin series.
It is also possible to place the lower wing's leading edge ahead of the upper wing, giving negative stagger. This is usually done in a given design for practical engineering reasons. Examples of negative stagger include the Airco DH.5, Sopwith Dolphin and Beechcraft Staggerwing. However, forward stagger is more common because it improves both downward visibility and ease of cockpit access for open cockpit biplanes.
Staggering the wings may distort the box girder effect of the wing and reduce the structural benefits of the biplane layout.
History.
Before any successful powered flight, most aeroplane designs envisaged monoplanes. The weakness of the materials and design techniques available made it difficult to design wings which were both light and strong enough to fly. Many designs used external bracing struts and wires. The Cody kite, which comprised a box kite with wings attached to its upper surfaces, took a different approach. By 1896 Octave Chanute was flying a biplane hang glider and concluded that the externally braced biplane offered better prospects for powered flight than the monoplane. The "Wright Flyer" biplane of 1903 became the first successful powered aeroplane.
Throughout the pioneer years, both biplanes and monoplanes were common, but by the outbreak of the First World War biplanes had started to gain favour due to their better manoeuvrability and inherent strength, as exeexternal bracing to create a clean "cantilever" wing.
. But by the 1930s engine power had risen to the point where the fast cantilever monoplane — first flown by the end of 1915, as a pioneering all-metal design from the workshops founded by Hugo Junkers — took over and the slower biplane designs began to be phased out.
At the start of World War II, several air forces still had biplane combat aircraft in front line use, and some were retained in specialist roles, such as primary training or shipboard operation, until the end of the war and even beyond. Examples of biplane fighters operational in 1939 were the British Gloster Gladiator and the Italian Fiat CR.42. The German Heinkel He 50 and the Soviet Polikarpov Po-2 were both used in the night ground attack role until late in the war; the latter was even used by the Korean People's Air Force in the Korean War for night bombing. The British Fleet Air Arm flew the Fairey Swordfish and Albacore from its aircraft carriers; originally intended as torpedo bombers, they continued to be used in the anti-submarine warfare role until the end of the war, because of their ability to operate from the decks of small escort carriers. Biplane trainers included the de Havilland Tiger Moth in the Royal Air Force, which also served beside the Fleet Finch in Canada (where Tiger Moths were also license-built), Stampe SV.4 in the French and Belgian Air Forces, and the Boeing Stearman in the USAF. In later civilian use, the Stearman became particularly associated with stunt flying such as wing-walking.
Modern biplane designs still exist in specialist niche roles such as aerobatics and agricultural aircraft with the competition aerobatics role and format for such a biplane well-defined in the mid-1930s by the Bücker Bü 133 "Jungmeister". The Pitts Special, said to have been inspired by the "Jungmeister", dominated aerobatics for many years after World War II and is still in production, while the WACO Classic YMF is a reproduction of the original Waco design.
The vast majority of biplane designs have been fitted with reciprocating engines of comparatively low power; exceptions include the Antonov An-3 and WSK-Mielec M-15 Belphegor, fitted with turboprop and turbofan engines respectively. Some older biplane designs, such as the Grumman Ag Cat and the aforementioned An-2 (in the form of the An-3) are available in upgraded versions with turboprop engines.
Other famous biplanes include the Antonov An-2, Beechcraft Staggerwing and Curtiss JN-4. The two most produced biplane designs initiated before 1945 — the 1913-origin British Avro 504 (8,970 built before November 1918) and the 1928-origin Soviet Polikarpov Po-2 (over 20,000 built, at least) — each contributed to air combat within their own era, with similar light bombing raids for each type undertaken during the World Wars, and coincidentally with the Po-2 being the direct replacement for the Soviet "Avrushka" copies of the 504 itself.
Ultralight aircraft.
Although most ultralights are monoplanes, the low speeds and simple construction involved have inspired a small number of biplane ultralights, such as Larry Mauro's "Easy Riser" (1975- ). Mauro also made a version powered with solar cells driving an electric motor - called the Solar Riser. Mauro's "Easy Riser" was used by the man who became known as "Father Goose", Bill Lishman.
Other biplane ultralights include the Belgian-designed Aviasud Mistral, the German FK12 Comet (1997- ), and the Lite Flyer Biplane.
Another biplane microlight is the Murphy Renegade.
In avian evolution.
It has been suggested the feathered dinosaur "Microraptor" glided, and perhaps even flew, on four wings, which were held in a biplane-like arrangement. This was made possible by the presence of flight feathers on both the forelimbs and hindlimbs of "Microraptor", and it has been suggested the earliest flying ancestors of birds may have possessed this morphology, with the monoplane arrangement of modern birds evolving later.

</doc>
<doc id="49202" url="http://en.wikipedia.org/wiki?curid=49202" title="Ignacy Krasicki">
Ignacy Krasicki

Ignacy Krasicki (3 February 1735 – 14 March 1801), from 1766 Prince-Bishop of Warmia (in German, "Ermland") and from 1795 Archbishop of Gniezno (thus, Primate of Poland), was Poland's leading Enlightenment poet ("the Prince of Poets"), a critic of the clergy, Poland's La Fontaine, author of the first Polish novel, playwright, journalist, encyclopedist, and translator from French and Greek.
His most notable literary works were his "Fables and Parables" (1779), "Satires" (1779), and poetic letters and religious lyrics, in which the artistry of his poetic language reached its summit.
Life.
Krasicki was born in Dubiecko, on southern Poland's San River, into a family bearing the title of count of the Holy Roman Empire. He was related to the most illustrious families in the Polish-Lithuanian Commonwealth and spent his childhood surrounded with the love and solicitude of his own family. 
He attended a Jesuit school in Lwów, then studied at a Warsaw Catholic seminary (1751–54). In 1759 he took holy orders and continued his education in Rome (1759–61). Two of his brothers also entered the priesthood.
Returning to Poland, Krasicki became secretary to the Primate of Poland and developed a friendship with future King Stanisław August Poniatowski. When Poniatowski was elected king (1764), Krasicki became his chaplain. He participated in the King's famous "Thursday dinners" and co-founded the "Monitor", the preeminent Polish Enlightenment periodical, sponsored by the King.
In 1766 Krasicki, after having served that year as coadjutor to Prince-Bishop of Warmia Adam Stanisław Grabowski, was himself elevated to Prince-Bishop of Warmia and "ex officio" membership in the Senate of the Commonwealth. This office gave him a high standing in the social hierarchy and a sense of independence. It did not, however, prove a quiet haven. The Warmia cathedral chapter welcomed its superior coolly, fearing changes. At the same time, there were growing provocations and pressures from Prussia, preparatory to seizure of Warmia in the First Partition of the Polish-Lithuanian Commonwealth. Krasicki protested publicly against external intervention.
In 1772, as a result of the First Partition, instigated by Prussia's King Frederick II ("the Great"), Krasicki became a Prussian subject. He did not, however, pay homage to Warmia's new master.
He now made frequent visits to Berlin, Potsdam and Sanssouci at the bidding of Frederick, with whom he cultivated an acquaintance. This created a difficult situation for the poet-bishop who, while a friend of the Polish king, maintained close relations with the Prussian king. These realities could not but influence the nature and direction of Krasicki's subsequent literary productions, perhaps nowhere more so than in the "Fables and Parables" (1779).
Soon after the First Partition, Krasicki officiated at the 1773 opening of Berlin's St. Hedwig's Cathedral, which Frederick had built for Catholic immigrants to Brandenburg and Berlin. In 1786 Krasicki was called to the Prussian Academy of Sciences. His residences in the castle of the bishops of Warmia at Lidzbark Warmiński (in German, "Heilsberg") and in the summer palace of the bishops of Warmia at Smolajny became centers of artistic patronage for all sectors of partitioned Poland.
After Frederick the Great's death, Krasicki continued relations with Frederick's successor. 
In 1795, six years before his death, Krasicki was elevated to Archbishop of Gniezno (thus, to Primate of Poland).
Krasicki was honored by Poland's King Stanisław August Poniatowski with the Order of the White Eagle and the Order of Saint Stanisław, as well as with a special 1780 medal featuring the Latin device, "Dignum laude virum Musa vetat mori" ("The Muse will not let perish a man deserving of glory"); and by Prussia's King Frederick the Great, with the Order of the Red Eagle.
Upon his death in Berlin in 1801, Krasicki was laid to rest at St. Hedwig's Cathedral, which he had consecrated. In 1829 his remains were transferred to Poland's Gniezno Cathedral.
Czesław Miłosz describes Krasicki:
He was a man of the golden mean, a smiling, skeptical sage [who] prais[ed] moderation and despis[ed] extremes. His was a mentality which returned to Horatian ideals of the Renaissance, to a life of contemplative retirement. This did not interfere with his talents as a courtier: he was a favorite of [Poland's King] Stanisław August [Poniatowski], and after the [F]irst [P]artition [of Poland, in 1772], when his bishopric of Warmia became the property of Prussia, he was a favorite of King Frederick the Great. [H]e was a cosmopolit[e] and owed his imposing literary knowledge to his readings in foreign languages, yet... he was indebted to the mentality of the Polish "Golden Age," and in this respect his admiration for Erasmus of Rotterdam is significant. As a poet, he was [chiefly responsible] for that distillation of the [Polish] language which for a while toned down the chaotic richness of the Baroque. In a way, he returned to the clear and simple language of [Jan] Kochanowski, and his role in Polish poetry may be compared to that of Alexander Pope in English poetry. [H]e conceived of literature as a specific vocation, namely, to intervene as a moralist in human affairs. Since he was not pugnacious by temperament (contrary to one of his masters, Voltaire), his moralizing, rarely distinguishable from sheer play, [does not show] vitriolic accents.
Works.
Ignacy Krasicki was the leading literary representative of the Polish Enlightenment—a prose writer and poet highly esteemed by his contemporaries, who admired his works for their wit, imagination and fluid style. 
Krasicki's literary writings lent splendor to the reign of Poland's King Stanisław August Poniatowski, while not directly advocating the King's political program.
Krasicki, the leading representative of Polish classicism, debuted as a poet with the strophe-hymn, "Święta miłości kochanej ojczyzny" ("O Sacred Love of the Beloved Country"). He was then nearing forty. It was thus a late debut that brought the extraordinary success of this strophe, which Krasicki would incorporate as part of song IX in his mock-heroic poem, "Myszeida" (Mouseiad, 1775). In "O Sacred Love," Krasicki formulated a universal idea of patriotism, expressed in high style and elevated tone. The strophe would later, for many years, serve as a national anthem and see many translations, including three into French.
The Prince Bishop of Warmia gave excellent Polish form to all the genres of European classicism. He also blazed paths for new genres. Prominent among these was the first modern Polish novel, "Mikołaja Doświadczyńskiego przypadki" (The Adventures of Nicholas Experience, 1776), a synthesis of all the varieties of the Enlightenment novel: the social-satirical, the adventure ("à la" "Robinson Crusoe"), the Utopian and the didactic.
Tradition has it that Krasicki's mock-heroic poem, "Monachomachia" (War of the Monks, 1778), was inspired by a conversation with Frederick II at the palace of Sanssouci, where Krasicki was staying in an apartment that had once been used by Voltaire. At the time, the poem's publication caused a public scandal.
The most enduring literary monument of the Polish Enlightenment is Krasicki's fables: "Bajki i Przypowieści" (Fables and Parables, 1779) and "Bajki nowe" (New Fables, published posthumously in 1802). The poet also set down his trenchant observations of the world and human nature in "Satyry" (Satires, 1779).
Other works by Krasicki include the novels, "Pan Podstoli" (Lord High Steward, published in three parts, 1778, 1784 and posthumously 1803), which would help inspire works by Mickiewicz, and "Historia" (History, 1779); the epic, "Wojna chocimska" (The Chocim War, 1780, about the Khotyn War); and numerous others, in homiletics, theology and heraldry. 
Krasicki also published, in 1781, a two-volume encyclopedia, "Zbiór potrzebniejszych wiadomości" (A Collection of Needful Knowledge), the second Polish general encyclopedia after Benedykt Chmielowski's "Nowe Ateny" (The New Athens, 1745–46). 
Krasicki wrote "Listy o ogrodach" (Letters about Gardens) and articles in the "Monitor", which he had co-founded, and in his own newspaper, "Co Tydzień" (Each Week). 
Krasicki translated, into Polish, Plutarch, "Ossian", fragments of Dante's "Divine Comedy", and works by Anacreon, Boileau, Hesiod and Theocritus. He wrote a 1772 essay "On the Translation of Books" ("O przekładaniu ksiąg") and another, published posthumously in 1803, "On Translating Books" ("O tłumaczeniu ksiąg").
Fame.
Krasicki's major works won European fame and were translated into Latin, French, German, Italian, Russian, Czech, Croatian, Slovene, and Hungarian. The broad reception of his works was sustained throughout the 19th century.
Krasicki has been the subject of works by poets of the Polish Enlightenment – Stanisław Trembecki, Franciszek Zabłocki, Wojciech Mier – and in the 20th century, by Konstanty Ildefons Gałczyński. He has been the hero of prose works by Wincenty Pol, Adolf Nowaczyński and Henryk Sienkiewicz.

</doc>
<doc id="49207" url="http://en.wikipedia.org/wiki?curid=49207" title="Laura Gemser">
Laura Gemser

Laurette Marcia "Laura" Gemser (born October 5, 1950, Surabaya, Indonesia) is a Dutch actress of Indo descent, now Italian citizen. She is known for her work with director Joe D'Amato and Bruno Mattei, in particular, for doing a set of exploitation-style and "Black Emanuelle" films.
Gemser has also been credited as Moira Chen, most notably in "Love Is Forever" (1983).
Life and career.
Gemser left Indonesia in 1955, at the age of four, and moved with her parents to the Netherlands. She grew up in the Dutch city of Utrecht, where she attended the MULO Regentesseschool high school. After that, she attended the Artibus Art School in Utrecht, where she specialized in fashion design. After modelling in various magazines in the Netherlands and Belgium, in 1974 she moved to Italy to star in the erotic film "Amore libero - Free Love". The film was a box office success and launched the career of Gemser, who later became internationally recognised after starring in a number of "Black Emanuelle" films in the 1970s.
Her most mainstream and well-received role was as Laotian refugee Keo Sirisomphone in Michael Landon's 1983 American television movie, "Love is Forever", in which she was credited as Moira Chen. Gemser continued to do films: at times, she worked with her actor husband, Gabriele Tinti. In the 1990s, she left the movies to do costume designing for film. In addition, she lost her husband, who died of cancer in 1991. Today she lives in retirement and low profile in Rome.

</doc>
<doc id="49209" url="http://en.wikipedia.org/wiki?curid=49209" title="Carburetor">
Carburetor

A carburetor (American and Canadian spelling), carburator, carburettor, or carburetter (Commonwealth spelling) is a device that blends air and fuel for an internal combustion engine. It is sometimes colloquially shortened to "carb" in North America or "carby" in Australia. To carburate or carburet (and thus carburetion or carburation) is to blend the air and fuel or to equip (an engine) with a carburetor for that purpose. 
Carburetors have largely been supplanted in the automotive industry by fuel injection. They are still common on small engines for lawn mowers, rototillers, and other equipment. 
Etymology.
The word "carburetor" comes from the French "carbure" meaning "carbide". "Carburer" means to combine with carbon (compare also carburizing). In fuel chemistry, the term has the more specific meaning of increasing the carbon (and therefore energy) content of a fluid by mixing it with a volatile hydrocarbon.
History and development.
The carburetor was invented by an Italian, Luigi De Cristoforis, in 1876. A carburetor was developed by Enrico Bernardi at the University of Padua in 1882, for his Motrice Pia, the first petrol combustion engine (one cylinder, 121.6 cc) prototyped on 5 August 1882.
A carburetor was among the early patents by Karl Benz as he developed internal combustion engines and their components.
Early carburetors were the surface carburetor type, in which air is charged with fuel by being passed over the surface of gasoline.
In 1885, Wilhelm Maybach and Gottlieb Daimler developed a float carburetor for their engine based on the atomizer nozzle. The Daimler-Maybach carburetor was copied extensively, leading to patent lawsuits, but British courts rejected the Daimler company's claim of priority in favor of Edward Butler's 1884 spray carburetor used on his Petrol Cycle.
Hungarian engineers János Csonka and Donát Bánki patented a carburetor for a stationary engine in 1893. 
Frederick William Lanchester of Birmingham, England, experimented with the wick carburetor in cars. In 1896, Frederick and his brother built the first gasoline-driven car in England: a single cylinder 5 hp internal combustion engine with chain drive. Unhappy with the performance and power, they re-built the engine the next year into a two-cylinder horizontally opposed version using his new wick carburetor design.
Carburetors were the usual method of fuel delivery for most US-made gasoline-fueled engines up until the late 1980s, when fuel injection became the preferred method. This change was dictated more by the requirements of catalytic converters than by any inherent inefficiency of carburation; a catalytic converter requires much more precise control over the fuel / air mixture, to closely control the amount of oxygen in the exhaust gases. In the U.S. market, the last carbureted cars were:
In Australia, some cars continued to use carburetors well into the 1990s; these included the Honda Civic (1993), the Ford Laser (1994), the Mazda 323 and Mitsubishi Magna sedans (1996), the Daihatsu Charade (1997), and the Suzuki Swift (1999). Low-cost commercial vans and 4WDs in Australia continued with carburetors even into the 2000s, the last being the Mitsubishi Express van in 2003. Elsewhere, certain Lada cars used carburetors until 2006. Many motorcycles still use carburetors for simplicity's sake, since a carburetor does not require an electrical system to function. Carburetors are also still found in small engines and in older or specialized automobiles, such as those designed for stock car racing, though NASCAR's 2011 Sprint Cup season was the last one with carbureted engines; electronic fuel injection was used beginning with the 2012 race season in Cup.
In Europe, carburetor-engined cars were being gradually phased out by the end of the 1980s in favor of fuel injection, which was already the established type of engine on more expensive vehicles including luxury and sports models. EEC legislation required all vehicles sold and produced in member countries to have a catalytic converter after December 1992; among the last carburetor-engined models produced in these countries were most of the Ford Fiesta MK2 range (1989) as well as cheaper versions of the Nissan Primera (1990) and Peugeot's 106 and 405 range - the French built 106 went into production just over a year before carburetor engines were outlawed in the EEC.
Principles.
The carburetor works on Bernoulli's principle: the faster air moves, the lower its static pressure, and the higher its dynamic pressure. The throttle (accelerator) linkage does not directly control the flow of liquid fuel. Instead, it actuates carburetor mechanisms which meter the flow of air being pulled into the engine. The speed of this flow, and therefore its pressure, determines the amount of fuel drawn into the airstream.
When carburetors are used in aircraft with piston engines, special designs and features are needed to prevent fuel starvation during inverted flight. Later engines used an early form of fuel injection known as a pressure carburetor.
Most production carbureted, as opposed to fuel-injected, engines have a single carburetor and a matching intake manifold that divides and transports the air fuel mixture to the intake valves, though some engines (like motorcycle engines) use multiple carburetors on split heads. Multiple carburetor engines were also common enhancements for modifying engines in the USA from the 1950s to mid-1960s, as well as during the following decade of high-performance muscle cars fueling different chambers of the engine's intake manifold.
Older engines used updraft carburetors, where the air enters from below the carburetor and exits through the top. This had the advantage of never flooding the engine, as any liquid fuel droplets would fall out of the carburetor instead of into the intake manifold; it also lent itself to use of an oil bath air cleaner, where a pool of oil below a mesh element below the carburetor is sucked up into the mesh and the air is drawn through the oil-covered mesh; this was an effective system in a time when paper air filters did not exist.
Beginning in the late 1930s, downdraft carburetors were the most popular type for automotive use in the United States. In Europe, the sidedraft carburetors replaced downdraft as free space in the engine bay decreased and the use of the SU-type carburetor (and similar units from other manufacturers) increased. Some small propeller-driven aircraft engines still use the updraft carburetor design.
Outboard motor carburetors are typically sidedraft, because they must be stacked one on top of the other in order to feed the cylinders in a vertically oriented cylinder block.
The main disadvantage of basing a carburetor's operation on Bernoulli's Principle is that, being a fluid dynamic device, the pressure reduction in a Venturi tends to be proportional to the square of the intake air speed. The fuel jets are much smaller and limited mainly by viscosity, so that the fuel flow tends to be proportional to the pressure difference. So jets sized for full power tend to starve the engine at lower speed and part throttle. Most commonly this has been corrected by using multiple jets. In SU and other movable jet carburetors, it was corrected by varying the jet size. For cold starting, a different principle was used in multi-jet carburetors. A flow resisting valve called a choke, similar to the throttle valve, was placed upstream of the main jet to reduce the intake pressure and suck additional fuel out of the jets.
Operation.
Under all engine operating conditions, the carburetor must:
This job would be simple if air and gasoline (petrol) were ideal fluids; in practice, however, their deviations from ideal behavior due to viscosity, fluid drag, inertia, etc. require a great deal of complexity to compensate for exceptionally high or low engine speeds. A carburetor must provide the proper fuel/air mixture across a wide range of ambient temperatures, atmospheric pressures, engine speeds and loads, and centrifugal forces:
In addition, modern carburetors are required to do this while maintaining low rates of exhaust emissions.
To function correctly under all these conditions, most carburetors contain a complex set of mechanisms to support several different operating modes, called "circuits".
Basics.
A carburetor basically consists of an open pipe through which the air passes into the inlet manifold of the engine. The pipe is in the form of a Venturi: it narrows in section and then widens again, causing the airflow to increase in speed in the narrowest part. Below the Venturi is a butterfly valve called the throttle valve — a rotating disc that can be turned end-on to the airflow, so as to hardly restrict the flow at all, or can be rotated so that it (almost) completely blocks the flow of air. This valve controls the flow of air through the carburetor throat and thus the quantity of air/fuel mixture the system will deliver, thereby regulating engine power and speed. The throttle is connected, usually through a cable or a mechanical linkage of rods and joints or rarely by pneumatic link, to the accelerator pedal on a car or the equivalent control on other vehicles or equipment.
Fuel is introduced into the air stream through small holes at the narrowest part of the Venturi and at other places where pressure will be lowered when not running on full throttle. Fuel flow is adjusted by means of precisely calibrated orifices, referred to as "jets", in the fuel path.
Off-idle circuit.
As the throttle is opened up slightly from the fully closed position, the throttle plate uncovers additional fuel delivery holes behind the throttle plate where there is a low pressure area created by the throttle plate blocking air flow; these allow more fuel to flow as well as compensating for the reduced vacuum that occurs when the throttle is opened, thus smoothing the transition to metering fuel flow through the regular open throttle circuit.
Main open-throttle circuit.
As the throttle is progressively opened, the manifold vacuum is lessened since there is less restriction on the airflow, reducing the flow through the idle and off-idle circuits. This is where the Venturi shape of the carburetor throat comes into play, due to Bernoulli's principle (i.e., as the velocity increases, pressure falls). The Venturi raises the air velocity, and this high speed and thus low pressure sucks fuel into the airstream through a nozzle or nozzles located in the center of the Venturi. Sometimes one or more additional "booster Venturis" are placed coaxially within the primary Venturi to increase the effect.
As the throttle is closed, the airflow through the Venturi drops until the lowered pressure is insufficient to maintain this fuel flow, and the idle circuit takes over again, as described above.
Bernoulli's principle, which is a function of the velocity of the fluid, is a dominant effect for large openings and large flow rates, but since fluid flow at small scales and low speeds (low Reynolds number) is dominated by viscosity, Bernoulli's principle is ineffective at idle or slow running and in the very small carburetors of the smallest model engines. Small model engines have flow restrictions ahead of the jets to reduce the pressure enough to suck the fuel into the air flow. Similarly the idle and slow running jets of large carburetors are placed after the throttle valve where the pressure is reduced partly by viscous drag, rather than by Bernoulli's principle. The most common rich mixture device for starting cold engines was the choke, which works on the same principle.
Power valve.
For open throttle operation a richer mixture will produce more power, prevent pre-ignition detonation, and keep the engine cooler. This is usually addressed with a spring-loaded "power valve", which is held shut by engine vacuum. As the throttle opens up, the vacuum decreases and the spring opens the valve to let more fuel into the main circuit. On two-stroke engines, the operation of the power valve is the reverse of normal — it is normally "on" and at a set rpm it is turned "off". It is activated at high rpm to extend the engine's rev range, capitalizing on a two-stroke's tendency to rev higher momentarily when the mixture is lean.
Alternative to employing a power valve, the carburetor may utilize a "metering rod" or "step-up rod" system to enrich the fuel mixture under high-demand conditions. Such systems were originated by Carter Carburetor in the 1950s for the primary two Venturis of their four barrel carburetors, and step-up rods were widely used on most 1-, 2-, and 4-barrel Carter carburetors through the end of production in the 1980s. The step-up rods are tapered at the bottom end, which extends into the main metering jets. The tops of the rods are connected to a vacuum piston and/or a mechanical linkage which lifts the rods out of the main jets when the throttle is opened (mechanical linkage) and/or when manifold vacuum drops (vacuum piston). When the step-up rod is lowered into the main jet, it restricts the fuel flow. When the step-up rod is raised out of the jet, more fuel can flow through it. In this manner, the amount of fuel delivered is tailored to the transient demands of the engine. Some 4-barrel carburetors use metering rods only on the primary two Venturis, but some use them on both primary and secondary circuits, as in the Rochester Quadrajet.
Accelerator pump.
Liquid gasoline, being denser than air, is slower than air to react to a force applied to it. When the throttle is rapidly opened, airflow through the carburetor increases immediately, faster than the fuel flow rate can increase. This transient oversupply of air causes a lean mixture, which makes the engine misfire (or "stumble")—an effect opposite what was demanded by opening the throttle. This is remedied by the use of a small piston or diaphragm pump which, when actuated by the throttle linkage, forces a small amount of gasoline through a jet into the carburetor throat. This extra shot of fuel counteracts the transient lean condition on throttle tip-in. Most accelerator pumps are adjustable for volume and/or duration by some means. Eventually the seals around the moving parts of the pump wear such that pump output is reduced; this reduction of the accelerator pump shot causes stumbling under acceleration until the seals on the pump are renewed.
The accelerator pump is also used to "prime" the engine with fuel prior to a cold start. Excessive priming, like an improperly adjusted choke, can cause "flooding". This is when too much fuel and not enough air are present to support combustion. For this reason, most carburetors are equipped with an "unloader" mechanism: The accelerator is held at wide open throttle while the engine is cranked, the unloader holds the choke open and admits extra air, and eventually the excess fuel is cleared out and the engine starts.
Choke.
When the engine is cold, fuel vaporizes less readily and tends to condense on the walls of the intake manifold, starving the cylinders of fuel and making the engine difficult to start; thus, a "richer mixture" (more fuel to air) is required to start and run the engine until it warms up. A richer mixture is also easier to ignite.
To provide the extra fuel, a "choke" is typically used; this is a device that restricts the flow of air at the entrance to the carburetor, before the Venturi. With this restriction in place, extra vacuum is developed in the carburetor barrel, which pulls extra fuel through the main metering system to supplement the fuel being pulled from the idle and off-idle circuits. This provides the rich mixture required to sustain operation at low engine temperatures.
In addition, the choke can be connected to a cam (the "fast idle cam") or other such device which prevents the throttle plate from closing fully while the choke is in operation. This causes the engine to idle at a higher speed. Fast idle serves as a way to help the engine warm up quickly, and give a more stable idle while cold by increasing airflow throughout the intake system which helps to better atomize the cold fuel.
In many carbureted cars, the choke is controlled by a cable connected to a pull-knob on the dashboard operated by the driver. In some carbureted cars it is automatically controlled by a thermostat employing a bimetallic spring, which is exposed to engine heat, or to an electric heating element. This heat may be transferred to the choke thermostat via simple convection, via engine coolant, or via air heated by the exhaust. More recent designs use the engine heat only indirectly: A sensor detects engine heat and varies electrical current to a small heating element, which acts upon the bimetallic spring to control its tension, thereby controlling the choke. A "choke unloader" is a linkage arrangement that forces the choke open against its spring when the vehicle's accelerator is moved to the end of its travel. This provision allows a "flooded" engine to be cleared out so that it will start.
Some carburetors do not have a choke but instead use a mixture enrichment circuit, or "enrichment". Typically used on small engines, notably motorcycles, enrichments work by opening a secondary fuel circuit below the throttle valves. This circuit works exactly like the idle circuit, and when engaged it simply supplies extra fuel when the throttle is closed.
Classic British motorcycles, with side-draft slide throttle carburetors, used another type of "cold start device", called a "tickler". This is simply a spring-loaded rod that, when depressed, manually pushes the float down and allows excess fuel to fill the float bowl and flood the intake tract. If the "tickler" is held down too long it also floods the outside of the carburetor and the crankcase below, and is therefore a fire hazard.
Other elements.
The interactions between each circuit may also be affected by various mechanical or air pressure connections and also by temperature sensitive and electrical components. These are introduced for reasons such as response, fuel efficiency or automobile emissions control. Various air bleeds (often chosen from a precisely calibrated range, similarly to the jets) allow air into various portions of the fuel passages to enhance fuel delivery and vaporization. Extra refinements may be included in the carburetor/manifold combination, such as some form of heating to aid fuel vaporization such as an early fuel evaporator.
Fuel supply.
Float chamber.
To ensure a ready mixture, the carburetor has a "float chamber" (or "bowl") that contains a quantity of fuel at near-atmospheric pressure, ready for use. This reservoir is constantly replenished with fuel supplied by a fuel pump. The correct fuel level in the bowl is maintained by means of a float controlling an inlet valve, in a manner very similar to that employed in a cistern (e.g. a toilet tank). As fuel is used up, the float drops, opening the inlet valve and admitting fuel. As the fuel level rises, the float rises and closes the inlet valve. The level of fuel maintained in the float bowl can usually be adjusted, whether by a setscrew or by something crude such as bending the arm to which the float is connected. This is usually a critical adjustment, and the proper adjustment is indicated by lines inscribed into a window on the float bowl, or a measurement of how far the float hangs below the top of the carburetor when disassembled, or similar. Floats can be made of different materials, such as sheet brass soldered into a hollow shape, or of plastic; hollow floats can spring small leaks and plastic floats can eventually become porous and lose their flotation; in either case the float will fail to float, fuel level will be too high, and the engine will not run unless the float is replaced. The valve itself becomes worn on its sides by its motion in its "seat" and will eventually try to close at an angle, and thus fails to shut off the fuel completely; again, this will cause excessive fuel flow and poor engine operation. Conversely, as the fuel evaporates from the float bowl, it leaves sediment, residue, and varnishes behind, which clog the passages and can interfere with the float operation. This is particularly a problem in automobiles operated for only part of the year and left to stand with full float chambers for months at a time; commercial fuel stabilizer additives are available that reduce this problem.
The fuel stored in the chamber (bowl) can be a problem in hot climates. If the engine is shut off while hot, the temperature of the fuel will increase, sometimes boiling ("percolation"). This can result in flooding and difficult or impossible restarts while the engine is still warm, a phenomenon known as "heat soak". Heat deflectors and insulating gaskets attempt to minimize this effect. The Carter Thermo-Quad carburetor has float chambers manufactured of insulating plastic (phenolic), said to keep the fuel 20 degrees Fahrenheit (11 degrees Celsius) cooler.
Usually, special vent tubes allow atmospheric pressure to be maintained in the float chamber as the fuel level changes; these tubes usually extend into the carburetor throat. Placement of these vent tubes is critical to prevent fuel from sloshing out of them into the carburetor, and sometimes they are modified with longer tubing. Note that this leaves the fuel at atmospheric pressure, and therefore it cannot travel into a throat which has been pressurized by a supercharger mounted upstream; in such cases, the entire carburetor must be contained in an airtight pressurized box to operate. This is not necessary in installations where the carburetor is mounted upstream of the supercharger, which is for this reason the more frequent system. However, this results in the supercharger being filled with compressed fuel/air mixture, with a strong tendency to explode should the engine backfire; this type of explosion is frequently seen in drag races, which for safety reasons now incorporate pressure releasing blow-off plates on the intake manifold, breakaway bolts holding the supercharger to the manifold, and shrapnel-catching ballistic nylon blankets surrounding the superchargers.
Diaphragm chamber.
If the engine must be operated in any orientation (for example a chain saw or a model airplane), a float chamber is not suitable. Instead, a diaphragm chamber is used. A flexible diaphragm forms one side of the fuel chamber and is arranged so that as fuel is drawn out into the engine, the diaphragm is forced inward by ambient air pressure. The diaphragm is connected to the needle valve and as it moves inward it opens the needle valve to admit more fuel, thus replenishing the fuel as it is consumed. As fuel is replenished the diaphragm moves out due to fuel pressure and a small spring, closing the needle valve. A balanced state is reached which creates a steady fuel reservoir level, which remains constant in any orientation.
Multiple carburetor barrels.
While basic carburetors have only one Venturi, many carburetors have more than one Venturi, or "barrel". Two barrel and four barrel configurations are commonly used to accommodate the higher air flow rate with large engine displacement. Multi-barrel carburetors can have non-identical primary and secondary barrel(s) of different sizes and calibrated to deliver different air/fuel mixtures; they can be actuated by the linkage or by engine vacuum in "progressive" fashion, so that the secondary barrels do not begin to open until the primaries are almost completely open. This is a desirable characteristic which maximizes airflow through the primary barrel(s) at most engine speeds, thereby maximizing the pressure "signal" from the Venturis, but reduces the restriction in airflow at high speeds by adding cross-sectional area for greater airflow. These advantages may not be important in high-performance applications where part throttle operation is irrelevant, and the primaries and secondaries may all open at once, for simplicity and reliability; also, V-configuration engines, with two cylinder banks fed by a single carburetor, may be configured with two identical barrels, each supplying one cylinder bank. In the widely seen V8 and 4-barrel carburetor combination, there are often two primary and two secondary barrels.
The "spread-bore" four-barrel carburetor, first released by Rochester in the 1965 model year as the "Quadrajet" has a much greater "spread" between the sizes of the primary and secondary throttle bores. The primaries in such a carburetor are quite small relative to conventional four-barrel practice, while the secondaries are quite large. The small primaries aid low-speed fuel economy and driveability, while the large secondaries permit maximum performance when it is called for. To tailor airflow through the secondary Venturis, each of the secondary throats has an air valve at the top. This is configured much like a choke plate, and is lightly spring-loaded into the closed position. The air valve opens progressively in response to engine speed and throttle opening, gradually allowing more air to flow through the secondary side of the carburetor. Typically, the air valve is linked to metering rods which are raised as the air valve opens, thereby adjusting secondary fuel flow.
Multiple carburetors can be mounted on a single engine, often with progressive linkages; two four-barrel carburetors (often referred to as "dual-quads") were frequently seen on high performance American V8s, and multiple two barrel carburetors are often now seen on very high performance engines. Large numbers of small carburetors have also been used (see photo), though this configuration can limit the maximum air flow through the engine due to the lack of a common plenum; with individual intake tracts, not all cylinders are drawing air at once as the engine's crankshaft rotates.
Carburetor adjustment.
The fuel and air mixture is too "rich" when it has an excess of fuel, and too "lean" when there is not enough. The mixture is adjusted by one or more needle valves on an automotive carburetor, or a pilot-operated lever on piston-engined aircraft (since the mixture changes with air density and therefore altitude). Independent of air density the (stoichiometric) air to gasoline ratio is 14.7:1, meaning that for each mass unit of gasoline, 14.7 mass units of air are required. There are different stoichiometric ratios for other types of fuel.
Ways to check carburetor mixture adjustment include: measuring the carbon monoxide, hydrocarbon, and oxygen content of the exhaust using a gas analyzer, or directly viewing the color of the flame in the combustion chamber through a special glass-bodied spark plug sold under the name "Colortune"; the flame color of stoichiometric burning is described as a "Bunsen blue", turning to yellow if the mixture is rich and whitish-blue if too lean. Another method, widely used in aviation, is to measure the exhaust gas temperature, which is close to maximum for an optimally adjusted mixture and drops off steeply when the mixture is either too rich or too lean.
The mixture can also be judged by removing and scrutinizing the spark plugs. black, dry, sooty plugs indicate a mixture too rich; white or light gray plugs indicate a lean mixture. A proper mixture is indicated by brownish-gray plugs.
On high-performance two-stroke engines, the fuel mixture can also be judged by observing piston wash. Piston wash is the color and amount of carbon buildup on the top (dome) of the piston. Lean engines will have a piston dome covered in black carbon, and rich engines will have a clean piston dome that appears new and free of carbon buildup. This is often the opposite of intuition. Commonly, an ideal mixture will be somewhere in-between the two, with clean dome areas near the transfer ports but some carbon in the center of the dome.
When tuning two-strokes It is important to operate the engine at the rpm and throttle input that it will most often be operated at. This will typically be wide-open or close to wide-open throttle. Lower RPM and idle can operate rich/lean and sway readings, due to the design of carburetors to operate well at high air-speed through the Venturi and sacrifice low air-speed performance.
Where multiple carburetors are used the mechanical linkage of their throttles must be properly synchronized for smooth engine running and consistent fuel/air mixtures to each cylinder.
Feedback carburetors.
In the 1980s, many American-market vehicles used "feedback" carburetors that dynamically adjusted the fuel/air mixture in response to signals from an exhaust gas oxygen sensor to provide a stoichiometric ratio to enable the optimal function of the catalytic converter. Feedback carburetors were mainly used because they were less expensive than fuel injection systems; they worked well enough to meet 1980s emissions requirements and were based on existing carburetor designs. Frequently, feedback carburetors were used in lower trim versions of a car (whereas higher trim versions were equipped with fuel injection). However, their complexity compared to both non-feedback carburetors and to fuel injection made them problematic and difficult to service. Eventually falling hardware prices and tighter emissions standards caused fuel injection to supplant carburetors in new-vehicle production.
Catalytic carburetors.
A catalytic carburetor mixes fuel vapor with water and air in the presence of heated catalysts such as nickel or platinum. This is generally reported as a 1940s-era product that would allow kerosene to power a gasoline engine (requiring lighter hydrocarbons). However reports are inconsistent; commonly they are included in descriptions of "200 MPG carburetors" intended for gasoline use. There seems to be some confusion with some older types of fuel vapor carburetors (see vaporizors below). There is also very rarely any useful reference to real-world devices. Poorly referenced material on the topic should be viewed with suspicion.
Vaporizers.
Internal combustion engines can be configured to run on many kinds of fuel, including gasoline, kerosene, tractor vaporizing oil (TVO), vegetable oil, diesel fuel, biodiesel, ethanol fuel (alcohol), and others. Multifuel engines, such as petrol-paraffin engines, can benefit from an initial vaporization of the fuel when they are running less volatile fuels. For this purpose, a vaporizer (or vaporiser) is placed in the intake system. The vaporizer uses heat from the exhaust manifold to vaporize the fuel. For example, the original Fordson tractor and various subsequent Fordson models had vaporizers. When Henry Ford & Son Inc designed the original Fordson (1916), the vaporizer was used to provide for kerosene operation. When TVO became common in various countries (including the United Kingdom and Australia) in the 1940s and 1950s, the standard vaporizers on Fordson models were equally useful for TVO. Widespread adoption of diesel engines in tractors made the use of tractor vaporizing oil obsolete.
Further reading.
 "Carburetor" Antoine Prosper Plaut]

</doc>
<doc id="49214" url="http://en.wikipedia.org/wiki?curid=49214" title="Clef">
Clef

A clef (from French: "clef" “key”) is a musical symbol used to indicate the pitch of written notes. Placed on one of the lines at the beginning of the stave, it indicates the name and pitch of the notes on that line. This line serves as a reference point by which the names of the notes on any other line or space of the stave may be determined. Only one clef that references a note in a space rather than on a line has ever been used.
There are three types of clef used in modern music notation: "F", "C", and "G". Each type of clef assigns a different reference note to the line (and in rare cases, the space) on which it is placed.
Once one of these clefs has been placed on one of the lines of the stave, the other lines and spaces can be read in relation to it.
The use of three different clefs makes it possible to write music for all instruments and voices, even though they may have very different tessituras (that is, even though some sound much higher or lower than others). This would be difficult to do with only one clef, since the modern stave has only five lines, and the number of pitches that can be represented on the stave, even with ledger lines, is not nearly equal to the number of notes the orchestra can produce. The use of different clefs for different instruments and voices allows each part to be written comfortably on the stave with a minimum of ledger lines. To this end, the G-clef is used for high parts, the C-clef for middle parts, and the F-clef for low parts—with the important exception of transposing parts, which are written at a different pitch than they sound, often even in a different octave.
Placement on the stave.
In order to facilitate writing for different tessituras, any of the clefs may theoretically be placed on any of the lines of the stave. The further down on the stave a clef is placed, the higher the tessitura it is for; conversely, the higher up the clef, the lower the tessitura.
Since there are five lines on the stave, and three clefs, it might seem that there would be fifteen possible clefs. Six of these, however, are redundant clefs (for example, a G-clef on the third line would be exactly the same as a C-clef on the first line). That leaves nine possible "distinct" clefs, all of which have been used historically: the G-clef on the two bottom lines, the F-clef on the three top lines, and the C-clef on any line of the stave except the topmost, earning the name of "movable C-clef". (The C-clef on the topmost line is redundant because it is exactly equivalent to the F-clef on the third line; both options have been used.)
Each of these clefs has a different name based on the tessitura for which it is best suited.
In modern music, only four clefs are used regularly: the treble clef, the bass clef, the alto clef, and the tenor clef. Of these, the treble and bass clefs are by far the most common.
Individual clefs.
Here follows a complete list of the clefs, along with a list of instruments and voice parts notated with them. Each clef is shown in its proper position on the stave, followed by its reference note.
An obelisk (†) after the name of a clef indicates that that clef is no longer in common use.
G-clefs.
Treble clef.
When the G-clef is placed on the second line of the stave, it is called the treble clef. This is the most common clef used today, and the only G-clef still in use. For this reason, the terms G-clef and treble clef are often seen as synonymous. The treble clef was historically used to mark a treble, or pre-pubescent, voice part.
Among the instruments that use treble clef are the violin, flute, oboe, bagpipe, English horn, all clarinets, all saxophones, horn, trumpet, cornet, vibraphone, xylophone, mandolin, recorder; it is also used for euphonium, baritone horn, and guitar (which sound an octave lower). Treble clef is the upper stave of the grand stave used for harp and keyboard instruments. It is also sometimes used, along with tenor clef, for the highest notes played by bass-clef instruments such as the cello, double bass (which sounds an octave lower), bassoon, and trombone. The viola also sometimes uses treble clef for very high notes. Treble clef is used for the soprano, mezzo-soprano, alto, contralto and tenor voices. The tenor voice sounds an octave lower, and is often written using an octave clef (see below) or double-treble clef.
French violin clef†.
When the G-clef is placed on the first line of the stave, it is called the French clef or French violin clef. It is identical to the bass clef transposed up 2 octaves.
F-clefs.
Bass clef.
When the F-clef is placed on the fourth line, it is called the bass clef. This is the only F-clef used today, so that the terms "F-clef" and "bass clef" are often regarded as synonymous.
This clef is used for the cello, euphonium, double bass, bass guitar, bassoon, contrabassoon, trombone, baritone horn, tuba, and timpani. It is also used for the lowest notes of the horn, and for the baritone and bass voices. Tenor voice is notated in bass clef when the tenor and bass are written on the same stave. Bass clef is the bottom clef in the grand stave for harp and keyboard instruments. The contrabassoon, double bass, and electric bass sound an octave lower than the written pitch; no notation is usually made of this fact, but some composers/publishers will place an "8" beneath the clef for these instruments on the conductor's full score to differentiate from instruments that naturally sound within the clef (see "Octave clefs" below).
Baritone clef†.
When the F-clef is placed on the third line, it is called the baritone clef.
This clef was used for the left hand of keyboard music (particularly in France; see Bauyn manuscript) as well as the baritone part in vocal music.
The baritone clef has less common variant as a C clef placed on the 5th line which is exactly equivalent (see below).
Sub-bass clef†.
When the F-clef is placed on the fifth line, it is called the sub-bass clef. It is identical to the treble clef transposed down 2 octaves.
This clef was used by Ockeghem and Heinrich Schütz to write low bass parts, making a late appearance in Bach's Musical Offering.
C-clefs.
Alto clef.
When the C-clef is placed on the third line of the stave, it is called the alto clef.
This clef (sometimes called the viola clef) is currently used for the viola, the viola da gamba, the alto trombone, and the mandola. It is also associated with the countertenor voice and therefore called the counter-tenor (or countertenor) clef, A vestige of this survives in Sergei Prokofiev's use of the clef for the English horn, as in his symphonies. It occasionally turns up in keyboard music to the present day (Brahms's Organ chorales, John Cage's "Dream" for piano)."
Tenor clef.
When the C-clef is placed on the fourth line of the stave, it is called the tenor clef.
This clef is used for the upper ranges of the bassoon, cello, euphonium, double bass, and trombone. These instruments use bass clef for their low- to mid-ranges; treble clef is also used for their upper extremes. When used for the double bass, the sound is an octave lower than the written pitch. The tenor violin parts were also written in this clef (see e.g. Giovanni Battista Vitali's Op. 11). Formerly, it was used by the tenor part in vocal music but its use has been largely supplanted either with an octave version of the treble clef when written alone or the bass clef when combined on one stave with the bass part.
Baritone clef†.
When the C-clef is placed on the 5th line of the stave it is called the baritone clef. It is precisely equivalent to the other more common form of the baritone clef, an F clef placed on the 3rd line (see above).
Mezzo-soprano clef†.
When the C-clef is placed on the second line of the stave, it is called the mezzo-soprano clef.
Soprano clef†.
When the C-clef is placed on the first line of the stave, it is called the soprano clef.
This clef was used for the right hand of keyboard music (particularly in France; see Bauyn manuscript) as well as in vocal music for sopranos.
Other clefs.
Octave clefs.
Starting in the 18th century treble clef has been used for transposing instruments that sound an octave lower, such as the guitar; it has also been used for the tenor voice. To avoid ambiguity, modified clefs are sometimes used, especially in the context of choral writing; of those shown, the C clef on the third space, easily confused with the tenor clef, is the rarest.
This is most often found in tenor parts in SATB settings, in which a treble clef is written with an eight below it, indicating that the pitches sound an octave below the written value. As the true tenor clef has generally fallen into disuse in vocal writings, this "octave-dropped" treble clef is often called the tenor clef. The same clef is sometimes used for the octave mandolin. In some scores, the same concept is construed by using a double clef—two G-clefs overlapping one another.
At the other end of the spectrum, treble clefs with an "8" positioned above the clef may be used in piccolo, penny whistle, soprano recorder, and other high woodwind parts.
The F clef can also be notated with an octave marker. The F clef notated to sound an octave lower is used for contrabass instruments such as the double bass and contrabassoon. The F clef notated to sound an octave higher is used for bass recorder. However, both of these are extremely rare (and in fact the countertenor clef is largely intended to be humorous as with the works of P.D.Q. Bach). In Italian scores up to Gioachino Rossini's Overture to "William Tell", the English horn was written in bass clef an octave lower than sounding. The unmodified bass clef is so common that performers of instruments and voice parts whose ranges lie below the stave simply learn the number of ledger lines for each note through common use, and if a line's true notes lie significantly above the bass clef the composer or publisher will often simply write the part in either the true treble clef or notated an octave down.
Use of octave-marked clefs appears to have increased as computers have become more important in musical transcription. Performers will normally know the right octave to use with or without the octave marking. However, the appropriate use of octave marking ensures that music files (such as MIDI files) generate tones in their proper octaves.
Neutral clef.
The "neutral" or "percussion" clef is not a clef in the same sense that the F, C, and G clefs are. It is simply a convention that indicates that the lines and spaces of the stave are each assigned to a percussion instrument with no precise pitch. With the exception of some common drum-kit and marching percussion layouts, the keying of lines and spaces to instruments is not standardized, so a legend or indications above the stave are necessary to indicate what is to be played. Percussion instruments with identifiable pitches do not use the neutral clef, and timpani (notated in bass clef) and mallet percussion (noted in treble clef or on a grand stave) are usually notated on different staves than unpitched percussion.
Staves with a neutral clef do not always have five lines. Commonly, percussion staves only have one line, although other configurations can be used.
The neutral clef is sometimes used when non-percussion instruments play non-pitched extended techniques, such as hitting the body of a violin, violoncello or acoustic guitar, or when a vocal choir is instructed to clap, stomp, or snap, but more often the rhythms are written with X marks in the instrument's normal stave with a comment placed above as to the appropriate rhythmic action.
Tablature.
For guitars and other fretted instruments, it is possible to notate tablature in place of ordinary notes. In this case, a TAB-sign is often written instead of a clef. The number of lines of the stave is not necessarily five: one line is used for each string of the instrument (so, for standard six-stringed guitars, six lines would be used, four lines for the traditional bass guitar). Numbers on the lines show on which fret the string should be played. This Tab-sign, like the Percussion clef, is not a clef in the true sense, but rather a symbol employed instead of a clef.
History.
Originally, instead of a special clef symbol, the reference line of the stave was simply labeled with the name of the note it was intended to bear: F and C and, more rarely, "G". These were the most often-used 'clefs', or "litteræ-clavis" (key-letters), in Gregorian chant notation. Over time the shapes of these letters became stylized, leading to their current versions.
Many other clefs were used, particularly in the early period of chant notation, including most of the notes from the low Γ ("gamma", the note written today on the bottom line of the bass clef) up to the G above middle C, written with a small letter "g", and including two forms of lowercase "b" (for the note just below middle C): round for B♭, and square for B♮. In order of frequency of use, these clefs were: "F", "c", "f", "C", "D", "a", "g", "e", Γ, "B", and the round/square "b".
In the polyphonic period up to 1600, unusual clefs were used occasionally for parts with extremely high or low written tessituras. For very low bass parts, the Γ clef is found on the middle, fourth, or fifth lines of the stave (e.g., in Pierre de La Rue’s Requiem and in a mid-16th-century dance book published by the Hessen brothers); for very high parts, the high-D clef ("d"), and the even higher "ff" clef (e.g., in the "Mulliner Book") were used to represent the notes written on the fourth and top lines of the treble clef, respectively.
Varying shapes of different clefs persisted until very recent times. The F-clef was, until as late as the 1980s in some cases (such as hymnals), written like this:
In printed music from the 16th and 17th centuries, the C clef often assumed a ladder-like form, in which the two horizontal rungs surround the stave line indicated as C:
The C-clef was formerly written in a more angular way, sometimes still used, or an even more simplified "K"-shape, when writing the clef by hand. 
In modern Gregorian chant notation, the C clef is written (on a four-line stave) in the form and the F clef as .
The flourish at the top of the G-clef probably derives from a cursive "S" for "sol", the name for "G" in solfege.
C clefs (along with G, F, Gamma, D, and A clefs) were formerly used to notate vocal music. Nominally, the soprano voice parts were written in first- or second-line C clef ("soprano clef" or "mezzo-soprano clef") or second-line G clef ("treble clef"), the alto or tenor voices in third-line C clef ("alto clef"), the tenor voice in fourth-line C clef ("tenor clef") and the bass voice in third- fourth- or fifth-line F clef ("baritone", "bass", or "sub-bass clef"). However, in practice transposition was applied to fit the range of the music to the available voices, so that almost any clef might be used by all voice types.
In more modern publications, four-part harmony on parallel staves is usually written more simply as:
This may be reduced to two staves, the soprano/alto stave with a treble clef, and tenor/bass stave marked with the bass clef.
Further uses.
Clef combinations played a role in the modal system toward the end of the 16th century, and it has been suggested certain clef combinations in the polyphonic music of 16th-century vocal polyphony are reserved for authentic (odd-numbered) modes, and others for plagal (even-numbered) modes, but the precise implications have been the subject of much scholarly debate.
Music can be transposed at sight if a different clef is mentally substituted for the written one. For example, to play an A-clarinet part, a B♭-clarinet player may mentally substitute tenor clef for the written treble clef. Concert-pitch music in bass clef can be read on an E♭ instrument as if it were in treble clef. (Notes will not always sound in the correct octave). The written key signature must always be adjusted to the correct key for the instrument being played.

</doc>
<doc id="49220" url="http://en.wikipedia.org/wiki?curid=49220" title="C (musical note)">
C (musical note)

In terms of musical pitch, C or Do is the first note of the fixed-Do solfège scale. Its enharmonic is B♯ (B-sharp), which is by definition a diatonic semitone below C♯.
Middle C.
With a frequency around 261.6 Hz, Middle C is designated C4 in scientific pitch notation because of the note's position as the fourth C key on a standard 88-key piano keyboard. 
Another system known as scientific pitch assigns a frequency of 256 Hz but, while numerically convenient, this is not used by orchestras. Other note-octave systems, including those used by some makers of digital music keyboards, may refer to Middle C differently. In MIDI, Middle C is note number 60.
The C4 designation is the most commonly recognized in auditory science, and in musical studies it is often used in place of the Helmholtz designation c'. 
While the expression "Middle C" is generally clear across instruments and clefs, some musicians naturally use the term to refer to the C note in the middle of their specific instrument's range. C4 may be called "Low C" by someone playing a Western concert flute, which has a higher and narrower playing range than the piano, while C5 (523.251 Hz) would be Middle C. This technically inaccurate practice has led some pedagogues to encourage standardizing on C4 as the definitive Middle C in instructional materials across all instruments.
In vocal music, the term Soprano C, sometimes called "High C" or "Top C," is the C two octaves above Middle C. It is so named because it is considered the defining note of the soprano voice type. It is C6 in scientific pitch notation (1046.502 Hz) and c''' in Helmholtz notation. The term Tenor C is sometimes used in vocal music to refer to C5, as it is the highest required note in the standard tenor repertoire. The term Low C is sometimes used in vocal music to refer to C2 because this is considered the divide between true basses and bass-baritones: a "basso" can sing this note easily while other male voices, including bass-baritones, cannot.
In organ music, the term Tenor C can refer to an organ builder's term for small C or C3 (130.813 Hz), the note one octave below Middle C. In stoplists it usually means that a rank is not full compass, omitting the bottom octave. 
For the frequency of each note on a standard piano, see piano key frequencies.
B sharp.
Twelve just perfect fifths (B♯) and seven octaves do not align as in equal temperament.
This difference, 23.46 cents (531441/524288), is known as the Pythagorean comma.

</doc>
<doc id="49229" url="http://en.wikipedia.org/wiki?curid=49229" title="Staff (music)">
Staff (music)

In Western musical notation, the staff, or stave, is a set of five horizontal lines and four spaces that each represent a different musical pitch—or, in the case of a percussion staff, different percussion instruments. Appropriate music symbols, depending upon the intended effect, are placed on the staff according to their corresponding pitch or function. Musical notes are placed by pitch, percussion notes are placed by instrument, and rests and other symbols are placed by convention.
The absolute pitch of each line of a non-percussive staff is indicated by the placement of a clef symbol at the appropriate vertical position on the left-hand side of the staff (possibly modified by conventions for specific instruments). For example, the treble clef, also known as the G clef, is placed upon the second line (counting upwards), fixing that line as the pitch first G above 'middle C'.
The lines and spaces are numbered from bottom to top; the bottom line is the "first line" and the top line is the "fifth line".
The musical staff is analogous to a mathematical graph of pitch with respect to time. Pitches of notes are given by their vertical position on the staff and notes to the left are played before notes to the right. Unlike a graph, however, the number of semitones represented by a vertical step from a line to an adjacent space depends on the key, and the exact timing of the beginning of each note is not directly proportional to its horizontal position; rather, exact timing is encoded by the musical symbol chosen for each note in addition to the tempo. 
A time signature to the right of the clef indicates the relationship between timing counts and note symbols, while bar lines group notes on the staff into measures.
Staff positions.
The vertical position of the notehead on the staff indicates which note to play: higher-pitched notes are marked higher on the staff. The notehead can be placed with the center of its notehead intersecting a line ("on a line"), or in between the lines touching the lines above and below ("in a space"). Notes outside the range of the staff are placed on or between ledger lines—lines the width of the note they need to hold—added above or below the staff.
Exactly which staff positions represent which notes is determined by a clef placed at the beginning of the staff. The clef identifies a particular line as a specific note, and all other notes are determined relative to that line. For example, the treble clef puts the G above middle C on the second line. The interval between adjacent staff positions is one step in the diatonic scale. Once fixed by a clef, the notes represented by the positions on the staff can be modified by the key signature, or by accidentals on individual notes. A clefless staff may be used to represent a set of percussion sounds; each line typically represents a different instrument.
Ensemble staves.
A single vertical line drawn to the left of multiple staves creates a system, indicating that the music on all the staves is to be played simultaneously. A bracket is an additional straight line joining staves, to show groupings of instruments that function as a unit, such as the string section of an orchestra. A brace is used to join multiple staves that represent a single instrument, such as a piano, organ, harp, or marimba. Sometimes, a second bracket is used to show instruments grouped in pairs, such as the first and second oboes, or the first and second violins in an orchestra. In some cases, a brace is used for this purpose instead of a bracket.
Four-part SATB vocal settings, especially in hymnals, use a "divisi" notation on a two-staff system with soprano and alto voices sharing the upper staff, and tenor and bass voices on the lower staff.
Grand staff.
When music on two staves is joined by a brace, or is intended to be played at once by a single performer (usually a keyboard instrument or the harp), a great stave (BrE) or grand staff (AmE) is created.
Typically, the upper staff uses a treble clef and the lower staff has a bass clef. In this instance, middle C is centered between the two staves, and it can be written on the first ledger line below the upper staff or the first ledger line above the lower staff. Very rarely, a centered line with a small alto clef is written, and usually used to indicate that B,C, or D notes on the line can be played with either hand (ledger lines are not used from a center alto as this creates confusion). 
When playing the piano or harp, the upper staff is normally played with the right hand and the lower staff with the left hand. In music intended for the organ, a grand staff comprises three staves, one for each hand on the manuals and one for the feet on the pedalboard.
History.
Early Western medieval notation was written with neumes, which didn't specify exact pitches but only the shape of the melodies, i.e. indicating when the musical line went up or down; presumably these were intended as mnemonics for melodies which had been taught by rote. 
During the 9th through 11th centuries a number of systems were developed to specify pitch more precisely, including diastematic neumes whose height on the page corresponded with their absolute pitch level (Longobardian and Beneventan manuscripts from Italy show this technique around AD 1000). Digraphic notation, using letter names similar to modern note names in conjunction with the neumes, made a brief appearance in a few manuscripts, but a number of manuscripts used one or more horizontal lines to indicate particular pitches. 
The treatise Musica enchiriadis (AD 900) uses Daseian notation for indicating specific pitches, but the modern use of staff lines is attributed to Guido d'Arezzo (AD 990-1050), whose four-line staff is still used (though without the red and yellow coloring he recommended) in Gregorian chant publications today. Five-line staves appeared in Italy in the 13th century, and staves with four, five, and six lines were used as late as 1600. 

</doc>
<doc id="49233" url="http://en.wikipedia.org/wiki?curid=49233" title="Robert Goddard (disambiguation)">
Robert Goddard (disambiguation)

Robert H. Goddard (1882–1945) was an American scientist and pioneer of modern rocketry.
Robert Goddard may also refer to:

</doc>
<doc id="49234" url="http://en.wikipedia.org/wiki?curid=49234" title="Chromatic scale">
Chromatic scale

The chromatic scale is a musical scale with twelve pitches, each a semitone above or below another. On a modern piano or other equal-tempered instrument, all the semitones are the same size (100 cents). In other words, the notes of an equal-tempered chromatic scale are equally spaced. An equal-tempered chromatic scale is a nondiatonic scale having no tonic because of the symmetry of its equally spaced notes.
The most common conception of the chromatic scale before the 13th century was the Pythagorean chromatic scale. Due to a different tuning technique, the twelve semitones in this scale have two slightly different sizes. Thus, the scale is not perfectly symmetric. Many other tuning systems, developed in the ensuing centuries, share a similar asymmetry. Equally spaced pitches are provided only by equal temperament tuning systems, which are widely used in contemporary music.
The term "chromatic" derives from the Greek word "chroma", meaning "color". Chromatic notes are traditionally understood as harmonically inessential embellishments, shadings, or inflections of "diatonic" notes.
Notation.
The chromatic scale may be notated in a variety of ways.
Ascending and descending:
The chromatic scale has no set spelling agreed upon by all. Its spelling is, however, often dependent upon major or minor key signatures and whether the scale is ascending or descending. The images above, therefore, are only examples of chromatic scale notations. As an abstract theoretical entity (that is, outside a particular musical context), the chromatic scale is usually notated such that no scale degree is used more than twice in succession (for instance G flat - G natural - G sharp).
Total chromatic.
The "total chromatic" (or "aggregate") is the set of all twelve pitch classes. An "array" is a succession of aggregates. See also: Tone row.

</doc>
<doc id="49239" url="http://en.wikipedia.org/wiki?curid=49239" title="St. Hedwig's Cathedral">
St. Hedwig's Cathedral

St. Hedwig's Cathedral (German: "Sankt-Hedwigs-Kathedrale") is a Roman Catholic cathedral on the Bebelplatz in Berlin, Germany. It is the seat of the archbishop of Berlin.
History and Architecture.
It was built in the 18th century as the first Catholic church in Prussia after the Protestant Reformation by permission of King Frederick II. The intention of Frederick was to offer the numerous Catholic immigrants who had arrived in Berlin, especially those from Upper Silesia, a place of worship. The church was therefore dedicated to the patron of Silesia and Brandenburg, Saint Hedwig of Andechs. The building was designed by Georg Wenzeslaus von Knobelsdorff modeled after the Pantheon in Rome and construction started in 1747, interrupted and delayed several times due to economic problems. It was not opened until November 1, 1773 when the king's friend, Ignacy Krasicki, then Bishop of Warmia (later Archbishop of Gniezno), officiated at the cathedral's consecration.
After the Kristallnacht pogroms that took place over the night of 9–10 November 1938, Bernhard Lichtenberg, a canon of the cathedral chapter of St Hedwig since 1931, prayed publicly for Jews in the evening prayer following. Lichtenberg was later jailed by the Nazis and died on the way to the concentration camp at Dachau. In 1965 Lichtenberg's remains were transferred to the crypt at St. Hedwig's.
The cathedral burned down completely in 1943 during air raids on Berlin and was reconstructed from 1952 up to 1963.

</doc>
<doc id="49240" url="http://en.wikipedia.org/wiki?curid=49240" title="Patrice Lumumba">
Patrice Lumumba

Patrice Émery Lumumba (2 July 1925 – 17 January 1961) was a Congolese independence leader and the first democratically elected prime minister of the Congo. As founder and leader of the mainstream "Mouvement national congolais" (MNC) party, Lumumba played an important role in campaigning for independence from Belgium.
Within twelve weeks of Congolese independence in 1960, Lumumba's government was deposed in a coup during the Congo Crisis following his attempt to solicit support from the Soviet Union against Katangan secessionists. This led to growing differences with President Joseph Kasa-Vubu and chief-of-staff Joseph-Désiré Mobutu as well as foreign opposition from the United States and Belgium. Lumumba was subsequently imprisoned by state authorities under Mobutu and executed by firing squad under the command of the Katangan authorities. The United Nations, which he had asked to come to the Congo, did not intervene to save him. Belgium, the United States, and the United Kingdom have all been accused of involvement in Lumumba's death.
Early life and career.
Lumumba was born to a farmer, François Tolenga Otetshima, and his wife, Julienne Wamato Lomendja, in Onalua in the Katakokombe region of the Kasai province of the Belgian Congo. He was a member of the Tetela ethnic group and was born with the name Élias Okit'Asombo. His original surname means "heir of the cursed" and is derived from the Tetela words "okitá/okitɔ́" ('heir, successor') and "asombó" ('cursed or bewitched people who will die quickly'). He had three brothers (Charles Lokolonga, Émile Kalema, and Louis Onema Pene Lumumba) and one half-brother (Tolenga Jean). Raised in a Catholic family, he was educated at a Protestant primary school, a Catholic missionary school, and finally the government post office training school, passing the one-year course with distinction. Lumumba spoke Tetela, French, Lingala, Swahili, and Tshiluba.
He worked in Léopoldville (now Kinshasa) and Stanleyville (now Kisangani) as a postal clerk and as a travelling beer salesman. In 1951, he married Pauline Opangu. In 1955, Lumumba became regional head of the "Cercles" of Stanleyville and joined the Liberal Party of Belgium, where he worked on editing and distributing party literature. After traveling on a three-week study tour in Belgium, he was arrested in 1955 on charges of embezzlement. His two-year sentence was commuted to twelve months after it was confirmed by Belgian lawyer Jules Chrome that Lumumba had returned the funds, and he was released in July 1956. 
Leader of MNC.
After his release, he helped found the broad-based Mouvement national congolais (MNC) in 1958, later becoming the organization's president. Lumumba and his team represented the MNC at the All-African Peoples' Conference in Accra, Ghana, in December 1958. At this international conference, hosted by Pan-African President Kwame Nkrumah of Ghana, Lumumba further solidified his Pan-Africanist beliefs.
In late October 1959, Lumumba, as leader of the organization, was arrested for inciting an anti-colonial riot in Stanleyville where thirty people were killed; he was sentenced to 69 months in prison. The trial's start date of 18 January 1960 was also the first day of a round-table conference in Brussels to finalize the future of the Congo. Despite Lumumba's imprisonment at the time, the MNC won a convincing majority in the December local elections in the Congo. As a result of strong pressure from delegates upset with Lumumba's trial, he was released and allowed to attend the Brussels conference.
Independence, and election as Prime Minister.
The conference culminated on 27 January with a declaration of Congolese independence, setting 30 June 1960, as the independence date with national elections from 11–25 May 1960. Lumumba and the MNC won this election and the right to form a government, with the announcement on 23 June 1960 of 34-year-old Lumumba as Congo's first prime minister and Joseph Kasa-Vubu as its president. In accordance with the constitution, on 24 June the new government passed a vote of confidence and was ratified by the Congolese Chamber and Senate.
Independence Day was celebrated on 30 June in a ceremony attended by many dignitaries including Belgian King Baudouin and the foreign press. Baudouin's speech praised developments under colonialism, his reference to the "genius" of his great-granduncle Léopold II of Belgium glossing over atrocities committed during the Congo Free State. The King continued, "Don't compromise the future with hasty reforms, and don't replace the structures that Belgium hands over to you until you are sure you can do better... Don't be afraid to come to us. We will remain by your side, give you advice." While President Kasa-Vubu thanked the King, Lumumba, who was not scheduled to speak, delivered an impromptu speech which reminded the audience that the independence of the Congo was not granted magnanimously by Belgium:
For this independence of the Congo, even as it is celebrated today with Belgium, a friendly country with whom we deal as equal to equal, no Congolese worthy of the name will ever be able to forget that it was by fighting that it has been won, a day-to-day fight, an ardent and idealistic fight, a fight in which we were spared neither privation nor suffering, and for which we gave our strength and our blood. We are proud of this struggle, of tears, of fire, and of blood, to the depths of our being, for it was a noble and just struggle, and indispensable to put an end to the humiliating slavery which was imposed upon us by force.
The King spoke of his father's great work in the country and asked its new leaders to measure up to their example. The speech of President Kasa-Vubu assured the King that they would try hard. Lumumba spoke of the suffering of the Congolese under Belgian colonialism, of “injustice, oppression and exploitation”. Neither the audience nor the King and his entourage were accustomed to hearing of the negatives that lay behind the pageantry and paternalism; it stirred the crowd while simultaneously humiliating and alienating the King. Lumumba was later harshly criticised for what many in the Western world—but virtually none in Africa—described as the inappropriate nature of his speech. Some media claimed at the time that he ended his speech by ad-libbing, "Nous ne sommes plus vos macaques!" (We are no longer your monkeys!)—referring to a common slur used against Africans by Belgians, however, these words are neither in his written text nor in radio tapes of his speech.
Actions as Prime Minister.
A few days after Congo gained its independence, Lumumba made the fateful decision to raise the pay of all government employees except for the army. Many units of the army also had strong objections toward the uniformly Belgian officers; General Émile Janssens, the army head, told them their lot would not change after independence, and they rebelled in protest. The rebellions quickly spread throughout the country, leading to a general breakdown in law and order. Although the trouble was highly localized, the country seemed to be overrun by gangs of soldiers and looters, causing a media sensation, particularly over Europeans fleeing the country.
The province of Katanga declared independence under regional premier Moïse Tshombe on 11 July 1960 with support from the Belgian government and mining companies such as Union Minière. Despite the arrival of UN troops, unrest continued. Since the United Nations refused to help suppress the rebellion in Katanga, Lumumba sought Soviet aid in the form of arms, food, medical supplies, trucks, and planes to help move troops to Katanga. Lumumba's decisive actions alarmed his colleagues and President Joseph Kasa-Vubu, who preferred a more moderate political approach.
Deposition.
In September, the President dismissed Lumumba from government. Lumumba immediately protested the legality of the President's actions. In retaliation, Lumumba declared Kasa-Vubu deposed and won a vote of confidence in the Senate, while the newly appointed prime minister failed to gain parliament's confidence. The country was torn by two political groups claiming legal power over the country.
On 14 September, a coup d’état organised by Colonel Joseph Mobutu incapacitated both Lumumba and Kasa-Vubu. Lumumba was placed under house arrest at the Prime Minister's residence, with UN troops positioned around the house. Nevertheless, Lumumba decided to rouse his supporters in Haut-Congo. Smuggled out of his residence at night, he escaped to Stanleyville, where his intention apparently was to set up his own government and army.
Pursued by troops loyal to Mobutu he was finally captured in Port Francqui on 1 December 1960 and flown to Léopoldville (now Kinshasa) in ropes, not handcuffs. Mobutu claimed Lumumba would be tried for inciting the army to rebellion and other crimes.
UN response.
United Nations Secretary General Dag Hammarskjöld made an appeal to Kasa-Vubu asking that Lumumba be treated according to due process of law. The USSR denounced Hammarskjöld and the Western powers as responsible for Lumumba's arrest and demanded his release.
The UN Security Council was called into session on 7 December 1960 to consider Soviet demands that the UN seek Lumumba's immediate release, the immediate restoration of Lumumba as head of the Congo government, the disarming of the forces of Mobutu, and the immediate evacuation of Belgians from the Congo. Hammarskjöld, answering Soviet attacks against his Congo operations, said that if the UN forces were withdrawn from the Congo "I fear everything will crumble."
The threat to the UN cause was intensified by the announcement of the withdrawal of their contingents by Yugoslavia, the United Arab Republic, Ceylon, Indonesia, Morocco, and Guinea. The pro-Lumumba resolution was defeated on 14 December 1960 by a vote of 8–2. On the same day, a Western resolution that would have given Hammarskjöld increased powers to deal with the Congo situation was vetoed by the Soviet Union.
Final days and execution.
Lumumba was sent first on 3 December, to Thysville military barracks Camp Hardy, 150 km (about 100 miles) from Léopoldville. However, when security and disciplinary breaches threatened his safety, it was decided that he should be transferred to the State of Katanga, which had recently declared independence from Congo.
Lumumba was forcibly restrained on the flight to Elizabethville (now Lubumbashi) on 17 January 1961. On arrival, he was conducted under arrest to Brouwez House where he was brutally beaten and tortured by Katangan and Belgian officers, while President Tshombe and his cabinet decided what to do with him.
Later that night, Lumumba was driven to an isolated spot where three firing squads had been assembled. The Belgian Commission (see below) has found that the execution was carried out by Katanga's authorities.
It reported that President Tshombe and two other ministers were present with four Belgian officers under the command of Katangan authorities. Lumumba and two ministers from his newly formed independent government (and who had also been tortured), Maurice Mpolo and Joseph Okito, were lined up against a tree and shot one at a time. The execution probably took place on 17 January 1961 between 21:40 and 21:43 according to the Belgian report. The Belgians and their counterparts wished to get rid of the bodies, which were dug up, dismembered, dissolved in sulphuric acid, with the bones ground and scattered.
Announcement of death.
No statement was released until three weeks later despite rumours that Lumumba was dead. His death was formally announced on Katangan radio, when it was alleged that he escaped and was killed by enraged villagers.
After the announcement of Lumumba's death, street protests were organized in several European countries; in Belgrade, capital of Yugoslavia, protesters sacked the Belgian embassy and confronted the police, and in London a crowd marched from Trafalgar Square to the Belgian embassy, where a letter of protest was delivered and where protesters clashed with police. A demonstration at the United Nations Security Council turned violent and spilled over into the streets of New York City.
Foreign involvement in his death.
According to "Democracy Now!", "Lumumba's pan-Africanism and his vision of a united Congo gained him many enemies. Both Belgium and the United States actively sought to have him killed. The CIA ordered his assassination but could not complete the job. Instead, the United States and Belgium covertly funneled cash and aid to rival politicians who seized power and arrested Lumumba."
Both Belgium and the US were clearly influenced in their unfavourable stance towards Lumumba by the Cold War. He seemed to gravitate around the Soviet Union, although according to the opinion of Sean Kelly this was not because he was a Communist, but the USSR was the only place he could find support for his country's effort to rid itself of colonial rule, although this opinion is disputed by US government officials. The US was the first country from which Lumumba requested help. Lumumba, for his part, not only denied being a Communist, but said he found colonialism and Communism to be equally deplorable, and professed his personal preference for neutrality between the East and West.
Belgian Involvement.
According to David Akerman, Ludo De Witte and Kris Hollington, the firing squads were commanded by a Belgian, Captain Julien Gat; another Belgian, Police Commissioner Verscheure, had overall command of the execution site. De Witte found written orders from the Belgian government requesting Lumumba's execution and documents on various arrangements, such as death squads.
On 18 January, panicked by reports that the burial of the three bodies had been observed, members of the execution team went to dig up the bodies and move them to a place near the border with Northern Rhodesia for reburial. Belgian Police Commissioner Gerard Soete later admitted in several accounts that he and his brother led the exhumation (and also a second exhumation, below). Police Commissioner Frans Verscheure also took part. According to Adam Hochschild, author of a book on the Congo rubber terror, Lumumba's body was later cut up and dissolved in acid by two Belgian agents. On the afternoon and evening of 21 January, Commissioner Soete and his brother dug up Lumumba's corpse for the second time, cut it up with a hacksaw, and dissolved it in concentrated sulfuric acid. In an interview on Belgian television in a program on the assassination of Lumumba in 1999, Soete displayed a bullet and two teeth that he boasted he had saved from Lumumba's body.
The Belgian Commission investigating Lumumba's assassination concluded that: (1) Belgium wanted Lumumba arrested, (2) Belgium was not particularly concerned with Lumumba's physical well being, and (3) although informed of the danger to Lumumba's life, Belgium did not take any action to avert his death, but the report also specifically denied that Belgium ordered Lumumba's assassination.
It has been argued that Belgium was legally culpable for failing to prevent the assassination from taking place due to its own 'Good Samaritan' laws. Belgium has also been accused of breaching its obligation (under U.N. Resolution 290 of 1949) to refrain from acts or threats "aimed at impairing the freedom, independence or integrity of another state." In February 2002, the Belgian government apologized to the Congolese people, and admitted to a "moral responsibility" and "an irrefutable portion of responsibility in the events that led to the death of Lumumba".
United States involvement.
The report of 2001 by the Belgian Commission mentions that there had been previous U.S. and Belgian plots to kill Lumumba. Among them was a Central Intelligence Agency-sponsored attempt to poison him, which may have come on orders from U.S. President Dwight D. Eisenhower. CIA chemist Sidney Gottlieb was a key person in this by devising a poison resembling toothpaste. In September 1960, Gottlieb brought a vial of the poison to the Congo with plans to place it on Lumumba's toothbrush. However, the plot was later abandoned; the plan is said to have been scrapped because the local CIA Station Chief, Larry Devlin, refused permission.
However, as Kalb points out in her book, "Congo Cables", the record shows that many communications by Devlin at the time urged elimination of Lumumba. Also, the CIA station chief helped to direct the search to capture Lumumba for his transfer to his enemies in Katanga; was involved in arranging his transfer to Katanga; and the CIA base chief in Elizabethville was in direct touch with the killers the night Lumumba was killed. Furthermore, John Stockwell indicates that a CIA agent had the body in the trunk of his car in order to try to get rid of it. Stockwell, who knew Devlin well, felt Devlin knew more than anyone else about the murder.
The inauguration of U.S. President John F. Kennedy in January 1961 caused fear among Mobutu's faction and within the CIA that the incoming administration would shift its favor to the imprisoned Lumumba. Lumumba was killed three days before Kennedy's inauguration on 20 January, though Kennedy would not learn of the killing until 13 February.
Church Committee.
In 1975, the Church Committee went on record with the finding that CIA chief Allen Dulles had ordered Lumumba's assassination as "an urgent and prime objective". Furthermore, declassified CIA cables quoted or mentioned in the Church report and in Kalb (1972) mention two specific CIA plots to murder Lumumba: the poison plot and a shooting plot. Although some sources claim that CIA plots ended when Lumumba was captured, that is not stated or shown in the CIA records.
Rather, those records show two still-partly-censored CIA cables from Elizabethville on days significant in the murder: 17 January, the day Lumumba died, and 18 January, the day of the first exhumation. The former, after a long censored section, talks about where they need to go from there. The latter expresses thanks for Lumumba being sent to them and then says that, had Elizabethville base known he was coming, they would have "baked a snake". This cable goes on to state that the writer's sources (not yet declassified) said that after being taken from the airport Lumumba was imprisoned by "all white guards".
The Committee later found that while the CIA had conspired to kill Lumumba, it was not directly involved in the actual murder.
U.S. government documents.
Declassified documents revealed that the CIA had plotted to assassinate Lumumba. These documents indicate that the Congolese leaders who killed Lumumba, including Mobutu and Joseph Kasavubu received money and weapons directly from the CIA. This same disclosure showed that at that time the U.S. government believed that Lumumba was a communist.
A recently declassified interview with then-US National Security Council minutekeeper Robert Johnson revealed that U.S. President Dwight D. Eisenhower had said "something [to CIA chief Allen Dulles] to the effect that Lumumba should be eliminated". The interview from the Senate Intelligence Committee's inquiry on covert action was released in August 2000.
In December 2013, the U.S. State Department admitted that President Eisenhower authorized the murder of Lumumba. The CIA Chief, Allan Dulles, allocated $100,000 to accomplish the act, but this plan was not carried out. 
British involvement.
In April 2013, in a letter to the "London Review of Books", a British parliamentarian, Lord Lea of Crondall reported having discussed Lumumba's death with Baroness Park of Monmouth shortly before she died in March 2010. Park had been an MI6 officer posted to Leopoldville at the time of Lumumba's death, and was later a semi-official spokesperson for MI6 in the House of Lords.
According to Lea, when he mentioned "the uproar" surrounding Lumumba's abduction and murder, and recalled the theory that MI6 might have had "something to do with it", she replied, "We did. I organised it." BBC reported that, subsequently, "Whitehall sources" described the claims of MI6 involvement as "speculative".
Legacy.
Political.
Patrice Lumumba was Prime Minister of The Congo for 81 days, from June 23 to September 14, 1960. To his supporters, Lumumba was an altruistic man of strong character. He favoured a unitary Congo and opposed division of the country along ethnic or regional lines. Like many other African leaders, he supported pan-Africanism and liberation for colonial territories. He proclaimed his regime one of "positive neutralism," defined as a return to African values and rejection of any imported ideology, including that of the Soviet Union: "We are not Communists or Catholics. We are African nationalists."
2006 Congolese elections.
The image of Patrice Lumumba continues to serve as an inspiration in contemporary Congolese politics. In the 2006 elections, several parties claimed to be motivated by his ideas, including the People's Party for Reconstruction and Democracy (PPRD), the political party initiated by the incumbent President Joseph Kabila. Antoine Gizenga, who served as Lumumba's Deputy Prime Minister in the post-independence period, was a 2006 Presidential candidate under the Unified Lumumbist Party (Parti Lumumbiste Unifié (PALU)) and was named prime minister at the end of the year. Other political parties that directly utilise his name include the Mouvement National Congolais-Lumumba (MNC-L) and the Mouvement Lumumbiste (MLP).
Family and politics.
Patrice Lumumba's family is actively involved in contemporary Congolese politics. Patrice Lumumba was married to Pauline Lumumba and had five children; François was the eldest followed by Patrice Junior, Julienne, Roland and Guy-Patrice Lumumba. François was 10 years old when Patrice died. Before his imprisonment, Patrice arranged for his wife and children to move into exile in Egypt, where François spent his childhood, then went to Hungary for education (he holds a doctorate in political economics).
Lumumba's youngest son, Guy-Patrice, born six months after his father's death, was an independent presidential candidate in the 2006 elections, but received less than 10% of the vote.
Bibliography.
Writings by Lumumba.
</dl>
Writings about Lumumba.
</dl>
Films.
</dl>
Archive video and audio.
</dl>
Other.
</dl>
References.
Sources.
</dl>

</doc>
<doc id="49241" url="http://en.wikipedia.org/wiki?curid=49241" title="Gustav Holst">
Gustav Holst

Gustav Theodore Holst (born Gustavus Theodore von Holst; 21 September 1874 – 25 May 1934) was an English composer, arranger and teacher. Best known for his orchestral suite "The Planets", he composed a large number of other works across a range of genres, although none achieved comparable success. His distinctive compositional style was the product of many influences, Richard Wagner and Richard Strauss being most crucial early in his development. The subsequent inspiration of the English folksong revival of the early 20th century, and the example of such rising modern composers as Maurice Ravel, led Holst to develop and refine an individual style.
There were professional musicians in the previous three generations of Holst's family, and it was clear from his early years that he would follow the same calling. He hoped to become a pianist, but was prevented by neuritis in his right arm. Despite his father's reservations, he pursued a career as a composer, studying at the Royal College of Music under Charles Villiers Stanford. Unable to support himself by his compositions, he played the trombone professionally and later became a teacher—a great one, according to his colleague Ralph Vaughan Williams. Among other teaching activities he built up a strong tradition of performance at Morley College, where he served as musical director from 1907 until 1924, and pioneered music education for women at St Paul's Girls' School, where he taught from 1905 until his death in 1934, raising standards and so laying the foundation for several professional musicians. He was the founder of a series of Whitsun music festivals, which ran from 1916 for the remainder of his life. Holst's works were played frequently in the early years of the 20th century, but it was not until the international success of "The Planets" in the years immediately after the First World War that he became a well-known figure. A shy man, he did not welcome this fame, and preferred to be left in peace to compose and teach.
In his later years his uncompromising, personal style of composition struck many music lovers as too austere, and his brief popularity declined. Nevertheless, he was a significant influence on a number of younger English composers, including Edmund Rubbra, Michael Tippett and Benjamin Britten. Apart from "The Planets" and a handful of other works, his music was generally neglected until the 1980s, since when recordings of much of his output have been available.
Life and career.
Early years.
Family background.
Holst was born in Cheltenham, Gloucestershire, the elder of the two children of Adolph von Holst, a professional musician, and his wife, Clara Cox, "née" Lediard. She was of mostly British descent, daughter of a respected Cirencester solicitor; the Holst side of the family was of mixed Swedish, Latvian and German ancestry, with at least one professional musician in each of the previous three generations.
Holst's great-grandfather, Matthias Holst, born in Riga, Latvia, was of German origin; he served as composer and harp-teacher to the Imperial Russian Court in St Petersburg. Matthias's son Gustavus, who moved to England with his parents as a child in 1802, was a composer of salon-style music and a well-known harp teacher. He appropriated the aristocratic prefix "von" and added it to the family name in the hope of gaining enhanced prestige and attracting pupils.
Holst's father, Adolph von Holst, became organist and choirmaster at All Saints' Church, Cheltenham; he also taught, and gave piano recitals. His wife, Clara, a former pupil, was a talented singer and pianist. They had two sons; Gustav's younger brother, Emil Gottfried, became known as Ernest Cossart, a successful actor in the West End, New York and Hollywood. Clara died in February 1882, and the family moved to another house in Cheltenham, where Adolph recruited his sister Nina to help raise the boys. Gustav recognised her devotion to the family and dedicated several of his early compositions to her. In 1885 Adolph married Mary Thorley Stone, another of his pupils. They had two sons, Matthias (known as "Max") and Evelyn ("Thorley"). Mary von Holst was absorbed in theosophy and not greatly interested in domestic matters. All four of Adolph's sons were subject to what one biographer calls "benign neglect", and Gustav in particular was "not overburdened with attention or understanding, with a weak sight and a weak chest, both neglected—he was 'miserable and scared'."
Childhood and youth.
Holst was taught to play the piano and the violin; he enjoyed the former very much more than the latter. At the age of twelve he took up the trombone at Adolph's suggestion, thinking that playing a brass instrument might improve his asthma. Holst was educated at Cheltenham Grammar School between 1886 and 1891. He started composing in or about 1886; inspired by Macaulay's poem "" he began, but soon abandoned, an ambitious setting of the work for chorus and orchestra. His early compositions included piano pieces, organ voluntaries, songs, anthems and a symphony (from 1892). His main influences at this stage were Mendelssohn, Chopin, Grieg and above all Sullivan. Adolph tried to steer his son away from composition, hoping that he would have a career as a pianist. Holst's health played a decisive part in his musical future; he had never been strong, and in addition to his asthma and poor eyesight he suffered from neuritis, which made playing the piano difficult. He said that the affected arm was "like a jelly overcharged with electricity".
After Holst left school in 1891, Adolph paid for him to spend four months in Oxford studying counterpoint with George Frederick Sims, organist of Merton College. On his return Holst obtained his first professional appointment, aged seventeen, as organist and choirmaster at Wyck Rissington, Gloucestershire. The post brought with it the conductorship of the Bourton-on-the-Water Choral Society, which offered no extra remuneration but provided valuable experience that enabled him to hone his conducting skills. In November 1891 Holst gave what was perhaps his first public performance as a pianist; he and his father played the Brahms "Hungarian Dances" at a concert in Cheltenham. The programme for the event gives his name as "Gustav" rather than "Gustavus"; he was called by the shorter version from his early years.
Royal College of Music.
In 1892 Holst wrote the music for an operetta in the style of Gilbert and Sullivan, "Lansdowne Castle, or The Sorcerer of Tewkesbury". The piece was performed at Cheltenham Corn Exchange in February 1893; it was well received and its success encouraged him to persevere with composing. He applied for a scholarship at the Royal College of Music (RCM) in London, but the composition scholarship for that year was won by Samuel Coleridge-Taylor. Holst was accepted as a non-scholarship student, and Adolph borrowed £100 to cover the first year's expenses. Holst left Cheltenham for London in May 1893. Money was tight, and partly from frugality and partly from his own inclination he became a vegetarian and a teetotaller. Two years later he was finally granted a scholarship, which slightly eased his financial difficulties, but he retained his austere personal regime.
Holst's professors at the RCM were Frederick Sharpe (piano), William Stephenson Hoyte (organ), George Case (trombone), George Jacobi (instrumentation) and the director of the college, Hubert Parry (history). After preliminary lessons with W. S. Rockstro and Frederick Bridge, Holst was granted his wish to study composition with Charles Villiers Stanford. To support himself during his studies Holst played the trombone professionally, at seaside resorts in the summer and in London theatres in the winter. His daughter and biographer, Imogen Holst, records that from his fees as a player "he was able to afford the necessities of life: board and lodging, manuscript paper, and tickets for standing room in the gallery at Covent Garden Opera House on Wagner evenings". He secured an occasional engagement in symphony concerts, playing in 1897 under the baton of Richard Strauss at the Queen's Hall.
Like many musicians of his generation, Holst came under Wagner's spell. He had recoiled from the music of "Götterdämmerung" when he heard it at Covent Garden in 1892, but encouraged by his friend and fellow-student Fritz Hart he persevered and quickly became an ardent Wagnerite. Wagner supplanted Sullivan as the main influence on his music, and for some time, as Imogen put it, "ill-assimilated wisps of "Tristan" inserted themselves on nearly every page of his own songs and overtures." Stanford admired some of Wagner's works, and had in his earlier years been influenced by him, but Holst's sub-Wagnerian compositions met with his disapprobation: "It won't do, me boy; it won't do". Holst respected Stanford, describing him to a fellow-pupil, Herbert Howells, as "the one man who could get any one of us out of a technical mess", but he found that his fellow students, rather than the faculty members, had the greater influence on his development.
In 1895, shortly after celebrating his twenty-first birthday, Holst met Ralph Vaughan Williams, who became a lifelong friend and had more influence on Holst's music than anybody else. Stanford emphasised the need for his students to be self-critical, but Holst and Vaughan Williams became one another's chief critics; each would play his latest composition to the other while still working on it. Vaughan Williams later observed, "What one really learns from an Academy or College is not so much from one's official teachers as from one's fellow-students ... [we discussed] every subject under the sun from the lowest note of the double bassoon to the philosophy of "Jude the Obscure". In 1949 he wrote of their relationship, "Holst declared that his music was influenced by that of his friend: the converse is certainly true." 1895 was also the bicentenary of Henry Purcell, which was marked by various performances including Stanford conducting "Dido and Aeneas" at the Lyceum Theatre; the work profoundly impressed Holst, who over twenty years later confessed to a friend that his search for "the (or a) musical idiom of the English language" had been inspired "unconsciously" by "hearing the recits in Purcell's Dido".
Another influence was William Morris. In Vaughan Williams's words, "It was now that Holst discovered the feeling of unity with his fellow men which made him afterwards a great teacher. A sense of comradeship rather than political conviction led him, while still a student, to join the Kelmscott House Socialist Club in Hammersmith." At Kelmscott House, Morris's home, Holst attended lectures by his host and Bernard Shaw. His own socialism was moderate in character, but he enjoyed the club for its good company and his admiration of Morris as a man. His ideals were influenced by Morris's but had a different emphasis. Morris had written, "I do not want art for a few any more than education for a few, or freedom for a few. I want all persons to be educated according to their capacity, not according to the amount of money which their parents happen to have". Holst said, "'Aristocracy in art'—art is not for all but only for the chosen few—but the only way to find those few is to bring art to everyone—then the artists have a sort of masonic signal by which they recognise each other in the crowd." He was invited to conduct the Hammersmith Socialist Choir, teaching them madrigals by Thomas Morley, choruses by Purcell, and works by Mozart, Wagner and himself. One of his choristers was (Emily) Isobel Harrison (1876–1969), a beautiful soprano two years his junior. He fell in love with her; she was at first unimpressed by him, but she came round and they were engaged, though with no immediate prospect of marriage given Holst's tiny income.
Professional musician.
In 1898 the RCM offered Holst a further year's scholarship, but he felt that he had learned as much as he could there and that it was time, as he put it, to "learn by doing". Some of his compositions were published and performed; the previous year "The Times" had praised his song "Light Leaves Whisper", "a moderately elaborate composition in six parts, treated with a good deal of expression and poetic feeling". Occasional successes notwithstanding, Holst found that "man cannot live by composition alone"; he took posts as organist at various London churches, and continued playing the trombone in theatre orchestras. In 1898 he was appointed first trombonist and "répétiteur" with the Carl Rosa Opera Company and toured with the Scottish Orchestra. Though a capable rather than a virtuoso player he won the praise of the leading conductor Hans Richter, for whom he played at Covent Garden. His salary was only just enough to live on, and he supplemented it by playing in a popular orchestra called the "White Viennese Band", conducted by Stanislas Wurm. Holst enjoyed playing for Wurm, and learned much from him about drawing rubato from players. Nevertheless, longing to devote his time to composing, Holst found the necessity of playing for "the Worm" or any other light orchestra "a wicked and loathsome waste of time". Vaughan Williams did not altogether agree with his friend about this; he admitted that some of the music was "trashy" but thought it had been useful to Holst nonetheless: "To start with, the very worst a trombonist has to put up with is as nothing compared to what a church organist has to endure; and secondly, Holst is above all an orchestral composer, and that sure touch which distinguishes his orchestral writing is due largely to the fact that he has been an orchestral player; he has learnt his art, both technically and in substance, not at second hand from text books and models, but from actual live experience."
With a modest income secured, Holst was able to marry Isobel; the ceremony was at Fulham Register Office on 22 June 1901. Their marriage lasted until his death; there was one child, Imogen, born in 1907. In 1902 Dan Godfrey and the Bournemouth Municipal Orchestra premiered Holst's "Cotswold Symphony", the slow movement of which is a lament for Morris, who had died in October 1896, three years before Holst began work on the piece. In 1903 Adolph von Holst died, leaving a small legacy. Holst and his wife decided, as Imogen later put it, that "as they were always hard up the only thing to do was to spend it all at once on a holiday in Germany".
Composer and teacher.
While in Germany, Holst reappraised his professional life, and in 1903 he decided to abandon orchestral playing to concentrate on composition. His earnings as a composer were too little to live on, and two years later he accepted the offer of a teaching post at James Allen's Girls' School, Dulwich, which he held until 1921. He also taught at the Passmore Edwards Settlement, where among other innovations he gave the British premieres of two Bach cantatas. The two teaching posts for which he is probably best known were director of music at St Paul's Girls' School, Hammersmith, from 1905 until his death, and director of music at Morley College from 1907 to 1924. Vaughan Williams wrote of the former establishment: "Here he did away with the childish sentimentality which schoolgirls were supposed to appreciate and substituted Bach and Vittoria; a splendid background for immature minds." Several of Holst's pupils at St Paul's went on to distinguished careers, notably the soprano Joan Cross, and the oboist and cor anglais player Helen Gaskell, who made history by becoming the first woman to join the woodwind section of the New Queen's Hall Orchestra, subsequently joining the BBC Symphony Orchestra on its foundation. Of Holst's impact on Morley College, Vaughan Williams wrote: "[A] bad tradition had to be broken down. The results were at first discouraging, but soon a new spirit appeared and the music of Morley College, together with its off-shoot the 'Whitsuntide festival' ... became a force to be reckoned with". Before Holst's appointment, Morley College had not treated music very seriously—the bad tradition to which Vaughan Williams referred; at first Holst's exacting demands drove many students away. He persevered, and gradually built up a class of dedicated music-lovers.
According to the composer Edmund Rubbra, who studied under him in the early 1920s, Holst was "a teacher who often came to lessons weighted, not with the learning of Prout and Stainer, but with a miniature score of "Petrushka" or the then recently published Mass in G minor of Vaughan Williams". He never sought to impose his own ideas on his composition pupils. Rubbra recalled that he would divine a student's difficulties and gently guide him to finding the solution for himself. "I do not recall that Holst added one single note of his own to anything I wrote, but he would suggest—if I agreed!—that, given such and such a phrase, the following one would be better if it took such and such a course; if I did not see this, the point would not be insisted upon ... He frequently took away [because of] his abhorrence of unessentials."
As a composer Holst was frequently inspired by literature. He set poetry by Thomas Hardy and Robert Bridges and, a particular influence, Walt Whitman, whose words he set in "Dirge for Two Veterans" and "The Mystic Trumpeter" (1904). He wrote an orchestral "Walt Whitman Overture" in 1899. While on tour with the Carl Rosa company Holst had read some of Max Müller's books, which inspired in him a keen interest in Sanskrit texts, particularly the Rig Veda hymns. He found the existing English versions of the texts unconvincing, and decided to make his own translations, despite his lack of skills as a linguist. He enrolled in 1909 at University College, London to study the language. Imogen commented on his translations: "He was not a poet, and there are occasions when his verses seem naïve. But they never sound vague or slovenly, for he had set himself the task of finding words that would be 'clear and dignified' and that would 'lead the listener into another world'." His settings of translations of Sanskrit texts included "Sita" (1899–1906), a three-act opera based on an episode in the "Ramayana" (which he eventually entered for a competition for English opera set by the Milan music publisher Tito Ricordi); "Savitri" (1908), a chamber opera based on a tale from the "Mahabharata"; four groups of "Hymns from the Rig Veda" (1908–14); and two texts originally by Kālidāsa: "Two Eastern Pictures" (1909–10) and "The Cloud Messenger" (1913).
Towards the end of the nineteenth century, British musical circles had experienced a new interest in national folk music. Some composers, such as Sullivan and Elgar, remained indifferent, but Parry, Stanford, John Stainer and Alexander Mackenzie were founding members of the Folk-Song Society. Parry considered that by recovering English folk song, English composers would find an authentic national voice; he commented, "in true folk-songs there is no sham, no got-up glitter, and no vulgarity". Vaughan Williams was an early and enthusiastic convert to this cause, going round the English countryside collecting and noting down folk songs. These had an influence on Holst. Though not as passionate on the subject as his friend, he incorporated a number of folk melodies in his own compositions and made several arrangements of folk songs collected by others. The "Somerset Rhapsody" (1906–07), was written at the suggestion of the folk-song collector Cecil Sharp and made use of three tunes that Sharp had noted down. Holst described its performance at the Queen's Hall in 1910 as "my first real success". A few years later Holst became excited by another musical renaissance—the rediscovery of English madrigal composers. Weelkes was his favourite of all the Tudor composers, but Byrd also meant much to him.
Holst was a keen rambler. He walked extensively in England, Italy, France and Algeria. In 1908 he travelled to Algeria on medical advice as a treatment for asthma and the depression that he suffered after his opera "Sita" failed to win the Ricordi prize. This trip inspired the suite "Beni Mora", which incorporated music he heard in the Algerian streets. Vaughan Williams wrote of this exotic work, "if it had been played in Paris rather than London it would have given its composer a European reputation, and played in Italy would probably have caused a riot."
1910s.
In June 1911 Holst and his Morley College students gave the first performance since the seventeenth century of Purcell's "The Fairy-Queen". The full score had been lost soon after Purcell's death in 1695, and had only recently been found. Twenty-eight Morley students copied out the complete vocal and orchestral parts. There were 1,500 pages of music and it took the students almost eighteen months to copy them out in their spare time. A concert performance of the work was given at The Old Vic, preceded by an introductory talk by Vaughan Williams. "The Times" praised Holst and his forces for "a most interesting and artistic performance of this very important work".
After this success, Holst was disappointed the following year by the lukewarm reception of his choral work "The Cloud Messenger". He again went travelling, accepting an invitation from H. Balfour Gardiner to join him and the brothers Clifford and Arnold Bax in Spain. During this holiday Clifford Bax introduced Holst to astrology, an interest that later inspired his suite "The Planets". Holst cast his friends' horoscopes for the rest of his life and referred to astrology as his "pet vice".
In 1913, St Paul's Girls' School opened a new music wing, and Holst composed "St Paul's Suite" for the occasion. The new building contained a sound-proof room, handsomely equipped, where he could work undisturbed. Holst and his family moved to a house in Brook Green, very close to the school. For the previous six years they had lived in a pretty house overlooking the Thames at Barnes, but the river air, frequently foggy, affected his breathing. For use at weekends and during school holidays, Holst and his wife bought a cottage in Thaxted, Essex, surrounded by mediaeval buildings and ample rambling opportunities. In 1917 they moved to a house in the centre of the town, where they stayed until 1925.
At Thaxted, Holst became friendly with the Rev Conrad Noel, known as the "Red Vicar", who supported the Independent Labour Party and espoused many causes unpopular with conservative opinion. Noel also encouraged the revival of folk-dancing and processionals as part of church ceremonies, innovations which caused controversy among traditionally-minded churchgoers. Holst became an occasional organist and choirmaster at Thaxted Parish Church; he also developed an interest in bell-ringing. He started an annual music festival at Whitsuntide in 1916; students from Morley College and St Paul's Girls' School performed together with local participants. Holst's "a cappella" carol, "This Have I Done For My True Love", was dedicated to Noel in recognition of his interest in the ancient origins of religion (the composer always referred to the work as "The Dancing Day"). It received its first performance during the Third Whitsun Festival at Thaxted in May 1918. During that festival, Noel, a staunch supporter of Russia's October Revolution, demanded in a Saturday message during the service that there should be a greater political commitment from those who participated in the church activities; his claim that several of Holst's pupils (implicitly those from St Paul's Girls' School) were merely "camp followers" caused offence. Holst, anxious to protect his students from being embroiled in ecclesiastical conflict, moved the Whitsun Festival to Dulwich, though he himself continued to help with the Thaxted choir and to play the church organ on occasion.
First World War.
At the outbreak of the First World War, Holst tried to enlist but was rejected as unfit for military service. He felt frustrated that he could not contribute to the war effort. His wife became a volunteer ambulance driver; Vaughan Williams went on active service to France as did Holst's brother Emil; Holst's friends the composers George Butterworth and Cecil Coles were killed in battle. He continued to teach and compose; he worked on "The Planets" and prepared his chamber opera "Savitri" for performance. It was first given in December 1916 by students of the London School of Opera at the Wellington Hall in St John's Wood. It attracted no attention at the time from the main newspapers, though when professionally staged five years later it was greeted as "a perfect little masterpiece." In 1917 he wrote "The Hymn of Jesus" for chorus and orchestra, a work which remained unperformed until after the war.
In 1918, as the war neared its end, Holst finally had the prospect of a job that offered him the chance to serve. The music section of the YMCA's education department needed volunteers to work with British troops stationed in Europe awaiting demobilisation. Morley College and St Paul's Girls' School offered him a year's leave of absence, but there remained one obstacle: the YMCA felt that his surname looked too German to be acceptable in such a role. He formally changed "von Holst" to "Holst" by deed poll in September 1918. He was appointed as the YMCA's musical organiser for the Near East, based in Salonica.
Holst was given a spectacular send-off. The conductor Adrian Boult recalled, "Just before the Armistice, Gustav Holst burst into my office: 'Adrian, the YMCA are sending me to Salonica quite soon and Balfour Gardiner, bless his heart, has given me a parting present consisting of the Queen's Hall, full of the Queen's Hall Orchestra for the whole of a Sunday morning. So we're going to do "The Planets", and you've got to conduct'." There was a burst of activity to get things ready in time. The girls at St Paul's helped to copy out the orchestral parts, and the women of Morley and the St Paul's girls learned the choral part in the last movement. The performance was given on 29 September to an invited audience including Sir Henry Wood and most of the professional musicians in London. Five months later, when Holst was in Greece, Boult introduced "The Planets" to the general public, at a concert in February 1919; Holst sent him a long letter full of suggestions, but failed to convince him that the suite should be played in full. The conductor believed that about half an hour of such radically new music was all the public could absorb at first hearing, and he gave only five of the seven movements on that occasion.
Holst enjoyed his time in Salonica, from where he was able to visit Athens, which greatly impressed him. His musical duties were wide-ranging, and even obliged him on occasion to play the violin in the local orchestra: "it was great fun, but I fear I was not of much use". He returned to England in June 1919.
Post-war.
On his return from Greece, Holst resumed his teaching and composing. In addition to his existing work he accepted a lectureship in composition at the University of Reading and joined Vaughan Williams in teaching composition at their "alma mater" the RCM. Inspired by Adrian Boult's conducting classes at the RCM, Holst tried to further pioneer music education for women by proposing to the High Mistress of St Paul's Girls' School that he should invite Boult to give classes the school: "It would be glorious if the SPGS turned out the only women conductors in the world!" In his soundproof room at SPGS he composed the "Ode to Death", a setting of a poem by Whitman, which according to Vaughan Williams is considered by many to be Holst's most beautiful choral work.
Holst, in his forties, suddenly found himself in demand. The New York Philharmonic and Chicago Symphony Orchestra vied to be the first to play "The Planets" in the US. The success of that work was followed in 1920 by an enthusiastic reception for "The Hymn of Jesus", described in "The Observer" as "one of the most brilliant and one of the most sincere pieces of choral and orchestral expression heard for some years." "The Times" called it "undoubtedly the most strikingly original choral work which has been produced in this country for many years." To his surprise and dismay Holst was becoming famous. Celebrity was something wholly foreign to his nature. As the music scholar Byron Adams puts it, "he struggled for the rest of his life to extricate himself from the web of garish publicity, public incomprehension and professional envy woven about him by this unsought-for success." He turned down honours and awards offered to him, and refused to give interviews or autographs.
Holst's comic opera "The Perfect Fool" (1923) was widely seen as a satire of "Parsifal", though Holst firmly denied it. The piece, with Maggie Teyte in the leading soprano role and Eugene Goossens conducting, was enthusiastically received at its premiere in the Royal Opera House. At a concert in Reading in 1923, Holst slipped and fell, suffering concussion. He seemed to make a good recovery, and he felt up to accepting an invitation to the US, lecturing and conducting at the University of Michigan. After he returned he found himself more and more in demand, to conduct, prepare his earlier works for publication, and, as before, to teach. The strain caused by these demands on him was too great; on doctor's orders he cancelled all professional engagements during 1924, and retreated to Thaxted. In 1925 he resumed his work at St Paul's Girls' School, but did not return to any of his other posts.
Later years.
Holst's productivity as a composer benefited almost at once from his release from other work. His works from this period include the "First Choral Symphony" to words by Keats (a "Second Choral Symphony" to words by George Meredith exists only in fragments). A short Shakespearian opera, "At the Boar's Head", followed; neither had the immediate popular appeal of "A Moorside Suite" for brass band of 1928.
In 1927 Holst was commissioned by the New York Symphony Orchestra to write a symphony. Instead, he wrote an orchestral piece "Egdon Heath", inspired by Thomas Hardy's Wessex. It was first performed in February 1928, a month after Hardy's death, at a memorial concert. By this time the public's brief enthusiasm for everything Holstian was waning, and the piece was not well received in New York. Olin Downes in "The New York Times" opined that "the new score seemed long and undistinguished". The day after the American performance, Holst conducted the City of Birmingham Orchestra in the British premiere. "The Times" acknowledged the bleakness of the work but allowed that it matched Hardy's grim view of the world: ""Egdon Heath" is not likely to be popular, but it says what the composer wants to say, whether we like it or not, and truth is one aspect of duty." Holst had been distressed by hostile reviews of some of his earlier works, but he was indifferent to critical opinion of "Egdon Heath", which he regarded as, in Adams's phrase, his "most perfectly realized composition".
Towards the end of his life Holst wrote the "Choral Fantasia" (1930) and he was commissioned by the BBC to write a piece for military band; the resulting prelude and scherzo "Hammersmith" was a tribute to the place where he had spent most of his life. The composer and critic Colin Matthews considers the work "as uncompromising in its way as "Egdon Heath", discovering, in the words of Imogen Holst, 'in the middle of an over-crowded London ... the same tranquillity that he had found in the solitude of Egdon Heath'". The work was unlucky in being premiered at a concert that also featured the London premiere of Walton's "Belshazzar's Feast", by which it was somewhat overshadowed.
Holst wrote a score for a British film, "The Bells" (1931), and was amused to be recruited as an extra in a crowd scene. Both film and score are now lost. He wrote a "jazz band piece" that Imogen later arranged for orchestra as "Capriccio". Having composed operas throughout his life with varying success, Holst found for his last opera, "The Wandering Scholar", what Matthews calls "the right medium for his oblique sense of humour, writing with economy and directness".
Harvard University offered Holst a lectureship for the first six months of 1932. Arriving via New York he was pleased to be reunited with his brother, Emil, whose acting career under the name of Ernest Cossart had taken him to Broadway; but Holst was dismayed by the continual attentions of press interviewers and photographers. He enjoyed his time at Harvard, but was taken ill while there: a duodenal ulcer prostrated him for some weeks. He returned to England, joined briefly by his brother for a holiday together in the Cotswolds. His health declined, and he withdrew further from musical activities. One of his last efforts was to guide the young players of the St Paul's Girls' School orchestra through one of his final compositions, the "Brook Green Suite", in March 1934.
Holst died in London on 25 May 1934, at the age of 59, of heart failure following an operation on his ulcer. His ashes were interred at Chichester Cathedral in Sussex, close to the memorial to Thomas Weelkes, his favourite Tudor composer. Bishop George Bell gave the memorial oration at the funeral, and Vaughan Williams conducted music by Holst and himself.
Music.
Style.
Holst's absorption of folksong, not only in the melodic sense but in terms of its simplicity and economy of expression, helped to develop a style that many of his contemporaries, even admirers, found austere and cerebral. This is contrary to the popular identification of Holst with "The Planets", which Matthews believes has masked his status as a composer of genuine originality. Against charges of coldness in the music, Imogen cites Holst's characteristic "sweeping modal tunes mov[ing] reassuringly above the steps of a descending bass", while Michael Kennedy points to the 12 Humbert Wolfe settings of 1929, and the 12 Welsh folksong settings for unaccompanied chorus of 1930–31, as works of true warmth.
Many of the characteristics that Holst employed—unconventional time signatures, rising and falling scales, ostinato, bitonality and occasional polytonality—set him apart from other English composers. Vaughan Williams remarked that Holst always said in his music what he wished to say, directly and concisely; "He was not afraid of being obvious when the occasion demanded, nor did he hesitate to be remote when remoteness expressed his purpose". Kennedy has surmised that Holst's economy of style was in part a product of the composer's poor health: "the effort of writing it down compelled an artistic economy which some felt was carried too far". However, as an experienced instrumentalist and orchestra member, Holst understood music from the standpoint of his players and made sure that, however challenging, their parts were always practicable. According to his pupil Jane Joseph, Holst fostered in performance "a spirit of practical comradeship ... none could know better than he the boredom possible to a professional player, and the music that rendered boredom impossible".
Early works.
Although Holst wrote a large number of works—particularly songs—during his student days and early adulthood, almost everything he wrote before 1904 he later classified as derivative "early horrors". Nevertheless, the composer and critic Colin Matthews recognises even in these apprentice works an "instinctive orchestral flair". Of the few pieces from this period which demonstrate some originality, Matthews pinpoints the G minor String Trio of 1894 (unperformed until 1974) as the first underivative work produced by Holst. Matthews and Imogen Holst each highlight the "Elegy" movement in "The Cotswold Symphony" (1899–1900) as among the more accomplished of the apprentice works, and Imogen discerns glimpses of her father's real self in the 1899 "Suite de ballet" and the "Ave Maria" of 1900. She and Matthews have asserted that Holst found his genuine voice in his setting of Whitman's verses, "The Mystic Trumpeter" (1904), in which the trumpet calls that characterise Mars in "The Planets" are briefly anticipated. In this work, Holst first employs the technique of bitonality—the use of two keys simultaneously.
Experimental years.
At the beginning of the 20th century, according to Matthews, it appeared that Holst might follow Schoenberg into late Romanticism. Instead, as Holst recognised afterwards, his encounter with Purcell's "Dido and Aeneas" prompted his searching for a "musical idiom of the English language"; the folksong revival became a further catalyst for Holst to seek inspiration from other sources during the first decade or so of the new century.
Indian period.
Holst's interest in Indian mythology, shared by many of his contemporaries, first became musically evident in the opera "Sita" (1901–06). During the opera's long gestation, Holst worked on other Indian-themed pieces. These included "Maya" (1901) for violin and piano, regarded by the composer and writer Raymond Head as "an insipid salon-piece whose musical language is dangerously close to Stephen Adams". Then, through Vaughan Williams, Holst discovered and became an admirer of the music of Ravel, whom he considered a "model of purity" on the level with Haydn, another composer he greatly admired. The combined influence of Ravel, Hindu spiritualism and English folk tunes enabled Holst to get beyond the once all-consuming influences of Wagner and Richard Strauss and to forge his own style. Imogen Holst has acknowledged Holst's own suggestion (written to Vaughan Williams): "[O]ne ought to follow Wagner until he leads you to fresh things". She notes that although much of his grand opera, "Sita", is "'good old Wagnerian bawling' ... towards the end a change comes over the music, and the beautifully calm phrases of the hidden chorus representing the Voice of the Earth are in Holst's own language."
According to Rubbra, the publication in 1911 of Holst's Rig Veda Hymns was a landmark event in the composer's development: "Before this, Holst's music had, indeed, shown the clarity of utterance which has always been his characteristic, but harmonically there was little to single him out as an important figure in modern music." Dickinson describes these vedic settings as pictorial rather than religious; although the quality is variable the sacred texts clearly "touched vital springs in the composer's imagination". While the music of Holst's Indian verse settings remained generally western in character, in some of the vedic settings he experimented with Indian "raga" (scales).
The chamber opera "Savitri" (1908) is written for three solo voices, a small hidden female chorus, and an instrumental combination of two flutes, a cor anglais and a double string quartet. The music critic John Warrack comments on the "extraordinary expressive subtlety" with which Holst deploys the sparse forces: "... [T]he two unaccompanied vocal lines opening the work skilfully convey the relationship between Death, steadily advancing through the forest, and Savitri, her frightened answers fluttering round him, unable to escape his harmonic pull". Head describes the work as unique in its time for its compact intimacy, and considers it Holst's most successful attempt to end the domination of Wagnerian chromaticism in his music. Dickinson considers it a significant step, "not towards opera, but towards an idiomatic pursuit of [Holst's] vision". Of the Kālidāsa texts, Dickinson dismisses "The Cloud Messenger" (1910–12) as an "accumulation of desultory incidents, opportunistic dramatic episodes and ecstatic outpourings" which illustrate the composer's creative confusion during that period; the "Two Eastern Pictures" (1911), in Dickinson's view, provide "a more memorable final impression of Kālidāsa".
Folksong and other influences.
Holst's settings of Indian texts formed only a part of his compositional output in the period 1900 to 1914. A highly significant factor in his musical development was the English folksong revival, evident in the orchestral suite "A Somerset Rhapsody" (1906–07), a work that was originally to be based around eleven folksong themes; this was later reduced to four. Observing the work's kinship with Vaughan Williams's "Norfolk Rhapsody", Dickinson remarks that, with its firm overall structure, Holst's composition "rises beyond the level of ... a song-selection". Imogen acknowledges that Holst's discovery of English folksongs "transformed his orchestral writing", and that the composition of "A Somerset Rhapsody" did much to banish the chromaticisms that had dominated his early compositions. In the "Two Songs without Words" of 1906, Holst showed that he could create his own original music using the folk idiom. An orchestral folksong fantasy "Songs of the West", also written in 1906, was withdrawn by the composer and never published, although it emerged in the 1980s in the form of an arrangement for wind band by James Curnow.
In the years before the First World War, Holst composed in a variety of genres. Matthews considers the evocation of a North African town in the "Beni Mora" suite of 1908 the composer's most individual work to that date; the third movement gives a preview of minimalism in its constant repetition of a four-bar theme. Holst wrote two suites for military band, in E flat (1909) and F major (1911) respectively, the first of which became and remains a brass band staple. This piece, a highly original and substantial musical work, was a signal departure from what Short describes as "the usual transcriptions and operatic selections which pervaded the band repertoire". Also in 1911 he wrote "Hecuba's Lament", a setting of Gilbert Murray's translation from Euripides built on a seven-beat refrain designed, says Dickinson, to represent Hecuba's defiance of divine wrath. In 1912 Holst composed two psalm settings, in which he experimented with plainsong; the same year saw the enduringly popular "St Paul's Suite" (a "gay but retrogressive" piece according to Dickinson), and the failure of his large scale orchestral work "Phantastes".
Full flowering.
"The Planets".
Holst conceived the idea of "The Planets" in 1913, partly as a result of his interest in astrology, and also from his determination, despite the failure of "Phantastes", to produce a large-scale orchestral work. The chosen format may have been influenced by Schoenberg's "Fünf Orchesterstücke", and shares something of the aesthetic, Matthews suggests, of Debussy's "Nocturnes" or "La mer". Holst began composing "The Planets" in 1914; the movements appeared not quite in their final sequence; "Mars" was the first to be written, followed by "Venus" and "Jupiter". "Saturn", "Uranus" and "Neptune" were all composed during 1915, and "Mercury" was completed in 1916.
Each planet is represented with a distinct character; Dickinson observes that "no planet borrows colour from another". In "Mars", a persistent, uneven rhythmic cell consisting of five beats, combined with trumpet calls and harmonic dissonance provides battle music which Short asserts is unique in its expression of violence and sheer terror, "... Holst's intention being to portray the reality of warfare rather than to glorify deeds of heroism". In "Venus", Holst incorporated music from an abandoned vocal work, "A Vigil of Pentecost", to provide the opening; the prevalent mood within the movement is of peaceful resignation and nostalgia. "Mercury" is dominated by uneven metres and rapid changes of theme, to represent the speedy flight of the winged messenger. "Jupiter" is renowned for its central melody, in Dickinson's view "a fantastic relaxation in which many retain a far from sneaking delight". Dickinson and other critics have decried the later use of the tune in the patriotic hymn "I Vow to Thee, My Country"—despite Holst's full complicity.
For "Saturn", Holst again used a previously-composed vocal piece, "Dirge and Hymeneal", as the basis for the movement, where repeated chords represent the relentless approach of old age. "Uranus", which follows, has elements of Berlioz's "Symphonie fantastique" and Dukas's "The Sorcerer's Apprentice", in its depiction of the magician who "disappears in a whiff of smoke as the sonic impetus of the movement diminishes from fff to ppp in the space of a few bars". "Neptune", the final movement, concludes with a wordless female chorus gradually receding, an effect which Warrack likens to "unresolved timelessness ... never ending, since space does not end, but drifting away into eternal silence". Apart from his concession with "I Vow to Thee", Holst insisted on the unity of the whole work, and opposed the performance of individual movements. Nevertheless, Imogen writes that the piece has "suffered from being quoted in snippets as background music".
Maturity.
During and after the composition of "The Planets", Holst wrote or arranged numerous vocal and choral works, many of them for the wartime Thaxted Whitsun Festivals, 1916–18. They include the "Six Choral Folksongs" of 1916, based on West Country tunes, of which "Swansea Town", with its "sophisticated tone", is deemed by Dickinson to be the most memorable. Holst downplayed such music as "a limited form of art" in which "mannerisms are almost inevitable"; the composer Alan Gibbs, however, believes Holst's set at least equal to Vaughan Williams's "Five English Folk Songs" of 1913. Holst's first major work after "The Planets" was the "Hymn of Jesus", completed in 1917. The words are from a Gnostic text, the apocryphal Acts of St John, using a translation from the Greek which Holst prepared with assistance from Clifford Bax and Jane Joseph. Head comments on the innovative character of the "Hymn": "At a stroke Holst had cast aside the Victorian and Edwardian sentimental oratorio, and created the precursor of the kind of works that John Tavener, for example, was to write in the 1970s". Matthews has written that the "Hymn"‍ '​s "ecstatic" quality is matched in English music "perhaps only by Tippett's "The Vision of Saint Augustine""; the musical elements include plainsong, two choirs distanced from each other to emphasise dialogue, dance episodes and "explosive chordal dislocations".
In the "Ode to Death" (1918–19), the quiet, resigned mood is seen by Matthews as an "abrupt volte-face" after the life-enhancing spirituality of the "Hymn". Warrack refers to its aloof tranquillity; Imogen Holst believed the "Ode" expressed Holst's private attitude to death. The piece has rarely been performed since its premiere in 1922, although the composer Ernest Walker thought it was Holst's finest work to that date. The influential critic Ernest Newman considered "The Perfect Fool" "the best of modern British operas", but its unusually short length (about an hour) and parodic, whimsical nature—described by "The Times" as "a brilliant puzzle"—put it outside the operatic mainstream. Only the ballet music from the opera, which "The Times" called "the most brilliant thing in a work glittering with brilliant moments", has been regularly performed since 1923. Holst's libretto attracted much criticism, although Edwin Evans remarked on the rare treat in opera of being able to hear the words being sung.
Later works.
Before his enforced rest in 1924, Holst demonstrated a new interest in counterpoint, in his "Fugal Overture" of 1922 for full orchestra and the neo-classical" Fugal Concerto" of 1923, for flute, oboe and strings. In his final decade he mixed song settings and minor pieces with major works and occasional new departures; the 1925 "Terzetto" for flute, violin and oboe, each instrument playing in a different key, is cited by Imogen as Holst's only "successful" chamber work. Of the "Choral Symphony" completed in 1924, Matthews writes that, after several movements of real quality, the finale is a rambling anticlimax. Holst's penultimate opera, "At the Boar's Head" (1924), is based on tavern scenes from Shakespeare's "Henry IV, Parts 1" and "2". The music, which is largely derived from old English melodies gleaned from Cecil Sharp and other collections, has pace and verve; the contemporary critic Harvey Grace discounted the lack of originality, a facet which he said "can be shown no less convincingly by a composer's handling of material than by its invention".
"Egdon Heath" (1927) was Holst's first major orchestral work after "The Planets". Matthews summarises the music as "elusive and unpredictable; three main elements: a pulseless wandering melody [for strings], a sad brass processional, and restless music for strings and oboe." The mysterious dance towards the end is, says Matthews, "the strangest moment in a strange work". Richard Greene in "Music & Letters" describes the piece as "a larghetto dance in a siciliano rhythm with a simple, stepwise, rocking melody", but lacking the power of "The Planets" and, at times, monotonous to the listener. A more popular success was the "Moorside Suite" for brass band, written as a test piece for the National Brass Band Festival championships of 1928. While written within the traditions of north-country brass band music, the suite, Short says, bears Holst's unmistakable imprint, "from the skipping 6/8 of the opening Scherzo, to the vigorous melodic fourths of the concluding March, the intervening Nocturne bearing a family resemblance to the slow-moving procession of "Saturn"".
After this, Holst tackled his final attempt at opera in a cheerful vein, with "The Wandering Scholar" (1929–30), to a text by Clifford Bax. Imogen refers to the music as "Holst at his best in a scherzando (playful) frame of mind"; Vaughan Williams commented on the lively, folksy rhythms: "Do you think there's a "little" bit too much 6/8 in the opera?" Short observes that the opening motif makes several reappearances without being identified with a particular character, but imposes musical unity on the work.
Holst composed few large-scale works in his final years. "A Choral Fantasia" of 1930 was written for the Three Choirs Festival at Gloucester; beginning and ending with a soprano soloist, the work, also involving chorus, strings, brass and percussion, includes a substantial organ solo which, says Imogen Holst, "knows something of the 'colossal and mysterious' loneliness of "Egdon Heath"". Apart from his final uncompleted symphony, Holst's remaining works were for small forces; the eight "Canons" of 1932 were dedicated to his pupils, though in Imogen's view that they present a formidable challenge to the most professional of singers. The "Brook Green Suite" (1932), written for the orchestra of St Paul's School, was a late companion piece to the "St Paul's Suite". The "Lyric Movement" for viola and small orchestra (1933) was written for Lionel Tertis. Quiet and contemplative, and requiring little virtuosity from the soloist, the piece was slow to gain popularity among violists. Robin Hull, in "Penguin Music Magazine", praised the work's "clear beauty—impossible to mistake for the art of any other composer"; in Dickinson's view, however, it remains "a frail creation". Holst's final composition, the orchestral scherzo movement of a projected symphony, contains features characteristic of much of Holst's earlier music—"a summing up of Holst's orchestral art", according to Short. Dickinson suggests that the somewhat casual collection of material in the work gives little indication of the symphony that might have been written.
Recordings.
Holst made some recordings, conducting his own music. For the Columbia company he recorded "Beni Mora", the "Marching Song" and the complete "Planets" with the London Symphony Orchestra (LSO) in 1922, using the acoustic process. The limitations of early recording prevented the gradual fade-out of women's voices at the end of "Neptune", and the lower strings had to be replaced by a tuba to obtain an effective bass sound. With an anonymous string orchestra Holst recorded the "St Paul's Suite" and "Country Song" in 1925. Columbia's main rival, HMV, issued recordings of some of the same repertoire, with an unnamed orchestra conducted by Albert Coates. When electrical recording came in, with dramatically improved recording quality, Holst and the LSO re-recorded "The Planets" for Columbia in 1926.
In the early LP era little of Holst's music was available on disc. Only six of his works are listed in the 1955 issue of "The Record Guide": "The Planets" (recordings under Boult on HMV and Nixa, and another under Sir Malcolm Sargent on Decca); the "Perfect Fool" ballet music; the "St Paul's Suite"; and three short choral pieces. In the stereo LP and CD eras numerous recordings of "The Planets" were issued, performed by orchestras and conductors from round the world. By the early years of the 21st century most of the major and many of the minor orchestral and choral works had been issued on disc. The 2008 issue of "The Penguin Guide to Recorded Classical Music" contained seven pages of listings of Holst's works on CD. Of the operas, "Savitri", "The Wandering Scholar", and "At the Boar's Head" have been recorded.
Legacy.
"[Holst's] influence is lasting in the work of all of us who value directness and sincerity and who view music not so much a secret preserve for the leisured few as a vital part of everyday life"
 "A tribute from Edmund Rubbra"
Warrack emphasises that Holst acquired an instinctive understanding—perhaps more so than any English composer—of the importance of folksong. In it he found "a new concept not only of how melody might be organized, but of what the implications were for the development of a mature artistic language". Holst did not found or lead a school of composition; nevertheless, he exercised influences over both contemporaries and successors. According to Short, Vaughan Williams described Holst as "the greatest influence on my music", although Matthews asserts that each influenced the other equally. Among later composers, Michael Tippett is acknowledged by Short as Holst's "most significant artistic successor", both in terms of compositional style and because Tippett, who succeeded Holst as director of music at Morley College, maintained the spirit of Holst's music there. Of an early encounter with Holst, Tippett later wrote: "Holst seemed to look right inside me, with an acute spiritual vision". Kennedy observes that "a new generation of listeners ... recognized in Holst the fount of much that they admired in the music of Britten and Tippett". Holst's pupil Edmund Rubbra acknowledged how he and other younger English composers had adopted Holst's economy of style: "With what enthusiasm did we pare down our music to the very bone".
Short cites other English composers who are in debt to Holst, in particular William Walton and Benjamin Britten, and suggests that Holst's influence may have been felt further afield. Above all, Short recognises Holst as a composer for the people, who believed it was a composer's duty to provide music for practical purposes—festivals, celebrations, ceremonies, Christmas carols or simple hymn tunes. Thus, says Short, "many people who may never have heard any of [Holst's] major works ... have nevertheless derived great pleasure from hearing or singing such small masterpieces as the carol 'In the Bleak Midwinter'".
On 27 September 2009, after a weekend of concerts at Chichester Cathedral in memory of Holst, a new memorial was unveiled to mark the 75th anniversary of the composer's death. It is inscribed with words from the text of "The Hymn of Jesus": "The heavenly spheres make music for us". In April 2011 a BBC television documentary, "Holst: In the Bleak Midwinter", charted Holst's life with particular reference to his support for socialism and the cause of working people.
Notes and references.
Notes
References

</doc>
<doc id="49242" url="http://en.wikipedia.org/wiki?curid=49242" title="Irina Privalova">
Irina Privalova

Irina Anatoljewna Privalova (Russian: Ирина Анатольевна Привалова; née Sergeyeva on 22 November 1968) is a Russian athlete who has won gold at the Olympics.
She first competed in the sprint events, winning two Olympic medals in the 100 m and 200 m in 1992 whilst representing the Unified Team. Irina Privalova had been a formidable competitor during most of the 1990s but had not yet won an outdoor world championship gold medal. In 2000, she gambled successfully and switched to the 400 m hurdles discipline winning the Olympic title in Sydney 2000 in 53.02 s and a bronze in the 4 x 400 m relay team for Russia. It has been suggested that this change at the late age of 31 was because her chances of defeating Marion Jones (the overwhelming favourite for the 100 m/200 m double) were slim and the 400 m was also a repeat showdown between Marie-José Pérec and Cathy Freeman from the 1996 Atlanta Games. (That repeat showdown would not happen after Pérec left the Sydney games.)
Irina Privalova is currently the world indoor record holder in the 50 m (5.96 s), 60 m (6.92 s) sprints. She has also been the world indoor champion at the 60 m (7.02 s in 1991), 200 m (22.15 s in 1993), and 400 m (50.23 s in 1995) events.
Privalova is still running, achieving her best time in the 100 m for nine years in 2008.
References.
<br>
<br>

</doc>
<doc id="49243" url="http://en.wikipedia.org/wiki?curid=49243" title="The Planets">
The Planets

The Planets, Op. 32, is a seven-movement orchestral suite by the English composer Gustav Holst, written between 1914 and 1916. Each movement of the suite is named after a planet of the Solar System and its corresponding astrological character as defined by Holst.
From its premiere to the present day, the suite has been enduringly popular, influential, widely performed and frequently recorded. The work was not heard in a complete public performance, however, until some years after it was completed. Although there were four performances between September 1918 and October 1920, they were all either private (the first performance, in London) or incomplete (two others in London and one in Birmingham). The premiere was at the Queen's Hall on 29 September 1918, conducted by Holst's friend Adrian Boult before an invited audience of about 250 people. The first complete public performance was finally given in London by Albert Coates conducting the London Symphony Orchestra on 15 November 1920.
Background.
The concept of the work is astrological rather than astronomical (which is why Earth is not included): each movement is intended to convey ideas and emotions associated with the influence of the planets on the psyche, not the Roman deities. The idea of the work was suggested to Holst by Clifford Bax, who introduced him to astrology when the two were part of a small group of English artists holidaying in Majorca in the spring of 1913; Holst became quite a devotee of the subject, and would cast his friends' horoscopes for fun. Holst also used Alan Leo's book "What is a Horoscope?" as a springboard for his own ideas, as well as for the subtitles (i.e., "The Bringer of...") for the movements.
When composing "The Planets" Holst initially scored the work for piano duet, except for "Neptune", which was scored for a single organ, as Holst believed that the sound of the piano was too percussive for a world as mysterious and distant as Neptune. Holst then scored the suite for a large orchestra, in which form it became enormously popular. Holst's use of orchestration was very imaginative and colourful, showing the influence of such contemporary composers as Igor Stravinsky and Arnold Schoenberg, as well as such late Russian romantics as Nikolai Rimsky-Korsakov and Alexander Glazunov. Its novel sonorities helped make the work an immediate success with audiences at home and abroad. Although "The Planets" remains Holst's most popular work, the composer himself did not count it among his best creations and later in life complained that its popularity had completely surpassed his other works. He was, however, partial to his own favourite movement, "Saturn".
Premieres.
Just before the Armistice, Gustav Holst burst into my office: "Adrian, the YMCA are sending me to Salonika quite soon and Balfour Gardiner, bless his heart, has given me a parting present consisting of the Queen's Hall, full of the Queen's Hall Orchestra for the whole of a Sunday morning. So we're going to do "The Planets", and you've got to conduct." 
”
Adrian Boult
The orchestral premiere of "The Planets" suite, conducted at Holst's request by Adrian Boult, was held at short notice on 29 September 1918, during the last weeks of World War I, in the Queen's Hall with the financial support of Holst's friend and fellow composer H. Balfour Gardiner. It was hastily rehearsed; the musicians of the Queen's Hall Orchestra first saw the complicated music only two hours before the performance, and the choir for "Neptune" was recruited from pupils from St Paul's Girls' School (where Holst taught). It was a comparatively intimate affair, attended by around 250 invited associates, but Holst regarded it as the public premiere, inscribing Boult's copy of the score, "This copy is the property of Adrian Boult who first caused the Planets to shine in public and thereby earned the gratitude of Gustav Holst."
A public concert was given in London under the auspices of the Royal Philharmonic Society on 27 February 1919, conducted by Boult. Five of the seven movements were played in the order Mars, Mercury, Saturn, Uranus, and Jupiter. It was Boult's decision not to play all seven movements at this concert. He felt that when the public were being given a totally new language like that, "half an hour of it was as much as they could take in". The anonymous critic in Hazell's Annual called it "an extraordinarily complex and clever suite". At a Queen's Hall symphony concert on 22 November of that year, Holst conducted Venus, Mercury and Jupiter (this was the first public performance of Venus). There was another incomplete public performance, in Birmingham, on 10 October 1920, with five movements (Mars, Venus, Mercury, Saturn and Jupiter). It is not clear whether this performance was conducted by Appleby Matthews or the composer.
His daughter Imogen recalled, "He hated incomplete performances of "The Planets", though on several occasions he had to agree to conduct three or four movements at Queen's Hall concerts. He particularly disliked having to finish with Jupiter, to make a 'happy ending', for, as he himself said, 'in the real world the end is not happy at all'".
The first complete performance of the suite at a public concert did not occur until 15 November 1920; the London Symphony Orchestra (LSO) was conducted by Albert Coates. This was the first time the movement "Neptune" had been heard in a public performance, all the other movements having been given earlier public airings.
The composer conducted a complete performance for the first time on 13 October 1923, with the Queen's Hall Orchestra at a Promenade Concert. Holst conducted the LSO in two recorded performances of "The Planets": the first was an acoustic recording made in sessions between 1922 and 1924 (now available on Pavilion Records' Pearl label); the second was made in 1926, and utilised the then-new electrical recording process (in 2003, this was released on compact disc by IMP and later on Naxos outside the United States). Because of the time constraints of the 78rpm format, the tempi are often much faster than is usually the case today.
Instrumentation.
The work is scored for a large orchestra consisting of four flutes (third doubling first piccolo and fourth doubling second piccolo and "bass flute in G", actually an alto flute), three oboes (third doubling bass oboe), one English horn, three clarinets in B-flat and A, one bass clarinet in B-flat, three bassoons, one contrabassoon; six horns in F, four trumpets in C, three trombones, one tenor tuba in B-flat (actually a euphonium scored for treble clef), one bass tuba; a percussion section with six timpani (requiring two players), bass drum, snare drum, cymbals, triangle, tam-tam, tambourine, glockenspiel, xylophone, tubular bells; celesta, pipe organ; 2 harps and strings. In "Neptune", two three-part women's choruses (S S A) located in an adjoining room which is to be screened from the audience are added.
Structure.
The suite has seven movements, each named after a planet and its corresponding astrological character (see Planets in astrology):
Holst's original title, as seen on the handwritten full score, was "Seven Pieces for Large Orchestra". Holst almost certainly attended an early performance of Schoenberg's "Five Pieces for Orchestra" in 1914 (the year he wrote "Mars", "Venus" and "Jupiter"), and owned a score of it, the only Schoenberg score he ever owned. Each movement of Holst's work was originally called only by the second part of each title (I "The Bringer of War", II "The Bringer of Peace" and so on); the present titles were added in time for the first (incomplete) public performance in September 1919, though they were never added to the original score. 
A typical performance of all seven movements is about fifty minutes long, though Holst's own electric recording from 1926 is just over forty-two and a half minutes. 
One explanation for the suite's structure, presented by Holst scholar Raymond Head, is the ruling of astrological signs of the zodiac by the planets: if the signs are listed along with their ruling planets in the traditional order starting with Aries, ignoring duplication and the luminaries (the Sun and Moon), the order of the movements corresponds. Critic David Hurwitz offers an alternative explanation for the piece's structure: that "Jupiter" is the centrepoint of the suite and that the movements on either side are in mirror images. Thus "Mars" involves motion and "Neptune" is static; "Venus" is sublime while "Uranus" is vulgar, and "Mercury" is light and "scherzando" while "Saturn" is heavy and plodding. This hypothesis is lent credence by the fact that the two outer movements, "Mars" and "Neptune", are both written in rather unusual quintuple meter.
Holst suffered neuritis in his right arm, which caused him to seek help from several amanuenses in scoring "The Planets". This is clear from the number of different hands apparent in the full score.
"Neptune" was one of the first pieces of orchestral music to have a fade-out ending, although several composers (including Joseph Haydn in the finale of his Farewell Symphony) had achieved a similar effect by different means. Holst stipulates that the women's choruses are "to be placed in an adjoining room, the door of which is to be left open until the last bar of the piece, when it is to be slowly and silently closed", and that the final bar (scored for choruses alone) is "to be repeated until the sound is lost in the distance". Although commonplace today, the effect bewitched audiences in the era before widespread recorded sound—after the initial 1918 run-through, Holst's daughter Imogen (in addition to watching the charwomen dancing in the aisles during "Jupiter") remarked that the ending was "unforgettable, with its hidden chorus of women's voices growing fainter and fainter... until the imagination knew no difference between sound and silence".
Additions by other composers.
Several attempts have been made, for a variety of reasons, to append further music to Holst's suite, though by far the most common presentation of the music in the concert hall and on record remains Holst's original seven-movement version.
Pluto.
Pluto was discovered in 1930, four years before Holst's death, and was hailed by astronomers as the ninth planet. Holst, however, expressed no interest in writing a movement for the new planet. He had become disillusioned by the popularity of the suite, believing that it took too much attention away from his other works. 
In the March 1972 final broadcast of his Young People's Concerts series, conductor Leonard Bernstein led the New York Philharmonic through a fairly straight interpretation of the suite, though Bernstein discarded the Saturn movement because he thought the theme of old age was irrelevant to a concert for children. The broadcast concluded with an improvised performance he called "Pluto, the Unpredictable." The March 26, 1972 performance may be viewed on the Kultur DVD set. 
In 2000, the Hallé Orchestra commissioned the English composer Colin Matthews, an authority on Holst, to write a new eighth movement, which he called "Pluto, the Renewer". Dedicated to the late Imogen Holst, Gustav Holst's daughter, it was first performed in Manchester on 11 May 2000, with Kent Nagano conducting the Hallé Orchestra. Matthews also changed the ending of "Neptune" slightly so that movement would lead directly into "Pluto".
On 24 August 2006, the International Astronomical Union (IAU) defined what it means to be a "planet" within the Solar System. This definition excluded Pluto as a planet and added it as a member of the new category "dwarf planet", along with Eris and Ceres.
"Asteroids".
In 2006, the Berlin Philharmonic, with Sir Simon Rattle and EMI Classics, commissioned four composers (Kaija Saariaho, Matthias Pintscher, Mark-Anthony Turnage, and Brett Dean) and recorded an additional, four-movement suite based on asteroids in the Solar System. The four movements were:
Adaptations of "The Planets".
Hymns.
Holst adapted the melody of the central section of "Jupiter" in 1921 to fit the metre of a poem beginning "I Vow to Thee, My Country". As a hymn tune it has the title "Thaxted", after the town in Essex where Holst lived for many years, and it has also been used for other hymns, such as "O God beyond all praising". It is by far the best-known melody of the suite.
"I Vow to Thee, My Country" was written between 1908 and 1918 by Sir Cecil Spring Rice and became known as a response to the human cost of World War I. The hymn was first performed in 1925 and quickly became a patriotic anthem. Although Holst had no such patriotic intentions when he originally composed the music, these adaptations have encouraged others to draw upon the score in similar ways throughout the 20th Century.
Notes and references.
Notes
References

</doc>
<doc id="49244" url="http://en.wikipedia.org/wiki?curid=49244" title="NaN">
NaN

In computing, NaN, standing for not a number, is a numeric data type value representing an undefined or unrepresentable value, especially in floating-point calculations. Systematic use of NaNs was introduced by the IEEE 754 floating-point standard in 1985, along with the representation of other non-finite quantities like infinities.
Two separate kinds of NaNs are provided, termed quiet NaNs and signaling NaNs. Quiet NaNs are used to propagate errors resulting from invalid operations or values, whereas signaling NaNs can support advanced features such as mixing numerical and symbolic computation or other extensions to basic floating-point arithmetic. For example, 0/0 is undefined as a real number, and so represented by NaN; the square root of a negative number is imaginary, and thus not representable as a real floating-point number, and so is represented by NaN; and NaNs may be used to represent missing values in computations.
Floating point.
In floating-point calculations, NaN is not the same as infinity, although both are typically handled as special cases in floating-point representations of real numbers as well as in floating-point operations. An invalid operation is also not the same as an arithmetic overflow (which might return an infinity) or an arithmetic underflow (which would return the smallest normal number, a denormal number, or zero).
IEEE 754 NaNs are represented with the exponent field filled with ones (like infinity values), and some non-zero number in the significand (to make them distinct from infinity values); this representation allows the definition of multiple distinct NaN values, depending on which bits are set in the significand, but also on the value of the leading sign bit (not all applications are required to provide distinct semantics for those distinct NaN values).
For example, a bit-wise IEEE floating-point standard single precision (32-bit) NaN would be: s111 1111 1xxx xxxx xxxx xxxx xxxx xxxx where "s" is the sign (most often ignored in applications) and "x" is non-zero (the value zero encodes infinities). Some bits from "x" (usually and preferably the first one) are used to determine the type of NaN: quiet NaN or signaling NaN. The remaining bits encode a payload (most often ignored in applications).
Floating point operations other than ordered comparisons normally propagate a quiet NaN ("qNaN"). Floating point operations on a signaling NaN ("sNaN") signal an invalid operation exception, the default exception action is then the same as for qNaN operands and they produce a qNaN if producing a floating point result.
A comparison with a NaN always returns an "unordered result" even when comparing with itself. The comparison predicates are either signaling or non-signaling; the signaling versions signal an invalid exception for such comparisons. The equality and inequality predicates are non-signaling so "x" = "x" returning false can be used to test if "x" is a quiet NaN. The other standard comparison predicates are all signaling if they receive a NaN operand, the standard also provides non-signaling versions of these other predicates. The predicate "isNaN(x)" determines if a value is a NaN and never signals an exception, even if "x" is a signaling NaN.
The propagation of quiet NaNs through arithmetic operations allows errors to be detected at the end of a sequence of operations without extensive testing during intermediate stages. However, note that depending on the language and the function, NaNs can silently be removed in expressions that would give a constant result for all other floating-point values e.g. NaN^0, which may be defined as 1, so in general a later test for a set INVALID flag is needed to detect all cases where NaNs are introduced (see section Function definition below for further details).
In section 6.2 of the revised IEEE 754-2008 standard there are two anomalous functions (the maxnum and minnum functions that return the maximum of two operands that are expected to be numbers) that favor numbers — if just one of the operands is a NaN then the value of the other operand is returned.
The for GNU Octave and MATLAB skips all NaNs when computing aggregates (like averages, standard deviations, etc.). NaNs are "assumed" to represent missing values and so the statistical functions ignore NaNs in the data instead of propagating them.
Operations generating NaN.
There are three kinds of operations that can return NaN:
NaNs may also be explicitly assigned to variables, typically as a representation for missing values. Prior to the IEEE standard, programmers often used a special value (such as −99999999) to represent undefined or missing values, but there was no guarantee that they would be handled consistently or correctly.
NaNs are not necessarily generated in all the above cases. If an operation can produce an exception condition and traps are not masked then the operation will cause a trap instead. If an operand is a quiet NaN, and there isn't also a signaling NaN operand, then there is no exception condition and the result is a quiet NaN. Explicit assignments will not cause an exception even for signaling NaNs.
Quiet NaN.
Quiet NaNs, or qNaNs, do not raise any additional exceptions as they propagate through most operations. The exceptions are where the NaN cannot simply be passed through unchanged to the output, such as in format conversions or certain comparison operations (which do not "expect" a NaN input).
Signaling NaN.
Signaling NaNs, or sNaNs, are special forms of a NaN that when consumed by most operations should raise an invalid exception and then, if appropriate, be "quieted" into a qNaN that may then propagate. They were introduced in IEEE 754. There have been several ideas for how these might be used:
When encountered a trap handler could decode the sNaN and return an index to the computed result. In practice this approach is faced with many complications. The treatment of the sign bit of NaNs for some simple operations (such as absolute value) is different from that for arithmetic operations. Traps are not required by the standard. There are other approaches to this sort of problem that would be more portable.
Function definition.
There are differences of opinion about the proper definition for the result of a numeric function that receives a quiet NaN as input. One view is that the NaN should propagate to the output of the function in all cases to propagate the indication of an error. Another view, and the one taken by the IEEE standard in general, is that if the function has multiple arguments and the output is uniquely determined by all the non-NaN inputs including infinity, then that value should be the result. Thus for example the value returned by hypot(±∞, qNaN) and hypot(qNaN, ±∞) is +∞.
The problem is particularly acute for the exponentiation function pow(x,y) = xy. The expressions 00, ∞0 and 1∞ are considered indeterminate forms when they occur as limits (just like ∞ × 0), and the question of whether zero to the zero power should be defined as 1 has divided opinion.
If the output is considered as undefined if a parameter is undefined then pow(1,qNaN) should produce a qNaN. However typically math libraries have returned 1 for pow(1,y) for any real number y, and even if y is infinity or -infinity. Similarly they produce 1 for pow(x,0) even when x is 0 or infinity. The rationale for returning the value 1 for the indeterminate forms was that the value of functions at singular points can be taken as a particular value if that value is in the limit the value for all but a vanishingly small part of a ball around the limit value of the parameters. The 2008 version of the IEEE 754 standard says that pow(1,qNaN) and pow(qNaN,0) should both return 1 since they return 1 whatever else is used instead of quiet NaN.
To satisfy those wishing a more strict interpretation of how the power function should act, the 2008 standard defines two additional power functions; pown(x, n) where the exponent must be an integer, and powr(x, y) which returns a NaN whenever a parameter is a NaN or the exponentiation would give an indeterminate form.
Integer NaN.
Most fixed sized integer formats do not have any way of explicitly indicating invalid data.
Perl's BigInt package uses "NaN" for the result of strings that don't represent valid integers.
Display.
Different operating systems and programming languages may have different string representations of NaN.
 nan
 NaN
 NaN%
 NAN
 NaNQ
 NaNS
 qNaN
 sNaN
 1.#SNAN
 1.#QNAN
 -1.#IND
Since, in practice, encoded NaNs have both a sign and optional 'diagnostic information' (sometimes called a "payload"), these will often be found in string representations of NaNs, too, for example:
 -NaN
 NaN12345
 -sNaN12300
 -NaN(s1234)
Encoding.
In IEEE 754 standard-conforming floating point storage formats, NaNs are identified by specific, pre-defined bit patterns unique to NaNs. The sign bit does not matter. Binary format NaNs are represented with the exponential field filled with ones (like infinity values), and some non-zero number in the significand (to make them distinct from infinity values). The original IEEE 754 standard from 1985 (IEEE 754-1985) only described binary floating point formats, and did not specify how the signaled/quiet state was to be tagged. In practice, the most significant bit of the significand determined whether a NaN is signalling or quiet. Two different implementations, with reversed meanings, resulted.
The 2008 revision of the IEEE 754 standard (IEEE 754-2008) makes formal recommendations for the encoding of the signaled/quiet bit.
The state/value of the remaining bits (i.e. other than the ones used to identify a NaN as NaN, including the quiet/signaled bits) are not defined by the standard except that they must not be all zero. This value is called the 'payload' of the NaN. If an operation has a single NaN input and propagates it to the output, the result NaN's payload should be that of the input NaN. If there are multiple NaN inputs, the result NaN's payload should be from one of the input NaNs; the standard does not specify which.

</doc>
<doc id="49246" url="http://en.wikipedia.org/wiki?curid=49246" title="Biological determinism">
Biological determinism

"Biological determinism" is a term used in some literature to describe the belief that human behavior is controlled solely by an individual's genes or some component of physiology.
In context.
Gender assignment.
Lynda Birke's "In Pursuit of Difference" argues that the discipline of human biology often presents "clear-cut differences" between sexes with regards to chromosomes, genetics, and inheritance. However, while obvious physical differences between males and females exist and develop during puberty, hormonal differences are "not absolute". There is a broad range of reproductive anatomy that doesn't necessarily fit the "gender definition" of male or female. According to the Intersex Society of North America, "a person may be born with mosaic genetics", differing in their chromosomal configuration.
Homosexuality.
Though scientists are unsure as to whether homosexuality can be attributed to biological or social factors, LGBT rights activists have used the theories of biological determinism to support their cause. This has become a frequent point of dissension between pro-gay individuals and anti-gay individuals. Because a single cause has not been determined as the cause of homosexuality, many scholars theorize that a combination of biological and social causes determine one's sexual orientation. Gay rights advocates believe that proving that homosexuality has a definite biological basis will prove it to be an unchangeable characteristic, thus allowing homosexuals to be protected under the Fourteenth Amendment. One area of research that has been a valuable tool for gay rights activists has been Dean Hamer's work studying the "gay gene". Another researcher who worked with Hamer in finding evidence for biological influence in male homosexuality was Simon LeVay, a neuroscientist. In 1991, LeVay published an article in "Science" journal that detailed the difference in hypothalamic structures between homosexual and heterosexual men. His findings in studying the INAH-3 implied that "sexual orientation has a biological substrate". Though his research showed that there was a biological basis in sexual orientation, LeVay cautioned against people interpreting his article to say that he found that homosexuality is genetic, emphasizing that he had not "locate a gay center in the brain--[as] INAH3 is less likely to be the sole gay nucleus of the brain than part of a chain of nuclei engaged in men and women's sexual behavior." He merely hoped that his work would serve as a catalyst in working towards finding more evidence that homosexuality is at least partly genetic.
Racism.
Nina Jablonski, a professor of anthropology at Pennsylvania State University, notes that while some people assert that race is a social construct, racist beliefs that one's skin color is somehow associated with one's moral, social, and intellectual characteristics persist. Although there is "no scientific evidence to support substantial differences between groups", the belief that one's race makes one innately superior over another endures as an unavoidable influence in today's world. Racism that stems from the belief of biological determinism appears to be detrimental to both parties, according to Jablonski. For the person with the racist ideals, it often plants the idea into their head that their own race is inarguably superior in every aspect and for the race being targeted, it puts into their mind the idea that they are somehow inferior, weaker, or stupider. This categorization “becomes determinative of personality and individual experience, and is itself a destination.
Sexism.
The evolutionary biologist Richard Lewontin, in his 1984 book "Not in Our Genes", explored how biological determinism worked in the scientific and academic realm. He argued against the view that determinism had a logical basis in differences such as that men statistically have larger brains, are stronger, are more likely to hold higher positions in the work place, and are recognized more often for their academic contributions. Lewontin argued that these statements for the most part went undisputed, and were viewed as being dependent on "underlying biological differences between males and females at the level of brain structure".
In "Nineteenth-century craniology: the study of the female skull", Elizabeth Fee, a historian of health and medicine discusses what many anthropologists of the 1860s viewed as a "social problem". In a time where the women's rights movement was viewed as a legitimate hazard, anthropologists of the Anthropological Society set out to undermine gender equality in the educational and scientific realm. They believed that women were assigned a specific role in nature and should never stray from that role. This role was motherhood, to which all women were "biologically destined." The Anthropological Society emphasized that women were to wholly accept and embrace this role because motherhood was supposedly completely "incompatible with intellectual pretension, economic competition, or the vote."
Sociobiology.
Sociobiology emerged in the 1970s with E. O. Wilson's book "". The existence of a putative altruism gene is debated. Kaplan and Rogers claim "Most sociobiologists agree that no such gene could exist for so long in a population as it would soon be lost because it would not compete successfully against the 'selfish' genes", and argue that "Genes and environment are not discrete opposites; they are both entirely integrated aspects of the developmental process." Consequently, genes can’t be "selfish", as "genes are expressed as biochemical processes; behavior is expressed by the whole organism."
Social construction.
Richard Lewontin, Steven Rose, and Leon Kamin were interested in the way that biological determinism was present in science. They wanted to figure out how much of it was true, and how much of it was socially constructed according to certain beliefs and societal norms and determined gender roles within society. In their book "Not in Our Genes" they explore the possibilities of biological determinism. In their studies, they found some very interesting evidence that points to the fact that biological determinism in science is actually greatly affected by certain norms and tendencies within society. According to them, biological determinism is more constructed by society than by anything else. In a study that was performed on girls who were relatively "masculinized", biological determinists John Money and Anke Ehrhardt looked for ways to describe femininity that fit into the common definition of it, such as clothing preference or using makeup. Although these scientists believed that they were providing evidence to support their definitions of femininity within nature, they fell into the trap of labeling these girls according to Western social standards. As Lewontin points out, this experiment not only embraces the stereotypes that already existed, but it also “ignores the existence of societies in which women wear pants, or in which men wear skirts, or in which men enjoy and appropriate jewelry to themselves.” Lewontin, Rose, and Kamin realize that biological determinism is clouded and, can in fact, be shaped according to the standards and norms of the society one lives in. Therefore, they choose to take a different approach. They decide to look at numbers and statistics instead of simple social experiments which can be easily misinterpreted. When they look at the numbers and statistics of men and women over the years, they discover that the differences between men and women are no longer as pronounced as they had been in the past. All of a sudden, there are more women in the work place holding higher ranking jobs. More women are excelling in areas that used to be male dominant, such as sports. And, even biologically, women are beginning to catch up to men in height while men are beginning to catch up in life expectancy. However, these changes are mostly visible in numbers and statistics, and social differences between men and women are still easily observed. Lewontin, Rose, and Kamin argue, however, that these differences are imposed by society itself.
The standard model for the difference between sex/gender states that there is a clear-cut dichotomy between males & females, with no overlap. Because of this norm, we have a historically constructed viewpoint of “average”, meaning that society holds the idea that one must be either male or female, feminine or masculine. Anne Fausto-Sterling's article “Of Gender and Genitals” discusses how this standard model shapes doctors’ ideas about gender and what is socially acceptable. She claims that (according to the standard model) “Bodies in the "normal" range are culturally intelligible as males or females, but the rules for living as male or female are strict”, meaning that we are culturally “trained” in believing that there is a sexual binary and anything outside of those confines is rejected.
The pipeline and children's toys.
Londa Schiebinger in "Has Feminism Changed Science?" describes the existing gender issue of the pipeline, in which "if more girls entered the educational end of the pipeline, more women would be turned into credentialed specialists and empty into the science job pool". Simply put, a much higher percentage of women need to be placed in the science and mathematical fields to produce an end result of a couple females who persisted and attained their PhD's in their male dominated fields. As for men, the amount of men who pursue within the scientific field, most likely carry through and a higher percent of males attain their professional degrees. Males outnumber females in scientific and mathematical realms due to their gender assignment. Schiebinger elaborates, "Factors that will lead girls to reject science as a career are thought to be cultivated very early-even moments after birth. In one study, parents were asked to describe their newborn babies-at a time when one of the few things they knew about the child was its sex." Parents described their male offspring as adventurous and observatory, as for female offspring, they were noted to be more fragile. Parents encourage their boys to conquer, enlighten, and provide the boost of ego to be successful within the scientific field. Simultaneously, parents treatment towards their female child carries the responsibility of teaching them homemaking skills and to abstain from "rough-and-tough play". Schiebinger notes, "adults tend to give children toys that reinforce sexual stereotypes". Girls are given dolls and encouraged to voice out their emotions, while boys play with cars and balls. Society has molded the norm for children to have distinct modes of play and interaction, which leads to advertisements and manufacturers who "insist that toys be clearly gendered". Handing a toy to a child, depending on their gender may seem harmless at their young age, yet "toys create aspirations, hone conceptual skills, and encourage certain behaviors to the exclusion of others."

</doc>
<doc id="49248" url="http://en.wikipedia.org/wiki?curid=49248" title="Byron White">
Byron White

Byron Raymond White (June 8, 1917 – April 15, 2002) won fame both as a football halfback and as an associate justice of the Supreme Court of the United States. Born and raised in Colorado, White played in the National Football League for three seasons and practiced law for 15 years before his Supreme Court appointment. White was the Colorado state chair of John F. Kennedy's 1960 presidential campaign. 
White was appointed to the Supreme Court by Kennedy in 1962. He viewed his own court decisions as based on the facts of each case rather than as representative of a specific legal philosophy. He retired in 1993 and is the twelfth longest-serving justice in Supreme Court history. He died in Denver at the age of 84. He was the first Supreme Court Justice from the state of Colorado.
Education.
White was born in Fort Collins, Colorado, the son of Maude Elizabeth (Burger) and Alpha Albert White. He was raised in the nearby town of Wellington, Colorado, where he obtained his high school diploma in 1930. After graduating at the top of his high school class, White attended the University of Colorado at Boulder on a scholarship. He joined the Phi Gamma Delta fraternity and served as student body president his senior year. Graduating in 1938, he won a Rhodes Scholarship to the University of Oxford and, after having deferred it for a year to play football, he went on to attend Hertford College, Oxford.
Football.
White was an All-American football halfback for the Colorado Buffaloes of the University of Colorado at Boulder, where he acquired the nickname "Whizzer" from a newspaper columnist. The nickname would follow him throughout his later legal and Supreme Court career, to White's chagrin. He also played basketball and baseball. After graduation he signed with the NFL's Pittsburgh Pirates (now Steelers), playing there during the 1938 season. He led the league in rushing in his rookie season and became the game's highest-paid player.
After Oxford, White played for the Detroit Lions from 1940 to 1941. In three NFL seasons, he played in 33 games. He led the league in rushing yards in 1938 and 1940, and he was one of the first "big money" NFL players, making $15,000 a year.
His career was cut short when he entered the United States Navy during World War II; after the war, he elected to attend law school rather than return to football. He was elected to the College Football Hall of Fame in 1954.
Military service.
During World War II, White served as an intelligence officer in the United States Navy stationed in the Pacific Theatre. He had originally wanted to join the Marines but was kept out due to being colorblind. He wrote the intelligence report on the sinking of future President John F. Kennedy's "PT-109". White was awarded two Bronze Star medals.
Personal life.
White first met his wife Marion (died January 2009), the daughter of president of the University of Colorado, when she was in high school and he was a college football star. During World War II, Marion served in the WAVES while her future husband was a Navy intelligence officer. They married in 1946 and had two children: a son named Charles and a daughter named Nancy. At the time of his death, White and his wife had moved back to Colorado and were living in Denver.
Legal career.
After World War II, he attended Yale Law School, graduating "magna cum laude" in 1946. 
After serving as a law clerk to Chief Justice Fred Vinson, White returned to Denver.
White practiced in Denver for roughly fifteen years with the law firm now known as Davis Graham & Stubbs. This was a time in which the Denver business community flourished, and White rendered legal service to that flourishing community. White was for the most part a transactional attorney. He drafted contracts and advised insolvent companies, and he argued the occasional case in court.
During the 1960 presidential election, White put his football celebrity to use as chair of John F. Kennedy's campaign in Colorado. White had first met the candidate when White was a Rhodes scholar and Kennedy's father, Joseph Kennedy, was Ambassador to the Court of St. James. During the Kennedy administration, White served as United States Deputy Attorney General, the number two man in the Justice Department, under Robert F. Kennedy. He took the lead in protecting the Freedom Riders in 1961, negotiating with Alabama Governor John Malcolm Patterson.
Supreme Court.
Acquiring renown within the Kennedy Administration for his humble manner and sharp mind, he was appointed by Kennedy in 1962 to succeed Justice Charles Evans Whittaker, who retired for disability. Kennedy said at the time: "He has excelled at everything. And I know that he will excel on the highest court in the land." The 44-year-old White was approved by a voice vote. He would serve until his retirement in 1993. His Supreme Court tenure was the fourth-longest of the 20th century.
Upon the request of Vice President-Elect Al Gore, Justice White administered the oath of office on January 20, 1993 to the 45th U.S. Vice President. It was the only time White administered an oath of office to a Vice President.
During his service on the high court, White wrote 994 opinions. He was fierce in questioning attorneys in court, and his votes and opinions on the bench reflect an ideology that has been notoriously difficult for popular journalists and legal scholars alike to pin down. He was seen as a disappointment by some Kennedy supporters who wished he would have joined the more liberal wing of the court in its opinions on "Miranda v. Arizona" and "Roe v. Wade".
White often took a narrow, fact-specific view of cases before the Court and generally refused to make broad pronouncements on constitutional doctrine or adhere to a specific judicial philosophy. He preferred to take what he viewed as a practical approach to the law to one based in any legal philosophy. In the tradition of the New Deal, White frequently supported a broad view and expansion of governmental powers. He consistently voted against creating constitutional restrictions on the police, dissenting in the landmark 1966 case of "Miranda v. Arizona". In his dissent in that case he noted that aggressive police practices enhance the individual rights of law-abiding citizens. His jurisprudence has sometimes been praised for adhering to the doctrine of judicial restraint.
Substantive due process doctrine.
Frequently a critic of the doctrine of "substantive due process", which involves the judiciary reading substantive content into the term "liberty" in the Due Process Clause of the Fifth Amendment and Fourteenth Amendment, White's first published opinion as a Supreme Court Justice, a sole dissent in "Robinson v. California" (1962), foreshadowed his career-long distaste for the doctrine. In "Robinson", he criticized the remainder of the Court's unprecedented expansion of the Eighth Amendment's prohibition of "cruel and unusual punishment" to strike down a California law providing for civil commitment of drug addicts. He argued that the Court was "imposing its own philosophical predilections" on the state in this exercise of judicial power, although its historic "allergy to substantive due process" would never permit it to strike down a state's economic regulatory law in such a manner.
In the same vein, he dissented in the controversial 1973 case of "Roe v. Wade". But White voted to strike down a state ban on contraceptives in the 1965 case of "Griswold v. Connecticut", although he did not join the majority opinion, which famously asserted a "right of privacy" on the basis of the "penumbras" of the Bill of Rights. White and Justice William Rehnquist were the only dissenters from the Court's decision in "Roe", though White's dissent used stronger language, suggesting that "Roe" was "an exercise in raw judicial power" and criticizing the decision for "interposing a constitutional barrier to state efforts to protect human life." White, who usually adhered firmly to the doctrine of "stare decisis", remained a critic of "Roe" throughout his term on the bench.
White explained his general views on the validity of substantive due process at length in his dissent in "Moore v. City of East Cleveland":
The Judiciary, including this Court, is the most vulnerable and comes nearest to illegitimacy when it deals with judge-made constitutional law having little or no cognizable roots in the language or even the design of the Constitution. Realizing that the present construction of the Due Process Clause represents a major judicial gloss on its terms, as well as on the anticipation of the Framers, and that much of the underpinning for the broad, substantive application of the Clause disappeared in the conflict between the Executive and the Judiciary in 1930s and 1940s, the Court should be extremely reluctant to breathe still further substantive content into the Due Process clause so as to strike down legislation adopted by a State or city to promote its welfare. Whenever the Judiciary does so, it unavoidably pre-empts for itself another part of the governance of the country without express constitutional authority.
White parted company with Rehnquist in strongly supporting the Supreme Court decisions striking down laws that discriminated on the basis of sex, agreeing with Justice William J. Brennan in 1973's "Frontiero v. Richardson" that laws discriminating on the basis of sex should be subject to strict scrutiny. However, only three justices joined Brennan's plurality opinion in "Frontiero"; in later cases gender discrimination cases would be subjected to intermediate scrutiny (see "Craig v. Boren").
White wrote the majority opinion in "Bowers v. Hardwick" (1986), which upheld Georgia's anti-sodomy law against a substantive due process attack.
The Court is most vulnerable and comes nearest to illegitimacy when it deals with judge-made constitutional law having little or no cognizable roots in the language or design of the Constitution... There should be, therefore, great resistance to ... redefining the category of rights deemed to be fundamental. Otherwise, the Judiciary necessarily takes to itself further authority to govern the country without express constitutional authority.
White's opinion in "Bowers" typified White's fact-specific, deferential style of deciding cases: White's opinion treated the issue in that case as presenting only the question of whether homosexuals had a fundamental right to engage in sexual activity, even though the statute in "Bowers" potentially applied to heterosexual sodomy (see "Bowers", 478 U.S. 186, 188, n. 1. Georgia, however, conceded during oral argument that the law would be inapplicable to married couples under the precedent set forth in "Griswold v. Connecticut".). A year after White's death, "Bowers" was overruled in "Lawrence v. Texas" (2003).
Death penalty.
White took a middle course on the issue of the death penalty: he was one of five justices who voted in "Furman v. Georgia" (1972) to strike down several state capital punishment statutes, voicing concern over the arbitrary nature in which the death penalty was administered. The Furman decision ended capital punishment in the U.S. until 1977, when Gary Gilmore, who decided not to appeal his death sentence, was executed by firing squad. White, however, was not against the death penalty in all forms: he voted to uphold the death penalty statutes at issue in "Gregg v. Georgia" (1976), even the mandatory death penalty schemes struck down by the Court.
White accepted the position that the Eighth Amendment to the United States Constitution required that all punishments be "proportional" to the crime; thus, he wrote the opinion in "Coker v. Georgia" (1977), which invalidated the death penalty for rape of a 16-year-old married girl. However, his first reported Supreme Court decision was a dissent in "Robinson v. California" (1962), in which he criticized the Court for extending the reach of the Eighth Amendment. In "Robinson" the Court for the first time expanded the constitutional prohibition of “cruel and unusual punishments” from examining the nature of the punishment imposed and whether it was an uncommon punishment − as, for example, in the cases of flogging, branding, banishment, or electrocution − to deciding whether any punishment at all was appropriate for the defendant’s conduct. White said: “If this case involved economic regulation, the present Court's allergy to substantive due process would surely save the statute and prevent the Court from imposing its own philosophical predilections upon state legislatures or Congress.” Consistent with his view in "Robinson", White thought that imposing the death penalty on minors was constitutional, and he was one of the three dissenters in "Thompson v. Oklahoma" (1988), a decision that declared that the death penalty as applied to offenders below 16 years of age was unconstitutional as a cruel and unusual punishment.
Abortion.
Along with Justice William Rehnquist, White dissented in "Roe v. Wade" (the dissenting decision was in the companion case, "Doe v. Bolton"), castigating the majority for holding that the U.S. Constitution "values the convenience, whim or caprice of the putative mother more than the life or potential life of the fetus."
Civil rights.
White consistently supported the Court's post-"Brown v. Board of Education" attempts to fully desegregate public schools, even through the controversial line of forced busing cases. He voted to uphold affirmative action remedies to racial inequality in an education setting in the famous "Regents of the University of California v. Bakke" case of 1978. Though White voted to uphold federal affirmative action programs in cases such as "Metro Broadcasting, Inc. v. FCC", 497 U.S. 547 (1990) (later overruled by "Adarand Constructors v. Peña", 515 U.S. 200 (1995)), White voted to strike down an affirmative action plan regarding state contracts in "Richmond v. J.A. Croson Co." (1989).
White dissented in "Runyon v. McCrary" (1976), which held that federal law prohibited private schools from discriminating on the basis of race. White argued that the legislative history of Title 42 U.S.C. § 1981 (popularly known as the "Ku Klux Klan Act") indicated that the Act was not designed to prohibit private racial discrimination, but only state-sponsored racial discrimination (as had been held in the "Civil Rights Cases" of 1883). White was concerned about the potential far-reaching impact of holding private racial discrimination illegal, which if taken to its logical conclusion might ban many varied forms of voluntary self-segregation, including social and advocacy groups that limited their membership to blacks: "Whether such conduct should be condoned or not, whites and blacks will undoubtedly choose to form a variety of associational relationships pursuant to contracts which exclude members of the other race. Social clubs, black and white, and associations designed to further the interests of blacks or whites are but two examples". "Runyon" was essentially overruled by 1989's "Patterson v. McLean Credit Union", which itself was superseded by the Civil Rights Act of 1991.
Relationships with other justices.
White said he was most comfortable on Rehnquist's court. He once said of Earl Warren, "I wasn't exactly in his circle." On the Burger Court, the Chief Justice was fond of assigning important criminal procedure and individual rights opinions to White, because of his frequently conservative views on these questions.
Court operations and retirement.
White frequently urged the Supreme Court to consider cases when federal appeals courts were in conflict on issues of federal law, believing that a primary role of the Supreme Court was to resolve such conflicts. Thus, White voted to grant certiorari more often than many of his colleagues, and he wrote numerous opinions dissenting from denials of certiorari. After White (along with fellow Justice Harry Blackmun, who also took a liberal line in voting to grant certiorari) retired, the number of cases heard each session of the Court declined steeply.
White disliked the politics of Supreme Court appointments. At one point, he turned down future Justice Samuel Alito for a clerkship. He retired in 1993, during Bill Clinton's presidency, saying that "someone else should be permitted to have a like experience." Clinton appointed Justice Ruth Bader Ginsburg, a judge from the Court of Appeals for the D.C. Circuit and a former Columbia University law professor, to succeed him.
Later years and death.
After retiring from the Supreme Court, White occasionally sat with lower federal courts. He maintained chambers in the federal courthouse in Denver until shortly before his death. He also served for the Commission on Structural Alternatives for the Federal Courts of Appeals.
White died of pneumonia on April 15, 2002 at the age of 84. He was the last living Warren Court Justice, and died the day before the fortieth anniversary of his swearing in as a Justice. From his death until the retirement of Sandra Day O'Connor, there were no living former Justices.
His remains are interred at All Souls Walk at the St. John's Cathedral in Denver.
Then-Chief Justice Rehnquist said White "came as close as anyone I have known to meriting Matthew Arnold's description of Sophocles: 'He saw life steadily and he saw it whole.' All of us who served with him will miss him."
Awards and honors.
The NFL Players Association gives the Byron "Whizzer" White NFL Man of the Year Award to one player each year for his charity work. Michael McCrary, who was involved in "Runyon v. McCrary", grew up to be a professional football player and won the award in 2000.
The federal courthouse in Denver that houses the Tenth Circuit is named after White.
White was posthumously awarded the Presidential Medal of Freedom in 2003 by President George W. Bush.
White was inducted into the Rocky Mountain Athletic Conference Hall of Fame on July 14, 2007, in addition to being a member of the College Football Hall of Fame and the University of Colorado's Athletic Hall of Fame, where he is enshrined as "The Greatest Buff Ever".
One of White's former law clerks, Dennis J. Hutchinson, wrote an unofficial biography of him called "The Man Who Once was Whizzer White".

</doc>
<doc id="49251" url="http://en.wikipedia.org/wiki?curid=49251" title="Dili">
Dili

Dili (Portuguese: "Díli") is the capital, largest city, chief port and commercial center of East Timor.
Geography and administration.
Dili lies on the northern coast of Timor island, the easternmost of the Lesser Sunda Islands. It is the seat of the administration of the district of Dili, which is the administrative entity of the area and includes the island of Atauro and some cities close to Dili city. The city is divided into the subdistricts of Nain Feto, Vera Cruz, Dom Aleixo and Cristo Rei and is divided into several sucos, which are headed by an elected "chefe de suco". 18 of the 26 sucos of the four subdistricts are categorized as urban.
There is no city administration beside the district administrator, who was appointed by state government. The East Timorese government started to plan in 2009 to change the status of districts into municipalities. These will have an elected mayor and council.
Demography.
The 2010 census recorded a population of 193,563 in the areas of Dili district classified as urban, with a population of 234,331 in the whole district including rural areas such as Atauro and Metinaro.
Dili is a melting pot of the different ethnic groups of East Timor, due partly to the internal migration of young men from around the country in search of work. This has led to a gender imbalance, with the male population significantly larger than the female. Between 2001 and 2004, the population of Dili district grew by 12.58%, with only 54% of the district's inhabitants born in the city. 7% were born in Bacau, 5% each in Viqueque and Bobonaro 4% in Ermera, and the remainder in other districts or overseas.
Climate.
Dili has a Tropical wet and dry climate under the Köppen climate classification.
History.
Dili was settled about 1520 by the Portuguese, who made it the capital of Portuguese Timor in 1769. It was proclaimed a city in January 1864. During World War II, Portugal and its colonies remained neutral, but the Allies saw East Timor as a potential target for Japanese invasion, and Australian and Dutch forces briefly occupied the island in 1941. In the night of the 19 February 1942, the Japanese attacked with a force of around 20,000 men, and occupied Dili before spreading out across the rest of the colony. On 26 September 1945, control of the island was officially returned to Portugal by the Japanese.
East Timor unilaterally declared independence from Portugal on 28 November 1975. However, nine days later, on 7 December, Indonesian forces invaded Dili. On 17 July 1976, Indonesia annexed East Timor, which it designated the 27th province of Indonesia, "Timor Timur" (Indonesian for "East Timor"), with Dili as its capital. A guerrilla war ensued from 1975 to 1999 between Indonesian and pro-independence forces, during which tens of thousands of East Timorese and some foreign civilians were killed. Media coverage of the 1991 Dili Massacre helped revitalise international support for the East Timorese independence movement.
In 1999, East Timor was placed under UN supervision and on 20 May 2002, Dili became the capital of the newly independent Democratic Republic of Timor-Leste. In May 2006, fighting and rioting sparked by conflict between elements of the military caused significant damage to the city and led to foreign military intervention to restore order.
Buildings and monuments.
Most buildings were damaged or destroyed in the violence of 1999, orchestrated by the Indonesian military and local pro-Indonesia militias (see Operation Scorched Earth). However, the city still has many buildings from the Portuguese era. The former Portuguese Governor's office is now the office of the Prime Minister. It was previously also used by the Indonesian-appointed Governor, and by the United Nations Transitional Administration in East Timor (UNTAET).
Even under Indonesian rule, during which the Portuguese language was banned, Portuguese street names like "Avenida Marechal Carmona" remained unchanged, although they were prefixed with the Indonesian word "Jalan" or 'road'. The Roman Catholic Church at Motael became a focus for resistance to Indonesian occupation. Legacies of Jakarta's occupation are the Church of the Immaculate Conception, seat of the Roman Catholic Diocese of Díli, purportedly the largest cathedral in Southeast Asia, and the 'Integration Monument', commemorating the Indonesian annexation of the territory in 1976. Featuring a statue of an East Timorese in traditional dress, breaking the chains round his wrists, the monument has not been demolished.
The Cristo Rei of Dili is a 27-metre (88.6 ft) tall statue of Jesus situated on top of a globe at the end of a peninsula in Dili. It is one of the town's landmarks. It was a present from the Indonesian Government during occupation for the 20th anniversary of East Timor's integration into Indonesia.
Education.
Schools in Dili include St. Joseph’s High School (Colégio de São José).
There are four International schools in Dili, a Portuguese school by the name os Escola Portuguesa Ruy Cinatti, an Australian managed school by the name of Dili International School, an American government sponsored school called QSI (Quality Schools International) International School of Dili and the Maharlika International School (Formerly Dili Education & Development Center), a Philippine International School. East Timor's major higher education institution, the Universidade Nacional de Timor-Leste, is based in Dili.
Transportation.
Dili is served by Presidente Nicolau Lobato International Airport, named after independence leader Nicolau Lobato. This is the only functioning international airport in East Timor, though there are airstrips in Baucau, Suai and Oecusse used for domestic flights. Until recently, Dili's airport runway has been unable to accommodate aircraft larger than the Boeing 737 or C-130 Hercules, but in January 2008, the Portuguese charter airline EuroAtlantic Airways operated a direct flight from Lisbon using a Boeing 757, carrying 140 members of the Guarda Nacional Republicana.
Under Portuguese rule, Baucau Airport, which has a much longer runway, was used for international flights, but following the Indonesian invasion this was taken over by the Indonesian military and closed to civilian traffic.

</doc>
<doc id="49253" url="http://en.wikipedia.org/wiki?curid=49253" title="Urysohn's lemma">
Urysohn's lemma

In topology, Urysohn's lemma is a lemma that states that a topological space is normal if and only if any two disjoint closed subsets can be separated by a function.
Urysohn's lemma is commonly used to construct continuous functions with various properties on normal spaces. It is widely applicable since all metric spaces and all compact Hausdorff spaces are normal. The lemma is generalized by (and usually used in the proof of) the Tietze extension theorem.
The lemma is named after the mathematician Pavel Samuilovich Urysohn.
Formal statement.
Two disjoint closed subsets "A" and "B" of a topological space "X" are said to be separated by neighbourhoods if there are neighbourhoods "U" of "A" and "V" of "B" that are also disjoint. "A" and "B" are said to be separated by a function if there exists a continuous function "f" from "X" into the unit interval [0,1] such that "f"("a") = 0 for all "a" in "A" and "f"("b") = 1 for all "b" in "B". Any such function is called a Urysohn function for "A" and "B".
A normal space is a topological space in which any two disjoint closed sets can be separated by neighbourhoods. Urysohn's lemma states that a topological space is normal if and only if any two disjoint closed sets can be separated by a continuous function.
The sets "A" and "B" need not be precisely separated by "f", i.e., we do not, and in general cannot, require that "f"("x") ≠ 0 and ≠ 1 for "x" outside of "A" and "B". This is possible only in perfectly normal spaces.
Urysohn's lemma has led to the formulation of other topological properties such as the 'Tychonoff property' and 'completely Hausdorff spaces'. For example, a corollary of the lemma is that normal "T"1 spaces are Tychonoff.
Sketch of proof.
For every dyadic fraction "r" ∈ (0,1), we are going to construct an open subset "U"("r") of "X" such that:
Once we have these sets, we define "f"("x") = 1 if "x" ∉ "U"("r") for any "r"; otherwise "f"("x") = inf { "r" : "x" ∈ "U"("r") } for every "x" ∈ "X". Using the fact that the dyadic rationals are dense, it is then not too hard to show that "f" is continuous and has the property "f"("A") ⊆ {0} and "f"("B") ⊆ {1}.
In order to construct the sets "U"("r"), we actually do a little bit more: we construct sets "U"("r") and "V"("r") such that
Since the complement of "V"("r") is closed and contains "U"("r"), the latter condition then implies condition (2) from above.
This construction proceeds by mathematical induction. First define "U"(1) = "X" \ "B" and "V"(0) = "X" \ "A". Since "X" is normal, we can find two disjoint open sets "U"(1/2) and "V"(1/2) which contain "A" and "B", respectively. Now assume that "n"≥1 and the sets "U"("k"/2"n") and "V"("k"/2"n") have already been constructed for "k" = 1...,2"n"-1. Since "X" is normal, for any "a" ∈ { 0,1...,2"n"-1 }, we can find two disjoint open sets which contain "X" \ "V"("a"/2"n") and "X" \ "U"(("a"+1)/2"n"), respectively. Call these two open sets "U"((2"a"+1)/2"n"+1) and "V"((2"a"+1)/2"n"+1), and verify the above three conditions.
The Mizar project has completely formalized and automatically checked a proof of Urysohn's lemma in the .

</doc>
<doc id="49256" url="http://en.wikipedia.org/wiki?curid=49256" title="Age of the Earth">
Age of the Earth

The age of the Earth is 4.54 ± 0.05 billion years (4.54 × 109 years ± 1%). This age is based on evidence from radiometric age dating of meteorite material and is consistent with the radiometric ages of the oldest-known terrestrial and lunar samples.
Following the scientific revolution and the development of radiometric age dating, measurements of lead in uranium-rich minerals showed that some were in excess of a billion years old.
The oldest such minerals analyzed to date – small crystals of zircon from the Jack Hills of Western Australia – are at least 4.404 billion years old. Comparing the mass and luminosity of the Sun to those of other stars, it appears that the solar system cannot be much older than those rocks. Calcium-aluminium-rich inclusions  – the oldest known solid constituents within meteorites that are formed within the Solar System – are 4.567 billion years old, giving an age for the solar system and an upper limit for the age of Earth.
It is hypothesised that the accretion of Earth began soon after the formation of the calcium-aluminium-rich inclusions and the meteorites. Because the exact amount of time this accretion process took is not yet known, and the predictions from different accretion models range from a few millions up to about 100 million years, the exact age of Earth is difficult to determine. It is also difficult to determine the exact age of the oldest rocks on Earth, exposed at the surface, as they are aggregates of minerals of possibly different ages.
Development of modern geologic concepts.
Studies of strata, the layering of rocks and earth, gave naturalists an appreciation that Earth may have been through many changes during its existence. These layers often contained fossilized remains of unknown creatures, leading some to interpret a progression of organisms from layer to layer.
Nicolas Steno in the 17th century was one of the first naturalists to appreciate the connection between fossil remains and strata. His observations led him to formulate important stratigraphic concepts (i.e., the "law of superposition" and the "principle of original horizontality"). In the 1790s, William Smith hypothesized that if two layers of rock at widely differing locations contained similar fossils, then it was very plausible that the layers were the same age. William Smith's nephew and student, John Phillips, later calculated by such means that Earth was about 96 million years old.
The naturalist Mikhail Lomonosov suggested in the mid-18th century that Earth had been created separately from the rest of the universe, several hundred thousand years before. Lomonosov's ideas were mostly speculative. In 1779 the Comte du Buffon tried to obtain a value for the age of Earth using an experiment: He created a small globe that resembled Earth in composition and then measured its rate of cooling. This led him to estimate that Earth was about 75,000 years old.
Other naturalists used these hypotheses to construct a history of Earth, though their timelines were inexact as they did not know how long it took to lay down stratigraphic layers. In 1830, geologist Charles Lyell, developing ideas found in James Hutton's works, popularized the concept that the features of Earth were in perpetual change, eroding and reforming continuously, and the rate of this change was roughly constant. This was a challenge to the traditional view, which saw the history of Earth as static, with changes brought about by intermittent catastrophes. Many naturalists were influenced by Lyell to become "uniformitarians" who believed that changes were constant and uniform.
Early calculations.
In 1862, the physicist William Thomson published calculations that fixed the age of Earth at between 20 million and 400 million years.
He assumed that Earth had formed as a completely molten object, and determined the amount of time it would take for the near-surface to cool to its present temperature. His calculations did not account for heat produced via radioactive decay (a process then unknown to science) or convection inside the Earth, which allows more heat to escape from the interior to warm rocks near the surface.
Geologists such as Charles Lyell had trouble accepting such a short age for Earth. For biologists, even 100 million years seemed much too short to be plausible. In Darwin's theory of evolution, the process of random heritable variation with cumulative selection requires great durations of time. (According to modern biology, the total evolutionary history from the beginning of life to today has taken since 3.5 to 3.8 billion years ago, the amount of time which passed since the last universal ancestor of all living organisms as shown by geological dating.)
In a lecture in 1869, Darwin's great advocate, Thomas H. Huxley, attacked Thomson's calculations, suggesting they appeared precise in themselves but were based on faulty assumptions. The physicist Hermann von Helmholtz (in 1856) and astronomer Simon Newcomb (in 1892) contributed their own calculations of 22 and 18 million years respectively to the debate: they independently calculated the amount of time it would take for the Sun to condense down to its current diameter and brightness from the nebula of gas and dust from which it was born. Their values were consistent with Thomson's calculations. However, they assumed that the Sun was only glowing from the heat of its gravitational contraction. The process of solar nuclear fusion was not yet known to science.
Other scientists backed up Thomson's figures as well. Charles Darwin's son, the astronomer George H. Darwin, proposed that Earth and Moon had broken apart in their early days when they were both molten. He calculated the amount of time it would have taken for tidal friction to give Earth its current 24-hour day. His value of 56 million years added additional evidence that Thomson was on the right track.
The last estimate Thomson gave, in 1897, was: "that it was more than 20 and less than 40 million year old, and probably much nearer 20 than 40". In 1899 and 1900, John Joly calculated the rate at which the oceans should have accumulated salt from erosion processes, and determined that the oceans were about 80 to 100 million years old.
Radiometric dating.
Overview.
By their chemical nature, rock minerals contain certain elements and not others; but in rocks containing radioactive isotopes, the process of radioactive decay generates exotic elements over time. By measuring the concentration of the stable end product of the decay, coupled with knowledge of the half life and initial concentration of the decaying element, the age of the rock can be calculated. Typical radioactive end products are argon from decay of potassium-40, and lead from decay of uranium and thorium. If the rock becomes molten, as happens in Earth's mantle, such nonradioactive end products typically escape or are redistributed. Thus the age of the oldest terrestrial rock gives a minimum for the age of Earth, assuming that no rock has been intact for longer than the Earth itself.
Convective mantle and radioactivity.
In 1892, Thomson had been made Lord Kelvin in appreciation of his many scientific accomplishments. Kelvin calculated the age of Earth by using thermal gradients, and arrived at an estimate of 100 million years old. He did not realize that Earth has a highly viscous fluid mantle, and this invalidated his estimate. In 1895, John Perry produced an age-of-Earth estimate of 2 to 3 billion years using a model of a convective mantle and thin crust. Kelvin stuck by his estimate of 100 million years, and later reduced it to about 20 million years.
The discovery of radioactivity introduced another factor in the calculation. After Henri Becquerel's initial discovery in 1896, Marie and Pierre Curie discovered the radioactive elements polonium and radium in 1898; and in 1903, Pierre Curie and Albert Laborde announced that radium produces enough heat to melt its own weight in ice in less than an hour. Geologists quickly realized that this upset the assumptions underlying most calculations of the age of Earth.
These had assumed that the original heat of the Earth and Sun had dissipated steadily into space, but radioactive decay meant that this heat had been continually replenished. George Darwin and John Joly were the first to point this out, in 1903.
Invention of radiometric dating.
Radioactivity, which had overthrown the old calculations, yielded a bonus by providing a basis for new calculations, in the form of radiometric dating.
Ernest Rutherford and Frederick Soddy jointly had continued their work on radioactive materials and concluded that radioactivity was due to a spontaneous transmutation of atomic elements. In radioactive decay, an element breaks down into another, lighter element, releasing alpha, beta, or gamma radiation in the process. They also determined that a particular isotope of a radioactive element decays into another element at a distinctive rate. This rate is given in terms of a "half-life", or the amount of time it takes half of a mass of that radioactive material to break down into its "decay product".
Some radioactive materials have short half-lives; some have long half-lives. Uranium and thorium have long half-lives, and so persist in Earth's crust, but radioactive elements with short half-lives have generally disappeared. This suggested that it might be possible to measure the age of Earth by determining the relative proportions of radioactive materials in geological samples. In reality, radioactive elements do not always decay into nonradioactive ("stable") elements directly, instead, decaying into other radioactive elements that have their own half-lives and so on, until they reach a stable element. Such "decay series", such as the uranium-radium and thorium series, were known within a few years of the discovery of radioactivity, and provided a basis for constructing techniques of radiometric dating.
The pioneers of radioactivity were chemist Bertram B. Boltwood and the energetic Rutherford. Boltwood had conducted studies of radioactive materials as a consultant, and when Rutherford lectured at Yale in 1904, Boltwood was inspired to describe the relationships between elements in various decay series. Late in 1904, Rutherford took the first step toward radiometric dating by suggesting that the alpha particles released by radioactive decay could be trapped in a rocky material as helium atoms. At the time, Rutherford was only guessing at the relationship between alpha particles and helium atoms, but he would prove the connection four years later.
Soddy and Sir William Ramsay had just determined the rate at which radium produces alpha particles, and Rutherford proposed that he could determine the age of a rock sample by measuring its concentration of helium. He dated a rock in his possession to an age of 40 million years by this technique. Rutherford wrote,
"I came into the room, which was half dark, and presently spotted Lord Kelvin in the audience and realized that I was in trouble at the last part of my speech dealing with the age of the Earth, where my views conflicted with his. To my relief, Kelvin fell fast asleep, but as I came to the important point, I saw the old bird sit up, open an eye, and cock a baleful glance at me! Then a sudden inspiration came, and I said, 'Lord Kelvin had limited the age of the Earth, provided no new source was discovered. That prophetic utterance refers to what we are now considering tonight, radium!' Behold! the old boy beamed upon me.
Rutherford assumed that the rate of decay of radium as determined by Ramsay and Soddy was accurate, and that helium did not escape from the sample over time. Rutherford's scheme was inaccurate, but it was a useful first step.
Boltwood focused on the end products of decay series. In 1905, he suggested that lead was the final stable product of the decay of radium. It was already known that radium was an intermediate product of the decay of uranium. Rutherford joined in, outlining a decay process in which radium emitted five alpha particles through various intermediate products to end up with lead, and speculated that the radium-lead decay chain could be used to date rock samples. Boltwood did the legwork, and by the end of 1905 had provided dates for 26 separate rock samples, ranging from 92 to 570 million years. He did not publish these results, which was fortunate because they were flawed by measurement errors and poor estimates of the half-life of radium. Boltwood refined his work and finally published the results in 1907.
Boltwood's paper pointed out that samples taken from comparable layers of strata had similar lead-to-uranium ratios, and that samples from older layers had a higher proportion of lead, except where there was evidence that lead had leached out of the sample. His studies were flawed by the fact that the decay series of thorium was not understood, which led to incorrect results for samples that contained both uranium and thorium. However, his calculations were far more accurate than any that had been performed to that time. Refinements in the technique would later give ages for Boltwood's 26 samples of 410 million to 2.2 billion years.
Arthur Holmes establishes radiometric dating.
Although Boltwood published his paper in a prominent geological journal, the geological community had little interest in radioactivity. Boltwood gave up work on radiometric dating and went on to investigate other decay series. Rutherford remained mildly curious about the issue of the age of Earth but did little work on it.
Robert Strutt tinkered with Rutherford's helium method until 1910 and then ceased. However, Strutt's student Arthur Holmes became interested in radiometric dating and continued to work on it after everyone else had given up. Holmes focused on lead dating, because he regarded the helium method as unpromising. He performed measurements on rock samples and concluded in 1911 that the oldest (a sample from Ceylon) was about 1.6 billion years old. These calculations were not particularly trustworthy. For example, he assumed that the samples had contained only uranium and no lead when they were formed.
More important research was published in 1913. It showed that elements generally exist in multiple variants with different masses, or "isotopes". In the 1930s, isotopes would be shown to have nuclei with differing numbers of the neutral particles known as "neutrons". In that same year, other research was published establishing the rules for radioactive decay, allowing more precise identification of decay series.
Many geologists felt these new discoveries made radiometric dating so complicated as to be worthless. Holmes felt that they gave him tools to improve his techniques, and he plodded ahead with his research, publishing before and after the First World War. His work was generally ignored until the 1920s, though in 1917 Joseph Barrell, a professor of geology at Yale, redrew geological history as it was understood at the time to conform to Holmes's findings in radiometric dating. Barrell's research determined that the layers of strata had not all been laid down at the same rate, and so current rates of geological change could not be used to provide accurate timelines of the history of Earth.
Holmes's persistence finally began to pay off in 1921, when the speakers at the yearly meeting of the British Association for the Advancement of Science came to a rough consensus that Earth was a few billion years old, and that radiometric dating was credible. Holmes published "The Age of the Earth, an Introduction to Geological Ideas" in 1927 in which he presented a range of 1.6 to 3.0 billion years. No great push to embrace radiometric dating followed, however, and the die-hards in the geological community stubbornly resisted. They had never cared for attempts by physicists to intrude in their domain, and had successfully ignored them so far. The growing weight of evidence finally tilted the balance in 1931, when the National Research Council of the US National Academy of Sciences decided to resolve the question of the age of Earth by appointing a committee to investigate. Holmes, being one of the few people on Earth who was trained in radiometric dating techniques, was a committee member, and in fact wrote most of the final report.
Thus, Arthur Holmes' report concluded that radioactive dating was the only reliable means of pinning down geological time scales. Questions of bias were deflected by the great and exacting detail of the report. It described the methods used, the care with which measurements were made, and their error bars and limitations.
Modern radiometric dating.
Radiometric dating continues to be the predominant way scientists date geologic timescales. Techniques for radioactive dating have been tested and fine-tuned on an ongoing basis since the 1960s. Forty or so different dating techniques have been utilized to date, working on a wide variety of materials. Dates for the same sample using these different techniques are in very close agreement on the age of the material.
Possible contamination problems do exist, but they have been studied and dealt with by careful investigation, leading to sample preparation procedures being minimized to limit the chance of contamination.
Why meteorites were used.
An age of 4.55 ± 0.07 billion years, very close to today's accepted age, was determined by C.C. Patterson using uranium-lead isotope dating (specifically lead-lead dating) on several meteorites including the Canyon Diablo meteorite and published in 1956.
The quoted age of Earth is derived, in part, from the Canyon Diablo meteorite for several important reasons and is built upon a modern understanding of cosmochemistry built up over decades of research.
Most geological samples from Earth are unable to give a direct date of the formation of Earth from the solar nebula because Earth has undergone differentiation into the core, mantle, and crust, and this has then undergone a long history of mixing and unmixing of these sample reservoirs by plate tectonics, weathering and hydrothermal circulation.
All of these processes may adversely affect isotopic dating mechanisms because the sample cannot always be assumed to have remained as a closed system, by which it is meant that either the parent or daughter nuclide (a species of atom characterised by the number of neutrons and protons an atom contains) or an intermediate daughter nuclide may have been partially removed from the sample, which will skew the resulting isotopic date. To mitigate this effect it is usual to date several minerals in the same sample, to provide an isochron. Alternatively, more than one dating system may be used on a sample to check the date.
Some meteorites are furthermore considered to represent the primitive material from which the accreting solar disk was formed. Some have behaved as closed systems (for some isotopic systems) soon after the solar disk and the planets formed. To date, these assumptions are supported by much scientific observation and repeated isotopic dates, and it is certainly a more robust hypothesis than that which assumes a terrestrial rock has retained its original composition.
Nevertheless, ancient Archaean lead ores of galena have been used to date the formation of Earth as these represent the earliest formed lead-only minerals on the planet and record the earliest homogeneous lead-lead isotope systems on the planet. These have returned age dates of 4.54 billion years with a precision of as little as 1% margin for error.
Statistics for several meteorites that have undergone isochron dating are as follows:
Canyon Diablo meteorite.
The Canyon Diablo meteorite was used because it is a very large representative of a particularly rare type of meteorite that contains sulfide minerals (particularly troilite, FeS), metallic nickel-iron alloys, plus silicate minerals.
This is important because the presence of the three mineral phases allows investigation of isotopic dates using samples that provide a great separation in concentrations between parent and daughter nuclides. This is particularly true of uranium and lead. Lead is strongly chalcophilic and is found in the sulfide at a much greater concentration than in the silicate, versus uranium. Because of this segregation in the parent and daughter nuclides during the formation of the meteorite, this allowed a much more precise date of the formation of the solar disk and hence the planets than ever before.
The age determined from the Canyon Diablo meteorite has been confirmed by hundreds of other age determinations, from both terrestrial samples and other meteorites. The meteorite samples, however, show a spread from 4.53 to 4.58 billion years ago. This is interpreted as the duration of formation of the solar nebula and its collapse into the solar disk to form the Sun and the planets. This 50 million year time span allows for accretion of the planets from the original solar dust and meteorites.
The moon, as another extraterrestrial body that has not undergone plate tectonics and that has no atmosphere, provides quite precise age dates from the samples returned from the Apollo missions. Rocks returned from the Moon have been dated at a maximum of around 4.4 and 4.5 billion years old. Martian meteorites that have landed upon Earth have also been dated to around 4.5 billion years old by lead-lead dating. Lunar samples, since they have not been disturbed by weathering, plate tectonics or material moved by organisms, can also provide dating by direct electron microscope examination of cosmic ray tracks. The accumulation of dislocations generated by high energy cosmic ray particle impacts provides another confirmation of the isotopic dates. Cosmic ray dating is only useful on material that has not been melted, since melting erases the crystalline structure of the material, and wipes away the tracks left by the particles.
Altogether, the concordance of age dates of both the earliest terrestrial lead reservoirs and all other reservoirs within the Solar System found to date are used to support the fact that Earth and the rest of the Solar System formed at around 4.53 to 4.58 billion years ago.
Helioseismic verification.
The radiometric date of meteorites can be verified with studies of the Sun. The Sun can be dated using helioseismic methods that strongly agree with the radiometric dates found for the oldest meteorites.

</doc>
<doc id="49257" url="http://en.wikipedia.org/wiki?curid=49257" title="Digital Audio Broadcasting">
Digital Audio Broadcasting

Digital Audio Broadcasting (DAB) is a digital radio technology for broadcasting radio stations, used in several countries across Europe and Asia Pacific. 
The DAB standard was initiated as a European research project in the 1980s. The Norwegian Broadcasting Corporation (NRK) launched the very first DAB channel in the world on 1 June 1995 (NRK Klassisk), and the BBC and SR launched their first DAB digital radio broadcasts in September 1995. DAB receivers have been available in many countries since the end of the 1990s.
DAB may offer more radio programmes over a specific spectrum than analogue FM radio. DAB is more robust with regard to noise and multipath fading for mobile listening , since DAB reception quality first degrades rapidly when the signal strength falls below a critical threshold, whereas FM reception quality degrades slowly with the decreasing signal.
Audio quality varies depending on the bitrate used and audio material. Most stations use a bit rate of 128 kbit/s or less with the MP2 audio codec, which requires 160 kbit/s to achieve perceived FM quality. 128 kbit/s gives better dynamic range or signal-to-noise ratio than FM radio, but a more smeared stereo image, and an upper cut-off frequency of 14 kHz, corresponding to 15 kHz of FM radio. However, "CD sound quality" with MP2 is possible "with 256…192 kbps".
An upgraded version of the system was released in February 2007, which is called DAB+. DAB is not forward compatible with DAB+, which means that DAB-only receivers are not able to receive DAB+ broadcasts. However, broadcasters can mix DAB and DAB+ programs inside the same transmission and so make a progressive transition to DAB+. DAB+ is approximately twice as efficient as DAB due to the adoption of the AAC+ audio codec, and DAB+ can provide high quality audio with bit rates as low as 64 kbit/s. Reception quality is also more robust on DAB+ than on DAB due to the addition of Reed-Solomon error correction coding.
In spectrum management, the bands that are allocated for public DAB services, are abbreviated with T-DAB, where the "T" stands for terrestrial.
More than 20 countries provide DAB transmissions, and several countries, such as Australia, Italy, Malta, Switzerland, The Netherlands and Germany, are transmitting DAB+ stations. See Countries using DAB/DMB. However, DAB radio has still not replaced the old FM system in popularity.
History.
DAB has been under development since 1981 at the Institut für Rundfunktechnik (IRT). In 1985 the first DAB demonstrations were held at the WARC-ORB in Geneva and in 1988 the first DAB transmissions were made in Germany. Later DAB was developed as a research project for the European Union (EUREKA), which started in 1987 on initiative by a consortium formed in 1986. The MPEG-1 Audio Layer II ("MP2") codec was created as part of the EU147 project. DAB was the first standard based on orthogonal frequency division multiplexing (OFDM) modulation technique, which since then has become one of the most popular transmission schemes for modern wideband digital communication systems.
A choice of audio codec, modulation and error-correction coding schemes and first trial broadcasts were made in 1990. Public demonstrations were made in 1993 in the United Kingdom. The protocol specification was finalized in 1993 and adopted by the ITU-R standardization body in 1994, the European community in 1995 and by ETSI in 1997. Pilot broadcasts were launched in several countries in 1995.
The UK was the first country to receive a wide range of radio stations via DAB. Commercial DAB receivers began to be sold in 1999 and over 50 commercial and BBC services were available in London by 2001.
By 2006, 500 million people worldwide were in the coverage area of DAB broadcasts, although by this time sales had only taken off in the United Kingdom and Denmark. In 2006 there were approximately 1,000 DAB stations in operation world wide.
The standard was coordinated by the European DAB forum, formed in 1995 and reconstituted to the World DAB Forum in 1997, which represents more than 30 countries. In 2006 the World DAB Forum became the World DMB Forum which now presides over both the DAB and DMB standard.
In October 2005, the World DMB Forum instructed its Technical Committee to carry out the work needed to adopt the AAC+ audio codec and stronger error correction coding. This work led to the launch of the new DAB+ system.
Technology.
Bands and modes.
DAB uses a wide-bandwidth broadcast technology and typically spectra have been allocated for it in Band III (174–240 MHz) and L band (1,452–1,492 MHz), although the scheme allows for operation almost anywhere above 30 MHz. The US military has reserved L-Band in the USA only, blocking its use for other purposes in America, and the United States has reached an agreement with Canada to restrict L-Band DAB to terrestrial broadcast to avoid interference.
DAB has a number of country specific transmission modes (I, II, III and IV). For worldwide operation a receiver must support all 4 modes:
Protocol stack.
From an OSI model protocol stack viewpoint, the technologies used on DAB inhabit the following layers: the audio codec inhabits the presentation layer. Below that is the data link layer, in charge of statistical time division multiplexing and frame synchronization. Finally, the physical layer contains the error-correction coding, OFDM modulation, and dealing with the over-the-air transmission and reception of data. Some aspects of these are described below.
Audio codec.
The older version of DAB that is being used in Denmark*, Ireland*, Norway*, Switzerland* and the UK, uses the MPEG-1 Audio Layer 2 audio codec, which is also known as "MP2" due to computer files using those characters for their file extension. (*Denmark, Ireland, Norway, and Switzerland also use DAB+).
The new DAB+ standard has adopted the HE-AAC version 2 audio codec, commonly known as "AAC+" or "aacPlus". AAC+ is approximately three-times more efficient than MP2, which means that broadcasters using DAB+ will be able to provide far higher audio quality or far more stations than they can on DAB, or, as is most likely, a combination of both higher audio quality and more stations will be provided.
One of the most important decisions regarding the design of a digital radio system is the choice of which audio codec to use, because the efficiency of the audio codec determines how many radio stations can be carried on a multiplex at a given level of audio quality. The capacity of a DAB multiplex is fixed, so the more efficient the audio codec is, the more stations can be carried, and vice versa. Similarly, for a fixed bit-rate level, the more efficient the audio codec is the higher the audio quality will be.
Error-correction coding.
Error-correction coding (ECC) is an important technology for a digital communication system because it determines how robust the reception will be for a given signal strength – stronger ECC will provide more robust reception than a weaker form.
The old version of DAB uses punctured convolutional coding for its ECC. The coding scheme uses unequal error protection (UEP), which means that parts of the audio bit-stream that are more susceptible to errors causing audible disturbances are provided with more protection (i.e. a lower code rate) and vice versa. However, the UEP scheme used on DAB results in there being a grey area in between the user experiencing good reception quality and no reception at all, as opposed to the situation with most other wireless digital communication systems that have a sharp "digital cliff", where the signal rapidly becomes unusable if the signal strength drops below a certain threshold. When DAB listeners receive a signal in this intermediate strength area they experience a "burbling" sound which interrupts the playback of the audio.
The new DAB+ standard has incorporated Reed-Solomon ECC as an "inner layer" of coding that is placed around the byte interleaved audio frame but inside the "outer layer" of convolutional coding used by the older DAB system, although on DAB+ the convolutional coding uses equal error protection (EEP) rather than UEP since each bit is equally important in DAB+. This combination of Reed-Solomon coding as the inner layer of coding, followed by an outer layer of convolutional coding – so-called "concatenated coding" – became a popular ECC scheme in the 1990s, and NASA adopted it for its deep-space missions. One slight difference between the concatenated coding used by the DAB+ system and that used on most other systems is that it uses a rectangular byte interleaver rather than Forney interleaving in order to provide a greater interleaver depth, which increases the distance over which error bursts will be spread out in the bit-stream, which in turn will allow the Reed-Solomon error decoder to correct a higher proportion of errors.
The ECC used on DAB+ is far stronger than is used on DAB, which, with all else being equal (i.e. if the transmission powers remained the same), would translate into people who currently experience reception difficulties on DAB receiving a much more robust signal with DAB+ transmissions. It also has a far steeper "digital cliff", and listening tests have shown that people prefer this when the signal strength is low compared to the shallower digital cliff on DAB.
Modulation.
Immunity to fading and inter-symbol interference (caused by multipath propagation) is achieved without equalization by means of the OFDM and DQPSK modulation techniques. For details, see the OFDM system comparison table.
Using values for the most commonly used transmission mode on DAB, Transmission Mode I (TM I), the OFDM modulation consists of 1,536 subcarriers that are transmitted in parallel. The useful part of the OFDM symbol period is 1 millisecond, which results in the OFDM subcarriers each having a bandwidth of 1 kHz due to the inverse relationship between these two parameters, and the overall OFDM channel bandwidth is 1,537 kHz. The OFDM guard interval for TM I is 246 microseconds, which means that the overall OFDM symbol duration is 1.246 milliseconds. The guard interval duration also determines the maximum separation between transmitters that are part of the same single-frequency network (SFN), which is approximately 74 km for TM I.
Single-frequency networks.
OFDM allows the use of single-frequency networks (SFN), which means that a network of transmitters can provide coverage to a large area – up to the size of a country – where all transmitters use the same transmission frequency. Transmitters that are part of an SFN need to be very accurately synchronised with other transmitters in the network, which requires the transmitters to use very accurate clocks.
When a receiver receives a signal that has been transmitted from the different transmitters that are part of an SFN, the signals from the different transmitters will typically have different delays, but to OFDM they will appear to simply be different multipaths of the same signal. Reception difficulties can arise, however, when the relative delay of multipaths exceeds the OFDM guard interval duration, and there are frequent reports of reception difficulties due to this issue when there is a "lift", such as when there's high pressure, due to signals travelling farther than usual, and thus the signals are likely to arrive with a relative delay that is greater than the OFDM guard interval.
Low power "gap-filler" transmitters can be added to an SFN as and when desired in order to improve reception quality, although the way SFNs have been implemented in the UK up to now they have tended to consist of higher power transmitters being installed at main transmitter sites in order to keep costs down.
Bit rates.
An ensemble has a maximum bit rate that can be carried, but this depends on which error protection level is used. However, all DAB multiplexes can carry a total of 864 "capacity units". The number of capacity units, or CU, that a certain bit-rate level requires depends on the amount of error correction added to the transmission, as described above. In the UK, most services transmit using 'protection level three', which provides an average ECC code rate of approximately ½, equating to a maximum bit rate per multiplex of 1,184 kbit/s.
Services and ensembles.
Various different services are embedded into one ensemble (which is also typically called a multiplex). These services can include:
DAB+.
The term DAB most commonly refers both to a specific DAB standard using the MP2 audio codec, but can sometimes refer to a whole family of DAB related standards, such as DAB+, DMB and DAB-IP.
DAB+.
WorldDMB, the organisation in charge of the DAB standards, announced DAB+, a major upgrade to the DAB standard in 2006, when the HE-AAC v2 audio codec (also known as eAAC+) was adopted. The new standard, which is called DAB+, has also adopted the MPEG Surround audio format and stronger error correction coding in the form of Reed-Solomon coding. DAB+ has been standardised as ETSI TS 102 563.
As DAB is not forward compatible with DAB+, older DAB receivers can not receive DAB+ broadcasts. However, DAB receivers that will be able to receive the new DAB+ standard via a firmware upgrade went on sale in July 2007. If a receiver is DAB+ compatible, there will be a sign on the product packaging.
DAB+ broadcasts have launched in several countries like Australia, Czech Republic, Denmark, Germany, Hong Kong, Italy, Malta, Norway, Poland, Switzerland, and The Netherlands. Malta was the first country to launch DAB+ in Europe. Several other countries are also expected to launch DAB+ broadcasts over the next few years, such as Austria, Hungary and Asian countries, such as Thailand, Vietnam and Indonesia. South Africa began a DAB+ technical pilot in November 2014 on channel 13F in Band 3. If DAB+ stations launch in established DAB countries, they can transmit alongside existing DAB stations that use the older MPEG-1 Audio Layer II audio format, and most existing DAB stations are expected to continue broadcasting until the vast majority of receivers support DAB+.
Ofcom in the UK has published a consultation with the intention to set up a new multiplex containing a mix of DAB and DAB+ services, with the intention of moving services to this format in the long term.
DMB.
Digital Multimedia Broadcasting (DMB) and DAB-IP are suitable for mobile radio and TV both because they support MPEG 4 AVC and WMV9 respectively as video codecs. However, a DMB video subchannel can easily be added to any DAB transmission, as it was designed to be carried on a DAB subchannel. DMB broadcasts in Korea carry conventional MPEG 1 Layer II DAB audio services alongside their DMB video services.
Norway, South Korea and France are countries currently broadcasting DMB.
Countries using DAB.
More than 30 countries provide DAB, DAB+ and/or DMB broadcasts, either as a permanent technology or as test transmissions.
DAB and AM/FM compared.
Traditionally radio programmes were broadcast on different frequencies via AM and FM, and the radio had to be tuned into each frequency, as needed. This used up a comparatively large amount of spectrum for a relatively small number of stations, limiting listening choice. DAB is a digital radio broadcasting system that through the application of multiplexing and compression combines multiple audio streams onto a relatively narrow band centred on a single broadcast frequency called a DAB ensemble.
Within an overall target bit rate for the DAB ensemble, individual stations can be allocated different bit rates. The number of channels within a DAB ensemble can be increased by lowering average bit rates, but at the expense of the quality of streams. Error correction under the DAB standard makes the signal more robust but reduces the total bit rate available for streams.
FM HD Radio versus DAB.
Some countries have implemented Eureka-147 Digital Audio Broadcasting (DAB). DAB broadcasts a single station that is approximately 1,500 kilohertz wide (~1,000 kilobits per second). That station is then subdivided into multiple digital streams of between 9 and 12 programs. In contrast FM HD Radio shares its digital broadcast with the traditional 200 kilohertz-wide channels, with capability of 300 kbit/s per station (pure digital mode).
The first generation DAB uses the MPEG-1 Audio Layer II (MP2) audio codec which has less efficient compression than newer codecs. The typical bitrate for DAB programs is only 128 kbit/s and as a result most radio stations on DAB have a lower sound quality than FM, prompting a number of complaints among the audiophile community. As with DAB+ or T-DMB in Europe, FM HD Radio uses a codec based upon the MPEG-4 HE-AAC standard.
HD Radio is proprietary system from the company Ibiquity. DAB is an open standard deposited at ETSI.
Use of frequency spectrum and transmitter sites.
DAB gives substantially higher spectral efficiency, measured in programmes per MHz and per transmitter site, than analogue communication. This has led to an increase in the number of stations available to listeners, especially outside of the major urban areas.
Numerical example: Analog FM requires 0.2 MHz per programme. The frequency reuse factor in most countries is approximately 15, meaning that only one out of 15 transmitter sites can use the same channel frequency without problems with co-channel interference, i.e. cross-talk. Assuming a total availability of 102 FM channels at a bandwidth of 0.2MHz over the Band II spectrum of 87.5 to 108.0 MHz, an average of 102/15 = 6.8 radio channels are possible on each transmitter site (plus lower-power local transmitters causing less interference). This results in a system spectral efficiency of 1 / 15 / (0.2 MHz) = 0.30 programmes/transmitter/MHz. DAB with 192 kbit/s codec requires 1.536 MHz * 192 kbit/s / 1,136 kbit/s = 0.26 MHz per audio programme. The frequency reuse factor for local programmes and multi-frequency broadcasting networks (MFN) is typically 4 or 5, resulting in 1 / 4 / (0.26 MHz) = 0.96 programmes/transmitter/MHz. This is 3.2 times as efficient as analog FM for local stations. For single frequency network (SFN) transmission, for example of national programmes, the channel re-use factor is 1, resulting in 1/1/0.25 MHz = 3.85 programmes/transmitter/MHz, which is 12.7 times as efficient as FM for national and regional networks.
Note the above capacity improvement may not always be achieved at the L-band frequencies, since these are more sensitive to obstacles than the FM band frequencies, and may cause shadow fading for hilly terrain and for indoor communication. The number of transmitter sites or the transmission power required for full coverage of a country may be rather high at these frequencies, to avoid that the system becomes noise limited rather than limited by co-channel interference.
Sound quality.
The original objectives of converting to digital transmission were to enable higher fidelity, more stations and more resistance to noise, co-channel interference and multipath than in analogue FM radio. However, the leading countries in implementing DAB on stereo radio stations use compression to such a degree that it produces lower sound quality than that received from non-mobile FM broadcasts. This is because of the bit rate levels being too low for the MPEG Layer 2 audio codec to provide high fidelity audio quality.
The BBC Research & Development department states that at least 192 kbit/s is necessary for a high fidelity stereo broadcast :
When BBC in July 2006 reduced the bit-rate of transmission of Radio 3 from 192 kbit/s to 160 kbit/s, the resulting degradation of audio quality prompted a number of complaints to the Corporation. BBC later announced that following this testing of new equipment, it would resume the previous practice of transmitting Radio 3 at 192 kbit/s whenever there were no other demands on bandwidth.
Despite the above a survey of DAB listeners (including mobile) has shown most find DAB to have equal or better sound quality than FM.
Notwithstanding the above, BBC Radio 4 has extended the periods it broadcasts programmes with a lower bit rate (80 kbit/s) and in mono in 2012, such as the "Today" programme, rather than 128 kbit/s and in stereo. Programmes which had traditionally been broadcast on BBC Radio 4 DAB in stereo (from 1999 to 2011), can now only be heard in the evenings in mono, even though the same programmes still go out in stereo on Radio 4 FM, Digital TV and On-Line. The BBC have issued a statement stating that stereo is still their default for BBC Radio 4 DAB, however after the Olympics, this does not appear to be the case in the evenings, making FM broadcasts (in good reception areas) superior. As very few car radios are currently fitted with DAB if the BBC switch FM off as indicated later in the decade, some listeners may be forced to receive mono broadcasts in the future, a somewhat backward step.
An Audio Quality comparison of PCM, DAB, DAB+, FM and AM is available 
Benefits of DAB.
Current AM and FM terrestrial broadcast technology is well established, compatible, and cheap to manufacture. Benefits of DAB over analogue systems are explained below.
Improved features for users.
DAB radios automatically tune to all the available stations, offering a list for the user to select from.
DAB can carry "radiotext" (in DAB terminology, "Dynamic Label Segment", or DLS) from the station giving real-time information such as song titles, music type and news or traffic updates. Advance programme guides can also be transmitted. A similar feature also exists on FM in the form of the RDS. (However, not all FM receivers allow radio stations to be stored by name.)
DAB receivers can display time of day as encoded into transmissions, so is automatically corrected when travelling between time zones and when changing to or from Daylight Saving. This is not implemented on all receivers, and some display time only when in "Standby" mode. (Similar Features on RDS: 4A Groups)
Some radios offer a pause facility on live broadcasts, caching the broadcast stream on local flash memory, although this function is limited.
More stations.
DAB is not more bandwidth efficient than analogue measured in programmes per MHz of a specific transmitter (the so-called link spectral efficiency). It is less susceptible to co-channel interference (cross talk), which makes it possible to reduce the reuse distance, i.e. use the same radio frequency channel more densely. The system spectral efficiency (the average number of radio programmes per MHz and transmitter) is a factor three more efficient than analogue FM for local radio stations, as can be seen in the above numerical example. For national and regional radio networks, the efficiency is improved by more than an order of magnitude due to the use of SFNs. In that case, adjacent transmitters use the same frequency.
In certain areas – particularly rural areas – the introduction of DAB gives radio listeners a greater choice of radio stations. For instance, in South Norway, radio listeners experienced an increase in available stations from 6 to 21 when DAB was introduced in November 2006.
Reception quality.
The DAB standard integrates features to reduce the negative consequences of multipath fading and signal noise, which afflict existing analogue systems.
Also, as DAB transmits digital audio, there is no hiss with a weak signal, which can happen on FM. However, radios in the fringe of a DAB signal, can experience a "bubbling mud" sound interrupting the audio and/or the audio cutting out altogether.
Due to sensitivity to doppler shift in combination with multipath propagation, DAB reception range (but not audio quality) is reduced when travelling speeds of more than 120 to 200 km/h, depending on carrier frequency.
Less undocumented station interference.
The specialised nature and cost of DAB broadcasting equipment provide barriers to undocumented stations broadcasting on DAB. In cities such as London with large numbers of undocumented radio stations broadcasting on FM, this means that some stations can be reliably received via DAB in areas where they are regularly difficult or impossible to receive on FM due to undocumented radio interference.
Variable bandwidth.
Mono talk radio, news and weather channels and other non-music programs need significantly less bandwidth than a typical music radio station, which allows DAB to carry these programmes at lower bit rates, leaving more bandwidth to be used for other programs.
However, this had led to the situation where some stations are being broadcast in mono, see "music radio stations broadcasting in mono" for more details.
Transmission costs.
It is common belief that DAB is more expensive to transmit than FM. It is true that DAB uses higher frequencies than FM and therefore there is a need to compensate with more transmitters, higher radiated powers, or a combination, to achieve the same coverage. However, the last couple of years has seen significant improvement in power efficiency for DAB-transmitters.
This efficiency originates from the ability a DAB network has in broadcasting more channels per network. One network can broadcast 6–10 channels (with MPEG audio codec) or 10–16 channels (with HE AAC codec). Hence, it is thought that the replacement of FM-radios and FM-transmitters with new DAB-radios and DAB-transmitters will not cost any more as opposed to newer FM facilities.
Lower transmission costs are supported by independent network studies from Teracom (Sweden) and SSR/SRG (Switzerland). Among other things they show that DAB is as low as one-sixth of the cost of FM transmission.
Disadvantages of DAB.
Reception quality.
The reception quality on DAB can be poor even for people who live well within the coverage area. The reason for this is that the old version of DAB uses weak error correction coding, so that when there are a lot of errors with the received data not enough of the errors can be corrected and a "bubbling mud" sound occurs. In some cases a complete loss of signal can happen. This situation will be improved upon in the new DAB standard (DAB+, discussed below) that uses stronger error correction coding and as additional transmitters are built.
Audio Quality.
Broadcasters have been criticized for ‘squeezing in’ more stations per ensemble than recommended, by:
Signal delay.
The nature of a SFN is such that the transmitters in a network must broadcast the same signal at the same time. To achieve synchronization, the broadcaster must counter any differences in propagation time incurred by the different methods and distances involved in carrying the signal from the multiplexer to the different transmitters. This is done by applying a delay to the incoming signal at the transmitter based on a timestamp generated at the multiplexer, created taking into account the maximum likely propagation time, with a generous added margin for safety. Delays in the receiver due to digital processing (e.g. deinterleaving) add to the overall delay perceived by the listener. The signal is delayed by 2–4 seconds depending on the decoding circuitry used. This has disadvantages:
Time signals, on the contrary, are not a problem in a well-defined network with a fixed delay. The DAB multiplexer adds the proper offset to the distributed time information. The time information is also independent from the (possibly varying) audio decoding delay in receivers since the time is not embedded inside the audio frames. This means that built in clocks in receivers will be spot on.
Coverage.
As DAB is at a relatively early stage of deployment, DAB coverage is poor in nearly all countries in comparison to the high population coverage provided by FM.
An exception is Norway, as the country will have 99.5 % coverage by the end of 2014.
Compatibility.
In 2006 tests began using the much improved HE-AAC codec for DAB+. Virtually none of the receivers made before 2008 support the new codec, however, thus making them partially obsolete once DAB+ broadcasts begin and completely obsolete once the old MPEG-1 Layer 2 stations are switched off. New receivers are both DAB and DAB+ compatible; however, the issue is exacerbated by some manufacturers disabling the DAB+ features on otherwise compatible radios to save on licensing fees when sold in countries without current DAB+ broadcasts.
Power requirements.
As DAB requires digital signal processing techniques to convert from the received digitally encoded signal to the analogue audio content, the complexity of the electronic circuitry required to do this is higher. This translates into needing more power to effect this conversion than compared to an analogue FM to audio conversion, meaning that portable receiving equipment will tend to have a shorter battery life, or require higher power (and hence more bulk). This means that they use more energy than analogue Band II VHF receivers.
As an indicator of this increased power consumption, some radio manufacturers quote the length of time their receivers can play on a single charge. For a commonly used FM/DAB-receiver from manufacturer PURE, this is stated as: DAB 10 hours, FM 22 hours.
Use of Licensed Codecs.
The use of MPEG previously and later AAC has prompted criticism of the fact that a (large) public system is financially supporting a private company. In general, an open system will permit equipment to be bought from various sources in competition with each other but by selecting a single vendor of codec, with which all equipment must be compatible, this is not possible.
FM radio switch-off.
Norway is the only country that has announced a complete switch-off of FM radio stations. Switch off will start on 11 January 2017.
At the "WorldDMB seminar" held in Riva del Garda, Italy, on 14 April 2013, it was announced that in Norway there will be 99.5 % DAB coverage by 2014, and that the country is planning to switch-off its national and regional FM radio services in 2017. There is no intention of switching off local FM services in Norway by that date and no subsequent date has been announced for such a move.
Other Nordic countries like Denmark and Sweden are evaluating a switch-off within 2022
UK is considering a progressive switch-off in the period 2017–2022.

</doc>
<doc id="49260" url="http://en.wikipedia.org/wiki?curid=49260" title="Willy Brandt">
Willy Brandt

Willy Brandt (]; born Herbert Ernst Karl Frahm; 18 December 1913 – 8 October 1992) was a German statesman and politician, leader of the Social Democratic Party of Germany (SPD) from 1964 to 1987 and chancellor of the Federal Republic of Germany from 1969 to 1974. He was awarded the Nobel Peace Prize in 1971 for his efforts to strengthen cooperation in western Europe through the EEC and to achieve reconciliation between West Germany and the countries of Eastern Europe. He was the first Social Democrat chancellor since 1930.
Fleeing to Norway and then Sweden during the Nazi regime and working as a leftist journalist, he took the name Willy Brandt as a pseudonym to avoid detection by Nazi agents, and then formally adopted the name in 1948. Brandt was originally considered one of the leaders of the right wing of the SPD, and earned initial fame as Governing Mayor of West Berlin. He served as Foreign Minister and Vice Chancellor in Kurt Georg Kiesinger's cabinet, and became Chancellor in 1969. As chancellor, he maintained West Germany's close alignment with the United States and focused on strengthening European integration in western Europe, while launching the new policy of "Ostpolitik" aimed at improving relations with Eastern Europe. Brandt was controversial on both the right wing, for his "Ostpolitik", and on the left wing, for his support of American policies, including the Vietnam War, and right-wing authoritarian regimes. The Brandt Report became a recognised measure for describing the general North-South divide in world economics and politics between an affluent North and a poor South. Brandt was also known for his fierce anti-communist policies at the domestic level, culminating in the Radikalenerlass (Anti-Radical Decree) in 1972.
Brandt resigned as Chancellor in 1974, after Günter Guillaume, one of his closest aides, was exposed as an agent of the Stasi, the East German secret service.
Early life, the war.
Willy Brandt was born Herbert Ernst Carl Frahm in the Free City of Lübeck (German Empire) on 18 December 1913. His mother was Martha Frahm, a single parent, who worked as a cashier for a department store. His father was an accountant from Hamburg named John Möller, whom Brandt never met. As his mother worked six days a week, he was mainly brought up by his mother's stepfather, Ludwig Frahm, and his second wife, Dora.
After passing his Abitur in 1932 at "Johanneum zu Lübeck", he became an apprentice at the shipbroker and ship's agent F.H. Bertling. He joined the "Socialist Youth" in 1929 and the Social Democratic Party (SPD) in 1930. He left the SPD to join the more left wing Socialist Workers Party (SAP), which was allied to the POUM in Spain and the Independent Labour Party in Britain. In 1933, using his connections with the port and its ships, he left Germany for Norway to escape Nazi persecution. It was at this time that he adopted the pseudonym Willy Brandt to avoid detection by Nazi agents. In 1934, he took part in the founding of the International Bureau of Revolutionary Youth Organizations, and was elected to its Secretariat.
Brandt was in Germany from September to December 1936, disguised as a Norwegian student named Gunnar Gaasland. He was married to Gertrud Meyer from Lübeck in a marriage of convenience to protect her from deportation. Meyer had joined Brandt in Norway in July 1933. In 1937, during the Civil War, Brandt worked in Spain as a journalist. In 1938, the German government revoked his citizenship, so he applied for Norwegian citizenship. In 1940, he was arrested in Norway by occupying German forces, but was not identified as he wore a Norwegian uniform. On his release, he escaped to neutral Sweden. In August 1940, he became a Norwegian citizen, receiving his passport from the Norwegian embassy in Stockholm, where he lived until the end of the war. Willy Brandt lectured in Sweden on 1 December 1940 at Bommersvik college about problems experienced by the social democrats in Nazi Germany and the occupied countries at the start of World War II. In exile in Norway and Sweden Brandt learned Norwegian and Swedish. Brandt spoke Norwegian fluently, and retained a close relationship with Norway.
In late 1946, Brandt returned to Berlin, working for the Norwegian government. In 1948, he joined the Social Democratic Party of Germany (SPD) and became a German citizen again, formally adopting the pseudonym, Willy Brandt, as his legal name.
Politician.
From 3 October 1957, to 1966, Willy Brandt was Governing Mayor of Berlin, during a period of increasing tension in East-West relations that led to the construction of the Berlin Wall. In Brandt's first year as mayor, he also served as the president of the Bundesrat in Bonn. Brandt was outspoken against the Soviet repression of the Hungarian Revolution of 1956 and against Nikita Khrushchev's 1958 proposal that Berlin receive the status of a "free city". He was supported by the influential publisher Axel Springer. As mayor of West Berlin, Brandt accomplished much in the way of urban development. New hotels, office-blocks and flats were constructed, while both Schloss Charlottenburg and the Reichstag building were restored. Sections of the "Stadtring" Bundesautobahn 100 inner city motorway were opened, while a major housing programme was carried out, with roughly 20,000 new dwellings built each year during his time in office.
At the start of 1961, U.S. President John F. Kennedy saw Brandt as the wave of the future in West Germany and was hoping he would replace Konrad Adenauer as chancellor following elections later in the year. Kennedy made this preference clear by inviting Brandt, the West German opposition leader, to an official meeting at the White House a month before meeting with Adenauer, the country's leader. The diplomatic snub strained relations between Kennedy and Adenauer further during an especially tense time for Berlin. However, following the building of the Berlin Wall in August 1961, Brandt was disappointed and angry with Kennedy. Speaking in Berlin three days later, Brandt criticized Kennedy, asserting "Berlin expects more than words. It expects political action." He also wrote Kennedy a highly critical public letter in which he warned that the development was liable "to arouse doubts about the ability of the three [Allied] Powers to react and their determination" and he called the situation "a state of accomplished extortion"."
Brandt became the Chairman of the SPD in 1964, a post that he retained until 1987, longer than any other party Chairman since its foundation by August Bebel. Brandt was the SPD candidate for the Chancellorship in 1961, but he lost to Konrad Adenauer's conservative Christian Democratic Union of Germany (CDU). In 1965, Brandt ran again, but lost to the popular Ludwig Erhard. Erhard's government was short-lived, however, and in 1966 a grand coalition between the SPD and CDU was formed, with Brandt as Foreign Minister and Vice-Chancellor.
Chancellor.
At the 1969 elections, again with Brandt as the leading candidate, the SPD became stronger, and after three weeks of negotiations, the SPD formed a coalition government with the smaller Free Democratic Party of Germany (FDP). Brandt was elected Chancellor of the Federal Republic of Germany.
Foreign policy.
As Chancellor, Brandt developed his "Neue Ostpolitik" ("New Eastern Policy"). Brandt was active in creating a degree of rapprochement with East Germany, and also in improving relations with the Soviet Union, Poland, Czechoslovakia, and other Eastern Bloc (communist) countries. A seminal moment came in December 1970 with the famous "Warschauer Kniefall" in which Brandt, apparently spontaneously, knelt down at the monument to victims of the Warsaw Ghetto Uprising. The uprising occurred during the Nazi German military occupation of Poland, and the monument is to those killed by the German troops who suppressed the uprising and deported remaining ghetto residents to the concentration camps for extermination.
"Time" magazine in the U.S. named Brandt as its Man of the Year for 1970, stating, "Willy Brandt is in effect seeking to end World War II by bringing about a fresh relationship between East and West. He is trying to accept the real situation in Europe, which has lasted for 25 years, but he is also trying to bring about a new reality in his bold approach to the Soviet Union and the East Bloc." President Richard Nixon also was pushing détente on behalf of the United States. The policies of Nixon and Henry Kissinger, after some initial suspicion, amounted to co-opting Brandt's Ostpolitik.
In 1971, Brandt received the Nobel Peace Prize for his work in improving relations with East Germany, Poland, and the Soviet Union. Brandt negotiated a peace treaty with Poland, and agreements on the boundaries between the two countries, signifying the official and long-delayed end of World War II. Brandt negotiated parallel treaties and agreements with Czechoslovakia.
In West Germany, Brandt's "Neue Ostpolitik" was extremely controversial, dividing the populace into two camps. One camp embraced all of the conservative parties, and most notably those West German residents and their families who had been driven west ("die Heimatvertriebenen") by Stalinist ethnic cleansing from Historical Eastern Germany, especially the part that was given to Poland as a consequence of the end of the war; western Czechoslovakia (the Sudetenland); and the rest of Eastern Europe, such as in Romania. These groups of displaced Germans and their descendants loudly voiced their opposition to Brandt's policy, calling it "illegal" and "high treason".
A different camp supported and encouraged Brandt's "Neue Ostpolitik" as aiming at "Wandel durch Annäherung" ("change through rapprochement"), encouraging change through a policy of engagement with the (communist) Eastern Bloc, rather than trying to isolate those countries diplomatically and commercially. Brandt's supporters claim that the policy did help to break down the Eastern Bloc's "siege mentality", and also helped to increase its awareness of the contradictions in its brand of Socialism/Communism, which – together with other events – eventually led to the downfall of Eastern European Communism.
Domestic policies.
Political and social changes.
West Germany in the late 1960s was shaken by student disturbances and a general "change of the times" that not all Germans were willing to accept or approve. What had seemed a stable, peaceful nation, happy with its outcome of the "Wirtschaftswunder" ("economic miracle") faced economic turbulence. The German baby-boom generation wanted to come to terms with the deeply conservative, bourgeois, and demanding parent generation. The baby-boomer students were the most outspoken, and they accused their "parental generation" of being outdated and old-fashioned and even of having a Nazi past. Compared to their forebears, the "skeptical generation" was much more capricious, willing to embrace more extreme socialist ideology (such as Maoism), and public heroes (such as Ho Chi Minh, Fidel Castro, and Che Guevara), while living a looser and more promiscuous lifestyle. Students and young apprentices could afford to move out of their parents' homes, and left-wing politics was considered to be "chic", as well as taking part in American-style political demonstrations against having American military forces in South Vietnam.
Brandt's popularity.
Brandt's predecessor as chancellor, Kurt Georg Kiesinger, had been a member of the Nazi party, and was a more old-fashioned conservative-liberal intellectual. Brandt, having fought the Nazis and having faced down communist Eastern Germany during several crises while he was the mayor of Berlin, became a controversial, but credible, figure in several different factions. As the Minister of Foreign Affairs in Kiesinger's grand coalition cabinet, Brandt helped to gain further international approval for Western Germany, and he laid the foundation stones for his future "Neue Ostpolitik". There was a wide public-opinion gap between Kiesinger and Brandt in the West German polls.
Both men had come to their own terms with the new baby boomer lifestyles. Kiesinger considered them to be "a shameful crowd of long-haired drop-outs who needed a bath and someone to discipline them". On the other hand, Brandt needed a while to get into contact with, and to earn credibility among, the "Ausserparlamentarische Opposition" (APO) ("the extra-parliamentary opposition"). The students questioned West German society in general, seeking social, legal, and political reforms. Also, the unrest led to a renaissance of right-wing parties in some of the Bundeslands' (German states under the Bundesrepublik) Parliaments.
Brandt, however, represented a figure of change, and he followed a course of social, legal, and political reforms. In 1969, Brandt gained a small majority by forming a coalition with the FDP. In his first speech before the Bundestag as the Chancellor, Brandt set forth his political course of reforms ending the speech with his famous words, "Wir wollen mehr Demokratie wagen" (literally: "Let's dare more democracy", or more figuratively, "We want to take a chance on more Democracy"). This speech made Brandt, as well as the Social Democratic Party, popular among most of the students and other young West German baby-boomers who dreamed of a country that would be more open and more colorful than the frugal and still somewhat-authoritarian Bundesrepublik that had been built after World War II. However, Brandt's "Neue Ostpolitik" lost him a large part of the German refugee voters from East Germany, who had been significantly pro-SPD in the postwar years.
Chancellor of domestic reform.
Although Brandt is perhaps best known for his achievements in foreign policy, his government oversaw the implementation of a broad range of social reforms, and was known as a "Kanzler der inneren Reformen" ('Chancellor of domestic reform'). According to the historian David Childs, "Brandt was anxious that his government should be a reforming administration and a number of reforms were embarked upon". Within a few years, the education budget rose from 16 billion to 50 billion DM, while one out of every three DM spent by the new government was devoted to welfare purposes. As noted by the journalist and historian Marion Dönhoff,
"People were seized by a completely new feeling about life. A mania for large scale reforms spread like wildfire, affecting schools, universities, the administration, family legislation. In the autumn of 1970 Jürgen Wischnewski of the SPD declared, 'Every week more than three plans for reform come up for decision in cabinet and in the Assembly.'"
According to Helmut Schmidt, Willy Brandt's domestic reform programme had accomplished more than any previous programme for a comparable period. More funds were allocated towards housing, transportation, schools, and communication, while substantial federal benefits were provided for farmers. Various measures were introduced to extend health care coverage, while federal aid to sports organisations increased. A number of liberal social reforms were instituted whilst the welfare state was significantly expanded (with total public spending on social programs nearly doubling between 1969 and 1975), with health, housing, and social welfare legislation bringing about welcome improvements, and by the end of the Brandt Chancellorship West Germany had one of the most advanced systems of welfare in the world.
Substantial increases were made in social security benefits such as injury and sickness benefits, pensions, unemployment benefits, housing allowances, basic subsistence aid allowances, and family allowances and living allowances. In the government's first budget, sickness benefits were increased by 9.3%, pensions for war widows by 25%, pensions for the war wounded by 16%, and retirement pensions by 5%. Numerically, pensions went up by 6.4% (1970), 5.5% (1971), 9.5% (1972), 11.4% (1973), and 11.2% (1974). Adjusted for changes in the annual price index, pensions went up in real terms by 3.1% (1970), 0.3% (1971), 3.9% (1972), 4.4% (1973), and 4.2% (1974). Between 1972 and 1974, the purchasing power of pensioners increased by 19%. In 1970, war pensions were increased by 16%.
In 1970, seagoing pilots became retrospectively insurable, and gained full social security as members of the Non-Manual Workers Insurance Institute. That same year, a special regulation came into force for District Master Chimney Sweeps, making them fully insurable under the Craftsman's Insurance Scheme. An increase was made in tax-free allowances for children, which enabled 1,000,000 families to claim an allowance for the second child, compared to 300,000 families previously. The Second Modification and Supplementation Law (1970) increased the allowance for the third child from DM 50 to DM 60, raised the income-limit for the second child allowance from DM 7,800 to DM 13,200, subsequently increased to DM 15,000 by the third modification law (December 1971), DM 16,800 by the fourth modification law (November 1973), and to DM 18,360 by the fifth modification law (December 1973). A flexible retirement age after 62 years was introduced (1972) for invalids and handicapped persons, and social assistance was extended to those who previously had to be helped by their relatives.
The Third Modification Law (1974) extended individual entitlements to social assistance by means of higher-income limits compatible with receipt of benefits and lowered age limits for certain special benefits. Rehabilitation measures were also extended, child supplements were expressed as percentages of standard amounts and were thus indexed to their changes, and grandparents of recipients were exempted from potential liability to reimburse expenditure of social assistance carrier. The Third Social Welfare Amendment Act (1974) brought considerable improvements for the handicapped, those in need of care, and older persons, and a new fund of 100 million marks for disabled children was established. Allowances for retraining and advanced training and for refugees from East Germany were also increased, together with federal grants for sport. In addition, increases were made in the pensions of 2.5 million war victims. Following a sudden increase in the price of oil, a law was passed in December 1973 granting recipients of social assistance and housing allowances a single heating-oil allowance (a procedure repeated in the winter of 1979 during the Schmidt Administration).
An April 1972 law providing for "promotion of social aid services" aimed to remedy, through various beneficial measures (particularly in the field of national insurance and working conditions), the staff-shortage suffered by social establishments in their medico-social, educational and other work. A bill to harmonize re-education benefit and another bill relating to severely handicapped persons became law in May and September 1972 respectively.
To assist family planning and marriage and family guidance, the government allocated DM 2 232 000 in 1973 for the payment and for the basic and further training of staff. A special effort was also made in 1973 to organize the recreation of handicapped persons, with a holiday guide for the handicapped issued with the aid of the Federal Ministry of Family and Youth Affairs and Health in order to help them find suitable holiday accommodation for themselves and their families. From 1972 to 1973, the total amount of individual aids granted by Guarantee Fund for the integration of young immigrants increased from 17 million DM to 26 million DM. Under a law passed in April 1974, the protection hitherto granted to the victims of war or industrial accidents for the purpose of their occupational and social reintegration was extended to all handicapped persons, whatever the cause of their handicap, provided that their capacity to work had been reduced by at least 50%.
A law on explosives (Sprengstoffgesetz) was the subject of two application ordinances (on the 17th of November 1970 and the 24th of August 1971 ) and a general regulatory provision (the 19th of May 1971), which covered respectively the application of the law to nationals of EC Member States, the duty of employers to notify in time the inspection authorities of detonation plans, the interpretation of the purpose and field of application of the law, authorizations for transport of explosives, and control and recognition of training courses on work with explosives. Taking into account the enormous high peaks of air traffic noise and its concentration at a limited number of airports, the Law for Protection against Aircraft Noise of 1971 sought to balance two conflicting demands, the first being the legitimate demand by industry, business and the public for an efficient air-traffic-system, and secondly, the understandable and by no means less legitimate claims of the affected people for protection and compensation. The legislation regulates the establishment of so-called "Larmschutzzonen" (protection areas against aircraft noise) for all 11 international airports and for those 34 military airports used for jet air craft, and the law authorises the Federal Department of the Interior to decree protection areas for each of those mentioned airports with approval by the "Bundesrat", the representation of the German Federal States.
In the field of health care, various measures were introduced to improve the quality and availability of health care provision. Free hospital care was introduced for 9 million recipients of social relief, while a contributory medical service for 23 million panel patients was introduced. Pensioners were exempted from paying a 2% health insurance contribution, while improvements in health insurance provision were carried out, as characterised by an expanded sickness insurance scheme, with the inclusion of preventative treatment. The income limit for compulsory sickness insurance was indexed to changes in the wage level (1970) and the right to medical cancer screening for 23.5 million people was introduced. In January 1971, the reduction of sickness allowance in case of hospitalisation was discontinued. That same year, compulsory health insurance was extended to the self-employed.
Pupils, students and children in kindergartens were incorporated into the accident insurance scheme, which benefited 11 million children. Free medical checkups were introduced that same year, while the Farmers' Sickness Insurance Law (1972) introduced compulsory sickness insurance for independent farmers, family workers in agriculture, and pensioners under the farmers' pension scheme, medical benefits for all covered groups, and cash benefits for family workers under compulsory coverage for pension insurance. Participation in employer's health insurance was extended to four million employees. A Development Law of December 1970 made it possible for all employees voluntarily to become members of the statutory sickness insurance. The level of income for compulsory sickness insurance was indexed to 75% of the respective assessment level for pension insurance, while voluntarily insured employees were granted a claim to an allowance towards their sickness insurance from their employer. This law also introduced a new type of sickness insurance benefit, namely facilities for the early diagnosis of disease. Apart from the discretionary service of disease prevention which had existed since 1923, insured persons now had a right in certain circumstances to medical examinations aimed at the early diagnosis of disease. According to one study, this marked a change in the concept of sickness insurance: it now aimed at securing good health.
The Hospital Financing Law (1972) secured the supply of hospitals and reduced the cost of hospital care, "defined the financing of hospital investment as a public responsibility, single states to issue plans for hospital development, and the federal government to bear the cost of hospital investment covered in the plans, rates for hospital care thus based on running costs alone, hospitals to ensure that public subsidies together with insurance fund payments for patients cover total costs". The Benefit Improvement Law (1973) made entitlement to hospital care legally binding (entitlements already enjoyed in practice), abolished time limits for hospital care, introduced entitlement to household assistance under specific conditions, and also introduced entitlement to leave of absence from work and cash benefits in the event of a child's illness. In 1971, to encourage the growth of registered family holiday centres, the Federal Government granted subsidies for the building and appointing of 28 of these centres at a total cost of 8 million DM.
The Pension Reform Law (1972) guaranteed all retirees a minimum pension regardless of their contributions and institutionalized the norm that the standard pension (of average earners with forty years of contributions) should not fall below 50% of current gross earnings. The 1972 pension reforms improved eligibility conditions and benefits for nearly every subgroup of the West German population. The income replacement rate for employees who made full contributions was raised to 70% of average earnings. The reform also replaced 65 as the mandatory retirement age with a "retirement window" ranging between 63 and 65 for employees who had worked for at least thirty-five years. Employees who qualified as disabled and had worked for at least thirty-five years were extended a more generous retirement window, which ranged between the ages of 60 and 62. Women who had worked for at least fifteen years (ten of which had to be after the age of age 40) and the long-term unemployed were also granted the same retirement window as the disabled. In addition, there were no benefit reductions for employees who had decided to retire earlier than the age of 65. The legislation also changed the way in which pensions were calculated for low-income earners who had been covered for twenty-five or more years. If the pension benefit fell below a specified level, then such workers were allowed to substitute a wage figure of 75% of the average wage during this period, thus creating something like a minimum wage benefit.
Voluntary retirement at 63 with no deductions in the level of benefits was introduced, together with the index-linking of war victim's pensions to wage increases. Guaranteed minimum pension benefits for all West Germans were introduced, along with automatic pension increases for war widows (1970). Fixed minimum rates for women in receipt of very low pensions were also introduced, together with equal treatment for war widows. A pension reform package incorporated an additional year of insurance for mothers, while improvements in pension provision were made for women and the self-employed. A new minimum pension for workers with at least twenty-five years' insurance was introduced, while faster pension indexation was implemented, with the annual adjustment of pensions brought forward by six months, and the Seventh Modification Law (1973) linked the indexation of farmers' pensions to the indexation of the general pension insurance scheme.
In education, the Brandt Administration sought to widen educational opportunities for all West Germans. An addition was made to the Basic Law which gave the Federal Government some responsibility for educational planning. A big increase in spending on education was carried out, with educational expenses per head of the population multiplied by five, while the government presided over an increase in the number of teachers. Generous public stipends were introduced for students to cover their living costs, while West German universities were converted from elite schools into mass institutions. The school leaving age was raised to 16, and spending on research and education increased by nearly 300% between 1970 and 1974. Fees for higher or further education were abolished, while a considerable increase in the number of higher education institutions took place. A much needed school and college construction program was carried out, together with the introduction of postgraduate support for highly qualified graduates, providing them with the opportunity to earn their doctorates or undertake research studies.
Grants were introduced for pupils from lower income groups to stay on at school, together with grants for those going into any kind of higher or further education. Increases were also made in educational allowances, as well as spending on science. In 1972, the government allocated 2.1 million DM in grants to promote marriage and family education. The Brandt Administration also introduced enabling legislation for the introduction of comprehensives, but left it to the Lander "to introduce them at their discretion." While the more left-wing Lander "rapidly began to do so," other Lander found "all sorts of pretexts for delaying the scheme." By the mid-Eighties, Berlin had 25 comprehensives while Bavaria only had 1, and in most Lander comprehensives were still viewed as "merely experimental."
In the field of housing, various measures were carried out to benefit householders, such as in improving the rights of tenants and increasing rental subsidies. Increased levels of protection and support for low-income tenants and householders led to a drop in the number of eviction notices. By 1974, three times as much was paid out in rent subsidies as in 1969, and nearly one and a half million households received rental assistance. Increases were made in public housing subsidies, as characterised by a 36% increase in the social housing budget in 1970 and by the introduction of a programme for the construction of 200,000 public housing units (1971). From 1970 to 1971, an 18.1% increase in building permits for social housing units was made.
A loose form of rent regulation was introduced under the name of "Vergleichmieten" ('comparable rents'), together with the provision of "for family-friendly housing" freight or rent subsidies to owners of apartments or houses whose ceiling had been adapted to increased expenses or incomes (1970). In addition, a law for the creation of property for workers was passed, under which a married worker would normally keep up to 95% of his pay, and graded tax remission for married wage-earners applied up to a wage of 48,000 marks, which indicated the economic prosperity of West Germany at that time. The Town Planning Act (1971) encouraged the preservation of historical heritage and helped open up the way to the future of many German cities, while the Urban Renewal Act (1971) helped the states to restore their inner cities and to develop new neighbourhoods.
The Second Housing Allowance Law of December 1970 simplified the administration of housing allowances and extended entitlements, increased the income limit to 9,600 DM per year plus 2,400 DM for each family member, raised the general deduction on income to determine reckonable income from 15% to 20%, allowance rates listed in tables replacing complicated calculation procedure based on "bearable rent burdens." The Housing Construction Modification Law (1971) increased the income-limit for access to low rent apartments under the social housing programme from 9,000 DM to 12,000 DM per annum plus 3,000 DM (instead of 2,400) for each family member. The law also introduced special subsidies to reduce the debt burden for builders not surpassing the regular income-limit by more than 40%. Under a 1973 law, the limits were increased to 1,000 DM plus 9,000 DM and 4,200 DM for additional family members. The Rent Improvement Law (1971) strengthened the position of tenants. Under this legislation, notice was to be ruled illegal "where appropriate substitute accommodation not available; landlords obliged to specify reasons for notice," whilst the Eviction Protection Law (1971) established tenant protection against rent rises and notice. The notice was only lawful if in the "justified interest of the landlord." Under this law, higher rents were not recognised as "justified interest." The Second Eviction Protection Law (1972) made the tenant protection introduced under the Eviction Protection Law of 1971 permanent. Under this new law, the notice was only lawful where the landlord proved justified personal interest in the apartment. In addition, rent increases were only lawful if not above normal comparable rents in the same area.
Directives on the housing of foreign workers came into force in April 1971. These directives imposed certain requirements for space, hygiene, safety, and amenities in the accommodation offered by employers. That same year, the Federal Government granted a sum of 17 million DM to the Länder for the improvement and modernization of housing built before 21 June 1948. The "German Council for town development", which was set up by virtue of Article 89 of a law to foster urban building, was partly aimed at planning a favourable environment for families (such as the provision of playgrounds). In 1971, the Federal Labour Office made available DM 425 million in the form of loans to provide 157 293 beds in 2 494 hostels. A year later, the Federal Government (Bund), the Lander and the Federal Labour Office promoted the construction of dwellings for migrant workers. They set aside 10 million DM for this purpose, which allowed the financing of 1650 family dwellings that year.
Development measures were begun in 1972 with federal financial aid granted to the Lander for improvement measures relating to towns and villages, and in the 1972 budget, DM 50 million was earmarked, i.e. a third of the total cost of some 300 schemes. A council for urban development was formed in May 1972 with the purpose of promoting future work and measures in the field of urban renovation. In 1973, the government provided assistance of DM 28 million for the modernisation of old dwellings.
New rules were introduced regarding improvements in the law relating to rented property, control of the rise in rents and protection against cancellation of leases also protected the rights of migrant workers in the sphere of housing. A law of July 1973 fixed the fundamental and minimum requirements regarding workers' dwellings, mainly concerning space, ventilation and lighting, protection against damp, heat and noise, power and heating facilities and sanitary installations.
In regards to civil rights, the Brandt Administration introduced a broad range of socially liberal reforms aimed at making West Germany a more open society. Greater legal rights for women were introduced, as exemplified by the standardisation of pensions, divorce laws, regulations governing use of surnames, and the introduction of measures to bring more women into politics. The voting age was lowered from 21 to 18, the age of eligibility for political office was lowered to 21, and the age of majority was lowered to 18 (March 1974). The Third Law for the Liberalization of the Penal Code (1970) liberalised "the right to political demonstration", while equal rights were granted to illegitimate children that same year. An amendment to a federal civil service reform bill (1971) enabled fathers to apply for part-time civil service work. Reforms were carried out to the armed forces, as characterised by a reduction in basic military training from eighteen to fifteen months and a reorganisation of education and training, as well as personnel and procurement procedures. In 1971, corporal punishment was banned in schools. A measure was introduced in 1973 that facilitated the adoption of young children by reducing the minimum age for adoptive parents from 35 to 25.
A women's policy machinery at the national level was established (1972) while amnesty was guaranteed in minor offences connected with demonstrations. A bill was also passed which introduced entitlement for the victims of violence to compensation under the Federal Supply Act, "if the offender or not determined or to pay compensation can not be used." From 1970 onwards, parents as well as landlords were no longer legally prohibited “to give or rent rooms or flats to unmarried couples or to allow them to stay overnight.”
Legislation aimed at safeguarding consumers was also implemented under the Brandt Administration. The consumer's right of withdrawal in case of hire purchase was strengthened (March 1974), and fixed prices for branded products were abolished by law (January 1974) which meant that manufacturers' recommended prices were not binding for retailers. In addition, a progressive anticartel law was passed.
In terms of working conditions, a number of reforms were introduced aimed at strengthening the rights of workers both at home and in the workplace. The Sickness Act of 1970 provided equal treatment of workers and employees in the event of incapacity for work, while maternity leave was increased. Legislation was introduced which ensured continued payment of wages for workers disabled by illness (1970). In 1970 all employees unit for work (with the exception of women in receipt of maternity benefits and temporarily and inconsiderably employed persons) were provided with an unconditional legal claim against their employer to continued payment of their gross wage for a period of 6 weeks, as also in the case of spa treatment approved by an Insurance Fund, the Fund bearing the full cost thereof. Previously, payment of employer's supplement and sick pay were only made from the day on which the doctor certified unfitness for work. In 1972, an Act on Agency Work was passed which sought to prevent works agencies from providing job placement services and aimed to provide job minimum protection for employees in agency work. A law on the hiring out of manpower, passed in October 1972, contained provisions to stipulate prior authorization for the hiring out of manpower, to draw a distinction between the system governing workers hired out and the placing of workers, to regulate and improve the rights of hired out workers pertaining to working conditions and social insurance, and provide for more severe penalties and fines to be imposed on offenders.
Improvements were also made in income and work conditions for home workers, together with an extension of paid annual leave and an extension of the dismissal deadline. Accident insurance was extended to non-working adults. The government also introduced a new employment law for young people, including providing for the introduction of the five-day week of 40 hours and longer holidays. In addition, the Border Zone Assistance Act (1971) increased levels of assistance to the declining zonal peripheral area. The Occupational Safety Act (1973) required employers to provide company doctors and safety experts. A directive on protection against noise at the place of work was adopted in November 1970. If measurements showed or there was reason to assume that a noise level guide value of 90 dB( A) may be exceeded at the place of work, then the authority had to instruct the employer to arrange check-ups of the employees concerned and these employees had to use personal noise protection devices. A matching fund program for 15 million employees was also introduced, which stimulated them to accumulate capital.
In a directive of 10 November 1970, the Minister of Labour and Social Affairs recommended to the higher authorities for work protection of the "Lander" to bring in the directive published, in agreement with the Ministry of Labour, by the German Engineers' Association on the evaluation of work station noise in relation to loss of hearing, in order to improve safeguards for workers against the noises in question. In September 1971, an ordinance was published concerning dangerous working materials. It safeguarded persons using these materials against the dangers involved. In August 1971, a law came into force which was directed at reducing atmospheric pollution from lead compounds in four-stroke engine fuels. As a safeguard against radiation, a decree on the system of authorisations for medicaments treated with ionizing radiation or containing radioactive substances, in its version of 8 August 1967, was remodelled by a new Decree of 10 May 1971 which added some radionuclides to the list of medicaments which doctors in private practice were authorized to use. A law on individual promotion of vocational training came into force in October 1971. It provided for financial grants for attendance at further general or technical teaching establishments from the second year of studies at higher technical schools, academies and higher education establishments, training centres of second degree, or certain courses of television teaching. Grants were also made in certain cases for attendance at training centres located outside the Federal Republic.
By a decree of the Federal Minister for Labour and Social Order, the Federal Institute for Industrial Protection became the Federal Agency for Industrial Protection and Accident Research. Amongst its designated tasks included the promotion of industrial protection, accident prevention on the journey to and from work and accident prevention in the home and leisure activities, the encouragement of training and advanced training in the area of industrial protection, and to promote and coordinate accident research. A regulation was issued in 1972 which permitted for the first time the employment of women as drivers of trams, omnibuses and lorries, while further regulations laid down new provisions for lifts and work with compressed air.
The Factory Constitution Law (1971) strengthened the rights of individual employees "to be informed and to be heard on matters concerning their place of work." The Works' Council was provided with greater authority while trade unions were given the right of entry into the factory "provided they informed the employer of their intention to do so", while a law was passed to encourage wider share ownership by workers and other rank-and-file employees. The Industrial Relations Law (1972) and the Personnel Representation Act (1974) broadened the rights of employees in matters which immediately affected their places of work, while also improving the possibilities for codetermination on operations committees, together with access of trade unions to companies. The Works Constitution Act of 1972 required in cases of collective dismissal at an establishment normally employing more than twenty employees that management and the works council must negotiate a social plan that stipulates compensation for workers who lose their jobs. In cases where the two parties cannot agree on a social plan, the law provides for binding arbitration. In 1972, the rights of works councils to information from management were not only strengthened, but works councils were also provided with full codetermination rights on issues such as working time arrangements in the plant, the setting of piece rates, plant wage systems, the establishment of vacation times, work breaks, overtime, and short-time work. Legislation was passed which acknowledged for the first time the presence of trade unions in the workplace, expanded the means of action of the works councils, and improved their work basics as well as those of the youth councils.
A law of January 1972 on the organization of labour in enterprises significantly extended the works council's right of cooperation and co-management in the matter of vocational training. That same year, the Safety Institute of the Federal Republic of Germany was transformed into a public Federal Agency (Bundesanstalt) with significantly enlarged powers, in the context of which special emphasis would be placed on its new task of promoting and coordinating research in the area of accident prevention. A law of 18 January 1974, designed to protect members of the supervisory boards of companies who are undergoing training, was aimed at ensuring that the representatives of young workers and youthful members of works councils still undergoing training could perform their duties with greater independence and without fear of disadvantageous consequences for their future careers.
New provisions were introduced for the rehabilitation of severely disabled people ("Schwerbehinderte") and accident victims. The Severely Disabled Persons Act (1974) obliged all employers with more than fifteen employees to ensure that 6% of their workforce consisted of people officially recognised as being severely handicapped. Employers who failed to do so were assessed 100 DM per month for every job falling before the required quota. These compensatory payments were used to "subsidise the adaptation of workplaces to the requirements of those who were severely handicapped."
A law passed in January 1974, designed to protect members of the supervisory boards of companies who are undergoing training, was aimed at ensuring that the representatives of young workers and youthful members of works councils still undergoing training could perform their duties with greater independence and without fear of disadvantageous consequences for their future careers. On request, workers' representatives on completion of their training courses had to have an employment relationship of unlimited duration. In the field of transport, the Municipal Transportation Finance Law was passed in 1971, which established federal guidelines for subsidies to municipal governments, while the Federal Transport Plan of 1973 provided a framework for all transport, including public transport.
A federal environmental programme was established in 1971, followed by a Federal Environment Agency in 1974 to conduct research into environmental issues and prevent pollution. Matching grants covering 90% of infrastructure development were allocated to local communities, which led to a dramatic increase in the number of public swimming pools and other facilities of consumptive infrastructure throughout West Germany. The federal crime-fighting apparatus was also modernised, while a Foreign Tax Act was passed which limited the possibility of tax evasion. In addition, efforts were made to improve the railways and motorways.
Under the Brandt Administration, West Germany attained a lower rate of inflation than in other industrialised countries at that time. while a rise in the standard of living took place, helped by the floating and revaluation of the mark. This was characterised by the real incomes of employees increasing more sharply than incomes from entrepreneurial work, with the proportion of employees' incomes in the overall national income rising from 65% to 70% between 1969 and 1973, while the proportion of income from entrepreneurial work and property fell over that same period from just under 35% to 30%.
1972 crisis.
Brandt's "Ostpolitik" led to a meltdown of the narrow majority Brandt's coalition enjoyed in the "Bundestag". In October 1970, FDP deputies Erich Mende, Heinz Starke, and Siegfried Zoglmann crossed the floor to join the CDU. On 23 February 1972, SPD deputy Herbert Hupka, who was also leader of the "Bund der Vertriebenen", joined the CDU in disagreement with Brandt's reconciliatory efforts towards the east. On 23 April 1972, Wilhelm Helms (FDP) left the coalition ; the FDP politicians Knud von Kühlmann-Stumm and Gerhard Kienbaum also declared that they would vote against Brandt; thus, Brandt had lost his majority. On 24 April 1972 a vote of no confidence was proposed and it was voted on three days later. Had this motion passed, Rainer Barzel would have replaced Brandt as Chancellor. To everyone's surprise, the motion failed: Barzel got only 247 votes out of 260 ballots; for an absolute majority, 249 votes would have been necessary. There were also 10 votes against the motion and three invalid ballots. Most deputies of SPD and FDP did not take part in the voting, as not voting had the same effect as voting for Brandt. It was not revealed until much later that two "Bundestag" members (Julius Steiner and Leo Wagner, both of the CDU/CSU) had been bribed by the East German Stasi to vote for Brandt.
New elections.
Though Brandt remained Chancellor, he had lost his majority. Subsequent initiatives in parliament, most notably on the budget, failed. Because of this stalemate, the Bundestag was dissolved and new elections were called. During the 1972 campaign, many popular West German artists, intellectuals, writers, actors and professors supported Brandt and the SPD. Among them were Günter Grass, Walter Jens, and even the soccer player Paul Breitner. Brandt's Ostpolitik as well as his reformist domestic policies were popular with parts of the young generation and led his SPD party to its best-ever federal election result in late 1972. The "Willy-Wahl", Brandt's landslide win was the beginning of the end; and Brandt's role in government started to decline.
Many of Brandt's reforms met with resistance from state governments (dominated by CDU/CSU). The spirit of reformist optimism was cut short by the 1973 oil crisis and the major public services strike 1974, which gave Germany's trade unions, led by Heinz Kluncker, a big wage increase but reduced Brandt's financial leeway for further reforms. Brandt was said to be more a dreamer than a manager and was personally haunted by depression. To counter any notions about being sympathetic to Communism or soft on left-wing extremists, Brandt implemented tough legislation that barred "radicals" from public service ("Radikalenerlass").
Guillaume affair.
Around 1973, West German security organizations received information that one of Brandt's personal assistants, Günter Guillaume, was a spy for the East German intelligence services. Brandt was asked to continue working as usual, and he agreed to do so, even taking a private vacation with Guillaume. Guillaume was arrested on 24 April 1974, and many blamed Brandt for having a communist spy in his inner circle. Thus disgraced, Brandt resigned from his position as chancellor on 6 May 1974. However, he remained in the Bundestag and as chairman of the Social Democrats through 1987.
This espionage affair is widely considered to have been just the trigger for Brandt's resignation, not the fundamental cause. Brandt was dogged by scandals about serial adultery, and reportedly also struggled with alcohol and depression. There was also the economic fallout on West Germany of the 1973 oil crisis, which almost seems to have been enough stress to finish off Brandt as the Chancellor. As Brandt himself later said, "I was exhausted, for reasons which had nothing to do with the trial [the Guillaume espionage scandal] going on at the time." 
Guillaume had been an espionage agent for East Germany, who was supervised by Markus Wolf, the head of the "Main Intelligence Administration" of the East German Ministry for State Security. Wolf stated after the reunification that the resignation of Brandt had never been intended, and that the planting and handling of Guillaume had been one of the largest mistakes of the East German secret services.
Brandt was succeeded as the Chancellor of the Bundesrepublik by his fellow Social Democrat, Helmut Schmidt. For the rest of his life, Brandt remained suspicious that his fellow Social Democrat (and longtime rival) Herbert Wehner had been scheming for Brandt's downfall. However, there is scant evidence to corroborate this suspicion.
Ex-Chancellor.
After his term as the Chancellor, Brandt retained his seat in the Bundestag, and he remained the Chairman of the Social Democratic Party through 1987. Beginning in 1987, Brandt stepped down to become the Honorary Chairman of the party. Brandt was also a member of the European Parliament from 1979 to 1983.
Socialist International.
For sixteen years, Brandt was the president of the Socialist International (1976–92), during which period the number of Socialist International's mainly European member parties grew until there were more than a hundred socialist, social democratic, and labour political parties around the world. For the first seven years, this growth in SI membership had been prompted by the efforts of the Socialist International's Secretary-General, the Swede Bernt Carlsson. However, in early 1983, a dispute arose about what Carlsson perceived as the SI president's authoritarian approach. Carlsson then rebuked Brandt saying: "this is a Socialist International — not a German International".
Next, against some vocal opposition, Brandt decided to move the next Socialist International Congress from Sydney, Australia to Portugal. Following this SI Congress in April 1983, Brandt retaliated against Carlsson by forcing him to step down from his position. However, the Austrian Prime Minister, Bruno Kreisky, argued on behalf of Brandt: "It is a question of whether it is better to be pure or to have greater numbers".
Carlsson was succeeded by the Finn, Pentti Väänänen as Secretary General of the Socialist International 
During Willy Brandt's presidency the SI developed activities and dialogue on a number of International issues. This concerned the East-West conflict and arms race where the SI held high level consultations with the leaderships of the United States and the Soviet Union. The SI met with such leaders as President Jimmy Carter and Vice-Presidents Walter Mondale and George Bush. They also met with the Secretaries General Leonid Brezhnev and Michail Gorbachev and with the Soviet President Andrei Gromyko. The SI also developed active contacts to promote dialogue concerning regional conflicts. Those included the Middle East, where they helped to build contacts between Israel and the PLO, and also in Southern Africa and Central America.
Brandt Report.
In 1977, Brandt was appointed as the chairman of the Independent Commission for International Developmental Issues. This produced a report in 1980, which called for drastic changes in the global attitude towards development in the Third World. This became known as the Brandt Report.
Reunification.
In October 1979, Brandt met with the East German dissident, Rudolf Bahro, who had written "The Alternative". Bahro and his supporters were attacked by the East German state security organization Stasi, headed by Erich Mielke, for his writings, which had laid the theoretical foundation of a leftist opposition to the ruling SED party and its dependent allies, and which promoted new and changed parties. All of this is now described as "change from within". Brandt had asked for Bahro's release, and Brandt welcomed Bahro's theories, which advanced the debate within his own Social Democratic Party. In late 1989, Brandt became one of the first leftwing leaders in West Germany to publicly favor a quick reunification of Germany, instead of some sort of two-state federation or other kind of interim arrangement. Brandt's public statement "Now grows together what belongs together," was widely quoted in those days.
Hostages in Iraq.
One of Brandt's last public appearances was in flying to Baghdad, Iraq, to free Western hostages held by Saddam Hussein, following the Iraqi invasion of Kuwait in 1990. Brandt secured the release of a large number of them, and on 9 November 1990, his airplane landed with 174 freed hostages on board at the Frankfurt Airport.
Death and memorials.
Willy Brandt died of colon cancer at his home in Unkel, a town on the Rhine River, on 8 October 1992, and was given a state funeral. He was buried at the cemetery at Zehlendorf in Berlin.
When the SPD moved its headquarters from Bonn back to Berlin in the mid-1990s, the new headquarters was named the "Willy Brandt Haus". One of the buildings of the European Parliament in Brussels was named after him in 2008.
On 6 December 2000, a memorial to Willy Brandt and "Warschauer Kniefall" was unveiled in Warsaw, Poland.
German artist Johannes Heisig painted several portraits of Brandt of which one was unveiled as part of an honoring event at German Historical Institute Washington, DC on 18 March 2003. Spokesmen amongst others were former German Federal Minister Egon Bahr and former U.S. Secretary of state Henry Kissinger.
In 2009, the Willy-Brandt-Memorial was opened up in Nuremberg at the Willy-Brandt Square. It was created by the artist Josef Tabachnyk.
In 2009, the University of Erfurt renamed its graduate school of public administration as the Willy Brandt School of Public Policy. A private German-language secondary school in Warsaw, Poland, is also named after Brandt.
The main boulevard on the north entrance to Montenegrin capital Podgorica is named Willy Brandt Boulevard in 2011.
Brandt's family.
From 1941 until 1948 Brandt was married to Anna Carlotta Thorkildsen (the daughter of a Norwegian father and a German-American mother). They had a daughter, Ninja Brandt (born in 1940). After Brandt and Thorkildsen were divorced in 1948, Brandt married the Norwegian Rut Hansen in the same year. Hansen and Brandt had three sons: Peter Brandt (born in 1948), Lars Brandt (born in 1951) and Matthias Brandt (born in 1961). After 32 years of marriage, Willy Brandt and Rut Hansen Brand divorced in 1980, and from the day that they were divorced they never saw each other again. On 9 December 1983, Brandt married Brigitte Seebacher (born in 1946).
References.
Bibliography.
</dl>

</doc>
<doc id="49261" url="http://en.wikipedia.org/wiki?curid=49261" title="Corporate personhood">
Corporate personhood

Corporate personhood is an American legal concept that a corporation, as a group of people, may be recognized as having some of the same legal rights and responsibilities as an individual. For example, corporations may contract with other parties and sue or be sued in court in the same way as natural persons or unincorporated associations of persons. The doctrine does not hold that corporations are flesh and blood "people" apart from their shareholders, executives, and managers, nor does it grant to corporations all of the rights of citizens.
Since at least "Trustees of Dartmouth College v. Woodward" – 17 U.S. 518 (1819), the U.S. Supreme Court has recognized corporations as having the same rights as natural persons to contract and to enforce contracts. In "Santa Clara County v. Southern Pacific Railroad" – 118 U.S. 394 (1886), the court reporter, Bancroft Davis, noted in the headnote to the opinion that the Chief Justice Morrison Waite began oral argument by stating, "The court does not wish to hear argument on the question whether the provision in the Fourteenth Amendment to the Constitution, which forbids a State to deny to any person within its jurisdiction the equal protection of the laws, applies to these corporations. We are all of the opinion that it does." While the headnote is not part of the Court's opinion and thus not precedent, two years later, in "Pembina Consolidated Silver Mining Co. v. Pennsylvania" – 125 U.S. 181 (1888), the Court clearly affirmed the doctrine, holding, "Under the designation of 'person' there is no doubt that a private corporation is included [in the Fourteenth Amendment]. Such corporations are merely associations of individuals united for a special purpose and permitted to do business under a particular name and have a succession of members without dissolution." This doctrine has been reaffirmed by the Court many times since.
Corporate personhood in the United States.
Corporations as persons in the United States.
As a matter of interpretation of the word "person" in the Fourteenth Amendment, U.S. courts have extended certain constitutional protections to corporations. Opponents of corporate personhood seek to amend the U.S. Constitution to limit these rights to those provided by state law and state constitutions.
The basis for allowing corporations to assert protection under the U.S. Constitution is that they are organizations of people, and the people should not be deprived of their constitutional rights when they act collectively. In this view, treating corporations as "persons" is a convenient legal fiction which allows corporations to sue and to be sued, provides a single entity for easier taxation and regulation, simplifies complex transactions that would otherwise involve, in the case of large corporations, thousands of people, and protects the individual rights of the shareholders as well as the right of association.
Generally, corporations are not able to claim constitutional protections that would not otherwise be available to persons acting as a group. For example, the Supreme Court has not recognized a Fifth Amendment right against self-incrimination for a corporation, since the right can be exercised only on an individual basis. In "United States v. Sourapas and Crest Beverage Company", "[a]ppellants [suggested] the use of the word 'taxpayer' several times in the regulations requires the fifth-amendment self-incrimination warning be given to a corporation." The Court did not agree.
Since the Supreme Court's ruling in "Citizens United v. Federal Election Commission" in 2010, upholding the rights of corporations to make political expenditures under the First Amendment, there have been several calls for a U.S. Constitutional amendment to abolish Corporate Personhood, even though the Citizens United majority opinion makes no reference to corporate personhood or to the Fourteenth Amendment.
Historical background in the United States.
During the colonial era, British corporations were chartered by the crown to do business in North America. This practice continued in the early United States. They were often granted monopolies as part of the chartering process. For example, the controversial Bank Bill of 1791 chartered a 20-year corporate monopoly for the First Bank of the United States. Although the Federal government has from time to time chartered corporations, the general chartering of corporations has been left to the states. In the late 18th and early 19th centuries, corporations began to be chartered in greater numbers by the states, under general laws allowing for incorporation at the initiative of citizens, rather than through specific acts of the legislature.
The degree of permissible government interference in corporate affairs was controversial from the earliest days of the nation. In 1790, John Marshall, a private attorney and a veteran of the Continental Army, represented the board of the College of William and Mary, in litigation that required him to defend the corporation's right to reorganize itself and in the process remove professors, "The Rev John Bracken v. The Visitors of Wm & Mary College" (7 Va. 573; 1790 Supreme Court of Virginia). The Supreme Court of Virginia ruled that the original crown charter provided the authority for the corporation's Board of Visitors to make changes including the reorganization.
As the 19th century matured, manufacturing in the U.S. became more complex as the Industrial Revolution generated new inventions and business processes. The favored form for large businesses became the corporation because the corporation provided a mechanism to raise the large amounts of investment capital large business required, especially for capital intensive yet risky projects such as railroads.
The Civil War accelerated the growth of manufacturing and the power of the men who owned the large corporations. Businessmen such as Mark Hanna, sugar trust magnate Henry O. Havemeyer, banker J. P. Morgan, steel makers Charles M. Schwab and Andrew Carnegie, and railroad owners Cornelius Vanderbilt and Jay Gould created corporations which influenced legislation at the local, state, and federal levels as they built businesses that spanned multiple states and communities. After the adoption of the 14th Amendment in 1868, there was some question as to whether the Amendment applied to other than freed slaves, and whether its protections could be invoked by corporations and other organizations of persons.
The primary purpose of the 14th Amendment was undoubtedly to protect freed slaves. However, the Amendment applies to all Americans, not only freed slaves and their descendants.
Following the reasoning of the Dartmouth College case and other precedents (see below, Case law in the United States), corporations could exercise the rights of their shareholders and these shareholders were entitled to some of the legal protections against arbitrary state action. Their cause was strengthened by the adoption of general incorporation statutes in the states in the late 19th century, most notably in New Jersey and Delaware, which allowed anyone to form corporations without any particular government grant or authorization, and thus without the government-granted monopolies that had been common in charters granted by the Crown or by acts of the legislature. See Delaware General Corporation Law. In "Santa Clara County v. Southern Pacific Railroad" (1886), the Supreme Court held, "ipse dixit", that the Fourteenth Amendment applied to corporations. Since then the Court has repeatedly reaffirmed this protection.
Case law in the United States.
In 1818, the United States Supreme Court decided "Dartmouth College v. Woodward", 17 U.S. 518 (1819), writing: "The opinion of the Court, after mature deliberation, is that this corporate charter is a contract, the obligation of which cannot be impaired without violating the Constitution of the United States. This opinion appears to us to be equally supported by reason, and by the former decisions of this Court."
Seven years after the Dartmouth College opinion, the Supreme Court decided "Society for the Propagation of the Gospel in Foreign Parts v. Town of Pawlet" (1823), in which an English corporation dedicated to missionary work, with land in the U.S., sought to protect its rights to the land under colonial-era grants against an effort by the state of Vermont to revoke the grants. Justice Joseph Story, writing for the court, explicitly extended the same protections to corporate-owned property as it would have to property owned by natural persons. Seven years later, Chief Justice Marshall stated; "The great object of an incorporation is to bestow the character and properties of individuality on a collective and changing body of men."
In the 1886 case "Santa Clara v. Southern Pacific", the Chief Justice Waite of the Supreme Court orally directed the lawyers that the Fourteenth Amendment equal protection clause guarantees constitutional protections to corporations in addition to natural persons, and the oral argument should focus on other issues in the case.
The 14th Amendment does not insulate corporations from all government regulation, any more than it relieves individuals from all regulatory obligations. Thus, for example, in "Northwestern Nat Life Ins. Co. v. Riggs" (203 U.S. 243 (1906)), the Court accepted that corporations are for legal purposes "persons," but still ruled that the Fourteenth Amendment was not a bar to many state laws which effectively limited a corporation's right to contract business as it pleased. However, this was not because corporations were not protected under the Fourteenth Amendment - rather, the Court's ruling was that the Fourteenth Amendment did not prohibit the type of regulation at issue, whether of a corporation or of sole proprietorship or partnership.
Opinions by two long serving Supreme Court judges, Hugo Black and William O. Douglas, indicate the extent to which corporate personhood is not an all-or-nothing doctrine, but rather relates to the purpose of government regulation and the underlying rights of the individuals making up the corporation. In a case challenging corporate tax rates, Justice Black wrote:
If the people of this nation wish to deprive the states of their sovereign rights to determine what is a fair and just tax upon corporations doing a purely local business within their own state boundaries, there is a way provided by the Constitution to accomplish this purpose. That way does not lie along the course of judicial amendment to that fundamental charter. An amendment having that purpose could be submitted by Congress as provided by the Constitution. I do not believe that the Fourteenth Amendment had that purpose, nor that the people believed it had that purpose, nor that it should be construed as having that purpose.
Justice Douglas, dissenting in "Wheeling Steel Corp. v. Glander" (337 U.S. 562, 1949), gave an opinion similar to, but shorter than, the one quoted above, to which Justice Black concurred.
By the time of those opinions, political contributions to candidates in federal races by corporations had been prohibited since the Tillman Act of 1907, even though individual contributions remained unlimited. Yet both Justice Black and Justice Douglas dissented from the Supreme Court's 1957 decision in "United States v. United Auto Workers", 352 U.S. 567 (1957), in which the Court, on procedural grounds, overruled a lower court decision striking down the prohibition on corporate and union political expenditures:
We deal here with a problem that is fundamental to the electoral process and to the operation of our democratic society. It is whether a union can express its views on the issues of an election and on the merits of the candidates, unrestrained and unfettered by the Congress. The principle at stake is not peculiar to unions. It is applicable as well to associations of manufacturers, retail and wholesale trade groups, consumers' leagues, farmers' unions, religious groups, and every other association representing a segment of American life and taking an active part in our political campaigns and discussions. It is as important an issue as has come before the Court, for it reaches the very vitals of our system of government.
Under our Constitution, it is We The People who are sovereign. The people have the final say. The legislators are their spokesmen. The people determine through their votes the destiny of the nation. It is therefore important -- vitally important -- that all channels of communication be open to them during every election, that no point of view be restrained or barred, and that the people have access to the views of every group in the community. Thus the two justices would have adjudicated the case and upheld the lower court opinion striking down the ban on corporate and union spending.
Although it is now well settled law that the 14th Amendment extends to corporations, the extent to which it should attach to corporations has continued to draw criticism from liberal legal theorists.
Legislation in the United States.
The laws of the United States hold that a legal entity (like a corporation or non-profit organization) shall be treated under the law as a person except when otherwise noted. This rule of construction is specified in 1 U.S.C. §1 (United States Code), which states:
In determining the meaning of any Act of Congress, unless the context indicates otherwise--
the words "person" and "whoever" include corporations, companies, associations, firms, partnerships, societies, and joint stock companies, as well as individuals;
This federal statute has many consequences. For example, a corporation is allowed to own property and enter contracts. It can also sue and be sued and held liable under both civil and criminal law. As well, because the corporation is legally considered the "person," individual shareholders are not legally responsible for the corporation's debts and damages beyond their investment in the corporation. Similarly, individual employees, managers, and directors are liable for their own malfeasance or lawbreaking while acting on behalf of the corporation, but are not generally liable for the corporation's actions. Among the most frequently discussed and controversial consequences of corporate personhood in the United States is the extension of a limited subset of the same constitutional rights.
Corporations as legal entities have always been able to perform commercial activities, similar to a person acting as a sole proprietor, such as entering into a contract or owning property. Therefore corporations have always had a 'legal personality' for the purposes of conducting business while shielding individual shareholders from personal liability (i.e., protecting personal assets which were not invested in the corporation).
Broadcaster Thom Hartmann that the Santa Clara County case was not intended to extend equal protection to corporations. Chief Justice Waite wrote in private correspondence; "we avoided meeting the [Constitutional] question." Hartmann that correspondence between Waite and Bancroft Davis ( in the Library of Congress) demonstrates Waite did not intend to create a legal precedent. The question of whether corporations were persons within the meaning of the Fourteenth Amendment in the lower courts and for the Supreme Court, but in this interpretation, the Waite Court did not explicitly decide upon this issue. Whatever the merits of Hartmann's theory about the Santa Clara County case, in numerous cases since the Court has reiterated that corporations are protected in many activities by the equal protection clause of the Fourteenth Amendment to the Constitution. The extent of the protection is what continues to be at issue. Generally speaking, corporations may invoke rights that groups of individuals may invoke, such as the right to petition, to speech, to enter into contracts and to hold property, to sue and to be sued, to hold religious beliefs. However, they may not exercise rights which are exclusive to individuals and cannot be exercised by other associations of individuals, including the right to vote and the right against self incrimination.
Ralph Nader, Phil Radford and others have argued that a strict originalist philosophy should reject the doctrine of corporate personhood under the Fourteenth Amendment. Indeed, Chief Justice William Rehnquist repeatedly criticized the Court's invention of corporate constitutional "rights," most famously in his dissenting opinion in the 1978 case "First National Bank of Boston v. Bellotti"; though, in "Bellotti", Justice Rhenquist's objections are based on his "views of the limited application of the First Amendment to the States" and not on whether corporations qualify as "persons" under the Fourteenth Amendment. Nonetheless, these justices' rulings have continued to affirm the assumption of corporate personhood, as the Waite court did, and Justice Rehnquist himself eventually endorsed the right of corporations to spend in elections (the majority view in Bellotti) in his dissenting opinion in "McConnell v. FEC".
Corporate political spending.
A central point of debate in recent years has been what role corporate money plays and should play in democratic politics. This is part of the larger debate on campaign finance reform and the role which money may play in politics.
In the United States, legal milestones in this debate include:
The corporate personhood aspect of the campaign finance debate turns on "Buckley v. Valeo" (1976) and "Citizens United v. Federal Election Commission" (2010): "Buckley" ruled that political spending is protected by the First Amendment right to free speech, while "Citizens United" ruled that corporate political spending is protected, holding that corporations have a First Amendment right to free speech. Opponents of these decisions have argued that if all corporate rights under the Constitution were abolished, it would clear the way for greater regulation of campaign spending and contributions. It should be noted, however, that neither decision relied on the concept of corporate personhood, and the Buckley decision in particular deals with the rights of individuals and political committees, not corporations.
In other countries.
Vatican City.
The Holy See, the corporate governing seat of the Catholic church, is technically regarded as a legal personality in international law.
United Kingdom.
City of London.
In the City of London (not to be confused with London), corporations operating within the city can appoint voters to represent the workforce that does not live there, but commutes there. This number of appointed voters depends on the number of employees that the corporation has. As of today, the majority of the City of London's voters are corporate appointees.
See also.
 is available from: 
 is available from: 
 is available from: 
 is available from: 
 is available from: 
 is available from: 
 is available from: 
 is available from: 
Further reading.
</dl>

</doc>
<doc id="49263" url="http://en.wikipedia.org/wiki?curid=49263" title="Lake Baringo">
Lake Baringo

Lake Baringo is, after Lake Turkana, the most northern of the Kenyan Rift Valley lakes, with a surface area of about 130 km2 and an elevation of about 970 m. The lake is fed by several rivers, Molo, Perkerra and Ol Arabel, and has no obvious outlet; the waters are assumed to seep through lake sediments into the faulted volcanic bedrock. It is one of the two freshwater lakes in the Rift Valley in Kenya, the other being Lake Naivasha.
It lies off the beaten track in a hot and dusty setting and over 470 species of birds have been recorded there, occasionally including migrating flamingos. A Goliath heronry is located on a rocky islet in the lake known as Gibraltar.
Description.
The lake is part of the East African Rift system. The Tugen Hills, an uplifted fault block of volcanic and metamorphic rocks, lies west of the lake. The Laikipia Escarpment lies to the east.
Water flows into the lake from the Mau Hills and Tugen Hills. It is a critical habitat and refuge for more than 500 species of birds and fauna, some of the migratory waterbird species being significant regionally and globally. The lake also provides an invaluable habitat for seven fresh water fish species. One, "Oreochromis niloticus baringoensis" (a Nile tilapia subspecies), is endemic to the lake. Lake fishing is important to local social and economic development. Additionally the area is a habitat for many species of animals including the hippopotamus ("Hippopotamus amphibius"), Nile crocodile ("Crocodylus niloticus") and many other mammals, amphibians, reptiles and the invertebrate communities.
While stocks of Nile tilapia in the lake are now low, the decline of this species has been mirrored by the success of another, the marbled lungfish ("Protopterus aethiopicus") which was introduced to the lake in 1974 and which now provides the majority of fish output from the lake. Water levels have been reduced by droughts and over-irrigation. The lake is commonly turbid with sediment, partly due to intense soil erosion in the catchment, especially on the Loboi Plain south of the lake.
The lake has several small islands, the largest being Ol Kokwe Island. Ol Kokwe, an extinct volcanic centre related to Korosi volcano north of the lake, has several hot springs and fumaroles, some of which have precipitated sulfur deposits. A group of hot springs discharge along the shoreline at Soro near the northeastern corner of the island.
Several important archaeological and palaeontological sites, some of which have yielded fossil hominoids and hominins, are present in the Miocene to Pleistocene sedimentary sequences of the Tugen Hills.
The main town near the lake is Marigat, while smaller settlements include Kampi ya Samaki and Loruk. The area is increasingly visited by tourists and is situated at the southern end of a region of Kenya inhabited largely by pastoralist ethnic groups including Il Chamus, Rendille, Turkana and Kalenjin. Accommodation (hotels, self-catering cottages and camping sites) as well as boating services are available at and near Kampi-Ya-Samaki on the western shore, as well as on several of the islands in the lake.

</doc>
<doc id="49266" url="http://en.wikipedia.org/wiki?curid=49266" title="Buchenwald concentration camp">
Buchenwald concentration camp

Buchenwald concentration camp (German: "Konzentrationslager (KZ) Buchenwald", ]; literally, in English: "beech forest") was a German Nazi concentration camp established on the "Ettersberg" (Etter Mountain) near Weimar, Germany, in July 1937, one of the first and the largest of the concentration camps on German soil, following Dachau's opening just over four years earlier.
Prisoners from all over Europe and the Soviet Union—Jews, Poles and other Slavs, the mentally ill and physically-disabled from birth defects, religious and political prisoners, Roma and Sinti, Freemasons, Jehovah's Witnesses (then called Bible Students), criminals, homosexuals, and prisoners of war—worked primarily as forced labor in local armaments factories. From 1945 to 1950, the camp was used by the Soviet occupation authorities as an internment camp, known as NKVD special camp number 2.
Today the remains of Buchenwald serve as a memorial and permanent exhibition and museum.
History.
In 1937, the Nazis constructed Buchenwald concentration camp, near Weimar, Germany. Embedded in the camp's main entrance gate is the slogan Jedem das Seine (literally "to each his own", but figuratively "everyone gets what he deserves”). The camp was operational until its liberation in 1945. Between 1945 and 1950, it was used by the Soviet Union as an NKVD special camp for Germans. On January 6, 1950, the Soviets handed over Buchenwald to the East German Ministry of Internal Affairs.
The camp was to be named "K. L. Ettersberg", but this was changed to "Buchenwald" ("beech forest"), since "Ettersberg" carried too many associations with Goethe, who strolled through the woods (his lover Charlotte von Stein lived there) and supposedly wrote his "Wanderer's Nightsong", or, alternately, the Walpurgisnacht passages of his "" under the oak tree which remained in the center of the camp after the forest was cleared for its construction: this tree is the famous Goethe Oak. Quickly the fate of the oak became associated with the fate of Germany: if the one was to fall, so was the other.
Between April 1938 and April 1945, some 238,380 people of various nationalities including 350 Western Allied prisoners of war (POW)s were incarcerated in Buchenwald. One estimate places the number of deaths at 56,000.
During an American bombing raid on August 24, 1944 that was directed at a nearby armaments factory, several bombs, including incendiaries, also fell on the camp, resulting in heavy casualties amongst the prisoners (2,000 prisoners wounded & 388 killed by the raid).
Today the remains of the camp serve as a memorial and permanent exhibition and museum administered by the Buchenwald and Mittelbau-Dora Memorials Foundation, which also oversees the camp's memorial at Mittelbau-Dora.
People.
Camp commandants.
Buchenwald’s first commandant was Karl-Otto Koch, who ran the camp from 1937 to July 1941. His second wife, Ilse Koch, became notorious as "Die Hexe von Buchenwald" ("the witch of Buchenwald") for her cruelty and brutality. Koch had a zoo built by the prisoners in the camp, with a bear pit ("Bärenzwinger") facing the "Appellplatz", the assembly square where prisoner "roll-calls" were conducted.
Koch himself was eventually imprisoned at Buchenwald by the Nazi authorities for incitement to murder. The charges were lodged by Prince Waldeck and Dr. Morgen, to which were later added charges of corruption, embezzlement, black market dealings, and exploitation of the camp workers for personal gain. Other camp officials were charged, including Ilse Koch. The trial resulted in Karl Koch being sentenced to death for disgracing both himself and the SS; he was executed by firing squad on April 5, 1945, one week before American troops arrived. Ilse Koch was sentenced to a term of four years' imprisonment after the war. Her sentence was reduced to two years and she was set free. She was subsequently arrested again and sentenced to life imprisonment by the post-war German authorities; she committed suicide in a Bavarian prison cell in September 1967.
The second commandant of the camp was Hermann Pister (1942–1945). He was tried in 1947 (Dachau Trials) and sentenced to death, but died in September 1948 of a heart condition before the sentence could be carried out.
Female prisoners and overseers.
The number of women held in Buchenwald was somewhere between 500 and 1,000. The first female inmates were twenty political prisoners who were accompanied by a female SS guard ("Aufseherin"); these women were brought to Buchenwald from Ravensbrück in 1941 and forced into sexual slavery at the camp's brothel. The SS later fired the SS woman on duty in the brothel for corruption, her position was taken over by “brothel mothers” as ordered by SS chief Heinrich Himmler.
The majority of women prisoners, however, arrived in 1944 and 1945 from other camps, mainly Auschwitz, Ravensbrück, and Bergen Belsen. Only one barrack was set aside for them; this was overseen by the female block leader ("Blockführerin") Franziska Hoengesberg, who came from Essen when it was evacuated. All the women prisoners were later shipped out to one of Buchenwald's many female satellite camps in Sömmerda, Buttelstedt, Mühlhausen, Gotha, Gelsenkirchen, Essen, Lippstadt, Weimar, Magdeburg, and Penig, to name a few. No female guards were permanently stationed at Buchenwald.
When the Buchenwald camp was evacuated, the SS sent the male prisoners to other camps, and the five-hundred remaining women (including one of the secret annexe members who lived with Anne Frank, "Mrs. van Daan", real name Auguste van Pels), were taken by train and on foot to the Theresienstadt concentration camp and ghetto in the protectorate of Bohemia and Moravia. Many, including van Pels, died sometime between April and May 1945. Because the female prisoner population at Buchenwald was comparatively small, the SS only trained female overseers at the camp and "assigned" them to one of the female subcamps. Twenty-two known female guards had personnel files at the camp, but it is unlikely that any of them stayed at Buchenwald for longer than a few days.
Ilse Koch served as head supervisor ("Oberaufseherin") of 22 other female guards and hundreds of women prisoners in the main camp. More than 530 women served as guards in the vast Buchenwald system of subcamps and external commands across Germany. Only 22 women served/trained in Buchenwald, compared to over 15,500 men.
Anna Fest was a guard at Ravensbrueck, who was later tried and acquitted.
Ulla Erna Frieda Jürß was a guard at Ravensbrück, who was convicted of her crimes.
Allied airmen.
Although it was highly unusual for German authorities to send Western Allied POWs to concentration camps, Buchenwald held a group of 168 aviators for two months. These men were from the United States, United Kingdom, Canada, Australia, New Zealand and Jamaica. They all arrived at Buchenwald on August 20, 1944.
All these airmen were in aircraft that had crashed in occupied France. Two explanations are given for them being sent to a concentration camp: first, that they had managed to make contact with the French Resistance, some were disguised as civilians, and they were carrying false papers when caught; they were therefore categorized by the Germans as spies, which meant their rights under the Geneva Convention were not respected. The second explanation is that they had been categorised as "Terrorflieger" ("terror aviators"). The aviators were initially held in Gestapo prisons and headquarters in France. In April or August 1944, they and other Gestapo prisoners were packed into covered goods wagons (US: boxcars) and sent to Buchenwald. The journey took five days, during which they received very little food or water. One aviator recalled their arrival at Buchenwald:
They were subjected to the same treatment and abuse as other Buchenwald prisoners until October 1944, when a change in policy saw the aviators dispatched to Stalag Luft III, a regular POW camp; nevertheless, two airmen died at Buchenwald. Those classed as "Terrorflieger" had been scheduled for execution after October 24; their rescue was effected by Luftwaffe officers who visited Buchenwald and, on their return to Berlin, demanded the airmen's release.
Buchenwald was also the main imprisonment for a number of Norwegian university students from 1943 until the end of the war. The students, being Norwegian, got better treatment than most, but had to resist Nazi schooling for months. They became remembered for resisting forced labor in a minefield, as the Nazis wished to use them as cannon fodder. An incident connected to this is remembered as the 'Strike at Burkheim'. The Norwegian students in Buchenwald lived in a warmer, stone-construction house and had their own clothes.
Death toll at Buchenwald.
Causes of death.
A primary cause of death was illness due to harsh camp conditions, with starvation—and its consequent illnesses—prevalent. Malnourished and suffering from disease, many were literally "worked to death" under the "Vernichtung durch Arbeit" policy (extermination through labor), as inmates only had the choice between slave labor or inevitable execution. Many inmates died as a result of human experimentation or fell victim to arbitrary acts perpetrated by the SS guards. Other prisoners were simply murdered, primarily by shooting and hanging.
Walter Gerhard Martin Sommer was an SS "Hauptscharführer" who served as a guard at the concentration camps of Dachau and Buchenwald. Known as the "Hangman of Buchenwald", he was considered a depraved sadist who reportedly ordered Otto Neururer and Mathias Spannlang, two Austrian priests, to be crucified upside-down. Sommer was especially infamous for hanging prisoners from trees with their wrists behind their backs in the "singing forest", so named because of the screams which emanated from this wooded area.
Summary executions of Soviet POWs were also carried out at Buchenwald. At least 1,000 men were selected in 1941–2 by a task force of three Dresden Gestapo officers and sent to the camp for immediate liquidation by a gunshot to the back of the neck, the infamous "Genickschuss".
The camp was also a site of large-scale trials for vaccines against epidemic typhus in 1942 and 1943. In all 729 inmates were used as test subjects, of whom 154 died. Other "experimentation" occurred at Buchenwald on a smaller scale. One such experiment aimed at determining the precise fatal dose of a poison of the alkaloid group; according to the testimony of one doctor, four Russian POWs were administered the poison, and when it proved not to be fatal they were "strangled in the crematorium" and subsequently "dissected". Among various other experiments was one which, in order to test the effectiveness of a balm for wounds from incendiary bombs, involved inflicting "very severe" white phosphorus burns on inmates. When challenged at trial over the nature of this testing, and particularly over the fact that the testing was designed in some cases to cause death and only to measure the time which elapsed until death was caused, one Nazi doctor's defence was that, although a doctor, he was a "legally appointed executioner".
Number of deaths.
The SS left behind accounts of the number of prisoners and people coming to and leaving the camp, categorizing those leaving them by release, transfer, or death. These accounts are one of the sources of estimates for the number of deaths in Buchenwald. According to SS documents, 33,462 died. These documents were not, however, necessarily accurate: Among those executed before 1944, many were listed as "transferred to the Gestapo". Furthermore, from 1941, Soviet POWs were executed in mass killings. Arriving prisoners selected for execution were not entered into the camp register and therefore were not among the 33,462 dead listed.
One former Buchenwald prisoner, Armin Walter, calculated the number of executions by the number of shootings in the back of the head. His job at Buchenwald was to set up and care for a radio installation at the facility where people were executed; he counted the numbers, which arrived by telex, and hid the information. He says that 8,483 Soviet prisoners of war were shot in this manner.
According to the same source, the total number of deaths at Buchenwald is estimated at 56,545. This number is the sum of:
This total (56,545) corresponds to a death rate of 24 percent, assuming that the number of persons passing through the camp according to documents left by the SS, 240,000 prisoners, is accurate.
Liberation from Nazi Germany.
On April 4, 1945, the US 89th Infantry Division overran Ohrdruf, a subcamp of Buchenwald. It was the first Nazi camp liberated by US troops.
Buchenwald was partially evacuated by the Germans from April 6, 1945. In the days before the arrival of the American army, thousands of the prisoners were forced to join the evacuation marches.
Thanks in large part to the efforts of Polish engineer Gwidon Damazyn, an inmate since March 1941, a secret short-wave transmitter and small generator were built and hidden in the prisoners' movie room. On April 8 at noon, Damazyn and Russian prisoner Konstantin Ivanovich Leonov sent the Morse code message prepared by leaders of the prisoners' underground resistance (supposedly Walter Bartel and Harry Kuhn):
The text was repeated several times in English, German, and Russian. Damazyn sent the English and German transmissions, while Leonov sent the Russian version. Three minutes after the last transmission sent by Damazyn, the headquarters of the US Third Army responded:
According to Teofil Witek, a fellow Polish prisoner who witnessed the transmissions, Damazyn fainted after receiving the message.
After this news had been received, Communist inmates stormed the watchtowers and killed the remaining guards, using arms they had been collecting since 1942 (one machine gun and 91 rifles). (See Buchenwald Resistance)
A detachment of troops of the US 9th Armored Infantry Battalion, from the 6th Armored Division, part of the US Third Army, and under the command of Captain Frederic Keffer, arrived at Buchenwald on April 11, 1945 at 3:15 P.M., (now the permanent time of the clock at the entrance gate). The soldiers were given a hero's welcome, with the emaciated survivors finding the strength to toss some liberators into the air in celebration.
Later in the day, elements of the US 83rd Infantry Division overran Langenstein, one of a number of smaller camps comprising the Buchenwald complex. There, the division liberated over 21,000 prisoners, ordered the mayor of Langenstein to send food and water to the camp, and hurried medical supplies forward from the 20th Field Hospital.
Third Army Headquarters sent elements of the 80th Infantry Division to take control of the camp on the morning of Thursday, April 12, 1945. Several journalists arrived on the same day, perhaps with the 80th, including Edward R. Murrow, whose was broadcast on CBS and became one of his most famous:
Soviet Special Camp 2.
After liberation, between 1945 and February 10, 1950, the camp was administered by the Soviet Union and served as Special Camp No. 2 of the NKVD. It was part of a "special camps" network operating since 1945, formally integrated into the Gulag in 1948. Another infamous "special camp" in Soviet occupied Germany was the former Nazi concentration camp Sachsenhausen (special camp No. 7).
Between August 1945 and the dissolution on March 1, 1950, 28,455 prisoners, including 1,000 women, were held by the Soviet Union at Buchenwald. A total of 7,113 people died in Special Camp Number 2, according to the Soviet records. They were buried in mass graves in the woods surrounding the camp. Their relatives did not receive any notification of their deaths. Prisoners comprised alleged opponents of Stalinism, and alleged members of the Nazi party or Nazi organization, others were imprisoned due to identity confusion and arbitrary arrests. The NKVD would not allow any contact of prisoners with the outside world and did not attempt to determine the guilt of any individual prisoner.
On January 6, 1950, Soviet Minister of Internal Affairs Kruglov ordered all special camps, including Buchenwald, to be handed over to the East German Ministry of Internal Affairs.
Demolition of the camp.
In October 1950, it was decreed that the camp would be demolished. The main gate, the crematorium, the hospital block, and two guard towers were spared. All prisoner barracks and other buildings were razed. Foundations of some still exist and many others have been rebuilt. According to the Buchenwald Memorial website, "the combination of obliteration and preservation was dictated by a specific concept for interpreting the history of Buchenwald Concentration Camp."
The first monument to victims was erected days after the initial liberation. Intended to be completely temporary, it was built by prisoners and made of wood. A second monument to commemorate the dead was erected in 1958 by the GDR near the mass graves. Inside the camp, there is a living monument in the place of the first monument that is kept at skin temperature all year round.
Notable inmates.
Camp literature.
Survivors who have written about their camp experiences include Jorge Semprún, who in "Quel beau dimanche!" describes conversations involving Goethe and Léon Blum, and Ernst Wiechert, whose "Der Totenwald" was written in 1939 but not published until 1945, and which likewise involved Goethe. Scholars have investigated how camp inmates used art to help deal with their circumstances, and according to Theodor Ziolkowski writers often did so by turning to Goethe. Artist Léon Delarbre's sketched, besides other scenes of camp life, the Goethe Oak, under which he used to sit and write. Finally, in his work Night (book), Elie Wiesel talks about his stay in Buchenwald, including his father's death.
Modern times.
Today the remains of Buchenwald serves as a memorial and permanent exhibition and museum administrated by Buchenwald and Mittelbau-Dora Memorials Foundation, which also administrates the camp memorial at Mittelbau-Dora.
Visit from President Obama and Chancellor Merkel.
On June 5, 2009, U.S. President Barack Obama and German Chancellor Angela Merkel visited Buchenwald after a tour of Dresden Castle and Church of Our Lady. During the visit they were accompanied by Elie Wiesel and , both survivors of the camp. , the director of the Buchenwald and Mittelbau-Dora Memorials Foundation and honorary professor of University of Jena, guided the four guests through the remainder of the site of the camp. During the visit Elie Wiesel, who together with were sent to the Little camp as 16-year old boys, said, "if these trees could talk." His statement marked the irony about the beauty of the landscape and the horrors that took place within the camp. President Obama mentioned during his visit that he had heard stories as a child from his great uncle, who was part of the 89th Infantry Division, the first Americans to reach the camp at Ohrdruf, one of Buchenwald's satellites.

</doc>
<doc id="49274" url="http://en.wikipedia.org/wiki?curid=49274" title="Tove Jansson">
Tove Jansson

Tove Marika Jansson (]; 9 August 1914 – 27 June 2001) was a Swedish-speaking Finnish novelist, painter, illustrator and comic strip author. For her contribution as a children's writer she received the Hans Christian Andersen Medal in 1966.
Brought up by artistic parents, Jansson studied art from 1930 to 1938 in Stockholm, Helsinki and then Paris. Her first solo art exhibition was in 1943. At the same time, she was writing short stories and articles for publication, as well as creating the graphics for book covers and other purposes. She continued to work as an artist for the rest of her life, alongside her writing.
Jansson is best known as the author of the "Moomin" books for children. The first such book, "The Moomins and the Great Flood", appeared in 1945, though it was the next two books, "Comet in Moominland" and "Finn Family Moomintroll", published in 1946 and 1948 respectively, that brought her fame.
Starting with the semi-autobiographical "Bildhuggarens dotter" ("Sculptor's Daughter") in 1968, she wrote six novels and five books of short stories for adults.
Biography.
Tove Jansson was born in Helsinki, Grand Duchy of Finland. Her family, part of the Swedish-speaking (Swedish: "finlandssvensk") minority of Finland, was an artistic one: her father Viktor Jansson was a sculptor and her mother Signe Hammarsten-Jansson was a graphic designer and illustrator. Tove's siblings also became artists: Per Olov Jansson became a photographer and Lars Jansson an author and cartoonist. Whilst their home was in Helsinki, the family spent many of their summers in a rented cottage on an island near Porvoo, 50 km east of Helsinki.
Jansson studied at University College of Arts, Crafts and Design in Stockholm in 1930–33, the Graphic School of the Finnish Academy of Fine Arts in 1933–1937 and finally at L'École d'Adrien Holy and L'École des Beaux-Arts in Paris in 1938. She displayed a number of artworks in exhibitions during the 30s and early 40s, and her first solo exhibition was held in 1943.
Aged 14, she wrote and illustrated her first picture book "Sara och Pelle och näckens bläckfiskar" ("Sara and Pelle and the Water Sprite's Octopuses") although it was not published until 1933, and had drawings published in magazines in the 1920s. During the 1930s she made several trips to other European countries, and wrote and illustrated short stories and articles which were also published in magazines, periodicals and daily papers.
During this period, Jansson designed many book covers, adverts and postcards, and, following her mother, she drew illustrations for "Garm", an anti-fascist Finnish-Swedish satirical magazine.
Briefly engaged in the 1940s to Atos Wirtanen, she later during her studies met her future partner Tuulikki Pietilä. The two women collaborated on many works and projects, including a model of the Moominhouse, in collaboration with Pentti Eistola. This is now exhibited at the Moomin museum in Tampere.
Jansson wrote and illustrated her first Moomin book, "The Moomins and the Great Flood", in 1945, during World War II. She said later that the war had depressed her and she had wanted to write something naïve and innocent. This first book was hardly noticed, but the next Moomin books, "Comet in Moominland" (1946) and "Finn Family Moomintroll" (1948), made her famous. She went on to write six more Moomin books, a number of picture books and comic strips. Her fame spread quickly and she became Finland's most widely read author abroad. For her "lasting contribution to children's literature" she received the biennial, international Hans Christian Andersen Award for Writing in 1966.
Jansson continued painting and writing for the rest of her life, although her contributions to the Moomin series became rare after 1970. Her first foray outside children's literature was "Bildhuggarens dotter" ("Sculptor's Daughter"), a semi-autobiographical book written in 1968. After that, she authored five more novels, including "Sommarboken" ("The Summer Book") and five collections of short stories. Although she had a studio in Helsinki, she lived many summers on a small island called Klovharu, one of the Pellinki Islands near the town of Porvoo. Jansson's and Pietilä's travels and summers spent together on the Klovharu island in Pellinki have been captured on several hours of film, shot by Pietilä. Several documentaries have been made of this footage, the latest being "Haru, yksinäinen saari" ("Haru, the lonely island") (1998) and "Tove ja Tooti Euroopassa" ("Tove and Tooti in Europe") (2004).
A lifelong smoker, Jansson developed lung cancer in early 2000, and died on 27 June 2001. She was 86 years old.
Career.
Comic strip artist.
Tove Jansson worked as illustrator and cartoonist for the Swedish-language satirical magazine "Garm" from the 1930s to 1953. One of her political cartoons achieved a brief international fame: she drew Adolf Hitler as a crying baby in diapers, surrounded by Neville Chamberlain and other great European leaders, who tried to calm the baby down by giving it slices of cake – Austria, Poland, Czechoslovakia, etc. Jansson also produced illustrations during this period for the Christmas magazines "Julen" and "Lucifer" (just as her mother had earlier) as well as several smaller productions. Her earliest comic strips were produced for productions including "Lunkentus" ("Prickinas och Fabians äventyr", 1929), "Vårbrodd" ("Fotbollen som Flög till Himlen", 1930), and "Allas Krönika" ("Palle och Göran gå till sjöss", 1933).
The figure of the Moomintroll appeared first in Jansson's political cartoons, where it was used as a signature character near the artist's name. This "Proto-Moomin," then called Snork or Niisku, was thin and ugly, with a long, narrow nose and devilish tail. Jansson said that she had designed the Moomins in her youth: after she lost a philosophical quarrel about Immanuel Kant with one of her brothers, she drew "the ugliest creature imaginable" on the wall of their outhouse and wrote under it "Kant". This Moomin later gained weight and a more pleasant appearance, but in the first Moomin book "The Moomins and the Great Flood" (originally "Småtrollen och den stora översvämningen"), the Immanuel-Kant-Moomin is still perceptible. The name "Moomin" comes from Tove Jansson's uncle, Einar Hammarsten: when she was studying in Stockholm and living with her Swedish relatives, her uncle tried to stop her pilfering food by telling her that a "Moomintroll" lived in the kitchen closet and breathed cold air down people's necks.
In 1952, after "Comet in Moominland" and "Finn Family Moomintroll" had been translated into English, a British publisher asked if Tove Jansson would be interested in drawing comic strips about the Moomins. Jansson had already drawn a long Moomin comic adventure, "Mumintrollet och jordens undergång" ("Moomintrolls and the End of the World"), based loosely on "Comet in Moominland", for the Swedish-language newspaper "Ny Tid", and she accepted the offer. The comic strip "Moomintroll", started in 1954 in the "Evening News", a newspaper for the London area and London commuters (no longer in business). Tove Jansson drew 21 long Moomin stories from 1954 to 1959, writing them at first by herself and then with her brother Lars Jansson. She eventually gave the strip up because the daily work of a comic artist did not leave her time to write books and paint, but Lars took over the strip and continued it until 1975.
The series was published in book form in Swedish, and books 1 to 6 have been published in English, "Moomin: The Complete Tove Jansson Comic Strip".
Inspiration for Moomins.
Critics have interpreted various Moomin characters as being inspired by real people, especially members of the author's family, and Jansson spoke in interviews about the backgrounds of, and possible models for, her characters.
Pietilä's personality inspired the character Too-Ticky in "Moominland Midwinter". and Moomintroll and Little My have been seen as psychological self-portraits of the artist.
The Moomins, generally speaking, relate strongly to Jansson's own family – they were bohemian, lived close to nature and were very tolerant towards diversity. Moominpappa and Moominmamma are often seen as portraits of Jansson's parents. Jansson remained close to her mother until her mother's death in 1970; even after Tove had become an adult, the two often travelled together, and during her final years Signe also lived with Tove part-time.
Author.
Jansson is principally known as the author of the Moomin books. Jansson created the Moomins, a family of trolls who are white, round and smooth in appearance, with large snouts that make them vaguely resemble hippopotamuses.
The first Moomin book, "The Moomins and the Great Flood", was written in 1945. Although the primary characters are Moominmamma and Moomintroll, most of the principal characters of later stories were only introduced in the next book, so "The Moomins and the Great Flood" is frequently considered a forerunner to the main series. The book was not a success (and was the last Moomin book to be translated into English), but the next two installments in the Moomin series, "Comet in Moominland" (1946) and "Finn Family Moomintroll" (1948), brought Jansson some fame. The original title of "Finn Family Moomintroll", "Trollkarlens Hatt", translates as "The Magician's Hat".
The style of the Moomin books changed as time went by. The first books, up to "Moominland Midwinter" (1957), are adventure stories that include floods, comets and supernatural events. "The Moomins and the Great Flood" deals with Moominmamma and Moomintroll's flight through a dark and scary forest, where they encounter various dangers. In "Comet in Moominland", a comet nearly destroys the Moominvalley (some critics have considered this an allegory of nuclear weapons). "Finn Family Moomintroll" deals with adventures brought on by the discovery of a magician's hat. "The Exploits of Moominpappa" (1950) tells the story of Moominpappa's adventurous youth and cheerfully parodies the genre of memoirs. Finally, "Moominsummer Madness" (1955) pokes fun at the world of the theatre: the Moomins explore an empty theatre and perform Moominpappa's pompous hexametric melodrama.
"Moominland Midwinter" marks a turning point in the series. The books take on more realistic settings ("realistic" in the context of the Moomin universe) and the characters start to acquire some psychological depth. "Moominland Midwinter" focuses on Moomintroll, who wakes up in the middle of the winter (Moomins hibernate from November to April, as mentioned on the back of the book), and has to cope with the strange and unfriendly world he finds. The short story collection "Tales from Moominvalley" (1962) and the novels "Moominpappa at Sea" (1965) and "Moominvalley in November" (1970) are serious and psychologically searching books, far removed from the light-heartedness and cheerful humor of "Finn Family Moomintroll".
After "Moominvalley in November" Tove Jansson stopped writing about Moomins and started writing for adults. The Summer Book is the best known of her adult fiction translated into English. It is a work of charm, subtlety and simplicity, describing the summer stay on an island of a young girl and her grandmother.
In addition to the Moomin novels and short stories, Tove Jansson also wrote and illustrated four original and highly popular picture books: "The Book about Moomin, Mymble and Little My" (1952), "Who will Comfort Toffle?" (1960), "The Dangerous Journey" (1977) and "An Unwanted Guest" (1980). As the Moomins' fame grew, two of the original novels, "Comet in Moominland" and "The Exploits of Moominpappa", were revised by Jansson and republished. The revised versions were, however, never translated into English.
Painter and illustrator.
Although she became known first and foremost as an author, Tove Jansson considered her careers as author and painter to be of equal importance. She painted her whole life, changing style from the classical impressionism of her youth to the highly abstract modernist style of her later years. Jansson displayed a number of artworks in exhibitions during the 1930s and early 1940s, and her first solo exhibition was held in 1943. Despite generally positive reviews, criticism induced Jansson to refine her style such that in her 1955 solo exhibition her style had become less overloaded in terms of detail and content. Between 1960 and 1970 Jansson held five more solo exhibitions.
Jansson also created a series of commissioned murals and public works throughout her career, which may still be viewed in their original locations. These works of Jansson's included:
In addition to providing the illustrations for her own Moomin books, Jansson also illustrated Swedish translations of classics such as J. R. R. Tolkien's "The Hobbit" and Lewis Carroll's "The Hunting of the Snark" and "Alice's Adventures in Wonderland" (some used later in Finnish translations as well). She also illustrated her late work, "The Summer Book" (1972).
Theatre.
Several stage productions have been made from Jansson's Moomin series, including a number that Jansson herself was involved in.
The earliest production was a 1949 theatrical version of "Comet in Moominland" performed at Åbo Svenska Teater.
In the early 1950s, Jansson collaborated on Moomin-themed children's plays with Vivica Bandler. In 1952, Jansson designed stage settings and dresses for "Pessi and Illusia", a ballet by Ahti Sonninen ("Radio tekee murron") which was performed at the Finnish National Opera. By 1958, Jansson began to become directly involved in theater as Lilla Teater produced "Troll i kulisserna" ("Troll in the wings"), a play with lyrics by Jansson and music composed by Erna Tauro. The production was a success, and later performances were held in Sweden and Norway.
In 1974 the first Moomin opera was produced, with music composed by Ilkka Kuusisto.
Jansson's cultural legacy.
The biennial Hans Christian Andersen Award conferred by the International Board on Books for Young People is the highest recognition available to a writer or illustrator of children's books. Jansson received the writing award in 1966.
Jansson's books, originally written in Swedish, have been translated into 45 languages. After the "Kalevala" and books by Mika Waltari, they are the most widely translated works of Finnish literature.
The Moomin Museum in Tampere displays much of Jansson's work on the Moomins. There is also a Moomin theme park named Moomin World in Naantali.
Tove Jansson was selected as the main motif in the 2004 minting of a Finnish commemorative coin, the €10 Tove Jansson and Finnish Children's Culture commemorative coin. The obverse depicts a combination of Tove Jansson portrait with several objects: the skyline, an artist's palette, a crescent and a sailing boat. The reverse design features three Moomin characters. In 2014 she was again featured on a commemorative coin, minted at €10 and €20 values, being the only person other than the former Finnish president Urho Kekkonen to be granted two such coins. She was also featured on a €2 commemorative coin that entered general circulation in June 2014.
Since 1988, Finland's Post has released several postage stamp sets and one postal card with Moomin motifs. In 2014, Jansson herself was featured on a Finnish stamp set.
In 2014 the City of Helsinki honored Jansson by renaming a park in Katajanokka as Tove Jansson's Park (Finnish: "Tove Janssonin puisto", Swedish: "Tove Janssons park"). The park is located near Jansson's childhood home.
In March 2014 the Ateneum Art Museum opened a major centenary exhibition showcasing Jansson's works as an artist, an illustrator, a political caricaturist and the creator of the Moomins. The exhibition drew nearly 300,000 visitors in six months. After Helsinki the exhibition embarked on a tour in Japan to visit five Japanese museums.

</doc>
<doc id="49281" url="http://en.wikipedia.org/wiki?curid=49281" title="Hydronium">
Hydronium

In chemistry, hydronium is the common name for the aqueous cation H3O+, the type of oxonium ion produced by protonation of water. It is the positive ion present when an Arrhenius acid is dissolved in water, as Arrhenius acid molecules in solution give up a proton (a positive hydrogen ion, H+) to the surrounding water molecules (H2O).
Determination of pH.
It is the presence of hydronium ion relative to hydroxide that determines a solution's pH. The molecules in pure water auto-dissociate into hydronium and hydroxide ions in the following equilibrium:
In pure water, there is an equal number of hydroxide and hydronium ions, so it has a neutral pH of 7. A pH value less than 7 indicates an acidic solution, and a pH value more than 7 indicates a basic solution.
Nomenclature.
According to IUPAC nomenclature of organic chemistry, the hydronium ion should be referred to as oxonium. Hydroxonium may also be used unambiguously to identify it. A draft IUPAC proposal also recommends the use of oxonium and oxidanium in organic and inorganic chemistry contexts, respectively.
An oxonium ion is any ion with a trivalent oxygen cation. For example, a protonated hydroxyl group is an oxonium ion, but not a hydronium.
Structure.
Since O+ and N have the same number of electrons, H3O+ is isoelectronic with ammonia. As shown in the images above, H3O+ has a trigonal pyramid geometry with the oxygen atom at its apex. The H-O-H bond angle is approximately 113°, and the center of mass is very close to the oxygen atom. Because the base of the pyramid is made up of three identical hydrogen atoms, the H3O+ molecule's symmetric top configuration is such that it belongs to the C3v point group. Because of this symmetry and the fact that it has a dipole moment, the rotational selection rules are ΔJ = ±1 and ΔK = 0. The transition dipole lies along the c axis and, because the negative charge is localized near the oxygen atom, the dipole moment points to the apex, perpendicular to the base plane.
Acids and acidity.
Hydronium is the cation that forms from water in the presence of hydrogen ions. These hydrons do not exist in a free state: they are extremely reactive and are solvated by water. An acidic solute is generally the source of these hydrons; however, hydroniums exist even in pure water. This special case of water reacting with water to produce hydronium (and hydroxide) ions is commonly known as the self-ionization of water. The resulting hydronium ions are few and short-lived. pH is a measure of the relative activity of hydronium and hydroxide ions in aqueous solutions. In acidic solutions, hydronium is the more active, its excess proton being readily available for reaction with basic species.
Hydronium is very acidic: at 25 °C, its pKa is -1.7. It is also the most acidic species that can exist in water (assuming sufficient water for dissolution): any stronger acid will ionize and protonate a water molecule to form hydronium. The acidity of hydronium is the implicit standard used to judge the strength of an acid in water: strong acids must be better proton donors than hydronium, otherwise a significant portion of acid will exist in a non-ionized state. Unlike hydronium in neutral solutions that result from water's autodissociation, hydronium ions in acidic solutions are long-lasting and concentrated, in proportion to the strength of the dissolved acid.
pH was originally conceived to be a measure of the hydrogen ion concentration of aqueous solution. We now know that virtually all such free protons quickly react with water to form hydronium; acidity of an aqueous solution is therefore more accurately characterized by its hydronium concentration. In organic syntheses, such as acid catalyzed reactions, the hydronium ion (H3O+) can be used interchangeably with the H+ ion; choosing one over the other has no significant effect on the mechanism of reaction.
Solvation.
Researchers have yet to fully characterize the solvation of hydronium ion in water, in part because many different meanings of solvation exist. A freezing-point depression study determined that the mean hydration ion in cold water is approximately H3O+(H2O)6: on average, each hydronium ion is solvated by 6 water molecules which are unable to solvate other solute molecules.
Some hydration structures are quite large: the H3O+(H2O)20 magic ion number structure (called "magic" because of its increased stability with respect to hydration structures involving a comparable number of water molecules – this is a similar usage of the word "magic" as in nuclear physics) might place the hydronium inside a dodecahedral cage. However, more recent ab initio method molecular dynamics simulations have shown that, on average, the hydrated proton resides on the surface of the H3O+(H2O)20 cluster. Further, several disparate features of these simulations agree with their experimental counterparts suggesting an alternative interpretation of the experimental results.
Two other well-known structures are the Zundel cations and Eigen cations. The Eigen solvation structure has the hydronium ion at the center of an H9O4+ complex in which the hydronium is strongly hydrogen-bonded to three neighbouring water molecules. In the Zundel H5O2+ complex the proton is shared equally by two water molecules in a symmetric hydrogen bond. Recent work indicates that both of these complexes represent ideal structures in a more general hydrogen bond network defect.
Isolation of the hydronium ion monomer in liquid phase was achieved in a nonaqueous, low nucleophilicity superacid solution (HF-SbF5SO2). The ion was characterized by high resolution O-17 nuclear magnetic resonance.
A 2007 calculation of the enthalpies and free energies of the various hydrogen bonds around the hydronium cation in liquid protonated water at room temperature and a study of the proton hopping mechanism using molecular dynamics showed that the hydrogen-bonds around the hydronium ion (formed with the three water ligands in the first solvation shell of the hydronium) are quite strong compared to those of bulk water.
A new model was proposed by Stoyanov based on infrared spectroscopy in which the proton exists as an H13O6+ ion. The positive charge is thus delocalized over 6 water molecules.
Solid hydronium salts.
For many strong acids, it is possible to form crystals of their hydronium salt that are relatively stable. Sometimes these salts are called acid monohydrates. As a rule, any acid with an ionization constant of 109 or higher may do this. Acids whose ionization constant is below 109 generally cannot form stable H3O+ salts. For example, hydrochloric acid has an ionization constant of 107, and mixtures with water at all proportions are liquid at room temperature. However, perchloric acid has an ionization constant of 1010, and if liquid anhydrous perchloric acid and water are combined in a 1:1 molar ratio, solid hydronium perchlorate forms.
The hydronium ion also forms stable compounds with the carborane superacid H(CB11H(CH3)5Br6). X-ray crystallography shows a C3v symmetry for the hydronium ion with each proton interacting with a bromine atom each from three carborane anions 320 pm apart on average. The [H3O][H(CB11HCl11)] salt is also soluble in benzene. In crystals grown from a benzene solution the solvent co-crystallizes and a H3O·(benzene)3 cation is completely separated from the anion. In the cation three benzene molecules surround hydronium forming pi-cation interactions with the hydrogen atoms. The closest (non-bonding) approach of the anion at chlorine to the cation at oxygen is 348 pm.
There are also many examples of hydrated hydronium ions known, such as the H5O2+ ion in HCl·2H2O, the H7O3+ and H9O4+ ions both found in HBr·4H2O.
Interstellar H3O+.
Motivation for study.
Hydronium is an abundant molecular ion in the interstellar medium and is found in diffuse and dense molecular clouds as well as the plasma tails of comets. Interstellar sources of hydronium observations include the regions of Sagittarius B2, Orion OMC-1, Orion BN–IRc2, Orion KL, and the comet Hale-Bopp.
Interstellar hydronium is formed by a chain of reactions started by the ionization of H2 into H2+ by cosmic radiation. H3O+ can produce either OH- or H2O through dissociative recombination reactions, which occur very quickly even at the low (≥10 K) temperatures of dense clouds. This leads to hydronium playing a very important role in interstellar ion-neutral chemistry.
Astronomers are especially interested in determining the abundance of water in various interstellar climates due to its key role in the cooling of dense molecular gases through radiative processes. However, H2O does not have many favorable transitions for ground based observations. Although observations of HDO (the deuterated version of water) could potentially be used for estimating H2O abundances, the ratio of HDO to H2O is not known very accurately.
Hydronium, on the other hand, has several transitions that make it a superior candidate for detection and identification in a variety of situations. This information has been used in conjunction with laboratory measurements of the branching ratios of the various H3O+ dissociative recombination reactions to provide what are believed to be relatively accurate OH- and H2O abundances without requiring direct observation of these species.
Interstellar chemistry.
As mentioned previously, H3O+ is found in both diffuse and dense molecular clouds. By applying the reaction rate constants (α, β, and γ) from udfa.net corresponding to all of the currently available characterized reactions involving H3O+, it is possible to calculate k(T) for each of these reactions. By multiplying these k(T) by the relative abundances of the products (also from udfa.net), the relative rates (cm3·s−1) for each reaction at a given temperature can be determined. These relative rates can be made in absolute rates by multiplying them by the [H2]2. By assuming T = 10 K for a dense cloud and T = 50 K for a diffuse could, the results indicate that most dominant formation and destruction mechanisms were the same for both cases. It should be mentioned that the relative abundances used in these calculations correspond to TMC-1, a dense molecular cloud, and that the calculated relative rates are therefore expected to be more accurate at T = 10 K. The three fastest formation and destruction mechanisms are listed in the table below, along with their relative rates. Note that the rates of these six reactions are such that they make up approximately 99% of H3O+'s chemical interactions under these conditions. More about these reactions can be found in. Finally, it should also be noted that all three destruction mechanisms in the table below are classified as dissociative recombination reactions.
It is also worth noting that the relative rates for the formation reactions in the table above are the same for a given reaction at both temperatures. This is due to the reaction rate constants for these reactions having β and γ constants of 0, resulting in k=α\alpha$ which is independent of temperature.
Since all three of these reactions produce either H2O or OH, these results reinforce the strong connection between their relative abundances and that of H3O+. The rates of these six reactions are such that they make up approximately 99% of H3O+'s chemical interactions under these conditions.
Astronomical detections.
As early as 1973 and before the first interstellar detection, chemical models of the interstellar medium (the first corresponding to a dense cloud) predicted that hydronium was an abundant molecular ion and that it played an important role in ion-neutral chemistry. However, before an astronomical search could be underway there was still the matter of determining hydronium's spectroscopic features in the gas phase, which at this point were unknown. The first studies of these characteristics came in 1977, which was followed by other, higher resolution spectroscopy experiments. Once several lines had been identified in the laboratory, the first interstellar detection of H3O+ was made by two groups almost simultaneously in 1986. The first, published in June 1986, reported observation of the JKvt = 11− - 21+ transition at 307192.41 MHz in OMC-1 and Sgr B2. The second, published in August, reported observation of the same transition toward the Orion-KL nebula.
These first detections have been followed by observations of a number of additional H3O+ transitions. The first observations of each subsequent transition detection are given below in chronological order:
In 1991, the 32+ - 22− transition at 364797.427 MHz was observed in OMC-1 and Sgr B2. One year later, the 30+ - 20− transition at 396272.412 MHz was observed in several regions, the clearest of which was the W3 IRS 5 cloud.
The first far-IR 43− - 33+ transition at 69.524 µm (4.3121 THz) was made in 1996 near Orion BN-IRc2. In 2001, three additional transitions of H3O+ in were observed in the far infrared in Sgr B2; 21− - 11+ transition at 100.577 µm (2.98073 THz), 11− - 11+ at 181.054 µm (1.65582 THz) and 20− - 10+ at 100.869 µm (2.9721 THz).

</doc>
<doc id="49284" url="http://en.wikipedia.org/wiki?curid=49284" title="Methylchloroisothiazolinone">
Methylchloroisothiazolinone

Methylchloroisothiazolinone (5-chloro-2-methyl-4-isothiazolin-3-one), also referred to as MCI, is a preservative with antibacterial and antifungal effects within the group of isothiazolinones. It is effective against gram-positive and gram-negative bacteria, yeast, and fungi.
Methylchloroisothiazolinone is found in many water-based personal care products and cosmetics. Methylchloroisothiazolinone was first used in cosmetics in the 1970s. It is also used in glue production, detergents, paints, fuels, and other industrial processes. Methylchloroisothiazolinone is known by the registered tradename Kathon CG when used in combination with methylisothiazolinone.
Methylchloroisothiazolinone may be used in combination with other preservatives including ethylparaben, benzalkonium chloride, or 2-bromo-2-nitropropane-1,3-diol.
Safety.
In pure form or in high concentrations, methylchloroisothiazolinone is a skin and membrane irritant and causes chemical burns. In the United States, accepted concentrations are 15 ppm in rinse-offs and 8 ppm in other cosmetics. In Canada, the accepted concentrations are 15 ppm and 7.5 ppm respectively. The Canadian levels are also reflected in a paper published in the International Journal of Toxicology.
The International Agency for Research on Cancer (IARC), does not currently list methylchloroisothiazolinone as a known, probable, or possible human carcinogen, nor have "in vivo" tests found evidence of carcinogenic activity.
Allergic contact dermatitis.
Methylchloroisothiazolinone is an allergen and can cause severe skin reactions in some people. Since 2013 the use of the product particularly in cosmetics has received increased media coverage in the UK. In 2013 GP doctors asked cosmetics companies to remove it from products. Some cosmetics with the MI in have been investigated by the BBC Watchdog programme. The first publication of the preservative as a contact allergen was in 1988. A common indication of an allergic reaction is eczema-like symptoms including redness and itching, and upon longer exposure also burning sensations and blisters on the part of the skin that is exposed to the allergen. Continued exposure can lead to high sensitization which will be triggered each time the individual comes in contact with the allergen due to the memory T-cells that will remain in the local skin area.

</doc>
<doc id="49287" url="http://en.wikipedia.org/wiki?curid=49287" title="USS Winston S. Churchill">
USS Winston S. Churchill

USS "Winston S. Churchill" (DDG-81) is a $1 billion "Arleigh Burke"-class guided missile destroyer of the United States Navy. She is the 31st destroyer of an originally planned 62-ship class. The "Churchill" is named after British Prime Minister Sir Winston Churchill. Her home port is in Naval Station Norfolk, Virginia. She is a component of Carrier Strike Group Ten.
Naming.
On 29 November 1995, on a visit to the United Kingdom, President Bill Clinton announced to both Houses of Parliament that the new ship would be named after former British Prime Minister and Honorary Citizen of the United States, Sir Winston Churchill. It would make it technically the first warship of the United States Navy to be named after a non-American citizen since 1975, and the first destroyer and only the fourth American warship named after a British citizen.
Other American warships named after Britons were "Alfred", an armed merchantman named after King Alfred the Great; "Raleigh", a continental frigate, named after Sir Walter Raleigh (though three later USS "Raleigh"s—and two Confederate warships—would be named for the North Carolina city, which did not exist at the time) and "Effingham", named after The 3rd Earl of Effingham who resigned his commission rather than fight the Americans during the American Revolutionary War. The former frigate "Harold E. Holt" was also named after a person from a country in the Commonwealth of Nations, the ill-fated Australian Prime Minister Harold Holt, however, this is the first ship to be named for a modern British hero, and British Prime Minister.
History.
The contract to build "Churchill" was awarded to the Bath Iron Works Corporation on 6 January 1995, and the keel was laid down on 7 May 1998. "Churchill" was launched 17 April 1999, delivered 13 October 2000, and commissioned 10 March 2001. The launch and christening of the ship was co-sponsored by Lady Soames, the daughter of Winston Churchill, and Mrs. Janet Cohen, wife of the Secretary of Defense. Her first commanding officer was Commander (now Rear Admiral) Michael T. Franken.
"Churchill" is the only U.S. Navy vessel to have a Royal Navy Officer permanently assigned to the ship's company. The U.S. Navy had a permanent U.S. Navy Officer on the Royal Navy ship, HMS "Marlborough", until its decommission on 8 July 2005. "Churchill" is also the only U.S. Naval vessel to fly a foreign ensign. The Royal Navy's White Ensign is flown as well as the Stars and Stripes (as shown in the photograph).
References.
"This article includes information collected from the Naval Vessel Register, which, as a U.S. government publication, is in the public domain."

</doc>
<doc id="49295" url="http://en.wikipedia.org/wiki?curid=49295" title="Fine-structure constant">
Fine-structure constant

In physics, the fine-structure constant, also known as Sommerfeld's constant, commonly denoted "α" (see the Greek letter α), is a fundamental physical constant characterizing the strength of the electromagnetic interaction between elementary charged particles. It is related to the electromagnetic coupling constant "g", which characterizes the strength of the coupling of an elementary charged particle with the electromagnetic field, by the formula "g"2 = 4π"ε"0"α". Being a dimensionless quantity, it has the same numerical value in all systems of units. Arnold Sommerfeld introduced the fine-structure constant in 1916.
The currently accepted value of "α" is .
Definition.
Some equivalent definitions of "α" in terms of other fundamental physical constants are:
where:
The definition reflects the relationship between "α" and the electromagnetic coupling constant "g", which equals √4"πε"0"α".
In non-SI units.
In electrostatic cgs units, the unit of electric charge, the statcoulomb, is defined so that the Coulomb constant, "k"e, or the permittivity factor, 4"πε"0, is 1 and dimensionless. Then the expression of the fine-structure constant, as commonly found in older physics literature, becomes
In natural units, commonly used in high energy physics, where "ε"0 = "c" = "ħ" = 1, the value of the fine-structure constant is
As such, the fine-structure constant is just another, albeit dimensionless, quantity determining (or determined by) the elementary charge: "e" = √4"πα" ≈ 0.30282212 in terms of such a natural unit of charge.
Measurement.
The 2010 CODATA recommended value of "α" is
This has a relative standard uncertainty of 0.32 parts per billion.
For reasons of convenience, historically the value of the reciprocal of the fine-structure constant is often specified. The 2010 CODATA recommended value is given by
While the value of "α" can be "estimated" from the values of the constants appearing in any of its definitions, the theory of quantum electrodynamics (QED) provides a way to measure "α" directly using the quantum Hall effect or the anomalous magnetic moment of the electron. The theory of QED predicts a relationship between the dimensionless magnetic moment of the electron and the fine-structure constant "α" (the magnetic moment of the electron is also referred to as "Landé "g"-factor" and symbolized as "g"). The most precise value of "α" obtained experimentally (as of 2012) is based on a measurement of "g" using a one-electron so-called "quantum cyclotron" apparatus, together with a calculation via the theory of QED that involved 12,672 tenth-order Feynman diagrams:
This measurement of "α" has a precision of 0.25 parts per billion. This value and uncertainty are about the same as the latest experimental results.
Physical interpretations.
The fine-structure constant, "α", has several physical interpretations. "α" is:
When perturbation theory is applied to quantum electrodynamics, the resulting perturbative expansions for physical results are expressed as sets of power series in "α". Because "α" is much less than one, higher powers of "α" are soon unimportant, making the perturbation theory extremely practical in this case. On the other hand, the large value of the corresponding factors in quantum chromodynamics makes calculations involving the strong nuclear force extremely difficult.
Variation with energy scale.
According to the theory of the renormalization group, the value of the fine-structure constant (the strength of the electromagnetic interaction) grows logarithmically as the energy scale is increased. The observed value of "α" is associated with the energy scale of the electron mass; the electron is a lower bound for this energy scale because it (and the positron) is the lightest charged object whose quantum loops can contribute to the running. Therefore 1/137.036 is the value of the fine-structure constant at zero energy. Moreover, as the energy scale increases, the strength of the electromagnetic interaction approaches that of the other two fundamental interactions, a fact important for grand unification theories. If quantum electrodynamics were an exact theory, the fine-structure constant would actually diverge at an energy known as the Landau pole. This fact makes quantum electrodynamics inconsistent beyond the perturbative expansions.
History.
Arnold Sommerfeld introduced the fine-structure constant in 1916, as part of his theory of the relativistic deviations of atomic spectral lines from the predictions of the Bohr model. The first physical interpretation of the fine-structure constant "α" was as the ratio of the velocity of the electron in the first circular orbit of the relativistic Bohr atom to the speed of light in the vacuum. Equivalently, it was the quotient between the minimum angular momentum allowed by relativity for a closed orbit, and the minimum angular momentum allowed for it by quantum mechanics. It appears naturally in Sommerfeld's analysis, and determines the size of the splitting or fine-structure of the hydrogenic spectral lines.
Is the fine-structure constant actually constant?
While at interaction energies above 80 GeV the fine-structure constant is known to approach 1/128,
physicists have pondered whether the fine-structure constant is in fact constant, or whether its value differs by location and over time. A varying "α" has been proposed as a way of solving problems in cosmology and astrophysics. String theory and other proposals for going beyond the Standard Model of particle physics have led to theoretical interest in whether the accepted physical constants (not just "α") actually vary.
Past rate of change.
The first experimenters to test whether the fine-structure constant might actually vary examined the spectral lines of distant astronomical objects and the products of radioactive decay in the Oklo natural nuclear fission reactor. Their findings were consistent with no variation in the fine-structure constant between these two vastly separated locations and times.
More recently, improved technology has made it possible to probe the value of "α" at much larger distances and to a much greater accuracy. In 1999, a team led by John K. Webb of the University of New South Wales claimed the first detection of a variation in "α". Using the Keck telescopes and a data set of 128 quasars at redshifts 0.5 < "z" < 3, Webb "et al." found that their spectra were consistent with a slight increase in "α" over the last 10–12 billion years. Specifically, they found that
In 2004, a smaller study of 23 absorption systems by Chand "et al.", using the Very Large Telescope, found no measureable variation:
However, in 2007 simple flaws were identified in the analysis method of Chand "et al.", discrediting those results.
King "et al." have used Markov Chain Monte Carlo methods to investigate the algorithm used by the UNSW group to determine formula_17 from the quasar spectra, and have found that the algorithm appears to produce correct uncertainties and maximum likelihood estimates for formula_17 for particular models. This suggests that the statistical uncertainties and best estimate for formula_17 stated by Webb "et al." and Murphy "et al." are robust.
Lamoreaux and Torgerson analyzed data from the Oklo natural nuclear fission reactor in 2004, and concluded that "α" has changed in the past 2 billion years by 4.5 parts in . They claimed that this finding was "probably accurate to within 20%." Accuracy is dependent on estimates of impurities and temperature in the natural reactor. These conclusions have to be verified.
In 2007, Khatri and Wandelt of the University of Illinois at Urbana-Champaign realized that the 21 cm hyperfine transition in neutral hydrogen of the early Universe leaves a unique absorption line imprint in the cosmic microwave background radiation. They proposed using this effect to measure the value of "α" during the epoch before the formation of the first stars. In principle, this technique provides enough information to measure a variation of 1 part in (4 orders of magnitude better than the current quasar constraints). However, the constraint which can be placed on "α" is strongly dependent upon effective integration time, going as "t"−1/2. The European LOFAR radio telescope would only be able to constrain Δ"α"/"α" to about 0.3%. The collecting area required to constrain Δ"α"/"α" to the current level of quasar constraints is on the order of 100 square kilometers, which is economically impracticable at the present time.
Present rate of change.
In 2008, Rosenband "et al." used the frequency ratio of Al+ and Hg+ in single-ion optical atomic clocks to place a very stringent constraint on the present time variation of "α", namely Δ"α̇"/"α" = per year. Note that any present day null constraint on the time variation of alpha does not necessarily rule out time variation in the past. Indeed, some theories that predict a variable fine-structure constant also predict that the value of the fine-structure constant should become practically fixed in its value once the universe enters its current dark energy-dominated epoch.
Spatial variation - Australian dipole.
In September 2010 researchers from Australia said they had identified a dipole-like structure in the variation of the fine-structure constant across the observable universe. They used data on quasars obtained by the Very Large Telescope, combined with the previous data obtained by Webb at the Keck telescopes. The fine-structure constant appears to have been larger by one part in 100,000 in the direction of the southern hemisphere constellation Ara, 10 billion years ago. Similarly, the constant appeared to have been smaller by a similar fraction in the northern direction, 10 billions of years ago.
In September and October 2010, after Webb's released research, physicists Chad Orzel and Sean M. Carroll suggested various approaches of how Webb's observations may be wrong. Orzel argues that the study may contain wrong data due to subtle differences in the two telescopes, in which one of the telescopes the data set was slightly high and on the other slightly low, so that they cancel each other out when they overlapped. He finds it suspicious that the triangles in the plotted graph of the quasars are so well-aligned (triangles representing sources examined with both telescopes). Carroll suggested a totally different approach; he looks at the fine-structure constant as a scalar field and claims that if the telescopes are correct and the fine-structure constant varies smoothly over the universe, then the scalar field must have a very small mass. However, previous research has shown that the mass is not likely to be extremely small. Both of these scientists' early criticisms point to the fact that different techniques are needed to confirm or contradict the results, as Webb, et al., also concluded in their study.
In October 2011, Webb "et al." reported
a variation in α dependent on both redshift and spatial direction. They report "the combined data set fits a spatial dipole" with an increase in α with redshift in one direction and a decrease in the other. "[I]ndependent VLT and Keck samples give consistent dipole directions and amplitudes..."
Anthropic explanation.
The anthropic principle is a controversial argument of why the fine-structure constant has the value it does: stable matter, and therefore life and intelligent beings, could not exist if its value were much different. For instance, were "α" to change by 4%, stellar fusion would not produce carbon, so that carbon-based life would be impossible. If "α" were > 0.1, stellar fusion would be impossible and no place in the universe would be warm enough for life as we know it.
However, if multiple coupling constants are allowed to vary simultaneously, not just "α", then in fact almost all combinations of values support a form of stellar fusion.
Numerological explanations.
As a dimensionless constant which does not seem to be directly related to any mathematical constant, the fine-structure constant has long fascinated physicists.
Arthur Eddington argued that the value could be "obtained by pure deduction" and he related it to the Eddington number, his estimate of the number of protons in the Universe. This led him in 1929 to conjecture that its reciprocal was precisely the integer 137. Other physicists neither adopted this conjecture nor accepted his arguments but by the 1940s experimental values for 1/α deviated sufficiently from 137 to refute Eddington's argument.
The fine-structure constant so intrigued physicist Wolfgang Pauli that he collaborated with psychiatrist Carl Jung in a quest to understand its significance. Similarly, Max Born believed if the value of alpha were any different, the universe would be degenerate, and thus that 1/137 was a law of nature.
Richard Feynman, one of the originators and early developers of the theory of quantum electrodynamics (QED), referred to the fine-structure constant in these terms:
There is a most profound and beautiful question associated with the observed coupling constant, e – the amplitude for a real electron to emit or absorb a real photon. It is a simple number that has been experimentally determined to be close to 0.08542455. (My physicist friends won't recognize this number, because they like to remember it as the inverse of its square: about 137.03597 with about an uncertainty of about 2 in the last decimal place. It has been a mystery ever since it was discovered more than fifty years ago, and all good theoretical physicists put this number up on their wall and worry about it.) Immediately you would like to know where this number for a coupling comes from: is it related to pi or perhaps to the base of natural logarithms? Nobody knows. It's one of the greatest damn mysteries of physics: a magic number that comes to us with no understanding by man. You might say the "hand of God" wrote that number, and "we don't know how He pushed his pencil." We know what kind of a dance to do experimentally to measure this number very accurately, but we don't know what kind of dance to do on the computer to make this number come out, without putting it in secretly!—Richard Feynman
Conversely, statistician I. J. Good argued that a numerological explanation would only be acceptable if it came from a more fundamental theory that also provided a Platonic explanation of the value.
Attempts to find a mathematical basis for this dimensionless constant have continued up to the present time. However, no numerological explanation has ever been accepted by the community.
Quotes.
The mystery about "α" is actually a double mystery. The first mystery – the origin of its numerical value "α" ≈ 1/137 has been recognized and discussed for decades. The second mystery – the range of its domain – is generally unrecognized.—Malcolm H. Mac Gregor

</doc>
<doc id="49298" url="http://en.wikipedia.org/wiki?curid=49298" title="Haidinger's brush">
Haidinger's brush

Haidinger's brush is an entoptic phenomenon first described by Austrian
physicist Wilhelm Karl von Haidinger in 1844.
Many people are able to perceive polarization of light.
It may be seen as a yellowish horizontal bar or bow-tie shape (with "fuzzy" ends, hence the name "brush") visible in the center of the visual field against the blue sky viewed while facing away from the sun, or on any bright background. It typically occupies roughly 3–5 degrees of vision, about the same size as the tip of one's thumb held at arm's length. The direction of light polarization is perpendicular to the yellow bar (i.e., vertical if the bar is horizontal). Fainter bluish or purplish areas may be visible between the yellow brushes (see illustration). Haidinger's brush may also be seen by looking at a white area on many LCD flat panel computer screens (due to the polarization effect of the display), in which case it is often diagonal.
Physiological causes of Haidinger's brush.
Haidinger's brush is usually attributed to the dichroism of the xanthophyll pigment of the macula. In this Fresnel–Arago laws effect, the unguided oblique rays in the cylindrical geometry of the foveal blue cones, along with their distribution, produce an extrinsic dichroism. The brush's size is consistent with the size of the macula. The macula's dichroism is thought to arise from some of its pigment molecules being arranged circularly. The small proportion of circularly arranged molecules accounts for the faintness of the phenomenon. Xanthophyll pigments tend to be parallel to visive nerves that, because the fovea is not flat, are almost orthogonal to the fovea in its central part while being nearly parallel in its outer region. As a result, two different areas of the fovea can be sensitive to two different degrees of polarization.
Seeing Haidinger's brush.
Many people find it difficult to see Haidinger's brush initially. It is very faint, much more so than generally indicated in illustrations, and, like other stabilized images, tends to appear and disappear. 
It is most easily seen when it can be made to move. Since it is always positioned on the macula, there is no way to make it move laterally, but it can be made to rotate, by viewing a white surface through a rotating polarizer, or by slowly tilting one's head to one side.
To see Haidinger's brush, start by using a polarizer, such as a lens from a pair of polarizing sunglasses. Gaze at an evenly lit, textureless surface through the lens and rotate the polarizer.
An option is to use the polarizer built into a computer's LCD screen. Look at a white area on the screen, and slowly tilt the head (a CRT monitor has no polarizer, and will not work for this purpose unless a separate polarizer is used).
It appears with more distinctness against a blue background. With practice, it is possible to see it in the naturally polarized light of a blue sky. Minnaert recommends practicing first with a polarizer, then trying it without. The areas of the sky with the strongest polarization are those 90 degrees away from the sun. Minnaert says that after a minute of gazing at the sky, "a kind of marble effect will appear. This is followed shortly by Haidinger's brush." He comments that not all observers see it in the same way. Some see the yellow pattern as solid and the blue pattern as interrupted, as in the illustrations on this page. Some see the blue as solid and the yellow as interrupted, and some see it alternating between the two states.
Use of Haidinger's brush.
The fact that the sensation of Haidinger's brush corresponds with the visual field correlate of the macula means that it can be utilised in training people to look at objects with their macula. People with certain types of strabismus may undergo an adaptation whereupon they look at the object of attention not with their fovea (at the centre of the macula) but with an eccentric region of the retina. This adaptation is known as eccentric fixation. To aid in training a person to look at an object with their fovea rather than their eccentric retinal zone, a training device can be used. One such apparatus utilises a rotating polarised plate backlit with a bright white light. Wearing blue spectacles (to enhance the Haidinger's brush image) and an occluder over the other eye, the user will hopefully notice the Haidinger's brush where their macula correlates with their visual field. The goal of the training is for the user to learn to look at the test object in such a way that the Haidinger's brush overlaps the test object (and the viewer is thus now looking at it with their fovea). The reason for such training is that the healthy fovea is far greater in its resolving power than any other part of the retina.

</doc>
<doc id="49299" url="http://en.wikipedia.org/wiki?curid=49299" title="Nellie Tayloe Ross">
Nellie Tayloe Ross

Nellie Tayloe Ross (November 29, 1876 – December 19, 1977) was an American politician, the 14th Governor of Wyoming from 1925 to 1927, and director of the United States Mint from 1933 to 1953. She was the first woman to be elected governor of a U.S. state, and remains the only woman to have served as governor of Wyoming. She was a staunch supporter of Prohibition during the 1920s.
Early years.
Nellie Davis Tayloe was born in St. Joseph, Missouri. She was the sixth child and first daughter of James Wynn Tayloe, a native of Stewart County, Tennessee, and his wife, Elizabeth Blair Green, who owned a plantation on the Missouri River. In 1884, when Nellie Tayloe was seven years of age, her family moved to Miltonvale in Cloud County in northern Kansas. This relocation happened after their Andrew County home burned, and the sheriff was about to foreclose on the property.
After she graduated from Miltonville High School in 1892, her family moved to Omaha, Nebraska. During this time she taught private piano lessons, and also attended a teacher-training college for two years. She then taught kindergarten for four years. Nellie was sent on a trip to Europe in 1896 by two of her brothers.
While on a visit to her relatives in Dover, Tennessee, in 1900, she met William Bradford Ross, whom she married on September 11, 1902. Ross practiced law and planned to live in the American West. He moved to Cheyenne and established a law practice, bringing his wife to join him there. Ross became a leader in the Democratic Party in Wyoming. He ran for office several times unsuccessfully, losing to Republican candidates each time.
Career.
In 1922, William Ross was elected governor of Wyoming by appealing to progressive voters in both parties. However, after little more than a year and a half in office, he died on October 2, 1924, from complications from an appendectomy. The Democratic Party then nominated his widow, Nellie Ross, to run for governor in a special election the following month. 
Nellie Tayloe Ross refused to campaign, but easily won the race on November 4, 1924. On January 5, 1925, she became the first female governor in the history of the United States. As governor she continued her late husband's policies, which called for tax cuts, government assistance for poor farmers, banking reform, and laws protecting children, women workers, and miners. She urged Wyoming to ratify a pending federal amendment prohibiting child labor. Like her husband, she advocated the strengthening of prohibition laws.
Ross ran for re-election in 1926, but was narrowly defeated. Ross blamed her loss in part on her refusal to campaign for herself and her support for prohibition. Nevertheless, she remained active in the Democratic Party and campaigned for Al Smith in the 1928 presidential election though the two disagreed on prohibition. At the 1928 Democratic National Convention, she received 31 votes from 10 states for vice president on the first ballot. She also gave a speech seconding Smith's nomination. After the convention, she served as vice chairman of the Democratic National Committee and as director of the DNC Women's Division.
U.S. President Franklin D. Roosevelt appointed her as the first female director of the U.S. Mint on May 3, 1933, where she served five full terms until her retirement in 1953, when Republicans under Dwight D. Eisenhower and Richard M. Nixon regained the executive branch of government.. She is famous for establishing the Franklin Half Dollar and starting the making of proof coins for public sale.
Retirement and death.
After her retirement, Ross contributed articles to various women's magazines and traveled extensively. She made her last trip to Wyoming in 1972 at the age of ninety-six. Five years later, she died in Washington, D.C., at the age of 101; at the time of her death, she was the oldest ex-governor in the United States. She is interred in the family plot in Lakeview Cemetery in Cheyenne.

</doc>
<doc id="49300" url="http://en.wikipedia.org/wiki?curid=49300" title="Isabella of Angoulême">
Isabella of Angoulême

Isabella of Angoulême (French: "Isabelle d'Angoulême", ]; c.1188 – 4 June 1246) was queen consort of England as the second wife of King John from 1200 until John's death in 1216. She was also reigning Countess of Angoulême from 1202 until 1246.
She had five children by the king including his heir, later Henry III. In 1220, Isabella married Hugh X of Lusignan, Count of La Marche, by whom she had another nine children.
Some of her contemporaries, as well as later writers, claim that Isabella formed a conspiracy against King Louis IX of France in 1241, after being publicly snubbed by his mother, Blanche of Castile for whom she had a deep-seated hatred. In 1244, after the plot had failed, Isabella was accused of attempting to poison the king. To avoid arrest, she sought refuge in Fontevraud Abbey where she died two years later, but none of this can be confirmed.
Queen of England.
She was the only daughter and heir of Aymer Taillefer, Count of Angoulême, by Alice of Courtenay, who was sister of Peter II of Courtenay, Latin Emperor of Constantinople and granddaughter of King Louis VI of France.
Isabella became Countess of Angoulême in her own right on 16 June 1202, by which time she was already queen of England. Her marriage to King John took place on 24 August 1200, in Angoulême, a year after he annulled his first marriage to Isabel of Gloucester. She was crowned queen in an elaborate ceremony on 8 October at Westminster Abbey in London. Isabella was originally betrothed to Hugh IX le Brun, Count of Lusignan, son of the then Count of La Marche. As a result of John's temerity in taking her as his second wife, King Philip II of France confiscated all of their French lands, and armed conflict ensued.
At the time of her marriage to John, the blonde and blue-eyed 12-year-old Isabella was already renowned by some for her beauty and has sometimes been called the Helen of the Middle Ages by historians. Isabella was much younger than her husband and possessed a volatile temper similar to his own. King John was infatuated with his young, beautiful wife; however, his acquisition of her had as much, if not more to do with spiting his enemies, than romantic love. She was already engaged to Hugh IX le Brun, when she taken by John. It had been said that he neglected his state affairs to spend time with Isabella, often remaining in bed with her until noon. However, these were rumors, ignited by John's enemies to discredit him as being a weak and grossly irresponsible ruler. Given that at the time they were made John was engaging in a desperate war with King Phillip of France to hold on to the remaining Plantagenet dukedoms. The common people began to term her a "siren" or "Messalina", which spoke volumes as to common opinion . Her mother-in-law, Eleanor of Aquitaine readily accepted her as John's wife.
On 1 October 1207 at Winchester Castle, Isabella gave birth to a son and heir who was named Henry after the King's father, Henry II. He was quickly followed by another son, Richard, and three daughters, Joan, Isabel, and Eleanor. All five children survived into adulthood, and would make illustrious marriages; all but Joan would produce offspring of their own.
Second marriage.
When King John died in October 1216, Isabella's first act was to arrange the speedy coronation of her nine-year-old son at the city of Gloucester on 28 October. As the royal crown had recently been lost in The Wash, along with the rest of King John's treasure, she supplied her own golden circlet to be used in lieu of a crown. The following July, less than a year after his crowning as King Henry III of England, she left him in the care of his regent, William Marshal, 1st Earl of Pembroke and returned to France to assume control of her inheritance of Angoulême.
In the spring of 1220, she married Hugh X of Lusignan, "le Brun", Seigneur de Luisignan, Count of La Marche, the son of her former fiancé, Hugh IX, to whom she had been betrothed before her marriage to King John. It had been previously arranged that her eldest daughter Joan should marry Hugh, and the little girl was being brought up at the Lusignan court in preparation for her marriage. Hugh, however, upon seeing Isabella, whose beauty had not diminished, preferred the girl's mother. Princess Joan was provided with another husband, King Alexander II of Scotland, whom she wed in 1221.
Isabella had married Hugh without waiting to receive the consent of the King's council in England, which was the required procedure for a former Queen of England, as the Council had the power to not only choose the Queen Dowager's second husband, but to decide whether or not she should be allowed to marry at all. Isabella's flouting of this law caused the Council to confiscate her dower lands and stop the payment of her pension. Isabella and her husband retaliated by threatening to keep Princess Joan, who had been promised in marriage to the King of Scotland, in France. The council first responded by sending furious letters, signed in the name of young King Henry, to the Pope, urging him to excommunicate Isabella and her husband, but then decided to come to terms with Isabella, as to avoid conflict with the Scottish king, who was eager to receive his bride. Isabella was granted, in compensation for her dower lands in Normandy, the stannaries in Devon and the revenue of Aylesbury for a period of four years. She also received £3000 as payment for arrears in her pension.
By Hugh X, Isabella had nine more children. Their eldest son Hugh XI of Lusignan succeeded his father as Count of La Marche and Count of Angoulême in 1249.
Isabella's children from her past marriage continued their lives in England.
Rebellion and death.
Described by some contemporaries as "vain, capricious, and troublesome," Isabella could not reconcile herself with her less prominent position in France. Though Queen dowager of England, Isabella was now mostly regarded as a mere Countess of La Marche and had to give precedence to other women. In 1241, when Isabella and Hugh were summoned to the French court to swear fealty to King Louis IX of France's brother, Alphonse, who had been invested as Count of Poitou, their mother, the Queen Dowager Blanche openly snubbed her. This so infuriated Isabella, who had a deep-seated hatred of Blanche due to the latter having fervently supported the French invasion of England during the First Barons' War in May 1216, that she began to actively conspire against King Louis. Isabella and her husband, along with other disgruntled nobles, including her son-in-law Raymond VII of Toulouse, sought to create an English-backed confederacy which united the provinces of the south and west against the French king. She encouraged her son Henry in his invasion of Normandy in 1230, but then did not provide him the support she had promised.
In 1244, after the confederacy had failed and Hugh had made peace with King Louis, two royal cooks were arrested for attempting to poison the King; upon questioning they confessed to having been in Isabella's pay. Before Isabella could be taken into custody, she fled to Fontevraud Abbey, where she died on 4 June 1246.
By her own prior arrangement, she was first buried in the Abbey's churchyard, as an act of repentance for her many misdeeds. On a visit to Fontevraud, her son King Henry III of England was shocked to find her buried outside the Abbey and ordered her immediately moved inside. She was finally placed beside Henry II and Eleanor of Aquitaine. Afterwards, most of her many Lusignan children, having few prospects in France, set sail for England and the court of Henry, their half-brother.
In popular culture.
She was played by actress Zena Walker in the TV series "The Adventures of Robin Hood" episode "Isabella" (1956), before her marriage to John, but not as a 12-year-old.
She was portrayed by actress Victoria Abril in the 1976 film "Robin and Marian".
She was played by actress Cory Pulman in the episode "The Pretender" (1986) of the TV series "Robin of Sherwood".
She was portrayed by actress Léa Seydoux in the 2010 film "Robin Hood".

</doc>
<doc id="49302" url="http://en.wikipedia.org/wiki?curid=49302" title="Pope Eleuterus">
Pope Eleuterus

Pope Eleuterus (died 189), also known as Eleutherius, was the Bishop of Rome from c. 174 to his death in 189. (The Vatican cites 171 or 177 to 185 or 193.) According to the "Liber Pontificalis", he was a Greek born in Nicopolis in Epirus, Greece. His contemporary Hegesippus wrote that he was a deacon of the Roman Church under Pope Anicetus (c. 154–164), and remained so under Pope Soter, whom he succeeded around 174.
Dietary law.
The 6th-century recension of "The Book of the Popes" known as the "Felician Catalog" includes additional commentary to the work's earlier entry on Eleuterus. One addition ascribes to Eleutherius a decree (or reïssuance of a decree) that no kind of food should be despised by Christians. Such a decree might have been issued against early continuations of Jewish dietary law and against similar laws practiced by the Gnostics and Montanists. It is also possible, however, that the editor of the passage attributed to Eleuterus a decree similar to another issued around the year 500 in order to give it greater authority.
British mission.
Another addition credited Eleuterus with receiving a letter from "Lucius, King of Britain" or "King of the Britons", declaring an intention to convert to Christianity. No earlier accounts of this mission have been found. It is now generally considered to be a pious forgery, although there remains disagreement over its original purpose. Haddan and Stubbs considered the passage "manifestly written in the time and tone" of St Prosper, secretary to Pope Leo the Great in the mid-5th century, and supportive of the missions of SS Germanus and Palladius. Duchesne dated the entry a little later to the pontificate of Boniface II around 530 and Mommsen to the early 7th century. Only the last would support the conjecture that it aimed to support the Gregorian mission to the Anglo-Saxons led by St Augustine, who encountered great difficulty with the native British Christians, as at the Synod of Chester. Indeed, the Celtic Christians invoked the antiquity of their church to generally "avoid" submission to Canterbury until the Norman conquest but it is noteworthy that no arguments invoking the mission to Lucius appear to have been made by either side during the synods among the Welsh and Saxon bishops.
The first British writer to mention the story was Bede and he seems to have taken it, not from native texts or traditions, but from "The Book of the Popes". Subsequently, it appeared in the 9th-century "History of the Britons" traditionally credited to Nennius: the account relates that a mission from the pope baptized "Lucius, the Britannic king, with all the petty kings of the whole Britannic people". The account, however, dates this baptism to AD 167 (a little before Eleuterus's pontificate) and credits it to Evaristus (reigned c. 99 – c. 107). In the 12th century, more details began to be added to the story. Geoffrey of Monmouth's pseudohistorical "History of the Kings of Britain" goes into great detail concerning Lucius and names the pope's envoys to him as Fagan and Duvian. "The Book of Llandaf" placed the court of Lucius in southern Wales and names his emissaries to the pope as Elfan and Medwy.
An echo of this legend penetrated even to Switzerland. In a homily preached at Chur and preserved in an 8th- or 9th-century manuscript, St Timothy is represented as an apostle to Gaul, whence he went into Britain and baptized a king named Lucius, who himself became a missionary to Gaul and finally settled at Chur, where he preached the gospel with great success. In this way Lucius, the early missionary of the Swiss district of Chur, became identified with the alleged British king of the "Liber Pontificalis".
Harnack suggests that in the document which the compiler of the "Liber Pontificalis" drew his information the name found was not Britanio, but Britio. Now this is the name (Birtha- Britium) of the fortress of Edessa. The king in question is, therefore, Lucius Ælius Septimus Megas Abgar IX, of Edessa, a Christian king, as is well known. The original statement of the "Liber Pontificalis", in this hypothesis, had nothing to do with Britain. The reference was to Abgar IX of Edessa. But the compiler of the "Liber Pontificalis" changed Britio to Brittanio, and in this way made a British king of the Syrian Lucius.
Death.
According to the "Liber Pontificalis", Pope Eleutherius died on 24 May and was buried on the Vatican Hill ("in Vaticano") near the body of St. Peter. Later tradition has his body moved to the church of San Giovanni della Pigna, near the pantheon. In 1591, his remains were again moved to the church of Santa Susanna at the request of Camilla Peretti, the sister of Pope Sixtus V. His feast is celebrated on 26 May.
References.
</dl>

</doc>
<doc id="49304" url="http://en.wikipedia.org/wiki?curid=49304" title="Henry Morton Stanley">
Henry Morton Stanley

Sir Henry Morton Stanley GCB (born John Rowlands; 28 January 1841 – 10 May 1904) was a Welsh journalist and explorer famous for his exploration of central Africa and his search for missionary and explorer David Livingstone. Upon finding Livingstone, Stanley allegedly asked, "Dr. Livingstone, I presume?" Stanley is also known for his search for the source of the Nile, his work in and development of the Congo Basin region in association with King Leopold II of the Belgians and for commanding the Emin Pasha Relief Expedition. He was knighted in 1899.
Early life.
He was born in 1841 as John Rowlands in Denbigh, Denbighshire, Wales. His mother, Elizabeth Parry, was 18 years old. She abandoned him as a very young baby and cut off all communication. She would have five more children by different men, only the youngest of whom was born in wedlock. Stanley never knew his father, who died within a few weeks of his birth; there is some doubt as to his true parentage. As his parents were unmarried, his birth certificate describes him as a bastard and the stigma of illegitimacy weighed heavily upon him all his life.
The boy John was given his father's surname of Rowlands and brought up by his maternal grandfather, Moses Parry. The once-prosperous butcher was living in reduced circumstances but cared for the boy until he died, when John was five. Rowlands stayed with families of cousins and nieces for a short time, but he was eventually sent to St Asaph Union Workhouse for the Poor. The overcrowding and lack of supervision resulted in his being frequently abused by older boys. Historian Robert Aldrich suggests that he was raped in 1847 by the headmaster of the workhouse. When John was ten, his mother and two half-siblings stayed for a short while in this workhouse, but of course he did not recognize them; the master told him who they were. He stayed until the age of 15. After completing an elementary education, he was employed as a pupil teacher in a National School.
New country, new name.
In 1859, at age 18, Rowlands emigrated to the United States in search of a new life. He disembarked at New Orleans and, according to his own declarations, became friends with a wealthy trader named Henry Hope Stanley, by accident: he saw Stanley sitting on a chair outside his store and asked him if he had any job opening. He did so in the British style: "Do you want a boy, sir?" As it happened, the childless man had indeed been wishing he had a son, and the inquiry led not only to a job, but to a close relationship between them. Out of admiration, John took Stanley's name. Later, he would write that his adoptive parent died two years after their meeting, but the elder Stanley did not die until 1878. Young Stanley assumed a local accent and began to deny being a foreigner.
Stanley reluctantly joined in the American Civil War, first enrolling in the Confederate Army's 6th Arkansas Infantry Regiment and fighting in the Battle of Shiloh in 1862. After being taken prisoner, he was recruited at Camp Douglas, Illinois, by its commander, Col. James A. Mulligan, as a "Galvanized Yankee." He joined the Union Army on 4 June 1862 but was discharged 18 days later due to severe illness. Recovering, he served on several merchant ships before joining the Navy in July 1864. On board the "Minnesota," he became a record keeper, which led him into freelance journalism. Stanley and a junior colleague jumped ship on 10 February 1865 at a port in New Hampshire, in search of greater adventures. Stanley was possibly the only man to serve in the Confederate Army, the Union Army, and the Union Navy.
Journalist.
Following the Civil War, Stanley began a career as a journalist. As part of this new career, Stanley organised an expedition to the Ottoman Empire that ended catastrophically when he was imprisoned. He eventually talked his way out of jail and received restitution for damaged expedition equipment.
In 1867, Stanley was recruited by Colonel Samuel Forster Tappan (a one-time journalist) of the Indian Peace Commission, to serve as a correspondent to cover the work of the Commission for several newspapers. Tappan had proposed that Indians be given more authority to govern themselves on reservations and President Ulysses S. Grant wanted to improve conditions on reservations. To reduce corruption, Grant proposed that religious ministers should be appointed as US Indian agents rather than military or commercial businessmen. Stanley was soon retained exclusively by James Gordon Bennett (1795–1872), founder of the "New York Herald", who was impressed by the young man's exploits and his direct style of writing. He describes this early period of his professional life in Volume I of his memoir, "My Early Travels and Adventures in America and Asia" (1895). He became one of the "Herald"’s overseas correspondents.
Finding Livingstone.
In 1869, Stanley was instructed by Bennett's son to find the Scottish missionary and explorer David Livingstone, who was known to be in Africa but had not been heard from for some time. According to Stanley's account, he asked James Gordon Bennett, Jr. (1841–1918), who had succeeded to the paper's management after his father's retirement in 1867, how much he could spend. The reply was "Draw £1,000 now, and when you have gone through that, draw another £1,000, and when that is spent, draw another £1,000, and when you have finished that, draw another £1,000, and so on — BUT FIND LIVINGSTONE!" Stanley had lobbied his employer for several years to mount this expedition. 
Stanley travelled to Zanzibar in March 1871 and outfitted an expedition with the best of everything, requiring no fewer than 200 porters. This 700 mi expedition through the tropical forest became a nightmare. His thoroughbred stallion died within a few days after a bite from a tsetse fly, many of his carriers deserted, and the rest were decimated by tropical diseases. Some 21st century authors suggest that Stanley treated his indigenous porters quite well in contemporary terms, helping to refute his reputation for brutality. But, statements by contemporaries of Stanley, such as Sir Richard Francis Burton, who claimed "Stanley shoots negroes as if they were monkeys", paints a very different picture.
Stanley found Livingstone on 10 November 1871, in Ujiji near Lake Tanganyika in present-day Tanzania. He may have greeted him with the now-famous line, "" It may also have been a fabrication, as Stanley tore out of his diary the pages relating to the encounter. Livingstone's account of the encounter does not mention these words. The phrase is first quoted in a summary of Stanley's letters published by "The New York Times" on 2 July 1872. Stanley biographer Tim Jeal argues that the explorer invented it afterwards as part of trying to raise his standing because of "insecurity about his background".
The "Herald"'s own first account of the meeting, published 1 July 1872, reports: 
"Preserving a calmness of exterior before the Arabs which was hard to simulate as he reached the group, Mr. Stanley said: – "Doctor Livingstone, I presume?" A smile lit up the features of the pale white man as he answered: "Yes, and I feel thankful that I am here to welcome you."Stanley joined Livingstone in exploring the region, establishing there was no connection between Lake Tanganyika and the River Nile. On his return, he wrote a book about his experiences: "How I Found Livingstone; travels, adventures, and discoveries in Central Africa".
Researching the Congo River.
In 1874, the "New York Herald," in partnership with Britain's "Daily Telegraph," financed Stanley on another expedition to the African continent. One of his missions was to solve a last great mystery of Africa by tracing the course of the Congo River to the sea. The difficulty of this expedition is hard to overstate. Stanley used sectional boats to pass the great cataracts that separated the Congo into distinct tracts. The boats had to be taken apart and transported around the rapids before being rebuilt to travel on the next section of river. After 999 days, on 9 August 1877, Stanley reached the Portuguese outpost of Boma, around 100 km from the mouth of the Congo River. Starting with 356 people, he reached Boma with 114 survivors, and he was the only European left. He wrote about his trials in his book "Through the Dark Continent".
Claiming the Congo for the Belgian king.
Stanley was approached by King Leopold II of the Belgians, the ambitious Belgian monarch, who in 1876 had organized a private holding company disguised as an international scientific and philanthropic association, which he called the International African Association. The King spoke of his intentions to introduce Western civilization and bring religion to that part of Africa, but did not mention he wanted to claim the lands. At the end of his life, the King was embittered by the growing perception that his establishment of a Congo Free State was mitigated by its unscrupulous government. In addition, the spread of sleeping sickness across central Africa is attributed to the movements of Stanley's enormous baggage train and the Emin Pasha relief expedition.
Emin Pasha Relief Expedition.
In 1886, Stanley led the Emin Pasha Relief Expedition to "rescue" Emin Pasha, the governor of Equatoria in the southern Sudan. King Leopold II demanded that Stanley take the longer route, via the Congo River, hoping to acquire more territory and perhaps even Equatoria. After immense hardships and great loss of life, Stanley met Emin in 1888, charted the Ruwenzori Range and Lake Edward, and emerged from the interior with Emin and his surviving followers at the end of 1890. But this expedition tarnished Stanley's name because of the conduct of the other Europeans: British gentlemen and army officers. An army major, Edmund Musgrave Barttelot, was shot by a carrier, after behaving with extreme cruelty. James Sligo Jameson, heir to Jameson's, an Irish whiskey manufacturer, bought an 11-year-old girl and offered her to cannibals to document and sketch how she was cooked and eaten. Stanley found out only when Jameson had died of fever.
Later years.
On his return to Europe, he married Welsh artist Dorothy Tennant, and they adopted a child, Denzil, who in 1954, donated some 300 items to the Stanley archives at the Royal Museum of Central Africa in Tervuren, Belgium. Denzil died in 1959. Stanley entered Parliament as a Liberal Unionist member for Lambeth North, serving from 1895 to 1900. He became Sir Henry Morton Stanley when he was made a Knight Grand Cross of the Order of the Bath in the 1899 Birthday Honours, in recognition of his service to the British Empire in Africa.
He died in London on 10 May 1904; at his funeral, he was eulogised by Daniel P. Virmar. His grave, in the churchyard of St. Michael's Church in Pirbright, Surrey, is marked by a large piece of granite inscribed with the words "Henry Morton Stanley, Bula Matari, 1841–1904, Africa". Bula Matari, which translates as "Breaker of Rocks" or "Breakstones" in Kongo, was Stanley's name among locals in Congo. It can be translated as a term of endearment: for as the leader of Leopold's expedition, he commonly worked with the labourers breaking rocks with which they built the first modern road along the Congo River. It can also be translated in far less flattering terms; Adam Hochschild suggested, while Stanley understood it as an heroic epithet, his Congolese companions understood it in a mocking and pejorative tone.
Stanley acknowledged that "[m]any people have called me hard," and wrote, in "Through the Dark Continent", that "the savage only respects force, power, boldness, and decision." His legacy of death and destruction in the Congo region is considered an inspiration for Joseph Conrad's "Heart of Darkness."
Stanley did receive and still does, some vilification for the way he conducted his expeditions. More specifically, the charges of indiscriminate cruelty against Africans. While some of the modern charges are explained away as exaggerations, it is important to note that some of his contemporaries, including many men who served under him or had first hand information, also laid the same charges. For instance, immediately after one of his expeditions in 1877, the Rev. J. P. Farler met with African porters who had been part of the expedition and wrote the following;
"Stanley's followers give dreadful accounts to their friends of the killing of inoffensive natives, stealing their ivory and goods, selling their captives, and so on. I do think a commission ought to inquire into these charges, because if they are true, it will do untold harm to the great cause of emancipating Africa. I have lived three years in Africa, I have travelled through and came in contact with many different tribes, even tribes that the Arabs report to be fierce, but I have invariably found them kind and inoffensive to me, a little selfish perhaps but fierce only when they are outraged. I cannot understand all the killing that Stanley has found necessary."
Legacy.
In 1939, a popular film called "Stanley and Livingstone" was released, with Spencer Tracy as Stanley and Cedric Hardwicke as Livingstone.
The 1949 comedy film "Africa Screams" is the story of a dimwitted clerk named Stanley Livington (played by Lou Costello), who is mistaken for a famous African explorer and recruited to lead a treasure hunt. The character's name appears to be a play on Stanley & Livingstone, but with a few crucial letters omitted from the surname; it is unknown whether this results from a typist's error or a deliberate obfuscation.
Stanley appears as a character in Simon Gray's 1978 play "The Rear Column", which tells the story of the men left behind to wait for Tippu Tib while Stanley went on to relieve Emin Pasha.
An NES game based on his life was released in 1992 called "".
In 1997, a made-for-television film, "Forbidden Territory: Stanley's Search for Livingstone", was produced by National Geographic. Stanley was portrayed by Aidan Quinn and Livingstone was portrayed by Nigel Hawthorne.
His great grandson, Richard Stanley, is a South African filmmaker and directs documentaries.
A hospital in St. Asaph, north Wales, is named after Stanley in honour of his birth in the area. It was the former workhouse in which he spent much of his early life. Memorials to H M Stanley have recently been erected in St Asaph and in Denbigh (a statue of H M Stanley with an outstretched hand).
In 1971, the BBC produced a six-part dramatised documentary series, "Search for the Nile". Much of the series was shot on location, with Stanley played by Keith Buckley.
In 2004, Welsh journalist Tim Butcher wrote his book "Blood River: A Journey Into Africa's Broken Heart", following Stanley's journey through the Congo.
The 2009 History Channel series, "Expedition Africa", documents a group of explorers attempting to traverse the route of Stanley's expedition in search of Livingstone.
Taxa named in honour.
Taxa named in honour of Henry Morton Stanley include:

</doc>
<doc id="49307" url="http://en.wikipedia.org/wiki?curid=49307" title="Standing (law)">
Standing (law)

In law, standing or locus standi is the term for the ability of a party to demonstrate to the court sufficient connection to and harm from the law or action challenged to support that party's participation in the case. Standing exists from one of three causes:
In the United States, the current doctrine is that a person cannot bring a suit challenging the constitutionality of a law unless the plaintiff can demonstrate that he/she/it is or will "imminently" be harmed by the law. Otherwise, the court will rule that the plaintiff "lacks standing" to bring the suit, and will dismiss the case without considering the merits of the claim of unconstitutionality. To have a court declare a law unconstitutional, there must be a valid reason for the lawsuit. The party suing must have something to lose in order to sue unless it has automatic standing by action of law.
International Courts.
The Council of Europe created the first international court before which individuals have automatic "locus standi".
Canada.
In Canadian administrative law, whether an individual has standing to bring an application for judicial review, or an appeal from the decision of a tribunal, is governed by the language of the particular statute under which the application or the appeal is brought. Some statutes provide for a narrow right of standing while others provide for a broader right of standing.
Frequently a litigant wishes to bring a civil action for a declaratory judgment against a public body or official. This is considered an aspect of administrative law, sometimes with a constitutional dimension, as when the litigant seeks to have legislation declared unconstitutional.
Public interest standing.
The Supreme Court of Canada developed the concept of public interest standing in three constitutional cases commonly called "the Standing trilogy": "Thorson v. Canada (Attorney General)", "Nova Scotia Board of Censors v. McNeil", and "Minister of Justice v. Borowski". The trilogy was summarized as follows in "Canadian Council of Churches v. Canada (Minister of Employment and Immigration)":
It has been seen that when public interest standing is sought, consideration must be given to three aspects. First, is there a serious issue raised as to the invalidity of legislation in question? Second, has it been established that the plaintiff is directly affected by the legislation or if not does the plaintiff have a genuine interest in its validity? Third, is there another reasonable and effective way to bring the issue before the court?
Public-interest standing is also available in non-constitutional cases, as the Court found in "Finlay v. Canada (Minister of Finance)".
United Kingdom.
In British administrative law, the applicant needs to have a sufficient interest in the matter to which the application relates. This sufficient interest requirement has been construed liberally by the courts. As Lord Diplock put it:
"[i]t would...be a grave danger to escape "lacuna" in our system of public law if a pressure group...or even a single public spirited taxpayer, were prevented by outdated technical rules of "locus standi" from bringing the matter to the attention of the court to vindicate the rule of law and get the unlawful conduct stopped."
Australia.
Australia has a Common law understanding of "locus standii" or standing which is expressed in statutes such as the ADJR and common law decisions of the High Court of Australia especially the case "Australian Conservation Foundation v Commonwealth" (1980). The test for Standing is:
1. Do the party have special interest in the matter.<br> 
2. Is that interest too distant?
There is no open standing unless statute allows it or represents needs of a specified class of people. The issue is one of remoteness.
Standing may apply to class of aggrieved people where, essentially the closeness of the plaintiff to the subject matter is the test.<br> Furthermore, a plaintiff must show that he or she has been specially affected in comparison with the public at large.
Also, while there is no open standing per se, Prerogative writs like certiorari, prohibition, Quo warranto and habeas corpus have a low burden in establishing standing. <br> Australian Courts also recognise amicus curiae (friend of the court). and the various Attorneys Generals have a presumed standing in Administrative Law cases.
United States.
In United States law, the Supreme Court of the United States has stated, "In essence the question of standing is whether the litigant is entitled to have the court decide the merits of the dispute or of particular issues."
There are a number of requirements that a plaintiff must establish to have standing before a federal court. Some are based on the case or controversy requirement of the judicial power of Article Three of the United States Constitution, § 2, cl.1. As stated there, "The Judicial Power shall extend to all Cases . . .[and] to Controversies . . ." The requirement that a plaintiff have standing to sue is a limit on the role of the judiciary and the law of Article III standing is built on the idea of separation of powers. Federal courts may exercise power only "in the last resort, and as a necessity".
The American doctrine of standing is assumed as having begun with the case of "Frothingham v. Mellon", (1923). But legal standing truly rests its first prudential origins in "Fairchild v. Hughes", (1922) which was authored by Justice Brandeis. In "Fairchild", a citizen sued the Secretary of State and the Attorney General to challenge the procedures by which the Nineteenth Amendment was ratified. Prior to it the doctrine was that all persons had a right to pursue a private prosecution of a public right. Since then the doctrine has been embedded in judicial rules and some statutes.
In 2011, in "Bond v. United States", the U.S. Supreme Court held that a criminal defendant has standing to challenge the federal statute he or she is charged with violating as being unconstitutional under the Tenth Amendment.
Standing requirements.
There are three standing requirements:
Prudential limitations.
Additionally, there are three major prudential (judicially created) standing principles. Congress can override these principles via statute:
Recent development of the doctrine.
In 1984, the Supreme Court reviewed and further outlined the standing requirements in a major ruling concerning the meaning of the three standing requirements of injury, causation, and redressability.
In the suit, parents of black public school children alleged that the Internal Revenue Service was not enforcing standards and procedures that would deny tax-exempt status to racially discriminatory private schools. The Court found that the plaintiffs did not have the standing necessary to bring suit. Although the Court established a significant injury for one of the claims, it found the causation of the injury (the nexus between the defendant’s actions and the plaintiff’s injuries) to be too attenuated. "The injury alleged was not fairly traceable to the Government conduct respondents challenge as unlawful".
In another major standing case, Lujan v. Defenders of Wildlife, 504 U.S. 555 (1992), the Supreme Court elaborated on the redressability requirement for standing. The case involved a challenge to a rule promulgated by the Secretary of the Interior interpreting §7 of the Endangered Species Act of 1973 (ESA). The rule rendered §7 of the ESA applicable only to actions within the United States or on the high seas. The Court found that the plaintiffs did not have the standing necessary to bring suit, because no injury had been established. The injury claimed by the plaintiffs was that damage would be caused to certain species of animals and that this in turn injures the plaintiffs by the reduced likelihood that the plaintiffs would see the species in the future. The court insisted though that the plaintiffs had to show how damage to the species would produce imminent injury to the plaintiffs. The Court found that the plaintiffs did not sustain this burden of proof. "The 'injury in fact' test requires more than an injury to a cognizable interest. It requires that the party seeking review be himself among the injured". The injury must be imminent and not hypothetical.
Beyond failing to show injury, the Court found that the plaintiffs failed to demonstrate the standing requirement of redressability. The Court pointed out that the respondents chose to challenge a more generalized level of Government action, "the invalidation of which would affect all overseas projects". This programmatic approach has "obvious difficulties insofar as proof of causation or redressability is concerned".
In a 2000 case, "Vermont Agency of Natural Resources v. United States ex rel. Stevens", 529 U.S. 765 (2000), the United States Supreme Court endorsed the "partial assignment" approach to qui tam relator standing to sue under the False Claims Act — allowing private individuals to sue on behalf of the U.S. government for injuries suffered solely by the government.
Taxpayer standing.
The initial case that established the doctrine of standing, "Frothingham v. Mellon", was a taxpayer standing case.
Taxpayer standing is the concept that any person who pays taxes should have standing to file a lawsuit against the taxing body if that body allocates funds in a way that the taxpayer feels is improper. The United States Supreme Court has held that taxpayer standing is not by itself a sufficient basis for standing against the United States government, unless the narrower Flast test is met. The Court has consistently found that the conduct of the federal government is too far removed from individual taxpayer returns for any injury to the taxpayer to be traced to the use of tax revenues, e.g., "United States v. Richardson."
In "DaimlerChrysler Corp. v. Cuno", the Court extended this analysis to state governments as well. However, the Supreme Court has also held that taxpayer standing is constitutionally sufficient to sue a municipal government in a federal court.
States are also protected against lawsuits by their sovereign immunity. Even where states waive their sovereign immunity, they may nonetheless have their own rules limiting standing against simple taxpayer standing against the state. Furthermore, states have the power to determine what will constitute standing for a litigant to be heard in a state court, and may deny access to the courts premised on taxpayer standing alone.
In Florida, a taxpayer has standing to sue if the state government is acting unconstitutionally with respect to public funds, or if government action is causing some special injury to the taxpayer that is not shared by taxpayers in general. In Virginia, the Supreme Court of Virginia has more or less adopted a similar rule. An individual taxpayer generally has standing to challenge an act of a city or county where they live, but does not have general standing to challenge state expenditures.
Standing to challenge statutes.
With limited exceptions, a party cannot have standing to challenge the constitutionality of a statute unless they will be subjected to the provisions of that statute. There are some exceptions, however, e.g. courts will accept First Amendment challenges to a statute on overbreadth grounds, where a person who is only partially affected by a statute can challenge parts that do not affect them on the grounds that laws that restrict speech have a chilling effect on other people's right to free speech.
The only other way someone can have standing to challenge the constitutionality of a statute is if the existence of the statute would otherwise deprive them of a right or a privilege even if the statute itself would not apply to them. The Virginia Supreme Court made this point clear in the case of "Martin v. Ziherl" 607 S.E.2d 367 (Va. 2005). Martin and Ziherl were girlfriend and boyfriend and engaged in unprotected sexual intercourse when Martin discovered that Ziherl had infected her with herpes, even though he knew he was infected and did not inform her of this. She sued him for damages, but because (at the time the case was filed) it was illegal to commit "fornication" (sexual intercourse between a man and a woman who are not married), Ziherl argued that Martin could not sue him because joint tortfeasors - those involved in committing a crime - cannot sue each other over acts occurring as a result of a criminal act ("Zysk v. Zysk", 404 S.E.2d 721 (Va. 1990)). Martin argued in rebuttal that because of the U.S. Supreme Court decision in "Lawrence v. Texas" (finding that state's sodomy law unconstitutional), Virginia's anti-fornication law was also unconstitutional for the reasons cited in Lawrence. Martin argued, therefore, she could, in fact, sue Ziherl for damages.
Lower courts decided that because the Commonwealth's Attorney doesn't prosecute fornication cases and no one had been prosecuted for fornication anywhere in Virginia in over 100 years, Martin had no risk of prosecution and thus lacked standing to challenge the statute. Martin appealed. Since Martin has something to lose - the ability to sue Ziherl for damages - if the statute is upheld, she had standing to challenge the constitutionality of the statute even though the possibility of her being prosecuted for violating it was zero. And since the U.S. Supreme Court in "Lawrence" has found that there is a privacy right in one's private, noncommercial sexual practices, the Virginia Supreme Court decided that the statute against fornication was unconstitutional. The finding gave Martin standing to sue Ziherl since the decision in "Zysk" is no longer applicable.
However, the only reason Martin had standing to challenge the statute was that she had something to lose if it stayed on the books.
State law.
State law on standing differs substantially from federal law and varies considerably from state to state.
California.
On December 29, 2009, the California Court of Appeal for the Sixth District ruled that California Code of Civil Procedure Section 367 cannot be read as imposing a federal-style standing doctrine on California's code pleading system of civil procedure. In California, the fundamental inquiry is "always" whether the plaintiff has sufficiently pleaded a cause of action, not whether the plaintiff has some entitlement to judicial action separate from proof of the substantive merits of the claim advanced. The court acknowledged that the word "standing" is often sloppily used to refer to what is really "jus tertii", and held that "jus tertii" in state law is not the same thing as the federal standing doctrine.

</doc>
<doc id="49308" url="http://en.wikipedia.org/wiki?curid=49308" title="Ripeness">
Ripeness

In United States law, ripeness refers to the readiness of a case for litigation; "a claim is not ripe for adjudication if it rests upon contingent future events that may not occur as anticipated, or indeed may not occur at all." For example, if a law of ambiguous quality has been enacted but never applied, a case challenging that law lacks the ripeness necessary for a decision.
The goal is to prevent premature adjudication; if a dispute is insufficiently developed, any potential injury or stake is too speculative to warrant judicial action. Ripeness issues most usually arise when a plaintiff seeks anticipatory relief, such as an injunction. When drafting his complaint, a plaintiff may be able to avoid dismissal on ripeness grounds by requesting alternative relief in the form of a declaratory judgment, which in many states and jurisdictions allows a court to declare the rights of parties under the facts as proven without actually ordering that anything be done.
The Supreme Court fashioned a two-part test for assessing ripeness challenges to federal regulations. The case is often applied to constitutional challenges to federal and state statutes as well. The Court said in "Abbott Laboratories v. Gardner", 387 U.S. (1967):
In both "Abbott Laboratories" and its first companion case, "Toilet Goods Association v. Gardner", 387 U.S. (1967), the Court upheld pre-enforcement review of an administrative regulation. However, the Court denied such review in the second companion case because any harm from noncompliance with the FDA regulation at issue was too speculative in the Court's opinion to justify judicial review. Justice Harlan wrote for the Court in all three cases.
The ripeness doctrine should not be confused with the advisory opinion doctrine, another justiciability concept in American law.

</doc>
<doc id="49309" url="http://en.wikipedia.org/wiki?curid=49309" title="Mootness">
Mootness

In United States law, a matter is moot if further legal proceedings with regard to it can have no effect, or events have placed it beyond the reach of the law. Thereby the matter has been deprived of practical significance or rendered purely academic.
This is different from the ordinary British meaning of "moot", which means "debatable". The shift in usage was first observed in the United States. The U.S. development of this word stems from the practice of moot courts, in which hypothetical or fictional cases were argued as a part of legal education. These purely academic issues led the U.S. courts to describe cases where developing circumstances made any judgment ineffective as "moot". The doctrine can be compared to the ripeness doctrine, another judge-made rule, that holds that judges should not rule on cases based entirely on anticipated disputes or hypothetical facts. Similar doctrines prevent the federal courts of the United States from issuing advisory opinions.
U.S. federal courts.
In the U.S. federal judicial system, a moot case must be dismissed, there being a constitutional limitation on the jurisdiction of the federal courts. The reason for this is that Article Three of the United States Constitution limits the jurisdiction of all federal courts to "cases and controversies". Thus, a civil action or appeal in which the court's decision will not affect the rights of the parties is ordinarily beyond the power of the court to decide, provided it does not fall within one of the recognized exceptions.
A textbook example of such a case is the United States Supreme Court case "DeFunis v. Odegaard", 416 U.S. (1974). The plaintiff was a student who had been denied admission to law school, and had then been provisionally admitted during the pendency of the case. Because the student was slated to graduate within a few months at the time the decision was rendered, and there was no action the law school could take to prevent that, the Court determined that a decision on its part would have no effect on the student's rights. Therefore, the case was dismissed as moot.
However, there is disagreement as to both the source of the standards, and their application in the courts. Some courts and observers opine that cases "must" be dismissed because this is a constitutional bar, and there is no "case or controversy"; others have rejected the pure constitutional approach and adopted a so-called "prudential" view, where dismissal "may" depend upon a host of factors, whether the particular person has lost a viable interest in the case, or whether the issue itself survives outside the interests of the particular person, whether the circumstance are likely to recur, etc. In actual practice, the U.S. federal courts have been uneven in their decisions, which has led to the accusation that determinations are "ad hoc" and 'result-oriented.'
There are four major exceptions to this mootness rule. These are cases of "voluntary cessation" on the part of the defendant; questions that involve secondary or collateral legal consequences; questions that are "capable of "repetition," yet evading review"; and questions involving class actions where the named party ceases to represent the class.
Voluntary cessation.
Where a defendant is acting wrongfully, but ceases to engage in such conduct once a litigation has been threatened or commenced, the court will still not deem this correction to moot the case. Obviously, a party could stop acting improperly just long enough for the case to be dismissed and then resume the improper conduct. For example, in "Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc.", 528 U.S. (2000), the Supreme Court held that an industrial polluter, against whom various deterrent civil penalties were being pursued, could not claim that the case was moot, even though the polluter had ceased polluting and had closed the factory responsible for the pollution. The court noted that so long as the polluter still retained its license to operate such a factory, it could open similar operations elsewhere if not deterred by the penalties sought.
Another example occurs when a court dismisses as "moot" a legal challenge to an existing law, where the law being challenged is either amended or repealed through legislation before the court case could be settled. A recent instance of this occurred in "Moore v. Madigan," when Illinois Attorney General Lisa Madigan declined to appeal a ruling of the Seventh Circuit United States Court of Appeals striking down Illinois handgun carry ban to the United States Supreme Court, as Illinois subsequently passed a law legalizing concealed carry with a state-issued license, which rendered the case moot.
Secondary or collateral legal consequences.
"The obvious fact of life is that most criminal convictions do in fact entail adverse collateral legal consequences. The mere possibility that this will be the case is enough to preserve a criminal case from ending ignominiously in the limbo of mootness." Sibron v. New York. 
Capable of repetition, yet evading review.
A court will allow a case to go forward if it is the type for which persons will frequently be faced with a particular situation, but will likely cease to be in a position where the court can provide a remedy for them in the time that it takes for the justice system to address their situation. The most frequently cited example is the 1973 United States Supreme Court case of "Roe v. Wade", 410 U.S. (1973), which challenged a Texas law forbidding abortion in most circumstances. The state argued that the case was moot because plaintiff Roe had given birth and was no longer pregnant by the time the case was heard. As Justice Blackmun wrote in the majority opinion:
The normal 266-day human gestation period is so short that the pregnancy will come to term before the usual appellate process is complete. If that termination makes a case moot, pregnancy litigation seldom will survive much beyond the trial stage, and appellate review will be effectively denied. Our law should not be that rigid.
Norma McCorvey, whose alias was Roe, became a pro-life advocate and attempted to have the decision of "Roe v. Wade" reversed and in "McCorvey v. Hill", 2004, the case failed to proceed based on being moot, without standing and out of time.
The Court cited "Southern Pacific Terminal Co. v. ICC", 219 U.S. (1911), which had held that a case was not moot when it presented an issue that was "capable of repetition, yet evading review". Perhaps in response to increasing workloads at all levels of the judiciary, the recent trend in the Supreme Court and other U.S. courts has been to construe this exception rather narrowly.
Many cases fall under the "capable of repetition" doctrine; however, because there is a review process available under most circumstances, the exception to declaring mootness did not apply to such cases. In "Memphis Light, Gas & Water Div. v. Craft", 436 U. S. 1, 8–9 (1978), the court noted that claims for damages save cases from mootness.
Class action representatives.
Where a class action lawsuit is brought, with one named plaintiff actually representing the interests of many others, the case will not become moot even if the named plaintiff ceases to belong to the class that is seeking a remedy. In "Sosna v. Iowa", 419 U.S. (1975), the plaintiff represented a class that was challenging an Iowa law that required persons to reside there for a year before seeking a divorce in Iowa's courts. The Supreme Court held that, although the plaintiff successfully divorced in another state, her attorneys could continue to competently advance the interests of other members of the class.
U.S. state courts.
The U.S. state courts are not subject to the Article III limitations on their jurisdiction, and some state courts are permitted by their local constitutions and laws to render opinions in moot cases where the establishment of a legal precedent is desirable. They may also establish exceptions to the doctrine. For instance, in some state courts the prosecution can lodge an appeal after a defendant is acquitted: although the appellate court cannot set aside a not-guilty verdict due to double jeopardy, it can issue a ruling as to whether a trial court's ruling on a particular issue during the trial was erroneous. This opinion will then be binding on future cases heard by the courts of that state.
Some U.S. states also accept certified questions from the federal courts or the courts of other states. Under these procedures, state courts can issue opinions, usually for the purpose of clarifying or updating state law, in cases not actually pending in those courts.
Outside of the U.S..
Although free from the U.S. Constitutional limitation, Canada has recognized that considerations of judicial economy and comity with the legislative and executive branch may justify a decision to dismiss an allegedly moot case, as deciding hypothetical controversies is tantamount to legislating. Considerations of the effectiveness of advocacy involved in the adversarial system and the possibility of recurrence of an alleged constitutional violation may sway the court. Additionally, the federal and provincial governments can ask for advisory opinions in hypothetical scenarios, termed reference questions, from their respective highest courts.
Moot point.
The phrase moot point refers to an issue that is irrelevant to a subject being discussed. Due to the relatively uncommon usage of the word moot, this is sometimes rendered as the malapropism "mute point".
See also.
Listen to this article ()
This audio file was created from a revision of the "Mootness" article dated 2006-07-29, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="49315" url="http://en.wikipedia.org/wiki?curid=49315" title="Default (law)">
Default (law)

In law, a default is the failure to do something required by law or to appear
at a required time in legal proceedings.
In the United States, for example, when a party has failed to file meaningful response to pleadings within
the time allowed, with the result that only one side of a controversy has been presented
to the court, the party who has pleaded a claim for relief and received no response may request entry of default. In some jurisdictions the court may proceed to enter judgment immediately: others require that the plaintiff file a
notice of intent to take the default judgment and serve it on the unresponsive party.
If this notice is not opposed, or no adequate justification for the delay or lack
of response is presented, then the plaintiff is entitled to judgment in his favor. Such a judgment is referred to as a "default judgment" and, unless otherwise ordered, has the same effect as a judgment entered in a contested case.
It is possible to vacate or remove the default judgment, depending on the particular state's law.
Entry of default in the United States district courts is governed by Rule 55 of the Federal Rules of Civil Procedure. 

</doc>
<doc id="49316" url="http://en.wikipedia.org/wiki?curid=49316" title="Clare Martin">
Clare Martin

Clare Majella Martin (born 15 June 1952) is a former Australian journalist and politician. She was elected to the Northern Territory Legislative Assembly in a shock by-election win in 1995. She was appointed Opposition Leader in 1999, and won a surprise victory at the 2001 territory election, becoming the first Australian Labor Party (ALP) and first female Chief Minister of the Northern Territory. At the 2005 election, she led Territory Labor to the second-largest majority government in the history of the Territory, before resigning as Chief Minister on 26 November 2007.
Early life.
Martin was one of ten children. Her parents were strong Catholics and passionate Democratic Labor Party supporters. Her uncle, Kevin Cairns, was a Liberal minister and MP in the McMahon government, but the family was not inclined towards his conservative politics. Martin's ancestry includes the Coughlin family, which also had NSW's first female statistician and the noted test cricketer Victor Trumper. The family was originally from County Offaly, Ireland until the Cromwell invasion, then left County Cork in the 1850s just after the Potato Famine.
After attending Loreto Normanhurst, Martin graduated from the University of Sydney in 1975 with a Bachelor of Arts degree, in which her major study was Music.
Pre-political career.
Having spent time in London and other overseas cities, she began working as a typist for the Australian Broadcasting Corporation in Sydney in 1978. In 1979, she became a trainee reporter. After several years, she began to take an interest in presenting, but was told that she would not be given a position in Sydney unless she had experience elsewhere . In February 1983, Martin was then offered a six-month position presenting a morning radio show in Darwin for the ABC Radio station 5DR.
She had little intention of staying there, and briefly returned to Canberra in May 1983, before being offered a job in Sydney. However, at the same time, Martin's partner was offered a partner's position at the law firm he had worked in Darwin. He liked living in Darwin and was keen to take up the position, so Martin agreed to decline the Sydney job and return to Darwin in May 1985 where she gained another position on an ABC Radio morning show.
In 1986, Martin made the move to television, as the presenter of "The 7.30 Report" until 1988. After returning from long service leave where she cared for her two young children, Martin returned to work in 1990 to work on ABC Radio's morning program.
Political career.
Martin had been interested in political journalism for some years, although she was not a member of any party, believing that party affiliation compromises journalistic integrity. In 1994, she was approached to contest the Darwin Legislative Assembly seat of Casuarina for the Australian Labor Party at the 1994 election. However, she was defeated by Country Liberal Party candidate Peter Adamson. She soon resigned from the party and returned to journalism, but when CLP Chief Minister Marshall Perron resigned from his Darwin seat of Fannie Bay, Martin opted to contest the ensuing by-election as the Labor candidate. Fannie Bay, like most Darwin electorates, had been a CLP stronghold; Perron held it with a majority of 8 percent. However, in a considerable upset, Martin went on to win the seat by 69 votes, becoming one of only two ALP MLAs in Darwin.
Martin worked hard to retain her seat at the 1997 election, and was successful, holding Fannie Bay despite a heavy defeat for the ALP. She subsequently served as Shadow Minister for Lands under then leader Maggie Hickey. When Hickey unexpectedly resigned in February 1999, Martin was in a position to succeed her, and was soon elected party leader, and hence Opposition Leader. She soon emerged as a vocal critic of the Burke government's policy of mandatory sentencing, and began preparing the ALP for the next election, which was then two years away.
Term as Chief Minister.
Martin faced her first electoral test as leader at the 2001 election. At the time, the Country Liberal Party had held office for 27 years, and Labor had never come particularly close to government. Indeed, it had never managed to win more than nine seats at any election. However, the ALP was coming off a particularly successful eighteen months, and Martin ran a skilled campaign. She was also able to take advantage of a number of gaffes made by Chief Minister Denis Burke, such as the decision to preference One Nation over the ALP - which lost the CLP a number of votes in crucial Darwin seats. The election also came during a bad time for the federal Coalition government, which was under fire for introducing a GST after previously vowing not to do so.
Despite this, most commentators were predicting the CLP would be returned for a ninth term in government, albeit with a reduced majority. However, in a shock result, Labor scored an eight-seat swing, achieving majority government by one seat. It did so on the strength of an unexpected Labor wave in Darwin. Labor had gone into the election holding only two seats in the capital--those of Martin and Paul Henderson--and had never held more than two seats in Darwin at any time. In the 2001 election, however, Labor took all seven seats in the northern part of the city. Darwin's northern suburbs are somewhat more diverse than the rest of the city. In the process, they ousted four sitting MLAs; Labor had not unseated a CLP incumbent since 1980]. Although the CLP won a bare majority of the two-party vote, Labor's gains in Darwin were enough to make Martin the first ALP and first female Chief Minister in the history of the Northern Territory. Martin herself was reelected with a healthy swing of 9.2 percent in Fannie Bay, turning it into a safe Labor seat in one stroke.
As Chief Minister, Martin immediately set about making changes, repealing the territory's controversial mandatory sentencing laws, and introducing freedom of information legislation, which had been neglected during the CLP's 27-year rule. 
Aboriginal issues.
Although Martin appointed Aboriginal Territorians to her cabinet, she has been criticised for not improving the lot of her Aboriginal constituents, the majority of whom have a life expectancy well below that of white Australians. A respected commentator in "The Bulletin" suggested that she had gone slow on Aboriginal issues because she feared a white backlash that could have resulted in her government being toppled.
The life expectancy of the Northern Territory's Aboriginal citizens did not increase markedly during Martin's administration. Alcohol abuse continued to be a major issue in Aboriginal communities and third-world diseases like trachoma could be seen in remote Aboriginal townships. However in 2006, Martin rejected accusations by John Howard and Federal Indigenous Affairs Minister, Mal Brough, that her government had been underfunding Aboriginal communities. A summit between the federal and territory governments was proposed by Mal Brough in May 2006, but this was snubbed by Martin.
Martin was critical of the Federal Government's intervention in Aboriginal communities as announced in 2007. She opposed certain aspects of the intervention such as removal of the permit system. In response, the Federal Government rejected the Territory's argument, saying it was essential to remove artificial barriers to Aboriginal townships that prevent the measures needed to improve living conditions for Indigenous children
Achievements.
In the longer term, she oversaw the completion of the Adelaide-Darwin railway, which had begun under the Burke government, and vowed to resurrect the stalled statehood movement. She also managed to markedly boost the ALP's standing amongst the electorate, as seen in the 2003 Katherine by-election, which saw a major swing to the party.
By 2005, the Northern Territory, under Martin's leadership, had achieved the following:
As Chief Minister, Martin led the ALP to the 2005 election, which was their first as an incumbent government in the Territory. Martin campaigned largely on law and order issues. It was predicted that the ALP would win a relatively narrow victory. However, to the surprise of nearly all commentators, Martin led the ALP to a smashing victory. The final result gave 19 seats to the ALP, 4 to the opposition CLP and 2 to independents. The ALP won six seats from the CLP, four of which they had never won before in any election. In the most unexpected victory of all, the ALP even managed to unseat the Opposition Leader and former Chief Minister, Burke, in his own electorate. Labor won the second-largest majority government in the history of the Territory, bettered only by the CLP's near-sweep of the Legislative Assembly at the first elections, in 1974. 
On 10 September 2007, Queensland Premier Peter Beattie announced he would leave politics that week. This left Martin as Labor's longest-serving current state or territory leader, and as the longest-serving state or territory head of government in Australia, until she herself announced her resignation on 26 November 2007.
Resignation.
On 26 November 2007, Clare Martin and her deputy Syd Stirling announced their resignations at a media conference in Darwin. Northern Territory education minister Paul Henderson was elected as the new leader and Chief Minister by the ALP caucus.
Post-political career.
In 2008, Martin became Chief Executive Officer of the Australian Council of Social Service, based in Sydney. In August 2010 she returned to the Northern Territory to become a Professorial Fellow in the Public and Social Policy Research Institute at Charles Darwin University.

</doc>
<doc id="49322" url="http://en.wikipedia.org/wiki?curid=49322" title="Rajmund Kanelba">
Rajmund Kanelba

Raymond Kanelba also "Rajmund Kanelba" (1897–1960) was a 20th-century Polish painter.
He was born in Warsaw and educated there as well as in Vienna and Paris. He was strongly influenced by the école de Paris but with rather realistic and anti-impressionist style. In 1926 his works were on display in Salon des Indépendants and Salon d'Automne and in 1952 he had a large exposition of his paintings in New York City.
Rajmund Kanelba lived most of his life in France but died in London.

</doc>
