<doc id="51215" url="http://en.wikipedia.org/wiki?curid=51215" title="Airliner">
Airliner

An airliner is an airplane, usually large, used for transporting passengers and air cargo. Such aircraft are most often operated by airlines. Although the definition of an airliner can vary from country to country, an airliner is typically defined as an aircraft intended for carrying multiple passengers or cargo in commercial service.
The largest airliners are "wide-body" jets. These aircraft are frequently called "twin-aisle aircraft" because they generally have two separate aisles running from the front to the back of the passenger cabin. These aircraft are usually used for long-haul flights between airline hubs and major cities with many passengers.
A smaller, more common class of airliners is the "narrow-body" or "single aisle" aircraft. These smaller airliners are generally used for short to medium-distance flights with fewer passengers than their wide-body counterparts.
"Regional airliners" typically seat fewer than 100 passengers and may be powered by turbofans or turboprops. These airliners are the non-mainline counterparts to the larger aircraft operated by the major carriers, legacy carriers, and flag carriers and are used to feed traffic into the large airline hubs.
The lightest (light aircraft, list of light transport aircraft) of short haul regional feeder airliner type aircraft that carry 19 or fewer passenger seats are called "commuter aircraft," "commuterliners, feederliners," and "air taxis", depending on their size, engines, how they are marketed, region of the world, and seating configurations. The Beechcraft 1900, for example, has only 19 seats.
History.
Inter-war period.
When the Wright brothers made the world’s first sustained heavier-than-air flight, they laid the foundation for what would become a major transport industry. Their flight in 1903 was just 11 years before what is often defined as the world’s first airliner. These airliners would change the world socially, economically, and politically in a way that had never been done before.
If an airliner is defined as an aircraft intended for carrying multiple passengers in commercial service, the Russian Sikorsky Ilya Muromets was the first aircraft meeting that definition. The Ilya Muromets was a luxurious aircraft with a separate passenger saloon, wicker chairs, bedroom, lounge and a toilet. The aircraft also had heating and electrical lighting. The Ilya Muromets first flew on December 10, 1913. On February 25, 1914, it took off for its first demonstration flight with 16 passengers aboard. From June 21 – June 23, it made a round-trip from Saint Petersburg to Kiev in 14 hours and 38 minutes with one intermediate landing. However, it was never used as a commercial airliner due to the onset of World War I.
In 1915 The very first airliner (for commercial use) was used by Elliot Air Service. The aircraft was a Curtiss JN 4, a small biplane which was used mainly in World War I as a trainer. Later, it was also used as a tour and familiarization flight aircraft in the early 20's. The JN 4 (or "Jenny") was the best aircraft to bring provide passenger and cargo service in Northern Canada
In 1919, after World War I, the Farman F.60 Goliath, originally designed as a long-range heavy bomber, was converted for commercial use into a passenger airliner. It could seat 14 passengers from 1919, and approximately 60 were built. Initially several publicity flights were made, including one on 8 February 1919, when the Goliath flew 12 passengers from Toussus-le-Noble to RAF Kenley, near Croydon, despite having no permission from the British authorities to land.
Another important airliner built in 1919 was the Airco DH.16; a re-designed Airco DH.9A with a wider fuselage to accommodate an enclosed cabin seating four passengers, plus pilot in an open cockpit. In March 1919, the prototype first flew at Hendon Aerodrome. Nine aircraft were built, all but one being delivered to the nascent airline, Aircraft Transport & Travel Limited (AT&T). AT&T used the first aircraft for pleasure flying, and on 25 August 1919 it inaugurated the first scheduled international airline service from London to Paris. One aircraft was sold to the River Plate Aviation Company in Argentina, to operate a cross-river service between Buenos Aires and Montevideo.
Meanwhile the competing Vickers converted its successful WW1 bomber, the Vickers Vimy, into a civilian version, the Vimy Commercial. It was redesigned with a larger diameter fuselage (largely of spruce plywood), and first flew from the Joyce Green airfield in Kent on 13 April 1919.
The world's first all-metal aircraft was the Junkers F.13, also from 1919 with 322 built.
The Dutch Fokker company produced the Fokker F.II followed by its development the F.III. These aircraft were used by the Dutch airline KLM when it re-opened an Amsterdam-London service in 1921. The Fokkers were soon flying to destinations across Europe, including Bremen, Brussels, Hamburg and Paris. They proved to be very reliable aircraft.
The Handley Page company in Britain produced the Handley Page Type W as the company's first civil transport aircraft. It housed two crew in an open cockpit and 15 passengers in an enclosed cabin. Powered by two 450 hp (336 kW) Napier Lion engines the prototype first flew on 4 December 1919, shortly after it was displayed at the 1919 Paris Air Show at Le Bourget. It was the world's first airliner to be designed with an on-board lavatory.
Meanwhile, in France the Bleriot-SPAD S.33 was a great success throughout the 1920s, initially serving the Paris-London route, and later on continental routes. The enclosed cabin could carry four passengers with an extra seat in the cockpit.
By 1921, it was becoming apparent that aircraft capacity needed to be larger for the economics to remain favourable. The English company de Havilland, therefore built the ten-passenger DH.29 monoplane, while starting work on the design of the DH.32, an eight-seater biplane with a less powerful but more economical Rolls-Royce Eagle engine. Owing to the urgent need for more capacity, however, work on the DH.32 was stopped and the DH.34 biplane was designed, accommodating ten passengers.
Throughout the 1920s, companies in Britain and France were at the forefront of the civil airliner industry, often considerably aided by government subsidies.
In America, the Ford Trimotor was an important early airliner. With two engines mounted on the wings and one in the nose and a slabsided body, it carried eight passengers and was produced from 1925 to 1933. It was used by the predecessor to Trans World Airlines, and by other airlines long after production ceased.
By the 1930s, the airliner industry had matured and large consolidated national airlines were established with regular international services that spanned the globe, including Imperial Airways in Britain, Lufthansa in Germany, KLM in the Netherlands and United Airlines in America. Multi-engined aircraft were now capable of transporting dozens of passengers in comfort.
In Britain, the de Havilland Dragon was a successful aircraft during the 1930s. Its simple design used a plywood box fuselage. It could carry six passengers each with 45 lb (20 kg) of luggage on the London-Paris route on a fuel consumption of just 13 gal (49 l) per hour. The wing panels outboard of the engines could be folded for storage. The type was attractive as a short-haul low capacity airliner and was soon in service worldwide. British production of the DH.84 ended when it was replaced on the assembly line by the more powerful and elegant de Havilland Dragon Rapide.
The first modern-looking sleek metal airliners also came into service in the 1930s. In 1932, in the United States, the 14-passenger Douglas DC-2 flew and in 1935 the more powerful, faster, 21–32 passenger Douglas DC-3. DC-3s were produced in quantity for World War II and sold as surplus afterward. The Douglas DC-3 was a particularly important airplane, because it was the first airliner to be profitable without a government subsidy.
Long-haul flights were expanded during the 1930s as both Pan American Airways and Imperial Airways competed in the provision of transatlantic travel using flying boats such as the British Short Empire and the American Boeing 314. This prefigured the dramatic growth of transatlantic travel in the post-war period.
The post-war jet age.
Britain.
In Britain, the Brabazon Committee was formed in 1942 under John Moore-Brabazon, 1st Baron Brabazon of Tara to investigate the future needs of the British Empire's civilian airliner market. The study was an attempt at defining in broad overview; the impact of projected advances in aviation technology and to forecast the global needs of the post war British Empire (in South Asia, Africa, the Near and Far East) and Commonwealth (Australia, Canada, New Zealand) in the area of air transport, for passengers, mail, and cargo. The crucial problem that the planners faced was that an agreement had been reached between the United States and the United Kingdom in 1942 to split responsibility for building multi-engine aircraft types for British use: the US would concentrate on transport aircraft while the UK would concentrate on their heavy bombers. This decision meant that the United Kingdom would be left at the close of the war with little experience in the design, manufacture and final assembly of transport aircraft.
The final report called for the construction of four general designs studied by the committee and members of the state-owned airlines British Overseas Airways Corporation (BOAC) and later British European Airways (BEA). The first three designs were piston-powered aircraft of varying sizes for different markets, while the Type IV design, at the urging of Geoffrey de Havilland whose company was involved in development the first jet fighters, was for a jet-powered 100-seat design.
The Type I design, after a brief contest was given to the Bristol Aeroplane Company, building on submissions they had made during the war for a "100 ton bomber". This evolved into the creation of the Bristol Brabazon.
The Type II process was complicated by the proposition of Vickers that there should be a move to the newly developed turboprop power. The specification was therefore split in two, with the conventional piston design going to the eventual de Havilland Dove and Airspeed Ambassador. The revolutionary VC.2 Viceroy, first flown in 1948 was the first turboprop design anywhere. The Type III requirement was developed as the Avro Tudor.
The Type IV for the jet-powered aircraft went to de Havilland and became, in 1949, the world's first jet airliner, the Comet. It featured an aerodynamically clean design with four de Havilland Ghost turbojet engines buried in the wings, a pressurised fuselage, and large square windows. For the era, it offered a relatively quiet, comfortable passenger cabin and showed signs of being a commercial success at its 1952 debut. However, a year after entering commercial service the Comets began suffering problems, with three of them breaking up during mid-flight in well-publicised accidents. This was later found to be due to catastrophic metal fatigue, not well understood at the time, in the airframes. The Comet was withdrawn from service and extensively tested to discover the cause. Rival manufacturers meanwhile heeded the lessons learned from the Comet while developing their own aircraft. Although sales never fully recovered, the improved Comet 2 and the prototype Comet 3 culminated in the redesigned Comet 4 series which debuted in 1958 and had a productive career of over 30 years.
This disaster, coupled with the fact that the UK's domestic market was much smaller than the US's, meant that by the 1960s it was increasingly clear that the UK had lost the airliner market to the US, and later designs like the BAC 1-11, Vickers VC-10, and Hawker Siddeley Trident although successful would be unable to win a substantial part of it back again. Another committee was formed to consider supersonic designs, STAC, and worked with Bristol to create the Bristol 223 design for a 100-passenger transatlantic airliner. However this was going to be so expensive to produce that the effort was later merged with similar efforts in France to create the first supersonic airliner - the Concorde.
United States.
The United States, conversely, gained a huge advantage in design and production in the airline industry in the years leading up to the war, but many of the developments would be put off until after the war as the manufacturing efforts were placed on the war effort. The advancements that the United States would make in this industry were in large part due to the cooperation of the airlines discussing what they desired with the airliner manufacturers.
Soon after the war though Douglas made a large advancement with the DC-4, although this could not cross the Atlantic at every point, it was able to make a nonstop flight from New York to the United Kingdom. Due to the war going on, the first batch of these planes went to the U.S. Army and Air Forces, and was named the C-54 Skymaster. Some of these that were used in the war would later be converted for the airline industry, along with the passenger and cargo versions that were placed on the market once the war ended. Douglas would later develop a version of this plane that was pressurized and five feet longer; this redesigned plane would become the DC-6. These DC-6s would be grounded for six months to rectify a few safety issues that were causing in-flight fires.
Soon after the DC-4, Lockheed developed the distinctive triple-tail Constellation. An aviation breakthrough, it was the first commercially successful pressurized airliner, allowing it to fly higher than other airliners. Its fuselage was some 127 inches wider than the DC-4s. Drafted by the military in World War II, it experienced a similar late entry into the civilian airline industry. Safety concerns grounded it for six months soon after it entered service while problems were investigated and repaired.
In 1947 the Boeing 377 Stratocruiser entered the industry with a completely different design than Douglas and Lockheed aircraft. Based on the C-97 Stratofreighter military transport, it had a double deck and pressurized fuselage. Luxury and a 100-passenger capacity distinguished it from its rivals. While 900 C-97s were supplied to the military, only 55 were produced for civil aviation.
The American companies had done a great job of advancing the status of transcontinental travel, but there was also the aging fleet of DC-3s that had to be addressed. Convair decided that they were going to address this market, and would begin producing the Convair 240, which was a 40-person fully pressurized plane. There were 566 of these planes that would fly, including two that were equipped with jet-assisted take off units. Convair would later develop the Convair 340, which was slightly larger and could accommodate between 44 and 52 passengers, and 311 of this model plane were produced. Finally Convair would create a Convair 440, which had small modifications, including much better soundproofing than the previous models. Convair would experience a little bit of competition from the Martin 2-0-2 and Martin 4-0-4, but in general Convair was able to control this market, as the 2-0-2 had safety concerns and was unpressurized, and the 4-0-4 only sold around 100 units.
The United States was dominant in this industry for several reasons, including a large domestic market for these planes. The market would also work in the United States favor as the American companies began to build pressurized airliners. During the postwar years engines became much larger and more powerful, and safety features such as deicing, navigation, and weather were added to the planes. Lastly, the planes produced in the United States were more comfortable and had superior flight decks than those produced in Europe.
France.
In the postwar years France developed a few significant airliners, some of these being planes that could land on water, part of the reason that the French companies were so focused on these flying boats is that in 1936 the French Air Ministry requested transatlantic flying boats that could hold at least 40 passengers. Only one model from this request would ever be put into service. The first set of these were three Latécoère 631's that Air France purchased and put into service in July 1947. However, two of these planes crashed, and the third plane was soon removed because of safety concerns. There would later be a SNCASE SE.161 Languedoc build, which was a much more successful plane, and over 100 of these were built, with 40 of them being placed into service through Air-France. The French also developed the Breguet 763 Deux Ponts, which first flew in February 1949. This was a double-decker transport airliner that would end up being used for both people and cargo. This four-engine airliner would end up being used to hold large amounts of cargo or 97 passengers. After a long silence, France then created the Caravelle, the world's first short-to-midrange jet airliner. Subsequent French efforts were part of the Airbus pan-European initiative.
USSR.
Soon after the war most of the Soviet fleet of airliners consisted of DC-3s or the Lisunov Li-2. These planes were in desperate need of replacement, and in 1946 the Ilyushin Il-12 made its first flight. The Il-12 was very similar in design to American Convair 240, except was unpressurized. In 1953 the Ilyushin Il-14 would make its first flight, and this version was equipped with much more powerful engines.The main contribution that the Soviets made in regards to airliners was the Antonov An-2. This plane is a biplane, unlike most of the other airliners, and sold more units than any other transport plane.
Types.
Wide-body airliners.
The largest airliners are "wide-body" jets. These aircraft are frequently called "twin-aisle aircraft" because they generally have two separate aisles running from the front to the back of the passenger cabin. Aircraft in this category are the Boeing 747, Boeing 767, Boeing 777, Boeing 787, Airbus A300/A310, Airbus A330, Airbus A340, Airbus A380, Lockheed L-1011 TriStar, McDonnell Douglas DC-10, McDonnell Douglas MD-11, Ilyushin Il-86, and Ilyushin Il-96. These aircraft are usually used for long-haul flights between airline hubs and major cities with many passengers. Future wide-body models include the Airbus A350.
Narrow-body airliners.
A smaller, more common class of airliners is the "narrow-body" or "single aisle" aircraft. These smaller airliners are generally used for medium-distance flights with fewer passengers than their wide-body counterparts.
Examples include the Boeing 717, 737, 757, McDonnell Douglas DC-9 and MD-80/MD-90 series, Airbus A320 family, Tupolev Tu-204, Tu-214, Embraer E-Jets 190&195 and Tu-334. Older airliners like the Boeing 707, 727, Caravelle, Douglas DC-8, Fokker F70/F100, VC10, Tupolev, and Yakovlev jets also fit into this category. Future narrow-body airliners include the Bombardier CSeries family.
Regional, short haul and feederliner aircraft.
"Regional airliners" typically seat fewer than 100 passengers and may be powered by turbofans or turboprops. These airliners are the non-mainline counterparts to the larger aircraft operated by the major carriers, legacy carriers, and flag carriers and are used to feed traffic into the large airline hubs or focus cities. These particular routes may need the size of a smaller aircraft to meet the frequency needs and service levels customers expect in the marketed product that is offered by larger airlines and their modern narrow and widebody aircraft. Therefore, these short-haul airliners are usually equipped with lavatories, stand up cabins, pressurization, overhead storage bins, reclining seats, and have a flight attendant to look after the in-flight needs of the passengers during Point-to-point transit or routes.
Because these aircraft are frequently operated by smaller airlines that are contracted to provide ("feed") passengers from smaller cities to hub airports (and reverse) for a "major" or "flag" carrier, regional airliners may be painted in the liveries of the major airline for whom they provide this "feeder" service so the regional airlines may offer and market a seamless transition between the larger airline to smaller airline.
Typical aircraft in this category include the Bombardier CRJ and Embraer ERJ regional jets along with the Bombardier "Q" (DASH-8) series, ATR 42/72 and Saab 340/2000 turboprop airliners.
Commuterliners used by regional airlines and air taxi operators.
The lightest (light aircraft, list of light transport aircraft) of short haul regional feeder airliner type aircraft that carry 19 or fewer passenger seats are called "commuter aircraft," "commuterliners, feederliners," and "air taxis", depending on their size, engines, how they are marketed, region of the world, and seating configurations. The Beechcraft 1900, for example, has only 19 seats. Depending on local and national regulations, a commuter aircraft may not qualify as an airliner and may not be subject to the regulations applied to larger aircraft. Members of this class of aircraft normally lack such amenities as lavatories and galleys and typically do not carry a flight attendant as an aircrew member.
Other aircraft that may fall into this category are the Fairchild Metro, Jetstream 31, and Embraer EMB 110 Bandeirante. The Cessna Caravan and Pilatus PC-12 are single-engine turboprops, sometimes used as small airliners, although many countries stipulate a minimum requirement of two engines for aircraft to be used as airliners.
Twin piston-engined aircraft made by Cessna, Piper, Britten-Norman, and Beechcraft are also in use as short haul, short range commuter type aircraft.
Engines.
Until the beginning of the Jet Age, piston engines were common on propliners like the Douglas DC-3. Nearly all modern airliners are now powered by turbine engines, either turbofans or turboprops. Gas turbine engines operate efficiently at much higher altitudes, are more reliable than piston engines, and produce less vibration and noise. The use of a common fuel type - kerosene-based jet fuel - is another advantage. Prior to the Jet Age, it was common for the same or very similar engines to be used in civilian airliners as in military aircraft. In recent years, divergence has occurred so that it is now unusual for the same engine to be used on a military type and a civilian type. Those military aircraft which do share engine technology with airliners are typically transports or tanker types.
Airliner variants.
Some variants of airliners have been developed for carrying freight or for luxury corporate use. Many airliners have also been modified for government use as VIP transports and for military functions such as airborne tankers (for example, the Vickers VC10, Lockheed L1011, Boeing 707), air ambulance (USAF/USN McDonnell Douglas DC-9), reconnaissance (Embraer ERJ 145, Saab 340, Boeing 737), as well as for troop-carrying roles.
Configuration.
Modern jetliners are usually low-wing designs with two engines mounted underneath the swept wings (turboprop aircraft are slow enough to use straight wings). The Boeing 747 and Airbus A380 are the only airliners in production which are too heavy (more than 400 tons maximum takeoff weight) for just two engines. Smaller airliners sometimes have their engines mounted on either side of the rear fuselage. There are numerous advantages and disadvantages to this arrangement. Perhaps the most important advantage to mounting the engines under the wings is that the total aircraft weight is more evenly distributed across the wingspan, which imposes less bending moment on the wings and allows for a lighter wing structure. This factor becomes more important as aircraft weight increases, and there are no in-production airliners with both a maximum takeoff weight of more than 50 tons and engines mounted on the fuselage. The Antonov An-148 is the only in-production jetliner with high-mounted wings (usually seen in military transport aircraft), which reduces the risk of damage from unpaved runways.
Except for a few experimental or military designs, all aircraft built to date have had all of their weight lifted off the ground by airflow across the wings. In terms of aerodynamics, the fuselage has been a mere burden. NASA and Boeing are currently developing a blended wing body design in which the entire airframe, from wingtip to wingtip, contributes lift. This promises a significant gain in fuel efficiency.
Current manufacturers.
The major manufacturers with airliners currently in production include:
A great majority of the global market for middle-sized and large-sized airliners is divided between Airbus and Boeing, although Russian/former Soviet manufacturers still sell significant numbers of airliners to their traditional markets. The market for smaller-sized airliners is mostly split between ATR, Embraer and Bombardier.
Airliner recycling.
As airliners are very expensive, most are leased out for times typically from 20 to 40 years. Very few go back into service after a long lease is up because evolving aerospace technology leaves older airliners unable to compete against newer machines that can be operated at a lower cost. Many end-of-service airliners end up in the Mojave Desert, at the Mojave Air and Space Port (also known as "The Boneyard"). From this, the term "Mojave" has come to refer to the temporary storage of aircraft (e.g., during decreased demand for air travel and between short-term leases). Another airliner retirement location is Marana, Arizona.
While almost every airliner will be reduced to scrap (the exceptions end up as museum pieces or flown by collector groups), they may pass through many owners before they are retired. A well-maintained airliner can operate safely for decades, depending on how often it is flown, its operating environment, and whether damage and wear and tear is properly repaired.
What may end an airliner's working life is a lack of spare parts, as the original manufacturer and third-party manufacturers may no longer provide or support them. Corrosion and metal fatigue are other issues that become more expensive to deal with as time goes on. Eventually, these factors and advances in aircraft technology lead to older airliners becoming too expensive or inefficient to operate.
To protect the environment, the Airbus company has set up a centre in France to decommission and recycle older aircraft. More than 200 airliners will finish active life each year and will be dismantled and recycled under the newly established PAMELA Project.
Cabin configurations and features.
An airliner will usually have several classes of seating: first class, business class, and/or economy class (which may be referred to as coach class or tourist class, and sometimes has a separate "premium" economy section with more legroom and amenities). The seats in more expensive classes are wider, more comfortable, and have more amenities such as "lie flat" seats for more comfortable sleeping on long flights. Generally, the more expensive the class, the better the beverage and meal service.
Domestic flights generally have a two-class configuration, usually first or business class and coach class, although many airlines instead offer all-economy seating. International flights generally have either a two-class configuration or a three-class configuration, depending on the airline, route and aircraft type. Many airliners offer movies or audio/video on demand (this is standard in first and business class on many international flights and may be available on economy). Cabins of any class are provided with lavatory facilities, reading lights and gaspers.
Seats.
The types of seats that are provided and how much legroom is given to each passenger are decisions made by the individual airlines, not the aircraft manufacturers. Seats are mounted in "tracks" on the floor of the cabin and can be moved back and forth by the maintenance staff or removed altogether. Naturally the airline tries to maximize the number of seats available in every aircraft to carry the largest possible (and therefore most profitable) number of passengers.
Passengers seated in an exit row (the row of seats adjacent to an emergency exit) usually have substantially more legroom than those seated in the remainder of the cabin, while the seats directly in front of the exit row may have less legroom and may not even recline (for evacuation safety reasons). However, passengers seated in an exit row may be required to assist cabin crew during an emergency evacuation of the aircraft opening the emergency exit and assisting fellow passengers to the exit. As a precaution, many airlines prohibit young people under the age of 15 from being seated in the exit row.
The seats are designed to withstand strong forces so as not to break or come loose from their floor tracks during turbulence or accidents. The backs of seats are often equipped with a fold-down tray for eating, writing, or as a place to set up a portable computer, or a music or video player. Seats without another row of seats in front of them have a tray that is either folded into the armrest or that clips into brackets on the underside of the armrests. However, seats in premium cabins generally have trays in the armrests or clip-on trays, regardless of whether there is another row of seats in front of them. Seatbacks now often feature small colour LCD screens for videos, television and video games. Controls for this display as well as an outlet to plug in audio headsets are normally found in the armrest of each seat.
Overhead bins.
The overhead bins are used for stowing carry-on baggage and other items. While the airliner manufacturer will normally specify a standard version of the product to supply, airlines can choose to have bins of differing size, shape, or color installed. Over time, overhead bins evolved out of what were originally overhead shelves that were used for little more than coat and briefcase storage. As concerns about falling debris during turbulence or in accidents increased, enclosed bins became the norm. Bins have increased in size to accommodate the larger carry-on baggage passengers can bring onto the aircraft. Newer bin designs have included a handrail, useful when moving through the cabin.
Passenger service units.
Above the passenger seats are Passenger Service Units (PSU). These typically contain reading lights, air vents, and a flight attendant call light. On most narrowbody aircraft (and some Airbus A300s and A310s), the flight attendant call button and the buttons to control the reading lights are located directly on the PSU, while on most widebody aircraft, the flight attendant call button and the reading light control buttons are usually part of the in-flight entertainment system. The units frequently have small "Fasten Seat Belt" and "No Smoking" illuminated signage and may also contain a speaker for the cabin public address system.
The PSU will also normally contain the drop-down oxygen masks which are activated if there is a sudden drop in cabin pressure. These are supplied with oxygen by means of a chemical oxygen generator. By using a chemical reaction rather than a connection to an oxygen tank, these devices supply breathing oxygen for long enough for the airliner to descend to thicker, more breathable air. Oxygen generators do generate considerable heat in the process. Because of this, the oxygen generators are thermally shielded and are only allowed in commercial airliners when properly installed – they are not permitted to be loaded as freight on passenger-carrying flights. ValuJet Flight 592 crashed on May 11, 1996 as a result of improperly loaded chemical oxygen generators.
Cabin pressurization.
Airliners developed since the 1940s have had pressurized cabins (or more accurately, pressurized hulls including baggage holds) to enable them to carry passengers safely at high altitudes where low oxygen levels and air pressure would otherwise cause sickness or death. High altitude flight enabled airliners to fly above most weather systems that cause turbulent or dangerous flying conditions, and also to fly faster and further as there is less drag due to the lower air density. Pressurisation is applied using compressed air, in most cases bled from the engines, and is managed by an environmental control system which draws in clean air, and vents stale air out through a valve.
Pressurization presents design and construction challenges to maintain the structural integrity and sealing of the cabin and hull and to prevent rapid decompression. Some of the consequences include small round windows, doors that open inwards and are larger than the door hole, and an emergency oxygen system.
To maintain a pressure in the cabin equivalent to an altitude close to sea level would, at a cruising altitude around 10,000 m (33,000 feet), create a pressure difference between inside the aircraft and outside the aircraft that would require greater hull strength and weight. Most people do not suffer ill effects up to an altitude of 1800–2500 m (6000–8000 feet), and maintaining cabin pressure at this equivalent altitude significantly reduces the pressure difference and therefore the required hull strength and weight. A side effect is that passengers experience some discomfort as the cabin pressure changes during ascent and descent to the majority of airports, which are at low altitudes.
Cabin climate control.
The air bled from the engines is hot and requires cooling by air conditioning units. It is also extremely dry at cruising altitude, and this causes sore eyes, dry skin and mucosa on long flights. Although humidification technology could raise its relative humidity to comfortable middle levels, this is not done since humidity promotes corrosion to the inside of the hull and risks condensation which could short electrical systems, so for safety reasons it is deliberately kept to a low value, around 10%.
Baggage holds.
Airliners must have space on board to store baggage that will not safely fit in the passenger cabin.
Designed to hold baggage as well as freight, these compartments are called "cargo bins", "holds", or occasionally "pits". Occasionally baggage holds may be referred to as cargo decks on the largest of aircraft. These compartments can be accessed through doors on the outside of the aircraft. Despite what is seen in many movies, access doors between passenger cabins and baggage holds are rare in modern airliners.
Depending on the aircraft, baggage holds are normally inside the hull and are therefore pressurized just like the passenger cabin although they may not be heated. While lighting is normally installed for use by the loading crew, typically the compartment is unlit when the door is closed.
Baggage holds on modern airliners are equipped with fire detection equipment and larger aircraft have automated or remotely activated fire-fighting devices installed.
Narrow-body airliners.
Most "narrow-body" airliners with more than 100 seats have space below the cabin floor, while smaller aircraft often have a special compartment separate from the passenger area but on the same level.
Baggage is normally stacked within the bin by hand, sorted by destination category. Netting that fits across the width of the bin is secured to limit movement of the bags. Airliners often carry items of freight and mail. These may be loaded separately from the baggage or mixed in if they are bound for the same destination. For securing bulky items "hold down" rings are provided to tie items into place.
Wide-body airliners.
"Wide-body" airliners frequently have a compartment like the ones described above, typically called a "bulk bin". It is normally used for late arriving luggage or bags which may have been checked at the gate.
However, most baggage and loose freight items are loaded into containers called Unit Load Devices (ULDs), often referred to as "cans". ULDs come in a variety of sizes and shapes, but the most common model is the LD3. This particular container has approximately the same height as the cargo compartment and fits across half of its width.
ULDs are loaded with baggage and are transported to the aircraft on dolly carts and loaded into the baggage hold by a loader designed for the task. By means of belts and rollers an operator can maneuver the ULD from the dolly cart, up to the aircraft baggage hold door, and into the aircraft. Inside the hold, the floor is also equipped with drive wheels and rollers that an operator inside can use to move the ULD properly into place. Locks in the floor are used to hold the ULD in place during flight.
For consolidated freight loads, like a pallet of boxes or an item too oddly shaped to fit into a container, flat metal pallets that resemble large baking sheets that are compatible with the loading equipment are used.

</doc>
<doc id="51218" url="http://en.wikipedia.org/wiki?curid=51218" title="Culture of Egypt">
Culture of Egypt

The culture of Egypt has thousands of years of recorded history. Ancient Egypt was among the earliest civilizations. For millennia, Egypt maintained a strikingly complex and stable culture that influenced later cultures of Europe, the Middle East and Africa. After the Pharaonic era, Egypt itself came under the influence of Hellenism, for a time Christianity and later, Islamic culture.
Language.
The Egyptian language, which formed a separate branch among the family of Afro-Asiatic languages, was among the first written languages, and is known from the hieroglyphic inscriptions preserved on monuments and sheets of papyrus. The Coptic language, the last stage of Egyptian, is today the liturgical language of the Coptic Orthodox Church. Hieroglyphs were written on people's front doors, so that the news of the pharaoh would travel to everyone.
The "Koiné" dialect of the Greek language was important in Hellenistic Alexandria, and was used in the philosophy and science of that culture, and was later studied by Arabic scholars.
Arabic came to Egypt in the 7th century, and Egyptian Arabic has become today the modern speech of the country. Of the many varieties of Arabic, it is the most widely spoken second dialect, due to the influence of Egyptian cinema and media throughout the Arabic-speaking world.
In the lower Nile Valley, around Kom Ombo and Aswan, there are about 300,000 speakers of Nubian languages, mainly Nobiin, but also Kenuzi-Dongola. The Berber languages are represented by Siwi, spoken by about 5,000 around the Siwa Oasis. There are over a million speakers of the Domari language (an Indo-Aryan language related to Romany), mostly living north of Cairo, and there are about 60,000 Greek speakers in Alexandria. Approximately 77,000 speakers of Bedawi (a Beja language) live in the Eastern Desert.
Literature.
The ancient Egyptian literature dates back to the Old Kingdom, in the third millennium BC. Religious literature is best known for its hymns to and its mortuary texts. The oldest extant Egyptian literature is the Pyramid Texts: the mythology and rituals carved around the tombs of rulers. The later, secular literature of ancient Egypt includes the 'wisdom texts', forms of philosophical instruction. The "Instruction of Ptahhotep", for example, is a collation of moral proverbs by an Egto (the middle of the second millennium BC) seem to have been drawn from an elite administrative class, and were celebrated and revered into the New Kingdom (to the end of the second millennium). In time, the Pyramid Texts became Coffin Texts (perhaps after the end of the Old Kingdom), and finally the mortuary literature produced its masterpiece, the Book of the Dead, during the New Kingdom.
The Middle Kingdom was the golden age of Egyptian literature. Some notable texts include the Tale of Neferty, the Instructions of Amenemhat I, the Tale of Sinuhe, the Story of the Shipwrecked Sailor and the Story of the Eloquent Peasant. "Instructions" became a popular literary genre of the New Kingdom, taking the form of advice on proper behavior. The Story of Wenamun and the Instruction of Any are well-known examples from this period.
During the Greco-Roman period (332 BC − AD 639), Egyptian literature was translated into other languages, and Greco-Roman literature fused with native art into a new style of writing. From this period comes the Rosetta Stone, which became the key to unlocking the mysteries of Egyptian writing to modern scholarship. The great city of Alexandria boasted its famous Library of almost half a million handwritten books during the third century BC. Alexandria's centre of learning also produced the Greek translation of the Hebrew Bible, the Septuagint.
Drep
During the first few centuries of the Christian era, Egypt was the ultimate source of a great deal of ascetic literature in the Coptic language. Egyptian monasteries translated many Greek and Syriac words, which are now only extant in Coptic. Under Islam, Egypt continued to be a great source of literary endeavour, now in the Arabic language. In 970, al-Azhar University was founded in Cairo, which to this day remains the most important centre of Sunni Islamic learning. In the 12th century Egypt, the Jewish Talmudic scholar Maimonides produced his most important work.
In contemporary times, Egyptian novelists and poets were among the first to experiment with modern styles of Arabic-language literature, and the forms they developed have been widely imitated. The first modern Egyptian novel Zaynab by Muhammad Husayn Haykal was published in 1913 in the Egyptian vernacular. Egyptian novelist Naguib Mahfouz was the first Arabic-language writer to win the Nobel Prize in Literature. Many Egyptian books and films are available throughout the Middle East. Other prominent Egyptian writers include Nawal El Saadawi, well known for her feminist works and activism, and Alifa Rifaat who also writes about women and tradition. Vernacular poetry is said to be the most popular literary genre amongst Egyptians, represented most significantly by Bayram el-Tunsi, Ahmed Fouad Negm (Fagumi), Salah Jaheen and Abdel Rahman el-Abnudi. 
Religion.
About 90% of Egypt's population is Muslim, with a Sunni majority. About 9% of the population is Coptic Christian; other religions and other forms of Christianity comprise the remaining one percent.
Visual art.
Egyptian art in antiquity.
The Egyptians were one of the first major civilizations to codify design elements in art. The wall painting done in the service of the Pharaohs followed a rigid code of visual rules and meanings. Early Egyptian art is characterized by absence of linear perspective, which results in a seemingly flat space. These artists tended to create images based on what they knew, and not as much on what they saw. Objects in these artworks generally do not decrease in size as they increase in distance and there is little shading to indicate depth. Sometimes, distance is indicated through the use of "tiered space", where more distant objects are drawn higher above the nearby objects, but in the same scale and with no overlapping of forms. People and objects are almost always drawn in profile. Also, you may notice the people in Egyptian art are never facing forward. Archaeologists are not yet sure of why, but they are leaning towards the fact that artists status was low in the hierarchy so they could never be in front of a higher authority figure, and never be faced towards them.
Painting achieved its greats height in Dynasty XVII during the reigns of Tuthmosis IV and Amenhotep III. The Fragmentary panel of the Lady Thepu, on the right, dates from the time of the latter king.
Early Egyptian artists did have a system for maintaining dimensions within artwork. They used a grid system that allowed them to create a smaller version of the artwork, and then scale up the design based upon proportional representation in a larger grid.
Egyptian art in modern times.
Modern and contemporary Egyptian art can be as diverse as any works in the world art scene. Some well-known names include Mahmoud Mokhtar, Abdel Hadi Al Gazzar, Farouk Hosny, Gazbia Sirry, Kamal Amin, Hussein El Gebaly, Sawsan Amer and many others. Many artists in Egypt have taken on modern media such as digital art and this has been the theme of many exhibitions in Cairo in recent times. There has also been a tendency to use the World Wide Web as an alternative outlet for artists and there is a strong Art-focused internet community on groups that has found origin in Egypt. .
Science.
Egypt's cultural contributions have included great works of science, art, and mathematics, dating from antiquity to modern times.
Ancient Egypt.
Mathematics.
The ancient Egyptians were one of the first civilizations to implement Mathematical numbers. The traditional view of Ancient Egypt's 'additive' scholars reports that Egyptians confined themselves to applications of practical arithmetic with many problems addressing how a number of loaves can be divided equally between a number of men.
Imhotep.
Considered to be the first engineer, architect and physician in history known by name, Imhotep designed the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630-2611 BC, and may have been responsible for the first known use of columns in architecture. The Egyptian historian Manetho credited him with inventing stone-dressed building during Djoser's reign, though he was not the first to actually build with stone. Imhotep is also believed to have founded Egyptian medicine, being the author of the world's earliest known medical document, the Edwin Smith Papyrus.
Ptolemaic and Roman Egypt.
The silk road led straight through ancient Alexandria. Also, the Royal Library of Alexandria was once the largest in the world. It is usually assumed to have been founded at the beginning of the 3rd century BC during the reign of Ptolemy II of Egypt after his father had set up the Temple of the Muses or Museum. The initial organization is attributed to Demetrius Phalereus. The Library is estimated to have stored at its peak 400,000 to 700,000 scrolls.
One of the reasons so little is known about the Library is that it was lost centuries after its creation. All that is left of many of the volumes are tantalizing titles that hint at all the history lost due to the building's destruction. Few events in ancient history are as controversial as the destruction of the Library, as the historical record is both contradictory and incomplete. Its destruction has been attributed by some authors to, among others, Julius Caesar, Augustus Caesar, and Catholic zealots during the purge of the Arian heresy, Not surprisingly, the Great Library became a symbol of knowledge itself, and its destruction was attributed to those who were portrayed as ignorant barbarians, often for purely political reasons.
A new library was inaugurated in 2003 near the site of the old library.
The Lighthouse of Alexandria, one of the Seven Wonders of the Ancient World, designed by Sostratus of Cnidus and built during the reign of Ptolemy I Soter served as the city's landmark, and later, lighthouse.
Mathematics and technology.
Alexandria, being the center of the Hellenistic world, produced a number of great mathematicians, astronomers, and scientists such as Ctesibius, Pappus, and Diophantus. It also attracted scholars from all over the Mediterranean such as Eratosthenes of Cyrene.
Ptolemy.
Ptolemy is one of the most famous astronomers and geographers from Egypt, famous for his work in Alexandria. Born Claudius Ptolemaeus (Greek: Κλαύδιος Πτολεμαίος; c. 85 – c. 165) in Upper Egypt, he was a geographer, astronomer, and astrologer.
Ptolemy was the author of two important scientific treatises. One is the astronomical treatise that is now known as the "Almagest" (in Greek "Η μεγάλη Σύνταξις", "The Great Treatise"). In this work, one of the most influential books of antiquity, Ptolemy compiled the astronomical knowledge of the ancient Greek and Babylonian world. Ptolemy's other main work is his "Geography". This too is a compilation, of what was known about the world's geography in the Roman Empire in his time.
In his "Optics", a work which survives only in an Arabic translation, he writes about properties of light, including reflection, refraction and colour. His other works include "Planetary Hypothesis", "Planisphaerium" and "Analemma". Ptolemy's treatise on astrology, the "Tetrabiblos", was the most popular astrological work of antiquity and also enjoyed great influence in the Islamic world and the medieval Latin West.
Ptolemy also wrote influential work Harmonics on music theory. After criticizing the approaches of his predecessors, Ptolemy argued for basing musical intervals on mathematical ratios (in contrast to the followers of Aristoxenus) backed up by empirical observation (in contrast to the over-theoretical approach of the Pythagoreans). He presented his own divisions of the tetrachord and the octave, which he derived with the help of a monochord. Ptolemy's astronomical interests also appeared in a discussion of the music of the spheres.
Tributes to Ptolemy include Ptolemaeus crater on the Moon and Ptolemaeus crater on Mars.
Modern Egypt.
Ahmed Zewail.
Ahmed Zewail (Arabic: أحمد زويل‎) (born February 26, 1946) is an Egyptian chemist, and the winner of the 1999 Nobel Prize in Chemistry for his work on femtochemistry. Born in Damanhur (60 km south-east of Alexandria) and raised in Disuq, he moved to the United States to complete his PhD at the University of Pennsylvania. He was awarded a faculty appointment at Caltech in 1976, where he has remained since.
Zewail's key work has been as the pioneer of femtochemistry. He developed a method using a rapid laser technique (consisting of ultrashort laser flashes), which allows the description of reactions at the atomic level. It can be viewed as a highly sophisticated form of flash photography.
In 1999, Zewail became the third Egyptian to receive the Nobel Prize, following Anwar Sadat (1978 in Peace) and Naguib Mahfouz (1988 in Literature). In 1999, he received Egypt's highest state honour, the Grand Collar of the Nile.
Egyptology.
In modern times, archaeology and the study of Egypt's ancient heritage as the field of Egyptology has become a major scientific pursuit in the country itself. The field began during the Middle Ages, and has been led by Europeans and Westerners in modern times. The study of Egyptology, however, has in recent decades been taken up by Egyptian archæologists such as Zahi Hawass and the Supreme Council of Antiquities he leads.
The discovery of the Rosetta Stone, a tablet written in ancient Greek, Egyptian Demotic script, and Egyptian hieroglyphs, has partially been credited for the recent stir in the study of Ancient Egypt. Greek, a well known language, gave linguists the ability to decipher the mysterious Egyptian hieroglyphic language. The ability to decipher hieroglyphics facilitated the translation of hundreds of the texts and inscriptions that were previously indecipherable, giving insight into Egyptian culture that would have otherwise been lost to the ages. The stone was discovered on July 15, 1799 in the port town of Rosetta Rosetta, Egypt, and has been held in the British Museum since 1802.
Sport.
Football is the most popular sport in Egypt. Egyptian football clubs especially El Ahly and El Zamalek are known throughout the Middle East and Africa and enjoy the reputation of long-time champions of the sport regionally. They enjoy popularity even among non-Egyptians.
The Egyptian national football team won the African Cup of Nations seven times setting a new record in Africa (years: 57, 59, 86, 98, 06, 08, 10). Egypt was the first African country to join FIFA, but it has only made it to the FIFA World Cup twice, in 1934 and 1990. In the World Military Cup, Egypt won the title 5 times, and was the runner-up another 2 times.
Other popular sports in Egypt are basketball, handball, squash, and tennis. Among all African nations, the Egypt national basketball team holds the record for best performance at the Basketball World Cup and at the Summer Olympics. Furthermore, the team has won a record number of 16 medals at the African Championship. The Egyptian national squash team is always known for its fierce competition in worldwide championships, from the 1930s to today. Handball has become another increasingly popular sport among Egyptians as well. Since the early 1990s, the Egyptian national handball team has become a growing international force in the sport, winning regional and continental tournaments as well as reaching up to fourth place internationally in 2001. The Junior national handball team reached the first rank in 1993 under the lead of Captain Gamal Shams, and it hosted the tournament in 2010 setting a record in the audience number specially the match between Egypt and Denmark in the semifinals, the stadium was completely full.
In older times (1930s and 40s), Egypt was a power house in weightlifting, boxing, and wrestling with several Olympic and world championship medals.
Also roller hockey ("quad") is available in Egypt, but the teams that are playing this game may not exceed 10 teams and the most famous club which plays roller hockey is "Al-Zamalek sporting club", and also "Nasr-City sporting club". The Egyptian national roller hockey team has taken part in many world competitions, but unlikely the team didn't win any tournament.
Local sports clubs receive financial support from the local governments, and many sporting clubs are financially and administratively supported by the government.
Cinema.
Egyptian cinema has flourished since the 1930s. As a result, the Egyptian capital has been dubbed the "Hollywood of the Middle East", where the world-renowned Cairo International Film Festival is held every year. The festival has been rated by the International Federation of Film Producers Associations as being among the 11 top-class film festivals worldwide.
Most of Arabic-language TV and cinema has been notably affected by the Egyptian dialect, due to its simplicity.
The Egyptian film industry is the largest within Arabic-speaking cinema.
Music.
Egyptian music is a rich mixture of indigenous Egyptian, African and Western influences.
As early as 4000 BC, ancient Egyptians were playing harps and flutes, as well as two indigenous instruments: the ney and the oud. However, there is little notation of Egyptian music before the 7th century AD, when Egypt became part of the Muslim world. Percussion and vocal music became important at this time, and has remained an important part of Egyptian music today.
Contemporary Egyptian music traces its beginnings to the creative work of luminaries such as Abdu-l Hamuli, Almaz, Sayed Mikkawi, and Mahmud Osman, who were all patronized by Khedive Ismail and who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab, Abdel Halim Hafez and other Egyptian music giants.
From the 1970s onwards, Egyptian pop music has become increasingly important in Egyptian culture, particularly among the large youth population of Egypt. Egyptian folk music is also popular, played during weddings and other festivities. In the last quarter of the 20th century, Egyptian music was a way to communicate social and class issues. The most popular Egyptian pop singers are Amr Diab, Tamer Hosny, Mohamed Mounir and Ali El Haggar.
Belly dance, or "Raqs Sharqi" (literally: oriental dancing) may have originated in Egypt, and today the country is considered the international center of the art.
Cuisine.
Egyptian cuisine consists of local culinary traditions such as Ful medames, Kushari, and Molokhia. It also shares similarities with food found throughout the eastern Mediterranean like kebab and falafel.

</doc>
<doc id="51219" url="http://en.wikipedia.org/wiki?curid=51219" title="Thrasamund">
Thrasamund

Thrasamund (450–523), King of the Vandals and Alans (496–523), was the fourth king of the north African Kingdom of the Vandals, and reigned longer than any other Vandal king in Africa other than his grandfather Genseric.
Thrasamund was the third son born to Genseric's fourth son, Gento, and became king in 496 after all of Genseric's sons and his own brother, King Gunthamund, had died. Upon Gunthamund's death, he was one of only two living grandsons of Genseric, and inherited the throne in accordance with a law enacted by his grandfather, which bestowed the kingship on the eldest male member of a deceased king's family. 
Theoderic the Great married his widowed sister Amalafrida to Thrasamund, providing a dowry consisting of the promontory of Lilybaeum in Sicily, and a retinue of a thousand elite troops and five thousand armed retainers. Herwig Wolfram believes this happened in 500, "immediately after his [Theoderic] Roman tricennial". Despite this alliance, Thrasamund failed to aid Theoderic when the Byzantine Navy ravaged the coast of southern Italy, preventing him from coming to the assistance of King Alaric of the Visigoths in the Battle of Vouillé, which contributed to Alaric's defeat.
Procopius describes a battle between the Berbers of Tripoli under Cabaon and the Vandals, in which the Berbers used unusual tactics to defeat the Vandal cavalry. In the final year of his reign, the important port city of Leptis Magna was sacked by the Berbers.
Thrasamund also ended many years of Catholic persecution which had begun under his uncle Huneric, a move which improved the Vandals' relations with the Byzantine Empire. Procopius states that he was "a very special friend of the Emperor Anastasius."
Thrasamund died in 523 and was succeeded by his cousin Hilderic, the firstborn son of Huneric.

</doc>
<doc id="51221" url="http://en.wikipedia.org/wiki?curid=51221" title="Gunthamund">
Gunthamund

Gunthamund (c. 450-496), King of the Vandals and Alans (484-496) was the third king of the north African Kingdom of the Vandals. He succeeded his unpopular uncle Huneric, and for that reason alone, enjoyed a rather successful reign.
Gunthamund was the second son born to Gento, the fourth and youngest son of Genseric, the founder of the Vandal kingdom in Africa. Because most of Genseric's immediate family was dead, his elder brothers having been murdered by Huneric, Gunthamund found himself as the eldest male member of the family when Huneric died in 484. In accordance with his grandfather's laws on succession, which decreed that the oldest member of the family will be the successor, he was proclaimed king.
Gunthamund benefited throughout his reign from the fact that the Vandals' most powerful rivals, the Visigoths, Ostrogoths, and the Byzantine Empire, were all heavily involved in wars. Although the Vandals' power had fallen off greatly since its zenith under Genseric, they enjoyed peace under Gunthamund. Gunthamund also eased up on the persecutions of the Catholics that had begun under Huneric, a move which eased some of the unrest in his kingdom, and stabilized the kingdom's economy, which was on the verge of collapse under Huneric.
Unfortunately for the Vandals, Gunthamund followed in the footsteps of his immediate family and died rather young, in his mid-forties. He was succeeded by his brother Thrasamund, who was not as effective in ruling the kingdom.

</doc>
<doc id="51224" url="http://en.wikipedia.org/wiki?curid=51224" title="War of Jenkins' Ear">
War of Jenkins' Ear

The War of Jenkins' Ear (known as "Guerra del Asiento" in Spain) was a conflict between Britain and Spain that lasted from 1739 to 1748, with major operations largely ended by 1742. Its unusual name, coined by Thomas Carlyle in 1858, refers to an ear severed from Robert Jenkins, captain of a British merchant ship. The severed ear was subsequently exhibited before the British Parliament. The tale of the ear's separation from Jenkins, following the boarding of his vessel by Spanish coast guards in 1731, provided the impetus to war against the Spanish Empire, ostensibly to encourage the Spanish not to renege on the lucrative "asiento" contract (permission to sell slaves in Spanish America).
After 1742, the war was subsumed by the wider War of the Austrian Succession, which involved most of the powers of Europe. Peace arrived with the Treaty of Aix-la-Chapelle in 1748. From the British perspective, the war was notable because it was the first time that a regiment of colonial American troops was raised and placed "on the Establishment" – made a part of the Regular British Army – and sent to fight outside North America.
Background.
At the conclusion of the War of the Spanish Succession, the Treaty of Utrecht in 1713 gave Britain a thirty-year asiento, or contract-right, to supply an unlimited number of slaves to the Spanish colonies, and 500 tons of goods per year. This provided British traders and smugglers potential inroads into the (traditionally) closed markets in Spanish America. But Britain and Spain were often at war during this period, fighting one another in the War of the Quadruple Alliance (1718–20), the Blockade of Porto Bello (1726) and the Anglo-Spanish War (1727–1729).
In the Treaty of Seville (1729), following the Anglo-Spanish War, Britain had accorded Spanish warships the right to stop British traders and verify if the asiento right was respected. Over time, the Spanish became suspicious that British traders were abusing the contract and began to board ships and confiscate their cargoes. After very strained relations between 1727 and 1732, the situation improved between 1732 and 1737, when Sir Robert Walpole supported Spain during the War of the Polish Succession. But the causes of the problems remained and, when the opposition against Walpole grew, so did the anti-Spanish sentiment amongst the British public.
Walpole gave in to the pressure and approved the sending of troops to the West Indies and a squadron to Gibraltar under Admiral Haddock, causing an immediate Spanish reaction. Spain asked for financial compensation, which led to the British demand to annul the "Visitation Right" agreed to in the Treaty of Seville (1729). In reaction, King Philip V of Spain annulled the "Asiento Right" and had all British ships in Spanish harbours confiscated.
The Convention of Pardo, an attempt to mediate the dispute, broke down. On 14 August, Britain recalled its ambassador to Spain and officially declared war on 23 October 1739. Despite the Pacte de Famille, France remained neutral. Walpole was deeply reluctant to declare war and reportedly remarked of the jubilation in Britain "they are ringing their bells, soon they will be wringing their hands".
Nomenclature.
The incident that gave its name to the war had occurred in 1731, off the coast of Florida, when the British brig "Rebecca" was boarded by the Spanish patrol boat "La Isabela", commanded by Julio León Fandiño. After boarding, Fandiño cut off the left ear of the "Rebecca"'s captain, Robert Jenkins, whom he accused of smuggling. Fandiño told Jenkins, "Go, and tell your King that I will do the same, if he dares to do the same." In March 1738, Jenkins was ordered to testify before Parliament, presumably to repeat his story before a committee of the House of Commons. According to some accounts, he produced the severed ear as part of his presentation, although no detailed record of the hearing exists. The incident was considered alongside various other cases of "Spanish Depredations upon the British Subjects", and was perceived as an insult to the honour of the nation and a clear casus belli.
The conflict was named by essayist and historian Thomas Carlyle, in 1858, one hundred and ten years after hostilities ended. Carlyle mentioned the ear in several passages of his "History of Friedrich II" (1858), most notably in Book XI, chap VI, where he refers specifically to "the War of Jenkins's Ear".
Conduct of the War.
First attack on La Guaira (22 October 1739).
Following the testimony of Jenkins, and petitions from other West India merchants, the opposition in Parliament voted on 28 March 1738 to send "an Address" to the King, asking his Majesty to seek redress from Spain. More than one year later, all diplomatic means having been exhausted, on 10 July 1739 King George II authorized the Admiralty Board to seek maritime reprisals against Spain. On 20 July, Vice Admiral Edward Vernon and a fleet of warships departed Britain, bound for the West Indies, to attack Spanish ships and "possessions". War was not declared against Spain until Saturday, 23 October 1739 (Old Style), one day after the attack on La Guaira.
After arriving at the island of Antigua in early October 1739, Vice Admiral Edward Vernon sent three ships under the command of Captain Thomas Waterhouse to intercept Spanish merchant ships that made the route between La Guaira and Porto Bello. Waterhouse spotted several small vessels in the port of La Guaira and decided to attack, implementing a rudimentary plan. He lowered the British flag on his ships and flew the Spanish flag, to enter the port and once there use his ships to assault the fort. The governor of the area, Brigadier Don Gabriel José de Zuloaga had prepared the port defenses very diligently, and Spanish troops were well commanded by Captain Don Francisco Saucedo. On October 22, Waterhouse entered the port of La Guaira flying the Spanish flag. Expecting attack, the port gunners were not deceived by his ruse; they waited until the English squadron was within range, and simultaneously opened fire. After three hours of heavy shelling, Waterhouse ordered a withdrawal. The battered British squadron sailed to Jamaica to undertake emergency repairs. Trying later to explain his actions, Waterhouse argued that the capture of a few small Spanish vessels would not have justified the loss of his men.
Capture of Porto Bello (20–22 November 1739).
One of the first major actions was the British capture, on 22 November 1739, of Porto Bello, a silver-exporting town on the coast of Panama; this was intended to damage Spain's finances and weaken its naval capabilities. The poorly defended port was attacked by six ships of the line under Vice Admiral Edward Vernon who captured it within twenty-four hours. The British occupied the town for three weeks before withdrawing, having destroyed its fortifications, port and warehouses.
As a result, the Spanish changed their trading practices. Rather than trading at centralised ports with a few large treasure fleets, they began using a larger number of smaller convoys trading at a wide variety of ports. They also began to travel around Cape Horn to trade on the west coast. Porto Bello's economy was so damaged that it did not recover until the building of the Panama Canal nearly two centuries later.
In Britain, the victory was greeted with much celebration. In 1740, at a dinner in honour of Vernon in London, the song "Rule Britannia" was performed in public for the first time. Portobello Road in London is named after this victory. More medals were awarded to participants than for any other event in the eighteenth century. The conquest of a port in Spain's American empire was widely considered a foregone conclusion by many Patriot Whigs and opposition Tories, who pressed a reluctant Walpole to launch larger naval expeditions to the Gulf of Mexico.
First attack on Cartagena de Indias (13–20 March 1740).
Following the success of Portobelo, Vernon decided to focus his efforts on the capture of Cartagena de Indias in present-day Colombia. Both Vernon and Edward Trelawny, governor of Jamaica, considered the Spanish gold shipping port to be a prime objective. Since the outbreak of the war, and Vernon's arrival in the Caribbean, the British had made a concerted effort to gain intelligence on the defences of Cartagena. In October 1739, Vernon sent First Lieutenant Percival to deliver a letter to Blas de Lezo and Don Pedro Hidalgo, governor of Cartagena. Percival was to use the opportunity to make a detailed study of the Spanish defences. This effort was thwarted when Percival was denied entry to the port.
On 7 March 1740, in a more direct approach, Vernon undertook a reconnaissance-in-force of the Spanish city. Vernon left Port Royal in command of a squadron including ships of the line, two fire ships, three bomb vessels, and transport ships. Reaching Cartagena on 13 March, Vernon immediately landed several men to map the topography and to reconnoitre the Spanish squadron anchored in Playa Grande, west of Cartagena. Having not seen any reaction from the Spanish, on March 18 Vernon ordered the three bomb vessels to open fire on the city. Vernon intended to provoke a response that might give him a better idea of the defensive capabilities of the Spanish. Understanding Vernon's motives, Lezo did not immediately respond. Instead, Lezo ordered the removal of guns from some of his ships, in order to form a temporary shore battery for the purpose of suppressive fire. Vernon next initiated an amphibious assault, but in the face of strong resistance, the attempt to land 400 soldiers was unsuccessful. The British then undertook a three-day naval bombardment of the city. In total, the campaign lasted 21 days. Vernon then withdrew his forces, leaving HMS "Windsor Castle" and HMS "Greenwich" in the vicinity, with a mission to intercept any Spanish ship that might approach.
Destruction of the fortress of San Lorenzo el Real Chagres (22–24 March 1740).
After the destruction of Portobelo the previous November, Vernon proceeded to remove the last Spanish stronghold in the area. He attacked the fortress of San Lorenzo el Real Chagres, in present-day Panama on the banks of the Chagres River, near Portobelo. The fort was defended by Spanish patrol boats, and was armed with four guns and about thirty soldiers under Captain of Infantry Don Juan Carlos Gutiérrez Cevallos.
At 3 pm on 22 March 1740, the English squadron, composed of the ships "Stafford", "Norwich", "Falmouth" and "Princess Louisa", the frigate "Diamond", the bomb vessels "Alderney", "Terrible", and "Cumberland", the fireships "Success" and "Eleanor", and transports "Goodly" and "Pompey", under command of Vernon, began to bombard the Spanish fortress. Given the overwhelming superiority of the British forces, Captain Cevallos surrendered the fort on 24 March, after resisting for two days.
Following the strategy previously applied at Porto Bello, the British destroyed the fort, and seized the guns along with two Spanish patrol boats.
During this time of British victories along the Caribbean coast, events taking place in Spain would prove to have a significant effect on the outcome of the largest engagement of the war. Spain had decided to replace Don Pedro Hidalgo as governor of Cartagena de Indias. But, the new governor-designate, Lieutenant General of the Royal Armies Sebastián de Eslava y Lazaga had first to dodge the Royal Navy in order to get to his new post. Starting from the Galician port of Ferrol, the vessels "Galicia" and "San Carlos" set out on the journey. Hearing the news, Vernon immediately sent four ships to intercept the Spanish. They were unsuccessful in their mission. The Spanish managed to circumvent the British interceptors and entered the port of Cartagena on 21 April 1740, landing there with the new governor and several hundred veteran soldiers.
Second attack on Cartagena de Indias (3 May 1740).
In May, Vernon returned to Cartagena de Indias in charge of 13 warships, with the intention of bombarding the city. Lezo reacted by deploying his six ships of the line so that the British fleet was forced into ranges where they could only make short or long shots that were of little value. Vernon withdrew, asserting that the attack was merely a manoeuver. The main consequence of this action was to help the Spanish test their defences.
Third attack on Cartagena de Indias (13 March – 20 May 1741).
The largest action of the war was a major amphibious attack launched by the British under Admiral Edward Vernon in March 1741 against Cartagena de Indias, one of Spain's principal gold-trading ports in their colony of New Granada (today Colombia). Vernon's expedition was hampered by inefficient organisation, his rivalry with the commander of his land forces, and the logistical problems of mounting and maintaining a major trans-Atlantic expedition. The strong fortifications in Cartagena and the able strategy of Spanish Commander Blas de Lezo were decisive in repelling the attack. Heavy losses on the British side were due in large part to virulent tropical diseases, primarily an outbreak of yellow fever, which took more lives than those lost in battle.
The extreme ease with which the British destroyed Porto Bello (which did not regain its importance until the construction of the Panama Canal) led to a change in British plans. Instead of Vernon concentrating his next attack on Havana as expected, in order to conquer Cuba, he planned to attack Cartagena de Indias. Located in Colombia, it was the main port of the Viceroyalty and main point of the West Indian fleet for sailing to the Iberian Peninsula. In preparation the British gathered in Jamaica one of the largest fleets ever assembled. It consisted of 186 ships (60 more than the famous Spanish Armada of Philip II), bearing 2,620 artillery pieces and more than 27,000 men. Of that number, 10,000 were soldiers responsible for initiating the assault. There were also 12,600 sailors, 1,000 Jamaican slaves and macheteros, and 4,000 recruits from Virginia. The latter were led by Lawrence Washington, the older half-brother of George Washington, future President of the United States.
Colonial officials assigned Admiral Blas de Lezo to defend the fortified city. He was a marine veteran hardened by numerous naval battles in Europe, beginning with the War of Spanish Succession, and by confrontations with European pirates in the Caribbean Sea and Pacific Ocean, and Barbary pirates in the Mediterranean Sea. Assisting in that effort were Melchor de Navarrete and Carlos Desnaux, with a squadron of six ships of the line (the flagship vessel "Galicia" together with the "San Felipe," "San Carlos," "África," "Dragón," and "Conquistador") and a force of 3,000 soldiers, 600 militia and a group of native Indian archers.
Vernon ordered his forces to clear the port of all scuttled ships. On 13 March 1741, he landed a contingent of troops under command of Major General Thomas Wentworth and artillery to take Fort de San Luis de Bocachica. In support of that action, the British ships simultaneously opened with cannon fire, at a rate of 62 shots per hour. In turn, Lezo ordered four of the Spanish ships to aid 500 of his troops defending Desnaux's position, but the Spanish eventually had to retire to the city. Civilians were already evacuating it. After leaving Fort Bocagrande, the Spanish regrouped at Fort San Felipe de Barajas, while Washington's Virginians took up positions in the nearby hill of La Popa. Vernon, believing the victory at hand, sent a message to Jamaica stating that he had taken the city. The report was subsequently forwarded to London, where there was much celebration. Commemorative medals were minted, depicting the defeated Spanish defenders kneeling before Vernon (). The robust image of the enemy depicted in the British medals bore little resemblance to Admiral Lezo. Maimed by years of battle, he was one-eyed and lame, with limited use of one hand.
On the evening of 19 April, the British mounted an assault in force upon Castillo San Felipe de Barajas. Three columns of grenadiers, supported by Jamaicans and several British companies, moved under cover of darkness, with the aid of an intense naval bombardment. The British fought their way to the base of the fort's ramparts where they discovered that the Spanish had dug deep trenches. This effectively rendered the British scaling equipment too short for the task. The British advance was stymied since the fort's walls had not been breached, and the ramparts could not be topped. Neither could the British easily withdraw in the face of intense Spanish fire and under the weight of their own equipment. The Spanish seized on this opportunity, with devastating effect.
Reversing the tide of battle, the Spanish initiated a fixed bayonet charge at first light, inflicting heavy casualties on the British. The surviving British forces retreated to the safety of their ships. The British maintained a naval bombardment, sinking what remained of the small Spanish squadron (after Lezo's decision to scuttle some of his ships in an effort to block the harbor entrance). The Spanish thwarted any British attempt to land another ground assault force. The British troops were forced to remain aboard ship for a month, without sufficient reserves. With supplies running low, and with the outbreak of disease (primarily yellow fever), which took the lives of many on the crowded ships, Vernon was forced to raise the siege on 9 May and return to Jamaica. Six thousand British died. Spanish casualties are listed as just under one thousand dead.
Vernon carried on, successfully attacking the Spanish at Guantanamo Bay, Cuba. On 5 March 1742, with the help of reinforcements from Europe, he launched an assault on Panama City, Panama, hoping to repeat the success of Portobelo. That attack was not completely successful. In 1742, Vernon was replaced by Admiral of the Fleet Chaloner Ogle and returned to England, where he gave an accounting to the Admiralty. He learned that he had been elected MP for Ipswich. Vernon maintained his naval career for another four years before retiring in 1746. In an active Parliamentary career, Vernon advocated an improvement in naval procedures. He continued to hold an interest in naval affairs until his death in 1757.
News of the defeat at Cartagena was a significant factor in the downfall of the British Prime Minister Robert Walpole. Walpole's anti-war views were considered by the Opposition to have contributed to his poor prosecution of the war effort. The new government under Lord Wilmington wanted to shift the focus of Britain's war effort away from the Americas and into Mediterranean. Spanish policy, dictated by Elisabeth of Parma, also shifted to a European focus, to recover lost Spanish possessions in Italy from the Austrians. In 1742, a large British fleet under Nicholas Haddock was sent to try and intercept a Spanish army being transported from Barcelona to Italy, which he failed to do having only 10 ships. With the arrival of additional ships from Britain in February 1742, Haddock successfully blockaded the Spanish coast failing to force the Spanish fleet into an action.
Lawrence Washington survived the yellow fever outbreak, and eventually retired to Virginia. He named his estate Mount Vernon, in honour of his former commander.
Anson expedition.
The success of the Porto Bello operation led the British, in September 1740, to send a squadron under Commodore George Anson to attack Spain's possessions in the Pacific. Before they reached the Pacific, numerous men had died from disease, and they were in no shape to launch any sort of attack. Anson reassembled his force in the Juan Fernández Islands, allowing them to recuperate before he moved up the Chilean coast, raiding the small town of Paita. He reached Acapulco too late to intercept the yearly Manila galleon, which had been one of the principal objectives of the expedition. He retreated across the Pacific, running into a storm that forced him to dock for repairs in Canton. After this he tried again the following year to intercept the Manila galleon. He accomplished this on 20 June 1743 off Cape Espiritu Santo, capturing more than a million gold coins.
Anson sailed home, arriving in London more than three and a half years after he had set out, having circumnavigated the globe in the process. Less than a tenth of his forces had survived the expedition. Anson's achievements helped establish his name and wealth in Britain, leading to his appointment as First Lord of the Admiralty.
Florida.
In 1740, the inhabitants of Georgia launched an overland attack on the fortified city of St. Augustine in Florida, supported by a British naval blockade, but were repelled. The British forces led by James Oglethorpe, the Governor of Georgia, besieged St. Augustine for over a month before retreating, and abandoned their artillery in the process. The failure of the Royal Navy blockade to prevent supplies reaching the settlement was a crucial factor in the collapse of the siege. Oglethorpe began preparing Georgia for an expected Spanish assault.
French neutrality.
When war broke out in 1739, both Britain and Spain expected that France would join the war on the Spanish side. This played a large role in the tactical calculations of the British. If the Spanish and French were to operate together, they would have a superiority of ninety ships of the line. In 1740, there was an invasion scare when it was believed that a French fleet at Brest and a Spanish fleet at Ferrol were about to combine and launch an invasion of England. Although this proved not to be the case, the British kept the bulk of their naval and land forces in southern England to act as a deterrent.
Many in the British government were afraid to launch a major offensive against the Spanish, for fear that a major British victory would draw France into the war to protect the balance of power.
Raids on Santiago de Cuba, La Guaira and Puerto Cabello.
The British attacked several locations in the Caribbean with little consequence to the geopolitical situation in the Atlantic. The weakened British forces under Vernon launched an attack against Cuba, landing in Guantánamo Bay with a plan to march the 45 miles to Santiago de Cuba and capture the city. Vernon clashed with the army commander, and the expedition withdrew when faced with heavier Spanish opposition than expected. Vernon remained in the Caribbean until October 1742, before heading back to Britain; he was replaced by Chaloner Ogle, who took command of a sickly fleet. Less than half the sailors were fit for duty. The following year, a smaller force led by Charles Knowles raided the Venezuelan coast, attacking La Guaira on 2 March 1743. Having suffered 97 killed and 308 wounded over three days, Knowles decided to retire west before sunrise on 6 March. He attacked nearby Puerto Cabello. Despite his orders to rendezvous at Borburata Keys—4 mi east of Puerto Cabello—captains of the detached "Burford", "Norwich", "Assistance", and "Otter" proceeded to Curaçao. The commodore angrily followed them in. On 28 March, he sent his smaller ships to cruise off Puerto Cabello, and once his main body had been refitted, went to sea again on 31 March. He struggled against contrary winds and currents for two weeks before finally diverting to the eastern tip of Santo Domingo by 19 April.
Invasion of Georgia.
In 1742, the Spanish launched an attempt to seize the British colony of Georgia. Manuel de Montiano commanded 2,000 troops, who were landed on St Simons Island off the coast. General Oglethorpe rallied the local forces and defeated the Spanish regulars at Bloody Marsh and Gully Hole Creek, forcing them to withdraw. Border clashes between the colonies of Florida and Georgia continued for the next few years, but neither Spain nor Britain undertook offensive operations on the North American mainland.
Merger with wider war.
By mid-1742, the War of the Austrian Succession had broken out in Europe. Principally fought by Prussia and Austria over possession of Silesia, the war soon engulfed most of the major powers of Europe, who joined two competing alliances. The scale of this new war dwarfed any of the fighting in the Americas, and drew Britain and Spain's attention back to operations on the European continent. The return of Vernon's fleet in 1742 marked the end of major offensive operations in the War of Jenkins' Ear. France entered the war in 1744, emphasizing the European theatre and planning an ambitious invasion of Britain. While it ultimately failed, the threat persuaded British policy makers of the dangers of sending significant forces to the Americas which might be needed at home.
Britain did not attempt any additional attacks on Spanish possessions. In 1745, William Pepperrell of New England led a colonial expedition, supported by a British fleet under Commodore Peter Warren, against the French fortress of Louisbourg on Cape Breton Island off Canada. Pepperrell was knighted for his achievement, but Britain returned Louisbourg to the French by the Treaty of Aix-La-Chapelle in 1748. A decade later, during the Seven Years' War (known as the French and Indian War in the North American theater), British forces under Lord Jeffrey Amherst and General Wolfe recaptured it in 1758.
Privateering.
The war involved privateering by both sides. Anson captured a valuable Manila galleon, but this was more than offset by the numerous Spanish privateering attacks on British shipping along the transatlantic triangular trade route. They seized hundreds of British ships, looting their goods and slaves, and operated with virtual impunity in the West Indies; they were also active in European waters. The Spanish convoys proved almost unstoppable. During the Austrian phase of the war, the British fleet attacked poorly protected French merchantmen instead.
Lisbon negotiations.
From August 1746, negotiations began in the city of Lisbon, in neutral Portugal, to try to arrange a peace settlement. The death of Philip V of Spain had brought his son Ferdinand VI to the throne, and he was more willing to be conciliatory over the issues of trade. However, because of their commitments to their Austrian allies, the British were unable to agree to Spanish demands for territory in Italy and talks broke down.
Aftermath.
The eventual diplomatic resolution formed part of the wider settlement of the War of the Austrian Succession by the Treaty of Aix-la-Chapelle. The issue of the "asiento" was not mentioned in the treaty, as it had lessened in importance to both nations. The issue was finally settled by the 1750 Treaty of Madrid in which Britain agreed to renounce its claim to the asiento in exchange for a payment of £100,000. It allowed British trade with Spanish America under favourable conditions.
Relations between Britain and Spain dramatically improved during subsequent years thanks to a concerted effort by the Duke of Newcastle to cultivate Spain as an ally. A succession of Anglophile ministers were appointed in Spain, including José de Carvajal and Ricardo Wall, all of whom were on good terms with the British Ambassador Benjamin Keene, in an effort to avoid a repeat of hostilities. As a result, Spain remained neutral during the early part of the Seven Years' War between Britain and France.
The Spanish Empire in the Caribbean remained intact. Spain later used its trading routes and resources to help the rebels' cause in the American Revolution of the late 18th century.
The War of Jenkins' Ear is commemorated annually on the last Saturday in May at Wormsloe Plantation in Savannah, Georgia.
References.
</dl>

</doc>
<doc id="51231" url="http://en.wikipedia.org/wiki?curid=51231" title="Wiltshire">
Wiltshire

Wilshire
Wiltshire ( or ) is a county in South West England with an area of 3485 km2. It is landlocked and borders the counties of Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire and Berkshire. Until 1930 the county town was Wilton but Wiltshire Council is now based at Trowbridge.
Wiltshire is characterised by its high downland and wide valleys. Salisbury Plain is famous as the location of the Stonehenge and Avebury stone circles and other ancient landmarks and as a training area for the British Army. The city of Salisbury is notable for its mediaeval cathedral. Important country houses open to the public include Longleat, near Warminster, and the National Trust's Stourhead, near Mere.
Toponymy.
The county, in the 9th century written as "Wiltunscir", later "Wiltonshire", is named after the former county town of Wilton.
History.
Wiltshire is notable for its pre-Roman archaeology. The Mesolithic, Neolithic and Bronze Age people that occupied southern Britain built settlements on the hills and downland that cover Wiltshire. Stonehenge and Avebury are perhaps the most famous Neolithic sites in the UK.
In the 6th and 7th centuries Wiltshire was at the western edge of Saxon Britain, as Cranborne Chase and the Somerset Levels prevented the advance to the west. The Battle of Bedwyn was fought in 675 between Escuin, a West Saxon nobleman who had seized the throne of Queen Saxburga, and King Wulfhere of Mercia. In 878 the Danes invaded the county. Following the Norman Conquest, large areas of the country came into the possession of the crown and the church.
At the time of the Domesday Survey the industry of Wiltshire was largely agricultural; 390 mills are mentioned, and vineyards at Tollard and Lacock. In the succeeding centuries sheep-farming was vigorously pursued, and the Cistercian monasteries of Kingswood and Stanley exported wool to the Florentine and Flemish markets in the 13th and 14th centuries.
In the 17th century English Civil War Wiltshire was largely Parliamentarian. The Battle of Roundway Down, a decisive Royalist victory, was fought near Devizes.
In 1794 it was decided at a meeting at the Bear Inn in Devizes to raise a body of ten independent troops of Yeomanry for the county of Wiltshire, which formed the basis for what would become the Royal Wiltshire Yeomanry, who served with distinction both at home and abroad, during the Boer War, World War I and World War II. The Royal Wiltshire Yeomanry currently lives on as A (RWY) Squadron of the Royal Yeomanry, based in Swindon, and B (RWY) Squadron of the Royal Wessex Yeomanry, based in Salisbury.
Around 1800 the Kennet and Avon Canal was built through Wiltshire providing a route for transporting cargoes from Bristol to London until the development of the Great Western Railway.
Information on the 261 civil parishes of Wiltshire is available on the website, run by the and services of This site includes maps, demographic data, historic and modern pictures and short histories.
The Moonrakers.
The local nickname for Wiltshire natives is "moonrakers." This originated from a story of smugglers who managed to foil the local Excise men by hiding their alcohol, possibly French brandy in barrels or kegs, in a village pond. When confronted by the excise men they raked the surface to conceal the submerged contraband with ripples, and claimed that they were trying to rake in a large round cheese visible in the pond, really a reflection of the full moon. The officials took them for simple yokels or mad and left them alone, allowing them to continue with their illegal activities. Many villages claim the tale for their own village pond, but the story is most commonly linked with The Crammer in Devizes.
Geology, landscape and ecology.
Two thirds of Wiltshire, a mostly rural county, lies on chalk, a kind of soft, white, porous limestone that is resistant to erosion, giving it a high chalk downland landscape. This chalk is part of a system of chalk downlands throughout eastern and southern England formed by the rocks of the Chalk Group and stretching from the Dorset Downs in the west to Dover in the east. The largest area of chalk in Wiltshire is Salisbury Plain, which is used mainly for arable agriculture and by the British Army as training ranges. The highest point in the county is the Tan Hill–Milk Hill ridge in the Pewsey Vale, just to the north of Salisbury Plain, at 295 m above sea level.
The chalk uplands run northeast into West Berkshire in the Marlborough Downs ridge, and southwest into Dorset as Cranborne Chase. Cranborne Chase, which straddles the border, has, like Salisbury Plain, yielded much Stone Age and Bronze Age archaeology. The Marlborough Downs are part of the North Wessex Downs AONB (Area of Outstanding Natural Beauty), a 1730 sqkm conservation area.
In the northwest of the county, on the border with South Gloucestershire and Bath and North East Somerset, the underlying rock is the resistant oolite limestone of the Cotswolds. Part of the Cotswolds AONB is also in Wiltshire, in the county's northwestern corner.
Between the areas of chalk and limestone downland are clay valleys and vales. The largest of these vales is the Avon Vale. The Avon cuts diagonally through the north of the county, flowing through Bradford on Avon and into Bath and Bristol. The Vale of Pewsey has been cut through the chalk into Greensand and Oxford Clay in the centre of the county. In the south west of the county is the Vale of Wardour. The southeast of the county lies on the sandy soils of the northernmost area of the New Forest.
Chalk is a porous rock so the chalk hills have little surface water. The main settlements in the county are therefore situated at wet points. Notably, Salisbury is situated between the chalk of Salisbury Plain and marshy flood plains.
Climate.
Along with the rest of South West England, Wiltshire has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately 10 °C. Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea temperatures. The summer months of July and August are the warmest with mean daily maxima of approximately 21 °C. In winter mean minimum temperatures of 1 °C or 2 °C are common. In the summer the Azores high pressure affects the south-west of England, however convective cloud sometimes forms inland, reducing the number of hours of sunshine. Annual sunshine rates are slightly less than the regional average of 1,600 hours. In December 1998 there were 20 days without sun recorded at Yeovilton. Most the rainfall in the south-west is caused by Atlantic depressions or by convection. Most of the rainfall in autumn and winter is caused by the Atlantic depressions, which is when they are most active. In summer, a large proportion of the rainfall is caused by sun heating the ground leading to convection and to showers and thunderstorms. Average rainfall is around 700 mm. About 8–15 days of snowfall is typical. November to March have the highest mean wind speeds, and June to August have the lightest winds. The predominant wind direction is from the south-west.
Economy.
This is a chart of trend of regional gross value added (GVA) of Wiltshire at current basic prices with figures in millions of British Pounds Sterling.
The Wiltshire economy benefits from the "M4 corridor effect", which attracts business, and the attractiveness of its countryside, towns and villages. The northern part of the county is richer than the southern part, particularly since Swindon is home to national and international corporations such as Honda, Intel, Motorola, Alcatel-Lucent, Patheon, Catalent (formerly known as Cardinal Health), Becton-Dickinson, WHSmith, Early Learning Centre and Nationwide, with Dyson located in nearby Malmesbury. Wiltshire’s employment structure is distinctive in having a significantly higher number of people in various forms of manufacturing (especially electrical equipment and apparatus, food products, and beverages, furniture, rubber, pharmaceuticals, and plastic goods) than the national average.
In addition, there is higher than average employment in public administration and defence, due to the military establishments around the county, particularly around Amesbury and Corsham. There are sizeable British Army barracks at Tidworth, Bulford and Warminster. Further north RAF Lyneham was home to the RAF's Hercules C130 fleet until 2011. RAF operations ceased in late-2012 and a new Defence Training Centre is currently being developed there. Wiltshire is also distinctive in having a high proportion of its working age population who are economically active – (86.6% in 1999–2000), and its low unemployment rates. The gross domestic product (GDP) level in Wiltshire did not reach the UK average in 1998, and was only marginally above the rate for South West England.
Government and infrastructure.
Service Children's Education has its headquarters in Trenchard Lines in Upavon, Wiltshire.
Education.
Wiltshire has twenty-nine county secondary schools, publicly funded, of which the largest is Warminster Kingdown, and another thirteen independent secondaries, including Marlborough College, St Mary's Calne, Dauntsey's near Devizes, and Warminster School. The county schools are nearly all comprehensives, with the older pattern of education surviving only in Salisbury, which has two grammar schools (South Wilts Grammar School for Girls and Bishop Wordsworth's School) and three secondary moderns. All but two of the county secondary schools in the former districts of West Wiltshire and North Wiltshire have Sixth forms, but only half of those in the rest of the county.
There are three further education colleges, New College, Swindon, Wiltshire College and Swindon College, providing some higher education.
As yet there are no universities within Wiltshire, except that Bath Spa University has a centre at Corsham Court in Corsham, and Oxford Brookes University maintains a minor campus in Swindon (almost 50 km from Oxford). Outline plans for a projected University of Swindon or University of Wiltshire were announced by the Borough of Swindon in November 2008, but the scheme remains uncommitted. Swindon is the UK's largest centre of population without its own university. The closest university to Wiltshire's county town of Trowbridge is the University of Bath. Wiltshire is therefore one of the few remaining English counties without a university or university college.
Demographics.
The county registered a population of 613,024 in the Census 2001. The population density is low at 178 PD/km2. In 1991 there were 230,109 dwellings in the county. In 1991 98.3% of the population was indigenous and 17.9% of the population were over 65.
Population of Wiltshire:
Politics and administration.
Council.
The ceremonial county of Wiltshire consists of two unitary authority areas, Wiltshire and Swindon, governed respectively by Wiltshire Council and Swindon Borough Council.
Until the 2009 structural changes to local government in England, Wiltshire (apart from Swindon) was a two-level county, divided into four local government districts, Kennet, North Wiltshire, Salisbury and West Wiltshire, which existed alongside Wiltshire County Council, covering the same area and carrying out more strategic tasks, such as education and county roads. However, on 1 April 2009 these five local authorities were merged into a single unitary authority called Wiltshire Council. With the abolition of the District of Salisbury, a new Salisbury City Council was created at the same time to carry out several city-wide functions and to hold the City's charter.
As a result of elections held in 2009, Wiltshire Council comprises 61 Conservatives, 24 Liberal Democrats, eight Independents, three Devizes Guardians and two Labour members. The council is led by Jane Scott (Conservative), who had previously led the former Wiltshire County Council since 2003.
Sport.
The county is represented in the Football League by Swindon Town, who play at the County Ground stadium near Swindon town centre. They joined the Football League on the creation of the Third Division in 1920, and have remained in the league ever since. Their most notable achievements include winning the Football League Cup in 1969 and the Anglo-Italian Cup in 1970, two successive promotions in 1986 and 1987 (taking them from the Fourth Division to the Second), promotion to the Premier League as Division One play-off winners in 1993 (as inaugural members), the Division Two title in 1996, and their promotion to League One in 2007 after finishing third in League Two.
Salisbury City have recently returned to the Football Conference for the 2012/13 season, after entering administration in the Summer of 2009 and dropping down to the Southern League Premier Division due to a breach of Conference rules.
Wiltshire County Cricket Club play in the Minor Counties league. They predominantly play at Trowbridge Cricket Club Ground but also play in Corsham and Salisbury.
Swindon Robins Speedway team, who compete in the Sky Elite League, have been at their track at the Blunsdon Abbey Stadium since 1949.
Principal settlements.
Wiltshire has twenty-one towns and one city:
A list of settlements is at List of places in Wiltshire.
Transport.
Road.
Roads running through Wiltshire include The Ridgeway, an ancient route, and Roman roads the Fosse Way, London to Bath road and Ermin Way. National Cycle Route 4 and the Thames Path, a modern long distance footpath, run through the county.
Routes through Wiltshire include:
Rail.
Three main railway routes, all of which carry passenger traffic, cross Wiltshire.
Other routes include:
The major junction stations are Salisbury and Westbury, and important junctions are also found at Swindon, Chippenham and Trowbridge.
There is also the Swindon and Cricklade Railway in the Thames Valley.
Air.
Airfields in Wiltshire include Old Sarum Airfield, Clench Common Airfield and Redlands Airfield. RAF Lyneham was an air transport hub for British forces until its closure in 2012. Airports for scheduled airlines near Wiltshire include Bristol Airport, Bristol Filton Airport, Gloucestershire Airport, London Oxford Airport, London Heathrow Airport and Southampton Airport.

</doc>
<doc id="51232" url="http://en.wikipedia.org/wiki?curid=51232" title="Single-occupant vehicle">
Single-occupant vehicle

A single-occupant vehicle (SOV) is a privately operated vehicle whose only occupant is the driver. The drivers of SOVs use their vehicles primarily for personal travel, daily commuting and for running errands. The types of vehicles include, but are not limited to sport utility vehicles (SUVs), light-duty trucks, and any combination thereof, along with all the various van and car sizes, but would generally be taken to exclude human-powered vehicles such as bicycles. This term is used by transportation engineers and planners. SOVs contrast with high-occupancy vehicles (HOV), which carry many passengers.

</doc>
<doc id="51239" url="http://en.wikipedia.org/wiki?curid=51239" title="Quercus kelloggii">
Quercus kelloggii

Quercus kelloggii, the California black oak, also known as simply black oak, or Kellogg oak, is an oak in the red oak section ("Quercus" sect. "Lobatae"), native to western North America. It is a close relative of the black oak ("Quercus velutina") found in eastern and central North America.
Distribution and habitat.
California black oak is a deciduous tree growing in mixed evergreen forests, oak woodlands, and Coniferous forests. California black oak is distributed along foothills and lower mountains of California and southern Oregon. 
It is found from Lane County, Oregon, south through the Cascade Range, the Sierra Nevada, and the Coast, Transverse, and Peninsular ranges to San Diego County, California and into Baja California. The tree occurs in pure or mixed stands. Pure stands usually indicate sites unfavorable to conifer growth or recurring disturbance such as fire or logging activities. The tree can grow in many types of soils, but it is important that the soil be well-drained.
Description.
"Quercus kelloggii" typically grows from 9–25 m (30–80 ft) in height and from 0.3-1.4 m (1-4.5 ft) in diameter. Large trees may exceed 36 m (120 ft) in height and 1.6 m (5 ft) diameter. The species also grows in shrubby scrub-oak form on poor sites. In open areas the crown is broad and rounded, with lower branches nearly touching the ground or forming a browse line. In closed stands, the crown is narrow and slender in young trees and irregularly broad in old trees. Trunks are usually free of branches on the lower 6–12 m (20–40 ft) in closed stands. Trunks are often forked, and usually decayed and hollow in older trees. The bark is thin and smooth in young trees, becoming thick, ridged, and plate-like with age. This oak grows from one to several vertical roots which penetrate to bedrock, with large, laterally spreading roots extending off from vertical ones. It also has a number of surface roots. 
Acorns are relatively large in this species, from 2.5–3 cm (1-1.2 in) long and 1.5-1.8 cm (0.6-0.7 in) wide. The deeply lobed leaves are typically 10–20 cm (4–8 in) long. While individual trees generally have a lifespan between 100 and 200 years, California black oak can live up to 500 years of age.
The tree reproduces when its acorns sprout to form seedlings. It also reproduces vegetatively with new growth sprouting from the root crown after the tree is top-killed by wildfire, logging, frost, or other events.
Ecology.
The California black oak, "Quercus kelloggii", is a critical species for wildlife. Oaks ("Quercus" spp.) may be the single most important genus used by wildlife for food and cover in California forests and rangelands, and California black oak occupies more total area in California than any other hardwood species. Livestock also make heavy use of this species for food and cover.
Cavities in the trees provide den or nest sites for owls, various woodpeckers, tree squirrels, and American black bears. Trees provide valuable shade for livestock and wildlife during the hot summer months. California black oak forest types are heavily used for spring, summer, and fall cover by black bear.
It is browsed by mule deer and livestock. Acorns are heavily utilized by livestock, mule deer, feral pigs, rodents, mountain quail, Steller's jay, and woodpeckers. Acorns constitute an average of 50% of the fall and winter diets of western gray squirrel and black-tailed deer during good mast years. Fawn survival rates increase or decrease with the size of the acorn crop.
It is a preferred foraging substrate for many birds. All of 68 bird species observed in oak woodlands of the Tehachapi Mountains of California used California black oak for part of their foraging activities. Acorn woodpecker, Bullock's oriole, and Nashville warbler show strong preference for California black oak. The parasitic plant Pacific mistletoe ("Phoradendron villosum"), which commonly grows on this oak, produces berries which attract birds as well.
Many animals cache the acorns, and acorns that have been stored in the ground or otherwise buried are more likely to sprout than those that remain on the surface.
The tree is adapted to wildfire. It is protected from smaller fires by its thick bark. If it is top-killed and burned away in a larger fire, it easily resprouts and has a good supply of nutrients and water stored in its root system. Acorns sprout into seedlings after fire and sites that have been cleared of canopy and leaf litter in fires are ideal for seedling success. 
This oak is vulnerable to sudden oak death.
Uses.
California Native Americans preferred California black oak acorns over those of other species for making acorn meal. This acorn was a staple food for many Native American groups. Native Americans recognized the importance of fire to this oak, and purposely lit fires in oak woodlands to promote its health and ensure their food source.
The wood is used for making furniture, pallets, and construction timber. It is used as an ornamental tree.
Timber.
California black oak comprises a total volume of 29% of California's hardwood timber resources, and is the major hardwood sawn into lumber there. The total estimated area of species occurrence is 361,800 ha (3,618 km² or 894,000 acres); 239,200 ha (2,392 km² or 591,000 acres) of timberland and 122,600 ha (1,226 km² or 303,000 acres) of woodland. Of this land 60% is privately owned, 31% is in National Forests, and 9% is on other public lands. It has greatly decreased from its historic abundance. This is due to a number of factors, including drought, disease, animal foraging, logging practices, fire suppression, and a variety of other human impacts. Cutting green trees for fuelwood has contributed to the decline of this species, and illegal harvesting of green trees from public lands is a continuing problem.
It was long considered by foresters and government agencies to be a weed tree. In its earlier years, its only use was to feed the boilers of donkey engines bringing in the valuable pine and fir logs. There was a period in the mid 1960s when U.S. Forest Service policy in California's National Forests was systematic extermination of California black oak by girdling the trees. The objective was to make room for more coniferous growth. In the rush to utilize the pines, firs and redwoods, the dense hardwoods were looked on with contempt. Like a few other visionaries in the 1960s, Guy Hall thought the California black oak presented a beautiful challenge that deserved better than eradication. In 1965 Hall convinced federal agencies to cease their extermination polices.
Plantations of California black oak have been successfully established in clearcuts from acorn plantings. Thinning such stands promotes stand productivity and wood quality, and is recommended when trees are from 9–15 m (30–50 ft) tall or when stand density (basal area) exceeds 29 m²/ha (125 ft²/acre). This tree has also been managed for hardwood production by maintaining scattered pure stands within coniferous forests. Stands of this species will often establish on poorer sites, where conifer seedling establishment has not been successful.
Cultivation.
"Quercus kelloggii" is cultivated in the specialty horticulture trade as an ornamental tree for native plant, drought tolerant water conserving, and habitat gardens, and various types of municipal, commercial, and agency sustainable landscape and restoration projects.

</doc>
<doc id="51246" url="http://en.wikipedia.org/wiki?curid=51246" title="Chernyakhovsk">
Chernyakhovsk

Chernyakhovsk (Russian: Черняхо́вск); prior to 1946 known by its German name   (Lithuanian: "Įsrutis"; Polish: "Wystruć") is a town and the administrative center of Chernyakhovsky District in Kaliningrad Oblast, Russia, located at the confluence of the Instruch and Angrapa Rivers, forming the Pregolya. Population:  (2010 Census)
History.
It was founded in 1336, after the Prussian Crusade, when the Grand Master of the Teutonic Knights Dietrich von Altenburg built a castle called Instierburg at the site of a former Old Prussian fortification. During their campaign against the Grand Duchy of Lithuania, the place was devastated in 1376 and again by Polish troops in 1457. The castle had been rebuilt as the seat of a Procurator and a settlement grew up to serve it, also called Insterburg.
When Albert of Brandenburg-Ansbach in 1525 secularized the monastic State of the Teutonic Order, Insterburg became part of the Duchy of Prussia and was granted town privileges on October 10, 1583 by the Prussian regent Margrave George Frederick. The town became part of the Kingdom of Prussia in 1701. Because the area had been depopulated by plague in the early 18th century, King Frederick William I of Prussia invited Protestant refugees who had been expelled from the Archbishopric of Salzburg to settle in Insterburg in 1732.
In 1818, after the Napoleonic Wars, the town became the seat of Insterburg District within the Gumbinnen Region. Michael Andreas Barclay de Tolly died at Insterburg in 1818 on his way from his Livonian manor to Germany, where he wanted to renew his health.
Insterburg became a part of the German Empire during the 1871 unification of Germany. On May 1, 1901, it became an independent city separate from Insterburg District. After World War I, the town was separated from the rest of Weimar Germany, as the province of East Prussia had become an exclave. The association football club Yorck Boyen Insterburg was formed in 1921.
During World War II, Insterburg was heavily bombed by the British Royal Air Force on July 27, 1944. The town was stormed by Red Army troops on January 21–22, 1945. As part of the northern part of East Prussia, Insterburg was transferred from Germany to the Soviet Union after the war as previously agreed between the victorious powers at the Potsdam Conference. The German population was either evacuated or expelled and replaced with Russians. In 1946, Insterburg was renamed Chernyakhovsk in honor of the Soviet World War II General of the Army Ivan Chernyakhovsky, who commanded the army that first entered East Prussia in 1944.
After 1989, a group of people introduced the Akhal-Teke horse breed to the area and opened an Akhal-Teke breeding stable.
Administrative and municipal status.
Within the framework of administrative divisions, Chernyakhovsk serves as the administrative center of Chernyakhovsky District. As an administrative division, it is, together with five rural localities, incorporated within Chernyakhovsky District as the town of district significance of Chernyakhovsk. As a municipal division, the town of district significance of Chernyakhovsk is incorporated within Chernyakhovsky Municipal District as Chernyakhovskoye Urban Settlement.
Military.
Chernyakhovsk is home to the Chernyakhovsk naval air facility.
International relations.
Twin towns and sister cities.
Chernyakhovsk is twinned with:

</doc>
<doc id="51249" url="http://en.wikipedia.org/wiki?curid=51249" title="Industrial organization">
Industrial organization

In economics, industrial organization is a field that builds on the theory of the firm by examining the structure of (and, therefore, the boundaries between) firms and markets. Industrial organization adds real-world complications to the perfectly competitive model, complications such as transaction costs, limited information, and barriers to entry of new firms that may be associated with imperfect competition. It analyzes determinants of firm and market organization and behavior as between competition and monopoly, including from government actions.
There are different approaches to the subject. One approach is descriptive in providing an overview of industrial organization, such as measures of competition and the size-concentration of firms in an industry. A second approach uses microeconomic models to explain internal firm organization and market strategy, which includes internal research and development along with issues of internal reorganization and renewal. A third aspect is oriented to public policy as to economic regulation, antitrust law, and, more generally, the economic governance of law in defining property rights, enforcing contracts, and providing organizational infrastructure.
The subject has a theoretical side and a practical side. According to one textbook: "On one plane the field is abstract, a set of analytical concepts about competition and monopoly. On a second plane the topic is about real markets, teeming with the excitement and drama of struggles among real firms" (Shepherd, W.; 1985; 1).
The extensive use of game theory in industrial economics has led to the export of this tool to other branches of microeconomics, such as behavioral economics and corporate finance. Industrial organization has also had significant practical impacts on antitrust law and competition policy.
The development of industrial organization as a separate field owes much to Edward Chamberlin, Edward S. Mason, and particularly Joe S. Bain among others.
Assessments of the subject have differed over time. The preface to a related research volume in 1972 remarked on "Whither industrial organization?": "That all is not well with this in this once flourishing field is readily apparent." A response came 15 years later: "[T]oday's verdict is that industrial organization is alive and well and the queen of applied microeconomics."
Subareas.
The Journal of Economic Literature (JEL) classification codes are one way of representing the range of economics subjects and subareas. There, Industrial Organization, one of 20 primary categories, has 9 secondary categories, each with multiple tertiary categories. The secondary categories are listed below with corresponding available article-preview links of The New Palgrave Dictionary of Economics Online and footnotes to their respective JEL-tertiary categories and associated New-Palgrave links.
Market structures.
The common market structures studied in this field are the following:
Areas of study.
Industrial organization investigates the outcomes of these market structures in environments with
History of the field.
A 2009 book "Pioneers of Industrial Organization" traces the development of the field from Adam Smith to recent times and includes dozens of short biographies of major figures in Europe and North America who contributed to the growth and development of the discipline.
Other reviews by publication year and earliest available cited works those in 1970/1937, 1972/1933, 1974, 1987/7 from 1968 on, 3 from 1937 to 1956, 2009/c. 1900., and 2010/1951.

</doc>
<doc id="51250" url="http://en.wikipedia.org/wiki?curid=51250" title="Wojciech Jaruzelski">
Wojciech Jaruzelski

Wojciech Witold Jaruzelski (]; 6 July 1923 – 25 May 2014) was a Polish military officer and communist politician. He was First Secretary of the Polish United Workers' Party from 1981 to 1989, and as such was the last leader of the Communist People's Republic of Poland. He also served as Prime Minister from 1981 to 1985 and the country's head of state from 1985 to 1990. He was also the last commander-in-chief of the Polish People's Army (LWP). He resigned from power after the Polish Round Table Agreement in 1989, which led to democratic elections in Poland.
Jaruzelski was chiefly responsible for the imposition of martial law in Poland on 13 December 1981 in an attempt to crush the pro-democracy movements, which included Solidarity, the first non-Communist trade union in Warsaw Pact history. Subsequent years saw his communist government and its internal security forces censor, persecute, and jail thousands of journalists and opposition activists without charge; others lost their lives during these same events. The resulting socio-economic crisis led to the rationing of basic foods such as sugar, milk, and meat, as well as materials such as gasoline and consumer products, while the median income of the population fell by as much as 40 percent. During Jaruzelski's rule from 1981 to 1989, around 700,000 people left the country.
Early life.
Wojciech Witold Jaruzelski was born on 6 July 1923 in Kurów, into a family of Polish gentry. He was the son of Wanda (née Zaremba) and Władysław Mieczysław Jaruzelski, and was raised on the family estate near Wysokie (in the vicinity of Białystok). He was educated in a Catholic school during the 1930s. World War II commenced on 1 September 1939 with the Invasion of Poland by Germany, with the latter aided by the Soviet invasion of Poland sixteen days later. These resulted in the defeat of Poland by October, and her partition between Soviet and German control. Jaruzelski and his family fled to Lithuania and stayed with some friends there. However, a few months later, after Lithuania and the other Baltic states were forcibly incorporated into the Soviet Union, Jaruzelski and his family were captured by the Red Army and deported to Siberia. In 1940 at the age of sixteen, Jaruzelski was sent to the Kazakh Soviet Socialist Republic, where he performed forced labour in the Karaganda coal mines. During his labour work he was stricken with snow blindness and suffered permanent damage to his eyes as well as his back. His eye condition forced him to wear dark sunglasses most of the time for the rest of his life, which became his trademark. Jaruzelski's father died in 1942 from dysentery. His mother and sister survived the war (his mother died in 1966).
Military career.
Jaruzelski was selected for enrollment into the Soviet Officer Training School by the Soviet authorities. During his time in the Kazakh Republic, Jaruzelski wanted to join the non-Soviet controlled Polish exile army led by Władysław Anders, but in 1943, by which time the Soviet Union was fighting in Europe against Germany in the Eastern Front, he joined the Polish army units being formed under Soviet command. He served in this Soviet-sponsored First Polish Army during the war. He participated in the 1945 Soviet military takeover of Warsaw and the Battle of Berlin. By the time the war ended that year, he had gained the rank of lieutenant. He "further credited himself in Soviet eyes" by engaging in combat against the non-communist Polish Home Army, from 1945 to 1947. After the end of the war, Jaruzelski graduated from the Polish Higher Infantry School, followed by graduation from the General Staff Academy. He joined Poland's communist party, the Polish United Workers' Party, in 1948 and became an informant for the Soviet supervised Main Directorate of Information of the Polish Army using the cover name Wolski. In the initial post-war years, he was among those who fought the Polish anti-communists ("cursed soldiers") in the Świętokrzyskie region. A BBC News profile of Jaruzelski states that his career "took off after the departure [from Poland] in 1956 of the Soviet Field Marshal, Konstantin Rokossovsky", who had been Poland's Commander in Chief and Minister of Defence. Jaruzelski became the chief political officer of the Polish armed forces in 1960, its chief of staff in 1964; and Polish Minister of Defense in 1968, four years after he was elected to be a member of the Central Committee of the Polish United Workers' Party. He participated in an antisemitic campaign in the army, during which more than 1000 Jewish officers were demoted or expelled. Even the non-Jewish minister of defence, Marshal Marian Spychalski was persecuted and Jaruzelski obtained his post.
In August 1968 General Jaruzelski as the defence minister ordered the 2nd Army under General Florian Siwicki (of the "LWP") to invade Czechoslovakia, resulting in military occupation of northern Czechoslovakia until 11 November 1968 when under his orders and agreements with the Soviet Union his Polish troops were withdrawn and replaced by the Soviet Army. In 1970, he was involved in the successful plot against Władysław Gomułka, which led to the appointment of Edward Gierek as General Secretary of the Polish United Workers Party. There is some question whether he took part in organizing the brutal suppression of striking workers; or whether his orders to the communist military led to massacres in the coastal cities of Gdańsk, Gdynia, Elbląg and Szczecin. As Minister of Defense general Jaruzelski was ultimately responsible for 27,000 troops used against unarmed civilians. He claims that he was circumvented, which is why he never apologized for his involvement, but he had an option of resigning open to him, especially after the resignation of foreign minister Adam Rapacki, and Jaruzelski didn't. Jaruzelski became a candidate member for the Politburo of the Polish United Workers' Party, the chief executive body of the party, obtaining full membership the following year.
Leader of the Polish military government.
On 11 February 1981, Jaruzelski was named Chairman of the Council of Ministers (Prime Minister). On 18 October, Stanisław Kania was ousted as First Secretary of the Central Committee of the Polish United Workers' Party after a listening device recorded him criticising the Soviet leadership. Jaruzelski was elected his successor, becoming the only professional soldier to become leader of a ruling European Communist party.
A fortnight after taking power, Jaruzelski met with Solidarity head Lech Wałęsa and Catholic primate Józef Glemp, and hinted that he wanted to bring the church and the union in a sort of coalition government. However, his intention was to crush Solidarity. As early as September, while he was still merely prime minister, he met with his aides to find an excuse to impose martial law. On 13 December, citing purported recordings of Solidarity leaders planning a coup, Jaruzelski organised his own coup by proclaiming martial law. A Military Council of National Salvation was formed, with Jaruzelski as chairman. A BBC News profile of Jaruzelski contends that the establishment of martial law was "an attempt to suppress the Solidarity movement."
According to Jaruzelski, martial law was necessary to avoid a Soviet invasion. In a May 1992 interview with "Der Spiegel", Jaruzelski said: "Given the strategic logic of the time, I probably would have acted the same way if I had been a Soviet general. At that time, Soviet political and strategic interests were threatened." However, at a press conference in September 1997 Viktor Kulikov, former supreme commander of Warsaw Pact forces, denied that the Soviet Union had either threatened or intended to intervene. According to Politburo minutes from 10 December 1981, Yuri Andropov stated "We do not intend to introduce troops into Poland. That is the proper position, and we must adhere to it until the end. I don't know how things will turn out in Poland, but even if Poland falls under the control of Solidarity, that's the way it will be."
Jaruzelski also claimed in 1997 that Washington had given him a "green light", stating that he had sent Eugeniusz Molczyk to confer with Vice President George H. W. Bush and Bush had agreed with Molczyk that martial law was the lesser of two evils. Whether this meeting with the American vice president occurred is disputed. While it is erroneously cited, Harvard historian Mark Kramer has pointed out that no documents support Jaruzelski's claim.
Historical evidence released under Russian President Boris Yeltsin has been brought to light indicating that the Soviet Union did not plan to invade Poland. In fact, Jaruzelski actually tried to persuade the Soviets to invade Poland in order to support martial law, only to be sternly turned down. This left the Solidarity "problem" to be sorted out by the Polish government (see also Soviet reaction to the Polish crisis of 1980–1981). However, the exact plans of the Soviet Union at that time have never been determined. Jaruzelski, however, has justified cracking down by alleging that the threat of Soviet intervention was quite likely had he not dealt with Solidarity internally. This question, as well as many other facts about Poland in the years 1945–1989, are presently under the investigation of government historians at the Institute of National Remembrance (Instytut Pamięci Narodowej, IPN), whose publications reveal facts from the Communist-era archives. Additionally, there are numerous confirmations from Czech army officers of the time speaking of "Operation Krkonoše", plan of armed invasion of Poland, because of which many units of the Czechoslovak People's Army were stationed on highest alert, ready for deployment within hours.
In 1982 he helped reorganize the Front of National Unity, the organization the Communists used to manage their satellite parties, as the Patriotic Movement for National Rebirth.
In 1985, Jaruzelski resigned as prime minister and defence minister and became the Chairman of the Polish Council of State — a post equivalent to that of head of state of Poland. However, his power centered on and firmly entrenched in his coterie of "LWP" generals and lower ranks officers of the Polish Communist Army.
Presidency.
The policies of Mikhail Gorbachev stimulated political reform in Poland. By the close of the tenth plenary session in December 1988, the Polish United Workers Party was forced, after strikes, to approach leaders of Solidarity for talks.
From 6 February to 15 April 1989, negotiations were held between 13 working groups during 94 sessions of the roundtable talks. These negotiations "radically altered the shape "of the Polish government and society", and resulted in an agreement which stated that a great degree of political power would be given to a newly created bicameral legislature. It also restored a post of president to act as head of state and chief executive. Solidarity was also declared a legal organization. During the following Polish elections the Communists were allocated 65 percent of the seats in the Sejm, Solidarity won all the remaining elected seats, and 99 out of the 100 seats in the fully elected Senate were also won by Solidarity-backed candidates. Jaruzelski won the presidential ballot by one vote on 19 July 1989.
Jaruzelski was unsuccessful in convincing Lech Wałęsa to include Solidarity in a "grand coalition" with the communists, and Jaruzelski resigned his position of general secretary of the Polish United Workers Party on 29 July 1989. Mieczysław Rakowski succeeded him as the general secretary of the party.
The Communists' two allied parties broke their long-standing alliance, forcing Jaruzelski to appoint Solidarity's Tadeusz Mazowiecki as the country's first non-Communist prime minister since 1948. Jaruzelski resigned as Poland's leader in 1990. He was succeeded by Wałęsa, who had won the presidential election on 9 December.
On 31 January 1991, General Jaruzelski retired from the army.
After retirement.
In an interview conducted in 2001, Jaruzelski said that he believed communism failed, and that he was now a social democrat. He also announced his support for then-President Aleksander Kwaśniewski, as well as future Prime Minister Leszek Miller. Both Kwaśniewski and Miller were members of the Democratic Left Alliance, the social democratic party that includes most of the remains of the PUWP.
In May 2005, Russian President Vladimir Putin awarded a medal commemorating the 60th anniversary of victory over Nazi Germany to Jaruzelski. Other former leaders awarded the medal include former Romanian King Michael I. Czech President Václav Klaus criticized this step, claiming that Jaruzelski was a symbol of the Warsaw Pact invasion of Czechoslovakia in 1968. Jaruzelski said that he had apologized and that the decision on the August 1968 invasion had been a great "political and moral mistake".
On 28 March 2006, Jaruzelski was awarded a Siberian Exiles Cross by Polish President Lech Kaczyński. However, after making this fact public Kaczyński claimed that this was a mistake and blamed the bureaucracy for giving him a document containing 1293 names without notifying him of Jaruzelski's presence within it. After this statement Jaruzelski returned the cross.
On 31 March 2006, the Institute of National Remembrance (IPN) charged him with committing communist crimes, mainly the creation of a criminal military organization with the aim of conducting crimes — mostly concerned with the illegal imprisonment of people. The second charge involved the incitement of state ministers to commit acts beyond their competence. Jaruzelski evaded most court appearances citing poor health. In December 2010, Jaruzelski suffered from severe pneumonia, and in March 2011, he was diagnosed with lymphoma. His wife Barbara threatened to file for divorce in 2014, saying she had caught his nurse Kasia in a compromising position with him.
Conversion to Roman Catholicism and death.
Jaruzelski died on 25 May 2014, in a Warsaw hospital after suffering a stroke earlier that month. Prior to his death, he reportedly requested last rites by a Catholic priest. President Bronisław Komorowski and former Presidents Lech Wałęsa and Aleksander Kwaśniewski as well as hundreds of other Poles attended his funeral mass at the Field Cathedral of the Polish Army in Warsaw on 30 May. Wałęsa and Komorowski, who were among the thousands imprisoned during the crackdown on Solidarity in 1981, both stated that judgment against Jaruzelski "would be left to God". Jaruzelski was then cremated and buried with full military honors at Powązki Military Cemetery in Warsaw, near the grave of Bolesław Bierut, the first Communist leader of Poland after World War II. The decision to bury Jaruzelski at Powązki, the resting place of Polish soldiers killed defending their country since the early 19th century, resulted in protests.
Personal life.
Jaruzelski married Barbara Halina Jaskólska (1930–) in 1961. They had a daughter, Monika who was born on 11 August 1963. Monika has a son, Gustav.
Legacy.
The BBC reported in 2001 that "for some Poles — particularly the Solidarity generation — he is little short of a traitor", even comparing his philosophy of "a strong Poland within a Soviet dominated bloc" to Vidkun Quisling's philosophy of a similar status for Norway within the Nazi controlled hemisphere. Meanwhile, opinion polls as of 15 May 2001 suggested that a majority of the Polish people were open to agreeing with his explanation that martial law was implemented to prevent a Soviet invasion. Available documents indicate that Jaruzelski actually lobbied for Soviet intervention. In interviews in Russian media (Rossiyskaya Gazeta for example) he has been presented as the harbinger of Poland's democracy.
Croatian writer Slavenka Drakulić described Jaruzelski as a "tragic believer in Communism who made a pact with the devil in good faith".
Written works.
"Różnić się mądrze" (English translation: "To Differ Wisely") (1999).
"Być może to ostatnie słowo (wyjaśnienia złożone przed Sądem)" (English translation:"It may be the last word (explanations given in the Court)") (2008).

</doc>
<doc id="51251" url="http://en.wikipedia.org/wiki?curid=51251" title="NFS">
NFS

NFS may refer to:

</doc>
<doc id="51252" url="http://en.wikipedia.org/wiki?curid=51252" title="Network File System">
Network File System

Network File System (NFS) is a distributed file system protocol originally developed by Sun Microsystems in 1984, allowing a user on a client computer to access files over a network much like local storage is accessed. NFS, like many other protocols, builds on the Open Network Computing Remote Procedure Call (ONC RPC) system. The Network File System is an open standard defined in RFCs, allowing anyone to implement the protocol.
Versions and variations.
Sun used version 1 only for in-house experimental purposes. When the development team added substantial changes to NFS version 1 and released it outside of Sun, they decided to release the new version as v2, so that version interoperation and RPC version fallback could be tested.
NFSv2.
Version 2 of the protocol (NFSv2) was first implemented in SunOS version 2.0, which was released in May 1985.
People involved in the creation of NFS version 2 include Russel Sandberg, Bob Lyon, Bill Joy, Steve Kleiman, and others.
The protocol is defined in RFC 1094, published in March 1989. 
NFSv2 originally operated only over UDP. Its designers meant to keep the server side stateless, with locking (for example) implemented outside of the core protocol. The decision to make the file system stateless was a key decision, since it makes recovery from server failures trivial: all network clients freeze up when a server becomes unavailable, but once the server is again operational all the state to retry each transaction is contained in each RPC, which is retried by the client stub(s). This design decision allows UNIX applications, which usually cannot tolerate file server crashes, to ignore the problem.
The Virtual File System interface allows a modular implementation, reflected in a simple protocol. By February 1986, implementations were demonstrated for operating systems such as System V release 2, Microsoft DOS, and VAX/VMS using Eunice.
Due to 32-bit limitations, NFSv2 allows only the first 2 GB of a file to be read.
NFSv3.
Version 3 (RFC 1813, June 1995) added:
At the time of introduction of Version 3, vendor support for TCP as a transport-layer protocol began increasing. While several vendors had already added support for NFS Version 2 with TCP as a transport, Sun Microsystems added support for TCP as a transport for NFS at the same time it added support for Version 3. Using TCP as a transport made using NFS over a WAN more feasible.
NFSv4.
Version 4 (RFC 3010, December 2000; last revised in RFC 7530, March 2015), influenced by AFS and CIFS, includes performance improvements, mandates strong security, and introduces a stateful protocol. Version 4 became the first version developed with the Internet Engineering Task Force (IETF) after Sun Microsystems handed over the development of the NFS protocols.
NFS version 4.1 (RFC 5661, January 2010) aims to provide protocol support to take advantage of clustered server deployments including the ability to provide scalable parallel access to files distributed among multiple servers (pNFS extension). NFS version 4.2 is currently being developed.
Not everyone was happy with the new protocol. In 2010, OpenBSD's Theo de Raadt wrote: "NFSv4 is not on our roadmap. It is a ridiculous bloated protocol which they keep adding crap to."
Other extensions.
WebNFS, an extension to Version 2 and Version 3, allows NFS to integrate more easily into Web-browsers and to enable operation through firewalls. In 2007, Sun Microsystems open-sourced their client-side WebNFS implementation.
Various side-band protocols have become associated with NFS, including:
NFS over RDMA is an adaptation of NFS that uses RDMA as a transport.
Platforms.
NFS is often used with Unix operating systems (such as Solaris, AIX and HP-UX) and Unix-like operating systems (such as Linux and FreeBSD). It is also available to operating systems such as the classic Mac OS, OpenVMS, IBM i, certain editions of Microsoft Windows, and Novell NetWare. Alternative remote file access protocols include the Server Message Block (SMB, also known as CIFS), Apple Filing Protocol (AFP), NetWare Core Protocol (NCP), and OS/400 File Server file system (QFileSvr.400).
SMB and NetWare Core Protocol (NCP) are more common than NFS on systems running Microsoft Windows, AFP is more common than NFS on Macintosh systems, and QFileSvr.400 was once more common on IBM i systems. Haiku recently added NFSv4 support as part of a Google Summer of Code project.
Typical implementation.
Assuming a Unix-style scenario in which one machine (the client) requires access to data stored on another machine (the NFS server):
Note that automation of the NFS mounting process may take place, perhaps by using codice_5 or automounting facilities.
Protocol development.
1980s.
NFS and ONC figured prominently in the network-computing war between Sun Microsystems and Apollo Computer, and later the UNIX wars (ca 1987-1996) between AT&T Corporation and Sun on one side, and Digital Equipment, HP, and IBM on the other.
During the development of the ONC protocol (called SunRPC at the time), only Apollo's Network Computing System (NCS) offered comparable functionality. Two competing groups developed over fundamental differences in the two remote procedure call systems. Arguments focused on the method for data-encoding – ONC's External Data Representation (XDR) always rendered integers in big-endian order, even if both peers of the connection had little-endian machine-architectures, whereas NCS's method attempted to avoid byte-swap whenever two peers shared a common endianness in their machine-architectures. An industry-group called the Network Computing Forum formed (March 1987) in an (ultimately unsuccessful) attempt to reconcile the two network-computing environments.
Later, Sun and AT&T announced they would jointly develop AT&T's UNIX System V Release 4. This caused many of AT&T's other licensees of UNIX System V to become concerned that this would put Sun in an advantaged position, and it ultimately led to Digital Equipment, HP, IBM, and others forming the Open Software Foundation (OSF) in 1988. Ironically, Sun and AT&T had previously competed over Sun's NFS versus AT&T's Remote File System (RFS), and the quick adoption of NFS over RFS by Digital Equipment, HP, IBM, and many other computer vendors tipped the majority of users in favor of NFS.
NFS interoperability was aided by events called "Connectathons" starting in 1986 that allowed vendor-neutral testing of implementations with each other. OSF adopted the Distributed Computing Environment (DCE) and the Distributed File System (DFS) over Sun/ONC RPC and NFS. DFS used DCE as the RPC, and DFS derived from the Andrew File System (AFS); DCE itself derived from a suite of technologies, including Apollo's NCS and Kerberos.
1990s.
Sun Microsystems and the Internet Society (ISOC) reached an agreement to cede "change control" of ONC RPC so that the ISOC's engineering-standards body, the Internet Engineering Task Force (IETF), could publish standards documents (RFCs) related to ONC RPC protocols and could extend ONC RPC. OSF attempted to make DCE RPC an IETF standard, but ultimately proved unwilling to give up change control. Later, the IETF chose to extend ONC RPC by adding a new authentication flavor based on GSSAPI, RPCSEC GSS, in order to meet IETF's requirements that protocol standards have adequate security.
Later, Sun and ISOC reached a similar agreement to give ISOC change control over NFS, although writing the contract carefully to exclude NFS version 2 and version 3. Instead, ISOC gained the right to add new versions to the NFS protocol, which resulted in IETF specifying NFS version 4 in 2003.
2000s.
By the 21st century, neither DFS nor AFS had achieved any major commercial success as compared to CIFS or NFS. IBM, which had previously acquired the primary commercial vendor of DFS and AFS, Transarc, donated most of the AFS source code to the free-software community in 2000. The OpenAFS project lives on. In early 2005, IBM announced end-of-sale for AFS and DFS.
In January 2010 Panasas proposed an NFSv4.1 based on their Parallel NFS (pNFS) technology; they claimed it improved data-access parallelism
capability. The NFSv4.1 protocol defines a method of separating the file system metadata from file data location; it goes beyond the simple name/data separation by striping the data amongst a set of data servers. This differs from the traditional NFS server which holds the names of files and their data under the single umbrella of the server. Some products provide multi-node NFS servers, but the participation of the client in separation of meta-data and data is limited.
The NFSv4.1 pNFS server is a collection of server resources or components; these are assumed to be controlled by the meta-data server.
The pNFS client still accesses a single meta-data server for traversal or interaction with the namespace; when the client moves data to and from the server it may directly interact with the set of data servers belonging to the pNFS server collection. The NFSv4.1 client can be enabled as a direct participant in the exact location of file data and to avoid solitary interaction with a single NFS server when moving data.
In addition to pNFS, NFSv4.1 provides:

</doc>
<doc id="51253" url="http://en.wikipedia.org/wiki?curid=51253" title="Nut">
Nut

Nut, NUT, Nuts or NUTS may refer to:

</doc>
<doc id="51254" url="http://en.wikipedia.org/wiki?curid=51254" title="Father of the House">
Father of the House

Father of the House is a term that has by tradition been unofficially bestowed on certain members of some national legislatures, most notably the House of Commons in the United Kingdom. In some legislatures the term refers to the oldest member, but in others it refers the longest-serving member. The term Mother of the House or Mother of Parliament is also found, although the usage varies between countries. It is used simply as the female alternative to Father of the House, being applied when the relevant member is a woman.
United Kingdom.
House of Commons.
The "Father of the House" is a title that is by tradition bestowed on the senior Member of the House of Commons who has the longest unbroken service. If two or more Members have the same length of current uninterrupted service, then whoever was sworn in earliest, as listed in Hansard, is named as Father of the House.
In the House of Commons, the only conventional duty of the Father of the House is to preside over the election of a new Speaker whenever that office becomes vacant. The relevant Standing Order does not refer to this member by the title of "Father of the House", referring instead to the longest-serving member of the House present who is not a Minister of the Crown (meaning that if the longest-serving member is absent or is a government minister, the next person in line presides).
The current Father of the House of Commons is Sir Gerald Kaufman, Labour MP for Manchester Gorton, who began his continuous service from the 1970 general election. 
Should Kaufman cease to be a Member of the House, the remaining members first elected in 1970 would become eligible to be Father of the House. They are Kenneth Clarke, Michael Meacher and Dennis Skinner.
The Father of the House is not necessarily the sitting Member with the earliest date of first election: David Winnick was first elected in 1966, and was the last Member to have served in the 1960s, but he lost his seat in 1970 and was out of Parliament until elected again in 1979. Michael Foot, as the only remaining Member from the 1945 election between 1987 and 1992, was never Father of the House because he had been out of Parliament between 1955 and a by-election in 1960. Similarly, though Winston Churchill was first elected in 1900, he did not become Father of the House until 1959, because he had lost his seat in 1922, not returning to the Commons until 1924.
Sir Henry Campbell-Bannerman was simultaneously Father of the House and Prime Minister from May 1907 until shortly before his death in April 1908.
House of Lords.
The current Father of the House of Lords is Lord Carrington (Conservative), who became eligible to take his seat on his 21st birthday in 1940 (having succeeded to the title in 1938 while still a minor) and actually first took his seat in October 1945. After the House of Lords Act 1999 removed the automatic right of hereditary peers to sit in the House of Lords, Carrington (along with all former Leaders of the House who were hereditaries) was given a life peerage to enable him to continue to sit.
Should Carrington cease to be a Member of the House of Lords, the remaining peers who first sat in the 1940s will become eligible to be Father of the House. There are currently two: Lord Montagu of Beaulieu (sat first on 26 November 1947) and Lord Denham (sat first on 13 December 1949). Both are hereditary peers who were elected to remain in the House under the provisions of the 1999 Act.
The senior life peer by date of creation is Lord Chalfont, who entered the House in 1964. 
House of Commons of Northern Ireland (defunct).
The Parliament of Northern Ireland, including the House of Commons of Northern Ireland, was prorogued in 1972 and abolished completely in 1973 leaving the title of Father of the House defunct.
Australia.
In Australia, the current member of the House of Representatives with the longest period of continuous service, whether a Minister or not, is known as "Father of the House". Similarly, the current member of the Senate with the longest period of continuous service is known as "Father of the Senate". The longer serving of the two Fathers is called "Father of the Parliament".
As in Britain, these terms have no official status. However, unlike Britain:
Since 6 February 2015, Senator Ian Macdonald, who was first appointed in 1990, has been the Father of the Senate.
Since 1 September 1998, Philip Ruddock, who was first elected in 1973, has been the Father of the House of Representatives and Father of the Parliament.
Canada.
The longest-serving member of the House of Commons who is not a cabinet minister is known as the Dean of the House, and presides over the election of the Speaker at the beginning of each Parliament. The same term is used for the equivalent position in the U.S. House of Representatives.
Germany.
Starting with the Frankfurter Nationalversammlung (Frankfurt Parliament) of 1848, all democratic German parliaments had a Father (or Mother) of the House, usually called "Alterspräsident" (President by right of age).
Under the current constitution (Grundgesetz) of 1949, the Alterspräsident will preside over the Parliament (Bundestag) at the start of each legislative period.
Following tradition, the Alterspräsident will first ascertain himself that he is the oldest member of the Bundestag by stating his birth date and asking if anyone is present who was born before his date. If no older member of the Bundestag is present (which is usually the case) he will formally declare that he indeed is the Alterspräsident and will start proceedings.
As acting President of the Bundestag (Bundestagspräsident) he delivers the first programmatic speech and oversees the elections of the President of the Bundestag and the Vicepresidents of the Bundestag (Bundestagsvizepräsidenten). He then stands down and yields his power to the newly elected Bundestagspräsident.
As the position of Father of the House usually draws a certain public attention, the PDS twice nominated old independents (Stefan Heym in 1994, Fred Gebhardt in 1998) to obtain this office. None of them served a complete term (Heym resigned in 1996, Gebhardt died in 2000). This was considered a manipulation.
Hungary.
In Hungary, term refers to the oldest member of the National Assembly (previously House of Representatives, the lower house). before the open session, the senior chair and junior notaries reviews the mandates of all the elected MPs in addition to their own. He or she preside ove the newly elected parliament until the appointment of the officials.
Israel.
In the beginning of each Knesset, before the election of a permanent speaker, there is a temporary speaker. In the past it was the oldest member of Knesset, now it is the longest-serving member. Michael Eitan is the most recent Knesset member to serve in this capacity, doing so from February 24 - March 30, 2010. In 2013 it was Benyamin Ben-Eliezer who had this position.
Ireland.
In the Republic of Ireland, the term Father of the Dáil is an unofficial title applied to the longest-serving Teachta Dála (TD) in Dáil Éireann, regardless of their position. The current Father is the Taoiseach and Fine Gael party leader, Enda Kenny, TD, since the retirement of Séamus Pattison at the 2007 general election. On a number of occasions two or more people have shared the position of Father of the Dáil.
New Zealand.
In New Zealand, the term Father or Mother of the House, as an unofficial title, designates the longest-serving MP in the House of Representatives, regardless of their position. The Father of the House has no official role in Parliament. Peter Dunne, the leader of the United Future party, holds the title in the New Zealand Parliament, having served continuously since the 1984 general election.
In New Zealand's first election of 1853, the Bay of Islands electorate became the first to declare the election of a successful candidate, returning Hugh Carleton unopposed. In the subsequent General Assembly of 1854, Carleton liked to be known as the "Father of the House".
Norway.
In Norway it is the representative of the Storting with longest seniority that is temporary Stortingspresident (speaker). Per Kristian Foss had this position in 2009 until Dag Terje Andersen was elected.
Finland.
"Note: this is a list of longest-serving Finnish MPs; however, before the election of the Speaker, the Finnish Parliament is chaired by the oldest MP, not the longest-serving one."
Serbia.
In the National Assembly of the Republic of Serbia, the oldest MP serves as the Acting Speaker presiding over the constitutive session, before the Speaker is elected.
Singapore.
Until his death on 23 March 2015, former Prime Minister Harry Lee was the longest serving Member of Parliament (Tanjong Pagar) and thus the Father of the House.As of April 2015, Emeritus Senior Minister Goh Chok Tong is Father of the House, as the longest serving MP (Marine Parade); he was the second longest serving MP in Parliament.

</doc>
<doc id="51255" url="http://en.wikipedia.org/wiki?curid=51255" title="Charles de Gaulle">
Charles de Gaulle

Charles André Joseph Marie de Gaulle (]; 22 November 1890 – 9 November 1970) was a French general, resistant, writer and statesman. He was the leader of Free France (1940–44) and the head of the Provisional Government of the French Republic (1944–46). In 1958, he founded the Fifth Republic and was elected as the 18th President of France, until his resignation in 1969. He was the dominant figure of France during the Cold War era and his memory continues to influence French politics.
Born in Lille in a devout Catholic and patriotic family, he embraced a military career and graduated from Saint-Cyr in 1912. He was a decorated officer of the First World War and came to the fore in the Interwar period as a proponent of mobile armoured divisions. At the beginning of the Second World War, he led an armoured division that inflicted several reverses on the invading German army. Refusing to accept his government's armistice with Nazi Germany in 1940, de Gaulle exhorted the French population to resist occupation and to continue the fight against Axis powers in his Appeal of 18 June. He led a government in exile and the Free French Forces against the Axis. Despite frosty relations with Britain and especially the United States, he emerged as the undisputed leader of the French resistance. He became Head of the Provisional Government of the Republic in June 1944, the interim government of France following the liberation of France.
When the Algerian war was ripping apart the unstable Fourth Republic, the National Assembly brought him back to power during the May 1958 crisis. De Gaulle founded the Fifth Republic with a strengthened presidency, and he was elected in the latter role. He managed to keep France together while taking steps to end the war, much to the anger of the Pieds-Noirs (Frenchmen settled in Algeria) and the military; both previously had supported his return to power to maintain colonial rule. He granted independence to Algeria and progressively to other French colonies. Economically, he pursued a dirigist policy, which included substantial state-directed control over a capitalist economy.
In the context of the Cold War, de Gaulle initiated his "Politics of Grandeur", asserting that France as a major power should not rely on other countries, such as the United States, for its national security and prosperity. To this end, de Gaulle pursued a policy of "national independence" which led him to withdraw from NATO's military integrated command and to launch an independent nuclear development program that made France the fourth nuclear power. He restored cordial Franco-German relations in order to create a European counterweight between the "Anglo-Saxon" (American and British) and Soviet spheres of influence. However, he opposed any development of a supranational Europe, favouring a Europe of sovereign Nations and vetoed twice Britain's entry into the European Community. De Gaulle openly criticised the U.S. intervention in Vietnam and the "exorbitant privilege" of the U.S. dollar, and supported an independent Quebec.
In May 1968, he appeared likely to lose power amidst widespread protests by students and workers, but survived the crisis with backing from the Army and won an election with an increased majority in the Assembly. Nonetheless, de Gaulle resigned in 1969 after losing a referendum in which he proposed more decentralization. He died a few months later at his residence in Colombey-les-Deux-Églises. His "War Memoirs," written in the 1950s, quickly became a classic of modern French literature. Many French political parties and figures claim the gaullist legacy.
Early life.
De Gaulle was born in the industrial region of Lille in the Nord departement, the third of five children. He was raised in a family of devout Roman Catholics who were patriotic, royalist and traditionalist and quite progressive. His father, Henri de Gaulle, was a professor of history and literature at a Jesuit college who eventually founded his own school.
His father came from a long line of parliamentary gentry from Normandy and Burgundy, while his mother, Jeanne (née Maillot), descended from a family of wealthy entrepreneurs from Lille. His mother had French, Irish, Scottish, Fleming, and German ancestry.
The family lost most of its land in the French Revolution, which it opposed. De Gaulle's father encouraged historical and philosophical debate between his children at mealtimes, and through his encouragement, de Gaulle grew familiar with French history from an early age. Struck by his mother's tale of how she cried as a child when she heard of the French capitulation to the Germans at Sedan in 1870, he developed a keen interest in military strategy and endlessly questioned his father about the other failures of the brief war at Vionville and Mars-la-Tour, and though a naturally shy person his entire life, often organised other children to re-enact ancient French battles. The wider de Gaulle family were also very literary and academic, and he was raised on tales of the flight of the Scottish Stuarts to France, to whom he was related on his mother's side. He was also influenced by his uncle, also called Charles de Gaulle, who was a historian and passionate Celticist who wrote books and pamphlets advocating the union of the Welsh, Scots, Irish and Bretons into one people. His grandfather Julien-Philippe was also a historian, and his grandmother Josephine-Marie wrote poems which impassioned his Christian faith.
The father inculcated in the son a profound belief in the glory of traditional Catholic France. Charles received a rigorous classical education that included a year 1907–08 at a Jesuit college in Belgium.
By the time he was ten he was reading medieval history, such as the Froissart's Chronicles of the Hundred Years War. De Gaulle began his own writing in his early teens, and later his family paid for a composition, a one-act play in verse about a traveller, to be privately published. Always a voracious reader, he later favored philosophical tomes by such writers as Henri Bergson, Charles Péguy, and Maurice Barrès. In addition to the German philosophers Friedrich Nietzsche, Immanuel Kant and Johann Wolfgang von Goethe, the works of the ancient Greeks (especially Plato) and the prose of the romanticist poet François-René de Chateaubriand.
De Gaulle was educated in Paris at the College Stanislas and also briefly in Belgium where he continued to display his interest in reading and studying history and shared the great pride many of his countrymen felt in their nation's achievements. As he grew older, he also developed a profound belief in his destiny to achieve great things, and, eager to avenge the French defeat of 1870, decided upon a military career as being the best way to make a name for himself. He matriculated at the Saint Cyr military academy in 1908 where he did well; in 1911 he was commissioned in the French army.
Officer cadet.
De Gaulle spent four years studying and training at the elite military academy, Saint-Cyr. While there, and because of his height, high forehead, and nose, he acquired the nickname of "the great asparagus"
He did well at the academy and received praise for his conduct, manners, intelligence, character, military spirit and resistance to fatigue. However, he often quarrelled with his company commander and other officers that there was a lack of preparation for war with Germany, and that the French training and equipment were inadequate to deal with a numerically superior adversary. Graduating in 1912 in 13th place out of 210 cadets, his passing out report noted that he was a highly gifted cadet who should go on to make an excellent officer. Preferring to serve in France rather than far away in North Africa or Indochina, he joined the 33rd infantry regiment of the French Army, based at Arras and commanded by Colonel (and future Marshal) Philippe Pétain. De Gaulle would follow Pétain for the next 20 years.
While at Arras, and in the build-up to World War I, de Gaulle developed a good rapport with his commanding officer, Pétain, with whom he shared a number of ideas on French military affairs, and was often seen on exercise and in officers' quarters with his superior debating great battles and the likely outcome of any coming war. Both men agreed that the invention of the machine gun and rapid-firing artillery rendered cavalry virtually obsolete and would require a shift to semi-static positions from which attacks would be made under the protection of a heavy barrage of Artillery fired at the enemy positions.
First World War.
When war finally broke out in France in early August 1914, the 33rd Regiment, considered one of the best fighting units in France, was immediately thrown into checking the German advance at Dinant. However, the traditionally minded French Fifth Army commander, General Charles Lanrezac threw his units into pointless bayonet charges with bugles and full colours flying against the German artillery, incurring heavy losses.
Promoted to platoon commander, de Gaulle was involved in fierce fighting from the outset and was among the first to be wounded, receiving a bullet in the fibula at the Battle of Dinant. In hospital, he grew bitter at the tactics used, and spoke with other injured officers against the outdated methods of the French army. Yet, with General Joseph Joffre's decision to stop the retreat and counter-attack, favoured by the arrival of British units and by changes in the command structure, the rapid German advance was eventually stalled by mid September at the First Battle of the Marne. Returning in the Marne in late 1914 to find many of his former comrades dead, he was put in charge of a company. De Gaulle's unit gained recognition for repeatedly crawling out into No mans land to listen to the conversations of the enemy in their trenches, and the information he brought back was so valuable that in January 1915 he received a citation for his bravery. The 10 January 1915, de Gaulle received a bullet in the left hand which incapacitated him for four months and which later forced him to wear his wedding ring to the right hand.
At the Battle of Verdun in March 1916, while leading a charge to try to break out of a position which had become surrounded by the enemy, he received a bayonet wound to the thigh after being stunned by a shell and, passing out from the effects of poison gas, was captured at Douaumont, one of the few survivors of his battalion. He spent 32 months in a German prisoner of war camp, where his treatment was satisfactory.
In captivity de Gaulle acquired yet another nickname, "Le Connétable" ("The Constable"). This came about because of his reading German newspapers (he had learned German at school and spent a vacation in the Black Forest region) and giving talks on his view of the progress of the conflict to fellow prisoners. These were delivered with such patriotic ardour and confidence in victory that they called him by the title which had been given to the commander-in-chief of the French army during the monarchy.
While a prisoner of war, de Gaulle wrote his first book, co-written by Matthieu Butler, "L'Ennemi et le vrai ennemi" ("The Enemy and the True Enemy"), analysing the issues and divisions within the German Empire and its forces; the book was published in 1924.
In all, he made five unsuccessful escape attempts, being moved to higher security accommodation and punished on his return with long periods of solitary confinement and with the withdrawal of privileges such as newspapers and tobacco. In his letters to his parents he constantly spoke of his frustration that the war was continuing without him, calling the situation "a shameful misfortune" and compared it to being cuckolded. As the war neared its end, he grew depressed that he was playing no part in the victory, but despite his efforts, he remained in captivity until the German surrender. On 1 December 1918, three weeks after the armistice, he returned to his father's house in the Dordogne to be reunited with his three brothers, who had all served in the army and survived the war.
Between the wars.
After the armistice, de Gaulle continued to serve in the army, and was with the staff of the French Military Mission to Poland as an instructor of Poland's infantry during its war with Communist Russia (1919–1921). He distinguished himself in operations near the River Zbrucz and won Poland's highest military decoration, the Virtuti Militari cross. He returned to France, where he taught at the École Militaire. Although he was a protégé of his old commander, Marshal Philippe Pétain, de Gaulle believed in the use of tanks and rapid maneuvers rather than trench warfare.
De Gaulle served with the Army of Occupation in the Rhineland in the mid-1920s. As a "commandant" ("major") by the late 1920s, he briefly commanded a light infantry battalion at Trier (Treves) and then served a tour of duty in Syria, then a French protectorate under a mandate from the League of Nations. During the 1930s, now a lieutenant-colonel, he served as a staff officer in France. In 1934 he wrote "Vers l'Armée de Métier" ("Toward a Professional Army"), which advocated a professional army based on mobile armored divisions. Such an army would both compensate for the poor French demography, and be an efficient tool to enforce international law, particularly the Treaty of Versailles, which forbade Germany from rearming. He proposed mechanization of the infantry, with stress on the wholesale use of tanks. Ironically the German panzer units, so effectively employed in the invasion of France in 1940, utilized similar theories, while the French dispersed and wasted their armor. The book sold only 700 copies in France, where Pétain advocated an infantry-based, defensive army, but 7,000 copies in Germany, where it was studied by Adolf Hitler.
Second World War.
The Battle of France.
At the outbreak of World War II, de Gaulle was still a colonel, having antagonised the leaders of the military through the 1920s and 1930s with his bold views. Initially commanding a tank regiment in the French Fifth Army, de Gaulle implemented many of his theories and tactics for armoured warfare against an enemy whose strategies resembled his own. After the German breakthrough at Sedan on 15 May 1940 he was given command of the improvised 4e Division cuirassée.
On 17 May, de Gaulle attacked German tank forces at Montcornet with 200 tanks but no air support. Although de Gaulle's tanks forced the German infantry to retreat to Caumont, the action brought only temporary relief and did little to slow the spearhead of the German advance. Nevertheless, it was one of the few successes the French enjoyed while suffering defeats elsewhere across the country. In recognition for his efforts, de Gaulle was promoted to acting brigadier general on 24 May, a rank he would hold for the rest of his life. On 28 May, he took part in an unsuccessful attempt to rescue the Allied force trapped at Dunkirk by cutting an escape route through German forces at Abbeville.
On 5 June, Prime Minister Paul Reynaud appointed him Under Secretary of State for National Defence and War and put him in charge of coordination with the British forces.
As a junior member of the French government, he unsuccessfully opposed surrender, advocating instead that the government remove itself to North Africa and carry on the war as best it could from France's African colonies.
Leader of the Free French.
Returning to Bordeaux, the temporary wartime capital, de Gaulle learned that Marshal Pétain had become prime minister and was planning to seek an armistice with Nazi Germany. De Gaulle and other officers rebelled against the new French government; on the morning of 17 June, de Gaulle and a few senior French officers flew to Britain with 100,000 gold francs in secret funds provided to him by the ex-prime minister Paul Reynaud.
De Gaulle strongly denounced the French government's decision to seek armistice with Germany and set about building the Free French Forces from the soldiers and officers deployed outside France or who had fled France with him. On 18 June, de Gaulle delivered a radio address via the BBC Radio service; the talk was authorised by Churchill.
De Gaulle's "Appeal of 18 June" exhorted the French people not to be demoralised and to continue to resist the occupation of France and work against the collaborationist Vichy regime, which had signed an armistice with Germany. Although the original broadcast could only be heard in a few parts of occupied France, de Gaulle's subsequent speeches reached many parts of the territories under the Vichy regime, helping to rally the French resistance movement and earning him much popularity amongst the French people and soldiers. At a court-martial by Vichy, on 2 August 1940, de Gaulle was condemned to death for treason.
De Gaulle made his headquarters at Carlton Gardens in central London. His wife and daughter lived in the country four hours from London and seldom saw the general. From 1942 to 1944, he lived in Hampstead, north-west London.
He organised the Free French Forces and the Allies gave increasing support and recognition to de Gaulle's efforts. In London in September 1941 de Gaulle formed the free French National Council, with himself as president. It was an all-encompassing coalition of resistance forces, ranging from conservative Catholics like himself to Communists. By early 1942, the "Fighting French" movement, as it was now called, gained rapidly in power and influence; it overcame Vichy in Syria and Lebanon, adding to its base. Dealing with the French Communists was a delicate issue, for they were under Moscow's control and the USSR was friendly with Germany in 1940–41. They came into the Free French movement only when Germany invaded Russia in June 1941. De Gaulle's policy then became one of friendship directly with Moscow, but Stalin showed little interest. In 1942, De Gaulle created the Normandie-Niemen squadroon, a Free French Air Force regiment, in order to fight on the Eastern Front. It is the only Western allied formation to have fought until the end of the war in the East.
De Gaulle's relations with the "Anglo-Saxons".
In his dealings with the British and Americans (both referred to as the "Anglo-Saxons", in de Gaulle's parlance), he always insisted on retaining full freedom of action on behalf of France and was constantly on the verge of being unsupported by the Allies. Many denials of the deep and mutual antipathy between de Gaulle and British and American political leaders are on historical record. De Gaulle explained his position:
"Never the Anglo-Saxons really treated us as real allies. They never consulted us, government to government, on any of their provisions. For political purpose or by convenience, they sought to use the French forces for their own goals, as if these forces belonged to them, alleging that they had provided weapons to them [...] I considered that I had to play the French game, since the others were playing theirs ... I deliberately adopted a stiffened and hardened attitude ...".
In addition, De Gaulle harboured a suspicion of the British in particular, believing that they were seeking to seize France's colonial possessions in the Levant. Winston Churchill was often frustrated at what he perceived as de Gaulle's patriotic arrogance, but also wrote of his "immense admiration" for him during the early days of his British exile. Although their relationship later became strained, Churchill tried to explain the reasons for de Gaulle's behaviour in the second volume of his history of World War II:
"He felt it was essential to his position before the French people that he should maintain a proud and haughty demeanour towards "perfidious Albion", although in exile, dependent upon our protection and dwelling in our midst. He had to be rude to the British to prove to French eyes that he was not a British puppet. He certainly carried out this policy with perseverance".
De Gaulle epitomised his adversarial relationship with Churchill in these words: "When I am right, I get angry. Churchill gets angry when he is wrong. We are angry at each other much of the time." On one occasion in 1941 Churchill spoke to him on the telephone. De Gaulle retorted that the French people thought he was a reincarnation of Joan of Arc, to which Churchill replied that the English had had to burn the last one. Clementine Churchill, who admired de Gaulle, once cautioned him, "General, you must not hate your friends more than you hate your enemies." De Gaulle himself stated famously, "No Nation has friends, only interests."
After his initial support, Churchill, emboldened by Washington's antipathy to the French general, urged his War Cabinet to remove de Gaulle as leader of the French resistance. But the War Cabinet warned Churchill that a precipitate break with de Gaulle would have a disastrous reaction on the whole resistance movement. By Autumn 1943, Churchill had to acknowledge that de Gaulle had won the struggle for leadership of Free France.
De Gaulle's relations with Washington were even more strained, Roosevelt for a long time refused to recognise de Gaulle as the representative of France, preferring to deal with representatives of the Vichy government. Roosevelt in particular hoped that it would be possible to wean Pétain away from Germany. President Roosevelt maintained recognition of the Vichy regime until late 1942, and saw de Gaulle as an impudent representative of a minority interest. After 1942, Roosevelt championed general Henri Giraud, more compliant with U.S. interests than de Gaulle, as the leader of the French Resistance. At the Casablanca Conference (1943), Roosevelt forced de Gaulle to cooperate with Giraud, but de Gaulle was considered as the undisputed leader of the Resistance by the French people and Giraud was progressively deprived of his political and military roles. British and Soviet Allies urged Roosevelt to recognise de Gaulle's provisional government, but Roosevelt delayed it as long as possible and even recognised the Italian provisional government before the French one. British and Soviet allies were outraged that the U.S. president unilaterally recognised the new government of a former enemy before de Gaulle's one and both recognised the French government in retaliation, forcing Roosevelt to recognise de Gaulle in late 1944, but Roosevelt managed to blackball him from the Yalta Conference. Later, Roosevelt finally renounced to rule France as an occupied territory and to transfer French Indochina to the United Nations.
Plane sabotage.
On April 21, 1943, De Gaulle was scheduled to fly in a Wellington Bomber to Scotland to inspect the Free French navy. On take-off, the bomber's tail dropped, and the plane nearly crashed into the airfield's embankment. Only the skill of the pilot saved them. On inspection, it was found that aeroplane's separator rod had been sabotaged, using acid. Britain's MI6 investigated the incident, but no one was ever apprehended. De Gaulle blamed the Western Allies, and later told colleagues that he no longer had confidence in them.
Algiers.
Working with the French Resistance and other supporters in France's colonial African possessions after the Anglo-U.S. invasion of North Africa in November 1942, de Gaulle moved his headquarters to Algiers in May 1943. He left Britain to be on French territory. He became first joint head (with the less resolutely independent General Henri Giraud, the candidate preferred by the U.S. who wrongly suspected de Gaulle of being a British puppet) and then—after squeezing out Giraud by force of personality—sole chairman of the French Committee of National Liberation.
De Gaulle was held in high regard by Allied commander General Dwight Eisenhower. In Algiers in 1943, Eisenhower gave de Gaulle the assurance in person that a French force would liberate Paris and arranged that the army division of French General Philippe Leclerc de Hauteclocque would be transferred from North Africa to the UK to carry out that liberation. Eisenhower was impressed by the combativeness of units of the Free French Forces and "grateful for the part they had played in mopping up the remnants of German resistance"; he also detected how strongly devoted many were to de Gaulle and how ready they were to accept him as the national leader.
Preparations for D-Day.
As preparations for the liberation of Europe gathered pace, the Americans in particular found de Gaulle's tendency to view everything from the French perspective to be extremely tiresome. Roosevelt, who refused to recognise any provisional authority in France until elections had been held, considered de Gaulle to be a potential dictator, a view backed by a number of leading Frenchmen in Washington, including Jean Monnet, who later became an instrumental figure in the setting up of the European Coal and Steel Community that led to the modern European Union. Roosevelt also did not allow Churchill to provide de Gaulle with strategic details of the imminent invasion because he did not trust him to keep the information to himself. French codes were considered weak, posing a risk since the Free French refused to use British or American codes. De Gaulle refused to share coded information with the British, who were then obliged secretly to break the codes to read French messages.
Nevertheless, a few days before D-Day, Churchill, whose relationship with the General had deteriorated since he arrived in Britain, decided he needed to keep him informed of developments, and on 2 June he sent two passenger aircraft and his representative, Duff Cooper to Algiers to bring de Gaulle back to Britain. De Gaulle refused because of Roosevelt's intention to install a provisional Allied military government in the former occupied territories pending elections, but he eventually relented and flew to Britain the next day.
Upon his arrival at RAF Northolt on 4 June 1944 he received an official welcome, and a letter reading "My dear general! Welcome to these shores, very great military events are about to take place!" Later, on his personal train, Churchill informed him that he wanted him to make a radio address, but when informed that the Americans continued to refuse to recognise his right to power in France, and after Churchill suggested he request a meeting with Roosevelt to improve his relationship with the president, de Gaulle became angry, demanding to know why he should "lodge my candidacy for power in France with Roosevelt; the French government exists".
De Gaulle was concerned at a general breakdown of civil order and of a potential communist takeover in the vacuum which might follow a German withdrawal from France. During the general conversation which followed with those present, de Gaulle was involved in an angry exchange with the Labour minister, Ernest Bevin, and, raising his concerns about the validity of the new currency to be circulated by the Allies after the liberation, de Gaulle commented scornfully, "go and wage war with your false money". De Gaulle was much concerned that an American takeover of the French administration would just provoke a communist uprising.
Churchill then also lost his temper, saying that Britain could not act separately from America, and that under the circumstances, if they had to choose between France and the U.S., Britain would always choose the latter. De Gaulle replied that he realised that this would always be the case. The next day, de Gaulle refused to address the French nation because the script again made no mention of his being the legitimate interim ruler of France. It instructed the French people to obey Allied military authorities until elections could be held, and so the row continued, with de Gaulle calling Churchill a "gangster". Churchill in turn accused the general of treason in the height of battle, and demanded he be flown back to Algiers "in chains if necessary".
De Gaulle and Churchill had a complex relationship during the wartime period. De Gaulle did show respect and admiration for Churchill, and even some light humorous interactions between the two have been noted by observers such as Alfred Duff Cooper, the British Ambassador to the French Committee of Liberation. Churchill explained his support for De Gaulle during the darkest hours, calling him “L’homme du destin". In Casablanca in 1943, Churchill supported de Gaulle as the embodiment of a French Army that was otherwise defeated, stating that "De Gaulle is the spirit of that Army. Perhaps the last survivor of a warrior race." Churchill also supported de Gaulle because he was one of the first major French leaders to reject Nazi German rule outright, stating in August 1944 that "I have never forgotten, and can never forget, that he [De Gaulle] stood forth as the first eminent Frenchman to face the common foe in what seemed to be the hour of ruin of his country and possibly, of ours"
In the years to come, the sometimes hostile, sometimes friendly dependent wartime relationship of de Gaulle and his future political peers re-enacted the historical national and colonial rivalry and lasting enmity between the French and English, and foreshadowed the deep distrust of France for post-war Anglo-American partnerships.
Return to France.
De Gaulle ignored "les Anglo-Saxons", and proclaimed the authority of Free France over the metropolitan territory the next day. Under the leadership of General de Lattre de Tassigny, France fielded an entire army – a joint force of Free French together with French colonial troops from North Africa – on the Western Front. Initially landing as part of Operation Dragoon, in the south of France, the French First Army helped to liberate almost one third of the country and participated in the invasion and occupation of Germany. As the invasion slowly progressed and the Germans were pushed back, de Gaulle made preparations to return to France.
On 14 June 1944 he left Britain for France for what was supposed to be a one-day trip. Despite an agreement that he would take only two staff, he was accompanied by a large entourage with extensive luggage, and although many rural Normans remained mistrustful of him, he was warmly greeted by the inhabitants of the towns he visited, such as the badly damaged Isigny. Finally he arrived at the city of Bayeux, which he now proclaimed as the capital of Free France. Appointing his Aide-de-Camp Francois Coulet as head of the civil administration, de Gaulle returned to the UK that same night on a French destroyer, and although the official position of the supreme military command remained unchanged, local Allied officers found it more practical to deal with the fledgling administration in Bayeux in everyday matters.
De Gaulle flew to Algiers on 16 June and then went on to Rome to meet the Pope and the new Italian government. At the beginning of July he at last visited Roosevelt in Washington, where he received the 17 gun salute of a senior military leader rather than the 21 guns of a visiting head of state. The visit was 'devoid of trust on both sides' according to the French representative, however Roosevelt did make some concessions towards recognising the legitimacy of the Bayeux administration.
Meanwhile, with the Germans retreating in the face of the Allied onslaught, harried all the way by the resistance, there were widespread instances of revenge attacks on those accused of collaboration. A number of prominent officials and members of the feared Milice were murdered, often by exceptionally brutal means, provoking the Germans into appalling reprisals, such as in the destruction of the village of Oradour-sur-Glane and the killing of its 642 inhabitants. Of little strategic value, Paris was initially not high on the list of Allied objectives, but both de Gaulle and the commander of the 2nd Armoured Division, General Philippe Leclerc were concerned that a possible communist attempt to take over the capital would plunge France into civil war. De Gaulle successfully lobbied for Paris to be made a priority for liberation on humanitarian grounds and obtained from Allied Supreme Commander General Dwight D. Eisenhower an agreement that French troops would be allowed to enter the capital first. A few days later, General Leclerc's French Armoured Division entered the outskirts of the city, and after six days of fighting in which the resistance played a major part, the German garrison of 5000 men surrendered on 25 August, although some sporadic outbreaks of fighting continued for several days. In surrendering, the German commander General Dietrich von Choltitz ignored Hitler's orders to raze the city to the ground.
It was fortunate for de Gaulle that the Germans had forcibly removed members of the Vichy government and taken them to Germany a few days earlier on 20 August; it allowed him to enter Paris as a liberator in the midst of the general euphoria, but there were serious concerns that communist elements of the resistance, which had done so much to clear the way for the military would try to seize the opportunity to proclaim their own 'Peoples' Government' in the capital. De Gaulle made contact with Leclerc and demanded the presence of the 2nd Armoured Division to accompany him on a massed parade down the Champs Elysees, "as much for prestige as for security". This was in spite of the fact that Leclerc's unit was fighting as part of the American 1st Army and were under strict orders to continue their next objective without obeying orders from anyone else. In the event, the American General Omar Bradley decided that Leclerc's division would be indispensable for the maintenance of order and the liquidation of the last pockets of resistance in the French capital. Earlier, on 21 August, de Gaulle had appointed his military advisor General Marie-Pierre Koenig as Governor of Paris.
As his procession came along the Place de la Concorde on Saturday 26 August, it came under machine gun fire by Vichy militia and fifth columnists who were unable to give themselves up. Later, on entering the Notre Dame cathedral to be received as head of the provisional government by the Committee of Liberation, loud shots broke out again, and Leclerc and Koenig tried to hustle him through the door, but de Gaulle shook off their hands and never faltered. While the battle began outside, he walked slowly down the aisle. Before he had gone far a machine pistol fired down from above, at least two more joined in, and from below the F.F.I. and police fired back. A BBC correspondent who was present reported;
"... the General is being presented to the people. He is being received…they have opened fire! ... firing started all over the place ... that was one of the most dramatic scenes I have ever seen. ... General de Gaulle walked straight ahead into what appeared to me to be a hail of fire ... but he went straight ahead without hesitation, his shoulders flung back, and walked right down the centre aisle, even while the bullets were pouring about him. It was the most extraordinary example of courage I have ever seen ... there were bangs, flashes all about him, yet he seemed to have an absolutely charmed life."
Later, in the great hall of the Hôtel de Ville, de Gaulle was greeted by a jubilant crowd and, proclaiming the continuity of the Third Republic, delivered a famous proclamation;
"Paris! Paris outraged, Paris broken, Paris martyred, but Paris liberated! Liberated by itself, liberated by its people with the assistance of the armies of France, with the support and assistance of the whole of France! ... The enemy is faltering but he is not yet beaten. He is still on our soil. It will not suffice that we, with the assistance of our dear and admirable allies, will have chased him from our home in order to be satisfied after what has happened. We want to enter his territory, as is fitting, as conquerors. ... It is for this revenge, this vengeance and this justice, that we will continue to fight until the last day, until the day of the total and complete victory."
That night the Germans launched a massive artillery and air bombardment on Paris by way of revenge, killing over a thousand people and wounding several thousand others. The situation in Paris remained tense, and a few days later de Gaulle, still unsure of the trend of events asked General Eisenhower to send some American troops into Paris as a show of strength. This he did 'not without some satisfaction', and so on 29 August, the U.S. 28th Infantry Division was rerouted from its journey to the front line and paraded down the Champs Elysees.
The same day, Washington and London agreed to accept the position of the Free French. The following day General Eisenhower gave his de facto blessing with a visit to the General in Paris.
Head of the Provisional Government of the Republic, June 1944 – January 1946.
An Allied Military Government for Occupied Territories (AMGOT) should have been implemented in France, but Charles de Gaulle opposed vehemently against the enforcement of this foreign controlled government. De Gaulle founded the Provisional Government of the French Republic (GPRF) in June 1944 in order to prepare the ground for a new constitutional order, that resulted in the Fourth Republic, and to avoid allied military administration. The legitimacy of the GRPF was only recognized on 23 October 1944 by Franklin D. Roosevelt.
With the pre-war parties and many of their leaders discredited, there was little opposition to de Gaulle and his associates forming an interim administration. In order not to be seen as presuming on his position in such austere times, de Gaulle did not use one of the grand official residences such as Hotel de Matignon or the presidential palace on the Elysee, but resided briefly in his old office at the War Ministry. When he was joined by his wife and daughters a short while later, they moved into a small state-owned villa on edge of Bois de Boulogne which had once been set aside for Hermann Göring.
Living conditions immediately after the liberation were even worse than under German rule. A quarter of housing had been damaged or destroyed, basic public services were at a standstill, petrol and electricity was extremely scarce and, apart from the wealthy who could afford high prices, the population had to get by on very little food. Large-scale public demonstrations erupted all over France, protesting the apparent lack of action at improving the supply of food, while in Normandy, bakeries were pillaged. The problem was that although wheat production was around 80% of prewar levels, transport was paralysed over virtually the whole of France. Large areas of track had been destroyed by bombing, most modern equipment, rolling stock, lorries and farm animals had been taken to Germany and all the bridges over the Seine, the Loire and the Rhone between Paris and the sea had been destroyed. The black market pushed real prices to four times the level of 1939, causing the government to print money to try to improve the money supply, which only added to inflation.
On 10 November 1944, Churchill flew to Paris to a reception by De Gaulle and the two together were greeted by thousands of cheering Parisians on the next day. Harold Nicolson stated that Anthony Eden told him that "not for one moment did Winston stop crying, and that he could have filled buckets by the time he received the Freedom of Paris." He said "they yelled for Churchill in a way that he has never heard any crowd yell before. At an official luncheon de Gaulle said, "It is true that we would not have seen [the liberation] if our old and gallant ally England, and all the British dominions under precisely the impulsion and inspiration of those we are honouring today, had not deployed the extraordinary determination to win, and that magnificent courage which saved the freedom of the world. There is no French man or woman who is not touched to the depths of their hearts and souls by this."
Curbing the Communist Resistance.
After the celebrations had died down, de Gaulle began conferring with leading Resistance figures who, with the Germans gone, intended to continue as a political and military force, and asked to be given a government building to serve as their headquarters. The Resistance, in which the Communists were competing with other trends for leadership, had developed its own manifesto for social and political change known as the National Council of the Resistance (CNR) Charter, and wanted special status to enter the army under their own flags, ranks and honours. Despite their decisive support in backing him against Giraud, de Gaulle disappointed some of the Resistance leaders by telling them that although their efforts and sacrifices had been recognised, they had no further role to play and, that unless they joined the regular army, they should lay down their arms and return to civilian life.
Believing them to be a dangerous revolutionary force, de Gaulle moved to break up the liberation committees and other militias. The political outlook of the Communists represented the complete opposite of his own views, and he was concerned at the amount of support they were receiving from the public. The potential power of the Communists also troubled the American government. As early as May 1943, the U.S. Secretary of State Cordell Hull had written to Roosevelt urging him to take action to attempt to curb the rise of Communism in France.
The Provisional Government of the French Republic.
On 10 September 1944, the Provisional Government of the French Republic, or Government of National Unanimity was formed. It included many of de Gaulle's Free French associates such as Gaston Palewski, Claude Guy, Claude Mauriac and Jacques Soustelle, together with members of the main parties, which included the Socialists and a new Christian Democratic Party, the MRP under the leadership of Georges Bidault, who served as Foreign Minister. The president of the prewar Senate Jules Jeanneney was brought back as second-ranking member, but because of their links with Russia, de Gaulle allowed the Communists only two minor positions in his government. While they were now a major political force with over a million members, of the full cabinet of 22 men, only Augustin Laurent and Charles Tillon—who as head of Francs-Tireurs-Partisans had been one of the most active members of the resistance—were given ministries. However, de Gaulle did pardon the Communists' leader Maurice Thorez, who had been sentenced to death "in absentia" by the French government for desertion. On his return home from Russia, Thorez delivered a speech supporting de Gaulle in which he said that for the present, the war against Germany was the only task that mattered.
There were also a number of new faces in the government, including a literary academic, Georges Pompidou, who had written to one of de Gaulle's recruiting agents offering his services, and Jean Monnet, who in spite of his past opposition to the General now recognised the need for unity and served as Commissoner for Economic Planning. Of equal rank to ministers and answerable only to the prime minister, a number of Commissioners of the Republic (Commissaires de la République) were appointed to re-establish the democratic institutions of France and to extend the legitimacy of the provisional government. A number of former Free French associates served as commissioners, including Henri Fréville, Raymond Aubrac and Michel Debré, who was charged with reforming the civil service. Controversially, de Gaulle also appointed Maurice Papon as Commissioner for Aquitaine in spite of his involvement in the deportation of Jews while serving as a senior police official in the Vichy regime during the occupation. Over the years, Papon remained in high official positions but continued to be implicated in controversial events such as the Paris Massacre of 1961, eventually being convicted of crimes against humanity in 1998.
In social policy, legislation was introduced in February 1945 that provided for the establishment of works committees in all private industrial establishments employing more than 50 (originally more than 100) people.
Tour of major cities.
De Gaulle's policy was to postpone elections as long as 2.6 million French were in Germany as prisoners of war and forced labourers. In mid-September, he embarked upon a tour of major provincial cities to increase his public profile and to help cement his position. Although he received a largely positive reception from the crowds who came out to see him, he reflected that only a few months previously the very same people had come out to cheer Marshal Pétain when he was serving the Vichy regime. Raymond Aubrac said that the General showed himself to be ill-at-ease at social functions; in Marseilles and Lyon he displayed great irritation when he was forced to sit next to local Resistance leaders at the post-rally banquet and was particularly scathing at what he regarded as the vulgar displays of exuberance among young men and women during the Maquisard parades which preceded his speech. When he reached Toulouse, de Gaulle also had to confront the leaders of a group which had proclaimed themselves to be the provincial government of the city.
During the tour, de Gaulle showed his customary lack of concern for his own safety by mixing with the crowds and thus making himself an easy target for an assassin. Although he was naturally shy, the good use of amplification and patriotic music enabled him to deliver his message that though all of France was fragmented and suffering, together they would rise again. During every speech he would stop halfway through to invite the crowd to join him in singing "La Marseillaise", before continuing and finishing by raising his hands in the air and crying "Vive la France!"
The legal purges (Épuration légale).
As the war entered its final stages, the nation was forced to confront the reality of how many of its people had behaved under German rule. In France, collaborators were more severely punished than in most other occupied countries. Immediately after the liberation, countless women accused of fraternising with the enemy were publicly shaved in the streets or daubed with feathers, although a significant number were less fortunate, being viciously killed. With so many of their former members having been hunted down and killed by the Nazis and paramilitary Milice, the Partisans had already summarily executed an estimated 4,500 people, and the Communists in particular continued to press for severe action against collaborators. In Paris alone, over 150,000 people were at some time detained on suspicion of collaboration, although most were later released. Famous figures accused included the industrialist Louis Renault, the actress Arletty, who had lived openly with a German officer in the Ritz, the opera star Tino Rossi, the stage actor Sacha Guitry and Coco Chanel, who was briefly detained but fled to Switzerland.
Keenly aware of the need to seize the initiative and to get the process under firm judicial control, de Gaulle appointed Justice Minister François de Menthon to lead the Legal Purge (Épuration légale) to punish traitors and to clear away the traces of the Vichy regime. Knowing that he would need to reprieve many of the 'economic collaborators'—such as police and civil servants who held minor roles under Vichy in order to keep the country running as normally as possible—he assumed, as head of state, the right to commute death sentences. In all, of the near 2,000 people who received the death sentence from the courts, fewer than 800 were actually executed. De Gaulle commuted 998 of the 1,554 capital sentences submitted before him, including all those involving women. Many others were given jail terms or sentenced to national humiliation (loss of civil rights). It is generally agreed that the purges were conducted arbitrarily, with often absurdly severe or overly lenient punishments being handed down. It was also notable that the less well-off people who were unable to pay for lawyers were more harshly treated. As time went by and feelings grew less intense, a number of people who had held fairly senior positions under the Vichy government—such as Maurice Papon and René Bousquet—escaped justice by claiming to have worked secretly for the resistance or to have played a double game, working for the good of France by serving the established order.
Later, there was the question of what to do with the former Vichy leaders when they were finally returned to France. Marshal Pétain and Maxime Weygand were war heroes from World War I and were now extremely old; convicted of treason, Pétain received a death sentence which his old protégé de Gaulle commuted to life imprisonment, while Weygand was eventually acquitted. Three Vichy leaders were executed. Joseph Darnand, who became an SS officer and led the Milice paramilitaries who hunted down members of the Resistance, was executed in October 1945. Fernand de Brinon, the third-ranking Vichy official was found guilty of war crimes and executed in April 1947. The two trials of the most infamous collaborator of all, Pierre Laval, who was heavily implicated in the murder of Jews were widely criticised as being unfair for depriving him of the opportunity to properly defend himself, although Laval antagonised the court throughout with his bizarre behaviour. He was found guilty of treason in May 1945 and de Gaulle was adamant that there would be no commuting the death sentence, saying that Laval's execution was "an indispensable symbolic gesture required for reasons of state". There was a widespread belief, particularly in the years that followed, that de Gaulle was trying to appease both the Third Republic politicians and the former Vichy leaders who had made Laval their scapegoat.
Winter of 1944.
The winter of 1944–45 was especially difficult for most of the population. Inflation showed no sign of slowing down and food shortages were severe. The prime minister and the other Gaullists were forced to try to balance the desires of ordinary people and public servants for a return to normal life with pressure from Bidault's MRP and the Communists for the large scale nationalisation programme and other social changes that formed the main tenets of the CNR Charter. At end of 1944 the coal industry and other energy companies were nationalised, followed shortly afterwards by major banks and finance houses, the merchant navy, the main aircraft manufacturers, airlines and a number of major private enterprises such as the Renault car company at Boulogne-Billancourt, whose owner had been implicated as a collaborator and accused of having made huge profits working for the Nazis. In some cases unions, feeling that things were not progressing quickly enough took matters into their own hands, occupying premises and setting up workers committees to run the companies. Women were also allowed the vote for the first time, a new social security system was introduced to cover most medical costs, unions were expanded and price controls introduced to try to curb inflation. At de Gaulle's request, the newspaper "Le Monde" was founded in December 1944 to provide France with a quality daily journal similar to those in other countries. "Le Monde" took over the premises and facilities of the older "Le Temps", whose independence and reputation had been badly compromised during the Vichy years.
During this period there were a number of minor disagreements between the French and the other Allies. The British ambassador to France Duff Cooper said that during this time de Gaulle seemed to seek out real or imagined insults to take offence at wherever possible. De Gaulle believed that Britain and America were intending to keep their armies in France after the war and were secretly working to take over her overseas possessions and to prevent her from regaining her political and economic strength. In late October he complained that the Allies were failing to adequately arm and equip the new French army and instructed Bidault to use the French veto at the European Council.
On Armistice Day in 1944, Winston Churchill made his first visit to France since the liberation and received a good reception in Paris where he laid a wreath to Clemenceau. The occasion also marked the first official appearance of de Gaulle's wife Yvonne, but the visit was less friendly than it appeared. De Gaulle had instructed that there be no excessive displays of public affection towards Churchill and no official awards without his prior agreement. When crowds cheered Churchill during a parade down the Elysee, de Gaulle was heard to remark, "Fools and cretins! Look at the rabble cheering the old bandit".
Visit to the Soviet Union.
With the Russian forces making more rapid advances into German-held territory than the Allies, there was a sudden public realisation that the Soviet Union was about to dominate large parts of eastern Europe. In fact, at the Cairo and Tehran Conferences in 1943 Britain and America had already agreed to allow Bulgaria, Romania and Hungary to fall under the Russian sphere of influence after the war, with shared influence in Yugoslavia. Britain was to retain hegemony over Greece, although there had been no agreement over Poland, whose eastern territories were already in Russian hands under the Molotov–Ribbentrop Pact with Germany, and which retained a government in exile in London. De Gaulle had not been invited to any of the 'Big Three' Conferences, although the decisions made by Stalin, Churchill and Roosevelt in dividing up Europe were of huge importance to France.
By now it was clear that although for the time being they remained allies, in the coming years the Western capitalist democracies would increasingly clash with the Communist ideology. De Gaulle and his Foreign Minister Bidault stated that they were not in favour of a 'Western Bloc' that would be separate from the rest of Europe, and hoped that a resurgent France might be able to act as a 'third force' in Europe to temper the ambitions of the two emerging superpowers, America and Russia. He began seeking an audience with Stalin to press his 'facing both ways' policy, and finally received an invitation in late 1944. In his memoirs, de Gaulle devoted 24 pages to his visit to Russia, but a number of writers make the point that his version of events differs significantly from that of the Russians, of foreign news correspondents, and with their own eye-witness accounts.
De Gaulle wanted access to German coal in the Ruhr as reparations after the war, the left bank of the Rhine to be incorporated into French territory, and for the Oder-Neisse line in Poland to become Germany's official eastern border. De Gaulle began by requesting that France enter into a treaty with the Soviet Union on this basis, but Stalin, who remained in constant contact with Churchill throughout the visit, said that it would be impossible to make such an agreement without the consent of Britain and America. He suggested that it might be possible to add France's name to the existing Anglo-Soviet Agreement if they agreed to recognise the Soviet-backed provisional Polish government known as the Lublin Committee as rightful rulers of Poland, but de Gaulle refused on the grounds that this would be 'un-French', as it would mean her being a junior partner in an alliance. During the visit, de Gaulle accompanied the deputy Russian leader Vyacheslav Molotov on a tour of the former battleground at Stalingrad, where he was deeply moved at the scene of carnage he witnessed and surprised Molotov by referring to "our joint sacrifice".
Though the treaty which was eventually signed by Bidault and Molotov carried symbolic importance in that it enabled de Gaulle to demonstrate that he was recognised as the official head of state and show that France's voice was being heard abroad, it was of little relevance to Stalin due of France's lack of real political and military power; it did not affect the outcome of the post-war settlement. Stalin later commented that like Churchill and Roosevelt, he found de Gaulle to be awkward and stubborn and believed that he was 'not a complicated person' (by which he meant that he was an old-style nationalist). Stalin also felt that he lacked realism in claiming the same rights as the major powers and did not object to Roosevelt's refusal to allow de Gaulle to attend the 'Big Three' conferences that were to come at Yalta and Potsdam.
Strasbourg.
At the end of 1944 French forces continued to advance as part of the American armies, but during the Ardennes Offensive there was a dispute over Eisenhower's order to French troops to evacuate Strasbourg, which had just been liberated so as to straighten the defensive line against the German counterattack. Strasbourg was an important political and psychological symbol of French sovereignty in Alsace and Lorraine, and de Gaulle, saying that its loss would bring down the government, refused to allow a retreat, predicting that "Strasbourg will be our Stalingrad". At a cabinet meeting he said that the French should be willing to die there alone if the U.S. pulled out its own troops. Churchill backed the French, and Eisenhower was so impressed with the French resolve that he eventually left his own troops in the city even at the risk of being cut off, for which de Gaulle expressed his extreme gratitude.
By early 1945 it was clear that the price controls which had been introduced to control inflation had only served to boost the black market and prices continued to move ever upwards. By this time the army had swelled to over 1.2 million men and almost half of state expenditure was going to military spending. De Gaulle was faced with his first major ministerial dispute when the very able but tough-minded economics minister Pierre Mendes-France demanded a programme of severe monetary reform which was opposed by the Finance Ministry headed by Aime Lepercq, who favoured a programme of heavy borrowing to stimulate the economy. When de Gaulle, knowing there would be little appetite for further austerity measures sided with Lepercq, Mendes-France tendered his resignation, which was rejected because de Gaulle knew he needed him. Lepercq was killed in a road accident a short time afterwards and was succeeded by Pleven, but when in March, Mendes-France asked unsuccessfully for taxes on capital earnings and for the blocking of certain bank accounts, he again offered his resignation and it was accepted.
The Yalta Conference.
De Gaulle was never invited to the summit conferences of Allied leaders such as Yalta and Potsdam. He never forgave the Big Three leaders for the neglect and continued to rage against it as having been a negative factor in European politics for the rest of his life.
After the Rhine crossings, the French First Army captured a large section of territory in southern Germany, but although this later allowed France to play a part in the signing of the German surrender, Roosevelt in particular refused to allow any discussion about de Gaulle participating in the Big Three conferences that would shape Europe in the post war world. Churchill pressed very hard for France to be included 'at the inter-allied table', but on 6 December 1944 the American president wired both Stalin and Churchill to say that de Gaulle's presence would "merely introduce a complicating and undesirable factor".
At the Yalta Conference in February 1945, despite Stalin's opposition, Churchill and Roosevelt insisted that France be allowed a post-war occupation zone in Germany, and also made sure that she was included among the five nations that invited others to the conference to establish the United Nations. This was important because it guaranteed France a permanent seat on the UN Security Council, a prestigious position that despite pressure from emerging nations she still holds today.
President Truman.
On his way back from Yalta, Roosevelt asked de Gaulle to meet him in Algiers for talks. The General refused, believing that there was nothing more to be said, and for this he received a rebuke from Georges Bidault and from the French press, and a severely angered Roosevelt criticised de Gaulle to Congress. Soon after, on 12 April 1945, Roosevelt died, and despite their uneasy relationship de Gaulle declared a week of mourning in France and forwarded an emotional and conciliatory letter to the new American president, Harry S. Truman, in which he said of Roosevelt, "all of France loved him".
De Gaulle's relationship with Truman was to prove just as difficult as it had been with Roosevelt. With Allied forces advancing deep into Germany, another serious situation developed between American and French forces in Stuttgart and Karlsruhe, when French soldiers were ordered to transfer the occupation zones to U.S. troops. Wishing to retain as much German territory in French hands as possible, de Gaulle ordered his troops, who were using American weapons and ammunition, to resist, and an armed confrontation seemed imminent. Truman threatened to cut off supplies to the French army and to take the zones by force, leaving de Gaulle with little choice but to back down. De Gaulle never forgave Truman and hinted he would work closely with Stalin, leading Truman to tell his staff, "I don't like the son of a bitch."
The first visit by de Gaulle to Truman in America was not a success. Truman told his visitor that it was time that the French got rid of the Communist influence from her government, to which de Gaulle replied that this was France's own business. But Truman, who admitted that his feelings towards the French were becoming 'less and less friendly', went on to say that under the circumstances, the French could not expect much economic aid and refused to accept de Gaulle's request for control of the west bank of the Rhine. During the argument which followed, de Gaulle reminded Truman that the U.S. was using the French port of Noumea in New Caledonia as a base against the Japanese.
Victory in Europe.
When, in May 1945 the German armies surrendered to the Americans and British at Rheims, a separate armistice was signed with France in Berlin. De Gaulle refused to allow any British participation in the victory parade in Paris. However, among the vehicles that took part was an ambulance from the Hadfield-Spears Ambulance Unit, staffed by French doctors and British nurses. One of the nurses was Mary Spears, who had set up the unit and had worked almost continuously since the Battle of France with Free French forces in the Middle East, North Africa and Italy. Mary's husband was General Edward Spears, the British liaison to the Free French who had personally spirited de Gaulle to safety in Britain in 1940. When de Gaulle saw the Union Flags and Tricolours side by side on the ambulance, and heard French soldiers cheering, "Voilà Spears! Vive Spears!" he ordered that the unit be closed down immediately and its British staff sent home. A number of French troops returned their medals in protest and Mary wrote, "it is a pitiful business when a great man suddenly becomes small."
Another confrontation with the Americans broke out soon after the armistice when the French sent troops to occupy the French-speaking Italian border region of Val d'Aoste. The French commander threatened to open fire on American troops if they tried to stop them, and an irate Truman ordered the immediate end to all arms shipments to France, and sent de Gaulle an angry letter saying that he found it unbelievable that the French could threaten to attack American troops after they had done so much to liberate France.
However, de Gaulle was generally well received in the United States immediately after World War II and supported the United States in public comments. He visited New York City, on 27 August 1945, to great welcome by the thousands of people of the city and its mayor Fiorello LaGuardia. On that day, De Gaulle wished "Long live the United States of America", visited New York City Hall and Idlewild Airport (now John F. Kennedy International Airport), and presented LaGuardia with the Grand Croix of the Legion of Honour award.
Confrontation in the Levant.
On VE Day, there were also serious riots in French Tunisia, while soon after there came a dispute with Britain over Syria and Lebanon which quickly developed into a major diplomatic incident. The U.S., already alienated from de Gaulle, sided with Britain and a major crisis was at hand.
The two Arab countries, occupying a region known as the Levant, had been ruled under a French mandate given by the League of Nations at the Paris Peace Conference (1919). For several months both countries had seen demonstrations against the French. Bidault announced that France intended to defend her economic and cultural rights in Syria and Lebanon, and that there would be "no problem, as long as the British do not interfere". In May, de Gaulle sent General Beynet to establish an air base in Syria and a naval base in Lebanon, provoking an outbreak of nationalism in which some French nationals were attacked and killed.
On 20 May, French troops opened fire on demonstrators in Damascus with artillery and mortars while aircraft dropped bombs. Later, colonial Senegalese troops were sent in with machine guns and, after several days, hundreds of Syrians lay dead in the bazaars and narrow streets of the capital, with reports of looting by the attacking forces. The British, who had substantial forces in the region, said that they would be forced to intervene if the violence did not stop. They were backed by President Truman, who declared "those French ought to be taken out and castrated".
Following the visit to Paris the previous year, and in spite of his efforts to preserve French interests at Yalta, Churchill's relationship with de Gaulle was now at rock bottom. In January he told a colleague that he believed that de Gaulle was "a great danger to peace and for Great Britain. After five years of experience, I am convinced that he is the worst enemy of France in her troubles ... he is one of the greatest dangers to European peace. ... I am sure that in the long run no understanding will be reached with General de Gaulle".
Finally, on 31 May, with the death toll exceeding a thousand Syrians, Churchill sent de Gaulle a message saying, "In order to avoid a collision between British and French forces, we request you immediately to order French troops to cease fire and withdraw to their barracks". Meanwhile the British, under General Bernard Paget moved in with troops and tanks and cut French General Oliva Roget's telephone line with his base at Beirut. Paget ordered Roget to tell his men to cease fire, but the Frenchman said that he would not take orders from the British. Eventually, heavily outnumbered, Roget ordered his men back to their base near the coast, angry that the British had arrived only after he had "restored order". He told a Syrian journalist, "You are replacing the easygoing French with the brutal British". But that night, with the Syrians killing any French or Senegalese troops they could find, the French were forced to accept the British escort back to the safety of their barracks. Later, Roget was sacked, but a furious row broke out between Britain and France.
In France, there were accusations that Britain had armed the demonstrators. De Gaulle raged against 'Churchill's ultimatum', saying that "the whole thing stank of oil". He summoned Duff Cooper and told him, "I recognise that we are not in a position to wage war against you, but you have betrayed France and betrayed the West. That cannot be forgotten".
France was isolated and suffering a diplomatic crisis. The secretary of the Arab League Edward Atiyah said, "France put all her cards and two rusty pistols on the table". In turn, the French press attacked Britain as an enemy of France and accused the U.S. of helping Italy and Germany more than it helped France. It also criticised the Russians when they made it clear that they believed that France was in the wrong.
The Potsdam Conference.
During the Potsdam Conference in July, to which de Gaulle was again not invited, Churchill was defeated in the British general election and replaced by the Labour leader Clement Attlee. De Gaulle was disappointed in Churchill's electoral loss and wrote that it was a "disgrace suddenly inflicted by the British Nation upon the great man who had so gloriously led her to salvation and victory" and that Churchill's wartime actions had become "inadequate in this era of mediocrity". The Potsdam Conference brought a decision to divide Vietnam, which had been a French colony for over a hundred years, into British and Chinese spheres of influence. Soon after the surrender of Japan in August 1945, de Gaulle sent the French Far East Expeditionary Corps to re-establish French sovereignty in French Indochina, making Admiral d'Argenlieu High Commissioner and General Leclerc commander-in-chief and commander of the expeditionary corps. However, the resistance leaders in Indo-China proclaimed the freedom and independence of Vietnam.
New elections and resignation.
Since the liberation, the only parliament in France had been an enlarged version of the Algiers Consultative Assembly, and at last, in October 1945, elections were held for a new Constituent Assembly whose main task was to provide a new constitution for the Fourth Republic. De Gaulle favoured a strong executive for the nation, but all three of the main parties wished to severely restrict the powers of the president. The Communists wanted an assembly with full constitutional powers and no time limit, whereas de Gaulle, the Socialists and the Popular Republican Movement (MRP) advocated one with a term limited to only seven months, after which the draft constitution would be submitted for another referendum.
In the election, which was run on the basis of proportional representation, the second option was approved by 13 million of the 21 million voters. The big three parties won 75% of the vote, with the Communists winning 158 seats, the MRP 152 seats, the Socialists 142 seats and the remaining seats going to the various far right parties.
On 13 November 1945, the new assembly unanimously elected Charles de Gaulle head of the government, but problems immediately arose when it came to selecting the cabinet, due to his unwillingness once more to allow the Communists any important ministries. The Communists, now the largest party and with their charismatic leader Maurice Thorez back at the helm, were not prepared to accept this for a second time, and a furious row ensued, during which de Gaulle sent a letter of resignation to the speaker of the Assembly and declared that he was unwilling to trust a party that he considered to be an agent of a foreign power (Russia) with authority over the police and armed forces of France.
Eventually, the new cabinet was finalised on 21 November, with the Communists receiving five out of the twenty-two ministries, and although they still did not get any of the key portfolios, Thorez did manage to obtain one of the four prestigious Ministry of State posts.
The old provisional government ruled by decree, but de Gaulle was frustrated by the new Constituent Assembly and its "regime of parties" and believed that the draft constitution placed too much power in the hands of parliament with its shifting party alliances. One of his ministers described him as "a man equally incapable of monopolizing power as of sharing it".
De Gaulle outlined a programme of further nationalisations and a new economic plan which were passed, but a further row came when the Communists demanded a 20% reduction in the military budget. Refusing to 'rule by compromise', de Gaulle once more threatened to resign. There was a general feeling that he was trying to blackmail the assembly into complete subservience by threatening to withdraw his personal prestige which he insisted was what alone kept the ruling coalition together. Although the MRP managed to broker a compromise which saw the budget approved with amendments, it was little more than a stop-gap measure.
Barely two months after forming the new government, de Gaulle abruptly resigned on 20 January 1946. The move was called "a bold and ultimately foolish political ploy", with de Gaulle hoping that as a war hero, he would be soon brought back as a more powerful executive by the French people. However, that did not turn out to be the case. With the war finally over, the initial period of crisis had passed. Although there were still shortages, particularly of bread, France was now on the road to recovery, and de Gaulle suddenly did not seem so indispensable. The Communist publication Combat wrote, "There was no cataclysm, and the empty plate didn't crack".
He was succeeded by Félix Gouin (French Section of the Workers' International, SFIO), then Georges Bidault (MRP) and finally Léon Blum (SFIO).
1946–58: Out of power.
After monopolizing French politics for six years, Charles de Gaulle suddenly dropped out of sight, and returned to his home at Colombey-les-Deux-Eglises to write his war memoirs.
The famous opening paragraph of "Mémoires de guerre" begins by declaring, "All my life, I have had a certain idea of France (une certaine idée de la France)", comparing his country to an old painting of a Madonna, and ends by declaring that, given the divisive nature of French politics, France cannot truly live up to this ideal without a policy of "grandeur". During this period of formal retirement, however, de Gaulle maintained regular contact with past political lieutenants from wartime and "RPF" days, including sympathizers involved in political developments in French Algeria, becoming "perhaps the best-informed man in France".
In April 1947, de Gaulle made a renewed attempt to transform the political scene by creating a "Rassemblement du Peuple Français" (Rally of the French People, or "RPF"), which he hoped would be able to move above the familiar party squabbles of the parliamentary system. Despite the new party's taking 40% of vote in local elections and 121 seats in 1951, lacking its own press and access to television, its support ebbed away. In May 1953, he withdrew again from active politics, though the "RPF" lingered until September 1955.
As with a number of other European countries during this period, France began to suffer the loss of its overseas possessions amid the surge of nationalism which came in the aftermath of World War II. Indochina (now Vietnam, Laos and Cambodia), colonised by France during the mid nineteenth century, had been lost to the Japanese after the defeat of 1940. De Gaulle had intended to hold on to France's Indochina colony, ordering the parachuting of French agents and arms into Indochina in late 1944 and early 1945 with orders to attack the Japanese as American troops hit the beaches. Although de Gaulle had moved quickly to consolidate French control of the territory during his brief first tenure as president in the 1940s, the communist Vietminh under Ho Chi Minh began a determined campaign for independence from 1946 onwards. The French fought a bitter 7 ½ year war (the First Indochina War) to hold onto Indochina; it was largely funded by the United States and grew increasingly unpopular, especially after the stunning defeat at the Battle of Dien Bien Phu. France pulled out that summer under Prime Minister Pierre Mendes-France.
The independence of Morocco and Tunisia was arranged by Mendes-France and proclaimed in March 1956. Meanwhile in Algeria some 350,000 French troops were fighting 150,000 combatants of the Algerian Liberation Movement (FLN). Within a few years, the Algerian war of independence reached a summit in terms of savagery and bloodshed and threatened to spill into metropolitan France itself.
Between 1946 and 1958 the Fourth Republic had 24 separate ministries. Frustrated by the endless divisiveness, de Gaulle famously asked "How can you govern a country which has 246 varieties of cheese?"
1958: Collapse of the Fourth Republic.
The Fourth Republic was tainted by political instability, failures in Indochina and inability to resolve the Algerian question.
On 13 May 1958, settlers seized the government buildings in Algiers, attacking what they saw as French government weakness in the face of demands among the Arab majority for Algerian independence. A "Committee of Civil and Army Public Security" was created under the presidency of General Jacques Massu, a Gaullist sympathiser. General Raoul Salan, Commander-in-Chief in Algeria, announced on radio that he was assuming provisional power, and appealed for confidence in himself.
At a 19 May press conference, de Gaulle asserted again that he was at the disposal of the country. As a journalist expressed the concerns of some who feared that he would violate civil liberties, de Gaulle retorted vehemently: "Have I ever done that? On the contrary, I have re-established them when they had disappeared. Who honestly believes that, at age 67, I would start a career as a dictator?" A constitutionalist by conviction, he maintained throughout the crisis that he would accept power only from the lawfully constituted authorities. De Gaulle did not wish to repeat the difficulty the Free French movement experienced in establishing legitimacy as the rightful government. He told an aide that the rebel generals "will not find De Gaulle in their baggage".
The crisis deepened as French paratroops from Algeria seized Corsica and a landing near Paris was discussed (Operation Resurrection). Political leaders on many sides agreed to support the General's return to power, except François Mitterrand, Pierre Mendès-France, Alain Savary, the Communist Party, and certain other leftists. On 29 May the French president, René Coty, appealed to the "most illustrious of Frenchmen" to confer with him and to examine what was immediately necessary for the creation of a government of national safety, and what could be done to bring about a profound reform of the country's institutions.
De Gaulle remained intent on replacing the constitution of the Fourth Republic, which he blamed for France's political weakness. He set as conditions for his return that he be given wide emergency powers for six months and that a new constitution be proposed to the French people. On 1 June 1958, de Gaulle became Premier and was given emergency powers for six months by the National Assembly, fulfilling his desire for parliamentary legitimacy.
On 28 September 1958, a referendum took place and 79.2 percent of those who voted supported the new constitution and the creation of the Fifth Republic. The colonies (Algeria was officially a part of France, not a colony) were given the choice between immediate independence and the new constitution. All African colonies voted for the new constitution and the replacement of the French Union by the French Community, except Guinea, which thus became the first French African colony to gain independence and immediately lost all French assistance.
1958–62: Founding of the Fifth Republic.
In the November 1958 elections, de Gaulle and his supporters (initially organised in the "Union pour la Nouvelle République-Union Démocratique du Travail", then the "Union des Démocrates pour la Vème République", and later still the "Union des Démocrates pour la République", UDR) won a comfortable majority. In December, de Gaulle was elected President by the electoral college with 78% of the vote, and inaugurated in January 1959.
On 6 November 1958, in Paris, now President de Gaulle presented to his old wartime ally Winston Churchill the Croix de la Libération. De Gaulle remarked: "I want Sir Winston to know this. Today’s ceremony means that France remembers what she owes him. I want him to know this: the man who has just had the honour of bestowing this distinction upon him values and admires him more than ever." De Gaulle and Churchill maintained a friendly relationship after World War II. Upon Churchill’s death on 24 January 1965 de Gaulle wrote to Queen Elizabeth, “In the great drama he was the greatest of all.
De Gaulle oversaw tough economic measures to revitalise the country, including the issuing of a new franc (worth 100 old francs). Internationally, he rebuffed both the United States and the Soviet Union, pushing for an independent France with its own nuclear weapons, and strongly encouraged a "Free Europe", believing that a confederation of all European nations would restore the past glories of the great European empires.
He set about building Franco-German cooperation as the cornerstone of the European Economic Community (EEC), paying the first state visit to Germany by a French head of state since Napoleon. In January 1963, Germany and France signed a treaty of friendship, the Élysée Treaty. France also reduced its dollar reserves, trading them for gold from the U.S. government, thereby reducing American economic influence abroad.
On 23 November 1959, in a speech in Strasbourg, de Gaulle announced his vision for Europe:
Oui, c'est l'Europe, depuis l'Atlantique jusqu'à l'Oural, c'est toute l'Europe, qui décidera du destin du monde.
"("Yes, it is Europe, from the Atlantic to the Urals, it is the whole of Europe, that will decide the destiny of the world.")"
His expression, "Europe, from the Atlantic to the Urals", has often been cited throughout the history of European integration. It became, for the next ten years, a favourite political rallying cry of de Gaulle's. His vision stood in contrast to the Atlanticism of the United States and Britain, preferring instead a Europe that would act as a third pole between the United States and the Soviet Union. By including in his ideal of Europe all the territory up to the Urals, de Gaulle was implicitly offering détente to the Soviets.
Algeria.
Upon becoming president, de Gaulle was faced with the urgent task of finding a way to bring to an end the bloody and divisive war in Algeria. 
His intentions were obscure. He had immediately visited Algeria and declared, "Je vous ai compris"—'I have understood you', and each competing interest had wished to believe it was them that he had understood. The settlers assumed he supported them, and would be stunned when he did not. In Paris, the left wanted independence for Algeria. Although the military's near-coup had contributed to his return to power, de Gaulle soon ordered all officers to quit the rebellious Committees of Public Safety. Such actions greatly angered the French settlers and their military supporters.
He faced uprisings in Algeria by French settlers and troops. On assuming the prime minister role in June 1958 he immediately went to Algeria, and neutralised the army there, with its 600,000 soldiers. The Algiers Committee of Public Safety was loud in its demands on behalf of the settlers, but De Gaulle made more visits and sidestepped them. For the long term he devised a plan to modernize Algeria's traditional economy, deescalated the war, and offered Algeria self-determination in 1959. A settlers revolt in 1960 failed, while another Army attempted coup failed in April 1961. French voters approved his course in a 1961 referendum on Algerian self-determination. De Gaulle arranged a cease-fire in Algeria with the March 1962 Evian Accords, legitimated by another referendum a month later. It gave victory by the FLN, which came to power and declared independence. The long crisis was over.
DeGaulle was targeted for death by the settlers' resistance group Organisation de l'armée secrète (OAS) and several assassination attempts were made on him; the most famous is that of 22 August 1962, when he and his wife narrowly escaped when their limosine was targeted by machine gun fire arranged by Colonel Jean-Marie Bastien-Thiry at Petit-Clamart. 
Although the Algerian issue was settled, Prime Minister Michel Debré resigned over the final settlement and was replaced with Georges Pompidou on 14 April 1962. France recognised Algerian independence on 3 July 1962, while a blanket amnesty law was belatedly voted in 1968, covering all crimes committed by the French army during the war. In just a few months in 1962, 900,000 French settlers left the country. After 5 July, the exodus accelerated in the wake of the French deaths during the Oran massacre of 1962
Direct presidential elections.
In September 1962, de Gaulle sought a constitutional amendment to allow the president to be directly elected by the people and issued another referendum to this end. After a motion of censure voted by the parliament on 4 October 1962, de Gaulle dissolved the National Assembly and held new elections. Although the left progressed, the Gaullists won an increased majority—this despite opposition from the Christian democratic Popular Republican Movement (MRP) and the National Centre of Independents and Peasants (CNIP) who criticised de Gaulle's euroscepticism and presidentialism.
De Gaulle's proposal to change the election procedure for the French presidency was approved at the referendum on 28 October 1962 by more than three-fifths of voters despite a broad "coalition of no" formed by most of the parties, opposed to a presidential regime. Thereafter the president was to be elected by direct universal suffrage for the first time since Louis Napoleon in 1848.
1962–68: Politics of grandeur.
With the Algerian conflict behind him, de Gaulle was able to achieve his two main objectives, the reform and development of the French economy, and the promotion of an independent foreign policy and a strong presence on the international stage. This was named by foreign observers the "politics of grandeur" ("politique de grandeur"). See Gaullism.
"Thirty glorious years".
In the immediate post-war years France was in a bad way; wages remained at around half prewar levels, the winter of 1946–1947 did extensive damage to crops—leading to a reduction in the bread ration, hunger and disease remained rife and the black market continued to flourish. Germany was in an even worse position but after 1948 things began to improve dramatically with the introduction of Marshall Aid—large scale American financial assistance given to help rebuild European economies and infrastructure. This laid the foundations of a meticulously planned program of investments in energy, transport and heavy industry, overseen by the government of Prime Minister Georges Pompidou.
In the context of a population boom unseen in France since the 18th century, the government intervened heavily in the economy, using "dirigisme"—a unique combination of capitalism and state-directed economy—with indicative five-year plans as its main tool. This brought about a rapid transformation and expansion of the French economy.
High-profile projects, mostly but not always financially successful, were launched: the extension of Marseille's harbour (soon ranking third in Europe and first in the Mediterranean); the promotion of the Caravelle passenger jetliner (a predecessor of Airbus); the decision to start building the supersonic Franco-British Concorde airliner in Toulouse; the expansion of the French auto industry with state-owned Renault at its centre; and the building of the first motorways between Paris and the provinces.
Aided by these projects, the French economy recorded growth rates unrivalled since the 19th century. In 1964, for the first time in nearly 100 years France's GDP overtook that of the United Kingdom. This period is still remembered in France with some nostalgia as the peak of the "Trente Glorieuses" ("Thirty Glorious Years" of economic growth between 1945 and 1974).
In 1967, De Gaulle decreed a law that obliged all firms over certain sizes to distribute a small portion of their profits to their employees. By 1974, as a result of this measure, French employees received an average of 700 francs per head, equivalent to 3.2% of their salary.
Fourth nuclear power.
During his first tenure as president, de Gaulle became enthusiastic about the possibilities of nuclear power. France had carried out important work in the early development of atomic energy and in October 1945 he established the French Atomic Energy Commission Commissariat à l'énergie atomique, (CEA) responsible for all scientific, commercial, and military uses of nuclear energy. However, partly due to communist influences in government opposed to proliferation, research stalled and France was excluded from American, British and Canadian nuclear efforts.
By October 1952 Britain had become the third country—after America and the Soviet Union—to independently test and develop nuclear weapons. This gave Britain the capability to launch a nuclear strike via its Vulcan bomber force and it began developing its own ballistic missile programme known as Blue Streak.
As early as April 1954 while out of power, de Gaulle had proposed that France should also have its own nuclear weapons; at the time nuclear weapons were seen as a national status symbol and a way of maintaining international prestige with a place at the 'top table' of the United Nations. Full-scale research began again in late 1954 when Prime Minister Pierre Mendes-France authorized a plan to develop the atomic bomb; large deposits of uranium had been discovered near Limoges in central France, providing the researchers with an unrestricted supply of nuclear fuel. France's independent Force de Frappe (strike force) came into being soon after de Gaulle's election with his authorisation for the first nuclear test.
With the cancellation of Blue Streak, the U.S. agreed to supply Britain with its Skybolt and later Polaris weapons systems, and in 1958 the two nations signed the Mutual Defence Agreement forging close links which have seen the U.S. and UK cooperate on nuclear security matters ever since. Although at the time it was still a full member of NATO, France proceeded to develop its own independent nuclear technologies—this would enable it to become a partner in any reprisals and would give it a voice in matters of atomic control.
After six years of development, on 13 February 1960 France became the world's fourth nuclear power when a high-powered nuclear device was exploded in the Sahara some 700 miles south-south-west of Algiers. In August 1963 France decided against signing the Partial Test Ban Treaty designed to slow the arms race because it would have prohibited her from testing nuclear weapons above ground. France continued to carry out tests at the Algerian site until 1966, under an agreement with the newly independent Algeria. France's testing program then moved to the Mururoa and Fangataufa Atolls in the South Pacific.
In November 1967, an article by the French Chief of the General Staff (but inspired by de Gaulle) in the "Revue de la Défense Nationale" caused international consternation. It was stated that French nuclear force should be capable of firing "in all directions"—thus including even America as a potential target. This surprising statement was intended as a declaration of French national independence, and was in retaliation to a warning issued long ago by Dean Rusk that U.S. missiles would be aimed at France if it attempted to employ atomic weapons outside an agreed plan. However, criticism of de Gaulle was growing over his tendency to act alone with little regard for the views of others. In August, concern over de Gaulle's policies had been voiced by Valéry Giscard d'Estaing when he queried 'the solitary exercise of power'.
NATO.
With the onset of the Cold War and the perceived threat of invasion from the Soviet Union and the countries of the eastern bloc, the United States, Canada and a number of western European countries set up the North Atlantic Treaty Organisation (NATO) to co-ordinate a military response to any possible attack. France played a key role during the early days of the organisation, providing a large military contingent and agreeing—after much soul-searching—to the participation of West German forces. But after his election in 1958 Charles de Gaulle took the view that the organisation was too dominated by the U.S. and UK, and that America would not fulfill its promise to defend Europe in the event of a Russian invasion.
De Gaulle demanded political parity with Britain and America in NATO, and for its geographic coverage to be extended to include French territories abroad, including Algeria, then experiencing civil war. This was not forthcoming, and so in March 1959 France, citing the need for it to maintain its own independent military strategy, withdrew its Mediterranean Fleet (ALESCMED) from NATO, and a few months later de Gaulle demanded the removal of all U.S. nuclear weapons from French territory.
De Gaulle hosted a superpower summit on 17 May 1960 for arms limitation talks and détente efforts in the wake of the 1960 U-2 Incident between United States President Dwight Eisenhower, Soviet Premier Nikita Khrushchev, and United Kingdom Prime Minister Harold Macmillan. De Gaulle's warm relations with Eisenhower were noticed by United States military observers at that time. De Gaulle told Eisenhower: "Obviously you cannot apologize but you must decide how you wish to handle this. I will do everything I can to be helpful without being openly partisan." When Khrushchev condemned the United States U-2 flights, de Gaulle expressed to Khrushchev his disapproval of 18 near-simultaneous secret Soviet satellite overflights of French territory; Khrushchev denied knowledge of the satellite overflights. Lieutenant General Vernon A. Walters wrote that after Khrushchev left, "De Gaulle came over to Eisenhower and took him by the arm. He took me also by the elbow and, taking us a little apart, he said to Eisenhower, 'I do not know what Khrushchev is going to do, nor what is going to happen, but whatever he does, I want you to know that I am with you to the end.' I was astounded at this statement, and Eisenhower was clearly moved by his unexpected expression of unconditional support". General Walters was struck by de Gaulle's "unconditional support" of the United States during that "crucial time". De Gaulle then tried to revive the talks by inviting all the delegates to another conference at the Champs Elysee Palace to discuss the situation, but the summit ultimately dissolved in the wake of the U-2 incident.
In 1964, de Gaulle visited the Soviet Union, where he hoped to establish France as an alternative influence in the Cold War. Later, he proclaimed a new alliance between the nations, but although the Soviet statesman Alexei Kosygin made a return visit to France, the Russians did not accept France as a super power, knowing that in any future conflict they would have to rely on the overall protection of the Western alliance. In 1965, de Gaulle pulled France out of SEATO, the southeast Asian equivalent of NATO and refused to participate in any future NATO maneuvers.
In February 1966, France withdrew from the NATO Military Command Structure, but remained within the organisation. De Gaulle, haunted by the memories of 1940, wanted France to remain the master of the decisions affecting it, unlike in the 1930s, when it had to follow in step with its British ally. He also declared that all foreign military personnel had to leave French territory and gave them one year to redeploy. This latter action was particularly badly received in the U.S., prompting Dean Rusk, the U.S. Secretary of State, to ask de Gaulle whether the removal of American military personnel was to include exhumation the 50,000 American war dead buried in French cemeteries.
European Economic Community (EEC).
Britain experienced a difficult time in the post-war world, while France and other European countries were enjoying booming economies, Britain experienced high inflation, stagnant growth and poor labour relations. A number of her important colonial possessions—not least India—quickly gained independence, and following the Suez Crisis, where Britain and France unsuccessfully sought to prevent the Egyptians from nationalising the Suez Canal, Britain struggled to adjust to its reduced world position. The U.S. Secretary of State Dean Acheson commented that Britain had "lost an empire and had not yet found a role".
France meanwhile, experiencing the disintegration of her own empire and severe problems in Algeria, turned towards Europe after Suez, and to West Germany in particular. In the years after, the economies of both nations integrated and they led the drive towards European unity.
One of the conditions of Marshall Aid was that the nations' leaders must co-ordinate economic efforts and pool the supply of raw materials. By far the most critical commodities in driving growth were coal and steel. France assumed it would receive large amounts of high-quality German coal from the Ruhr as reparations for the war, but America refused to allow this, fearing a repetition of the bitterness after the Treaty of Versailles which partly caused World War II.
Under the inspiration of the French statesmen Jean Monnet and Robert Schuman, together with the German leader Konrad Adenauer, the rift between the two nations had begun to heal and along with Italy and the Benelux countries, they formed the European Coal and Steel Community, which following the Treaty of Rome of 1957 became the European Economic Community, also known as the Common Market, beginning around the same time as de Gaulle's presidency. Though he had not been instrumental in setting up the new organisation, de Gaulle spoke enthusiastically of his vision of "an imposing confederation" of European states and of formulating a common European foreign policy.
De Gaulle, who in spite of recent history admired Germany and spoke excellent German, as well as English, established a good relationship with the aging West German Chancellor Konrad Adenauer—culminating in the Elysee Treaty in 1963—and in the first few years of the Common Market, France's industrial exports to the other five members tripled and its farm export almost quadrupled. The franc became a solid, stable currency for the first time in half a century, and the economy mostly boomed. Adenauer however, all too aware of the importance of American support in Europe, gently distanced himself from the general's more extreme ideas, wanting no suggestion that any new European community would in any sense challenge or set itself at odds with the U.S. In Adenauer's eyes, the support of the U.S. was more important than any question of European prestige. Adenauer was also anxious to reassure Britain that nothing was being done behind her back and was quick to inform British Prime Minister Harold Macmillan of any new developments.
Great Britain initially declined to join the Common Market, preferring to remain with another organisation known as the European Free Trade Area, mostly consisting of the northern European countries and Portugal. By the late 1950s German and French living standards began to exceed those in Britain, and the government of Harold Macmillan, realising that the EEC was a stronger trading bloc than EFTA, began negotiations to join.
De Gaulle vetoed the British application to join the European Economic Community (EEC) in 1963, famously uttering the single word 'non' into the television cameras at the critical moment, a statement used to sum up French opposition and belligerence towards Britain for many years afterwards. Macmillan said afterwards that he always believed that de Gaulle would prevent Britain joining, but thought he would do it quietly, behind the scenes. He later complained privately that "all our plans are in tatters".
One reason given for de Gaulle's refusal was that he saw Britain as a "Trojan Horse" for the USA, as Churchill once said to him that if he had the choice between France and the United States, he would always choose the United States. As it appears that British prime Minister Macmillan prioritised the rebuilding of the Anglo-American “Special Relationship”, with the recent American agreement to supply Britain with the Skybolt nuclear missile, it persuaded de Gaulle that the United Kingdom lacked the necessary political will to adhere to his bid for a West European strategic independence from the United States. He maintained there were incompatibilities between continental European and British economic interests. In addition, he demanded that the United Kingdom accept all the conditions laid down by the six existing members of the EEC (Belgium, France, West Germany, Italy, Luxembourg, Netherlands) and revoke its commitments to countries within its own free trade area (which France had not done with its own). He supported a deepening and an acceleration of Common Market integration rather than an expansion.
However, in this latter respect, a detailed study of the formative years of the EEC argues that the defence of French economic interests, especially in agriculture, in fact played a more dominant role in determining de Gaulle's stance towards British entry than the various political and foreign policy considerations that have often been cited.
Dean Acheson believed that Britain made a grave error in not signing up to the European idea right from the start, and that they continued to suffer the political consequences for at least two decades afterwards. However he also stated his belief that de Gaulle used the 'Common Market' (as it was then termed) as an "exclusionary device to direct European trade towards the interest of France and against that of the United States, Britain and other countries."
Claiming continental European solidarity, de Gaulle again rejected British entry when they next applied to join the community in December 1967 under the Labour leadership of Harold Wilson. During negotiations, de Gaulle chided Britain for relying too much on the Americans, saying that sooner or later they would always do what was in their best interests. Wilson said he then gently raised the spectre of the threat of a newly powerful Germany as a result of the EEC, which de Gaulle agreed was a risk. After de Gaulle left office the United Kingdom applied again and finally became a member of the EEC in January 1973.
Recognition of the People's Republic of China.
De Gaulle was convinced that a strong and independent France could act as a balancing force between the United States and the Soviet Union, a policy seen as little more than posturing and opportunism by his critics, particularly in Britain and the United States, to which France was formally allied. In January 1964, France established diplomatic relations with the People's Republic of China (PRC) – the first step towards formal recognition. This was done without first severing links with the Republic of China (Taiwan), led by Chiang Kai-shek. Hitherto the PRC had insisted that all nations abide by a "one China" condition, and at first it was unclear how the matter would be settled. However, the agreement to exchange ambassadors was subject to a delay of three months and in February, Chiang Kai-shek resolved the problem by cutting off diplomatic relations with France. Eight years later, U.S. President Richard Nixon visited the PRC and began normalising relations—a policy which was confirmed in the Shanghai Communiqué of 28 February 1972.
As part of a European tour, Nixon visited France in 1969. He and de Gaulle both shared the same non-Wilsonian approach to world affairs, believing in nations and their relative strengths, rather than in ideologies, international organisations, or multilateral agreements. De Gaulle is famously known for calling the UN the pejorative "" ("the thingamajig").
Visit to Latin America.
In September and October 1964, despite a recent operation for prostate cancer and fears for his security, he set out on a punishing 20,000-mile tour of Latin America. He had visited Mexico the previous year and spoke, in Spanish, to the Mexican people on the eve of their celebrations of their independence at the Palacio Nacional in Mexico City. During his new 26-day visit, he was again keen to gain both cultural and economic influence. He spoke constantly of his resentment of U.S. influence in Latin America—"that some states should establish a power of political or economic direction outside their own borders". Yet France could provide no investment or aid to match that from Washington.
U.S. dollar crisis.
In the Bretton Woods system put in place in 1944, U.S. dollars were convertible to gold. In France, it was called "America's exorbitant privilege" as it resulted in an "asymmetric financial system" where foreigners "see themselves supporting American living standards and subsidizing American multinationals". As American economist Barry Eichengreen summarized:"It costs only a few cents for the Bureau of Engraving and Printing to produce a $100 bill, but other countries had to pony up $100 of actual goods in order to obtain one". In February 1965 President Charles de Gaulle announced his intention to exchange its U.S. dollar reserves for gold at the official exchange rate. He sent the French Navy across the Atlantic to pick up the French reserve of gold and was followed by several countries. As it resulted in considerably reducing U.S. gold stock and U.S. economic influence, it led U.S. President Richard Nixon to unilaterally end the convertibility of the dollar to gold on August 15, 1971 (the "Nixon Shock"). This was meant to be a temporary measure but the dollar became permanently a floating fiat money and in October 1976, the U.S. government officially changed the definition of the dollar; references to gold were removed from statutes.
Second term.
In December 1965, de Gaulle returned as president for a second seven-year term. In the first round he did not win the expected majority, only receiving 45% of the vote. Both of his main rivals did better than expected; the leftist François Mitterrand received 32% and Jean Lecanuet, who advocated for what "Life" described as "Gaullism without de Gaulle", received 16%. De Gaulle won a majority in the second round, with Mitterrand receiving 45%.
In September 1966, in a famous speech in Phnom Penh (Cambodia), he expressed France's disapproval of the U.S. involvement in the Vietnam War, calling for a U.S. withdrawal from Vietnam as the only way to ensure peace. However, de Gaulle conversed frequently with George Ball, United States President Lyndon Johnson's Under Secretary of State, and told Ball that he feared that the United States risked repeating France's tragic experience in Vietnam, which de Gaulle called "ce pays pourri" ("the rotten country"). Ball later sent a 76-page memorandum to Johnson critiquing Johnson's current Vietnam policy in October 1964.
De Gaulle later visited Guadeloupe, in the aftermath of Hurricane Inez for two days, bringing aid which totalled billions of francs.
Empty Chair Crisis.
During the establishment of the European Community, de Gaulle helped precipitate one of the greatest crises in the history of the EEC, the "Empty Chair Crisis". It involved the financing of the Common Agricultural Policy, but almost more importantly the use of qualified majority voting in the EC (as opposed to unanimity). In June 1965, after France and the other five members could not agree, de Gaulle withdrew France's representatives from the EC. Their absence left the organisation essentially unable to run its affairs until the Luxembourg compromise was reached in January 1966. De Gaulle succeeded in influencing the decision-making mechanism written into the Treaty of Rome by insisting on solidarity founded on mutual understanding. He vetoed Britain's entry into the EEC a second time, in June 1967.
Six-Day War.
With tension rising in the Middle East in 1967, de Gaulle on 2 June declared an arms embargo against Israel, just three days before the outbreak of the Six-Day War. This, however, did not affect spare parts for the French military hardware with which the Israeli armed forces were equipped.
This was an abrupt change in policy. In 1956 France, Britain and Israel had cooperated in an elaborate effort to retake the Suez Canal from Egypt. Israel's air force operated French Mirage and Mystère jets in the Six-Day War, and its navy was building its new missile boats in Cherbourg. Though paid for, their transfer to Israel was now blocked by de Gaulle's government. But they were smuggled out in an operation that drew further denunciations from the French government. The last boats took to the sea in December 1969, directly after a major deal between France and now-independent Algeria exchanging French armaments for Algerian oil.
Under de Gaulle, following the independence of Algeria, France embarked on foreign policy more favourable to the Arab side. President de Gaulle's position in 1967 at the time of the Six Day War played a part in France's newfound popularity in the Arab world. Israel turned towards the United States for arms, and toward its own industry.
In a televised news conference on 27 November 1967, de Gaulle described the Jewish people as "this elite people, sure of themselves and domineering". In his letter to David Ben-Gurion dated 9 January 1968, he explained that he was convinced that Israel had ignored his warnings and overstepped the bounds of moderation by taking possession of Jerusalem, and so much Jordanian, Egyptian, and Syrian territory by force of arms. He felt Israel had exercised repression and expulsions during the occupation and that it amounted to annexation. He said that provided Israel withdrew her forces, it appeared that it might be possible to reach a solution through the UN framework which could include assurances of a dignified and fair future for refugees and minorities in the Middle East, recognition from Israel's neighbours, and freedom of navigation through the Gulf of Aqaba and the Suez Canal.
Nigerian Civil War.
The Eastern Region of Nigeria declared itself independent under the name of the Independent Republic of Biafra on 30 May 1967. On 6 July the first shots in the Nigerian Civil War were fired, marking the start of a conflict that lasted until January 1970. Britain provided military aid to the Federal Republic of Nigeria—yet more was made available by the Soviet Union. Under de Gaulle's leadership, France embarked on a period of interference outside the traditional French zone of influence. A policy geared toward the break-up of Nigeria put Britain and France into opposing camps. Relations between France and Nigeria had been under strain since the third French nuclear explosion in the Sahara in December 1960. From August 1968, when its embargo was lifted, France provided limited and covert support to the breakaway province. Although French arms helped to keep Biafra in action for the final 15 months of the civil war, its involvement was seen as insufficient and counterproductive. The Biafran chief of staff stated that the French "did more harm than good by raising false hopes and by providing the British with an excuse to reinforce Nigeria."
"Vive le Québec libre!".
In July 1967, de Gaulle visited Canada, which was celebrating its centenary with a world fair in Montreal, Expo 67. On 24 July, speaking to a large crowd from a balcony at Montreal's city hall, de Gaulle shouted "Vive le Québec libre!" (Long live free Quebec!) then added, "Vive le Canada français!" (Long live French Canada!), and finally, "Et vive la France." (And long live France!) The Canadian media harshly criticized the statement, and the Prime Minister of Canada, Lester B. Pearson, stated that "Canadians do not need to be liberated." De Gaulle left Canada abruptly two days later, without proceeding to Ottawa as scheduled. He never returned to Canada. The speech offended many English-speaking Canadians and was heavily criticized in France as well, and led to a significant diplomatic rift between the two countries. However, the event was seen as a watershed moment by the Quebec sovereignty movement, and is still a significant milestone of Quebec's history to the eyes of most French Canadians.
In the following year, de Gaulle visited Brittany, where he declaimed a poem written by his uncle (also called Charles de Gaulle) in the Breton language. The speech followed a series of crackdowns on Breton nationalism. De Gaulle was accused of hypocrisy, on the one hand supporting a "free" Quebec because of linguistic and ethnic differences from other Canadians, while on the other hand suppressing a regional and ethnic nationalist movement in Brittany.
May 1968.
De Gaulle's government was criticised within France, particularly for its heavy-handed style. While the written press and elections were free, and private stations such as Europe 1 were able to broadcast in French from abroad, the state's ORTF had a monopoly on television and radio. This monopoly meant that the executive was in a position to bias the news. In many respects, Gaullist France was conservative, Catholic, and there were few women in high-level political posts (in May 1968, the executive was 100% male). Many factors contributed to a general weariness of sections of the public, particularly the student youth, which led to the events of May 1968.
The huge demonstrations and strikes in France in May 1968 severely challenged de Gaulle's legitimacy. He and other government leaders feared that the country was on the brink of revolution or civil war. On 29 May, de Gaulle disappeared without notifying Prime Minister Pompidou or anyone else in the government, stunning the country. He fled to Baden-Baden, Germany to meet with General Massu, now head of the French military there, to discuss possible army intervention against the protesters. De Gaulle returned to France after being assured of the military's support, in return for which De Gaulle agreed to amnesty for the 1961 coup plotters and OAS members.
In a private meeting discussing the students' and workers' demands for direct participation in business and government he coined the phrase "La réforme oui, la chienlit non", which can be politely translated as 'reform yes, masquerade/chaos no.' It was a vernacular scatological pun meaning 'chie-en-lit, no' (crap-in-bed, no). The term is now common parlance in French political commentary, used both critically and ironically referring back to de Gaulle.
But de Gaulle offered to accept some of the reforms the demonstrators sought. He again considered a referendum to support his moves, but on 30 May, Pompidou persuaded him to dissolve parliament (in which the government had all but lost its majority in the March 1967 elections) and hold new elections instead. The June 1968 elections were a major success for the Gaullists and their allies; when shown the spectre of revolution or civil war, the majority of the country rallied to him. His party won 352 of 487 seats, but de Gaulle remained personally unpopular; a survey conducted immediately after the crisis showed that a majority of the country saw him as too old, too self-centred, too authoritarian, too conservative, and too anti-American. The general consensus was that de Gaulle was out of touch with the average Frenchman, preoccupying himself with military and foreign affairs while giving scant attention to domestic issues.
Retirement.
Charles de Gaulle resigned the presidency at noon, 28 April 1969, following the rejection of his proposed reform of the Senate and local governments in a nationwide referendum. De Gaulle vowed that if the referendum failed, he would resign his office. Despite an eight-minute-long speech by de Gaulle, the referendum failed and he duly resigned. Two months later Georges Pompidou was elected as his successor.
De Gaulle retired once again to his beloved nine-acre country estate, La Boisserie (the woodland glade), in Colombey-les-Deux-Églises, 120 miles southeast of Paris. There the General, who often described old age as a "shipwreck," continued his memoirs, dictated to his secretary from notes. To visitors, de Gaulle said, "I will finish three books, if God grants me life." "The Renewal", the first of three planned volumes to be called "Memoirs of Hope", was quickly finished and immediately became the fastest seller in French publishing history. During the day he also usually took two strolls, one alone and the other with his wife Yvonne around the village.
He did not accept the sizable pensions to which he was entitled as a retired president and as a retired general, but only a much smaller colonel's pension. He was punctilious with regard to money, taking care to separate his private expenses from those of his official function. He paid for his own haircuts and the stamps for personal correspondence, and had an electricity meter installed in the private accommodation at his official residence.
Private life.
Charles de Gaulle married Yvonne Vendroux on 7 April 1921. They had three children: Philippe (born 1921), Élisabeth (1924–2013), who married General Alain de Boissieu, and Anne (1928–1948). Anne had Down's syndrome and died of pneumonia at the age of 20. De Gaulle always had a particular love for Anne; one Colombey resident recalled how he used to walk with her hand-in-hand around the property, caressing her and talking quietly about the things she understood.
Like her husband, Yvonne de Gaulle was a conservative Catholic, and campaigned against prostitution, the sale of pornography in newsstands and the televised display of nudity and sex, for which she earned the nickname "Tante ("Auntie") Yvonne." Later she unsuccessfully tried to persuade de Gaulle to outlaw miniskirts in France.
Charles de Gaulle had an older brother Xavier (1887–1955) and sister Marie-Agnes (1889–1983), and two younger brothers, Jacques (1893–1946) and Pierre (1897–1959). He was particularly close to the youngest, Pierre, who so resembled him that presidential bodyguards often saluted him by mistake when he visited his famous brother or accompanied him on official visits.
One of Charles de Gaulle's grandsons, also named Charles de Gaulle, was a member of the European Parliament from 1994 to 2004, his last tenure being for the National Front. The younger Charles de Gaulle's move to the anti-gaullist Front National was widely condemned by other family members, in open letters and newspaper interviews. "It was like hearing the pope had converted to Islam", one said. Another grandson, Jean de Gaulle, was a member of the French parliament until his retirement in 2007.
Death.
On 9 November 1970, two weeks short of what would have been his 80th birthday, Charles de Gaulle died suddenly, despite enjoying very robust health his entire life (except for a prostate operation a few years earlier). He had been watching the evening news on television and playing Solitaire around 7:40pm when he suddenly pointed to his neck and said, "I feel a pain right here", and then collapsed. His wife called the doctor and the local priest, but by the time they arrived he had died from a ruptured blood vessel.
His wife asked that she be allowed to inform her family before the news was released. She was able to contact her daughter in Paris quickly, but their son, who was in the navy, was difficult to track down, so President Georges Pompidou was not informed until 4am the next morning and announced the general's death on television some 18 hours after the event. He simply said, "General de Gaulle is dead. France is a widow."
De Gaulle had made arrangements that insisted his funeral be held at Colombey, and that no presidents or ministers attend his funeral—only his "Compagnons de la Libération".
Despite his wishes, such were the number of foreign dignitaries who wanted to honour de Gaulle that Pompidou was forced to arrange a separate memorial service at the Notre-Dame Cathedral, to be held at the same time as his actual funeral. Among those at the memorial service were 63 present or former heads of state, including American president Richard Nixon, Soviet president Nikolai Podgorny, British prime minister Edward Heath, Yugoslavian president Josip Broz Tito, Indian prime minister Indira Gandhi, Shah of Iran Mohammad Reza Pahlavi, Thai king Bhumibol Adulyadej, Cuban president Fidel Castro, Swedish prime minister Olof Palme, the president of Italy, representatives of 17 of France's former African colonies and the reigning monarchs of Ethiopia, the Netherlands, Belgium, Monaco and Luxembourg. Also in the congregation were David Ben-Gurion, Anthony Eden, Harold Macmillan, Harold Wilson, former West German chancellors Ludwig Erhard and Kurt-Georg Kiesinger, Marlene Dietrich and U.S. Senator Edward Kennedy, who remembered de Gaulle's immediate decision to attend the funeral of Kennedy's brother John following his assassination in 1963. The Chinese leader Mao Zedong was unable to attend but sent a wreath. The only notable absentee was Canadian PM Pierre Trudeau, possibly because he was still angry over de Gaulle's cry of "Vive le Quebec libre" during his 1967 visit.
The funeral on 12 November 1970 was the biggest such event in French history, with hundreds of thousands of French people—many carrying blankets and picnic baskets—and thousands of cars parked in the roads and fields along the routes to the two venues. Special trains were laid on to bring extra mourners to the region and the crowd was packed so tightly that those who fainted had to be passed overhead toward first-aid stations at the rear.
The General was conveyed to the church on an armoured reconnaissance vehicle and carried to his grave, next to his daughter Anne, by eight young men of Colombey. As he was lowered into the ground, the bells of all the churches in France tolled, starting from Notre Dame and spreading out from there.
Madame de Gaulle asked the undertaker to provide the same type of simple oak casket that he used for everyone else, but because of the General's extreme height, the coffin cost $9 more than usual. He specified that his tombstone bear the simple inscription of his name and his years of birth and death. Therefore, it simply states, "Charles de Gaulle, 1890–1970".
At the service, President Pompidou said, "de Gaulle gave France her governing institutions, her independence and her place in the world." André Malraux, the writer and intellectual who served as his Minister of Culture, called him "a man of the day before yesterday and the day after tomorrow."
His family has turned the La Boisserie residence into a foundation. It is currently the Charles de Gaulle Museum.
Legacy.
According to a 2005 survey, carried out in the context of the tenth anniversary of the death of Socialist President François Mitterrand, 35% of respondents said Mitterrand was the best French president ever, followed by Charles de Gaulle (30%) and then Jacques Chirac (12%). Another poll by BVA four years later showed that 87% of French people regarded his presidency positively.
Statues have been erected in his honour in Warsaw, Moscow, Bucharest and Quebec. The first Algerian president, Ahmed Ben Bella, said that de Gaulle was the "military leader who brought us the hardest blows" prior to Algerian independence, but "saw further" than other politicians, and had a "universal dimension that is too often lacking in current leaders." Likewise, Léopold Sédar Senghor, the first president of Senegal, said that few Western leaders could boast of having risked their lives to grant a colony independence.
In 1990, his old political enemy, the Socialist President François Mitterrand, presided over the celebrations to mark the 100th anniversary of his birth. Mitterrand, who once wrote a vitriolic critique of him called the 'Permanent Coup d'État', quoted a then recent opinion poll, saying, "As General de Gaulle, he has entered the pantheon of great national heroes, where he ranks ahead of Napoleon and behind only Charlemagne."
Under the influence of the paleo-left CERES leader Jean-Pierre Chevènement, de Gaulle's former nemesis François Mitterrand had, except on certain economic and social policies, rallied to much of Gaullism. Between the mid-1970s and mid-1990s their developed a left-right consensus, dubbed "Gaullo-Mitterrandism", behind the "French status" in NATO: i.e. outside the integrated military command.
Although he initially enjoyed good relations with U.S. President John F. Kennedy, who admired his stance against the Soviet Union—particularly when the Berlin Wall was being built—and who called him "a great captain of the Western world", their relationship later cooled. De Gaulle was Kennedy's most loyal ally during the Cuban Missile Crisis and supported the right that the United States claimed to defend its interests in the western hemisphere, in contrast to then German Chancellor Konrad Adenauer who doubted Kennedy's commitment to Europe and thought the crisis could have been avoided. De Gaulle accepted that it might be necessary for the United States to take preemptive military action against Cuba, unlike many other European leaders of his time. De Gaulle was a prominent figure at Kennedy's funeral. De Gaulle was very much admired by the later President Nixon. After a meeting at the Palace of Versailles just before the general left office, Nixon declared that "He did not try to put on airs but an aura of majesty seemed to envelop him ... his performance—and I do not use that word disparagingly—was breathtaking." On arriving for his funeral several months later, Nixon said of him, "greatness knows no national boundaries".
Lt. General Vernon A. Walters, a military attaché of Dwight Eisenhower and later military attaché in France from 1967–1973, noted the strong relationship between de Gaulle and Eisenhower, de Gaulle's unconditional support of Eisenhower during the U-2 incident, and de Gaulle's strong support of John F. Kennedy during the Cuban Missile Crisis. Thus Walters was intensely curious as to the great contrast between de Gaulle's close relations with two United States presidents during notable Cold War crises and de Gaulle's later decision to withdraw France from NATO's military command, and Walters spoke with many close military and political aides of de Gaulle. Walters' conclusion, based upon de Gaulle's comments to many of his aides (and to Eisenhower during a meeting at Ramboullet Castle in 1959), is that de Gaulle feared that later United States presidents after Eisenhower would not have Eisenhower's special ties to Europe and would not risk nuclear war over Europe. Also, de Gaulle interpreted the peaceful resolution of the Cuban Missile Crisis without fighting to take back Cuba from communism a mere 90 miles from the United States as an indication that the United States might not fight for Europe's defense 3,500 miles away following Soviet aggression in Europe, but would only go to war following a nuclear strike against the United States itself. De Gaulle told Eisenhower that France did not seek to compete with the Strategic Air Command or army of the United States, but believed that France needed a way to strike the Soviet Union.
A number of commentators have been critical of de Gaulle for his failure to prevent the massacres after Algerian independence while others take the view that the struggle had been so long and savage that it was perhaps inevitable. The Australian historian Brian Crozier wrote, "that he was able to part with Algeria without civil war was a great though negative achievement which in all probability would have been beyond the capacity of any other leader France possessed." In April 1961, when four rebel generals seized power in Algeria, he "did not flinch in the face of this daunting challenge", but appeared on television in his general's uniform to forbid Frenchmen to obey the rebels' orders in an "inflexible display of personal authority".
De Gaulle was an excellent manipulator of the media, as seen in his shrewd use of television to persuade around 80% of Metropolitan France to approve the new constitution for the Fifth Republic. In so doing, he refused to yield to the reasoning of his opponents who said that, if he succeeded in Algeria, he would no longer be necessary. He afterwards enjoyed massive approval ratings, and once said that "every Frenchman is, has been or will be Gaullist".
That de Gaulle did not necessarily reflect mainstream French public opinion with his veto was suggested by the decisive majority of French people who voted in favour of British membership when the much more conciliatory Pompidou called a referendum on the matter in 1972. His early influence in setting the parameters of the EEC can still be seen today, most notably with the controversial Common Agricultural Policy.
Some writers take the view that Pompidou was a more progressive and influential leader than de Gaulle because, though also a Gaullist, he was less autocratic and more interested in social reforms. Although he followed the main tenets of de Gaulle's foreign policy, he was keen to work towards warmer relations with the United States. A banker by profession, Pompidou is also widely credited, as de Gaulle's prime minister from 1962–1968, with putting in place the reforms which provided the impetus for the economic growth which followed.
In 1968, shortly before leaving office, de Gaulle refused to devalue the Franc on grounds of national prestige, but upon taking over Pompidou reversed the decision almost straight away. It was ironic, that during the financial crisis of 1968, France had to rely on American (and West German) financial aid to help shore up the economy. Perry has written "The events of 1968 illustrated the brittleness of de Gaulle's rule. That he was taken by surprise is an indictment of his rule; he was too remote from real life and had no interest in the conditions under which ordinary French people lived. Problems like inadequate housing and social services had been ignored. The French greeted the news of his departure with some relief as the feeling had grown that he had outlived his usefulness. Perhaps he clung onto power too long, perhaps he should have retired in 1965 when he was still popular."
Brian Crozier has said "the fame of de Gaulle outstrips his achievements, he chose to make repeated gestures of petulance and defiance that weakened the west without compensating advantages to France"
Régis Debray called de Gaulle "super-lucide" and pointed out that virtually all of his predictions, such as the fall of communism, the reunification of Germany and the resurrection of 'old' Russia, came true after his death. Debray compared him with Napoleon ('the great political myth of the 19th century'), calling de Gaulle his 20th century equivalent. "The sublime, it seems, appears in France only once a century ... Napoleon left two generations dead on battlefield. De Gaulle was more sparing with other people's blood; even so, he left us, as it were, stranded, alive but dazed… A delusion, perhaps, but one that turns the world upside down: causes events and movements; divides people into supporters and adversaries; leaves traces in the form of civil and penal codes and railways, factories and institutions (the Fifth Republic has already lasted three times as long as the Empire). A statesman who gets something going, who has followers, escapes the reality of the reports and statistics and become part of imagination. Napoleon and de Gaulle modified the state of things because they modified souls". However, Debray pointed out that there is a difference between Napoleon and de Gaulle: "How can the exterminator be compared with the liberator? ... The former ran the whole enterprise into the ground, while the latter managed to save it. So that to measure the rebel against the despot, the challenger against the leader, is just glaringly idiotic. You simply do not put an adventurer who worked for himself or his family on the same level as a commander-in-chief serving his country. ... Regrettably, Gaullism and Bonapartism have a number of features in common, but Napoleon and de Gaulle do not have the same moral value. ... the first wanted a Holy French Empire without the faith, a Europe under French occupation. The second wanted to rescue the nation from the emperors and establish a free France in a free Europe".
While De Gaulle had many admirers, he was at the same time one of the most hated and reviled men in modern French history.
The website WW2history.com lists De Gaulle as one of the most overrated leaders of the Second World War, among others such as Churchill, Hitler and Stalin. According to American historian William Hitchcock, De Gaulle built his entire postwar career on his wartime heroism, which contained many exaggerations and myths. Hitchcock describes him as obstreperous, inflexible, and unwilling to compromise his positions, but he also portrays him as an "extraordinary figure and a fascinating man who had a lot of courage and a lot of guts".
3rd Government, 9 June 1958 – 8 January 1959.
Changes
In popular culture.
In France, he is commonly referred to as "Général de Gaulle" or simply "Le Général". Free French sometimes called him "Le grand Charles". His detractors sometimes call him "la Grande Zohra".
De Gaulle is the main character in the 2009 movie "Adieu De Gaulle adieu".
De Gaulle is a presence in the Frederick Forsyth novel "The Day of the Jackal", in which the Organisation de l'armée secrète—after the failure of the factual August 1962 Petit Clamart assassination attempt—hire an English professional assassin to kill him on Liberation Day 1963. The novel was made into a film, starring Edward Fox and Michel Lonsdale, in 1973.
The Flanders and Swann song "All Gall" contains highlights from de Gaulle's career set to the tune of This Old Man.
Charles de Gaulle's head was in the Futurama movie "Bender's Big Score" as a reference to the Scott Walker song "30 Century Man" also featured in the movie.
De Gaulle was seen in "" played by actor George Shevtsov. In the film he opposes the plans to invade Normandy and Dwight D. Eisenhower's request that the French people accept Eisenhower as the united voice of the Allies.
An assassination attempt on De Gaulle is the subject of an episode of the drama-documentary series "Days That Shook The World" entitled "Conspiracy To Kill: The Real Day Of The Jackal". It details the attempt made on 22 August 1962, led by Jean Bastien-Thiry and depicted in "The Day Of The Jackal".
Memorials.
A number of monuments have been built to commemorate the life of Charles de Gaulle. 
France's largest airport, located in Roissy, outside Paris, is named Charles de Gaulle Airport in his honour. France's nuclear-powered aircraft carrier is also named after him.
External links.
 

</doc>
<doc id="51256" url="http://en.wikipedia.org/wiki?curid=51256" title="Public choice">
Public choice

Public choice or public choice theory refers to "the use of economic tools to deal with traditional problems of political science". Its content includes the study of political behavior. In political science, it is the subset of positive political theory that studies self-interested agents (voters, politicians, bureaucrats) and their interactions, which can be represented in a number of ways, such as standard constrained utility maximization, game theory, or decision theory. Public choice analysis has roots in positive analysis (“what is”) but is often used for normative purposes (“what ought to be”) in order to identify a problem or suggest improvements to constitutional rules (i.e., constitutional economics).
Under the "Journal of Economic Literature"'s classification code, public choice is a subarea of microeconomics, under JEL: D7: “Analysis of Collective Decision-Making” (specifically, JEL: D72: “Economic Models of Political Processes: Rent-Seeking, Elections, Legislatures, and Voting Behavior”). Public choice theory is also closely related to social choice theory, a mathematical approach to aggregation of individual interests, welfares, or votes. Much early work had aspects of both, and both use the tools of economics and game theory. Since voter behavior influences the behavior of public officials, public choice theory often uses results from social choice theory. General treatments of public choice may also be classified under public economics.
Background and development.
A precursor of modern public choice theory was the work of Knut Wicksell (1896), which treated government as political exchange, a "quid pro quo", in formulating a benefit principle linking taxes and expenditures.
Some subsequent economic analysis has been described as treating government as though it attempted “to maximize some kind sort of welfare function for society” and as distinct from characterizations of economic agents, such as those in business. In contrast, public choice theory modeled government as made up of officials who, besides pursuing the public interest, might act to benefit themselves, for example in the budget-maximizing model of bureaucracy, possibly at the cost of efficiency.
Modern public-choice theory has been dated from the work of Duncan Black, sometimes called “the founding father of public choice”. In a series of papers from 1948, which culminated in "The Theory of Committees and Elections" (1958), and later, Black outlined a program of unification toward a more general “Theory of Economic and Political Choices” based on common formal methods, developed underlying concepts of what would become median voter theory, and rediscovered earlier works on voting theory.
Kenneth J. Arrow’s "Social Choice and Individual Values" (1951) influenced formulation of the theory. Among other important works are Anthony Downs (1957) "An Economic Theory of Democracy" and Mancur Olson (1965) "The Logic of Collective Action".
James M. Buchanan and Gordon Tullock coauthored "" (1962), considered one of the landmarks in public choice. In particular, the Preface describes the book as “about the "political organization”" of a free society. But its methodology, conceptual apparatus, and analytics “are derived, essentially, from the discipline that has as its subject the "economic" organization of such a society” (1962, p. ). The book focuses on positive-economic analysis as to the development of constitutional democracy but in an ethical context of consent. The consent takes the form of a compensation principle like Pareto efficiency for making a policy change and unanimity or at least no opposition as a point of departure for social choice.
Somewhat later, the probabilistic voting theory started to displace the median voter theory in showing how to find Nash equilibria in multidimensional space. The theory was later formalized further by Peter Coughlin.
Decision-making processes and the state.
One way to organize the subject matter studied by public choice theorists is to begin with the foundations of the state itself. According to this procedure, the most fundamental subject is the origin of government. Although some work has been done on anarchy, autocracy, revolution, and even war, the bulk of the study in this area has concerned the fundamental problem of collectively choosing constitutional rules. This work assumes a group of individuals who aim to form a government, then it focuses on the problem of hiring the agents required to carry out government functions agreed upon by the members.
“Expressive interests” and democratic irrationality.
Geoffrey Brennan and Loren Lomasky claim that democratic policy is biased to favor “expressive interests” and neglect practical and utilitarian considerations. Brennan and Lomasky differentiate between instrumental interests (any kind of practical benefit, both monetary and non-monetary) and expressive interests (forms of expression like applause). According to Brennan and Lomasky, the voting paradox can be resolved by differentiating between expressive and instrumental interests.
This argument has led some public choice scholars to claim that politics is plagued by irrationality. In articles published in the Econ Journal Watch, economist Bryan Caplan contended that voter choices and government economic decisions are inherently irrational. Caplan‘s ideas are more fully developed in his book "The Myth of the Rational Voter" (Princeton University Press 2007). Countering Donald Wittman's arguments in "The Myth of Democratic Failure", Caplan claims that politics is biased in favor of irrational beliefs.
According to Caplan, democracy effectively subsidizes irrational beliefs. Anyone who derives utility from potentially irrational policies like protectionism can receive private benefits while imposing the costs of such beliefs on the general public. Were people to bear the full costs of their “irrational beliefs”, they would lobby for them optimally, taking into account both their instrumental consequences and their expressive appeal. Instead, democracy oversupplies policies based on irrational beliefs. Caplan defines rationality mainly in terms of mainstream price theory, pointing out that mainstream economists tend to oppose protectionism and government regulation more than the general population, and that more educated people are closer to economists on this score, even after controlling for confounding factors such as income, wealth or political affiliation. One criticism is that many economists do not share Caplan's views on the nature of public choice. However, Caplan does have data to support his position. Economists have, in fact, often been frustrated by public opposition to economic reasoning. As Sam Peltzman puts it: Economists know what steps would improve the efficiency of HSE [health, safety, and environmental] regulation, and they have not been bashful advocates of them. These steps include substituting markets in property rights, such as emission rights, for command and control...The real problem lies deeper than any lack of reform proposals or failure to press them. It is our inability to understand their lack of political appeal.Public choice's application to government regulation was developed by George Stigler (1971) and Sam Peltzman (1976).
Special interests.
Public choice theory is often used to explain how political decision-making results in outcomes that conflict with the preferences of the general public. For example, many advocacy group and pork barrel projects are not the desire of the overall democracy. However, it makes sense for politicians to support these projects. It may make them feel powerful and important. It can also benefit them financially by opening the door to future wealth as lobbyists. The project may be of interest to the politician’s local constituency, increasing district votes or campaign contributions. The politician pays little or no cost to gain these benefits, as he is spending public money. Special-interest lobbyists are also behaving rationally. They can gain government favors worth millions or billions for relatively small investments. They face a risk of losing out to their competitors if they don't seek these favors. The taxpayer is also behaving rationally. The cost of defeating any one government give-away is very high, while the benefits to the individual taxpayer are very small. Each citizen pays only a few pennies or a few dollars for any given government favor, while the costs of ending that favor would be many times higher. Everyone involved has rational incentives to do exactly what they are doing, even though the desire of the general constituency is opposite. Costs are diffused, while benefits are concentrated. The voices of vocal minorities with much to gain are heard over those of indifferent majorities with little to individually lose.
While good government tends to be a pure public good for the mass of voters, there may be many advocacy groups that have strong incentives for lobbying the government to implement specific policies that would benefit them, potentially at the expense of the general public. For example, lobbying by the sugar manufacturers might result in an inefficient subsidy for the production of sugar, either direct or by protectionist measures. The costs of such inefficient policies are dispersed over all citizens, and therefore unnoticeable to each individual. On the other hand, the benefits are shared by a small special-interest group with a strong incentive to perpetuate the policy by further lobbying. Due to rational ignorance, the vast majority of voters will be unaware of the effort; in fact, although voters may be aware of special-interest lobbying efforts, this may merely select for policies which are even harder to evaluate by the general public, rather than improving their overall efficiency. Even if the public were able to evaluate policy proposals effectively, they would find it infeasible to engage in collective action in order to defend their diffuse interest. Therefore, theorists expect that numerous special interests will be able to successfully lobby for various inefficient policies. In public choice theory, such scenarios of inefficient government policies are referred to as "government failure" — a term akin to "market failure" from earlier theoretical welfare economics.
Rent-seeking.
A field that is closely related to public choice is “rent-seeking”. This field combines the study of a market economy with that of government. Thus, one might regard it as a “new political economy”. Its basic thesis is that when both a market economy and government are present, government agents provide numerous special market privileges. Both the government agents and self-interested market participants seek these privileges in order to partake in the resulting monopoly rent. Rentiers gain benefits above what the market would have offered, but in the process allocate resources in sub-optimal fashion from a societal point of view.
Rent-seeking is broader than public choice in that it applies to autocracies as well as democracies and, therefore, is not directly concerned with collective decision making. However, the obvious pressures it exerts on legislators, executives, bureaucrats, and even judges are factors that public choice theory must account for in its analysis of collective decision-making rules and institutions. Moreover, the members of a collective who are planning a government would be wise to take prospective rent-seeking into account.
Another major claim is that much of political activity is a form of rent-seeking which wastes resources. Gordon Tullock, Jagdish Bhagwati, and Anne Osborn Krueger have argued that rent-seeking has caused considerable waste. In a parallel line of research Fred McChesney claims that rent extraction causes considerable waste, especially in the developing world. As the term implies, rent extraction happens when officials use threats to extort payments from private parties.
Bureaucracy.
Another major sub-field is the study of bureaucracy. The usual model depicts the top bureaucrats as being chosen by the chief executive and legislature, depending on whether the democratic system is presidential or parliamentary. The typical image of a bureau chief is a person on a fixed salary who is concerned with pleasing those who appointed him. The latter have the power to hire and fire him more or less at will. The bulk of the bureaucrats, however, are civil servants whose jobs and pay are protected by a civil service system against major changes by their appointed bureau chiefs. This image is often compared with that of a business owner whose profit varies with the success of production and sales, who aims to maximize profit, and who can in an ideal system hire and fire employees at will. William Niskanen is generally considered the founder of public choice literature on the bureaucracy.
Political stance.
From such results it is sometimes asserted that public choice theory has an anti-state tilt. But there is ideological diversity among public choice theorists. Mancur Olson for example was an advocate of a strong state and instead opposed political interest group lobbying. More generally, James Buchanan has suggested that public choice theory be interpreted as “politics without romance”, a critical approach to a pervasive earlier notion of idealized politics set against market failure.
Recognition.
Several notable public choice scholars have been awarded the Nobel Prize in Economics, including James M. Buchanan (1986), George Stigler (1982), Gary Becker (1992), Vernon Smith (2002) and Elinor Ostrom (2009). In addition, Vernon Smith and Elinor Ostrom were former Presidents of the Public Choice Society.
Criticisms.
In their 1994 book "Pathologies of Rational Choice Theory", political scientists Donald P. Green and Ian Shapiro argue that rational choice theory (of which public choice theory is a branch) has contributed less to the field than its popularity suggests. They write:
Linda McQuaig writes in "All You Can Eat":
Amartya Sen has acknowledged the contribution of Buchanan and Tullock's analysis of unanimity as a criterion for collective choice, but argued for the logical inconsistency of the Pareto-principle version of unanimity with even minimal liberty in a social-choice framework. More broadly he qualified its use when other information besides personal utility is given a weight in public judgments.
Buchanan and Tullock themselves outline methodological qualifications of the approach developed in their work "The Calculus of Consent" (1962), p. :
References.
</dl>

</doc>
<doc id="51257" url="http://en.wikipedia.org/wiki?curid=51257" title="Peach">
Peach

The peach ("Prunus persica") is a deciduous tree, native to Northwest China, in the region between the Tarim Basin and the north slopes of the Kunlun Shan mountains, where it was first domesticated and cultivated. It bears an edible juicy fruit also called a peach.
The specific epithet "persica" refers to its widespread cultivation in Persia, whence it was transplanted to Europe. It belongs to the genus "Prunus" which includes the cherry and plum, in the family Rosaceae. The peach is classified with the almond in the subgenus "Amygdalus", distinguished from the other subgenera by the corrugated seed shell.
Peaches and nectarines are the same species, even though they are regarded commercially as different fruits. In contrast to peaches, whose fruits present the characteristic fuzz on the skin, nectarines are characterized by the absence of fruit-skin trichomes (fuzz-less fruit); genetic studies suggest nectarines are produced due to a recessive allele, whereas peaches are produced from a dominant allele for fuzzy skin.
China is the world's largest producer of peaches.
Description.
"Prunus persica" grows to 4 – tall and 6 in. in diameter. The leaves are lanceolate, 7 – long, 2 – broad, pinnately veined. The flowers are produced in early spring before the leaves; they are solitary or paired, 2.5–3 cm diameter, pink, with five petals. The fruit has yellow or whitish flesh, a delicate aroma, and a skin that is either velvety (peaches) or smooth (nectarines) in different cultivars. The flesh is very delicate and easily bruised in some cultivars, but is fairly firm in some commercial varieties, especially when green. The single, large seed is red-brown, oval shaped, approximately 1.3–2 cm long, and is surrounded by a wood-like husk. Peaches, along with cherries, plums and apricots, are stone fruits (drupes). There are various heirloom varieties, including the Indian peach, which arrives in the latter part of the summer.
Cultivated peaches are divided into clingstones and freestones, depending on whether the flesh sticks to the stone or not; both can have either white or yellow flesh. Peaches with white flesh typically are very sweet with little acidity, while yellow-fleshed peaches typically have an acidic tang coupled with sweetness, though this also varies greatly. Both colours often have some red on their skin. Low-acid white-fleshed peaches are the most popular kinds in China, Japan, and neighbouring Asian countries, while Europeans and North Americans have historically favoured the acidic, yellow-fleshed kinds.
Etymology.
The scientific name "persica", along with the word "peach" itself and its cognates in many European languages, derives from an early European belief that peaches were native to Persia. The Ancient Romans referred to the peach as "malum persicum" "Persian apple", later becoming French "pêche", hence the English "peach".
History.
Although its botanical name "Prunus persica" refers to Persia (present Iran) from where it came to Europe, genetic studies suggest peaches originated in China, where they have been cultivated since the early days of Chinese culture, circa 2000 BC. Peaches were mentioned in Chinese writings as far back as the 10th century BC and were a favoured fruit of kings and emperors. As of late, the history of cultivation of peaches in China has been extensively reviewed citing numerous original manuscripts dating back to 1100 BC.
The peach was brought to India and Western Asia in ancient times. Peach cultivation also went from China, through Persia, and reached Greece by 300 BC. Alexander the Great introduced the fruit into Europe after he conquered the Persians. Peaches were well known to the Romans in first century AD, and were cultivated widely in Emilia-Romagna. Peach trees are portrayed in the wall paintings of the towns destroyed by the Vesuvius eruption of 79 AD, while the oldest known artistic representations of the fruit are in the two fragments of wall paintings, dated back to the 1st century AD, in Herculaneum, now preserved in the National Archaeological Museum in Naples.
Peach was brought to the Americas by Spanish explorers in the 16th century, and eventually made it to England and France in the 17th century, where it was a prized and expensive treat. The horticulturist George Minifie supposedly brought the first peaches from England to its North American colonies in the early 17th century, planting them at his Estate of Buckland in Virginia. Although Thomas Jefferson had peach trees at Monticello, United States farmers did not begin commercial production until the 19th century in Maryland, Delaware, Georgia and finally Virginia.
In April 2010, an International Consortium, The International Peach Genome Initiative (IPGI), that include researchers from USA, Italy, Chile, Spain and France announced they had sequenced the peach tree genome (doubled haploid Lovell). Recently, IPGI published the peach genome sequence and related analyses. The peach genome sequence is composed of 227 millions of nucleotides arranged in 8 pseudomolecules representing the 8 peach chromosomes (2n = 16). In addition, a total of 27,852 protein-coding genes and 28,689 protein-coding transcripts were predicted. Particular emphasis in this study is reserved to the analysis of the genetic diversity in peach germplasm and how it was shaped by human activities such as domestication and breeding. Major historical bottlenecks were individuated, one related to the putative original domestication that is supposed to have taken place in China about 4,000–5,000 years ago, the second is related to the western germplasm and is due to the early dissemination of peach in Europe from China and to the more recent breeding activities in US and Europe. These bottlenecks highlighted the strong reduction of genetic diversity associated with domestication and breeding activities.
Cultivation.
Peaches grow in a fairly limited range in dry, continental or temperate climates, since the trees have a chilling requirement that tropical or subtropical areas generally cannot satisfy except at high altitudes (for example in parts of Ecuador, Colombia, Ethiopia, India and Nepal). Most cultivars require between 600 and 1,000 hours of chilling around 0 to. During the chilling period, key chemical reactions occur but the plant appears dormant. Once the chilling period is fulfilled, the plant enters a second type of dormancy, the quiescence period. During quiescence, buds break and grow when sufficient warm weather favorable to growth is accumulated.
The trees themselves can usually tolerate temperatures to around -26 to, although the following season's flower buds are usually killed at these temperatures, preventing a crop that summer. Flower bud death begins to occur between -15 and, depending on the cultivar and on the timing of the cold, with the buds becoming less cold tolerant in late winter.
Another climate constraint is spring frost. The trees flower fairly early (in March in western Europe) and the blossom is damaged or killed if temperatures drop below about -4 C. However, if the flowers are not fully open, they can tolerate a few degrees colder.
Climates with significant winter rainfall at temperatures below 16 C are also unsuitable for peach cultivation as the rain promotes peach leaf curl, which is the most serious fungal disease for peaches. In practice, fungicides are extensively used for peach cultivation in such climates, with >1% of European peaches exceeding legal pesticide limits in 2013.
Finally, summer heat is required to mature the crop, with mean temperatures of the hottest month between 20 and. 
Typical peach cultivars begin bearing fruit in their third year and have a lifespan of about 12 years. 
Cultivars.
There are hundreds of peach and nectarine cultivars. These are classified into two categories—the freestones and the clingstones. Freestones are those whose flesh separates readily from the pit. Clingstones are those whose flesh clings tightly to the pit. Some cultivars are partially freestone and clingstone, and these are called semi-free. Freestone types are preferred for eating fresh, while clingstone for canning. The fruit flesh may be creamy white or deep yellow; the hue and shade of the color depends on the cultivar.
Peach breeding has favored cultivars with more firmness, more red color, and shorter fuzz on fruit surface. These characteristics ease shipping and supermarket sales by improving eye appeal. However, this selection process has not necessarily led to increased flavor. Peaches have short shelf life, so commercial growers typically plant a mix of different cultivars in order to have fruit to ship all season long.
Different countries have different cultivars. In United Kingdom, for example, the following cultivars have gained the Royal Horticultural Society's Award of Garden Merit:-
Nectarines.
The variety "P. persica" var. "nucipersica" (or var. "nectarina"), commonly called nectarine, has a smooth skin. It is on occasion referred to as a "shaved peach" or "fuzzless peach", due to its lack of fuzz or short hairs. Though fuzzy peaches and nectarines are regarded commercially as different fruits, with nectarines often erroneously believed to be a crossbreed between peaches and plums, or a "peach with a plum skin", nectarines belong to the same species as peaches. Several genetic studies have concluded nectarines are produced due to a recessive allele, whereas a fuzzy peach skin is dominant. Nectarines have arisen many times from peach trees, often as bud sports.
As with peaches, nectarines can be white or yellow, and clingstone or freestone. On average, nectarines are slightly smaller and sweeter than peaches, but with much overlap. The lack of skin fuzz can make nectarine skins appear more reddish than those of peaches, contributing to the fruit's plum-like appearance. The lack of down on nectarines' skin also means their skin is more easily bruised than peaches.
The history of the nectarine is unclear; the first recorded mention in English is from 1616, but they had probably been grown much earlier within the native range of the peach in central and eastern Asia. Although one source states that nectarines were introduced into the United States by David Fairchild of the Department of Agriculture in 1906, a number of colonial era newspaper articles make reference to nectarines being grown in the United States prior to the Revolutionary War. 28 March 1768 edition of the "New York Gazette" (p. 3), for example, mentions a farm in Jamaica, Long Island, New York, where nectarines were grown.
Peacherines.
Peacherine is claimed to be a cross between a peach and a nectarine, and are marketed in Australia and New Zealand. The fruit is intermediate in appearance between a peach and a nectarine, large and brightly colored like a red peach. The flesh of the fruit is usually yellow but white varieties also exist. The Koanga Institute lists varieties that ripen in the Southern hemisphere in February and March.
In 1909, "Pacific Monthly" mentioned peacherines in a news bulletin for California. Louise Pound, in 1920, claimed the term peacherine is an example of language stunt.
Planting.
Most peach trees sold by nurseries are cultivars budded or grafted onto a suitable rootstock. This is done to improve predictability of the fruit quality.
Peach trees need full sun, and a layout that allows good natural air flow to assist the thermal environment for the tree. Peaches are planted in early winter. During the growth season, peach trees need a regular and reliable supply of water, with higher amounts just before harvest.
Peaches need nitrogen rich fertilizers more than other fruit trees. Without regular fertilizer supply, peach tree leaves start turning yellow or exhibit stunted growth. Blood meal, bone meal, and calcium ammonium nitrate are suitable fertilizers.
The number of flowers on a peach tree are typically thinned out, because if the full number of peaches mature on a branch, they are under-sized and lacking in flavor. Fruits are thinned midway in the season by commercial growers. Fresh peaches are easily bruised, and do not store well. They are most flavorful when they ripen on the tree and eaten the day of harvest.
The peach tree can be grown in an espalier shape. The Baldassari palmette is a palmette design created around 1950 used primarily for training peaches. In walled gardens constructed from stone or brick, which absorb and retain solar heat and then slowly release it, raising the temperature against the wall, peaches can be grown as espaliers against south-facing walls as far north as southeast Great Britain and southern Ireland.
Interaction with fauna.
The first pest to attack the tree early in the year when other food is scarce is the earwig ("Forficula auricularia") which feeds on blossoms and young leaves at night, preventing fruiting and weakening newly planted trees. The pattern of damage is distinct from that of caterpillars later in the year, as earwigs characteristically remove semi-circles of petal and leaf tissue from the tips, rather than internally. Greasebands applied just before blossom are effective.
The larvae of such moth species as the peachtree borer ("Synanthedon exitiosa"), the yellow peach moth ("Conogethes punctiferalis"), the well-marked cutworm ("Abagrotis orbis"), "Lyonetia prunifoliella", "Phyllonorycter hostis", the fruit tree borer ("Maroga melanostigma"), "Parornix anguliferella", "Parornix finitimella", "Caloptilia zachrysa", "Phyllonorycter crataegella", "Trifurcula sinica", the Suzuki's Promolactis moth ("Promalactis suzukiella"), the white-spotted tussock moth ("Orgyia thyellina"), the apple leafroller ("Archips termias"), the catapult moth ("Serrodes partita"), the wood groundling ("Parachronistis albiceps") or the omnivorous leafroller ("Platynota stultana") are reported to feed on "P. persica".
The flatid planthopper ("Metcalfa pruinosa") causes damage to fruit trees.
The tree is also a host plant for such species as the Japanese beetle ("Popillia japonica"), the unmonsuzume ("Callambulyx tatarinovii"), the Promethea silkmoth ("Callosamia promethea"), the orange oakleaf ("Kallima inachus"), "Langia zenzeroides", the speckled emperor ("Gynanisa maja") or the brown playboy ("Deudorix antalus").
It is a good pollen source for honey bees and a honeydew source for aphids.
The European red mite ("Panonychus ulmi") or the yellow mite ("Lorryia formosa") are also found on the peach tree.
Diseases.
Peach trees are prone to a disease called leaf curl, which usually does not directly affect the fruit, but does reduce the crop yield by partially defoliating the tree. The fruit is susceptible to brown rot or a dark reddish spot. Ziram is a fungicide often used on peach trees to protect against fungal infection.
Storage.
Peaches and nectarines are best stored at temperatures of 0 °C (32 °F) and high-humidity. They are highly perishable, and typically consumed or canned within two weeks of harvest.
Peaches are climacteric fruits and continue to ripen after being picked from the tree.
Production.
Important historical peach-producing areas are China, Iran, and the Mediterranean countries, such as France, Italy, Spain and Greece. More recently, the United States (where the three largest producing states are California, South Carolina, and Georgia), Canada (British Columbia), and Australia (the Riverland region) have also become important; peach growing in the Niagara Peninsula of Ontario, Canada, was formerly intensive, but slowed substantially in 2008 when the last fruit cannery in Canada was closed by the proprietors. Oceanic climate areas, like the Pacific Northwest and coastline of northwestern Europe, are generally not satisfactory for growing peaches due to inadequate summer heat, though they are sometimes grown trained against south-facing walls to catch extra heat from the sun. Trees grown in a sheltered and south-facing position in the southeast of England are capable of producing both flowers and a large crop of fruit. In Vietnam, the most famous variety of peach fruit product is grown in Mẫu Sơn commune, Lộc Bình District, Lạng Sơn Province.
For home gardeners, semi-dwarf (3 to) and dwarf (2 to) varieties have been developed by grafting desirable cultivars onto dwarfing rootstock. Fruit size is not affected. Another mutation is flowering peaches, selected for ornamental display rather than fruit production.
The State of Georgia, in the U.S., has long been known as a centre for growers and consumers of peaches. Georgia is known as the "Peach State" because of the production of its peaches. In 1875, Samuel Rumph, a Georgia peach farmer, made possible and practical large-scale peach farming by inventing a refrigerated railcar and mortised-end peach crate; these enabled farmers to ship large quantities of peaches a long distance.
The most productive farms for peaches and nectarines, on average, were in Austria. In comparison to world average yield of 13 metric tons per hectare, Austrian farm yields topped 40 metric tonnes per hectare for each of the years between 2006 and 2010, with highest observed average yield of 56.8 metric tonnes per hectare in 2010.
Depending on climate and cultivar, peach harvest can occur from late May into August (Northern Hemisphere); harvest from each tree lasts about a week.
Cultural significance.
Peaches are not only a popular fruit, but are symbolic in many cultural traditions, such as in art, paintings and folk tales such as Peaches of Immortality.
China.
Peach blossoms are highly prized in Chinese culture. The ancient Chinese believed the peach to possess more vitality than any other tree because their blossoms appear before leaves sprout. When early rulers of China visited their territories, they were preceded by sorcerers armed with peach rods to protect them from spectral evils. On New Year's Eve, local magistrates would cut peach wood branches and place them over their doors to protect against evil influences. Another author writes:
The Chinese also considered peach wood ("t'ao-fu") protective against evil spirits, who held the peach in awe. In ancient China, peach-wood bows were used to shoot arrows in every direction in an effort to dispel evil. Peach-wood slips or carved pits served as amulets to protect a person's life, safety, and health.
Peach-wood seals or figurines guarded gates and doors, and, as one Han account recites, "the buildings in the capital are made tranquil and pure; everywhere a good state of affairs prevails." Writes the author, further:
Another aid in fighting evil spirits were peach-wood wands. The Li-chi (Han period) reported that the emperor went to the funeral of a minister escorted by a sorcerer carrying a peach-wood wand to keep bad influences away. Since that time, peach-wood wands have remained an important means of exorcism in China.
 Peach kernels (桃仁 "táo rén") are a common ingredient used in traditional Chinese medicine to dispel blood stasis, counter inflammation and reduce allergies.
It was in an orchard of flowering peach trees that Liu Bei, Guan Yu, and Zhang Fei took an oath of brotherhood in the opening chapter of the classic Chinese novel "Romance of the Three Kingdoms". Another peach forest, “The Peach Blossom Spring” by poet Tao Yuanming is the setting of the favourite Chinese fable and a metaphor of utopias. A peach tree growing on a precipice was where the Taoist master Zhang Daoling tested his disciples.
The Old Man of the South Pole one of the deities of the Chinese folk religion "fulu shou" is sometimes seen holding a large peach, representing long life and health.
The term "bitten peach", first used by Legalist philopher Han Fei in his work "Han Feizi", became a byword for homosexuality. The book records the incident when courtier Mizi Xia bit into an especially delicious peach and gave the remainder to his lover, Duke Ling of Wei, as a gift so that he could taste it as well.
Japan.
Momotaro, one of Japan's most noble and semihistorical heroes, was born from within an enormous peach floating down a stream. Momotaro or "Peach Boy" went on to fight evil "oni" and face many adventures.
Korea.
In Korea, peaches have been cultivated from ancient times. According to "Samguk Sagi", peach trees were planted during the Three Kingdoms of Korea period, and "Sallim gyeongje" also mentions cultivation skills of peach trees. The peach is seen as the fruit of happiness, riches, honours and longevity. The rare peach with double seeds is seen as a favorable omen of a mild winter. It is one of the ten immortal plants and animals, so peaches appear in many "minhwa" (folk paintings). Peaches and peach trees are believed to chase away spirits, so peaches are not placed on tables for "jesa" (ancestor veneration), unlike other fruits.
Vietnam.
A Vietnamese mythic history states that, in the spring of 1789, after marching to Ngọc Hồi and then winning a great victory against invaders from the Qing dynasty of China, the Emperor Quang Trung ordered a messenger to gallop to Phú Xuân citadel (now Huế) and deliver a flowering peach branch to the Princess Ngọc Hân. This took place on the fifth day of the first lunar month, two days before the predicted end of the battle. The branch of peach flowers that was sent from the north to the centre of Vietnam was not only a message of victory from the King to his wife, but also the start of a new spring of peace and happiness for all the Vietnamese people. In addition, since the land of Nhật Tân had freely given that very branch of peach flowers to the King, it became the loyal garden of his dynasty.
It was by a peach tree that the protagonists of "the Tale of Kieu" fell in love. And in Vietnam, the blossoming peach flower is the signal of spring. Finally, peach bonsai trees are used as decoration during Vietnamese New Year (Tết) in northern Vietnam.
Europe.
Many famous artists have painted still life with peach fruits placed in prominence. Caravaggio, Vicenzo Campi, Pierre-Auguste Renoir, Claude Monet, Édouard Manet, Henri Jean Fantin-Latour, George Forster, James Peale, Severin Roesen, Peter Paul Rubens, Van Gogh are among the many influential artists who painted peaches and peach trees in various settings. Scholars suggest that many compositions are symbolic, some an effort to introduce realism. For example, Tresidder claims the artists of Renaissance symbolically used peach to represent heart, and a leaf attached to the fruit as the symbol for tongue, thereby implying speaking truth from one's heart; a ripe peach was also a symbol to imply a ripe state of good health. Caravaggio paintings introduce realism by painting peach leaves that are molted, discolored or in some cases have wormholes – conditions common in modern peach cultivation.
Nutrition and research.
A medium peach, weighing 75 g, contains diverse essential nutrients but none as a significant proportion of the Daily Value (DV, right table). Nectarines have similar broad content of nutrients but also without any one at significant DV.
As with many other members of the rose family, peach seeds contain cyanogenic glycosides, including amygdalin (note the subgenus designation: "Amygdalus"). These substances are capable of decomposing into a sugar molecule and hydrogen cyanide gas. While peach seeds are not the most toxic within the rose family (see bitter almond), large consumption of these chemicals from any source is potentially hazardous to animal and human health.
Peach allergy or intolerance is a relatively common form of hypersensitivity to proteins contained in peaches and related fruit (almonds). Symptoms range from local symptoms (e.g. oral allergy syndrome, contact urticaria) to systemic symptoms, including anaphylaxis (e.g. urticaria, angioedema, gastrointestinal and respiratory symptoms). Adverse reactions are related to the "freshness" of the fruit: peeled or canned fruit may be tolerated.
Aroma.
Some 110 chemical compounds contribute to peach aroma, including alcohols, ketones, aldehydes, esters, polyphenols and terpenoids.
In other products.
Peach aroma is also a characteristic of some wines, such as Saint-Amour Beaujolais wine. It is one of the components of the aroma of Sancerre blanc.
The odour of the synthetic chemical weapon agent cyclosarin is also described as resembling peach.
Phenolic composition.
Total phenolics in mg/100 g of fresh weight were 14–102 in white-flesh nectarines, 18–54 in yellow-flesh nectarines, 28–111 in white-flesh peaches, 21–61 in yellow-flesh peaches. The major phenolic compounds identified in peach are chlorogenic acid, (+)-catechin and (-)-epicatechin. Other compounds, identified by HPLC, are gallic acid, neochlorogenic acid, procyanidin B1 and B3, procyanidin gallates, ellagic acid.
Rutin and isoquercetin are the primary flavonols found in "Clingstone" peaches.
Red-fleshed peaches are rich in anthocyanins of the cyanidin-3-O-glucoside type in six peach and six nectarine cultivars and of the malvin type in the "Clingstone" variety.
Color.
Peach is a color named for the pale color of the interior flesh of the peach fruit.
Trivia.
The Peachoid is a 4-story (150 feet tall) water tower in Gaffney, South Carolina, United States, that resembles a peach.

</doc>
<doc id="51258" url="http://en.wikipedia.org/wiki?curid=51258" title="Onion">
Onion

The onion ("Allium cepa" L.) (Latin 'cepa' = onion), also known as the bulb onion or common onion, is used as a vegetable and is the most widely cultivated species of the genus "Allium". This genus also contains several other species variously referred to as onions and cultivated for food, such as the Japanese bunching onion ("A. fistulosum"), the Egyptian onion ("A." ×"proliferum"), and the Canada onion ("A. canadense"). The name "wild onion" is applied to a number of "Allium" species but "A. cepa" is exclusively known from cultivation and its ancestral wild original form is not known, although escapes from cultivation have become established in some regions. The onion is most frequently a biennial or a perennial plant, but is usually treated as an annual and harvested in its first growing season.
The onion plant has a fan of hollow, bluish-green leaves and the bulb at the base of the plant begins to swell when a certain day-length is reached. In the autumn the foliage dies down and the outer layers of the bulb become dry and brittle. The crop is harvested and dried and the onions are ready for use or storage. The crop is prone to attack by a number of pests and diseases, particularly the onion fly, the onion eelworm and various fungi that cause rotting. Some varieties of "A. cepa" such as shallots and potato onions produce multiple bulbs.
Onions are cultivated and used around the world. As a foodstuff they are usually served cooked, as a vegetable or part of a prepared savoury dish, but can also be eaten raw or used to make pickles or chutneys. They are pungent when chopped and contain certain chemical substances which irritate the eyes.
Taxonomy and etymology.
The onion plant ("Allium cepa"), also known as the bulb onion or common onion, is the most widely cultivated species of the genus "Allium." It was first officially described by Carolus Linnaeus in his 1753 work "Species Plantarum". A number of synonyms have appeared in its taxonomic history:
"Allium cepa" is known exclusively from cultivation, but related wild species occur in Central Asia. The most closely related species include "Allium vavilovii" (Popov & Vved.) and "Allium asarense" (R.M. Fritsch & Matin) from Iran. However, Zohary and Hopf state that "there are doubts whether the "A. vavilovii" collections tested represent genuine wild material or only feral derivatives of the crop."
The vast majority of cultivars of "A. cepa" belong to the "common onion group" ("A. cepa" var. "cepa") and are usually referred to simply as "onions". The Aggregatum Group of cultivars ("A. cepa" var. "aggregatum") includes both shallots and potato onions.
The genus "Allium" also contains a number of other species variously referred to as onions and cultivated for food, such as the Japanese bunching onion ("A. fistulosum"), Egyptian onion ("A." ×"proliferum"), and Canada onion ("A. canadense").
'Cepa' is commonly accepted as Latin for 'onion' and has an affinity with Ancient Greek: κάπια (kápia), Albanian: qepë, Aromanian: tseapã, Catalan: ceba, English: chive, Occitan: ceba, Old French: cive, Romanian: ceapă.
Description.
The onion plant ("Allium cepa") is unknown in the wild but has been grown and selectively bred in cultivation for at least 7,000 years. It is a biennial plant but is usually grown as an annual. Modern varieties typically grow to a height of 15 to. The leaves are yellowish-green and grow alternately in a flattened, fan-shaped swathe. They are fleshy, hollow and cylindrical, with one flattened side. They are at their broadest about a quarter of the way up beyond which they taper towards a blunt tip. The base of each leaf is a flattened, usually white sheath that grows out of a basal disc. From the underside of the disc, a bundle of fibrous roots extends for a short way into the soil. As the onion matures, food reserves begin to accumulate in the leaf bases and the bulb of the onion swells.
In the autumn the leaves die back and the outer scales of the bulb become dry and brittle, and this is the time at which the crop is normally harvested. If left in the soil over winter, the growing point in the middle of the bulb begins to develop in the spring. New leaves appear and a long, stout, hollow stem expands, topped by a bract protecting a developing inflorescence. The inflorescence takes the form of a globular umbel of white flowers with parts in sixes. The seeds are glossy black and triangular in cross section.
Uses.
Historical use.
Bulbs from the onion family are thought to have been used as a food source for millennia. In Bronze Age settlements, traces of onion remains were found alongside date stones and fig remains that date back to 5000 BC. However, it is not clear if these were cultivated onions. Archaeological and literary evidence such as the Book of Numbers 11:5 suggests that onions were probably being cultivated around two thousand years later in ancient Egypt, at the same time that leeks and garlic were cultivated. Workers who built the Egyptian pyramids may have been fed radishes and onions.
The onion is easily propagated, transported and stored. The ancient Egyptians worshipped it, believing its spherical shape and concentric rings symbolized eternal life. Onions were even used in Egyptian burials, as evidenced by onion traces being found in the eye sockets of Ramesses IV.
In ancient Greece, athletes ate large quantities of onion because it was believed to lighten the balance of the blood. Roman gladiators were rubbed down with onions to firm up their muscles. In the Middle Ages, onions were such an important food that people would pay their rent with onions, and even give them as gifts. Doctors were known to prescribe onions to facilitate bowel movements and erections, and to relieve headaches, coughs, snakebite and hair loss.
Onions were taken by the first settlers to North America, where the Native Americans were already using wild onions in a number of ways, eating them raw or cooked in a variety of foods. They also used them to make into syrups, to form poultices and in the preparation of dyes. According to diaries kept by the colonists, bulb onions were one of the first things planted by the Pilgrim Fathers when they cleared the land for cropping in 1648.
Onions were also prescribed by doctors in the early 16th century to help with infertility in women. They were similarly used to raise fertility levels in dogs, cats and cattle, but this was an error as recent research has shown that onions are toxic to dogs, cats, guinea pigs and many other animals.
Culinary uses.
Onions are commonly chopped and used as an ingredient in various hearty warm dishes, and may also be used as a main ingredient in their own right, for example in French onion soup or onion chutney. They are very versatile and can be baked, boiled, braised, grilled, fried, roasted, sautéed or eaten raw in salads. Their layered nature makes them easy to hollow out once cooked, facilitating stuffing them. Onions are a staple in Indian cuisine, used as a thickening agent for curries and gravies. Onions pickled in vinegar are eaten as a snack. These are often a side serving in pubs and fish and chip shops throughout the United Kingdom and The Commonwealth, usually served with cheese and/or ale in the United Kingdom. In North America, sliced onions are battered and deep-fried and served as onion rings.
Onion types and products.
Common onions are normally available in three colour varieties. "Yellow" or "brown" onions (called "red" in some European countries), are full-flavoured and are the onions of choice for everyday use. Yellow onions turn a rich, dark brown when caramelized and give French onion soup its sweet flavour. The "red" onion (called "purple" in some European countries) is a good choice for fresh use when its colour livens up the dish. It is also used in grilling. "White" onions are the traditional onions that are used in classic Mexican cuisine. They have a golden colour when cooked and a particularly sweet flavour when sautéed.
While the large mature onion bulb is the onion most often eaten, onions can be eaten at immature stages. Young plants may be harvested before bulbing occurs and used whole as spring onions or scallions. When an onion is harvested after bulbing has begun but the onion is not yet mature, the plants are sometimes referred to as "summer" onions.
Additionally, onions may be bred and grown to mature at smaller sizes. Depending on the mature size and the purpose for which the onion is used, these may be referred to as "pearl", "boiler", or "pickler" onions, but differ from true pearl onions which are a different species. Pearl and boiler onions may be cooked as a vegetable rather than as an ingredient and pickler onions are often preserved in vinegar as a long-lasting relish.
Onions are available in fresh, frozen, canned, caramelized, pickled and chopped forms. The dehydrated product is available as kibbled, sliced, rings, minced, chopped, granulated and powder forms. Onion powder is a spice widely used when the fresh ingredient is not available. It is made from finely ground, dehydrated onions, mainly the pungent varieties of bulb onions, and has a strong odour. Being dehydrated, it has a long shelf life and comes in several varieties: yellow, red and white.
Non-culinary uses.
Onions have particularly large cells that are readily observed under low magnification. Forming a single layer of cells, the bulb epidermis is easy to separate for educational, experimental and breeding purposes.
Onions are therefore commonly employed in science education to teach the use of a microscope for observing cell structure.
The pungent juice of onions has been used as a moth repellent and can be rubbed on the skin to prevent insect bites. When applied to the scalp it is said to promote growth of hair and on the face to reduce freckling . It has been used to polish glass and copperware and to prevent rust on iron. If boiling water is poured onto chopped onions and left to cool, the resulting liquid can be sprayed onto plants to increase their resistance to pests, and the onion plants when growing are reputed to keep away moles and insects. Onion skins have been used to produce a yellow-brown dye.
Historically, onions were often used for cromniomancy across Europe, Africa and Northern Asia, and they continue to be used for this practice in some rural areas.
Nutrients and phytochemicals.
Most onion cultivars are about 89% water, 4% sugar, 1% protein, 2% fibre and 0.1% fat. Onions contain low amounts of essential nutrients (right table), are low in fats, and have an energy value of 166kJ (40 kcal) per 100 g (3.5 oz) serving. They contribute their flavor to savory dishes without raising caloric content appreciably.
Onions contain phytochemical compounds such as phenolics that are under basic research to determine their possible properties in humans.
There are considerable differences between onion varieties in polyphenol content, with shallots having the highest level, six times the amount found in Vidalia onions, the variety with the smallest amount. Yellow onions have the highest total flavonoid content, an amount 11 times higher than in white onions. Red onions have considerable content of anthocyanin pigments, with at least 25 different compounds identified representing 10% of total flavonoid content.
Some people suffer from allergic reactions after handling onions. Symptoms can include contact dermatitis, intense itching, rhinoconjunctivitis, blurred vision, bronchial asthma, sweating and anaphylaxis. There may be no allergic reaction in these individuals to the consumption of onions, perhaps because of the denaturing of the proteins involved during the cooking process.
While onions and other members of the genus "Allium" are commonly consumed by humans, they can be deadly for dogs, cats, guinea pigs, monkeys and other animals. The toxicity is caused by the sulfoxides present in raw and cooked onions, which many animals are unable to digest. Ingestion results in anaemia caused by the distortion and rupture of red blood cells. Sick pets are sometimes fed with tinned baby foods and any that contain onion should be avoided. The typical toxic doses are 5 g per kg (2.2 lb) bodyweight for cats and 15 to per kg for dogs.
Eye irritation.
Chopping an onion causes damage to cells which allows enzymes called alliinases to break down amino acid sulfoxides and generate sulfenic acids. A specific sulfenic acid, 1-propenesulfenic acid, is rapidly acted on by a second enzyme, the lachrymatory factor synthase (LFS), giving syn-propanethial-S-oxide, a volatile gas known as the onion lachrymatory factor or LF. This gas diffuses through the air and soon reaches the eye, where it activates sensory neurons, creating a stinging sensation. Tear glands produce tears in order to dilute and flush out the irritant.
Eye irritation can be avoided by cutting onions under running water or submerged in a basin of water. Leaving the root end intact also reduces irritation as the onion base has a higher concentration of sulphur compounds than the rest of the bulb. Refrigerating the onions before use reduces the enzyme reaction rate and using a fan can blow the gas away from the eyes. The more often one chops onions, the less one experiences eye irritation.
The amount of sulfenic acids and LF released and the irritation effect differs among "Allium" species. In 2008, the New Zealand Crop and Food institute created a strain of "no tears" onions by using gene-silencing biotechnology to prevent synthesis by the onions of the LFS enzyme.
Cultivation.
Onions are best cultivated in fertile soils that are well-drained. Sandy loams are good as they are low in sulphur, while clayey soils usually have a high sulphur content and produce pungent bulbs. Onions require a high level of nutrients in the soil. Phosphorus is often present in sufficient quantities but may be applied before planting because of its low level of availability in cold soils. Nitrogen and potash can be applied at intervals during the growing season, the last application of nitrogen being at least four weeks before harvesting. Bulbing onions are day-length sensitive; their bulbs begin growing only after the number of daylight hours has surpassed some minimal quantity. Most traditional European onions are what is referred to as "long-day" onions, producing bulbs only after 14+ hours of daylight occurs. Southern European and North African varieties are often known as "intermediate-day" types, requiring only 12–13 hours of daylight to stimulate bulb formation. Finally, "short-day" onions, which have been developed in more recent times, are planted in mild-winter areas in the fall and form bulbs in the early spring, and require only 11–12 hours of daylight to stimulate bulb formation. Onions are a cool-weather crop and can be grown in USDA zones 3 to 9. Hot temperatures or other stressful conditions cause them to "bolt", meaning that a flower stem begins to grow.
Onions may be grown from seed or from sets. Onion seeds are short-lived and fresh seed germinates better. The seeds are sown thinly in shallow drills, thinning the plants in stages. In suitable climates, certain cultivars can be sown in late summer and autumn to overwinter in the ground and produce early crops the following year. Onion sets are produced by sowing seed thickly in early summer in poor soil and the small bulbs produced are harvested in the autumn. These bulbs are planted the following spring and grow into mature bulbs later in the year. Certain cultivars are used for this purpose and these may not have such good storage characteristics as those grown directly from seed.
Routine care during the growing season involves keeping the rows free of competing weeds, especially when the plants are young. The plants are shallow-rooted and do not need a great deal of water when established. Bulbing usually takes place after twelve to eighteen weeks. The bulbs can be gathered when needed to eat fresh, but if they will be kept in storage, they should be harvested after the leaves have died back naturally. In dry weather they can be left on the surface of the soil for a few days to dry out properly; then they can be placed in nets, roped into strings or laid in layers in shallow boxes. They should be stored in a well-ventilated, cool place such as a shed.
Pests and diseases.
Onions suffer from a number of plant disorders. The most serious for the home gardener are likely to be the onion fly, stem and bulb eelworm, white rot and neck rot. Diseases affecting the foliage include rust and smut, downy mildew and white tip disease. The bulbs may be affected by splitting, white rot and neck rot. Shanking is a condition in which the central leaves turn yellow and the inner part of the bulb collapses into an unpleasant-smelling slime. Most of these disorders are best treated by removing and burning affected plants. The larvae of the onion leaf miner or leek moth ("Acrolepiopsis assectella") sometimes attacks the foliage and may burrow down into the bulb.
The onion fly ("Delia antiqua") lays eggs on the leaves and stems and on the ground close to onion, shallot, leek and garlic plants. The fly is attracted to the crop by the smell of damaged tissue and is liable to occur after thinning. Plants grown from sets are less prone to attack. The larvae tunnel into the bulbs and the foliage wilts and turns yellow. The bulbs are disfigured and rot, especially in wet weather. Control measures may include crop rotation, the use of seed dressings, early sowing or planting and the removal of infested plants.
The onion eelworm ("Ditylenchus dipsaci"), a tiny parasitic soil-living nematode, causes swollen distorted foliage. Young plants are killed and older ones produce soft bulbs. There is no cure and affected plants should be uprooted and burnt. The site should not be used for growing onions again for several years and should also be avoided for growing carrots, parsnips and beans, which are also susceptible to the eelworm.
White rot of onions, leeks and garlic is caused by the soil-borne fungus "Sclerotium cepivorum". As the roots rot, the foliage turns yellow and wilts. The base of the bulbs is attacked and becomes covered by a fluffy white mass of mycelia which later produces small, globular black structures called sclerotia. These resting structures remain in the soil to reinfect a future crop. There is no cure for this fungal disease, so affected plants should be removed and destroyed and the ground used for unrelated crops in subsequent years.
Neck rot is a fungal disease affecting onions in storage. It is caused by "Botrytis allii", which attacks the neck and upper parts of the bulb, causing a grey mould to develop. The symptoms often first occur where the bulb has been damaged and spread downwards in the affected scales. Large quantities of spores are produced and crust-like sclerotia may also develop. In time a dry rot sets in and the bulb becomes a dry, mummified structure. This disease may be present throughout the growing period but only manifest itself when the bulb is in store. Anti-fungal seed dressings are available and the disease can be minimised by preventing physical damage to the bulbs at harvesting, careful drying and curing of the mature onions and correct storage in a cool, dry place with plenty of circulating air.
Storage in the home.
Cooking onions and sweet onions are better stored at room temperature, optimally in a single layer, in mesh bags in a dry, cool, dark, well-ventilated location. In this environment, cooking onions have a shelf life of three to four weeks and sweet onions one to two weeks. Cooking onions will absorb odours from apples and pears. Also, they draw moisture from vegetables with which they are stored which may cause them to decay.
Sweet onions have a greater water and sugar content than cooking onions. This makes them sweeter- and milder-tasting but reduces their shelf life. Sweet onions can be stored refrigerated; they have a shelf life of approximately one month. Irrespective of type, any cut pieces of onion are best tightly wrapped, stored away from other produce, and used within two to three days.
Varieties.
Common onion group (var. "cepa").
Most of the diversity within "A. cepa" occurs within this group, the most economically important "Allium" crop. Plants within this group form large single bulbs, and are grown from seed or seed-grown sets. The majority of cultivars grown for dry bulbs, salad onions, and pickling onions belong to this group. The range of diversity found among these cultivars includes variation in photoperiod (length of day that triggers bulbing), storage life, flavour, and skin colour. Common onions range from the pungent varieties used for dried soups and onion powder to the mild and hearty sweet onions, such as the Vidalia from Georgia, USA, or Walla Walla from Washington that can be sliced and eaten on a sandwich instead of meat.
Aggregatum group (var. "aggregatum").
This group contains shallots and potato onions, also referred to as multiplier onions. The bulbs are smaller than those of common onions, and a single plant forms an aggregate cluster of several bulbs. They are propagated almost exclusively from daughter bulbs, although reproduction from seed is possible. Shallots are the most important subgroup within this group and comprise the only cultivars cultivated commercially. They form aggregate clusters of small, narrowly ovoid to pear-shaped bulbs. Potato onions differ from shallots in forming larger bulbs with fewer bulbs per cluster, and having a flattened (onion-like) shape. However, intermediate forms exist.
I'itoi onion ("Allium cepa") is a prolific multiplier onion cultivated in the Baboquivari Peak Wilderness, Arizona area. This small bulb type has a shallot-like flavour and is easy to grow and ideal for hot, dry climates. Bulbs are separated, and planted in the fall 1 inch below surface and 12 inches apart. Bulbs will multiply into clumps and can be harvested throughout the cooler months. Tops will die back in the heat of summer and may return with heavy rains; bulbs can remain in the ground or be harvested and stored in a cool dry place for planting in the fall. The plants rarely flower; propagation is by division.
Hybrids with "A. cepa" parentage.
A number of hybrids are cultivated that have "A. cepa" parentage, such as the diploid tree onion or Egyptian onion ("A." ×"proliferum"), Wakegi onion ("A." ×"wakegi"), and the triploid onion ("A." ×"cornutum").
The tree onion or Egyptian onion produces bulblets in the umbel instead of flowers, and is now known to be a hybrid of "A. cepa" × "A. fistulosum". It has previously been treated as a variety of "A. cepa", for example "A. cepa" var. "proliferum", "A. cepa" var. "bulbiferum", and "A. cepa" var. "viviparum".
The Wakegi onion is also known to be a hybrid between "A. cepa" and "A. fistulosum", with the "A. cepa" parent believed to be from the Aggregatum Group of cultivars. It has been grown for centuries in Japan and China for use as a salad onion.
Under the rules of botanical nomenclature, both the Egyptian onion and Wakegi onion should be combined into one hybrid species, having the same parent species. Where this is followed, the Egyptian onion is named "A." ×"proliferum" Eurasian Group and the Wakegi onion is named "A." ×"proliferum" East Asian Group.
The triploid onion is a hybrid species with three sets of chromosomes, two sets from "A. cepa" and the third set from an unknown parent. Various clones of the triploid onion are grown locally in different regions, such as 'Ljutika' in Croatia, and 'Pran', 'Poonch' and 'Srinagar' in the India-Kashmir region. 'Pran' is grown extensively in the Northern Indian provinces of Jammu and Kashmir. There are very small genetic differences between 'Pran' and the Croatian clone 'Ljutika', implying a monophyletic origin for this species.
Some authors have used the name "A. cepa" var. "viviparum" (Metzg.) Alef. for the triploid onion, but this name has also been applied to the Egyptian onion. The only name unambiguously connected with the triploid onion is "A." ×"cornutum".
Spring onions or salad onions may be grown from the Welsh onion ("A. fistulosum") as well as from "A. cepa". Young plants of "A. fistulosum" and "A. cepa" look very similar, but may be distinguished by their leaves, which are circular in cross-section in "A. fistulosum" rather than flattened on one side.
Production and trade.
It is estimated that around the world, over 9000000 acre of onions are grown annually. About 170 countries cultivate onions for domestic use and about eight percent of the global production is traded internationally.
The Onion Futures Act, passed in 1958, bans the trading of futures contracts on onions in the United States. This prohibition came into force after farmers complained about alleged market manipulation by Sam Seigel and Vincent Kosuga at the Chicago Mercantile Exchange two years earlier. The subsequent investigation provided economists with a unique case study into the effects of futures trading on agricultural prices. The act remains in effect as of 2013[ [update]].
Further reading.
</dl>

</doc>
<doc id="51259" url="http://en.wikipedia.org/wiki?curid=51259" title="Montgomery County, New York">
Montgomery County, New York

Montgomery County is a county located in the U.S. state of New York. As of the 2010 census, the population was 50,219. The county seat is Fonda. The county was named in honor of Richard Montgomery, an American Revolutionary War general killed in 1775 at the Battle of Quebec. It was created in 1772 as Tryon County and in 1784 was renamed Montgomery County.
Montgomery County comprises the Amsterdam, NY Micropolitan Statistical Area. The county borders the north and south banks of the Mohawk River.
History.
In 1784, following end of the American Revolutionary War, the name of Tryon County was changed to Montgomery County. This change was to honor the general, Richard Montgomery, who had captured several places in Canada and died in 1775 attempting to capture the city of Quebec during the Revolutionary War. It replaced the name formerly honoring the last provincial governor of New York.
In 1789, Ontario County was split off from Montgomery. The area split off from Montgomery County was much larger than the present county, as it also included the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne counties.
In 1791, Herkimer, Otsego, and Tioga counties were split off from Montgomery.
In 1802, portions of Clinton, Herkimer, and Montgomery counties were combined to form St. Lawrence County.
In 1816, Hamilton County was split off from Montgomery.
In 1838, Fulton County was split off from Montgomery.
In 2012, Montgomery County voters approved a charter, making it the 21st county in New York to do so. In 2013, Matthew L. Ossenfort was elected the first County Executive in the county's history. Ossenfort took office in 2014, the same year the charter went into effect. Under the terms of the charter, the Board of Supervisors was replaced by a nine-member County Legislature, with members elected from single-member districts. Thomas L. Quackenbush was elected the first Chairman of the new Legislature.
Geography.
According to the U.S. Census Bureau, the county has a total area of 410 sqmi, of which 403 sqmi is land and 7.3 sqmi (1.8%) is water.
Montgomery County is located in the central part of the state, west of the city of Schenectady and northwest of Albany.
Adjacent counties.
The Erie Canal runs through Montgomery County parallel to the Mohawk River and went west through the state to the Wood River. It connected Great Lakes shipping with the Hudson River. Several towns and villages grew up along the canal, as it carried much trade and passenger traffic during its peak years. After the railroad was built through the state, along the same river plain, it superseded the canal, which was filled in some areas. In the mid-twentieth century, the NYS Thruway was constructed parallel to the former east-west routes of the canal and railroad.
Today the Erie Canal and its lock system is used primarily for recreational boat use among locals and tourists. At the time of the canal's construction, Montgomery County was the only place where there was a break in the Appalachian Mountains. Called 'The Noses,' because of canal construction, it became known as "the gateway to the West".
Montgomery County is located in the heart of the state's Mohawk Valley region. Foothills of the Catskill Mountains dot the southern part of the county, while foothills of the Adirondack Mountains dot the north.
Demographics.
As of the census of 2010, there were 50,208 people, 20,073 households, and 13,131 families residing in the county. The population density was 123 people per square mile (47/km²). There were 22,522 housing units at an average density of 56 per square mile (21/km²). The racial makeup of the county was 92.87% (83.8% Non-Hispanic) (9.07 White Hispanic) White, 1.15% African American, 0.25% Native American, 0.53% Asian, 0.01% Pacific Islander, 3.92% from other races, and 1.27% from two or more races. Hispanic or Latino of any race were 12.91% of the population. 19.0% were of Italian, 15.9% German, 13.5% Polish, 9.8% Puerto RIcan 9.1% Irish, 7.9% American and 6.4% English ancestry according to Census 2010. 86.8% spoke English, 9.3% Spanish,1.8% Italian and 1.1% Polish as their first language.
There were 20,038 households out of which 29.40% had children under the age of 18 living with them, 49.00% were married couples living together, 11.60% had a female householder with no husband present, and 34.60% were non-families. 29.50% of all households were made up of individuals and 14.90% had someone living alone who was 65 years of age or older. The average household size was 2.42 and the average family size was 2.98.
In the county the population was spread out with 24.50% under the age of 18, 7.20% from 18 to 24, 26.30% from 25 to 44, 22.90% from 45 to 64, and 19.20% who were 65 years of age or older. The median age was 40 years. For every 100 females there were 91.40 males. For every 100 females age 18 and over, there were 87.90 males.
The median income for a household in the county was $33,128, and the median income for a family was $40,688. Males had a median income of $31,818 versus $23,359 for females. The per capita income for the county was $17,005. About 9.00% of families and 13.7% of the population were below the poverty line, including 17.80% of those under age 18 and 9.89% of those age 65 or over.
Politics and government.
Montgomery County lies in New York's 21st Congressional District and is represented in Congress by Paul Tonko, a lifelong resident of Amsterdam. While Democrats have been elected to local office, Republican candidates have a +5 margin in Presidential elections.

</doc>
<doc id="51260" url="http://en.wikipedia.org/wiki?curid=51260" title="Bulb">
Bulb

In botany, a bulb is a short stem with fleshy leaves or leaf bases that function as food storage organs during dormancy. (In gardening, plants with other kinds of storage organ are also called "ornamental bulbous plants" or just "bulbs".)
A bulb's leaf bases, also known as scales, generally do not support leaves, but contain food reserves to enable the plant to survive adverse weather conditions. At the center of the bulb is a vegetative growing point or an unexpanded flowering shoot. The base is formed by a stem, and plant growth occurs from this basal plate. Roots emerge from the underside of the base, and new stems and leaves from the upper side. Tunicate bulbs have dry, membranous outer scales that protect the continuous lamina of fleshy scales. Species in the genera "Allium", "Hippeastrum", "Narcissus", and "Tulipa" all have tunicate bulbs. Non-tunicate bulbs, such as "Lilium" and "Fritillaria" species, lack the protective tunic and have looser scales.
 
Other types of storage organs (such as corms, rhizomes, and tubers) are sometimes referred to as bulbs, although as the term is used in botany, they are not. The technical term for plants that form underground storage organs, including bulbs as well as tubers and corms, is geophyte. Some epiphytic orchids (family Orchidaceae) form above-ground storage organs called pseudobulbs, that superficially resemble bulbs.
Nearly all plants that form true bulbs are monocotyledons, and include:
"Oxalis", in the family Oxalidaceae, is the only dicotyledon genus that produces true bulbs.
Bulbous plant species cycle through vegetative and reproductive growth stages; the bulb grows to flowering size during the vegetative stage and the plant flowers during the reproductive stage. Certain environmental conditions are needed to trigger the transition from one stage to the next, such as the shift from a cold winter to spring. Once the flowering period is over, the plant enters a foliage period of about six weeks during which time the plant absorbs nutrients from the soil and energy from the sun for setting flowers for the next year. Bulbs dug up before the foliage period is completed will not bloom the following year but then should flower normally in subsequent years.
After the foliage period is completed, bulbs may be dug up for replanting elsewhere. Any surface moisture should be dried, then the bulbs may be stored up to about 4 months for a fall planting. Storing them much longer than that may cause the bulbs to dry out inside and become nonviable.
Bulbil.
A bulbil is a small bulb, and may also be called a bulblet, bulbet, or bulbel.
Small bulbs can develop that replace or propagate a large bulb. If one or several moderate-sized bulbs form to replace the original bulb, they are called renewal bulbs. Increase bulbs are small bulbs that develop either on each of the leaves inside a bulb, or else on the end of small underground stems connected to the original bulb.
Some lilies form small bulbs, called bulbils, in their leaf axils. Several members of the onion family, Alliaceae, including "Allium sativum" (garlic), form bulbils in their flower heads, sometimes as the flowers fade, or even instead of the flowers. The so-called tree onion ("Allium cepa" var. "proliferum") forms small onions which are large enough for pickling.
Some ferns, such as Hen and Chicken Fern produce new plants at the tips of the fronds' pinnae, which are sometimes referred to a bulbils.

</doc>
<doc id="51263" url="http://en.wikipedia.org/wiki?curid=51263" title="Iaijutsu">
Iaijutsu

Iaijutsu (居合術), a combative quick-draw sword technique. This art of drawing the Japanese sword, katana, is one of the Japanese "koryū" martial art disciplines in the education of the classical warrior (bushi).:50
Purpose.
Iaijutsu is a combative sword-drawing art but not necessarily an aggressive art because iaijutsu is also a counterattack-oriented art. Iaijutsu technique may be used aggressively to wage a premeditated surprise attack against an unsuspecting enemy.:14,50 The formulation of iaijutsu as a component system of classical bujutsu was made less for the dynamic situations of the battlefield than for the relatively static applications of the warrior's daily life off the field of battle. . :52
Etymology.
Historically, it is unclear when the term "iaijutsu" accurately performed, and when techniques to draw katana from the scabbard was first practiced as decided form of exercise. The Japanese sword has existed since the Nara period (710-794), where techniques to draw the sword have been practiced under other names than 'iaijutsu'. The term 'iaijutsu' was first verified in connection with Iizasa Chōisai Ienao (c. 1387 - c. 1488), founder of the school Tenshin Shōden Katori Shintō-ryū.
History.
Archaeological excavations dated the oldest sword in Japan from at least as early as second century B.C.:4 The "Kojiki" (Record of Ancient Matters) and the "Nihon Shoki" (History of Japan), ancient texts on early Japanese history and myth that were compiled in the eighth century A.D., describe iron swords and swordsmanship that pre-date recorded history, attributed to the mythological age of the gods (kami).:3
The development of Japanese swordsmanship as a component system of classical bujutsu created by and for professional warriors (bushi), begins only with the invention and widespread use of the Japanese sword, the curved, single-cutting-edged long sword.
In its curved form, the sword is known to the Japanese as tachi in the eighth century.:8 It evolved from and gained ascendancy over its straight-bladed prototype because years of battlefield experience proved that the curved form of sword was better suited to the needs of the bushi than the straight-bladed kind. Around the curved long sword the bushi built a mystique of fantastic dimensions, one that still influences Japanese culture today. The nature of the bushi's combative deployment, mounted as he was on horseback, required the classical warrior to reach out for his enemy, who might either be similarly mounted or otherwise ground-deployed.
During the Kamakura period (1185-1333) the Japanese sword smiths achieved the highest level of technical excellence and because the war between two influential families, the Minamoto and the Taira, made it possible to test and evaluate swords under the severest of conditions. By the end of the Kamakura period the tachi was superseded by a shorter weapon in a new form, called katana.:13
It was with the general widespread use of the curved sword mounted and worn as a katana that classical Japanese swordsmanship for infantry applications really begins. It is not until the fifteenth century that there are evidence in reliable documentary form to prove that the bushi practiced swordsmanship in a systematic manner. In this connection it is belief that kenjutsu, which deals with the art of swordsmanship as it is performed with a sword that has already been brought into unsheathed position, is the senior form to iaijutsu.:
Iaijutsu is extant today but there also exists a modern form for drawing the Japanese sword, called iaido. Iaido, the way of drawing the sword, appeared as a term in 1932.
Postures.
According to Donn F. Draeger, iaijutsu is a combative art and, therefore, the warrior considered only two starting positions in the execution of a sword-drawing technique. The first techniques is from the low crouching posture, named "iai-goshi". The other is the standing posture, named "tachi-ai".:50
The seated posture, "tate-hiza", is not used in iaijutsu because it does not permit all-around mobility. "Seiza", the formal kneeling-sitting posture, is not used because it is a "dead" posture which is regarded by the warrior as less combatively efficient. It would be difficult for the swordsman using either of these two latter postures to go quickly into action in an emergency.:50
Koryū Schools.
Ryū that still exist and include iaijutsu in their curriculum are listed below. The school below are "koryū", or arts developed before the Meiji era.:
Mugai ryu.
Mugai ryu was once one of the more famous styles in Japan in the Edo period and was developed from a strong influence of Zen. It is characterized by short, direct movements. As it was developed in 1697 by Tsuji Gettan Sukemochi [or Sukeshige], a Zen practitioner, it has deep links with Zen Buddhism. The original style created by Gettan was a kenjutsu school rather than iaido. Today's Mugai ryu iaido was established by Takahashi Hachisuke Mitsusuke and his younger brother Hidezu in mid Edo period. They studied a style called Jikyo-ryū under the fifth and last generation headmasters Yamamura Masashige. There are several distinct lineages of Mugai ryu throughout Japan today.
Suiō-ryū.
Suiō-ryū is a traditional style that specialises in sword drawing, both solo and paired, but other arts, like jōjutsu, naginatajutsu, kenpō and kusarigamajutsu are practised as well. It was founded by Mima Yoichizaemon Kagenobu c. 1615.
Other styles.
Other styles that incorporate sword drawing in their curriculum are, for example, Shindō Munen-ryū, Hokushin Ittō-ryū, Shinkage-ryū, Hōki-ryū, Tatsumi-ryū, Tamiya-ryū, Takenouchi-ryū, Eishin-ryū and:

</doc>
<doc id="51265" url="http://en.wikipedia.org/wiki?curid=51265" title="Kenjutsu">
Kenjutsu

Kenjutsu (剣術) is the umbrella term for all ("koryū") schools of Japanese swordsmanship, in particular those that predate the Meiji Restoration. The modern styles of kendo and iaido that were established in the 20th century included modern form of kenjutsu in their curriculum too. Kenjutsu, which originated with the samurai class of feudal Japan, means "the method, or technique, of the sword." This is opposed to kendo, which means "the way of the sword".
The exact activities and conventions undertaken when practicing "kenjutsu" vary from school to school, where the word school here refers to the practice, methods, ethics, and metaphysics of a given tradition, yet commonly include practice of battlefield techniques without an opponent and techniques whereby two practitioners perform "kata" (featuring full contact strikes to the body in some styles and no body contact strikes permitted in others).
Historically, schools incorporated sparring under a variety of conditions, from using solid wooden "bokutō" to use of bamboo sword ("shinai") and armor ("bōgu").:XII, XIII In modern times sparring in Japanese martial art is more strongly associated with "kendo".
History.
Early development.
It is thought likely that the first iron swords were manufactured in Japan in the fourth century, based on technology imported from China via Korean peninsula.:1 While swords clearly played an important cultural and religious role in ancient Japan,:5, 14 in the Heian period the globally recognised curved Japanese sword was developed and swords became important weapons and symbolic items.:15 The oldest schools in existence today arose in the Muromachi period (1336 to 1573), known for long periods of inter-state warfare. Three major schools emerged during this period.:XII
These schools form the ancestors for many descendent styles, for example, from Ittō ryū has branched Ono-ha Ittō ryū and Mizoguchi-ha Ittō-ryū (among many others).
On the island of Okinawa, the art of Udundi includes a unique method of both Kenjutsu and Iaijutsu. This is the only surviving sword system from Okinawa. It was the martial art of the noble Motobu family during the Ryukyu Kingdom.
Edo period.
During the Edo period schools proliferated to number more than 500,:XIII and training techniques and equipment advanced. The 19th century led to the development of the bamboo practice sword, the "shinai", and protective armor, "bogu". This allowed practice of full speed techniques in sparring, while reducing risk of serious harm to the practitioner. Before this, training in Kenjutsu had consisted mainly of basic technique practice and paired "kata", using solid wooden practice swords ("bokutō") or live blades.:XIII
Decline.
Beginning in 1868, the Meiji Restoration led to the breakup of the military class and the modernization of Japan along the lines of western industrial nations. As the "samurai" class was officially dissolved at this time, "kenjutsu" fell into decline, an unpopular reminder of the past.:XIII, XIV This decline continued for approximately 20 years, until rising national confidence led to an increase of the uptake of traditional sword arts again, particularly in the military and the police.
In 1886 the Japanese Police gathered together "kata" from a variety of "kenjutsu" schools into a standardised set for training purposes.:11 This process of standardization of martial training continued when, in 1895, a body for martial arts in Japan, the Dai Nippon Butoku Kai, was established. Work on standardizing "kenjutsu" "kata" continued for years, with several groups involved:11,12 until in 1912 an edict was released by the Dai Nippon Butoku Kai. This edict highlighted a lack of unity in teaching and introduced a standard core teaching curriculum to which the individual "kenjutsu" schools would add their distinctive techniques. This core curriculum, and its ten "kata" evolved into the modern martial art of "kendo".:11,14 This point could be regarded as the end of the development of Kendo Kata was provided for the unification of many schools to enable them to pass on the techniques and spirit of the Japanese sword.
20th and 21st century.
With the increasing interest in Japanese martial arts outside Japan during the 20th century, people in other countries started taking an interest in kenjutsu. Many martial artists who study Japanese martial arts know the principles of kenjutsu. For modern kenjutsu type training, most practice is done in suburi style with bokken.
Weapons.
One of the more common training weapons is the wooden sword ("bokuto" or "bokken"). For various reasons, many schools make use of very specifically designed "bokuto", altering its shape, weight and length according to the style's specifications. For example, "bokuto" used within Yagyū Shinkage-ryū are relatively thin and without a handguard in order to match the school's characteristic approach to combat. Alternatively, Kashima Shin-ryū practitioners use a thicker than average "bokuto" with no curvature and with a rather large hilt. This of course lends itself well to Kashima Shin-ryū's distinct principles of combat.
Some schools practice with "fukuro" "shinai" (a bamboo sword covered with leather or cloth) under circumstances where the student lacks the ability to safely control a "bokuto" at full speed or as a general safety precaution. In fact, the "fukuro shinai" dates as far back as the 15th century.
Techniques and styles.
Nitōjutsu.
A distinguishing feature of many kenjutsu syllabi is the use of a paired katana or "daitō" and wakizashi or "shōtō" commonly referred to as "nitōjutsu" (二刀術, two sword methods). Styles that teach it are called "nitōryū" (二刀流, two sword school); contrast "ittō-ryū" (一刀流, one sword school). The most famous exponent of "nitōjutsu" was Miyamoto Musashi (1584 – 1645), the founder of Hyōhō Niten Ichi-ryū, who advocates it in "The Book of Five Rings". "Nitōjutsu" is not however unique to Hyoho Niten Ichi-ryū, nor was "nitōjutsu" the creation of Musashi. Both Tenshin Shōden Katori Shinto-ryū were founded in the early Muromachi period (ca. 1447), and Tatsumi-ryu founded Eishō period (1504–1521), contain extensive "nitōjutsu" curricula while also preceding the establishment of Musashi’s Hyoho Niten Ichi-ryū.
Techniques.
Kenjutsu techniques can be compared to the strategies of warfare, while batto-jutsu or kendo can be compared to shooting range techniques. As in the "Book of Five Rings", by Miyamoto Musashi, a kenjutsuka (a practitioner of kenjutsu) relies on the conditions of the ground, light source, as well as the opponents' capabilities, before implementing a practical attack. The attack is not set on any particular weapon or move to capitulate, nor is there a predisposed target or trajectory. Any exposed part of the opponents body is a possible target (as in Musashi's "Injuring the Corners"). The most basic cutting technique, used in kendo and, particularly, in Eishin-ryū is kesagake or kesagiri. It is a downward diagonal cut, once used to cut the enemy from shoulder (collar bone) to waist (hip-bone). Opening the front rib-cage.
To be effective, a kenjutsu strike/or counter-strike is a composition of several techniques: feigning, cutting, jabbing, thrusting, parrying or binding, footwork, choice of weapon, and even knowing the opponents weapon. It was mentioned that once Musashi realized the physics of the chain-and-sickle (kusarigama)
, he was then able to defeat it.
The feigning techniques are effective movements of the weapon, footwork, center of gravity, and even the use of kiai. Applied effectively, the opponent is set-back one move, while creating an opening elsewhere. The feigning technique should be angled to allow a quick direct shot from this position. Only sufficient practice will perfect these techniques and teaching to convey the training of proper reflexes. There is not much time to think during a skirmish or battle. A fluent continuation of techniques must be deployed to manage even multiple opponents. One second per opponent is too long. Managing an army should be treated the same way. A practical understanding of the body, weapon and timing is necessary to be able to dispatch a strike or counter strike whether standing, walking, or rolling around the ground (or whether an army is attacking or retreating). There is no time-out or ready position. It might be a fight under minimum visibility or total darkness. When striking range is reached, reflexes dictate the outcome.
Cutting, jabbing, and thrusting techniques must be all preceded by a feint (except when using the quick strike techniques). The defender can easily parry a strong attack, due to the telegraphing momentum behind the attacker's weapon. Therefore, a strong cutting technique can easily receive a deadly cut across the sword hand or forearm. The feigning movement should complement both double-sword, two-handed sword, or any weapon.
There are some strikes that do not require a preceding subterfuge. These are referred to as "quick strikes". They are done with two hands on the sword or with a sword in each hand. One hand is at the base of the tsuka (to provide longer reach) and the other hand is at the ridge of the blade to provide the initial force to flick the sword as quick as an arrow to hit the target. This could be done with the double sword, with one sword providing the push for the dispatch. These postures are hidden and the ready positions are implemented while switching hands or while changing steps. These flicking strikes can be administered from any angle (top, sides or below).
When parrying, always try to direct the point of the sword to the target. This minimizes the step needed to be able to counter-attack. Thus the opponent is at an immediate disadvantage. Also, using the quick strike at the opponent's sword hand or forearm will immediately incapacitate his attack before having to parry it. A simple rule — to keep the point of the sword pointed to the opponent or at within the area of the gate, while attempting to parry in all angles — will provide a good foundation for appropriate counter-maneuver reflexes.
Musashi said that the footwork shall be adapted to terrain and purpose. The correct stride is to be applied to whatever leverage is needed to effectively wield the weapon at hand. The choice of weapon and knowing the opponents' weapons is essential for the choice of right technique and strategy. Knowing the center of gravity of a weapon can help the assessment of its maneuverability and speed, as much as its effects on leverage and kinetic forces.
The use of the double-sword (one in each hand) can provide the ultimate control of the gate. The "gate", as referred to by Miyamoto Musashi, is the opening between two fighters. All attacks must go through this gate to reach the target from any angle. To close or disrupt the gate at the right moment is necessary to deflect incoming attacks. The double swords' ability to alternate and complement their trajectories provides a strong continuous flowing barricade as well as trapping and striking repetitions. Timing is essential in the use of this technique, and Musashi advised that the double-sword technique should be learned early on.
In the later stages of kenjutsu, one can win without the use of a blade by merely understanding the physics of sword work. A kenjutsuka can resolve or win without having to fight (or without having to cut) — and gain followers instead. There is no individual or religion that started this. Any level-headed person would not want to maim or kill another human being. A kenjutsuka (a true swordsman) strives to attain well beyond cutting techniques: to serve his master or act on his own as a diplomat of fairness in the living hell.

</doc>
<doc id="51269" url="http://en.wikipedia.org/wiki?curid=51269" title="Unit 101">
Unit 101

Commando Unit 101 (Hebrew: יחידה 101) was a special forces unit of the Israeli Defense Forces (IDF), founded and commanded by Ariel Sharon on orders from Prime Minister David Ben-Gurion in August 1953. They were armed with non-standard weapons and tasked with carrying out retribution operations across the state's borders—in particular, establishing small unit maneuvers, activation and insertion tactics.
Members of the unit were recruited only from agricultural kibbutzim and moshavim. Membership in the unit was by invitation only, and any new member had to be voted on by all existing members before they were accepted. 
The unit was merged into the 890th Paratroop Battalion during January 1954, on orders of General Dayan, Chief of Staff, because he wanted their experience and spirit to be spread among all infantry units of IDF starting with the paratroopers. They are considered to have had a significant influence on the development of subsequent Israeli infantry-oriented units.
Background.
Following the 1948 Arab–Israeli War, Israel was faced with cross-border raids and infiltrations by Arab militants and non-militants respectively. Many of these were small scale infiltrations that consisted of unarmed Palestinian refugees attempting to rejoin their families and of smugglers bringing in contraband for Israeli markets.<ref name="UNS/636">'No one would deny that the Israel authorities would be justified, and are justified, in using strong measures to check (infiltration), in so far as damage to property or loss of life results. But not everyone who crosses the armistice demarcation line does so with criminal intent. Acts of violence are indeed committed, but as the volume of illegal crossings of the demarcation line is so considerable, if one is to judge from the available statistics, it seems probable that "many crossings are carried out by persons – sometimes, I understand, even by children – with no criminal object in view". 'England's ambassador to the UN = para.52 9 November 1953</ref> These were later followed with attacks launched by refugees often motivated by economic reasons, but they were quickly adopted by the military of the neighboring Arab states, who organized them into semi-formal brigades which mounted larger scale operations from 1954 onwards. According to Israel, about 9,000 attacks were launched from 1949 to 1956, resulting in hundreds of Israeli civilian casualties.
At the same time the IDF was ill-prepared to respond to these raids. The Palmach, its three best combat units of the 1948 war, had been disbanded at Ben-Gurion's instruction. Many experienced officers had left the army after the war, and Israeli society had undergone a difficult period of impoverishment. As a result the IDF did not have any units capable of effective reprisal, and did not perform well in offensive operations.
The Palestinians must learn that they will pay a high price for Israeli lives.—A conversation between David Ben-Gurion and Ariel Sharon.
As a response to this problem the IDF formed Unit 30 in 1951—a secret unit that belonged to the IDF Southern Command. Their purpose was to execute retribution missions while operating in compact and well-trained teams. Unfortunately for the IDF the officers lacked the required training and executed their duties poorly, leading to the unit's disbandment 1952.
One of Sharon's final operations before leaving the army in 1952 was the semi-successful Operation Bin Nun Alef into Jordan. During the operation he suffered serious injuries, after which Sharon had recommended to the General Staff that an elite force, trained in commando tactics, be set up for reprisal operations. After a series of unsuccessful retribution infiltrations by existing IDF units, Ben Gurion pressed Chief of Staff Mordechai Maklef to establish such a special forces unit in the summer of 1953. This was Israel's first, and reservist Ariel Sharon was called back to duty.
Sharon was given the rank of Major and chosen to command the company-sized unit, with Shlomo Baum as deputy in command. The unit was to consist of 50 men, most of them former Tzanhanim and Unit 30 personnel. They were armed with non-standard weapons and tasked with carrying out special reprisals across the state’s borders—mainly establishing small unit maneuvers, activation and insertion tactics that are utilized even today.
The new unit began a hard process of day and night training. Some of their exercises frequently took them across the border, as enemy engagement was seen as the best preparation. The recruits went on forced marches and undertook weapons and sabotage training at their base camp at Sataf, a depopulated Arab village just west of Jerusalem.
In addition to the unit's tactical variation, they were also unique in two ways:
Originally T'zanhanim (Hebrew: הצנחנים‎, "Paratroopers") company's officers were the biggest opposition against the creation of Unit 101. The reason for this was simply that they didn't want another competitor for retaliation missions. Before the formation of Unit 101 only they undertook these missions. One of the unit's tactical commanders was Meir Har-Zion, who was later awarded the rank of an officer solely for his conduct in battle. The tactics of Unit 101 was politically very effective and soon the fighters simply could not keep up with the attrition.
This meant that the attacks on Israel decreased and the political objective of Unit 101 was accomplished. The creation of Unit 101 was a major landmark in the Israeli Special Forces history. Beside the Sayeret Matkal, they are considered to be the unit with the most influence on the Israeli infantry oriented units including both special and conventional units.
Recruitment.
Members of the unit were recruited mostly from agricultural Kibbutzs and Moshavs, with the view that those who were raised as farmers on the land had the spirit to defend it.
Operations.
Palestinian refugee camp.
According to Yoav Gelber, after one month of training a patrol of Unit 101 infiltrated into the Gaza Strip as an exercise. Some sources estimate that a result of the infiltration was 20 killed Arabs. Unit 101 suffered two wounded soldiers. The raid was heavily condemned by foreign observers, who called it "an appalling case of deliberate mass murder", and was publicly criticized in the Israeli cabinet by at least one minister.
Qibya massacre.
Two months later, in October, the unit was involved in the raid into the village of Qibya in the northern West Bank, then a part of Jordan. During this operation that inflicted heavy damage on the Arab Legion forces in Qibya 42 villagers were killed, and 15 wounded. According to United Nations observers, bullet-riddled bodies near the doorways and multiple bullet hits on the doors of the demolished houses indicated that the inhabitants may have remained inside until their homes were blown up over them.
The international outcry caused by the operation required a formal reply by Israel. The Israelis denied responsibility, making diplomats and other officials believe that Israeli settlers or a local kibbutz had carried out the raid on their own initiative. Uri Avnery, founder and editor of the magazine Haolam Hazeh, claims he had both hands broken when he was ambushed for criticizing the massacre at Qibya in his newspaper.
 The new recruits began a harsh regimen of day and night training, their orientation and navigation exercises often taking them across the border; encounters with enemy patrols or village watchmen were regarded as the best preparation for the missions that lay ahead. Some commanders, such as Baum and Sharon, deliberately sought firefights. Unit 101 recruits went on long marches and did callisthenics, judo, and weapons and sabotage training, at their base camp at Sataf, an abandoned Arab village just west of Jerusalem.
 — Israeli historian Benny Morris describes Unit 101.
Disbandment.
After realizing the huge success of Unit 101, the Chief of Staff, General Moshe Dayan decided that the experience gained by it must be shared with all IDF infantry units starting with the Paratroopers Battalion 890. This was done by merging the two together under the command of Ariel Sharon who was then promoted to the rank of Lt. Colonel. After the merger and the addition of a NACHAL MUTZNACH battalion, the combined outfit turned into a brigade size unit, named Brigade 202. Sharon became the commander of the merged brigade which was now composed of two battalions — 890 and 88 and a few months later joined by reserve battalion 771 which included ex-101 members together with reserve paratroopers and NACHAL paratroopers.
The merge with T'zanhanim company was actually ironic since their officers were originally the biggest opposition against the creation of Unit 101. The reason for this was simply that they didn't want another competitor for retaliation missions. Before the formation of Unit 101 only they undertook these missions.
Operating within the brigade, they carried out a large-scale attack on the Egyptian army positions in the Gaza strip during February 1955. Sharon personally led the raid, codenamed Operation Black Arrow. It resulted in 42 Egyptian soldiers killed and 36 wounded, versus 8 Israeli dead. The newly formed brigade did most of the Israeli special forces operations during the remainder of the 1950s.
Egyptian shock over the magnitude of their losses is often cited as one of the catalysts for the Soviet-Egyptian arms deal that opened the Middle East to the Soviet Union. Up to 20 such attacks were carried out between 1955 and 1956, culminating in the Qalqilya Police raid of October 1956. This particular raid targeted a position of the Jordanian Arab Legion in one of the old British police forts, during which 18 Israeli soldiers and up to a hundred Legionnaires were killed.
During the end of the 1950s the IDF realized that they were lacking a small SF unit, since the T'zanhanim company had turned into an infantry brigade. That is the main reason why Avraham Arnan formed the Sayeret Matkal in 1958. In various ways the Sayeret Matkal combined the operational experience gathered by Unit 101 and utilised the structure of the British Special Air Service. After losing their special forces title, the T'zanhanim company formed its own SF unit— the Sayeret T'zanhanim in October 1958.

</doc>
<doc id="51270" url="http://en.wikipedia.org/wiki?curid=51270" title="Prince-bishop">
Prince-bishop

A prince-bishop is a bishop who is the civil governor of some secular principality. Thus the principality ruled politically by a prince-bishop could wholly or largely overlap with his diocesan jurisdiction, since some parts of his diocese, even the city of his residence, could be exempt from his civil rule, obtaining the status of free imperial city. If the episcopal see is an archbishopric, the correct term is prince-archbishop; the equivalent in the regular (monastic) clergy is prince-abbot.
In the West, with the decline of imperial power from the 4th century onwards in the face of the barbarian invasions, sometimes Christian bishops of cities took the place of the Roman commander, made secular decisions for the city and led their own troops when necessary. Later relations between a prince-bishop and the burghers were invariably not cordial. As cities demanded charters from emperors, kings, or their prince-bishops and declared themselves independent of the secular territorial magnates, friction intensified between burghers and bishops.
In the Byzantine Empire, the still autocratic Emperors passed general legal measures assigning all bishops certain rights and duties in the secular administration of their dioceses, but that was part of a caesaropapist development putting the Eastern Church in the service of the Empire, with its Ecumenical Patriarch almost reduced to the Emperor's minister of religious affairs. The Russian Empire went even further, abolishing its own patriarchy and placing the church under direct control of the secular government.
Holy Roman Empire.
Bishops had been involved in the government of the Frankish realm and subsequent Carolingian Empire frequently as the clerical member of a duo of envoys styled Missus "dominicus", but that was an individual mandate, not attached to the see. Prince-bishoprics were most common in the feudally fragmented Holy Roman Empire, where many were formally awarded the rank of an Imperial Prince "Reichsfürst", granting them the immediate power over a certain territory and a representation in the Imperial Diet ("Reichstag").
The stem duchies of the German kingdom inside the Empire had strong and powerful dukes (originally, war-rulers), always more looking out for their duchy's "national interest" than for the Empire's. In turn the first Ottonian (Saxon) king Henry the Fowler and more so his son, Emperor Otto I, intended to weaken their power by granting loyal bishops Imperial lands and vest them with "regalia" privileges. Unlike dukes they could not pass hereditary titles and lands to any descendants. Instead the Emperors reserved the implementation of the bishops of their proprietary church for themselves, defying the fact that according to canon law they were part of the transnational Catholic Church. This met with increasing opposition by the Popes, culminating in the fierce Investiture Controversy of 1076. Nevertheless the Emperors continued to grant major territories to the most important (arch)bishops. The immediate territory attached to the episcopal see then became a prince-diocese or bishopric ("Fürstbistum"). The German term "Hochstift" was often used to denote the form of secular authority held by bishops ruling a prince-bishopric with "Erzstift" being used for prince-archbishoprics.
Emperor Charles IV by the Golden Bull of 1356 confirmed the privileged status of the Prince-Archbishoprics of Mainz, Cologne and Trier as members of the electoral college. At the eve of the Protestant Reformation, the Imperial states comprised 53 ecclesiastical principalities. They were finally secularized in the 1803 German Mediatization upon the territorial losses to France in the Treaty of Lunéville, except for the Mainz prince-archbishop and German archchancellor Karl Theodor Anton Maria von Dalberg, who continued to rule as Prince of Aschaffenburg and Regensburg. With the dissolution of the Holy Roman Empire in 1806, the title became finally defunct. However in some countries outside of French control, such as in the Austrian Empire (Salzburg, Seckau, and Olomouc) and the Kingdom of Prussia (Breslau), the institution nominally continued, and in some cases was revived; a new, titular type arose.
No less than three of the (originally only seven) prince-electors, the highest order of "Reichsfürsten" (comparable in rank with the French pairs), were prince-archbishops, each holding the title of Archchancellor (the only arch-office amongst them) for a part of the Empire; given the higher importance of an electorate, their principalities were known as "Kurfürstentum" ("electoral principality") rather than prince-archbishoprics:
The suffragan-bishoprics of Gurk (established 1070), Chiemsee (1216), Seckau (1218), and Lavant (1225) sometimes used the "Fürstbischof" title, but never held any "reichsfrei" territory. The bishops of Vienna (established 1469) and Wiener Neustadt (1469–1785) didn't control any territory, nor did they claim a princely title.
State of the Teutonic Order.
Upon the incorporation of the Livonian Brothers of the Sword in 1237, the territory of the Order's State largely corresponded with the Diocese of Riga. Bishop Albert of Riga in 1207 had received the lands of Livonia as an Imperial fief from the hands of German king Philip of Swabia, he however had to come to terms with the Brothers of the Sword. At the behest of Pope Innocent III the "Terra Mariana" confederation was established, whereby Albert had to cede large parts of the episcopal territory to the Livonian Order. Albert proceeded tactically in the conflict between the Papacy and Emperor Frederick II: in 1225 he reached the acknowledgement of his status as a Prince-Bishop of the Empire, though the Roman Curia insisted on the fact that the Christianized Baltic territories were solely under the suzerainty of the Holy See. By the 1234 Bull of Rieti, Pope Gregory IX stated that all lands acquired by the Teutonic Knights were no subject of any conveyancing by the Emperor.
Within this larger conflict, the continued dualism of the autonomous Riga prince-bishop and the Teutonic Knights led to a lengthy friction. Around 1245 the Papal legate William of Modena reached a compromise: though incorporated into the Order's State, the archdiocese and its suffragan bishoprics were acknowledged with their autonomous ecclesiastical territories by the Teutonic Knights. The bishops pursued the conferment of the princely title by the Holy Roman Emperor to stress their sovereignty. In the original Prussian lands of the Teutonic Order, Willam of Modena established the suffragan bishoprics of Culm, Pomesania, Samland and Warmia. From the late 13th century onwards, the appointed Warmia bishops were no longer members of the Teutonic Knights, a special status confirmed by the bestowal of the princely title by Emperor Charles IV in 1356.
Elsewhere.
In Montenegro.
The bishops of Cetinje, Montenegro, who took the place of the earlier secular (Grand) Voivodes in 1516 had a unique position of Slavonic, Orthodox prince-bishops of Montenegro under Ottoman suzerainty. They actually became the secularized, hereditary princes and ultimately Kings of Montenegro in 1852, as reflected in their styles:
In England.
The Bishops of Durham were also territorial prince-bishops, with the extraordinary secular rank of Earl palatine, for it was their duty not only to be head of the large diocese, but also to help protect the Kingdom against the Scottish threat from the north. The title survived the union of England and Scotland into the Kingdom of Great Britain in 1707 until 1836.
In France.
Apart from territories formerly within the Holy Roman Empire, no French diocese had a principality of political significance linked to its see.
However, a number of French bishops did hold a noble title, with a tiny territory usually about their seat; it was often a princely title, especially Count. Indeed, six of the original Pairies (the royal vassals awarded with the highest precedence at Court) were episcopal: the Archbishop of Reims and five other bishops (suffragans to Reims, except the Bishop of Langres); the three highest ones held a ducal title and the others a comital title.
They were later joined by the Archbishop of Paris, with a ducal title, but with precedence over the others. See also Peerage of France.
In Portugal.
From 1472 to 1967, the bishop of Coimbra held the comital title of Count of Arganil, being thus called "bishop-count" ("Bispo-Conde", in Portuguese). The comital title is still held "de jure", but since Portugal is a republic and nobility privileges are abolished, its use declined during the 20th century.
Beyond Catholic feudalism.
While one might expect that the Protestant Schism, Counter-Reformation and more modern regimes than the traditional feudal principality would have eradicated the prince-bishopric, they didn't quite.
Even when the true prince-(arch)bishoprics disappeared from the map of Europe as it was redrawn by Napoleon I Bonaparte (who caused the end of the Holy Roman Empire) and the Congress of Vienna after his defeat, the title found a new, "titular" use.
In the Habsburg dynasty's "new" empire, the Danubian Double Monarchy Austria-Hungary, reduced to the parts south of Prussia's (German) sphere of dominance that would become the (largely Protestant) German Empire, actual territorial power was no longer held by the bishops, but the status of "Fürst(erz)bischof" was maintained, and could be given a similar political role in the more modern, almost standardized Cisleithanian provincial level, the "Kronland" 'crown land', as ex officio members of its Landtag, the representative and legislative assembly, often with "Virilstimme", while other bishops could collectively be represented as a 'prelates bench' (an elected "Kurie").
The Emperors of Austria now bestowed the title upon bishops even "without" any feudal principality, but as a princely style and rank (as had been usual for centuries with secular noble titles of peerage ranks) awarded to episcopal sees, carrying the privilege of a seat in the estates, e.g., for the bishop of Laibach (as a consolation prize for the see's loss of metropolitan rank to Görz), the archbishop of Vienna (probably due to Vienna's rank as Imperial residence) and for the archbishop of Esztergom (here reflecting his longstanding rôle as the first magnate of Hungary).
Special cases.
The ultimate prince-bishop is the Bishop of Rome, i.e. the Pope, universal head (Supreme Pontiff) of the Roman Catholic Church. His claims to territorial power were bolstered by the forged early-medieval document "Donation of Constantine", and the authentic "Donation of Pepin", establishing the Patrimonium Petri which was further extended as the powerful Papal States. Pope Pius IX was the last of the true, sovereign prince-bishops, divested of territorial powers when the Papacy was forced to surrender the rule of Rome in 1870 to the united Kingdom of Italy, which was supported by liberal-nationalists. The pope however re-gained sovereign power over Vatican City in 1929 after successful negotiations with the Italian government under Benito Mussolini, leading to the Lateran Treaties.
The Spanish Catalonian Bishop of Urgell, who no longer has any secular rights in Spain, remains one of two co-princes of Andorra, along with the French head of state (currently its President).

</doc>
<doc id="51271" url="http://en.wikipedia.org/wiki?curid=51271" title="Louis XVI of France">
Louis XVI of France

Louis XVI (23 August 1754 – 21 January 1793), also known as Louis Capet, was King of France from 1774 until his deposition in 1792, although his formal title after 1791 was King of the French. He was executed on 21,January 1793 His father, Louis, Dauphin of France, was the son and heir apparent of Louis XV of France. As a result of the Dauphin's death in 1765, Louis succeeded his grandfather in 1774.
The first part of Louis' reign was marked by attempts to reform France in accordance with Enlightenment ideals. These included efforts to abolish serfdom, remove the "taille", and increase tolerance toward non-Catholics. The French nobility reacted to the proposed reforms with hostility, and successfully opposed their implementation; increased discontent among the common people ensued. From 1776 Louis XVI actively supported the North American colonists, who were seeking their independence from Great Britain, which was realized in the 1783 Treaty of Paris.
The ensuing debt and financial crisis contributed to the unpopularity of the "Ancien Régime" which culminated at the Estates-General of 1789. Discontent among the members of France's middle and lower classes resulted in strengthened opposition to the French aristocracy and to the absolute monarchy, of which Louis and his wife, queen Marie Antoinette, were viewed as representatives. In 1789, the storming of the Bastille during riots in Paris marked the beginning of the French Revolution.
Louis's indecisiveness and conservatism led some elements of the people of France to view him as a symbol of the perceived tyranny of the Ancien Régime, and his popularity deteriorated progressively. His disastrous flight to Varennes in June 1791, four months before the constitutional monarchy was declared, seemed to justify the rumors that the king tied his hopes of political salvation to the prospects of foreign invasion. The credibility of the king was deeply undermined and the abolition of the monarchy and the establishment of a republic became an ever increasing possibility.
In a context of civil and international war, Louis XVI was suspended and arrested at the time of the insurrection of 10 August 1792 one month before the constitutional monarchy was abolished and the First French Republic proclaimed on 21 September 1792. He was tried by the National Convention (self-instituted as a tribunal for the occasion), found guilty of high treason, and executed by guillotine on 21 January 1793 as a desacralized French citizen known as "Citizen Louis Capet", a nickname in reference to Hugh Capet, the founder of the Capetian dynasty – which the revolutionaries interpreted as Louis' family name. Louis XVI is the only King of France ever to be executed, and his death brought an end to more than a thousand years of continuous French monarchy.
Childhood.
"Louis-Auguste de France", who was given the title Duc de Berry at birth, was born in the Palace of Versailles. Out of seven children, he was the second son of Louis, the "Dauphin" of France, and thus the grandson of Louis XV of France and of his consort, Maria Leszczyńska. His mother was Marie-Josèphe of Saxony, the daughter of Frederick Augustus II of Saxony, Prince-Elector of Saxony and King of Poland.
Louis-Auguste had a difficult childhood because his parents neglected him in favour of his, said to be, bright and handsome older brother, Louis, duc de Bourgogne, who died at the age of nine in 1761. A strong and healthy boy, but very shy, Louis-Auguste excelled in his studies and had a strong taste for Latin, history, geography, and astronomy, and became fluent in Italian and English. He enjoyed physical activities such as hunting with his grandfather, and rough-playing with his younger brothers, Louis-Stanislas, comte de Provence, and Charles-Philippe, comte d'Artois. From an early age, Louis-Auguste had been encouraged in another of his hobbies: locksmithing, which was seen as a 'useful' pursuit for a child.
Upon the death of his father, who died of tuberculosis on 20 December 1765, the eleven-year-old Louis-Auguste became the new "Dauphin". His mother never recovered from the loss of her husband, and died on 13 March 1767, also from tuberculosis. The strict and conservative education he received from the Duc de La Vauguyon, "gouverneur des Enfants de France" (governor of the Children of France), from 1760 until his marriage in 1770, did not prepare him for the throne that he was to inherit in 1774 after the death of his grandfather, Louis XV. Throughout his education, Louis-Auguste received a mixture of studies particular to religion, morality, and humanities. His instructors may have also had a good hand in shaping Louis-Auguste into the indecisive king that he became. Abbé Berthier, his instructor, taught him that timidity was a value in strong monarchs, and Abbé Soldini, his confessor, instructed him not to let people read his mind.
Family life.
On 16 May 1770, at the age of fifteen, Louis-Auguste married the fourteen-year-old Habsburg Archduchess Maria Antonia (better known by the French form of her name, "Marie Antoinette"), his second cousin once removed and the youngest daughter of the Holy Roman Emperor Francis I and his wife, the formidable Empress Maria Theresa.
This marriage was met with some hostility by the French public. France's alliance with Austria had pulled the country into the disastrous Seven Years' War, in which it was defeated by the British, both in Europe and in North America. By the time that Louis-Auguste and Marie-Antoinette were married, the French people generally regarded the Austrian alliance with dislike, and Marie-Antoinette was seen as an unwelcome foreigner. For the young couple, the marriage was initially amiable but distant. Louis-Auguste's shyness and, among other factors, the young age and inexperience of the newlyweds, who were near total strangers to each other - having met only two days prior to their wedding -, meant that the 15-year old bridegroom failed to consummate the union with his 14-year old bride. His fear of being manipulated by her for Imperial purposes caused him to behave coldly towards her in public. Over time, the couple became closer, though while their marriage was reportedly consummated in July 1773, it was not in fact really so until 1777.
Nevertheless, the royal couple failed to produce any children for several years after their wedding, placing a strain upon their marriage, whilst the situation was worsened by the publication of obscene pamphlets ("libelles") which mocked the infertility of the pair. One questioned, "Can the King do it? Can't the King do it?"
The reasons behind the couple's initial failure to have children were debated at that time, and they have continued to be so since. One suggestion is that Louis-Auguste suffered from a physiological dysfunction, most often thought to be phimosis, a suggestion first made in late 1772 by the royal doctors. Historians adhering to this view suggest that he was circumcised (a common treatment for phimosis) to relieve the condition seven years after their marriage. Louis's doctors were not in favour of the surgery – the operation was delicate and traumatic, and capable of doing "as much harm as good" to an adult male. The argument for phimosis and a resulting operation is mostly seen to originate from Stefan Zweig, who is now known to have given undue prominence to evidence suggesting that Louis had phimosis, and to have suppressed other evidence that contradicted that interpretation. Zweig, a novelist not an historian, was influenced by the theories of his close friend Sigmund Freud, and argued that Antoinette's notorious frivolity and spendthrift ways resulted from her sexual frustration in the first seven years of her marriage
Most modern historians agree that Louis had no surgery – for instance, as late as 1777, the Prussian envoy, Baron Goltz, reported that the King of France had definitely declined the operation. The fact was that Louis was frequently declared to be perfectly fit for sexual intercourse, confirmed by Joseph II, and during the time he was purported to have had the operation, he went out hunting almost every day, according to his journal. This would not have been possible if he had undergone a circumcision; at the very least, he would have been unable to ride to the hunt for a few weeks thereafter. The couple's consummation problems are now attributed to other factors. Antonia Fraser's biography of the queen discusses Joseph II's letter on the matter to one of his brothers after he visited Versailles in 1777. In the letter, Joseph describes in astonishingly frank detail Louis' inadequate performance in the marriage bed and Antoinette's disinterest in conjugal activity. Joseph described the couple as "complete fumblers" but with his advice, Louis began to apply himself more effectively to his duties as a husband, and sometime in the third week of August 1777 Marie Antoinette finally became pregnant.
In spite of all their earlier difficulty, the royal couple became the parents of four children. Marie Antoinette's lady-in-waiting, Mme Campan, notes a miscarriage the queen suffered after the birth of her first child, an incident dated to July 1779 by a letter to the queen from the empress. Mme Campan states that Louis spent an entire morning consoling his wife at her bedside, and swore to secrecy everyone who knew of the incident. Marie Antoinette suffered a second miscarriage at the beginning of November 1783. The four live-born children were:
Absolute monarch of France, 1774–1789.
When Louis XVI succeeded to the throne in 1774, he was 19 years old. He had an enormous responsibility, as the government was deeply in debt, and resentment to 'despotic' monarchy was on the rise. Louis also felt woefully unqualified for the job.
As king, Louis focused primarily on religious uniformity and foreign policy. While none doubted Louis's intellectual ability to rule France, it was quite clear that, although raised as the "Dauphin" since 1765, he lacked firmness and decisiveness. His desire to be loved by his people is evident in the prefaces of many of his edicts that would often explain the nature and good intention of his actions as benefiting the people. He aimed to earn the love of his people by reinstating the "parlements". When questioned about his decision, he said: "It may be considered politically unwise, but it seems to me to be the general wish and I want to be loved." In spite of his indecisiveness, Louis XVI was determined to be a good king, stating that he "must always consult public opinion; it is never wrong." He therefore appointed an experienced advisor, Jean-Frédéric Phélypeaux, comte de Maurepas who, until his death in 1781, would take charge of many important ministerial functions.
 Among the major events of Louis XVI's reign was his signing of the Edict of Versailles, also known as the Edict of Tolerance, on 7 November 1787, which was registered in the parlement on 29 January 1788. This edict effectively nullified the Edict of Fontainebleau that had been law for 102 years. It granted non-Catholics – Calvinists Huguenots, Lutherans, as well as Jews – civil and legal status in France, and gave them the right to openly practice their faiths. The Edict of Versailles did not legally proclaim freedom of religion in France – this took two more years, with the Declaration of the Rights of Man and Citizen of 1789 – however, it was an important step in eliminating religious tensions and it officially ended religious persecution within his realm.
Radical financial reforms by Turgot and Malesherbes angered the nobles and were blocked by the "parlements" who insisted that the King did not have the legal right to levy new taxes. So, in 1776, Turgot was dismissed and Malesherbes resigned, to be replaced by Jacques Necker. Necker supported the American Revolution, and he carried out a policy of taking out large international loans instead of raising taxes. He attempted to gain public favor in 1781 when he had published the first ever statement of the French Crown's expenses and accounts, the "Compte rendu au roi." This allowed the people of France to view the king's accounts in modest surplus. When this policy failed miserably, Louis dismissed him, and then replaced him in 1783 with Charles Alexandre de Calonne, who increased public spending to "buy" the country's way out of debt. Again this failed, so Louis convoked the Assembly of Notables in 1787 to discuss a revolutionary new fiscal reform proposed by Calonne. When the nobles were informed of the extent of the debt, they were shocked into rejecting the plan. This negative turn of events signaled to Louis that he had lost the ability to rule as an absolute monarch, and he fell into depression.
As power drifted from him, there were increasingly loud calls for him to convoke the Estates-General, which had not met since 1614, at the beginning of the reign of Louis XIII. As a last-ditch attempt to get new monetary reforms approved, Louis XVI convoked the Estates-General on 8 August 1788, setting the date of their opening at 1 May 1789. With the convocation of the Estates-General, as in many other instances during his reign, Louis placed his reputation and public image in the hands of those who were perhaps not as sensitive to the desires of the French public as he was. Because it had been so long since the Estates-General had been convened, there was some debate as to which procedures should be followed. Ultimately, the "parlement de Paris" agreed that "all traditional observances should be carefully maintained to avoid the impression that the Estates-General could make things up as it went along." Under this decision, the King agreed to retain many of the divisionary customs which had been the norm in 1614, but which were intolerable to a Third Estate buoyed by the recent proclamations of equality. For example, the First and Second Estates proceeded into the assembly wearing their finest garments, while the Third Estate was required to wear plain, oppressively somber black, an act of alienation that Louis would likely have not condoned. He seemed to regard the deputies of the Estates-General with at least respect: in a wave of self-important patriotism, members of the Estates refused to remove their hats in the King's presence, so Louis removed his to them.
This convocation was one of the events that transformed the general economic and political "malaise" of the country into the French Revolution. In June 1789, the Third Estate unilaterally declared itself the National Assembly. Louis's attempts to control it resulted in the Tennis Court Oath ("serment du jeu de paume"), on 20 June, the declaration of the National Constituent Assembly on 9 July, and eventually led to the storming of the Bastille on 14 July, which started the French Revolution.(Louis' "diary" entry for 14 July, the single word "rien (nothing)" has been used to show how out of touch with reality he was, but the document was more of a hunting log than a personal journal. When he did not go hunting, he wrote "rien". He did not mean nothing important had happened that day). Within three short months, the majority of the king's executive authority had been transferred to the elected representatives of the people's nation.
Foreign policy.
French involvement in the Seven Years' War had left Louis XVI a disastrous inheritance. Britain's victories had seen them capture most of France's colonial territories. While some were returned to France at the 1763 Treaty of Paris a vast swathe of North America was ceded to the British.
This had led to a strategy amongst the French leadership of seeking to rebuild the French military in order to fight a war of revenge against Britain, in which it was hoped the lost colonies could be recovered. France still maintained a strong influence in the West Indies, and in India maintained five trading posts, leaving opportunities for disputes and power-play with Great Britain.
Concerning the American Revolution.
In the spring of 1776, Vergennes, the Foreign Secretary, saw an opportunity to humiliate France's long-standing enemy, Great Britain, as well as recover territory lost during the Seven Years' War, by supporting the American Revolution. Louis XVI was convinced by Pierre Beaumarchais to secretly send supplies, ammunition and guns from 1776, sign a formal Treaty of Alliance in early 1778, and go to war with Britain. In deciding in favor of war despite France's large financial problems, the King was materially influenced by alarmist reports after the Battle of Saratoga which suggested that Britain was preparing to make huge concessions to the colonies and then, allied with them, strike at French and Spanish possessions in the West Indies. Spain and the Netherlands soon joined the French in an anti-British coalition. After 1778, Britain switched its focus to the West Indies, as defending the sugar islands was considered more important than trying to recover the thirteen colonies. France and Spain planned to invade the British Isles with the Armada of 1779, but the operation never went ahead.
France's initial military assistance to the American rebels was a disappointment with defeats at Rhode Island and Savannah. In 1780, France sent Rochambeau and de Grasse to help the Americans, along with large land and naval forces. The French expeditionary force arrived in North America in July 1780. The appearance of French fleets in the Caribbean was followed by the capture of a number of the sugar islands, including Tobago and Grenada. In October 1781, the French naval blockade was instrumental in forcing a British army under Lord Cornwallis to surrender at the Siege of Yorktown. When news of this reached London, the government of Lord North fell in March 1782 and Great Britain immediately sued for peace terms; however, France delayed the end of the war until September 1783 in the hope of overrunning more British colonies in India and the West Indies.
Great Britain recognised the independence of the thirteen colonies as the United States of America, and the French war ministry rebuilt the army. However, the British defeated the main French fleet in 1782 and successfully defended Jamaica and Gibraltar. France gained little from the 1783 Treaty of Paris that ended the war, except the colonies of Tobago and Senegal. Louis XVI was wholly disappointed in his aims of recovering Canada, India and other islands in the West Indies from Britain, as they were too well defended and the Royal Navy made any invasion attempt impossible. The war cost 1,066 million livres, financed by new loans at high interest (with no new taxes). Necker concealed the crisis from the public by explaining only that ordinary revenues exceeded ordinary expenses, and not mentioning the loans. After he was forced from office in 1781, new taxes were levied.
Concerning India.
Louis XVI hoped to use the American Revolutionary War as an opportunity to expel the British from India. In 1782, he sealed an alliance with the Peshwa Madhu Rao Narayan. As a consequence, Bussy moved his troops to the Isle de France (now Mauritius) and later contributed to the French effort in India in 1783. Suffren became the ally of Hyder Ali in the Second Anglo-Mysore War against British rule in India, in 1782–1783, fighting the British fleet along the coasts of India and Ceylon.
Concerning Vietnam and Indo-China.
France also intervened in Cochinchina following Mgr Pigneau de Béhaine's intervention to obtain military aid. A France-Cochinchina alliance was signed through the Treaty of Versailles of 1787, between Louis XVI and Prince Nguyễn Ánh. As the French regime was under considerable strain, France was unable to follow through with the application of the Treaty, but Mgr Pigneau de Béhaine persisted in his efforts and with the support of French individuals and traders mounted a force of French soldiers and officers that would contribute to the modernization of the armies of Nguyễn Ánh, contributing to his victory and his reconquest of all of Vietnam by 1802.
Concerning world exploration.
Louis XVI also encouraged major voyages of exploration. In 1785, he appointed La Pérouse to lead a sailing expedition around the world.
Revolutionary constitutional reign, 1789–1792.
On 5 October 1789, an angry mob of Parisian working women was incited by revolutionaries and marched on the Palace of Versailles, where the royal family lived. At dawn, they infiltrated the palace and attempted to kill the queen, who was associated with a frivolous lifestyle that symbolized much that was despised about the "Ancien Régime". After the situation had been defused by La Fayette, who was leading the "Garde nationale", the king and his family were brought by the crowd to the Tuileries Palace in Paris. The reasoning behind this forced departure from Versailles was the opinion the king would be more accountable to the people if he lived among them in Paris.
The revolution's principles of popular sovereignty, though central to democratic principles of later eras, marked a decisive break from the absolute monarchical principle that was at the heart of traditional French government. As a result, the revolution was opposed by many of the rural people of France and by practically all the governments of France's neighbors. As the revolution became more radical and the masses became more uncontrollable, several leading figures in the initial formation of the revolution began to doubt its benefits. Some, like Honoré Mirabeau, secretly plotted with the Crown to restore its power in a new constitutional form.
Beginning in 1791, Montmorin, Minister of Foreign Affairs, started to organize covert resistance to the Revolutionary forces. Thus, the funds of the Civil List ("la Liste civile"), voted annually by the National Assembly were partially assigned to secret expenses in order to preserve the monarchy. Arnault Laporte was in charge of the Civil List and he collaborated with both Montmorin and Mirabeau. After the sudden death of Mirabeau, Maximilien Radix de Sainte-Foix, a noted financier, took his place. In effect, he headed a secret council of advisers to the King that tried to preserve the Monarchy; these schemes proved unsuccessful, and were exposed later as the armoire de fer scandal.
Mirabeau's death, and Louis's indecision, fatally weakened negotiations between the Crown and moderate politicians. On one hand, Louis was nowhere near as reactionary as his brothers, the comte de Provence and the comte d'Artois, and he repeatedly sent messages to them requesting a halt to their attempts to launch counter-coups. This was often done through his secretly nominated regent, the Cardinal Loménie de Brienne. On the other hand, Louis was alienated from the new democratic government both by its negative reaction to the traditional role of the monarch and in its treatment of him and his family. He was particularly irked by being kept essentially as a prisoner in the Tuileries, and by the refusal of the new regime to allow him to have confessors and priests of his choice rather than 'constitutional priests' pledged to the state and not the Roman Catholic Church.
Flight to Varennes (1791).
On 21 June 1791, Louis XVI attempted to secretly flee with his family from Paris to the royalist fortress town of Montmédy on the northeastern border of France, where he would join the "émigrés" and be protected by Austria. While the National Assembly worked painstakingly towards a constitution, Louis and Marie-Antoinette were involved in plans of their own. Louis had appointed the baron de Breteuil to act as plenipotentiary, dealing with other foreign heads of state in an attempt to bring about a counter-revolution. Louis himself held reservations against depending on foreign assistance. Like his mother and father, he thought that the Austrians were treacherous and the Prussians were overly ambitious. As tensions in Paris rose and he was pressured to accept measures from the Assembly against his will, Louis XVI and the queen plotted to secretly escape from France. Beyond escape, they hoped to raise an "armed congress" with the help of the "émigrés", as well as assistance from other nations with which they could return and, in essence, recapture France. This degree of planning reveals Louis' political determination; unfortunately, it was for this determined plot that he was eventually convicted of high treason. He left behind a long manifesto explaining his rejection of the constitutional system as illegitimate, which was printed in the newspapers. However, his indecision and misunderstanding of France were responsible for the failure of the escape. The royal family was arrested at Varennes-en-Argonne shortly after Jean-Baptiste Drouet, who recognised the king from his profile on a 50 livres "assignat" (paper money), had given the alert. Louis XVI and his family were taken back to Paris where they arrived on 25 June. Viewed suspiciously as traitors, they were placed under tight house arrest upon their return to the Tuileries.
At the microscopic level, the failure of the escape plans was due to a series of misadventures, delays, misinterpretations, and poor judgments. In a wider perspective, the failure was attributable to the king's indecision—he repeatedly postponed the schedule, allowing for smaller problems to become severe. Furthermore, he totally misunderstood the political situation. He thought only a small number of radicals in Paris were promoting a revolution that the people as a whole rejected. He thought, mistakenly, that he was beloved by the peasants and the common folk. The king's flight in the short term was traumatic for France, inciting a wave of emotions that ranged from anxiety to violence to panic. Everyone realized that war was imminent. The deeper realization that the king had in fact repudiated the revolution, was an even greater shock for people who until then had seen him as a good king who governed as a manifestation of God's will. They felt betrayed. Republicanism now burst out of the coffee houses and became a dominating philosophy of the rapidly radicalized French revolution.
Intervention by foreign powers.
The other monarchies of Europe looked with concern upon the developments in France, and considered whether they should intervene, either in support of Louis or to take advantage of the chaos in France. The key figure was Marie Antoinette's brother, the Holy Roman Emperor Leopold II. Initially, he had looked on the revolution with equanimity. However, he became more and more disturbed as it became more and more radical. Despite this, he still hoped to avoid war.
On 27 August, Leopold and King Frederick William II of Prussia, in consultation with émigrés French nobles, issued the Declaration of Pillnitz, which declared the interest of the monarchs of Europe in the well-being of Louis and his family, and threatened vague but severe consequences if anything should befall them. Although Leopold saw the Pillnitz Declaration as an easy way to appear concerned about the developments in France without committing any soldiers or finances to change them, the revolutionary leaders in Paris viewed it fearfully as a dangerous foreign attempt to undermine France's sovereignty.
In addition to the ideological differences between France and the monarchical powers of Europe, there were continuing disputes over the status of Austrian estates in Alsace, and the concern of members of the National Constituent Assembly about the agitation of "émigrés" nobles abroad, especially in the Austrian Netherlands and the minor states of Germany.
In the end, the Legislative Assembly, supported by Louis, declared war on Austria ("the King of Bohemia and Hungary") first, voting for war on 20 April 1792, after a long list of grievances was presented to it by the foreign minister, Charles François Dumouriez. Dumouriez prepared an immediate invasion of the Austrian Netherlands, where he expected the local population to rise against Austrian rule. However, the revolution had thoroughly disorganised the army, and the forces raised were insufficient for the invasion. The soldiers fled at the first sign of battle and, in one case, on 28 April 1792, murdered their general, Irish-born "comte" Théobald de Dillon, whom they accused of treason.
While the revolutionary government frantically raised fresh troops and reorganised its armies, a Prussian-Austrian army under Charles William Ferdinand, Duke of Brunswick assembled at Coblenz on the Rhine. In July, the invasion commenced, with Brunswick's army easily taking the fortresses of Longwy and Verdun. The duke then issued on 25 July a proclamation called the Brunswick Manifesto, written by Louis's émigré cousin, the Prince de Condé, declaring the intent of the Austrians and Prussians to restore the king to his full powers and to treat any person or town who opposed them as rebels to be condemned to death by martial law.
Contrary to its intended purpose of strengthening the position of the King against the revolutionaries, the Brunswick Manifesto had the opposite effect of greatly undermining Louis XVI's already highly tenuous position in Paris. It was taken by many to be the final proof of a collusion between the king and foreign powers in a conspiracy against his own country. The anger of the populace boiled over on 10 August when an armed mob – with the backing of a new municipal government of Paris that came to be known as the "Insurrectional" Paris Commune – besieged the Tuileries Palace. The royal family took shelter with the Legislative Assembly.
Imprisonment and execution, 1792–1793.
Louis was officially arrested on 13 August 1792, and sent to the Temple, an ancient fortress in Paris that was used as a prison. On 21 September, the National Assembly declared France to be a Republic and abolished the Monarchy. Louis was stripped of all of his titles and honours, and from this date was known as simply "Citoyen Louis Capet."
The Girondins were partial to keeping the deposed king under arrest, both as a hostage and a guarantee for the future. Members of the Commune and the most radical deputies, who would soon form the group known as the Mountain, argued for Louis's immediate execution. The legal background of many of the deputies made it difficult for a great number of them to accept an execution without the due process of law of some sort, and it was voted that the deposed monarch be tried before the National Convention, the organ that housed the representatives of the sovereign people. In many ways, the former king's trial represented the trial of the revolution. The trial was seen as such, with the death of one came the life of the other. Michelet argued that the death of the former king would lead to the acceptance of violence as a tool for happiness. He said, "If we accept the proposition that one person can be sacrificed for the happiness of the many, it will soon be demonstrated that two or three or more could also be sacrificed for the happiness of the many. Little by little, we will find reasons for sacrificing the many for the happiness of the many, and we will think it was a bargain."
In November 1792, the "armoire de fer" (iron chest) incident took place at the Tuileries Palace, when the existence, in the king's bedroom, of the hidden safe containing compromising documents and correspondence, was revealed by François Gamain, the Versailles locksmith who had installed it, went to Paris on 20 November and told Jean-Marie Roland, Girondinist Minister of the Interior. The resulting scandal served to discredit the king.
On 11 December, among crowded and silent streets, the deposed king was brought from the Temple to stand before the Convention and hear his indictment, an accusation of high treason and crimes against the State. On 26 December, his counsel, Raymond de Sèze, delivered Louis' response to the charges, with the assistance of François Tronchet and Malesherbes.
On 15 January 1793, the Convention, composed of 721 deputies, voted on the verdict. Given overwhelming evidence of Louis's collusion with the invaders, the verdict was a foregone conclusion – with 693 deputies voting guilty, none for acquittal, with 23 abstaining. The next day, a roll-call vote was carried out to decide upon the fate of the former king, and the result was uncomfortably close for such a dramatic decision. 288 of the deputies voted against death and for some other alternative, mainly some means of imprisonment or exile. 72 of the deputies voted for the death penalty, but subject to a number of delaying conditions and reservations. 361 of the deputies voted for Louis's immediate death. Philippe Égalité, formerly the duke of Orléans and Louis' own cousin, voted for Louis' execution, a cause of much future bitterness among French monarchists.
The next day, a motion to grant Louis XVI reprieve from the death sentence was voted down: 310 of the deputies requested mercy, but 380 voted for the immediate execution of the death penalty. This decision would be final. On Monday, 21 January 1793, Louis XVI was beheaded by guillotine on the "Place de la Révolution". The executioner, Charles Henri Sanson, testified that the former king had bravely met his fate.
As Louis XVI mounted the scaffold, he appeared dignified and resigned. He delivered a short speech in which he reasserted his innocence ("I pardon those who are the cause of my death... ") He declared himself innocent of the crimes he was accused of, praying that his blood would not fall back on France. Many accounts suggest Louis XVI's desire to say more, but Antoine-Joseph Santerre, a general in the National Guard, halted the speech by ordering a drum roll. The former king was then quickly beheaded. Some accounts of Louis's beheading indicate that the blade did not sever his neck entirely the first time. There are also accounts of a blood-curdling scream issuing from Louis after the blade fell but this is unlikely, since the blade severed Louis's spine. It is agreed that while Louis's blood dripped to the ground many members of the crowd ran forward to dip their handkerchiefs in it. This account was proven true in 2012 after a DNA comparison linked blood thought to be from Louis XVI's beheading to DNA taken from tissue samples originating from what was long thought to be the mummified head of Henry IV of France. The blood sample was taken from a gourd carved to commemorate the heroes of the French Revolution that had, according to legend, been used to house Louis's blood.
Legacy.
The 19th-century historian Jules Michelet attributed the restoration of the French monarchy to the sympathy that had been engendered by the execution of Louis XVI. Michelet's "Histoire de la Révolution Française" and Alphonse de Lamartine's "Histoire des Girondins", in particular, showed the marks of the feelings aroused by the revolution's regicide. The two writers did not share the same sociopolitical vision, but they agreed that, even though the monarchy was rightly ended in 1792, the lives of the royal family should have been spared. Lack of compassion at that moment contributed to a radicalization of revolutionary violence and to greater divisiveness among Frenchmen. For the 20th century novelist Albert Camus the execution signaled the end of the role of God in history, for which he mourned. For the 20th century philosopher Jean-François Lyotard the regicide was the starting point of all French thought, the memory of which acts as a reminder that French modernity began under the sign of a crime.
His daughter, Marie-Thérèse-Charlotte, the future Duchess of Angoulême, survived the French Revolution, and she lobbied in Rome energetically for the canonization of her father as a saint of the Catholic Church. Despite his signing of the "Civil Constitution of the Clergy", Louis had been described as a martyr by Pope Pius VI in 1793. In 1820, however, a memorandum of the Congregation of Rites in Rome, declaring the impossibility of proving that Louis had been executed for religious rather than political reasons, put an end to hopes of canonization.
In film and literature.
King Louis XVI has been portrayed in numerous films. In "Marie Antoinette" (1938), he was played by Robert Morley. Jean-François Balmer portrayed him in the 1989 two-part miniseries "La Révolution française". More recently, he was depicted in the 2006 film "Marie Antoinette" by Jason Schwartzman. In Sacha Guitry's "Si Versailles m'était conté", Louis was portrayed by one of the film's producers, Gilbert Bokanowski, using the alias Gilbert Boka. Several portrayals have upheld the image of a bumbling, almost foolish king, such as that by Jacques Morel in the 1956 French film "Marie-Antoinette reine de France" and that by Terence Budd in the "Lady Oscar" live action film. In "Start the Revolution Without Me", Louis XVI is portrayed by Hugh Griffith as a laughable cuckold. Mel Brooks played a comic version of Louis XVI in "The History of the World Part 1", portraying him as a libertine who has such a distaste for the peasantry he uses them as targets in skeet shooting. In the 1996 film "Ridicule"; Urbain Cancelier plays Louis.
Louis has been the subject of novels as well, including two of the alternate histories anthologized in If It Had Happened Otherwise (1931): "If Drouet's Cart Had Stuck" by Hilaire Belloc and "If Louis XVI Had Had an Atom of Firmness" by André Maurois, which tell very different stories but both imagine Louis surviving and still reigning in the early 19th century. Louis appears in the children's book Ben and Me by Robert Lawson but does not appear in the 1953 animated short film based on the same book.
Titles, styles, honours and arms.
Titles and styles.
Louis's formal style before the revolution was "Louis XVI, par la grâce de Dieu, roi de France et de Navarre", or "Louis XVI, by the Grace of God, King of France and of Navarre".
Bibliography.
Primary sources.
</dl>
</dl>
External links.
‹ The below is being merged. See for the discussion that led to this result. ›

</doc>
<doc id="51272" url="http://en.wikipedia.org/wiki?curid=51272" title="Guillotine">
Guillotine

A guillotine (; ]) is an apparatus designed for carrying out executions by beheading. It consists of a tall, upright frame in which a weighted and angled blade is raised to the top and suspended. The condemned person is secured at the bottom of the frame, with his or her neck held directly below the blade. The blade is then released, to fall swiftly and sever the head from the body. The device is best known for its use in France, in particular during the French Revolution, when it "became a part of popular culture"; and it was celebrated as the people's avenger by supporters of the Revolution and vilified as the pre-eminent symbol of the Reign of Terror by opponents. The guillotine continued to be used long after the Revolution and remained France's standard method of judicial execution until the abolition of capital punishment with the backing of President François Mitterrand in 1981. The last person guillotined in France was Hamida Djandoubi, on 10 September 1977.
Etymology.
For a period of time after its invention, the guillotine was called a louisette. However, it was later named after French Physician and Freemason Dr. Joseph-Ignace Guillotin who proposed on 10 October 1789 the use of a device to carry out death penalties in France, as a less painful method of execution or as Capital Punishment instead of Breaking wheel to Louis XVI of France. While he did not invent the guillotine, and in fact opposed the death penalty, his name became an eponym for it.
Invention.
Antoine Louis, together with German engineer Tobias Schmidt, built a prototype for the guillotine. Schmidt recommended using an angled blade as opposed to a round one.
Precursors.
There were other beheading machines in countries other than France before 1792. A number of countries, especially in Europe, continued to employ this method of execution into modern times.
Although the guillotine was invented in the late 18th century, similar beheading machines have a longer history. The Halifax Gibbet was a wooden structure of two wooden uprights, capped by a horizontal beam, of a total height of 4.5 m. The blade was an axe head weighing 3.5 kg (7.7 lb), attached to the bottom of a massive wooden block that slid up and down in grooves in the uprights. This device was mounted on a large square platform 1.25 m high. It is not known when the Halifax Gibbet was first used; the first recorded execution in Halifax dates from 1280, but that execution may have been by sword, axe, or the gibbet. The machine remained in use until Oliver Cromwell forbade capital punishment for petty theft. It was used for the last time, for the execution of two criminals on a single day, on 30 April 1650.
Another early example is immortalized in the picture 'The execution of Murcod Ballagh near to Merton in Ireland 1307'. As the title states, the victim was named Murcod Ballagh, and he was decapitated by equipment looking remarkably similar to the later French guillotines. Another unrelated picture depicts an execution with elements of both a guillotine-style device and a traditional beheading. While the condemned is lying on a bench, a device holds an axe head in position above the neck. The executioner, who is shown wielding a large hammer, strikes down on the mechanism and drives the blade down. Traditional executions by decapitation via sword or axe were notably gruesome, and the device depicted in the illustration may have been conceived in an attempt to improve the accuracy and effectiveness. No reference to its actual use has been found.
An earlier example is to be found in "High History of the Holy Grail", which is dated to about 1210. Although the device is imaginary, its function is clear. The text says:
Within these three openings are the hallows set for them. And behold what I would do to them if their three heads were therein ... She setteth her hand toward the openings and draweth forth a pin that was fastened into the wall, and a cutting blade of steel droppeth down, of steel sharper than any razor, and closeth up the three openings. "Even thus will I cut off their heads when they shall set them into those three openings thinking to adore the hallows that are beyond." 
French Revolution.
On 10 October 1789, Joseph-Ignace Guillotin, a French physician, stood before the National Assembly and proposed the following six articles in favour of the reformation of capital punishment:
Sensing the growing discontent, Louis XVI banned the use of the breaking wheel. In 1791, as the French Revolution progressed, the National Assembly researched a new method to be used on all condemned people regardless of class. Their concerns contributed to the idea that the purpose of capital punishment was simply to end life rather than to inflict pain.
A committee was formed under Antoine Louis, physician to the King and Secretary to the Academy of Surgery. Guillotin was also on the committee. The group was influenced by the Italian Mannaia (or Mannaja), the Scottish Maiden and the Halifax Gibbet, which was fitted with an axe head weighing 7 pounds 12 ounces (3.5 kg). While these prior instruments usually crushed the neck or used blunt force to take off a head, devices also usually used a crescent blade and a lunette (a hinged two part yoke to immobilize the victim's neck).
Laquiante, an officer of the Strasbourg criminal court, designed a beheading machine and employed Tobias Schmidt, a German engineer and harpsichord maker, to construct a prototype. Antoine Louis is also credited with the design of the prototype. An apocryphal story claims that King Louis XVI (an amateur locksmith) recommended that a triangular blade with a beveled edge be used instead of a crescent blade, but it was Schmidt who suggested placing a straight blade at a 45 degree angle. The first execution by guillotine was performed on highwayman Nicolas Jacques Pelletier on 25 April 1792. He was executed in front of what is now the city hall of Paris (Place de l'hôtel de ville). All citizens deemed guilty of a crime punishable by death were from then on executed there, until the scaffold was moved on 21 August to the Place du Carrousel.
The machine was successful as it was considered a humane form of execution, contrasting with the methods used in pre-revolutionary, "Ancien Régime" France. In France, before the guillotine, members of the nobility were beheaded with a sword or axe, which often took two or more blows to kill the condemned, while commoners were usually hanged, which could take minutes or longer. In the early phase of the French Revolution, the slogan "À la lanterne" (in English: To the Lamp Post!, String Them Up! or Hang Them!) had become a symbol of popular justice in revolutionary France. The revolutionary radicals hanged officials and aristocrats from street lanterns. Other more gruesome methods of executions were also used, such as the wheel or burning at the stake. The condemned or their family would sometimes pay the executioner to ensure that the blade was sharp, to achieve a quick and relatively painless death.
The guillotine was thus perceived to deliver an immediate death without risk of suffocation. Furthermore, having only one method of civil execution was seen as an expression of equality among citizens. The guillotine was then the only civil legal execution method in France until the abolition of the death penalty in 1981, apart from certain crimes against the security of the state, or for the death sentences passed by military courts, which entailed execution by firing squad.
Reign of Terror.
The period from June 1793 to July 1794 in France is known as the Reign of Terror or simply "the Terror". The upheaval following the overthrow of the monarchy, invasion by foreign monarchist powers and the revolt in the Vendée combined to throw the nation into chaos and the government into paranoia. Most of the democratic reforms of the revolution were suspended and the Revolutionary Tribunal sentenced thousands to the guillotine. The first political prisoner to be executed was Collenot d'Angremont of the National Guard, followed soon after by the King's trusted collaborator in his ill-fated attempt to moderate the Revolution, Arnaud de Laporte, both in 1792. Former King Louis XVI and Queen Marie Antoinette were executed in 1793. Maximilien Robespierre became one of the most powerful men in the government, and the figure most associated with the Terror. Nobility and commoners, intellectuals, politicians and prostitutes, all were liable to be executed on little or no grounds; suspicion of "crimes against liberty" was enough to earn one an appointment with "Madame Guillotine" or "The National Razor". Estimates of the death toll range between 16,000 and 40,000.
At this time, Paris executions were carried out in the Place de la Revolution (former Place Louis XV and current Place de la Concorde); the guillotine stood in the corner near the Hôtel Crillon where the statue of Brest can be found today.
For a time, executions by guillotine were a popular entertainment that attracted great crowds of spectators. Vendors sold programs listing the names of those scheduled to die. Many people came day after day and vied for the best locations from which to observe the proceedings; knitting women (tricoteuses) formed a cadre of hardcore regulars, inciting the crowd. Parents often brought their children. By the end of the Terror, the crowds had thinned drastically. Repetition had staled even this most grisly of entertainments, and audiences grew bored.
Eventually, the National Convention had enough of the Terror, partially fearing for their own lives, and turned against Maximilien Robespierre. He was arrested, and on 28 July 1794, was executed in the same fashion as those whom he had condemned. This arguably ended the Terror, as the French expressed their discontent with Robespierre's policy by guillotining him.
Retirement.
After the French Revolution, the executions began again in the city center. On 4 February 1832, the guillotine was moved behind the church of Saint Jacques, just before being moved again, to the Grande Roquette prison, on 29 November 1851.
On 6 August 1909, the guillotine was used on the junction of the Boulevard Arago and the rue de la Santé, behind the prison which bears the latter street's name.
The last public guillotining in France was of Eugen Weidmann, who was convicted of six murders. He was beheaded on 17 June 1939 outside the prison Saint-Pierre, rue Georges Clemenceau 5 at Versailles, which is now the Palais de Justice. A number of problems with that execution (inappropriate behavior by spectators, incorrect assembly of the apparatus, and the fact that it was secretly filmed) caused the French government to order that future executions be conducted in private in the prison courtyard.
The guillotine remained the official method of execution in France until the death penalty was abolished in 1981. The final three guillotinings in France before abolition were those of child-murderers Christian Ranucci on 28 July 1976 in Marseille and Jérôme Carrein on 23 June 1977 in Douai, and torturer-murderer Hamida Djandoubi on 10 September 1977 in Marseille.
In the late 1840s the Tussaud brothers Joseph and Francis, gathering relics for Madame Tussauds wax museum, visited the aged Henry-Clément Sanson, grandson of the executioner Charles Henri Sanson, from whom they obtained parts, the knife and lunette, of one of the original guillotines used during the Age of Terror. The executioner had "pawned his guillotine, and got into woeful trouble for alleged trafficking in municipal property".
Elsewhere.
The Scottish Maiden (supposedly based on the Halifax Gibbet) was introduced to Edinburgh by James Douglas, 4th Earl of Morton in the 16th century and remained in use until 1716. The scaffold itself is now housed in the National Museum of Scotland.
In Antwerp (Belgium), the last person to be beheaded was Francis Kol. Convicted for robbery with murder, he underwent his punishment on 8 May 1856. During the period from 19 March 1798, until 30 March 1856, there were 19 beheadings in Antwerp.
In Germany, where the guillotine is known as the "Fallbeil" ("falling axe"), it was used in various German states from the 17th century onwards, becoming the preferred method of execution in Napoleonic times in many parts of Germany. The guillotine and the firing squad were the legal methods of execution during the German Empire (1871–1918) and the Weimar Republic (1919–1933).
The original German guillotines resembled the French Berger 1872 model, but they eventually evolved into more specialised machines largely built of metal with a much heavier blade enabling shorter uprights to be used. Accompanied by a more efficient blade recovery system and the eventual removal of the tilting board (or bascule) this allowed a quicker turn-around time between executions, the condemned being decapitated either face-up or face-down, depending on how the executioner predicted they would react to the sight of the machine. Those deemed likely to struggle were backed up from behind a curtain to shield their view of the device. As such the blades of some models were covered by a metal screen to hide it from sight.
The guillotine was used by Nazi Germany between 1933 and 1945 to execute 16,500 prisoners, including 10,000 executions between 1944–1945 alone. It was used for the last time in West Germany in 1949 (in the execution of Richard Schuh) and in East Germany in 1966 (in the execution of Horst Fischer). The guillotine continued to be used in East Germany by the Stasi between 1950 and 1990 for secret executions. In Switzerland it was used for the last time by the canton of Obwalden in the execution of murderer Hans Vollenweider in 1940.
In Sweden, where beheading became the mandatory method of execution in 1866, the guillotine replaced manual beheading in 1903 and was used only once, in the execution of murderer Alfred Ander in 1910 at Långholmen Prison, Stockholm. He was also the last person executed in Sweden before capital punishment was abolished in that country in 1921.
In South Vietnam, after the Diệm regime enacted the 10/59 Decree in 1959, mobile special military courts dispatched to the countryside to intimidate the rural peoples used guillotines belonging to the former French colonial power to carry out death sentences on the spot. One such guillotine is still on show at the War Remnants Museum in Ho Chi Minh City.
In 1996 in the US, Georgia State Representative Doug Teper unsuccessfully sponsored a bill to replace the state's electric chair with the guillotine.
Living heads.
From its first use, there has been debate as to whether the guillotine always provided a swift death as Guillotin had hoped. With previous methods of execution intended to be painful, there was little concern about the suffering inflicted. As the guillotine was invented specifically to be humane the issue was seriously considered. The blade cuts quickly enough for there to be relatively little impact on the brain case, and perhaps less likelihood of immediate unconsciousness than with a more violent decapitation, or long-drop hanging.
Audiences to guillotinings told numerous stories of blinking eyelids, speaking, moving eyes, movement of the mouth, even an expression of "unequivocal indignation" on the face of the decapitated Charlotte Corday when her cheek was slapped.
The following report was written by a Dr. Beaurieux, who experimented with the head of a condemned prisoner by the name of Henri Languille, on 28 June 1905:
Here, then, is what I was able to note immediately after the decapitation: the eyelids and lips of the guillotined man worked in irregularly rhythmic contractions for about five or six seconds. This phenomenon has been remarked by all those finding themselves in the same conditions as myself for observing what happens after the severing of the neck ...
I waited for several seconds. The spasmodic movements ceased. [...] It was then that I called in a strong, sharp voice: "Languille!" I saw the eyelids slowly lift up, without any spasmodic contractions – I insist advisedly on this peculiarity – but with an even movement, quite distinct and normal, such as happens in everyday life, with people awakened or torn from their thoughts.
Next Languille's eyes very definitely fixed themselves on mine and the pupils focused themselves. I was not, then, dealing with the sort of vague dull look without any expression, that can be observed any day in dying people to whom one speaks: I was dealing with undeniably living eyes which were looking at me. After several seconds, the eyelids closed again [...].
It was at that point that I called out again and, once more, without any spasm, slowly, the eyelids lifted and undeniably living eyes fixed themselves on mine with perhaps even more penetration than the first time. Then there was a further closing of the eyelids, but now less complete. I attempted the effect of a third call; there was no further movement – and the eyes took on the glazed look which they have in the dead.
Anatomists and other scientists in several countries have tried to perform more definitive experiments on severed human heads as recently as 1956. Inevitably, the evidence is only anecdotal. At the very least, it seems that the massive drop in cerebral blood pressure would cause a victim to lose consciousness in a few seconds.
Names for the guillotine.
During the span of its usage, the French guillotine has gone by many names, some of which include these:

</doc>
<doc id="51273" url="http://en.wikipedia.org/wiki?curid=51273" title="Rabbi">
Rabbi

In Judaism, a rabbi is a teacher of Torah. This title derives from the Hebrew word רַבִּי "rabi" ], meaning "My Master" (irregular plural רבנים "rabanim" ]), which is the way a student would address a master of Torah. The word "master" רב "rav" ] literally means "great one".
The basic form of the rabbi developed in the Pharisaic and Talmudic era, when learned teachers assembled to codify Judaism's written and oral laws. In more recent centuries, the duties of the rabbi became increasingly influenced by the duties of the Protestant Christian minister, hence the title "pulpit rabbis", and in 19th-century Germany and the United States rabbinic activities including sermons, pastoral counseling, and representing the community to the outside, all increased in importance.
Within the various Jewish denominations there are different requirements for rabbinic ordination, and differences in opinion regarding who is to be recognized as a rabbi. All types of Judaism except for Orthodox Judaism and some conservative strains ordain women as rabbis and cantors.
Etymology.
The word "rabbi" derives from the Semitic root R-B-B, in Hebrew script רַב "rav", which in biblical Aramaic means ‘great’ in many senses, including "revered", but appears primarily as a prefix in construct forms. Although the usage "rabbim" "many" (as 1 Kings 18:25, הָרַבִּים) "the majority, the multitude" occurs for the assembly of the community in the Dead Sea scrolls there is no evidence to support an association with the later title "Rabbi."
The root is cognate to Arabic ربّ "rabb", meaning "lord" (generally used when talking about God, but also about temporal lords). As a sign of great respect, some great rabbis are simply called "The Rav".
Rabbi is not an occupation found in the Hebrew Bible, and ancient generations did not employ related titles such as "Rabban", "Ribbi", or "Rab" to describe either the Babylonian sages or the sages in Israel. The titles "Rabban" and "Rabbi" are first mentioned in the Mishnah (c. 200 CE). The term was first used for Rabban Gamaliel the elder, Rabban Simeon his son, and Rabban Johanan ben Zakkai, all of whom were patriarchs or presidents of the Sanhedrin. The title "Rabbi" occurs (in Greek transliteration ῥαββί "rhabbi") in the books of Matthew, Mark, and John in the New Testament, where it is used in reference to "Scribes and Pharisees" as well as to Jesus.
Pronunciation.
Sephardic and Yemenite Jews pronounce this word רִבִּי "ribbī" ; the modern Israeli pronunciation רַבִּי "rabi" is derived from an 18th-century innovation in Ashkenazic prayer books, although this vocalization is also found in some ancient sources. Other variants are "rəvī" and, in Yiddish, "rebbə". The word could be compared to the Syriac word ܪܒܝ "rabi".
In ancient Hebrew, "rabbi" was a proper term of address while speaking to a superior, in the second person, similar to a vocative case. While speaking about a superior, in the third person one could say "ha-rav" ("the Master") or "rabbo" ("his Master"). Later, the term evolved into a formal title for members of the Patriarchate. Thus, the title gained an irregular plural form: רַבָּנִים "rabbanim" ("rabbis"), and not רַבָּי "rabbay" ("my Masters").
Honor.
According to the Talmud, it is a commandment ("mitzvah") to stand up for a Rabbi or Torah scholar, and one should also stand for their spouses and address them with respect. Kohanim are required to honor Rabbis and Torah scholars like everybody else. However, if one is more learned than the Rabbi or the scholar there is no need to stand.
In many places today and throughout history, Rabbis and Torah scholars had and still have the power to place individuals who insulted them in excommunication.
Historical overview.
The governments of the kingdoms of Israel and Judah were based on a system of Jewish kings, prophets, the legal authority of the court of the Sanhedrin and the ritual authority of priesthood. Members of the Sanhedrin had to receive their ordination ("semicha") derived in an uninterrupted line of transmission from Moses, yet rather than being referred to as "rabbis" they were more frequently called judges ("dayanim") akin to the "Shoftim" or "Judges" as in the Book of Judges.
All of the above personalities would have been expected to be steeped in the wisdom of the Torah and the commandments, which would have made them "rabbis" in the modern sense of the word. This is illustrated by a two-thousand-year-old teaching in the Mishnah, "Ethics of the Fathers" ("Pirkei Avot"), which observed about King David,
With the destruction of the two Temples in Jerusalem, the end of the Jewish monarchy, and the decline of the dual institutions of prophets and the priesthood, the focus of scholarly and spiritual leadership within the Jewish people shifted to the sages of the Men of the Great Assembly ("Anshe Knesset HaGedolah"). This assembly was composed of the earliest group of "rabbis" in the more modern sense of the word, in large part because they began the formulation and explication of what became known as Judaism's "Oral Law" ("Torah SheBe'al Peh"). This was eventually encoded and codified within the Mishnah and Talmud and subsequent rabbinical scholarship, leading to what is known as Rabbinic Judaism.
Sages.
The title "Rabbi" was borne by the sages of ancient Israel, who were ordained by the Sanhedrin in accordance with the custom handed down by the elders. They were titled "Ribbi" and received authority to judge penal cases. "Rab" was the title of the Babylonian sages who taught in the Babylonian academies.
After the suppression of the Patriarchate and Sanhedrin by Theodosius II in 425, there was no more formal ordination in the strict sense. A recognised scholar could be called "Rab" or "Hacham", like the Babylonian sages. The transmission of learning from master to disciple remained of tremendous importance, but there was no formal rabbinic qualification as such.
Middle Ages.
Maimonides rules that every congregation is obliged to appoint a preacher and scholar to admonish the community and teach Torah, and the social institution he describes is the germ of the modern congregational rabbinate. In the fifteenth century in Central Europe, the custom grew up of licensing scholars with a diploma entitling them to be called "Mori" (my teacher). At the time this was objected to as "hukkat ha-goy" (imitating the ways of the Gentiles), as it was felt to resemble the conferring of doctorates in Christian universities. However, the system spread, and it is this diploma that is referred to as "semicha" (ordination) at the present day.
18th–19th centuries.
In 19th-century Germany and the United States, the duties of the rabbi became increasingly influenced by the duties of the Protestant Christian minister, hence the title "pulpit rabbis". Sermons, pastoral counseling, representing the community to the outside, all increased in importance. Non-Orthodox rabbis, on a day-to-day business basis, now spend more time on these traditionally non-rabbinic functions than they do teaching, or answering questions on Jewish law and philosophy. Within the Modern Orthodox community, rabbis still mainly deal with teaching and questions of Jewish law, but are increasingly dealing with these same pastoral functions. Orthodox Judaism's National Council of Young Israel and Modern Orthodox Judaism's Rabbinical Council of America have set up supplemental pastoral training programs for their rabbis.
Traditionally, rabbis have never been an intermediary between God and humans. This idea was traditionally considered outside the bounds of Jewish theology. Unlike spiritual leaders in many other faiths, they are not considered to be imbued with special powers or abilities.
Authority.
Acceptance of rabbinic credentials involves both issues of practicality and principle. As a practical matter, communities and individuals typically tend to follow the authority of the rabbi they have chosen as their leader (called by some the "mara d'atra") on issues of Jewish law. They may recognize that other rabbis have the same authority elsewhere, but for decisions and opinions important to them they will work through their own rabbi.
The same pattern is true within broader communities, ranging from Hasidic communities to rabbinical or congregational organizations: there will be a formal or "de facto" structure of rabbinic authority that is responsible for the members of the community.
Ordination.
Traditionally, a person obtains "semicha" ("rabbinic ordination") after the completion of an arduous learning program in the codes of Jewish law and responsa.
The most general form of "semicha" is "Yore yore" ("he shall teach"). Most Rabbis hold this qualification; they are sometimes called a "moreh hora'ah" ("a teacher of rulings"). A more advanced form of "semicha" is "Yadin yadin" ("he shall judge"). This enables the recipient to adjudicate cases of monetary law, amongst other responsibilities. Although the recipient can now be formally addressed as a "dayan" ("judge"), the vast majority retain the title "rabbi." Only a small percentage of rabbis earn this ordination.
Although not strictly necessary, many Orthodox rabbis hold that a "beth din" (court of Jewish law) should be made up of "dayanim".
Orthodox and Haredi Judaism.
Orthodox and Modern-Orthodox.
An Orthodox semicha requires the successful completion of a program encompassing Jewish law and responsa in keeping with longstanding tradition. Orthodox rabbinical students work to gain knowledge in Talmud, Rishonim and Acharonim (early and late medieval commentators) and Jewish law. They study sections of the Shulchan Aruch (codified Jewish law) and its main commentaries that pertain to daily-life questions (such as the laws of keeping kosher, Shabbat, and the laws of family purity). Orthodox rabbis typically study at yeshivas, which are dedicated religious schools. Modern Orthodox rabbinical students, such as those at Yeshiva University, study some elements of modern theology or philosophy, as well as the classical rabbinic works on such subjects.
The entrance requirements for an Orthodox yeshiva include a strong background within Jewish law, liturgy, Talmudic study, and attendant languages (e.g., Hebrew, Aramaic and in some cases Yiddish). Since rabbinical studies typically flow from other yeshiva studies, those who seek a semicha are typically not required to have completed a university education. There are some exceptions to this rule, including Yeshiva University, which requires all rabbinical students to complete an undergraduate degree before entering the program and a Masters or equivalent before ordination. Yeshivat Chovevei Torah Rabbinical School also requires an undergraduate degree before entering the program.
On March 22, 2009, the Hebrew Institute of Riverdale, an Orthodox Synagogue, held a formal ceremony officially giving Ms. Sara Hurwitz the title MaHaRa"T – Manhigah Halakhtit Ruchanit Toranit. However, some Orthodox leaders, such as the Rabbinical Council of America, opposed this move and said it was not in keeping with Orthodoxy; in any case, Hurwitz was not given the title "rabbi."
Haredi Judaism.
While some Haredi (including Hasidic) yeshivas (also known as "Talmudical/Rabbinical schools or academies") do grant official "semicha" ("ordination") to many students wishing to become rabbis, most of the students within the yeshivas engage in learning Torah or Talmud without the goal of becoming rabbis or holding any official positions.
The curriculum for obtaining "semicha" ("ordination") as rabbis for Haredi and Hasidic scholars is the same as described above for all Orthodox students wishing to obtain the official title of "Rabbi" and to be recognized as such.
Within the Hasidic world, the positions of spiritual leadership are dynastically transmitted within established families, usually from fathers to sons, while a small number of students obtain official ordination to become dayanim ("judges") on religious courts, poskim ("decisors" of Jewish law), as well as teachers in the Hasidic schools. The same is true for the non-Hasidic Litvish yeshivas that are controlled by dynastically transmitted rosh yeshivas and the majority of students will not become rabbis, even after many years of post-graduate kollel study.
Some yeshivas, such as (in New York) and (in Baltimore, Maryland), may encourage their students to obtain "semicha" and mostly serve as rabbis who teach in other yeshivas or Hebrew day schools. Other yeshivas, such as Yeshiva Chaim Berlin (Brooklyn, New York) or the Mirrer Yeshiva (in Brooklyn and Jerusalem), do not have an official "semicha/rabbinical program" to train rabbis, but provide semicha on an "as needed" basis if and when one of their senior students is offered a rabbinical position but only with the approval of their "rosh yeshivas".
Consequently, within the world of Haredi Judaism, the English word and title of "Rabbi" for "anyone" is often scorned and derided, because in their view the once-lofty title of "Rabbi" has been debased in modern times. This is one reason that Haredim will often prefer using Hebrew names for rabbinic titles based on older traditions, such as: "Rav" (denoting "[great] rabbi"), "HaRav" ("the [great] rabbi"), "Moreinu HaRav" ("our teacher the [great] rabbi"), "Moreinu" ("our teacher"), "Moreinu VeRabeinu HaRav" ("our teacher and our rabbi/master the [great] rabbi"), "Moreinu VeRabeinu" ("our teacher and our rabbi/master"), "Rosh yeshiva" ("[the] head [of the] yeshiva"), "Rosh HaYeshiva" ("head [of] the yeshiva"), "Mashgiach" (for Mashgiach ruchani) ("spiritual supervsor/guide"), "Mora DeAsra" ("teacher/decisor" [of] the/this place"), "HaGaon" ("the genius"), "Rebbe" ("[our/my] rabbi"), "HaTzadik" ("the righteous/saintly"), "ADMOR" ("Adoneinu Moreinu VeRabeinu") ("our master, our teacher and our rabbi/master") or often just plain "Reb" which is a shortened form of "rebbe" that can be used by, or applied to, any married Jewish male as the situation applies.
Note: A "rebbetzin" (a Yiddish usage common among Ashkenazim) or a "rabbanit" (in Hebrew and used among Sephardim) is the official "title" used for, or by, the wife of any Orthodox, Haredi, or Hasidic rabbi. "Rebbetzin" may also be used as the equivalent of "Reb" and is sometimes abbreviated as such as well.
Reform Movement.
Progressive, Liberal and Reform Judaism.
Progressive, Liberal and Reform Judaism are forms of the Reform movement in Judaism. Its reform rabbinic studies are mandated in pastoral care, the historical development of Judaism, and academic biblical criticism, in addition to the traditional study of rabbinic texts. Rabbinic students also are required to gain practical rabbinic experience by working at a congregation.
All Reform seminaries ordain women and openly lesbian and gay people as rabbis and cantors.
The seminary of Reform Judaism in the United States is Hebrew Union College-Jewish Institute of Religion. It has campuses in Cincinnati, New York City, Los Angeles, and in Jerusalem. In addition to training and ordaining women and openly lesbian and gay people as rabbis and cantors, Hebrew Union College-Jewish Institute of Religion has trained and ordained openly transgender people as rabbis (see Elliot Kukla and Reuben Zellman).
In the United Kingdom the Reform and Liberal movements maintain Leo Baeck College for the training and ordination of rabbis, and in Germany the progressive Abraham Geiger College trains and ordains Europeans for the rabbinate.
Conservative and Reconstructionist Judaism.
Conservative and Reconstructionist Judaism are forms of the Reform movement in Judaism. Conservative Judaism confers Conservative rabbinic ordination after the completion of a program in the codes of Jewish law and responsa in keeping with Jewish tradition. Conservative Judaism has less stringent study requirements for Talmud and responsa compared with Orthodoxy, but adds the following subjects as requirements for rabbinic ordination: pastoral care and psychology, the historical development of Judaism; biblical criticism, Hebrew Bible, Mishna and Talmud, the Midrash literature, Jewish ethics and lore, the codes of Jewish law, the Conservative responsa, and both traditional and modern Jewish works on theology and philosophy.
Entrance requirements to Conservative rabbinical study centers include a background within Jewish law and liturgy, knowledge of Hebrew, familiarity with rabbinic literature, Talmud, etc., ritual observance at the Conservative level, and the completion of an undergraduate university degree. Rabbinical students usually earn a secular degree (e.g., Master of Hebrew Letters) upon graduation. Ordination is granted at the Ziegler School of Rabbinic Studies in Los Angeles, the Rabbinical School of the Jewish Theological Seminary of America in New York, the Schechter Institute of Jewish Studies in Jerusalem, the Budapest University of Jewish Studies and the Seminario Rabinico Latinoamericano in Buenos Aires (Argentina).
Most Conservative seminaries ordain women and openly lesbian and gay people as rabbis and cantors.
Reconstructionist Judaism has the Reconstructionist Rabbinical College, which is located in Pennsylvania and ordains women as well as men (and openly lesbian and gay people) as rabbis and cantors.
Non-orthodox seminaries unaffiliated with main denominations.
There are several possibilities for receiving rabbinic ordination in addition to seminaries maintained by the large Jewish denominations. These include seminaries maintained by smaller denominational movements, and nondenominational (also called "transdenominational" or "postdenominational") Jewish seminaries.
Interdenominational recognition.
Historically and until the present, recognition of a rabbi relates to a community's perception of the rabbi's competence to interpret Jewish law and act as a teacher on central matters within Judaism. More broadly speaking, it is also an issue of being a worthy successor to a sacred legacy.
As a result, there have always been greater or lesser disputes about the legitimacy and authority of rabbis. Historical examples include Samaritans and Karaites.
The divisions between the various religious branches within Judaism may have their most pronounced manifestation on whether rabbis from one movement recognize the legitimacy or the authority of rabbis in another.
As a general rule within Orthodoxy and among some in the Conservative movement, rabbis are reluctant to accept the authority of other rabbis whose Halakhic standards are not as strict as their own. In some cases, this leads to an outright rejection of even the legitimacy of other rabbis; in others, the more lenient rabbi may be recognized as a spiritual leader of a particular community but may not be accepted as a credible authority on Jewish law.
These debates cause great problems for recognition of Jewish marriages, conversions, and other life decisions that are touched by Jewish law. Orthodox rabbis do not recognize conversions by non-Orthodox rabbis. Conservative rabbis recognise all conversions done according to halakha. Finally, the North American Reform and Reconstructionst movemements recognize patrilineality, under certain circumstances, as a valid claim towards Judaism, whereas Conservative and Orthodox maintain the position expressed in the Talmud and Codes that one can be a Jew only through matrilineality (born of a Jewish mother) or through conversion to Judaism.
Women.
With some rare exceptions (see below), women historically have generally not served as rabbis until the 1970s and the influence of second-wave feminism, when the Hebrew Union College-Jewish Institute of Religion first ordained women rabbis. Today, female rabbis are ordained within all branches of Progressive Judaism, while in Orthodox Judaism, women cannot become rabbis.
While there is no prohibition against women learning halakhah that pertains to them, nor is it any more problematic for a woman to rule on such issues than it is for any lay person to do so, the issue lies in the rabbi's position of communal authority. Following the ruling of the Talmud, the decisors of Jewish law held that women were not allowed to serve in positions of authority over a community, such as judges or kings. The position of official rabbi of a community, "mara de'atra" ("master of the place"), has generally been treated in the responsa as such a position. This ruling is still followed in traditional and orthodox circles but has been relaxed in branches like Conservative and Reform Judaism that are less strict in their adherence to traditional Jewish law.
There were some rare cases of women acting as rabbis in earlier centuries, such as the 17th century Asenath Barzani, who acted as a rabbi among Kurdish Jews. Hannah Rachel Verbermacher, also known as the Maiden of Ludmir, was a 19th-century Hasidic rebbe, the only female rebbe in the history of Hasidism.
The first formally ordained female rabbi was Regina Jonas, ordained in Germany in 1935. Since 1972, when Sally Priesand became the first female rabbi in Reform Judaism, Reform Judaism's Hebrew Union College has ordained 552 women rabbis (as of 2008).
Sandy Eisenberg Sasso became the first female rabbi in Reconstructionist Judaism in 1974 (one of 110 by 2006); and Amy Eilberg became the first female rabbi in Conservative Judaism in 1985 (one of 177 by 2006). Lynn Gottlieb became the first female rabbi in Jewish Renewal in 1981, and Tamara Kolton became the very first rabbi (and therefore, since she was female, the first female rabbi) in Humanistic Judaism in 1999. In 2009 Alysa Stanton became the world's first African-American female rabbi.
In Europe, Leo Baeck College had ordained 30 female rabbis by 2006 (out of 158 ordinations in total since 1956), starting with Jackie Tabick in 1975.
The Kohenet Institute, based at the Isabella Freedman Jewish Retreat Center in Connecticut, offers a two-year course of study to women who are then ordained as Jewish priestesses. “Kohenet" is a feminine variation on “kohan," meaning priest. The Kohenet Institute's training involves earth-based spiritual practices that they believe harken back to pre–rabbinic Judaism; a time when, according to Kohenet’s founders, women took on many more (and much more powerful) spiritual leadership roles than are commonly taken by women today. A Jewish priestess may, according to Kohenet, act as a rabbi, but the two roles are not the same.
The consensus of the Orthodox Jewish community has been that women are ineligible to becoming rabbis; the growing calls for Orthodox yeshivas to admit women as rabbinical students have resulted in widespread opposition among the Orthodox rabbinate. Rabbi Norman Lamm, one of the leaders of Modern Orthodoxy and Rosh Yeshiva of Yeshiva University's Rabbi Isaac Elchanan Theological Seminary, opposes giving semicha to women. "It shakes the boundaries of tradition, and I would never allow it." (Helmreich, 1997) Writing in an article in the "Jewish Observer", Moshe Y'chiail Friedman states that Orthodox Judaism prohibits women from being given semicha and serving as rabbis. He holds that the trend towards this goal is driven by sociology, and not halakha ("Jewish law"). In his words, the idea is a "quirky fad." No Orthodox rabbinical association (e.g. Agudath Yisrael, Rabbinical Council of America) has allowed women to be ordained using the term "rabbi".
However, in the last twenty years Orthodox Judaism has begun to develop clergy-like roles for women as halakhic court advisors and congregational advisors. Rabbi Aryeh Strikovski (Machanaim Yeshiva and Pardes Institute) worked in the 1990s with Rabbi Avraham Shapira (then a co-Chief rabbi of Israel) to initiate the program for training Orthodox women as halakhic "Toanot" ("advocates") in rabbinic courts. They have since trained nearly seventy women in Israel. Strikovski states that "The knowledge one requires to become a court advocate is more than a regular ordination, and now to pass certification is much more difficult than to get ordination." In 2012 Ephraim Mirvis appointed Lauren Levin as Britain’s first Orthodox female halakhic adviser, at Finchley Synagogue in London.
Some Orthodox Jewish women now serve in Orthodox Jewish congregations in roles that previously were reserved for males. The grammatically correct Hebrew feminine parallel to the masculine title rabbi is "rabbanit" (רבנית) sometimes used for women in this role. Sara Hurwitz, considered by some the first Orthodox woman rabbi, following correct Hebrew feminized grammar of rav (רב), used the title "rabba" (רבה). Other women in Jewish leadership, like Rachel Kohl Finegold and Lynn Kaye function as de facto assistant rabbis.The newer title of "Maharat" has been used by those who receive this title at Yeshivat Maharat, the first Orthodox seminary for women to confer an equivalent to rabbinic ordination.
In Israel, the Shalom Hartman Institute, founded by Orthodox Rabbi David Hartman, opened a program in 2009 that will grant semicha to women and men of all Jewish denominations, including Orthodox Judaism, although the students are meant to "assume the role of 'rabbi-educators' – not pulpit rabbis- in North American community day schools.
In Israel a growing number of Orthodox women are being trained as "yoatzot halakhah" (halakhic advisers).
In 2013, the first class of female halachic advisers trained to practice in the US graduated; they graduated from the North American branch of Nishmat’s yoetzet halacha program in a ceremony at Congregation Sheartith Israel, Spanish and Portuguese Synagogue in Manhattan.
Rahel Berkovits, an Orthodox Talmud teacher at Jerusalem's Pardes Institute of Jewish Studies, states that as a result of such changes in Haredi and Modern Orthodox Judaism, "Orthodox women found and oversee prayer communities, argue cases in rabbinic courts, advise on halachic issues, and dominate in social work activities that are all very associated with the role a rabbi performs, even though these women do not have the official title of rabbi."
The use of Toanot is not restricted to any one segment of Orthodoxy; In Israel they have worked with Haredi and Modern Orthodox Jews. Orthodox women may study the laws of family purity at the same level of detail that Orthodox males do at Nishmat, the Jerusalem Center for Advanced Jewish Study for Women. The purpose is for them to be able to act as halakhic advisors for other women, a role that traditionally was limited to male rabbis. This course of study is overseen by Rabbi Yaakov Varhaftig.
Modern Orthodox trends.
Furthermore, several efforts are underway within Modern Orthodox communities to include qualified women in activities traditionally limited to rabbis:
References.
General.
</dl>

</doc>
<doc id="51275" url="http://en.wikipedia.org/wiki?curid=51275" title="Kenneth Horne">
Kenneth Horne

Charles Kenneth Horne, generally known as Kenneth Horne, (27 February 1907 – 14 February 1969) was an English comedian and businessman. He is perhaps best remembered for his work on three BBC Radio series: "Much-Binding-in-the-Marsh" (1944–51), "Beyond Our Ken" (1958–64) and "Round the Horne" (1965–68).
The son of a clergyman who was also a politician, Horne had a burgeoning business career with Triplex Safety Glass, which was interrupted by service with the Royal Air Force during the Second World War. While serving in a barrage balloon unit he was asked to broadcast as a quizmaster on the BBC radio show "Ack-Ack, Beer-Beer". The experience brought him into contact with the more established entertainer Richard Murdoch, and the two wrote and starred in the comedy series "Much-Binding-in-the-Marsh". After demobilisation Horne returned to his business career, and kept his broadcasting as a sideline. His career in industry flourished and he later became the chairman and managing director of toy manufacturers Chad Valley.
In 1958 Horne suffered a stroke and gave up his business dealings to focus on his entertainment work. He was the anchor figure in "Beyond Our Ken", which also featured Kenneth Williams, Hugh Paddick, Betty Marsden and Bill Pertwee. When the programme came to an end in 1964, the same cast recorded four series of the comedy "Round the Horne".
Before the planned fifth series of "Round the Horne" began recording, Horne died of a heart attack while hosting the annual Guild of Television Producers' and Directors' Awards; "Round the Horne" could not continue without him and was withdrawn. Since his death the series has been regularly re-broadcast. A 2002 BBC radio survey to find listeners' favourite British comedian placed Horne third, behind Tony Hancock and Spike Milligan.
Biography.
Early life.
Kenneth Horne was born Charles Kenneth Horne on 27 February 1907 at Ampthill Square, London. He was the seventh and youngest child of Silvester Horne and his wife, Katherine Maria "neé" Cozens-Hardy. Katherine's father was Herbert Cozens-Hardy, the Liberal MP for North Norfolk who became the Master of the Rolls in 1907 and Baron Cozens-Hardy on 1 July 1914. Silvester, a powerful orator, was a leading light in the Congregationalist movement, as minister at the Whitefield's Tabernacle, Tottenham Court Road from 1903 and, from 1910, chairman of the Congregational Union of England and Wales. Between 1910–14 he was the Liberal MP for Ipswich.
By 1913 Silvester was suffering from continual poor health. He resigned his position at the tabernacle on medical advice in January 1914, and intended to resign his parliamentary seat. On a speaking tour of the US and Canada he lectured at Yale University, and then took the ferry to Toronto; as it entered the harbour, he collapsed and died, aged 49; Horne was aged seven at the time. From September that year Horne attended St George's School, Harpenden as a boarder—the seventh of the Horne children to attend the school. Although he was not strong academically, he developed into a good sportsman, representing the school in rugby and cricket, and during the summer holidays took part in the Public Schoolboys Lawn Tennis Championship at Queen's Club; in his final appearance in 1925 he was knocked out by the future Wimbledon finalist Bunny Austin.
Horne enrolled at the London School of Economics in October 1925, where his tutors included Hugh Dalton and Stephen Leacock; he was dissatisfied with his time at the university and called Leacock "one of the most boring lecturers I ever came across". During the general strike in 1926 volunteers were asked to enlist at the Organisation for the Maintenance of Supplies to take over the essential services; Horne joined and spent two days driving a London bus before the strike was called off. Through the influence and generosity of an uncle, Austin Pilkington of the Pilkington glassmaking family of St Helens, he was able to enrol at Magdalene College, Cambridge in October 1926. He committed himself to the sporting side of life and represented the college at rugby, and in the relay team alongside the future Olympic gold medallist Lord Burghley. He also played tennis for the university, partnering Bunny Austin. Distracted by his athletic exploits, he neglected his studies and was sent down in December 1927.
Austin Pilkington was aggrieved at Horne's failure to make the most of the opportunity he had provided, and decided against offering the young man a post in the family firm. Despite the disappointment, through his contacts within the industry, he secured for the young Horne an interview with the Triplex Safety Glass Company at King's Norton, a district of Birmingham. Horne's sporting record commended him to the manager of the Triplex factory, and he was taken on as a management trainee on a modest salary. In September 1930, despite his unimpressive finances, he married Lady Mary Pelham-Clinton-Hope, daughter of the 8th Duke of Newcastle. The marriage was happy at first, but had broken down by 1932. Mary applied for an annulment in November 1932; she declared the reason was "the incapacity of the respondent [Charles Kenneth Horne] to consummate the marriage", which was dissolved in 1933, although the two remained on friendly terms thereafter.
When Horne's first marriage was dissolved, he was sought out by a former girlfriend, Joan Burgess, daughter of a neighbour at King's Norton. Unlike his first wife, she had much in common with him, including a liking for squash, tennis, golf and dancing. A month before her 21st birthday they were married, in September 1936. Joan became pregnant soon after the wedding, and in July 1937 a baby boy was delivered; he was stillborn.
Service in the RAF.
In 1938 Horne enlisted in the Royal Air Force Volunteer Reserve on a part-time training scheme. He was commissioned as an acting pilot officer in No. 911 (County of Warwick) Squadron, a barrage balloon unit in Sutton Coldfield, and was called up into the RAF full-time on the outbreak of war. In the initial months of the conflict—the Phoney War—Horne's duties were undemanding, and he formed a concert party from his friends and colleagues. In November 1940 he was promoted to flight lieutenant, and to squadron leader a year later. In early 1942 the BBC producer Bill McLurg asked whether the RAF station at which Horne was based could put on an edition of his programme "Ack-Ack, Beer-Beer". Horne was ordered to put on the show, and he made his broadcasting debut on 16 April 1942, as the compere. Although the standard of the talent on the show was not high, McLurg was impressed with Horne's presentation, especially the way he hosted the programme's quiz; he invited Horne to be the programme's regular quizmaster, a role the latter fulfilled on over fifty "Ack-Ack, Beer-Beer" quizzes over the next two years. In January 1943 he became one of the show's regular comperes and presented the entire show for the first time.
In March 1943 Horne was posted to the Air Ministry in London with the rank of wing commander. Continuing to broadcast on "Ack-Ack, Beer-Beer", he also began to write sketches for the programme, and make broadcasts on other shows, including the Overseas Recorded Broadcasting Service (ORBS), to be transmitted to British forces in the Middle East. His work with ORBS brought him into contact with Flight Lieutenant Richard Murdoch, who he jokingly introduced in one broadcast as "the station commander of Much-Binding-in-the-Marsh"; with a great deal in common in their backgrounds and a similar sense of humour, the pair quickly formed a friendship. Horne informed Murdoch of a squadron leader vacancy in his section at the Ministry, and Murdoch became his colleague. Murdoch, a professional actor and entertainer for 12 years before the war, recognised Horne's talent as a performer, and used his contacts to secure him more broadcasting work.
After Horne has been on holiday:Murdoch: But sir, honestly you are looking sunburned, all except the top of your head.Horne: Well Murdoch, it was pretty hot on the beach in Bermuda so I kept my bowler on most of the time.Murdoch: Very wise of you sir. ...You are looking sunburned. Are you like that all over?Horne: "(pause)" There's one little place that you really must see when you go to Bermuda, Murdoch...
"Much-Binding-in-the-Marsh", 1949
"Ack-Ack, Beer-Beer" came to an end in February 1944 when the BBC decided to direct their programming at the general armed forces, rather than the barrage balloon crews. A month later Horne and Murdoch had expanded the idea of the remote and fictitious Royal Air Force station, Much-Binding-in-the-Marsh. The pair took the idea to the BBC producer Leslie Bridgemont who was responsible for the show "Merry-go-Round", which featured, in weekly rotation, shows based on the Army, Navy and RAF. Bridgemont included a "Much-Binding-in-the-Marsh" section in "Merry-go-Round" on 31 March 1944; Horne played "an officer so dim that even the other officers noticed", with Murdoch as his harassed second-in-command and Sam Costa as an "amiable chump who always got things wrong".
During 1944 Horne met and fell in love with Marjorie Thomas, a war widow with a young daughter. He was divorced in early 1945, and he and Thomas were married in November that year, three months after he had been demobilised.
Postwar, a double career: 1945–58.
On his return to civilian life, Horne resumed working at Triplex, and was promoted to the position of sales director. Despite his subsequent joint career in broadcasting and business, his commercial activities always took precedence. He declared that his work on radio was only a hobby, and that he would give it up before his business career. He combined his two roles by working full-time, and writing scripts with Murdoch at weekends.
"Much-Binding-in-the-Marsh" had gained sufficient popularity over its run of 20 "Merry-go-Round" episodes to be given its own 39-week series beginning in January 1947. With the coming of peace, the supposed RAF station became a civil airport, and the show continued much as before, written by and starring Horne and Murdoch, with Sam Costa. Maurice Denham—described by Murdoch as a vocal chameleon—joined the cast and played over 60 roles. The programme became popular, with audiences of 20 million, and ran for four series until September 1950.
In March 1948 Horne appeared with Murdoch in six episodes of the BBC Television comedy series "Kaleidoscope". In June that year he and Murdoch again appeared on television in a one-off sitcom, "At Home", which they wrote. The following year Horne began his connection with "Twenty Questions", an association that lasted, on-and-off, for 20 years. By the fourth series of "Much-Binding" in 1950, the listener figures had declined to a level that concerned the BBC and they decided against a fifth series. Rather than wait to see what other offers of work would come in from the Corporation, Horne and Murdoch signed the comedy to a 35-programme series on Radio Luxembourg between October 1950 and June 1951. The programme was poorly received on the commercial channel: Murdoch observed that "it wasn't really a great success—even my mother said it was rotten, and she was my greatest fan". After one series, the show returned to the BBC in 1951–52, although renamed as "Over to You". Murdoch and Horne again appeared together, in April 1952, on "Desert Island Discs".
In 1954, after nine years in his senior position at Triplex, and 27 years at the company, Horne accepted the position of managing director of the British Industries Fair, a government-backed organisation promoting British goods worldwide; he took up his position in July 1955. Much of the work involved liaising with foreign buyers and delegations, and he accompanied the Queen and Duke of Edinburgh on visits to the annual fair. In 1956 the government withdrew its funding and the BIF closed. Horne received several attractive job offers, and chose the post of chairman and managing director of the toy manufacturers Chad Valley, where he was a success. In September that year he and Murdoch appeared in a one-off television programme "Show for the Telly".
In January 1957 Horne appeared as the compere on the popular Saturday evening comedy and music radio show "Variety Playhouse", initially for a run of four months, but soon extended until the end of June. He also began to write a weekly column for the women's magazine "She", and to appear in an increasing number of other programmes. After his work on "Variety Playhouse" had finished, he and the programme's writers, Eric Merriman and Barry Took, prepared a script for a pilot episode of a new show, "Beyond Our Ken". The show, in which Horne was joined by Kenneth Williams, Ron Moody, Hugh Paddick and Betty Marsden, was broadcast in October 1957.
A single career: 1958–69.
The pilot episode of "Beyond Our Ken" was well received by the BBC, and they commissioned a series to start in April 1958. On 27 February that year—his 51st birthday—Horne suffered a debilitating stroke and was totally paralysed down his left-hand side and lost the power of speech. He underwent a course of intensive physiotherapy and was able to return home after two weeks. His voice returned when, during heavy massage on his left thigh, a sharp pain led to him shouting "You bugger!" at the physiotherapist. His doctor told him that the stroke was caused by the stress of combining a full-time business post with his broadcasting work. He also told Horne that when he had recovered he would never be fit enough to continue as before. Horne considered that it was not the physical problem of combining his two careers, but the mental strain of problems in his business life; accordingly he decided to give up commerce and concentrate on a career in entertainment. Because of the stroke, plans for "Beyond Our Ken" were suspended.
In April 1958 Horne eased himself back into broadcasting as chairman of "Twenty Questions". This evidence of his recovery was sufficient for the BBC to begin recording "Beyond Our Ken" in June, in preparation for the broadcast of the first series between July and November. "Beyond Our Ken" was written around the imperturbable establishment figure of Horne, while the other performers played a "spectrum of characters never before heard on the radio", including the exaggeratedly upper class Rodney and Charles, the genteel pensioners Ambrose and Felicity, the cook Fanny Haddock—a parody of popular TV cook Fanny Cradock—and the gardener Arthur Fallowfield. The first episode was not well received by a sample audience, but the BBC decided to back Horne and his team, and the initial six-week contract was extended to 21 weeks. Before the series came to an end a second had been commissioned to run the following year. After the first series Moody was succeeded by Bill Pertwee; Took left after the second series, leaving Merriman to write the remaining programmes on his own.
The second series of "Beyond Our Ken" followed in 1959; a third in 1960. Horne also continued his work in television, hosting his own series, "Trader Horne", and appearing on a number of other programmes. In April 1961 he made his second appearance on "Desert Island Discs", this time unaccompanied by Murdoch. In October that year—three weeks after the fifth series of "Beyond Our Ken" began recording—Horne appeared as the anchorman on a new BBC television series, "Let's Imagine", a discussion programme which ran for 20 editions over 18 months. He was the subject of "This Is Your Life" in February 1962, hosted by Eamonn Andrews, in which guests included friends and colleagues from his connections in business and entertainment. In June 1963 he began "Ken's Column", a series of 15-minute one-man programmes for Anglia Television.
The seventh series of "Beyond Our Ken" finished in February 1964, with an average audience of ten million listeners per programme. In September that year Horne returned from holiday and was scheduled to appear in a number of programmes; Eric Merriman objected to Horne's activities, saying that Horne had been made into a star by the writer, and that "no other comedy series should be allowed to use him". When the BBC refused to withdraw Horne from the second programme, "Down with Women", Merriman resigned from writing "Beyond Our Ken" and the show came to an end. After some pressure from Horne to keep the remainder of the team together, the BBC commissioned a replacement series, "Round the Horne", on similar lines. They turned to one of the original writers of "Beyond Our Ken", Barry Took and his new writing partner, Marty Feldman. Horne remained the genial and unflappable focal figure, and the writers invented several new and eccentric characters to revolve round him. They included J. Peasemold Gruntfuttock, the walking slum; the Noël Coward parodies Charles and Fiona; the incompetent villain Dr. Chou En Ginsberg; the folk singer Rambling Syd Rumpo and the "outrageously camp" Julian and Sandy. The resulting programme was described by radio historians Andy Foster and Steve Furst as "one of the seminal comedies to come out of the BBC", while "The Spectator" described it as "one of the great radio successes". The first series of "Round the Horne", consisting of 16 episodes, ran from March to June 1965. Horne's role was to provide "the perfect foil to the inspired lunacy happening all around him":
On 7 October 1966, at the age of 59, Horne suffered a major heart attack. He was much weakened, and was unfit to work for three months. As a result, he did not appear in the "Round the Horne" Christmas special. He returned to work in January 1967 to record the third series.
"Round the Horne" ran to four series, broadcast in successive years, and finished in June 1968. Three weeks after the fourth series finished, the first episode of "Horne A'Plenty" was broadcast on ITV. In a sketch show format, and with Barry Took as script editor (and later producer), this was an attempt to translate the spirit of "Round the Horne" to television, although with different actors supporting Horne: Graham Stark, for example, substituted for Kenneth Williams and Sheila Steafel for Betty Marsden. The first six-part series ran from 22 June to 27 July 1968, the second (by which time ABC had become Thames Television) from 27 November to 1 January 1969.
Death and tributes.
Because of his heart condition, Horne had been prescribed an anticoagulant, but had stopped taking it on the advice of a faith healer. Horne died of a heart attack on 14 February 1969, while hosting the annual Guild of Television Producers' and Directors' Awards at the Dorchester hotel in London. Presenting the awards was Earl Mountbatten of Burma; an award had gone to Barry Took and Marty Feldman for their TV series "Marty", and Horne had just urged viewers to tune into the fifth series of "Round the Horne" (which was due to start on 16 March) when he fell from the podium. The televised recording of the event omitted the incident, with announcer Michael Aspel explaining, "Mr Horne was taken ill at this point and has since died." A memorial service was held at St Martin-in-the-Fields in March that year.
After his death, Horne was eulogised in "The Times" as "a master of the scandalous double-meaning delivered with shining innocence", while "The Sunday Mirror" called him "one of the few personalities who bridged the generation gap" and "perhaps the last of the truly great radio comics." In the December 1970 issue of "The Listener", Barry Took recalled "Round the Horne" and said of its star:
"He was an unselfish performer, but it was still always "his" show. You just knew it. A Martian would have known it. His warmth tempered the sharpness of the writing ... To say that everyone loved him sounds like every obituary ever written – nonetheless it's true ... Horne was one of the few great men I have met, and his generosity of spirit and gesture have, in my experience, never been surpassed. I mourn him still."
On hearing the news Kenneth Williams wrote in his diary that "I loved that man. His unselfish nature, his kindness, tolerance and gentleness were an example to everyone". In "The Sunday Times" in February 1969, Paul Jennings wrote of him: "If I ever knew a gentleman, it was Kenneth Horne. ... He gave you his whole attention, his whole courtesy. And what a courtesy it was! ... I knew him in the context of panel games, to which his marvellous unforced humour, spontaneous but beautifully timed, always added sparkle."
Technique.
Horne's friend, Barry Took, considered that "Horne's rich, fruity voice and warm patrician manner made him the ideal link man and that, coupled with a mischievous sense of humour, ensured that any programme in which he was involved was the better for his presence". Horne attributed his voice and delivery "to 'the Grace of God', his grandfather Lord Cozens-Hardy, the former Master of the Rolls, and the hard training of being 'a jovial chap among the golf and motoring fraternity'."
The obituarist for "The Times" highlighted Horne's "remarkably skilful but very personal comic technique" of playing "a friendly good-natured old buffer who was simply doing his best, apparently lost in wonder, at the glossier, more spectacular talents of those among whom he found himself". The media analysts Frank Krutnik and Steve Neale see a similar role and consider that "Horne functioned, like [Jack] Benny, [Fred] Allen and [Tommy] Handley before him, as a 'stooge' rather than a joke-wielder, frequently switching roles between announcer and in-sketch performer".
In "Round the Horne", as well as acting as link man, Horne also played other character roles in the film and melodrama spoofs, but always sounded exactly like Kenneth Horne. Referring to his ability with voices, he commented that "between them Betty, Ken W., Hugh and Bill Pertwee can provide at least 100 voices, and if you take me into account the figure leaps to 101." Williams reported that Horne had a card index mind, "in which there seemed to be stored every funny voice, every dialect, every comedy trick, which he knew that each member of the cast was capable of", and would suggest a change in approach if a line did not work during rehearsals.
Graham Ball, writing in the "Sunday Express" observed that Horne "didn't tell jokes in the usual manner, didn't have a catchphrase and never resorted to blue comedy". Ball also identified that Horne's "stage character, that of a slightly bufferish English gent, was adored by middle- and working-class audiences alike. His humour was original, almost underplayed, but the effortless delivery and uncanny timing concealed an almost anarchic sense of mischief."
Legacy.
By 24 February 1969 it had been decided that "Round the Horne" could not continue without its star. As a result, the scripts for Series Five (which Horne had jokingly suggested should be subtitled 'The First All-Nude Radio Show') were hastily adapted into a new series for Kenneth Williams called "Stop Messing About", which ran for two series but was widely judged a failure and discontinued in 1970. On the first day of recording the new show, Williams wrote in his diary that "I miss [Horne] dreadfully. I could weep for all that "goodness" gone from our atmosphere at the show".
A successful stage show called "Round the Horne ... Revisited" opened in London in October 2003, compiled by Series Four co-writer Brian Cooke from original scripts. It ran until April 2005, and also generated three nationwide tours and a BBC television film. Horne was played by Stephen Critchlow, who also played him in the 2006 BBC television drama "".
On 27 February 2007 (Horne's centenary), BBC Radio 4 broadcast a half-hour documentary tribute entitled "Sound the Horne", hosted by Jimmy Carr. The following year, on 18 September, another Radio 4 documentary was broadcast; called "Thoroughly Modest Mollie", this one focused on Horne's frequent ghost-writer, Mollie Millest, and featured Jonathan Rigby as Horne. Rigby reprised his role in 2008–09 in a new show, devised this time by Barry Took's widow Lyn, called "Round the Horne – Unseen and Uncut", which took the form of a recording session for one of the episodes. In 2009 an unbroadcast pilot script written by Horne and Millest in 1966 was produced by the same Radio 4 team. Called "Twice Ken is Plenty" and intended as a two-man showcase for Horne and Kenneth Williams, the 21st century version was performed by Rigby, again playing Horne, and Robin Sebastian. The show was broadcast on 1 September 2009.
Horne has been the subject of two biographies, Norman Hackforth's "Solo for Horne" in 1976 and Barry Johnston's "Round Mr Horne: The Life of Kenneth Horne" in 2006. In 1998 Ernie Wise unveiled a blue plaque to Horne at BBC Broadcasting House. Editions of "Beyond Our Ken" and "Round the Horne" are regularly broadcast on the digital radio service BBC 4 Extra, and by 2006 over half a million copies of tapes and CDs of "Round the Horne" had been sold by the BBC. In a 2002 survey conducted by the BBC to find listeners' favourite British comedian, Horne appeared third, behind Tony Hancock and Spike Milligan.
Notes and references.
Notes
References
Sources.
</dl>

</doc>
<doc id="51276" url="http://en.wikipedia.org/wiki?curid=51276" title="Marty Feldman">
Marty Feldman

Martin Alan "Marty" Feldman (8 July 1934 – 2 December 1982) was an English comedy writer, comedian and actor, easily identified by his bulbous and crooked eyes. He starred in several British television comedy series, including "At Last the 1948 Show" and "Marty", the latter of which won two BAFTA awards. He was the first Saturn Award winner for Best Supporting Actor for his role in "Young Frankenstein".
Early life.
Feldman was born on 8 July 1934 in the East End of London, the son of Jewish immigrants from Kiev, Ukraine, Cecilia (née Crook) and Myer Feldman, a gown manufacturer. He recalled his childhood as "solitary."
A BBC documentary explained that a botched operation for his Graves' disease resulted in his eyes being more protruded and misaligned (strabismus). Leaving school at 15, he worked at the Dreamland funfair in Margate, but had dreams of a career as a jazz trumpeter, and performed in the first group in which tenor saxophonist Tubby Hayes was a member. Feldman joked that he was "the world's worst trumpet player." By the age of 20 though, he had decided to pursue a career as a comedian.
Early career.
Although his early performing career was undistinguished, he became part of a comedy act, Morris, Marty and Mitch, which made their first television appearance on a BBC series called "Showcase" in April 1955. Later in the decade, Feldman worked on the scripts of "Educating Archie", in both its radio and television incarnations with Ronald Chesney and Ronald Wolfe.
In 1954 Feldman first met Barry Took, while they were both working as performers, and with Took he would eventually form an enduring writing partnership which lasted until 1974. They wrote a few episodes of "The Army Game" (1960) and the bulk of "Bootsie and Snudge" (1960–62), both situation comedies made by Granada Television for the ITV network. For BBC radio they wrote "Round the Horne" (1964–67), their best remembered comedy series, which starred Kenneth Horne and Kenneth Williams. The last series of "Round the Horne" in 1968 was written by other hands. This work placed Feldman and Took "in the front rank of comedy writers" according to Denis Norden.
Feldman became the chief writer and script editor on "The Frost Report" (1966–67). He co-wrote the much shown "Class sketch" with John Law, in which John Cleese, Ronnie Barker and Ronnie Corbett faced the audience, with their descending order of height suggesting their relative social status as upper class (Cleese), middle class (Barker) and working class (Corbett).
The television sketch comedy series "At Last the 1948 Show" raised Feldman's profile as a performer. The other three participants – future Pythons Graham Chapman and Cleese and future Goodie Tim Brooke-Taylor needed a fourth cast member and had Feldman in mind. In one sketch on 1 March 1967, Feldman's character harassed a patient shop assistant (played by Cleese) for a series of fictitious books, achieving success with "Ethel the Aardvark Goes Quantity Surveying". His character in "At Last the 1948 Show" seems often to be called Mr Pest, according to John Cleese. Feldman was co-author, along with Cleese, Chapman and Brooke-Taylor of the "Four Yorkshiremen" sketch, which was written for "At Last the 1948 Show".
Feldman was given his own series on the BBC called "Marty" (1968); it featured Brooke-Taylor, John Junkin and Roland MacLeod, with Cleese as one of the writers. Feldman won two BAFTA awards. The second series in 1969 was renamed "It's Marty" (the second title being retained for the DVD of the show)
After 1970.
In 1971, Feldman gave evidence in favour of the defendants in the "Oz" trial. He would not swear on the Bible, choosing to affirm. Throughout his testimony he was disrespectful to the judge after it was implied that he had no religion for not being Christian. By this time, "The Marty Feldman Comedy Machine" (1971–72) was in preparation, a TV series co-produced by Associated Television (ATV) and the American Broadcasting Company (ABC). This show lasted for one series.
In 1974, Dennis Main Wilson produced a short BBC sketch series for Feldman entitled "Marty Back Together Again" – a reference to reports about the star's health. But this never captured the impact of the earlier series. The "Marty" series proved popular enough with an international audience (the first series won the Golden Rose Award at Montreux) to launch a film career. His first feature role was in "Every Home Should Have One" (1970). Feldman spent time in Soho jazz clubs, as he found a parallel between "riffing" in a comedy partnership and the improvisation of jazz.
Feldman's performances on American television included "The Dean Martin Show". On film, he was Igor (pronounced "EYE-gore" – a comic response to Wilder's claim that "it's pronounced FRONK-EN-SHTEEN") in Mel Brooks' "Young Frankenstein" (1974) where many lines were improvised. Gene Wilder says he had Feldman in mind when he wrote the part. At one point, Dr Frankenstein (Wilder) scolds Igor with the phrase, "Damn your eyes!" Feldman turns to the camera, points to his misaligned eyes with a grin and says, "Too late!" 
In 1976, Feldman ventured into Italian cinema, starring with Barbara Bouchet in "40 gradi all'ombra del lenzuolo" "(Sex with a Smile"), a sex comedy. He appeared in "The Adventure of Sherlock Holmes' Smarter Brother" and Mel Brooks' "Silent Movie", as well as directing and starring in "The Last Remake of Beau Geste". He guest-starred in the "Arabian Nights" episode of "The Muppet Show" with several "Sesame Street" characters, especially Cookie Monster with whom he shared a playful cameo comparing their eyes side by side.
During the course of his career, Feldman recorded one LP, "I Feel a Song Going Off" (1969), re-released as "The Crazy World of Marty Feldman". The songs were written by Denis King, John Junkin and Bill Solly (a writer for Max Bygraves and "The Two Ronnies"). It was re-released as a CD in 2007.
Personal life.
Feldman was married to Lauretta Sullivan (29 September 1935 – 12 March 2010) from January 1959 until his death in 1982. She died at age 74 in Studio City, California.
His younger sister, Pamela, who no longer uses her family name, lives in the Tereken district of the Belgian town of Sint-Niklaas, and now uses the name Veronique.
Death.
Feldman died from a heart attack in a hotel room in Mexico City on 2 December 1982 at age 48, during the making of the film "Yellowbeard". On the DVD commentary of "Young Frankenstein", Mel Brooks cites factors that may have contributed to Marty's death: "He smoked sometimes half-a-carton (5 packs) of cigarettes daily, drank copious amounts of black coffee, and ate a diet rich in eggs and dairy products".
Michael Mileham, who made the behind-the-scenes movie "Group Madness" about the making of "Yellowbeard", said that on the day before Feldman died, the two of them had swum to a small island in a lake in Mexico City where a local was selling lobster and coconuts. Mileham and Feldman used the same knife on their lobsters; Mileham claimed he got shellfish poisoning the next day, and theorised that this could also have contributed to Marty's death.
He is buried in Forest Lawn – Hollywood Hills Cemetery near his idol, Buster Keaton, in the Garden of Heritage.

</doc>
<doc id="51278" url="http://en.wikipedia.org/wiki?curid=51278" title="Albuquerque, New Mexico">
Albuquerque, New Mexico

Albuquerque is the most populous city in the U.S. state of New Mexico. It is a high-altitude city and serves as the county seat of Bernalillo County, and it is situated in the central part of the state, straddling the Rio Grande. The city population was 555,417 as of the July 1, 2012, population estimate from the United States Census Bureau, and ranks as the 32nd-largest city in the U.S. The Albuquerque MSA has a population of 902,797 according to the United States Census Bureau's most recently available estimate for July 1, 2013. Albuquerque is the 59th-largest United States metropolitan area. The Albuquerque MSA population includes the city of Rio Rancho, Bernalillo, Placitas, Corrales, Los Lunas, Belen, Bosque Farms, and forms part of the larger Albuquerque – Santa Fe – Las Vegas combined statistical area, with a total population of 1,163,964 as of the July 1, 2013 Census Bureau estimates. With its population growing bigger over the years, Albuquerque is the fastest growing city in New Mexico.
Albuquerque is home to the University of New Mexico (UNM), Kirtland Air Force Base, Sandia National Laboratories, Lovelace Respiratory Research Institute, Central New Mexico Community College (CNM), Presbyterian Health Services, and Petroglyph National Monument. The Sandia Mountains run along the eastern side of Albuquerque, and the Rio Grande flows through the city, north to south.
History.
Etymology.
The growing village soon to become Albuquerque was named by the provincial governor Francisco Cuervo y Valdes in honor of Francisco, Duke of Alburquerque, who was viceroy of New Spain from 1653 to 1660. Francisco's title referred to the Spanish town of Alburquerque, the name of which derived from the Latin "albus quercus" meaning "white oak". This name was probably in reference to the prevalence of cork oaks in the region, which have a white wood when the bark is removed. Alburquerque is still a center of the Spanish cork industry, and the town coat-of-arms features a white cork oak. 
The first "r" in Alburquerque was later dropped, probably due to association with the prominent general Alfonso de Albuquerque, whose family originated from the Spanish town, but used a variant spelling in their name. The change was also in part due to the fact that citizens found the original name difficult to pronounce.
Western folklore offers a different explanation, tracing the name Albuquerque to the Galician word "albaricoque", meaning "apricot". The apricot was brought to New Mexico by Spanish settlers, possibly as early as 1743. As the story goes, the settlement was established near an apricot tree, and became known as "La Ciudad de Albaricoque". As frontiersmen were unable to correctly pronounce the Galician word, it became corrupted to "Albuquerque".
Early settlers.
Albuquerque was founded in 1706 as the Spanish colonial outpost of "Ranchos de Alburquerque". Present-day Albuquerque retains much of its historical Spanish cultural heritage.
Albuquerque was a farming community and strategically located military outpost along the Camino Real. The town was also the sheep-herding center of the West. Spain established a presidio (military garrison) in Albuquerque in 1706. After 1821, Mexico also had a military garrison there. The town of Alburquerque was built in the traditional Spanish village pattern: a central plaza surrounded by government buildings, homes, and a church. This central plaza area has been preserved and is open to the public as a museum, cultural area, and center of commerce. It is referred to as "Old Town Albuquerque" or simply "Old Town." Historically it was sometimes referred to as "La Placita" ("little plaza" in Spanish). On the north side of Old Town Plaza is San Felipe de Neri Church. Built in 1793, it is one of the oldest surviving buildings in the city.
After the American occupation of New Mexico, Albuquerque had a federal garrison and quartermaster depot, the Post of Albuquerque, from 1846 to 1867. During the Civil War Albuquerque was occupied in February 1862 by Confederate troops under General Henry Hopkins Sibley, who soon afterward advanced with his main body into northern New Mexico. During his retreat from Union troops into Texas he made a stand on April 8, 1862, at Albuquerque and fought the Battle of Albuquerque against a detachment of Union soldiers commanded by Colonel Edward R. S. Canby. This daylong engagement at long range led to few casualties.
When the Atchison, Topeka and Santa Fe Railroad arrived in 1880, it bypassed the Plaza, locating the passenger depot and railyards about 2 miles (3 km) east in what quickly became known as New Albuquerque or New Town. Many Anglo merchants, mountain men, and settlers slowly filtered into Albuquerque creating a major mercantile commercial center which is now Downtown Albuquerque. Due to a rising rate of violent crime, gunman Milt Yarberry was appointed the town's first marshal that year. New Albuquerque was incorporated as a town in 1885, with Henry N. Jaffa its first mayor, and it was incorporated as a city in 1891.:232–233 Old Town remained a separate community until the 1920s when it was absorbed by the city of Albuquerque. Old Albuquerque High School, the city's first public high school, was established in 1879.
Early 20th century.
By 1900, Albuquerque boasted a population of 8,000 inhabitants and all the modern amenities, including an electric street railway connecting Old Town, New Town, and the recently established University of New Mexico campus on the East Mesa. In 1902, the famous Alvarado Hotel was built adjacent to the new passenger depot, and it remained a symbol of the city until it was razed in 1970 to make room for a parking lot. In 2002, the Alvarado Transportation Center was built on the site in a manner resembling the old landmark. The large metro station functions as the downtown headquarters for the city's transit department. It also serves as an intermodal hub for local buses, Greyhound buses, Amtrak passenger trains, and the Rail Runner commuter rail line.
New Mexico's dry climate brought many tuberculosis patients to the city in search of a cure during the early 20th century, and several sanitaria sprang up on the West Mesa to serve them. Presbyterian Hospital and St. Joseph Hospital, two of the largest hospitals in the Southwest, had their beginnings during this period. Influential New Deal–era governor Clyde Tingley and famed Southwestern architect John Gaw Meem were among those brought to New Mexico by tuberculosis.
Decades of growth.
The first travelers on Route 66 appeared in Albuquerque in 1926, and before long, dozens of motels, restaurants, and gift shops had sprung up along the roadside to serve them. Route 66 originally ran through the city on a north-south alignment along Fourth Street, but in 1937 it was realigned along Central Avenue, a more direct east-west route. The intersection of Fourth and Central downtown was the principal crossroads of the city for decades. The majority of the surviving structures from the Route 66 era are on Central, though there are also some on Fourth. Signs between Bernalillo and Los Lunas along the old route now have brown, historical highway markers denoting it as "Pre-1937 Route 66."
The establishment of Kirtland Air Force Base in 1939, Sandia Base in the early 1940s, and Sandia National Laboratories in 1949, would make Albuquerque a key player of the Atomic Age. Meanwhile, the city continued to expand outward onto the West Mesa, reaching a population of 201,189 by 1960. In 1990, it was 384,736 and in 2007 it was 518,271. In June 2007, Albuquerque was listed as the sixth fastest-growing city in America by CNN and the United States Census Bureau. In 1990, the Census Bureau reported Albuquerque's population as 34.5% Hispanic and 58.3% non-Hispanic white.
Albuquerque's downtown entered the same phase and development (decline, "urban renewal" with continued decline, and gentrification) as nearly every city across the United States. As Albuquerque spread outward, the downtown area fell into a decline. Many historic buildings were razed in the 1960s and 1970s to make way for new plazas, high-rises, and parking lots as part of the city's urban renewal phase. s of 2010[ [update]], only recently has downtown come to regain much of its urban character, mainly through the construction of many new loft apartment buildings and the renovation of historic structures such as the KiMo Theater, in the gentrification phase.
New millennium.
During the 21st century, the Albuquerque population has continued to grow rapidly. The population of the city proper was estimated at 528,497 in 2009, up from 448,607 in the 2000 census.
During 2005 and 2006, the city celebrated its tricentennial with a diverse program of cultural events.
Urban trends and issues.
The passage of the Planned Growth Strategy in 2002–2004 was the community's strongest effort to create a framework for a more balanced and sustainable approach to urban growth.
A critical finding of the study is that many of the 'disconnects' between the public's preferences and what actually is taking place are caused by weak or non-existent implementation tools – rather than by inadequate policies, as contained in the City/County Comprehensive Plan and other already adopted legislation.
Urban sprawl is limited on three sides—by the Sandia Pueblo to the north, the Isleta Pueblo and Kirtland Air Force Base to the south, and the Sandia Mountains to the east. Suburban growth continues at a strong pace to the west, beyond Petroglyph National Monument, once thought to be a natural boundary to sprawl development.
Because of less-costly land and lower taxes, much of the growth in the metropolitan area is taking place outside of the city of Albuquerque itself. In Rio Rancho to the northwest, the communities east of the mountains, and the incorporated parts of Valencia County, population growth rates approach twice that of Albuquerque. The primary cities in Valencia County are Los Lunas and Belen, both of which are home to growing industrial complexes and new residential subdivisions. The mountain towns of Tijeras, Edgewood, and Moriarty, while close enough to Albuquerque to be considered suburbs, have experienced much less growth compared to Rio Rancho, Bernalillo, Los Lunas, and Belen. Limited water supply and rugged terrain are the main limiting factors for development in these towns. The Mid Region Council of Governments (MRCOG), which includes constituents from throughout the Albuquerque area, was formed to ensure that these governments along the middle Rio Grande would be able to meet the needs of their rapidly rising populations. MRCOG's cornerstone project is the New Mexico Rail Runner Express. In October 2013, the "Albuquerque Journal" reported Albuquerque as the third best city to own an investment property.
Geography.
According to the United States Census Bureau, Albuquerque has a total area of 490.9 sqkm, of which 486.2 km2 is land and 4.7 km2, or 0.96%, is water.
Albuquerque lies within the northern, upper edges of the Chihuahuan Desert ecoregion, based on long-term patterns of climate, associations of plants and wildlife, and landforms, including drainage patterns.
Located in central New Mexico, the city also has noticeable influences from the adjacent Colorado Plateau Semi-Desert, Arizona-New Mexico Mountains, and Southwest Plateaus and Plains Steppe ecoregions, depending on where one is located. Its main geographic connection lies with southern New Mexico, while culturally, Albuquerque is a crossroads of most of New Mexico.
Albuquerque has one of the highest elevations of any major city in the United States, though the effects of this are greatly tempered by its southwesterly continental position. The elevation of the city ranges from 4,900 feet (1,490 m) above sea level near the Rio Grande (in the Valley) to over 6,700 feet (1,950 m) in the foothill areas of Sandia Heights and Glenwood Hills. At the airport, the elevation is 5,352 feet (1,631 m) above sea level.
The Rio Grande is classified, like the Nile, as an "exotic" river because it flows through a desert. The New Mexico portion of the Rio Grande lies within the Rio Grande Rift Valley, bordered by a system of faults, including those that lifted up the adjacent Sandia and Manzano Mountains, while lowering the area where the life-sustaining Rio Grande now flows.
Geology.
Albuquerque lies in the Albuquerque Basin, a portion of the Rio Grande rift.
The Sandia Mountains are the predominant geographic feature visible in Albuquerque. "Sandía" is Spanish for "watermelon", and is popularly believed to be a reference to the brilliant coloration of the mountains at sunset: bright pink (melon meat) and green (melon rind). The pink is due to large exposures of granodiorite cliffs, and the green is due to large swaths of conifer forests. However, Robert Julyan notes in "The Place Names of New Mexico", "the most likely explanation is the one believed by the Sandia Pueblo Indians: the Spaniards, when they encountered the Pueblo in 1540, called it Sandia, because they thought the squash growing there were watermelons, and the name Sandia soon was transferred to the mountains east of the pueblo." He also notes that the Sandia Pueblo Indians call the mountain "Bien Mur", "big mountain."
The Sandia foothills, on the west side of the mountains, have soils derived from that same rock material with varying sizes of decomposed granite, mixed with areas of clay and caliche (a calcium carbonate deposit common in the arid southwestern USA), along with some exposed granite bedrock.
Below the foothills, the area usually called the "Northeast Heights" consists of a mix of clay and caliche soils, overlain by a layer of decomposed granite, resulting from long-term outwash of that material from the adjacent mountains. This bajada is quite noticeable when driving into Albuquerque from the north or south, due to its fairly uniform slope from the mountains' edge downhill to the valley. Sand hills are scattered along the I-25 corridor and directly above the Rio Grande Valley, forming the lower end of the Heights.
The Rio Grande Valley, due to long-term shifting of the actual river channel, contains layers and areas of soils varying between caliche, clay, loam, and even some sand. It is the only part of Albuquerque where the water table often lies close to the surface, sometimes less than 10 ft.
The last significant area of Albuquerque geologically is the West Mesa: this is the elevated land west of the Rio Grande, including "West Bluff", the sandy terrace immediately west and above the river, and the rather sharply defined volcanic escarpment above and west of most of the developed city. The west mesa commonly has soils often referred to as "blow sand", along with occasional clay and caliche and even basalt, nearing the escarpment.
Landscape.
A panoramic view of the city of Albuquerque.
Climate.
Albuquerque has an arid desert climate ("BSk" or "BSh", depending on the particular scheme of the Köppen climate classification system one uses), with mild winters, and hot summers. Albuquerque is in the northern tip of the Chihuahuan Desert, near the edge of the Colorado Plateau. The average annual precipitation is less than half of evaporation, and no month averages below freezing.
Albuquerque's climate is usually sunny and dry, with low relative humidity, with an average of 3,415 sunshine hours per year. Brilliant sunshine defines the region, averaging 278 days a year; periods of variably mid and high-level cloudiness temper the sun at other times. Extended cloudiness is rare. The city has four distinct seasons, but the heat and cold are mild compared to the extremes that occur more commonly in other parts of the country.
Winters are rather brief, with December, the coolest month, averaging 36.3 °F, although low temperatures bottom out in January, and the coldest temperature of the year is typically around 10 °F. There are 2.8 days where the high is at or below freezing.
Spring is windy and warm, sometimes unsettled with some rain, though spring is usually the driest part of the year in Albuquerque. March and April tend to see many days with the wind blowing at 20 to, and afternoon gusts can produce periods of blowing sand and dust. In May, the winds tend to subside, as temperatures start to feel like summer.
The summer heat is relatively tolerable because of low humidity, except the onset of the monsoon can bring increased humidity. There are 2.7 days of 100 °F+ highs annually, mostly in June and July and rarely in August due in part to the monsoon; an average 60 days see 90 °F+ highs.
Fall sees warm days and cool nights with less rain, though the weather can be more unsettled closer to winter.
The city was one of several in the region experiencing a severe winter storm on December 28–30, 2006, with locations in Albuquerque receiving between 10.5 and of snow.
The mountains and highlands beyond the city create a rain shadow effect, due to the drying of air ascending the mountains; the city usually receives very little rain or snow, averaging 8–9 inches (216 mm) of precipitation per year. Valley and west mesa areas, farther from the mountains are drier, averaging 6–8 inches of annual precipitation; the Sandia foothills tend to lift any available moisture, enhancing precipitation to about 10–17 inches annually.
Traveling to the west, north and east of Albuquerque, one quickly rises in elevation and leaves the sheltering effect of the valley to enter a noticeably cooler and slightly wetter environment. One such area is still considered part of metro Albuquerque, commonly called the "East Mountain" area; it is covered in savannas or woodlands of low juniper and piñon trees, reminiscent of the lower parts of the southern Rocky Mountains, which do not actually contact Albuquerque proper. Most rain occurs during the summer monsoon season (also called a chubasco in Mexico), typically starting in early July and ending in mid-September.
Hydrology.
Albuquerque's drinking water presently comes from a combination of Rio Grande water (river water diverted from the Colorado River basin through the San Juan-Chama Project) and a delicate aquifer that was once described as an "underground Lake Superior". The Albuquerque Bernalillo County Water Utility Authority (ABCWUA) has developed a water resources management strategy, which pursues conservation and the direct extraction of water from the Rio Grande for the development of a stable underground aquifer in the future.
The aquifer of the Rio Puerco is too saline to be cost-effectively used for drinking purposes. Much of the rainwater that Albuquerque receives does not recharge its aquifer. It is diverted through a network of paved channels and arroyos, and emptied into the Rio Grande.
Of the 62780 acre.ft per year of the water in the upper Colorado River basin entitled to municipalities in New Mexico by the Upper Colorado River Basin Compact, Albuquerque owns 48,200. The water is delivered to the Rio Grande by the San Juan–Chama Project. The project's construction was initiated by legislation enacted by President John F. Kennedy in 1962, and completed in 1971. This diversion project transports water under the continental divide from Navajo Lake to Lake Heron on the Rio Chama, a tributary of the Rio Grande. In the past much of this water was resold to downstream owners in Texas. These arrangements ended in 2008 with the completion of the ABCWUA's Drinking Water Supply Project.
The ABCWUA's Drinking Water Supply Project uses a system of adjustable height dams to skim water from the Rio Grande into sluices which lead to water treatment facilities for direct conversion to potable water. Some water is allowed to flow through central Albuquerque, mostly to protect the endangered Rio Grande Silvery Minnow. Treated effluent water is recycled into the Rio Grande to the south of the city. The ABCWUA expects river water to comprise up to seventy percent of its water budget in 2060. Groundwater will constitute the remainder. One of the policies of the ABCWUA's strategy is the acquisition of additional river water. :Policy G, 14
Quadrants.
Albuquerque is geographically divided into four quadrants which are officially part of the mailing address. They are NE (northeast), NW (northwest), SE (southeast), and SW (southwest). The north-south dividing line is Central Avenue (the path that Route 66 took through the city) and the east-west dividing line is the BNSF Railway tracks.
Northeast Quadrant.
This quadrant has been experiencing a housing expansion since the late 1950s. It abuts the base of the Sandia Mountains and contains portions of the foothills neighborhoods, which are significantly higher, in elevation and price range, than the rest of the city. Running from Central Avenue and the railroad tracks to the Sandia Peak Aerial Tram, this is the largest quadrant both geographically and by population. The University of New Mexico, the Maxwell Museum of Anthropology, Nob Hill, the Uptown area which includes two shopping malls (Coronado Center and ABQ Uptown), Journal Center, and Balloon Fiesta Park are all located in this quadrant.
Some of the most affluent neighborhoods in the city are located here, including: High Desert, Tanoan, Sandia Heights, and North Albuquerque Acres. (Parts of Sandia Heights and North Albuquerque Acres are outside the city limits proper). A few houses in the farthest reach of this quadrant lie in the Cibola National Forest, just over the line into Sandoval County.
Northwest Quadrant.
This quadrant contains historic Old Town Albuquerque, which dates back to the 18th century, as well as the Indian Pueblo Cultural Center. The area has a mixture of commercial districts and low- to middle-income neighborhoods. Northwest Albuquerque includes the largest section of downtown, Rio Grande Nature Center State Park and the Bosque ("woodlands"), Petroglyph National Monument, Double Eagle II Airport, Martineztown, the Paradise Hills neighborhood, Taylor Ranch, and Cottonwood Mall.
Additionally, the "North Valley" area, which has some expensive homes and small ranches along the Rio Grande, is located here. The city of Albuquerque engulfs the village of Los Ranchos de Albuquerque and borders Corrales in the North Valley. A small portion of the rapidly developing area on the west side of the river south of the Petroglyphs, known as the "West Mesa" or "Westside", consisting primarily of traditional residential subdivisions, also extends into this quadrant. The city proper is bordered on the north by the city of Rio Rancho.
Southeast Quadrant.
Kirtland Air Force Base, Sandia National Laboratories, Sandia Science & Technology Park, Albuquerque International Sunport, Eclipse Aerospace, American Society of Radiologic Technologists, Central New Mexico Community College, Albuquerque Veloport, University Stadium, Isotopes Park, The Pit, Mesa del Sol, The Pavilion, Albuquerque Studios, Isleta Resort & Casino, National Museum of Nuclear Science & History, New Mexico Veterans' Memorial, and Talin Market are all located in the Southeast (SE) quadrant.
The upscale neighborhood of Four Hills is located in the foothills of Southeast Albuquerque. Other neighborhoods include Nob Hill, Ridgecrest, Willow Wood, and Volterra.
Southwest Quadrant.
Traditionally consisting of agricultural and rural areas and suburban neighborhoods, the Southwest quadrant contains the community of South Valley, New Mexico, often referred to as "The South Valley". Although the city limits of Albuquerque do not include the South Valley, it extends all the way to the Isleta Indian Reservation. Newer suburban subdivisions on the West Mesa near the southwestern city limits join homes of older construction, some dating back as far as the 1940s. This quadrant includes the old communities of Atrisco, Los Padillas, Kinney, Westgate, Westside, Alamosa, Mountainview, and Pajarito. The south end of downtown Albuquerque, the Bosque ("woodlands"), the Barelas neighborhood, the National Hispanic Cultural Center, and the Albuquerque Biological Park are also located here.
A proposed development, Santolina, would extend the city further west past 118th Street SW to the edge of the Rio Puerco Valley, and house 100,000 by 2050.
Demographics.
As of the United States census of 2010, there were 545,852 people, 239,166 households, and 224,330 families residing in the city. The population density was 3010.7/mi² (1162.6/km²). There were 239,166 housing units at an average density of 1,556.7 per square mile (538.2/km²).
The racial makeup of the city was:
The ethnic makeup of the city was:
There were 239,116 households out of which 33.3% had children under the age of 18 living with them, 43.6% were married couples living together, 12.9% had a female householder with no husband present, and 38.5% were non-families. 30.5% of all households were made up of individuals and 8.4% had someone living alone who was 65 years of age or older. The average household size was 2.40 and the average family size was 3.02.
The age distribution was 24.5% under 18, 10.6% from 18 to 24, 30.9% from 25 to 44, 21.9% from 45 to 64, and 12.0% who were 65 or older. The median age was 35 years. For every 100 females there were 94.4 males. For every 100 females age 18 and over, there were 91.8 males.
The median income for a household in the city was $38,272, and the median income for a family was $46,979. Males had a median income of $34,208 versus $26,397 for females. The per capita income for the city was $20,884. About 10.0% of families and 13.5% of the population were below the poverty line, including 17.4% of those under age 18 and 8.5% of those age 65 or over.
Arts and culture.
One of the major art events in the state is the summertime New Mexico Arts and Crafts Fair, a non-profit show exclusively for New Mexico artists and held annually in Albuquerque since 1961. Albuquerque is home to over 300 other visual arts, music, dance, literary, film, ethnic, and craft organizations, museums, festivals and associations.
Points of interest.
Some of the local museums, galleries, shops and other points of interest include the Albuquerque Biological Park, Albuquerque Museum of Art and History, Museum of Natural History and Science, and Old Town Albuquerque. Albuquerque's live music/performance venues including; Isleta Amphitheater, Tingley Coliseum, Sunshine Theater and the KiMo Theater.
The local cuisine prominently features green chile, which is widely available in restaurants, including national fast-food chains. The restaurant scene of Albuquerque is quite prominent throughout the city, and receive statewide attention, alongside several of them becoming chains throughout the state.
The Sandia Peak Tramway, located adjacent to Albuquerque, is the world's second-longest passenger aerial tramway. It also has the world's third-longest single span. It stretches from the Northeast edge of the city to the crestline of the Sandia Mountains. Elevation at the top of the tramway is roughly 10300 ft.
above sea level.
Architecture.
John Gaw Meem, credited with developing and popularizing the Pueblo Revival style, was based in Santa Fe but received an important Albuquerque commission in 1933 as the architect of the University of New Mexico. He retained this commission for the next quarter-century and developed the University's distinctive Southwest style. :317 Meem also designed the Cathedral Church of St. John in 1950.
Albuquerque boasts a unique nighttime cityscape. Many building exteriors are illuminated in vibrant colors such as green and blue. The Wells Fargo Building is illuminated green. The DoubleTree Hotel and the Compass Bank building are illuminated blue. The rotunda of the county courthouse is illuminated yellow, while the tops of the Bank of Albuquerque and the Bank of the West are illuminated reddish-yellow. Due to the nature of the soil in the Rio Grande Valley, the skyline is lower than might be expected in a city of comparable size elsewhere.
Albuquerque has expanded greatly in area since the mid-1940s. During those years of expansion, the planning of the newer areas has considered that people drive rather than walk. The pre-1940s parts of Albuquerque are quite different in style and scale from the post 1940s areas. These older areas include the North Valley, the South Valley, various neighborhoods near downtown, and Corrales. The newer areas generally feature four to six lane roads in a 1 mile (1.61 km) grid. Each 1 square mile (2.59 km²) is divided into four 160 acre neighborhoods by smaller roads set 0.5 miles (0.8 km) between major roads. When driving along major roads in the newer sections of Albuquerque, one sees strip malls, signs, and cinderblock walls. The upside of this planning style is that neighborhoods are shielded from the worst of the noise and lights on the major roads. The downside is that it is virtually impossible to go anywhere from home without driving.
Sports.
The Albuquerque Isotopes are a minor league affiliate of the Colorado Rockies, having derived their name from "The Simpsons" episode "Hungry Hungry Homer", which involves the Springfield Isotopes baseball team considering relocating to Albuquerque. Prior to 2002, the Albuquerque Dukes served as the city's minor league team, having played at the Albuquerque Sports Stadium. The stadium was torn down to make room for the current Isotopes Park. In 2013 the United Soccer League announced the Albuquerque Sol soccer club will begin play in 2014. Albuquerque is also home to Jackson–Winkeljohn gym, one of the leading mixed martial arts (MMA) gyms in the world. Several MMA world champions and elite fighters train in that facility.
Parks and recreation.
The city was ranked No. 1 as the fittest city in the United States, according to a March 2007 issue of Men's Fitness magazine. The criteria used in the study included the availability of gyms and bike paths, commute times, and federal health statistics on obesity-related injuries and illnesses.
Government.
Albuquerque is a charter city. City government is divided into an executive branch, headed by a Mayor:V and a nine-member Council that holds the legislative authority.:IV The form of city government is therefore mayor-council government. The mayor is Richard J. Berry, a former state legislator, who was elected in 2009.
The Mayor of Albuquerque holds a full-time paid elected position with a four-year term.
Albuquerque City Council members hold part-time paid positions and are elected from the nine districts for four-year terms, with four or five Councilors elected every two years. Elections for Mayor and Councilor are nonpartisan.:IV.4 Each December, a new Council President and Vice-President are chosen by members of the Council. Each year, the Mayor submits a city budget proposal for the year to the Council by April 1, and the Council acts on the proposal within the next 60 days.:VII
The Albuquerque City Council is the legislative authority of the city, and has the power to adopt all ordinances, resolutions, or other legislation.
The Council meets two times a month, with meetings held in the Vincent E. Griego Council Chambers in the basement level of Albuquerque/Bernalillo County Government Center.
Ordinances and resolutions passed by the Council are presented to the Mayor for his approval. If the Mayor vetoes an item, the Council can override the veto with a vote of two-thirds of the membership of the Council.:XI.3
The Albuquerque Police Department (APD) is the police department with jurisdiction within the city limits, with anything outside of the city limits being considered the unincorporated area of Bernalillo County and policed by the Bernalillo County Sheriff's Department. It is the largest municipal police department in New Mexico, and in September 2008 the US Department of Justice recorded the APD as the 49th largest police department in the United States.
The Bernalillo County Metropolitan Court is the judicial system in Albuquerque.
Economy.
Albuquerque lies at the center of the New Mexico Technology Corridor, a concentration of high-tech private companies and government institutions along the Rio Grande. Larger institutions whose employees contribute to the population are numerous and include Sandia National Laboratories, Kirtland Air Force Base, and the attendant contracting companies which bring highly educated workers to a somewhat isolated region. Intel operates a large semiconductor factory or "fab" in suburban Rio Rancho, in neighboring Sandoval County, with its attendant large capital investment. Northrop Grumman is located along I-25 in northeast Albuquerque, and TempurPedic is located on the West Mesa next to I-40.
The solar energy and architectural-design innovator Steve Baer located his company, Zomeworks, to the region in the late 1960s; and Los Alamos National Laboratory, Sandia, and Lawrence Livermore National Laboratory cooperate here in an enterprise that began with the Manhattan Project. In January 2007, Tempur-Pedic opened an 800000 sqft mattress factory in northwest Albuquerque. SCHOTT Solar, Inc., announced in January 2008 they will open a 200000 sqft facility manufacturing receivers for concentrated solar thermal power plants (CSP) and 64MW of photovoltaic (PV) modules. The facility closed in 2012.
"Forbes" magazine rated Albuquerque as the best city in America for business and careers in 2006 and as the 13th best (out of 200 metro areas) in 2008. The city was rated seventh among America's Engineering Capitals in 2014 by "Forbes" magazine. Albuquerque ranked among the Top 10 Best Cities to Live by U.S. News & World Report in 2009 and was recognized as the fourth best place to live for families by the TLC network. It was ranked among the Top Best Cities for Jobs in 2007 and among the Top 50 Best Places to Live and Play by National Geographic Adventure magazine.
Education.
Albuquerque is home to the University of New Mexico, the largest public flagship university in the state. UNM includes a School of Medicine which was ranked in the top 50 primary care-oriented medical schools in the country. The Central New Mexico Community College is a county-funded junior college serving new high school graduates and adults returning to school. (The school was formerly called the Albuquerque Technical Vocational Institute or TVI).
Albuquerque is also home to the following programs and non-profit schools of higher learning: Southwest University of Visual Arts, Southwestern Indian Polytechnic Institute, Trinity Southwest University, the University of St. Francis College of Nursing and Allied Health Department of Physician Assistant Studies, and the St. Norbert College Master of Theological Studies program. The Ayurvedic Institute, one of the first Ayurveda colleges specializing in Ayurvedic medicine outside of India was established in the city in 1984. Other state and not-for-profit institutions of higher learning have moved some of their programs into Albuquerque. These include: New Mexico State University, Highlands University, Lewis University, Wayland Baptist University, and Webster University. Several for-profit technical schools including Pima Medical Institute, ITT Technical Institute, National American University, Grand Canyon University, the University of Phoenix and several barber/beauty colleges have established their presence in the area.
Albuquerque Public Schools (APS), one of the largest school districts in the nation, provides educational services to over 87,000 children across the city. Schools within APS include both public and charter entities. Numerous accredited private preparatory schools also serve Albuquerque students. These include various pre-high school religious (Christian, Jewish, Islamic) affiliates and Montessori schools, as well as Menaul School, Albuquerque Academy, St. Pius X High School, Sandia Preparatory School, the Bosque School, Evangel Christian Academy, Hope Christian School, Hope Connection School, Temple Baptist Academy, and Victory Christian. Accredited private schools serving students with special education needs in Albuquerque include: Desert Hills, Pathways Academy, and Presbyterian Ear Institute Oral School. The New Mexico School for the Deaf runs a preschool for children with hearing impairments in Albuquerque.
Infrastructure.
Transportation.
Main highways.
Some of the main highways in the metro area include:
The interchange between I-40 and I-25 is known as the "Big I".:248 Originally built in 1966, it was rebuilt in 2002. The Big I is the only 5 stack interchange in the state of New Mexico.
There are other major roads in Albuquerque too. These include San Mateo, Osuna, San Antonio, Academy, Carlisle, Lead, Coal, Rio Bravo, University, Lomas, Broadway/Edith, Montgomery/Montano, Comanche, Rio Grande, Unser, 98th Street, Eubank, Juan Tabo, Louisiana, Wyoming, San Pedro, Gibson, Jefferson, Candelaria, and Menaul. The minor ones include Bridge, Corrales, Ellison/McMahon, Paradise Blvd, Golf Course Rd, Avenida Cesar Chavez, Isleta, Sunport, Dennis Chavez, Indian School, and Constitution.
Bridges.
There are six road bridges that cross the Rio Grande and serve the municipality on at least one end if not both. The eastern approaches of the northernmost three all pass through adjacent unincorporated areas, the Village of Los Ranchos de Albuquerque, or the North Valley. In downstream order they are:
Two more bridges serve urbanized areas contiguous to the city's perforated southern boundary.
Rail.
The state owns most of the city's rail infrastructure which is used by a commuter rail system, long distance passenger trains, and the freight trains of the BNSF Railway.
Freight Service.
BNSF Railway operates a small yard operation out of Abajo yard, located just south of the Cesar E. Chavez Ave. overpass and the New Mexico Rail Runner Express yards. Most freight traffic through the Central New Mexico region is processed via a much larger hub in nearby Belen, New Mexico.
Intercity rail.
Amtrak's Southwest Chief, which travels between Chicago and Los Angeles, serves the Albuquerque area daily with one stop in each direction at the Alvarado Transportation Center in downtown.
Commuter rail.
The New Mexico Rail Runner Express, a commuter rail line, began service between Sandoval County and Albuquerque in July 2006 using an existing BNSF right-of-way which was purchased by New Mexico in 2005. Service expanded to Valencia County in December 2006 and to Santa Fe on December 17, 2008. Rail Runner now connects Santa Fe, Sandoval, Bernalillo, and Valencia Counties with thirteen station stops, including three stops within Albuquerque.
The trains connect Albuquerque to downtown Santa Fe with eight roundtrips per weekday. The section of the line running south to Belen is served less frequently.
Local mass transit.
Albuquerque was one of two cities in New Mexico to have had electric street railways. Albuquerque's horse-drawn streetcar lines were electrified during the first few years of the 20th century. The Albuquerque Traction Company assumed operation of the system in 1905. The system grew to its maximum length of 6 mi during the next ten years by connecting destinations such as Old Town to the west and the University of New Mexico to the east with the town's urban center near the former Atchison, Topeka & Santa Fe Railway depot. The Albuquerque Traction Company failed financially in 1915 and the vaguely named City Electric Company was formed. Despite traffic booms during the first world war, and unaided by lawsuits attempting to force the streetcar company to pay for paving, that system also failed later in 1927, leaving the streetcar's "motorettes" unemployed.:177–181
Today, Alvarado Station provides convenient access to other parts of the city via the city bus system, ABQ RIDE. ABQ RIDE operates a variety of bus routes, including the Rapid Ride express bus service.
In 2006 the City of Albuquerque under the mayorship of Martin Chavez had planned and attempted to "fast track" the development of a "Modern Streetcar" project. Funding for the US$270 million system was not resolved as many citizens vocally opposed the project. The city and its transit department maintain a policy commitment to the streetcar project. The project would run mostly in the southeast quadrant on Central Avenue and Yale Boulevard.
s of 2011[ [update]], the city is working on a study to develop a bus rapid transit system through the Central Ave. corridor. This corridor currently carries 44% of all bus riders in the ABQ Ride system, making it a natural starting point for enhanced service.
Bicycle transit.
Albuquerque has a well-developed bicycle network. In and around the City there are trails, bike routes, and paths that provide the residents and visitors with alternatives to motorized travel. In 2009, the city was reviewed as having a major up and coming bike scene in North America. The same year, the City of Albuquerque opened its first Bicycle Boulevard on Silver Avenue. There are plans for more investment in bikes and bike transit by the city, including bicycle lending programs, in the coming years.
Walkability.
A 2011 study by Walk Score ranked Albuquerque 28th most walkable of the fifty largest U.S. cities.
Airports.
Albuquerque is served by two airports, the larger of which is Albuquerque International Sunport. It is located 3 miles (5 km) southeast of the central business district of Albuquerque. The Albuquerque International Sunport served 5,888,811 passengers in 2009. Double Eagle II Airport is the other airport. It is primarily used as an air ambulance, corporate flight, military flight, training flight, charter flight, and private flight facility.
Utilities.
Energy.
PNM Resources, New Mexico's largest electricity provider, is based in Albuquerque. They serve about 487,000 electricity customers statewide. New Mexico Gas Company provides natural gas services to more than 500,000 customers in the state, including the Albuquerque metro area.
Sanitation.
The Albuquerque Bernalillo County Water Utility Authority is responsible for the delivery of drinking water and the treatment of wastewater.
South Side Water Reclamation Plant.
Healthcare.
Albuquerque is the medical hub of New Mexico, hosting numerous state-of-the-art medical centers. Some of the city's top hospitals include the VA Medical Center, Presbyterian Hospital, Heart Hospital of New Mexico, and Lovelace Women's Hospital. The University of New Mexico Hospital is the primary teaching hospital for the state's only medical school and provides the state's only residency training programs, children's hospital, burn center and level I pediatric and adult trauma centers. The Univeristy of New Mexico Hospital is also the home of a certified advanced primary stroke center as well as the largest collection of adult and pediatric specialty and subspecialty programs in the state.
Media.
The city is served by one major newspaper, the "Albuquerque Journal", and several smaller daily and weekly papers, including the alternative "Weekly Alibi". Albuquerque is also home to numerous radio and television stations that serve the metropolitan and outlying rural areas.
Pop culture.
In film.
Many Bugs Bunny cartoon shorts feature Bugs traveling around the world by burrowing underground; he often gets lost while traveling and remarks, while consulting a map, "Should have made a left toin at Albukoykee". (Bugs first uses that line in "Herr Meets Hare" (1945).)
Some parts of the 1999 movie, "Pirates of the Silicon Valley" was shot in Albuquerque.
Albuquerque has been featured in Hollywood movies such as "Little Miss Sunshine" (2006), "Sunshine Cleaning" (2008), and "Brothers" (2009).
In 2013, Albuquerque was listed on "MovieMaker" magazine’s annual list of Top 10 Cities to be a Movie Maker.
All three of Disney's "High School Musical" movies are set in Albuquerque, at the fictional Albuquerque East High School (Wildcats) whose athletic (and academic) rivals are the fictional Albuquerque West High School (Knights).
Marvel Studios' film "The Avengers" (2012) was mostly (>75%) filmed at the Albuquerque Studios.
"A Million Ways to Die in the West" (2014), directed by Seth MacFarlane, was filmed in various areas in and around Albuquerque.
In music.
Musicians who have lived in Albuquerque include Glen Campbell, Bo Diddley, Demi Lovato, Eric McFadden, Rahim Al-Haj, and Bernadette Seacrest.
Music groups that have been based in Albuquerque include A Hawk and A Hacksaw, Beirut, The Eyeliners, Hazeldine, Leiahdorus, Scared of Chaka, and The Shins.
The song "Albuquerque" by Weird Al Yankovic tells the story of a man moving to the city, and his absurd misadventures while living there.
Neil Young's song "Albuquerque" can be found on the album "Tonight's the Night".
"Point me in the direction of Albuquerque" from "The Partridge Family Album".
In television.
Albuquerque is the setting for the television shows "In Plain Sight" and "Breaking Bad", with the latter significantly boosting tourism in the area.
The 2015 TV series "Better Call Saul", a "Breaking Bad" spin-off, also takes place in Albuquerque.
Ethel Mertz (played by Vivian Vance), from the 1950s sitcom "I Love Lucy", often refers to Albuquerque as her hometown.
Sister cities.
Albuquerque has ten sister cities, as designated by Sister Cities International:

</doc>
<doc id="51282" url="http://en.wikipedia.org/wiki?curid=51282" title="Maluku Islands">
Maluku Islands

The Maluku Islands or the Moluccas () are an archipelago within Indonesia. Tectonically they are located on the Halmahera Plate within the Molucca Sea Collision Zone. Geographically they are located east of Sulawesi, west of New Guinea, and north and east of Timor. The islands were also historically known as the "Spice Islands" by the Chinese and Europeans, but this term has also been applied to other islands outside Indonesia.
They have been known as the Spice Islands due to the nutmeg, mace and cloves that were originally found only there, and the presence of these sparked colonial interest from Europe in the 16th century.
Though originally Melanesian, many island populations, especially in the Banda Islands, were killed off in the 17th century during the Spice Wars. A second influx of Austronesian immigrants began in the early twentieth century under the Dutch and continues in the Indonesian era.
The Maluku Islands formed a single province since Indonesian independence until 1999, when it was split into two provinces. A new province, North Maluku, incorporates the area between Morotai and Sula, with the arc of islands from Buru and Seram to Wetar remaining within the existing Maluku Province. North Maluku is predominantly Muslim and its capital is Sofifi on Halmahera island. Maluku province has a larger Christian population and its capital is Ambon.
Between 1999 and 2002, conflict between Muslims and Christians killed thousands and displaced half a million people.
Etymology.
The name "Maluku" is thought to have been derived from the Arab trader's term for the region, "Jazirat al-Muluk" ("the island of the kings").
Administrative divisions.
The Maluku Islands were a single province from Indonesian independence until 1999 when they were split into North Maluku and Maluku.
North Maluku province includes Ternate (the former site of the provincial capital), Tidore, Bacan, Halmahera (the largest of the Maluku Islands) Morotai, the Obi Islands, and the Sula Islands. The residual Maluku province includes Ambon (the site of the provincial capital) and the other Lease Islands; the much larger islands of Seram and Buru; the smaller islands lying south and east of Seram—the Banda Islands, Gorong Islands, Watubela Islands, Kai Islands and Aru Islands; and in the far south the Babar Islands, Damar Islands, Romang, Kisar, the Leti Islands, Tanimbar Islands, and Wetar. 
Demographics.
Maluku's population is about 2 million, less than 1% of Indonesia's population.
Over 130 languages were once spoken across the islands; however many have now mixed to form local pidgin dialects of Ternatean and Ambonese, the lingua franca of northern and southern Maluku respectively.
A long history of trade and seafaring has resulted in a high degree of mixed ancestry in Malukans. Austronesian peoples added to the native Melanesian population around 2000 BCE. Melanesian features are strongest in the islands of Kei and Aru and amongst the interior people of Seram and Buru islands. Later added to this Austronesian-Melanesian mix were Indian, Arab, Chinese, Portuguese and Dutch descent. More recent arrivals include Bugis trader settlers from Sulawesi and Javanese transmigrants.
History.
Early history.
The earliest archaeological evidence of human occupation of the region is about thirty-two thousand years old, but evidence of even older settlements in Australia may mean that Maluku had earlier visitors. Evidence of increasingly long-distance trading relationships and of more frequent occupation of many islands, begins about ten to fifteen thousand years later. Onyx beads and segments of silver plate used as currency on the Indian subcontinent around 200BC have been unearthed on some of the islands. In addition, local dialects employ derivations of the Malay word then in use for 'silver', in contrast to the term used in wider Melanesian society, which has etymological roots in Chinese, a consequence of the regional trade with China that was developed in the 6th and 7th centuries. 
Maluku was a cosmopolitan society where spice traders from across the region took residence in settlements, or in nearby enclaves, including Arab and Chinese traders who visited or lived in the region. Social organization was usually local, and relatively flat—a general populace guided by a council of elders or rich men, or Orang kaya which is Indonesian word can be translated as rich man.
Arabic merchants began to arrive in the 14th century, bringing Islam. Peaceful conversion to Islam occurred in many islands, especially in the centres of trade, while aboriginal animism persisted in the hinterlands and more isolated islands. Archaeological evidence here relies largely on the occurrence of pigs' teeth, as evidence of pork eating or abstinence therefrom.
Portuguese.
The most significant lasting effects of the Portuguese presence was the disruption and reorganization of the Southeast Asian trade, and in eastern Indonesia—including Maluku—the introduction of Christianity. The Portuguese had conquered the city state of Malacca in the early 16th century and their influence was most strongly felt in Maluku and other parts of eastern Indonesia. After the Portuguese annexed Malacca in August 1511, one Portuguese diary noted 'it is thirty years since they became Moors'- giving a sense of the competition then taking place between Islamic and European influences in the region.
Afonso de Albuquerque learned of the route to the Banda Islands and other 'Spice Islands', and sent an exploratory expedition of three vessels under the command of António de Abreu, Simão Afonso Bisigudo and Francisco Serrão. On the return trip, Francisco Serrão was shipwrecked at Hitu island (northern Ambon) in 1512. There he established ties with the local ruler who was impressed with his martial skills. The rulers of the competing island states of Ternate and Tidore also sought Portuguese assistance and the newcomers were welcomed in the area as buyers of supplies and spices during a lull in the regional trade due to the temporary disruption of Javanese and Malay sailings to the area following the 1511 conflict in Malacca. The spice trade soon revived but the Portuguese would not be able to fully monopolize nor disrupt this trade.
Allying himself with Ternate's ruler, Serrão constructed a fortress on that tiny island and served as the head of a mercenary band of Portuguese seamen under the service of one of the two local feuding sultans who controlled most of the spice trade. Such an outpost far from Europe generally only attracted the most desperate and avaricious, and as such the feeble attempts at Christianisation only strained relations with Ternate's Muslim ruler. Serrão urged Ferdinand Magellan to join him in Maluku, and sent the explorer information about the Spice Islands. Both Serrão and Magellan, however, perished before they could meet one another.
The Portuguese first landed in Ambon in 1513, but it only became the new centre for their activities in Maluku following the expulsion from Ternate. European power in the region was weak and Ternate became an expanding, fiercely Islamic and anti-European state under the rule of Sultan Baab Ullah (r. 1570–1583) and his son Sultan Said.
Following Portuguese missionary work, there have been large Christian communities in eastern Indonesia through to contemporary times, which has contributed to a sense of shared interest with Europeans, particularly among the Ambonese. By the 1560s there were 10,000 Catholics in the area, mostly on Ambon, and by the 1590s there were 50,000 to 60,000, although most of the region surrounding Ambon remained Muslim.
Dutch.
The Dutch arrived in 1599 and noted the native discontent with Portuguese attempts to monopolise their traditional trade. 
The Dutch East India Company was a mercantile corporation with three obstacles in its way: the Portuguese, the aboriginal populations, and the English. In time the Dutch would overcome all three and achieve almost complete control of the islands down to modern times, leaving smuggling as the only native alternative to the Dutch monopoly. 
After Indonesian independence.
With the declaration of a single republic of Indonesia in 1950 to replace the federal state, a Republic of South Maluku (Republik Maluku Selatan, RMS) was declared and attempted to secede. The RMS was centred around Seram, Ambon, and Buru and led by Chris Soumokil (former Supreme Prosecutor of the Eastern Indonesia state) and supported by the Moluccan members of the Netherlands special troops. This movement was defeated by the Indonesian army and by special agreement with the Netherlands the troops were transferred to the Netherlands. The commencement of Indonesian transmigration of (mainly Javanese) populations to the outer islands (including Maluku) during the 1960s is thought to have aggravated independence and issues of religious / ethnic politics. There has been occasional ethnic and nationalist violence on the islands.
Maluku is one of the first provinces of Indonesia, proclaimed in 1945 until 1999, when the Maluku Utara and Halmahera Tengah Regencies were split off as a separate province of North Maluku. Its capital is Ternate, on a small island to the west of the large island of Halmahera. The capital of the remaining part of Maluku province remains at Ambon.
1999–2003 inter-communal conflict.
Religious conflict erupted across the islands in January 1999. The subsequent 18 months were characterized by fighting between largely local groups of Muslims and Christians, the destruction of thousands of houses, the displacement of approximately 500,000 people, the loss of thousands of lives, and the segregation of Muslims and Christians.
Geology and geography.
The Maluku Islands have a total area of 850,000  km2, 90% of which is sea. There are an estimated 1027 islands. The largest two islands, Halmahera and Seram are sparsely populated, while the most developed, Ambon and Ternate are small.
The majority of the islands are forested and mountainous. The Tanimbar Islands are dry and hilly, while the Aru Islands are flat and swampy. Mount Binaya (3027 m) on Seram is the highest mountain. A number of islands, such as Ternate (1721 m) and the TNS islands, are volcanoes emerging from the sea with villages sited around their coasts. There have been over 70 serious volcanic eruptions in the last 500 years and earthquakes are common.
The geology of the Maluku Islands share much similar history, characteristics and processes with the neighbouring Nusa Tenggara region. There is a long history of geological study of these regions since Indonesian colonial times; however, the geological formation and progression is not fully understood, and theories of the island's geological evolution have changed extensively in recent decades. The Maluku Islands comprise some of the most geologically complex and active regions in the world, resulting from its position at the meeting point of four geological plates and two continental blocks.
Biota and environment.
Biogeographically, all of the islands apart from the Aru group lie in Wallacea, the region between the Sunda Shelf (part of the Asia block), and the Arafura Shelf (part of the Australian block). More specifically, they lie between Weber's Line and Lydekker's Line, and thus have a fauna that is rather more Australasian than Asian. Malukan biodiversity and its distribution are affected by various tectonic activities; most of the islands are geologically young, being from 1 million to 15 million years old, and have never been attached to the larger landmasses. The Maluku islands differ from other areas in Indonesia; they contain some of the country's smallest islands, coral island reefs scattered through some of the deepest seas in the world, and no large islands such as Java or Sumatra. Flora and fauna immigration between islands is thus restricted, leading to a high rate of endemic biota evolving.
The ecology of the Maluku Islands has fascinated naturalists for centuries; Alfred Wallace's book, "The Malay Archipelago" was the first significant study of the area's natural history, and remains an important resource for studying Indonesian biodiversity. Maluku is the subject of two major historical works of natural history by Georg Eberhard Rumphius: the "Herbarium Amboinense" and the "Amboinsche Rariteitkamer".
Rainforest covered most of northern and central Maluku, which, on the smaller islands has been replaced by plantations, including the region's endemic cloves and nutmeg. The Tanimbar Islands and other southeastern islands are arid and sparsely vegetated, much like nearby Timor. In 1997 the Manusela National Park, and in 2004 the Aketajawe-Lolobata National Park have been established, for the protection of endangered species.
Nocturnal marsupials, such as cuscus and bandicoots, make up the majority of the mammal species, and introduced mammals include Malayan civets and wild pigs. Bird species include approximately 100 endemics with the greatest variety on the large islands of Halmahera and Seram. North Maluku has two species of endemic birds of paradise. Uniquely among the Maluku Islands, the Aru Islands have a purely Papuan fauna including kangaroos, cassowaries, and birds of paradise.
While many ecological problems affect both small islands and large landmasses, small islands suffer their particular problems. Development pressures on small islands are increasing, although their effects are not always anticipated. Although Indonesia is richly endowed with natural resources, the resources of the small islands of Maluku are limited and specialised; furthermore, human resources in particular are limited.
General observations about small islands that can be applied to the Maluku Islands include:
Climate.
Central and southern Maluku Islands experience the dry monsoon between October to March and the wet monsoon from May to August, which is the reverse of the rest of Indonesia. The dry monsoon's average maximum temperature is 30 °C while the wet's average maximum is 23 °C. Northern Maluku has its wet monsoon from December to March in line with the rest of Indonesia. Each island group have their own climatic variations, and the larger islands tend to have drier coastal lowlands and their mountainous hinterlands are wetter.
Economy.
Cloves and nutmeg are still cultivated, as are cocoa, coffee and fruit. Fishing is a big industry across the islands but particularly around Halmahera and Bacan. The Aru Islands produce pearls, and Seram exports lobsters. Logging is a significant industry on the larger islands with Seram producing ironwood and teak and ebony are produced on Buru.

</doc>
<doc id="51283" url="http://en.wikipedia.org/wiki?curid=51283" title="Maluku">
Maluku

Maluku may refer to:

</doc>
<doc id="51287" url="http://en.wikipedia.org/wiki?curid=51287" title="Spice Islands">
Spice Islands

Spice Islands may refer to:

</doc>
<doc id="51288" url="http://en.wikipedia.org/wiki?curid=51288" title="Translation memory">
Translation memory

A translation memory (TM) is a database that stores "segments", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators. The translation memory stores the source text and its corresponding translation in language pairs called “translation units”. Individual words are handled by terminology bases and are not within the domain of TM.
Software programs that use translation memories are sometimes known as translation memory managers (TMM).
Translation memories are typically used in conjunction with a dedicated computer assisted translation (CAT) tool, word processing program, terminology management systems, multilingual dictionary, or even raw machine translation output.
Research indicates that many companies producing multilingual documentation are using translation memory systems. In a survey of language professionals in 2006, 82.5% out of 874 replies confirmed the use of a TM. Usage of TM correlated with text type characterised by technical terms and simple sentence structure (technical, to a lesser degree marketing and financial), computing skills, and repetitiveness of content.
Using translation memories.
The program breaks the source text (the text to be translated) into segments, looks for matches between segments and the source half of previously translated source-target pairs stored in a translation memory, and presents such matching pairs as translation candidates. The translator can accept a candidate, replace it with a fresh translation, or modify it to match the source. In the last two cases, the new or modified translation goes into the database.
Some translation memories systems search for 100% matches only, that is to say that they can only retrieve segments of text that match entries in the database exactly, while others employ fuzzy matching algorithms to retrieve similar segments, which are presented to the translator with differences flagged. It is important to note that typical translation memory systems only search for text in the source segment.
The flexibility and robustness of the matching algorithm largely determine the performance of the translation memory, although for some applications the recall rate of exact matches can be high enough to justify the 100%-match approach.
Segments where no match is found will have to be translated by the translator manually. These newly translated segments are stored in the database where they can be used for future translations as well as repetitions of that segment in the current text.
Translation memories work best on texts which are highly repetitive, such as technical manuals. They are also helpful for translating incremental changes in a previously translated document, corresponding, for example, to minor changes in a new version of a user manual. Traditionally, translation memories have not been considered appropriate for literary or creative texts, for the simple reason that there is so little repetition in the language used. However, others find them of value even for non-repetitive texts, because the database resources created have value for concordance searches to determine appropriate usage of terms, for quality assurance (no empty segments), and the simplification of the review process (source and target segment are always displayed together while translators have to work with two documents in a traditional review environment).
If a translation memory system is used consistently on appropriate texts over a period of time, it can save translators considerable work.
Main benefits.
Translation memory managers are most suitable for translating technical documentation and documents containing specialized vocabularies. Their benefits include:
Main obstacles.
The main problems hindering wider use of translation memory managers include:
Functions of a translation memory.
The following is a summary of the main functions of a Translation Memory.
Off-line functions.
Import.
This function is used to transfer a text and its translation from a text file to the TM. Import can be done from a "raw format", in which an external source text is available for importing into a TM along with its translation. Sometimes the texts have to be reprocessed by the user. There is another format that can be used to import: the "native format". This format is the one that uses the TM to save translation memories in a file.
Analysis.
The process of analysis involves the following steps:
Export.
Export transfers the text from the TM into an external text file. Import and export should be inverses.
Online functions.
When translating, one of the main purposes of the TM is to retrieve the most useful matches in the memory so that the translator can choose the best one. The TM must show both the source and target text pointing out the identities and differences.
Retrieval.
Several different types of matches can be retrieved from a TM.
Updating.
A TM is updated with a new translation when it has been accepted by the translator. As always in updating a database, there is the question what to do with the previous contents of the database. A TM can be modified by changing or deleting entries in the TM. Some systems allow translators to save multiple translations of the same source segment.
Automatic translation.
Translation memory tools often provide automatic retrieval and substitution.
Networking.
Networking enables a group of translators to translate a text together faster than if each was working in isolation, because sentences and phrases translated by one translator are available to the others. Moreover, if translation memories are shared before the final translation, there is an opportunity for mistakes by one translator to be corrected by other team members.
Text memory.
"Text memory" is the basis of the proposed Lisa OSCAR xml:tm standard. Text memory comprises author memory and translation memory.
Translation memory.
The unique identifiers are remembered during translation so that the target language document is 'exactly' aligned at the text unit level. If the source document is subsequently modified, then those text units that have not changed can be directly transferred to the new target version of the document without the need for any translator interaction. This is the concept of 'exact' or 'perfect' matching to the translation memory. xml:tm can also provide mechanisms for in-document leveraged and fuzzy matching.
History of translation memories.
 1970s is the infancy stage for TMS in which scholars carried on a preliminary round of exploratory discussions. The original idea for TMS is often attributed to Martin Kay's "Proper Place" paper, but the details of it are not fully given. In this paper, it has shown the basic concept of the storing system:"The translator might start by issuing a command causing the system to display anything in the store that might be relevant to ... Before going on, he can examine past and future fragments of text that contain similar material". This oberservation from Kay was actually influenced by the suggestion of Peter Arthern that translators can use similar, already translated documents online. In his 1978 article he gave fully demonstration of what we call TMS today: Any new text would be typed into a word processing station, and as it was being typed, the system would check this text against the earlier texts stored in its memory, together with its translation into all the other official languages [of the European Community]. ... One advantage over machine translation proper would be that all the passages so retrieved would be grammatically correct. In effect, we should be operating an electronic ‘cut and stick’ process which would, according to my calculations, save at least 15 per cent of the time which translators now employ in effectively producing translations. 
 Another person named Alan Melby and his group at Brigham Young University were also claimed to be the founding fathers of TMS [citation needed]. The idea was incorporated from ALPS(Automated Language Processing Systems) Tools first developed by researcher from Brigham Young University, and at that time the idea of TMS was mixed with a tool call "Repetitions Processing" which only aimed to find matched strings. Only after a long time, did the concept of so-called Translation Memory come into being.
 The real exploratory stage of TMS would be 1980s. One of the first implementation of TMS appeared in Sadler and Vendelmans' Bilingual Knowledge Bank. A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a translation of the other, in which translation units are cross-coded between the corpora. The aim of Bilingual Knowledge Bank is to develop a corpus-based general-purpose knowledge source for applications in machine translation and computer- aided translation(Sadler&Vendelman, 1987). Another important step was made by Brian Harris with his "Bi-text". He has defined the bi-text as "a single text in two dimensions" (1988), the source and target texts related by the activity of the translator through translation units which made a similar echoes with Sadler's Bilingual Knowledge Bank. And in Harris's work he proposed something like TMS without using this name: a database of paired translations, searchable either by individual word, or by" whole translation unit", in the latter case the search being allowed to retrieve similar rather than identical units.
 TM technology only became commercially available on a wide scale in the late 1990s, so the efforts made by several engineers and translators. Of note is the first TM tool called Trados (SDL Trados nowadays). In this tool, when opening the source file and applying the translation memory so that any "100% matches" (identical matches) or "fuzzy matches" (similar, but not identical matches) within the text are instantly extracted and placed within the target file. Then, the "matches" suggested by the translation memory can be either accepted or overridden with new alternatives. If a translation unit is manually updated, then it is stored within the translation memory for future use as well as for repetition in the current text. In a similar way, all segments in the target file without a "match" would be translated manually and then automatically added to the translation memory. Another significant milestone of TMS is the projects at IBM's European Language Services (Denmark) in which massive translation memory were used to remove language barrier.
Support for new languages.
Translation memory tools from majority of the companies do not support many upcoming languages. Recently Asian countries like India also jumped in to language computing, and there is high demand for translation memories in such developing countries. As most of the CAT software companies are concentrating on legacy languages, nothing much is happening on Asian languages.
Recent trends.
One recent development is the concept of 'text memory' in contrast to translation memory. This is also the basis of the proposed LISA OSCAR standard. Text memory within xml:tm comprises 'author memory' and 'translation memory'. Author memory is used to keep track of changes during the authoring cycle. Translation memory uses the information from author memory to implement translation memory matching. Although primarily targeted at XML documents, xml:tm can be used on any document that can be converted to XLIFF format.
Second generation translation memories.
Much more powerful than first-generation TMs, they include a linguistic analysis engine, use chunk technology to break down segments into intelligent terminological groups, and automatically generate specific glossaries.
Translation memory and related standards.
TMX.
Translation Memory eXchange (TMX) is a standard that enables the interchange of translation memories between translation suppliers. TMX has been adopted by the translation community as the best way of importing and exporting translation memories. The current version is 1.4b - it allows for the recreation of the original source and target documents from the TMX data.
TBX.
TermBase eXchange. This LISA standard, which was revised and republished as ISO 30042, allows for the interchange of terminology data including detailed lexical information. The framework for TBX is provided by three ISO standards: ISO 12620, ISO 12200 and ISO 16642. ISO 12620 provides an inventory of well-defined “data categories” with standardized names that function as data element types or as predefined values. ISO 12200 (also known as MARTIF) provides the basis for the core structure of TBX. ISO 16642 (also known as Terminological Markup Framework) includes a structural metamodel for Terminology Markup Languages in general.
UTX.
Universal Terminology eXchange (UTX) format is a standard specifically designed to be used for user dictionaries of machine translation, but it can be used for general, human-readable glossaries. The purpose of UTX is to accelerate dictionary sharing and reuse by its extremely simple and practical specification.
SRX.
Segmentation Rules eXchange (SRX) is intended to enhance the TMX standard so that translation memory data that is exchanged between applications can be used more effectively. The ability to specify the segmentation rules that were used in the previous translation may increase the leveraging that can be achieved.
GMX.
GILT Metrics. GILT stands for (Globalization, Internationalization, Localization, and Translation). The GILT Metrics standard comprises three parts: GMX-V for volume metrics, GMX-C for complexity metrics and GMX-Q for quality metrics. The proposed GILT Metrics standard is tasked with quantifying the workload and quality requirements for any given GILT task.
OLIF.
Open Lexicon Interchange Format. OLIF is an open, XML-compliant standard for the exchange of terminological and lexical data. Although originally intended as a means for the exchange of lexical data between proprietary machine translation lexicons, it has evolved into a more general standard for terminology exchange.
XLIFF.
XML Localisation Interchange File Format (XLIFF) is intended to provide a single interchange file format that can be understood by any localization provider. XLIFF is the preferred way of exchanging data in XML format in the translation industry.
TransWS.
Translation Web Services. TransWS specifies the calls needed to use Web services for the submission and retrieval of files and messages relating to localization projects. It is intended as a detailed framework for the automation of much of the current localization process by the use of Web Services.
xml:tm.
The xml:tm (XML-based Text Memory) approach to translation memory is based on the concept of text memory which comprises author and translation memory. xml:tm has been donated to Lisa OSCAR by XML-INTL.
PO.
Gettext Portable Object format. Though often not regarded as a translation memory format, Gettext PO files are bilingual files that are also used in translation memory processes in the same way translation memories are used. Typically, a PO translation memory system will consist of various separate files in a directory tree structure. Common tools that work with PO files include the GNU Gettext Tools and the Translate Toolkit. Several tools and programs also exist that edit PO files as if they are mere source text files.
Desktop translation memory software.
Desktop translation memory tools are typically what individual translators use to complete translations. They are a specialized tool for translation in the same way that a word processor is a specialized tool for writing.
Centralized translation memory.
Centralized translation memory systems store TM on a central server. They work together with desktop TM and can increase TM match rates by 30-60% more than the TM leverage attained by desktop TM alone. They export prebuilt "translation kits" or "t-kits" to desktop TM tools. A t-kit contains content to be translated pre-segmented on the central server and a subset of the TM containing all applicable TM matches. Centralized TM is usually part of a globalization management system (GMS), which may also include a centralized terminology database (or glossary), a workflow engine, cost estimation, and other tools.

</doc>
<doc id="51294" url="http://en.wikipedia.org/wiki?curid=51294" title="Mohamed Farrah Aidid">
Mohamed Farrah Aidid

General Mohamed Farrah Hassan Aidid (Somali: "Maxamed Faarax Xasan Caydiid", Arabic: محمد فرح حسن عيديد‎) (December 15, 1934 – August 2, 1996) was a Somali military commander and faction leader. A former general and diplomat, he was the chairman of the United Somali Congress (USC) and later led the Somali National Alliance (SNA). Along with other armed opposition groups, they drove out President Mohamed Siad Barre's regime from Somalia's capital Mogadishu during the Somali Civil War that broke out in the early 1990s.
In 1992, Aidid challenged the presence of US-led United Nations troops in the nation. He was one of the main targets of the Unified Task Force. After eventually forcing UN forces to abandon the country in 1995, Aidid declared himself President of Somalia for a few months until his death the following year.
Early years.
Aidid was born in 1934 in Beledweyne, Italian Somaliland to a Habar Gidir family. He was educated in Rome and Moscow and served in the Italian colonial police force in the 1950s. He later joined the Somali National Army.
For advanced military training, Aidid studied at the Frunze Military Academy (Военная академия им. М. В. Фрунзе) in the former Soviet Union, an elite institution reserved for the most qualified officers of the Warsaw Pact armies and their allies.
In 1969, a few days after the assassination of Somalia's second president Abdirashid Ali Sharmarke, a military junta led by Major General Mohamed Siad Barre staged a bloodless coup d'état. Aidid was one of many officer serving at the central command of the Army at the time of the putsch. He quickly fell out of favour with the new regime's leaders and was subsequently detained. Aidid was eventually released from prison six years afterwards to take part in the 1977–78 war against Ethiopia over the disputed Ogaden region.
He later served in President Barre's advisor and as Somalia's ambassador to India, before finally being appointed intelligence chief.
United Somali Congress.
After fallout from the unsuccessful Ogaden campaign of the late 1970s, the Barre administration began arresting government and military officials under suspicion of participation in the abortive 1978 coup d'état. Most of the people who had allegedly helped plot the putsch were summarily executed. However, several officials managed to escape abroad and started to form the first of various dissident groups dedicated to ousting Barre's regime by force.
By the late 1980s, Barre's regime had become increasingly unpopular. The authorities became increasingly totalitarian, and resistance movements, encouraged by Ethiopia's communist Derg administration, sprang up across the country. This eventually led in 1991 to the outbreak of the civil war, the toppling of Barre's government, and the disbandment of the Somali National Army (SNA). Many of the opposition groups subsequently began competing for influence in the power vacuum that followed the ouster of Barre's regime. Armed factions led by United Somali Congress (USC) commanders General Aidid and Ali Mahdi Mohamed, in particular, clashed as each sought to exert authority over the capital. However, Aidid failed to attract many Somali leaders and intellectuals to the USC's cause, including fellow Frunze graduate General Abdullahi Ahmed Irro, who opted instead to remain politically neutral.
UN Security Council Resolution 733 and UN Security Council Resolution 746 led to the creation of UNOSOM I, the first stabilization mission in Somalia after the dissolution of the central government. United Nations Security Council Resolution 794 was unanimously passed on December 3, 1992, which approved a coalition of United Nations peacekeepers led by the United States. Forming the Unified Task Force (UNITAF), the alliance was tasked with assuring security until humanitarian efforts were transferred to the UN. Landing in 1993, the UN peacekeeping coalition started the two-year United Nations Operation in Somalia II (UNOSOM II) primarily in the south.
Presidency declaration.
Aidid subsequently declared himself President of Somalia in June 1995. However, his declaration received no international recognition, as his rival Ali Mahdi Muhammad had already been elected interim president at a conference in Djibouti and recognized as such by the international community.
Consequently, Aidid's faction continued its quest for hegemony in the south. In September 1995, militia forces loyal to him attacked the city of Baidoa, killing 10 local residents and capturing at least 20 foreign aid workers.
Killing Of Aidid.
On July 24, 1996, Aidid and his men clashed with the forces of former allies Ali Mahdi Muhammad and Osman Ali Atto. Atto was a former supporter and financier of Aidid, and of the same subclan. Atto is alleged to have master minded the defeat of Aidid. Aidid suffered a gunshot wound in the ensuing battle. He later died from a heart attack on August 1, either during or after surgery to treat his injuries.
Other officers allegedly targeted by Atto include General Talan. The U.S. Department of State asserted, in its Country Report for Somalia for the year 2000, that the killing of Yusuf Tallan, a former general under the Barre regime, was connected to Osman Ali Atto. The report did not provide specific corroboration for the assertion.
Family.
During the events leading up to the civil war, Aidid's wife Khadiga Gurhan sought asylum in Canada in 1989, moving the couple's four children with her. Local media shortly afterwards alleged that she had returned to Somalia for a five-month stay while still receiving welfare payments. Gurhan admitted in an interview to collecting welfare and having briefly traveled to Somalia in late 1991. However, it was later brought to light that she had been granted landed immigrant status in June 1991, thereby making her a legal resident of Canada. Additionally, Aidid's rival President Barre had been overthrown in January of that year. This altogether ensured that Gurhan's five-month trip would not have undermined her initial 1989 claim of refugee status. An official probe by Canadian immigration officials into the allegations also concluded that she had obtained her landing papers through normal legal processes.
Hussein Mohamed Farrah, son of General Aidid, emigrated to the United States when he was 17 years old. Staying 16 years in the country, he eventually became a naturalized citizen and later a United States Marine who served in Somalia. Two days after his father's death, the Somali National Alliance declared Hussein as the new President, although he too was not internationally recognized as such.

</doc>
<doc id="51298" url="http://en.wikipedia.org/wiki?curid=51298" title="Aaron Copland">
Aaron Copland

Aaron Copland (; November 14, 1900 – December 2, 1990) was an American composer, composition teacher, writer, and later in his career a conductor of his own and other American music. Instrumental in forging a distinctly American style of composition, in his later years he was often referred to as "the Dean of American Composers" and is best known to the public for the works he wrote in the 1930s and 1940s in a deliberately accessible style often referred to as "populist" and which the composer labeled his "vernacular" style. Works in this vein include the ballets "Appalachian Spring", "Billy the Kid" and "Rodeo", his "Fanfare for the Common Man" and Third Symphony. The open, slowly changing harmonies of many of his works are archetypical of what many people consider to be the sound of American music, evoking the vast American landscape and pioneer spirit. In addition to his ballets and orchestral works, he produced music in many other genres including chamber music, vocal works, opera and film scores.
After some initial studies with composer Rubin Goldmark, Copland traveled to Paris, where he studied at first with Isidor Philipp and Paul Vidal, then with noted pedagogue Nadia Boulanger. He studied three years with Boulanger, whose eclectic approach to music inspired his own broad taste in that area. Determined upon his return to the U.S. to make his way as a full-time composer, Copland gave lecture-recitals, wrote works on commission and did some teaching and writing. He found composing orchestral music in the "modernist" style he had adapted abroad a financially contradictory approach, particularly in light of the Great Depression. He shifted in the mid-1930s to a more accessible musical style which mirrored the German idea of "Gebrauchsmusik" ("music for use"), music that could serve utilitarian and artistic purposes. During the Depression years, he traveled extensively to Europe, Africa, and Mexico, formed an important friendship with Mexican composer Carlos Chávez and began composing his signature works.
During the late 1940s Copland felt a need to compose works of greater emotional substance than his utilitarian scores of the late 1930s and early 1940s. He was aware that Stravinsky, as well as many fellow composers, had begun to study Arnold Schoenberg's use of twelve-tone (serial) techniques. In his personal style, Copland began to make use of twelve-tone rows in several compositions. He incorporated serial techniques in some of his later works, including his Piano Quartet (1951), Piano Fantasy (1957), "Connotations" for orchestra (1961) and "Inscape" for orchestra (1967). From the 1960s onward, Copland's activities turned more from composing to conducting. He became a frequent guest conductor of orchestras in the U.S. and the UK and made a series of recordings of his music, primarily for Columbia Records.
Biography.
Early life.
Aaron Copland was born in Brooklyn into a Conservative Jewish family of Lithuanian origins, the last of five children, on November 14, 1900. While emigrating from Russia to the United States, Copland's father, Harris Morris Copland, Anglicized his surname ""Kaplan" to "Copland"" while living and working in Scotland for two to three years to pay for the boat fare to the US. Copland was however unaware until late in his life that the family name had been Kaplan, and his parents never told him this. Throughout his childhood, Copland and his family lived above his parents' Brooklyn shop, H.M. Copland's, at 628 Washington Avenue (which Aaron would later describe as "a kind of neighborhood Macy's"), on the corner of Dean Street and Washington Avenue, and most of the children helped out in the store. His father was a staunch Democrat. The family members were active in Congregation Baith Israel Anshei Emes, where Aaron celebrated his Bar Mitzvah. Not especially athletic, the sensitive young man became an avid reader and often read Horatio Alger stories on his front steps.
Copland's father had no musical interest at all, but his mother, Sarah Mittenthal Copland, sang and played the piano, and arranged for music lessons for her children. Of his siblings, oldest brother Ralph was the most advanced musically, proficient on the violin, while his sister Laurine had the strongest connection with Aaron, giving him his first piano lessons, promoting his musical education, and supporting him in his musical career. She attended the Metropolitan Opera School and was a frequent opera goer. She often brought home libretti for Aaron to study. Copland attended Boys' High School and in the summer went to various camps. Most of his early exposure to music was at Jewish weddings and ceremonies, and occasional family musicales.
At the age of eleven, Copland devised an opera scenario he called "Zenatello", which included seven bars of music, his first notated melody. From 1913 to 1917 he took music lessons with Leopold Wolfsohn, who taught him the standard classical fare. Copland's first public music performance was at a Wanamaker's recital.
By the age of 15, after attending a concert by composer-pianist Ignacy Jan Paderewski, Copland decided to become a composer. After attempts to further his music study from a correspondence course, Copland took formal lessons in harmony, theory, and composition from Rubin Goldmark, a noted teacher and composer of American music (who had given George Gershwin three lessons). Goldmark gave the young Copland a solid foundation, especially in the Germanic tradition, as he stated later: "This was a stroke of luck for me. I was spared the floundering that so many musicians have suffered through incompetent teaching." But Copland also commented that the maestro had "little sympathy for the advanced musical idioms of the day" and his "approved" composers ended with Richard Strauss.
Copland's graduation piece from his studies with Goldmark was a three-movement piano sonata in a Romantic style. But he had also composed more original and daring pieces which he did not share with his teacher. In addition to regularly attending the Metropolitan Opera and the New York Symphony, where he heard the standard classical repertory, Copland continued his musical development through an expanding circle of musical friends. After graduating from high school, Copland played in dance bands. Continuing his musical education, he received further piano lessons from Victor Wittgenstein, who found his student to be "quiet, shy, well-mannered, and gracious in accepting criticism." Copland's fascination with the Russian Revolution and its promise for freeing the lower classes drew a rebuke from his father and uncles. In spite of that, in his early adult life Copland would develop friendships with people with socialist and communist leanings.
Studying in Paris.
From 1917 to 1921, Copland composed juvenile works of short piano pieces and art songs. Copland's passion for the latest European music, plus glowing letters from his friend Aaron Schaffer, inspired him to go to Paris for further study. His father wanted him to go to college, but his mother's vote in the family conference allowed him to give Paris a try. On arriving in France, he studied at the Fontainebleau School of Music with noted pianist and pedagogue Isidor Philipp and with Paul Vidal. But finding Vidal too much like Goldmark, Copland switched to famed teacher Nadia Boulanger, then aged thirty-four. He had initial reservations: "No one to my knowledge had ever before thought of studying with a woman." She interviewed him, and recalled later: "One could tell his talent immediately."
Boulanger had as many as forty students at once and employed a formal regimen that Copland had to follow, too. Copland found her incisive mind much to his liking and stated: "This intellectual Amazon is not only professor at the Conservatoire, is not only familiar with all music from Bach to Stravinsky, but is prepared for anything worse in the way of dissonance. But make no mistake ... A more charming womanly woman never lived." Though he planned on only one year abroad, he studied with her for three years, finding her eclectic approach inspired his own broad musical taste.
Adding to the heady cultural atmosphere of the early 1920s in Paris was the presence of expatriate American writers Paul Bowles, Ernest Hemingway, Sinclair Lewis, Gertrude Stein, and Ezra Pound, as well as artists like Picasso, Chagall, and Modigliani. Also influential on the new music were the French intellectuals Marcel Proust, Paul Valéry, Sartre, and André Gide, the latter cited by Copland as being his personal favorite and most read. Travels to Italy, Austria, and Germany rounded out Copland's musical education. During his stay in Paris, Copland began writing musical critiques, the first on Gabriel Fauré, which helped spread his fame and stature in the music community. Instead of wallowing in self-pity and self-destruction like many of the expatriate members of the Lost Generation, Copland returned to America optimistic and enthusiastic about the future.
1925 to 1950.
Upon returning to the U.S., Copland was determined to make his way as a full-time composer. He rented a studio apartment on New York City's Upper West Side in the Empire Hotel, which kept him close to Carnegie Hall and other musical venues and publishers. He remained in that area for the next thirty years, later moving to Westchester County, New York. Copland lived frugally and survived financially with help from two $2,500 Guggenheim Fellowships—one in 1925 and one in 1926. Lecture-recitals, awards, appointments, and small commissions, plus some teaching, writing, and personal loans kept him afloat in the subsequent years through World War II. Also important were wealthy patrons who supported the arts community during the Depression, underwriting performances, publication, and promotion of musical events and composers.
Copland's compositions in the early 1920s reflected the prevailing "modernist" attitude among intellectuals: that they were a small vanguard leading the way for the masses, who would only come to appreciate their efforts over time. In this view, music and the other arts need be accessible to only a select cadre of the enlightened. Toward this end, Copland formed the Young Composer's Group, modeled after France's "Six", gathering together promising young composers, acting as their guiding spirit.
Soon after his return, Copland was introduced to the artistic circle of Alfred Stieglitz and met many of the leading artists of that time. Stieglitz's conviction that the American artist should reflect "the ideas of American Democracy" influenced Copland and a whole generation of artists and photographers, including Paul Strand, Edward Weston, Ansel Adams, Georgia O'Keeffe, and Walker Evans. Evans' photographs inspired portions of Copland's opera "The Tender Land".
In his quest to take up Stieglitz's challenge, Copland had few established American contemporaries to emulate apart from Carl Ruggles and the reclusive Charles Ives, although the 1920s were Golden Years for American popular music and jazz, with George Gershwin, Bessie Smith, and Louis Armstrong leading the way. Later, however, Copland joined up with his younger contemporaries and formed a group termed the "commando unit," which included Roger Sessions, Roy Harris, Virgil Thomson, and Walter Piston. They collaborated in joint concerts showcasing their work to new audiences.
Copland's relationship with the "commando unit" was one of both support and rivalry, and he played a key role in keeping them together. The five young American composers helped promote each other and their works but also had testy exchanges, inflamed by the assertion of the press that Copland was the "truly American" composer. Going beyond the five, Copland was generous with his time with nearly every American young composer he met during his life, later earning the title the "Dean of American Music."
Mounting troubles with the "Symphonic Ode" (1929) and "Short Symphony" (1933) caused him to rethink the paradigm of composing orchestral music for a select group, as it was a financially contradictory approach, particularly in the Depression. In many ways, this shift mirrored the German idea of Gebrauchsmusik ("music for use"), as composers sought to create music that could serve a utilitarian as well as artistic purpose. This approach encompassed two trends: first, music that students could easily learn, and second, music which would have wider appeal, such as incidental music for plays, movies, radio, etc. Copland undertook both goals, starting in the mid-1930s.
Perhaps motivated by the plight of children during the Depression, around 1935 Copland began to compose musical pieces for young audiences, in accordance with the first goal of American Gebrauchsmusik. These works included piano pieces ("The Young Pioneers") and an opera ("The Second Hurricane").
During the Depression years, Copland traveled extensively to Europe, Africa, and Mexico. He formed an important friendship with Mexican composer Carlos Chávez and would return often to Mexico for working vacations conducting engagements. During his initial visit to Mexico, Copland began composing the first of his signature works, "El Salón México", which he completed four years later in 1936. This and other incidental commissions fulfilled the second goal of American Gebrauchsmusik, creating music of wide appeal.
During this time, he composed (for radio broadcast) "Prairie Journal," one of his first pieces to convey the landscape of the American West. Branching out into theater, Copland also played an important role providing musical advice and inspiration to The Group Theater—Stella Adler's and Lee Strasberg's "method" acting school. The Group Theater followed Copland's musical agenda and focused on plays that illuminated the American experience. After Hitler and Mussolini's attacks on Spain in 1936, leftist parties had united in a Popular Front against Fascism. Many Group Theater members were influenced by Marxism and other progressive philosophies, and several had joined the Communist Party, including Elia Kazan and Clifford Odets. Copland also had contact later with other major American playwrights, including Thornton Wilder, William Inge, Arthur Miller, and Edward Albee, and considered projects with all of them. During the 1930s, Copland wrote incidental music for several plays, including Irwin Shaw's "Quiet City" (1939), considered one of his most personal and poignant scores.
In 1939, Copland completed his first two Hollywood film scores, for "Of Mice and Men" and "Our Town", and received sizable commissions. In the same year, he composed the radio score "John Henry", based on the folk ballad. But it wasn't until the worldwide market for classical recordings boomed after World War II that he achieved economic security. Even after securing a comfortable income, he continued to write, teach, lecture, and, eventually, conduct.
Demonstrating his broad range, Copland in the 1930s began composing music for ballet, including his highly successful "Billy the Kid" (1939), the second of four ballets he scored (after "Hear Ye! Hear Ye!" (1934)). In an interview with Vivian Perlis, Eugene Loring said of the ballet, "In our western states, there were still a few old-timers who remembered Billy. One came backstage in San Francisco to tell us that it was all fine, except that Billy really shot left-handed!" Copland's ballet music established him as an authentic composer of American music much as Stravinsky's ballet scores connected the composer with Russian music. Copland's timing was excellent; he helped fill a vacuum for the American choreographers who needed suitable music to score their own nationalistic dance repertory.
In keeping with the wartime period, Copland's "Piano Sonata" (1941) was a piece characterized as "grim, nervous, elegiac, with pervasive bell-like tolling of alarm and mourning." It was later adapted to "Day on Earth," a landmark American dance by Doris Humphrey.
Copland started to publish some of his lectures in the 1930s, "What to Listen for in Music" being one of the most notable of his writings. He also took a leading role in the American Composers Alliance, whose mission was "to regularize and collect all fees pertaining to performance of their copyrighted music" and "to stimulate interest in the performance of American music." Copland eventually moved over to rival ASCAP. Through royalties and with his great success from 1940 on, Copland amassed a multi-million dollar fortune by the time of his death.
The decade of the 1940s was arguably Copland's most productive, and it firmly established his worldwide fame. His two ballet scores for "Rodeo" (1942) and "Appalachian Spring" (1944) were huge successes. His pieces "Lincoln Portrait" and "Fanfare for the Common Man" have become patriotic standards (See Popular works, below). Also important was the "Third Symphony". Composed in a two-year period from 1944 to 1946, it became Copland's best-known symphony.
In 1945, Copland contributed to "Jubilee Variation", a work commissioned by the Cincinnati Symphony in which ten American composers collaborated, but the piece is seldom heard in the concert hall. Copland's "In the Beginning" (1947) is a choral work using the first chapter and the first seven verses of the second chapter of Genesis from the King James Version of the Bible and is a masterpiece of the choral repertory.
Copland's "Clarinet Concerto" (1948), scored for solo clarinet, strings, harp, and piano, was a commission piece for bandleader and clarinetist Benny Goodman and a complement to Copland's earlier jazz-influenced work, the "Piano Concerto" (1926). His "Four Piano Blues" is an introspective composition with a jazz influence.
Copland finished the 1940s with two film scores, one for William Wyler's 1949 film "The Heiress" and one for the film adaptation of John Steinbeck's novel "The Red Pony".
In 1949, he returned to Europe to find Pierre Boulez dominating the group of post-war avant-garde composers. He also met with proponents of twelve-tone technique, based on the works of Arnold Schoenberg, and found himself interested in adapting serial methods to his own musical voice.
1950s and 1960s.
In 1950, Copland received a U.S.-Italy Fulbright Commission scholarship to study in Rome, which he did the following year. Around this time, he also composed his "Piano Quartet", adopting Schoenberg's twelve-tone method of composition, and "Old American Songs" (1950), the first set of which was premiered by Peter Pears and Benjamin Britten, the second by William Warfield.
Because of the political climate of that era, "A Lincoln Portrait" was withdrawn from the 1953 inaugural concert for President Eisenhower. That same year, Copland was called before Congress, where he testified that he was never a communist.
Despite the difficulties that his suspected Communist sympathies posed, Copland nonetheless traveled extensively during the 1950s and early 1960s, observing the avant-garde styles of Europe while experiencing the new school of Soviet music. In addition, he was rather taken with the work of Toru Takemitsu while in Japan and began a correspondence with him that would last over the next decade. Copland wrote of the Japanese composer: "He has the 'pure gold' touch, he chooses his notes carefully and meaningfully." Copland also gained exposure to the latest musical trends in Poland and Scandinavia. In observing these new musical forms, Copland revised his text "The New Music" with comments on the styles that he encountered. In particular, while Copland explained the importance of the work of John Cage and others (in his chapter titled "The Music of Chance"), he found that these radical trends in music which appealed to those "who enjoy teetering on the edge of chaos" were less likely to gain the appreciation of a wider audience "who envisage art as a bulwark against the irrationality of man's nature." As he summarized: "I've spent most of my life trying to get the right note in the right place. Just throwing it open to chance seems to go against my natural instincts."
In 1954, Copland received a commission from Richard Rodgers and Oscar Hammerstein to create music for the opera "The Tender Land", based on James Agee's "Let Us Now Praise Famous Men". Copland had been wary of writing an opera, being especially aware of the pitfalls of that form, including weak libretti and demanding production values. Nevertheless, Copland decided to try his hand at "la forme fatale," especially as the 1950s were boom times for American playwrights, with Arthur Miller, Clifford Odets and Thornton Wilder doing some of their best work. Originally two acts, "The Tender Land" was later expanded to three. As Copland feared, critics found the libretto to be the opera's weakness, and he later stated: "I admit that if I have one regret it is that I never did write a 'grand opera'." In spite of its flaws, the opera has established itself as one of the few American operas in the standard repertory.
In 1957, 1958, and 1976, Copland was the Music Director of the Ojai Music Festival, a classical and contemporary music festival in Ojai, California.
Copland exerted a major influence on the compositional style of an entire generation of American composers, including his friend and protégé Leonard Bernstein. Bernstein was considered the finest conductor of Copland's works and cites Copland's "aesthetic, simplicity with originality" as being his strongest and most influential traits.
For the occasion of the Metropolitan Museum of Art Centennial, Copland composed "Ceremonial Fanfare For Brass Ensemble" to accompany the exhibition "Masterpieces Of Fifty Centuries." Leonard Bernstein, Walter Piston, William Schuman, and Virgil Thomson also composed pieces for the Museum's Centennial exhibitions.
Later life.
From the 1960s onward, Copland's activities turned more from composing to conducting. Though not enamored with the prospect, he found himself without new ideas for composition, saying: "It was exactly as if someone had simply turned off a faucet." Copland was a frequent guest conductor of orchestras in the U.S. and the UK. He made a series of recordings of his music, primarily for Columbia Records. In 1960, RCA Victor released Copland's recordings with the Boston Symphony Orchestra of the orchestral suites from "Appalachian Spring" and "The Tender Land"; these recordings were later reissued on CD, as were most of Copland's Columbia recordings (by Sony).
From 1960 to his death, he resided at Cortlandt Manor, New York. His home, known as Rock Hill, was added to the National Register of Historic Places in 2003. It was further designated a National Historic Landmark in 2008. Copland's health deteriorated through the 1980s, and he died of Alzheimer's disease and respiratory failure on December 2, 1990, in North Tarrytown, New York (now Sleepy Hollow). Much of his large estate was bequeathed to the creation of the Aaron Copland Fund for Composers, which bestows over $600,000 per year to performing groups.
Personal life.
Deciding not to follow the example of his father, a solid Democrat, Copland never enrolled as a member of any political party, but he espoused a general progressive view and had strong ties with numerous colleagues and friends in the Popular Front, including Odets. Copland supported the Communist Party USA ticket during the 1936 presidential election, at the height of his involvement with The Group Theater, and remained a committed opponent of militarism and the Cold War, which he regarded as having been instigated by the United States. He condemned it as "almost worse for art than the real thing". Throw the artist "into a mood of suspicion, ill-will, and dread that typifies the cold war attitude and he'll create nothing". In keeping with these attitudes, Copland was a strong supporter of the Presidential candidacy of Henry A. Wallace on the Progressive Party ticket. As a result, he was later investigated by the FBI during the Red scare of the 1950s and found himself blacklisted.
Copland was included on an FBI list of 151 artists thought to have Communist associations. Joseph McCarthy and Roy Cohn questioned Copland about his lecturing abroad and his affiliations with various organizations and events, neglecting completely Copland's works which made a virtue of American values. Copland made several denials on record of any serious involvement with a list of political/cultural organizations identified as subversive by the House of Un-American Activities (HUAC). Copland has also been on record saying he does not think music has political importance despite having composed some of the most iconic American art music of the 20th century.
Given the nature of the hearings, Copland was asked to prepare explanations for his seemingly large involvement in explicitly communist and communist leaning organizations. The danger Copland potentially presented was not in belonging to communist organizations, but in the possibility of spreading those ideas in the Latin American countries he was paid by the state to lecture in. The U.S, at the time, still had an interest with overseeing the continuation of democracy in Latin America.
Outraged by the accusations, many members of the musical community held up Copland's music as a banner of his patriotism. The investigations ceased in 1955 and were closed in 1975. Though taxing of his time, energy, and emotional state, the McCarthy probes did not seriously affect Copland's career and international artistic reputation. In any case, beginning in 1950, Copland, who had been appalled at Stalin's persecution of Shostakovich and other artists, began resigning from participation in leftist groups. He decried the lack of artistic freedom in the Soviet Union, and in his 1954 Norton lecture he asserted that loss of freedom under Soviet Communism deprived artists of "the immemorial right of the artist to be wrong." He began to vote Democratic, first for Stevenson and then for Kennedy.
Copland was an agnostic. However, Copland has had various encounters with organized religious thought, which have influenced some of his early compositions. Copland was once close with the Zionist movement during the Popular Front movement, when it was endorsed by the left. In relation to his compositions one of his earliest musical interests was with klezmer music. the music of his childhood synagogue would be one of the early influences of his fresh musical aesthetic.
Copland is documented as gay in author Howard Pollack's biography, "Aaron Copland: The Life and Work of an Uncommon Man". Like many of his contemporaries he guarded his privacy, especially in regard to his homosexuality, providing very few written details about his private life. However, he was one of the few composers of his stature to live openly and travel with his intimates, most of whom were talented, much younger men. Among Copland's love affairs, most of which lasted for only a few years yet became enduring friendships, were ones with photographer Victor Kraft (photographer), artist Alvin Ross, pianist Paul Moor, dancer Erik Johns, composer John Brodbin Kennedy, and painter Prentiss Taylor.
Victor Kraft would prove to be the one constant romantic relationship in Copland's life. Originally a student of music under Copland, Kraft gave up music in pursuit of a career in photography on Copland's urging. Kraft would leave and re-enter Copland's life, often bringing much stress with him: their relationship would fluctuate from contentedness to erratically confrontational on Kraft's part. Kraft fathered a child to whom Copland later provided financial security, through a bequest from his estate.
Composer.
Influences.
Copland's earliest musical inclinations as a teenager ran toward Chopin, Debussy, Verdi and the Russian composers. Some of his preferences might also have been formed by the anti-German feelings during World War I, as later he studied German music. Copland's curiosity about the latest music from Debussy and Scriabin was frustrated by the fact that the scores of "avant-garde" works were expensive at that time and hard to come by. So he borrowed these works from a music library and studied them intensely. Some of his earliest compositions were songs and piano pieces inspired by these European influences.
Copland's teacher and mentor Nadia Boulanger was his most important influence. In gratitude for the immense support and promotion on his behalf, he stated to her in 1950: "I shall count our meeting the most important of my musical life ... Whatever I have accomplished is intimately associated in my mind with those early years, and with what you have since been as inspiration and example." Of all her students, she listed Copland first. Copland especially admired Boulanger's total grasp of all classical music, and he was encouraged to experiment and develop a "clarity of conception and elegance in proportion." Following her model, he studied all periods of classical music and all forms—from madrigals to symphonies. This breadth of vision led Copland to compose music for numerous settings—orchestra, opera, solo piano, small ensemble, art song, ballet, theater and film. Boulanger particularly emphasized "la grande ligne" (the long line), "a sense of forward motion ... the feeling for inevitability, for the creating of an entire piece that could be thought of as a functioning entity."
In discovering Johann Sebastian Bach, Copland pointed out: "[Bach has an] inexhaustible wealth of musical riches, which no music lover can afford to ignore ... What strikes me most markedly about Bach's work is the marvelous rightness of it. It is the rightness not merely of a single individual, but a whole musical epoch." Copland stated that an ideal music might combine Mozart's "spontaneity and refinement" with Palestrina's "purity" and Bach's "profundity".
Copland was excited to be so close to the new post-Impressionistic French music of Ravel, Roussel, and Satie, as well as Les six, a group that included Milhaud, Poulenc, and Honegger. Webern, Berg, and Bartók also impressed him. Copland was "insatiable" in seeking out the newest European music, whether in concerts, score reading or heated debate. These "moderns" were discarding the old laws of composition and experimenting with new forms, harmonies and rhythms, and including the use of jazz and quarter-tone music. Serge Koussevitzky had just arrived in Paris and was adding to the ferment by conducting and promoting the new music of Russia and France. Later he would conduct many Copland premieres in New York. Among the first performances that Copland attended was Milhaud's "La création du monde", which caused riots in Paris. Milhaud was Copland's inspiration for some of his earlier "jazzy" works. He was also exposed to Schoenberg and admired his earlier atonal pieces, thinking Schoenberg's "Pierrot Lunaire" a landmark work comparable to Stravinsky's "The Rite of Spring." Copland even tried out Schoenberg's innovative twelve-tone system and adapted it to his style.
Above all others, Copland named Igor Stravinsky as his "hero" and his favorite 20th-century composer. Stravinsky was in many ways his premiere model. Stravinsky's rhythm and vitality is apparent in many of his works. Copland especially admired Stravinsky's "jagged and uncouth rhythmic effects," "bold use of dissonance," and "hard, dry, crackling sonority." In a 1950 radio interview, Copland is quoted saying that there is a "freshness of atmosphere; a freshness of personality--which looks very attractive to American composers. Europeans are not seeking freshness of music as much as American composers. The reason being that through their long tradition in music--they already know in advance what they are supposed to write." As a publicly identified composer of iconic American music, Copland's claim that American composers are still in search for a certain freshness to composition—found in Stravinsky—show they continuing uncertainty of the American art music scene in the 1950s. Despite using folk themes as a tool for signifying Americanness, Copland continued to find "freshness" in Stravinsky's work—especially in his usage of rhythm. Copland was similarly but not quite as strongly impressed by Sergei Prokofiev's "fresh, clean-cut, articulate style."
Another inspiration for much of Copland's music was jazz. Although familiar with jazz back in America—having listened to it and also played it in bands—he fully realized its potential while traveling in Austria: "The impression of jazz one receives in a foreign country is totally unlike the impression of such music heard in one's own country ... when I heard jazz played in Vienna, it was like hearing it for the first time." He also found that the distance from his native country helped him see the United States more clearly. Beginning in 1923, he employed "jazzy elements" in his classical music, but by the late 1930s, he moved on to Latin and American folk tunes in his more successful pieces. His earlier works especially demonstrate the influence of jazz rhythmic, timbral and harmonic practices. That influence is apparent in a few later works, such as the Clarinet Concerto commissioned by Benny Goodman. During the late 1920s and 1930s, Copland sought out jazz at the Cotton Club and heard Duke Ellington, Benny Carter and Bix Beiderbecke, among others. Of Duke Ellington among other jazz composers, Copland said he was "the master of them all."
Although Copland was intrigued by the idea of a "jazz concerto" and "symphonic jazz," his Concerto for Piano and Orchestra did not succeed in that form as had those of Maurice Ravel and George Gershwin, who was praised by such eminent musical exiles as Schoenberg, Bartók, and Stravinsky (Gershwin had recently died at 38 and so was no longer a potential rival). Copland would go on to write extensively and deliver the Norton lectures about jazz in America, especially the big band sound (1930s) and cool West Coast jazz (1950s). Yet, enthusiastic as he was about jazz throughout his life, Copland also recognized its limitations: With the [Piano] Concerto I felt I had done all I could with the idiom, considering its limited emotional scope. True, it was an easy way to be American in musical terms, but all American music could not possibly be confined to two dominant jazz moods – the blues and the snappy number.
Jazz played an important role for some of Copland's compositions. What constituted as Jazz was contested by many musicians and scholars. Copland believed that the essence of Jazz was rooted in rhythm. Copland identified any sort of syncopation as metrical phenomenon. He called ragtime Jazz' closest ancestor, while also citing the foxtrot rhythm—and later the usage of poly-rhythms as the basis for modern jazz. By the 1950s,Copland had come to see the possibilities of Jazz less and less in his compositions, though the idea of syncopated rhythm would continue to feature prominently in many of his works.
Although his early focus of jazz gave way to other influences, Copland continued to make use of jazz in more subtle ways in later works. But it was the synthesizing of all his influences and inclinations which create the "Americanism" of his music. Copland pointed out in summarizing the American character of his music, "the optimistic tone", "his love of rather large canvases", "a certain directness in expression of sentiment", and "a certain songfulness". As he advanced in his career (by 1941), he said of himself and advised other composers: I no longer feel the need of seeking out conscious Americanisms [folksongs and folk rhythms]. Because we live here and work here, we can be certain that when our music is mature it will also be American in quality. In contradiction to this statement, however, he continued to look for and employ folk material for several more years.
Copland's work from the late 1940s onward included experimentation with Schönberg's twelve-tone system, resulting in two major works, the "Piano Quartet" (1950) and the "Piano Fantasy" (1957).
Early work.
Copland's earliest compositions before leaving for Paris were short works for piano and some art songs, inspired mostly by Liszt and Debussy. He experimented with ambiguous beginnings and endings, rapid key changes, and the frequent use of tritones. His first published work was "The Cat and the Mouse" (1920), a piano solo piece based on a fable by Jean de la Fontaine. In "Three Moods" (1921), Copland's final movement is entitled "Jazzy", which he noted "is based on two jazz melodies and ought to make the old professors sit up and take notice".
One of Copland's first significant works upon returning from his studies in Paris was the necromantic ballet "Grohg". This ballet, suggested to Copland by the film "Nosferatu", a free adaptation of the Dracula tale, provided the source material for his later "Dance Symphony". Originally intended as an orchestral exercise while he was studying in Paris, Copland completed it as a full orchestral score after returning to New York in 1925. It too had "jazz elements" as did many of Copland's works in the 1920s.
Copland's "Symphony for Organ and Orchestra" (1924) brought him into contact with Serge Koussevitzky, a conductor known as a champion of "new music", and another figure who would prove to be influential in Copland's life, perhaps the second most important after Boulanger. Koussevitzky performed twelve Copland works during his tenure as conductor of the Boston Symphony. Copland's relationship with Koussevitzky was apparently unique, as his interpretations of Copland's works reflected the particular admiration that the latter had for the young composer. Copland's "Music for the Theatre" (1925) and the "Piano Concerto" (1926) were both composed for Koussevitzky.
Visits to Europe in 1926 and 1927 brought him into contact with the most recent developments there, including Webern's "Five Pieces for Orchestra", which greatly impressed him. In August 1927, while staying in Königstein, Copland wrote "Poet's Song", a setting of a text by E. E. Cummings and his first composition using Schoenberg's twelve-tone technique. This was followed by the "Symphonic Ode" (1929) and the "Piano Variations" (1930), both of which rely on the exhaustive development of a single short motive. This procedure, which provided Copland with more formal flexibility and a greater emotional range than in his earlier music, is similar to Schoenberg's idea of "continuous variation" and, according to Copland's own admission, was influenced by the twelve-tone method, though neither work actually uses a twelve-tone row.
Other major works of his first period include the "Piano Variations" (1930), and the "Short Symphony" (1933). However, this jazz-inspired period was relatively brief, as his style evolved toward the goal of writing more accessible works using folk sources.
Popular works.
Impressed with the success of Virgil Thomson's "Four Saints in Three Acts", Copland wrote "El Salón México" between 1932 and 1936, which met with a popular acclaim that contrasted the relative obscurity of most of his previous works. It appears he intended it to be a popular favorite, as he wrote in 1955: "It seems a long long time since anyone has written an "España" or "Bolero"—the kind of brilliant orchestral piece that everyone loves." Inspiration for this work came from Copland's vivid recollection of visiting the "Salon Mexico" dancehall where he witnessed a more intimate view of Mexico's nightlife. For Copland, the biggest impact came, not from the music of the people dancing, but from the spirit of the environment. Copland said that he could literally feel the essence of the Mexican people in the dance hall. This prompted him to write a piece celebrating the spirit of Mexico using Mexican Themes. Copland derived freely from two collections of Mexican folk tunes, changing pitches and varying rhythms. The use of a folk tune with variations set in a symphonic context started a pattern he repeated in many of his most successful works right on through the 1940s. This work also marked the return of jazz patterns to Copland's compositional style, though they appeared in a more subdued form than before and were no longer the centerpiece. Chávez conducted the premiere, and "El Salón México" became an international hit, gaining Copland wide recognition.
Copland achieved his first major success in ballet music with his groundbreaking score "Billy the Kid", based on a Walter Noble Burns novel, with choreography by Eugene Loring. The ballet was among the first to display an American music and dance vocabulary, adapting the "strong technique and intense charm of Astaire" and other American dancers. It was distinctive in its use of polyrhythm and polyharmony, particularly in the cowboy songs. The ballet premiered in New York in 1939, with Copland recalling "I cannot remember another work of mine that was so unanimously received." John Martin wrote, "Aaron Copland has furnished an admirable score, warm and human, and with not a wasted note about it anywhere." It became a staple work of the American Ballet Theatre, and Copland's twenty-minute suite from the ballet became part of the standard orchestral repertoire. When asked how a Jewish New Yorker managed so well to capture the Old West, Copland answered "It was just a feat of imagination."
In the early 1940s, Copland produced two important works intended as national morale boosters. "Fanfare for the Common Man", scored for brass and percussion, was written in 1942 at the request of the conductor Eugene Goossens, conductor of the Cincinnati Symphony Orchestra. It would later be used to open many Democratic National Conventions, and to add dignity to a wide range of other events. Even musical groups from Woody Herman's jazz band to the Rolling Stones adapted the opening theme. Emerson, Lake & Palmer recorded a "progressive rock" version of the composition in 1977. The fanfare was also used as the main theme of the fourth movement of Copland's "Third Symphony," where it first appears in a quiet, pastoral manner, then in the brassier form of the original. In the same year, Copland wrote "A Lincoln Portrait", a commission from conductor André Kostelanetz, leading to a further strengthening of his association with American patriotic music. The work is famous for the spoken recitation of Lincoln's words, though the idea had been previously employed by John Alden Carpenter's "Song of Faith" based on George Washington's quotations. "Lincoln Portrait" is often performed at national holiday celebrations. Many Americans have performed the recitation, including politicians, actors, and musicians and Copland himself, with Henry Fonda doing the most notable recording.
Continuing his string of successes, in 1942 Copland composed the ballet "Rodeo," a tale of a ranch wedding, written around the same time as "Lincoln Portrait". "Rodeo" is another enduring composition for Copland and contains many recognizable folk tunes, well-blended with Copland's original music. Notable in the final movement, is the striking "Hoedown". This was a recreation of Appalachian fiddler W. H. Stepp's version of the square-dance tune "Bonypart" ("Bonaparte's Retreat"), which had been transcribed for piano by Ruth Crawford Seeger and published in Alan Lomax and Seeger's book, "Our Singing Country" (1941). For the "Hoedown" in "Rodeo" Copland borrowed note for note from Seeger's piano transcription of Stepp's tune. This fragment (lifted from Ruth Crawford Seeger) is now one of the best-known compositions by any American composer, having been used numerous times in movies and on television, including commercials for the American beef industry. "Hoedown" was given a rock arrangement by Emerson, Lake & Palmer in 1972. The ballet, originally titled "The Courting at Burnt Ranch", was choreographed by Agnes de Mille, niece of film giant Cecil B. DeMille. It premiered at the Metropolitan Opera on October 16, 1942, with de Mille dancing the principal "cowgirl" role and the performance received a standing ovation. A reduced score is still popular as an orchestral piece, especially at "Pops" concerts.
Copland was commissioned to write another ballet, "Appalachian Spring", originally written using thirteen instruments, which he ultimately arranged as a popular orchestral suite. The commission for "Appalachian Spring" came from Martha Graham, who had requested of Copland merely "music for an American ballet". Copland titled the piece "Ballet for Martha", having no idea of how she would use it on stage but he had her in mind. "When I wrote 'Appalachian Spring' I was thinking primarily about Martha and her unique choreographic style, which I knew well ... And she's unquestionably very American: there's something prim and restrained, simple yet strong, about her which one tends to think of as American." Copland borrowed the flavor of Shaker songs and dances, and directly used the dance song Simple Gifts. Graham took the score and created a ballet she called "Appalachian Spring" (from a poem by Hart Crane which had no connection with Shakers). It was an instant success, and the music later acquired the same name. Copland was amused and delighted later in life when people would come up to him and say: "Mr. Copland, when I see that ballet and when I hear your music I can see the Appalachians and just feel spring." Copland had no particular setting in mind while writing the music, he just tried to give it an American flavor, and had no knowledge of the borrowed title, in which "spring" refers to a spring of water, not the season Spring.
Symphonic works.
Copland composed three numbered symphonies, but applied the word "symphony" to more than just symphonies of typical structure. He re-orchestrated his early three-movement Organ Symphony omitting the organ, calling the result his First Symphony. His fifteen-minute "Short Symphony" was the Second Symphony, though it also exists as the Sextet. His "Dance Symphony" was hurriedly extracted from the earlier unproduced ballet "Grohg" to meet an RCA Records commission deadline.
The Third Symphony is in the more traditional format (four movements; second movement, scherzo; third movement, adagio) and is his most famous symphony. At forty minutes, it is his longest orchestral composition. He composed it with Koussevitzky's unique character in mind, "I knew exactly the kind of music he enjoyed conducting and the sentiments he brought with it, and I knew the sound of his orchestra, so I had every reason to do my darnedest to write a symphony in the grand manner." Among the details of interest in the work is Copland's use of palindromic structure—whole movements as well as melodies end as they began. Completing the work after World War II was won by the Allies, he stated that the symphony was "intended to reflect the euphoric spirit of the country at the time." The work received generally strong acclaim. Koussevitzky "declared it simply the greatest American symphony ever written." Arthur Berger stated that it achieved "a kind of panorama of all the musical resources that have through the years formed his musical language." While Leonard Bernstein "deemed it the epitome of a decades-long search by many composers for a distinctly American music." It is the best known, most performed, and most recorded American symphony of the 20th Century.
Later work.
Copland's work in the late 1940s and 1950s included use of Schönberg's twelve-tone system, a development that he recognized as important, but which he did not fully embrace. His first result was his "Piano Quartet" (1950). However, he found the atonality of serialized music to run counter to his desire to reach a wide audience. So, in contrast to the Second Viennese School, Copland's use of the system emphasized the importance of the "classicalizing principles", in order to prevent the material from falling into "near-chaos".
In 1951, Copland undertook one of his most challenging works, the "Piano Fantasy" (1957) which he labored over for several years. It was a commission for the young virtuoso pianist William Kapell, who died in an aircraft crash in 1953 during the years of the work's development. The piece adapted the twelve-tone system as a ten-note row, reserving the last two notes as a tonal resolution and anchor. Critics lauded the effort, calling the piece "an outstanding addition to his own oeuvre and to contemporary piano literature" and "a tremendous achievement". Jay Rosenfield stated, "This is a new Copland to us, an artist advancing with strength and not building on the past alone."
Other late works include: "Dance Panels" (1959, ballet music), "Something Wild" (1961, his last film score, much of which would be later incorporated into his "Music for a Great City"), "Connotations" (1962, for the new Lincoln Center Philharmonic hall), "Emblems" (1964, for wind band), "Night Thoughts" (1972, for the Van Cliburn International Piano Competition), and "Proclamation'" (1982, his last work, started in 1973).
Film composer.
By the 1930s, Hollywood began to beckon "serious" composers with promises of better films and higher pay. The reality, however, was that few found good projects. Copland sought to enter that arena, as both a challenge for his abilities as a composer and an opportunity to expand his reputation and audience for his more serious works. Unlike the total attention he would hope to get from a concert-goer, Copland wrote that film music had to achieve a balance. It should be "secondary in importance to the story being told on the screen" while notably adding to the dramatic and emotional content of the film—but without diverting the viewer's attention from the action.
Upon arriving in Hollywood in 1937, he had high hopes: "It is just a matter of finding a feature film that needs my kind of music." What he found, however, was the ongoing tendency of studios to edit and cut movie scores, which often subverted a composer's intentions. No projects seemed suitable at first. But his patience paid off two years later when Copland found a kindred spirit in director Lewis Milestone, who allowed Copland to supervise his own orchestration and who refrained from interfering with his work. Copland composed three of his five film scores for Milestone.
This collaboration resulted in the notable film "Of Mice and Men" (1939), from the novel by John Steinbeck, that earned Copland his first nomination for an Academy Award ( he actually received two nominations, one for "best score" and another for "original score"). He considered himself lucky with his first film score: "Here was an American theme, by a great American writer, demanding appropriate music." Having accepted small sums for other projects in the past, especially to help out cash-strapped productions involving friends, this time Copland would capitalize on his efforts: "I thought if I was to sell myself to the movies, I ought to sell myself good." From then on, he became one of Hollywood's highest paid film composers, earning as much as $15,000 per film.
In a departure from other film scores of the time, Copland's work largely reflected his own style, instead of the usual borrowing from the late-Romantic period. Many silent and early talking films used classical music themes directly, both in the credit sequences and during the action. But with Copland, the film score's purpose was more comprehensive and subtle, setting the atmosphere of time and place, illustrating the thoughts of the actors, providing continuity and filler, and shaping the emotion and drama. He often avoided the full orchestra, and he rejected the common practice of using a leitmotiv to identify characters with their own personal themes. He instead matched a theme to the action, while avoiding the underlining of every action with exaggerated emphasis.
Another technique Copland employed was to keep silent during intimate screen moments and only begin the music as a confirming motive toward the end of a scene. Virgil Thompson wrote that the score for "Of Mice and Men" established "the most distinguished populist musical style yet created in America." Many composers who scored for western movies, particularly between 1940 and 1960, were influenced by Copland's style, though some also followed the "Max Steiner" approach, which was more bombastic and obvious. As a commentator on film scores, Copland singled out Bernard Herrmann, Miklós Rózsa, Alex North and Erich Wolfgang Korngold as innovative leaders in the field.
Copland's score for "The North Star" (1943) was nominated for an Academy Award, and his score for William Wyler's 1949 film, "The Heiress" won the award. Several themes from his scores are incorporated in the suite "Music for Movies." His score for the film adaptation of John Steinbeck's novel "The Red Pony" was arranged by commission of the Houston Symphony Orchestra as a suite for their performance in October 1948 and became widely popular. His score for the 1961 independent film "Something Wild" was released in 1964 as "Music For a Great City". Copland also composed scores for two documentary films, "The City" (1939) and "The Cummington Story" (1945).
When commenting on the effectiveness of film scores, Copland said: "I'd love to be able to have audiences see a film with the music, then see it a second time with the music turned off, and then see it a third time with the music turned on. Then, I think they'd get a much more specific idea of what the music does for a film.".
Critic, writer, and teacher.
Copland had a large following of pupils—oftentimes mixing his personal life with them. Of notable students, Leonard Bernstein and Victor Kraft were two with whom he continued having intimately personal relationships. Bernstein would go on to champion Copland as one of the greatest American composers of all time while being one of the few people Copland opened up to.
Copland also wrote prolifically on the subject of music. Across decades, Copland has published pieces on music criticism analysis on musical trends, and on his own compositions. Starting with his first critiques in 1924, Copland began a long career as music critic, teacher, and observer, mostly of contemporary classical music. He was an avid lecturer and lecturer-performer. He wrote reviews of specific works, trends, composers, festivals, books about music, and recordings. He took on a wide range of issues from the most general ("Creativity") to the most practical ("Composer Economics"). Copland also wrote three books, "What to Listen for in Music (1939)", "Our New Music (1941)", and "Music and Imagination" (1952). He had a long list of notable students (see below). Copland put a good deal of time and energy into supporting young musicians, especially through his association with the Berkshire Music Center at Tanglewood, both as a guest conductor and teacher. In working with young composers, Copland thought it more important to focus on expressive content than on technical points.
Conductor.
Copland studied conducting in Paris in 1921, but not until his involvement conducting his own Hollywood scores, did he undertake it except out of necessity. On his international travels in the 1940s, however, he began to make appearances as a guest conductor, performing his own works. By the 1950s, he was conducting the works of other composers as well. From the 1960s on, he conducted far more than he composed.
A self-taught conductor, Copland developed a very personal style. He occasionally asked friend Leonard Bernstein for advice. Copland took an understated and unpretentious approach to conducting and modeled his style after other composer/conductors such as Stravinsky and Hindemith. Observers of Copland noted that he had "none of the typical conductorial vanities". Though his friendly and modest persona, and his great enthusiasm, were appreciated by professional orchestra musicians, some criticized his beat as "unsteady" and his interpretations as "unexciting". Some of his peers, like Koussevitzky, went even further, advising him to "stay home and compose". Copland thoroughly enjoyed conducting but admitted that he did it in part because in the last seventeen years of his life he felt little inspiration to compose. He was offered "permanent" conducting posts but preferred to operate as a guest conductor. Nearly all of Copland's conducting appearances included his own works, which added to the intoxication of conducting. As he stated, "Conducting puts one in a very powerful position ... Best of all, it is a use of power for a good purpose." It also allowed him the freedom to travel which he always enjoyed.
Copland was a strong advocate for newer music and composers, and his programs always included heavy representation of 20th-century music and lesser-known composers. Performers and audiences generally greeted his conducting appearances as positive opportunities to hear his music as the composer intended, but sometimes found his efforts with other composers to be lacking. From Copland's point of view, he found both the New York Philharmonic and the Boston Symphony Orchestra to be "tough" groups, resistant to newer music. Newton Mansfield, violinist with the New York Philharmonic, stated, "The orchestra didn't take him too seriously. It was like going out to a nice lunch." Copland also found resistance from European orchestras; however, he was warmly received and respected in England. Copland recorded nearly all his orchestral works with himself conducting.

</doc>
<doc id="51299" url="http://en.wikipedia.org/wiki?curid=51299" title="Jammu and Kashmir">
Jammu and Kashmir

Jammu and Kashmir () is a state in northern India. It is located mostly in the Himalayan mountains, and shares a border with the states of Himachal Pradesh and Punjab to the south. Jammu and Kashmir has an international border with China in the north and east, and the Line of Control separates it from the Pakistani-controlled territories of Azad Kashmir and Gilgit–Baltistan in the west and northwest respectively. The state has special autonomy under Article 370 of the Indian constitution.
A part of the erstwhile Princely State of Kashmir and Jammu, the region is the subject of a territorial conflict among China, India and Pakistan. Pakistan, which claims the territory as disputed, refers to it alternatively as "Indian-occupied Kashmir" or "Indian-held Kashmir," whereas some international agencies such as the United Nations call it "Indian-administered Kashmir." The regions under the control of Pakistan are referred to as "Pakistan-occupied Kashmir" (PoK) within India, as "Azad (Free) Jammu and Kashmir" in Pakistan, and as "Pakistan-administered Kashmir" or "Pakistan-controlled Kashmir" generally.
Jammu and Kashmir consists of three regions: Jammu, the Kashmir Valley and Ladakh. Srinagar is the summer capital, and Jammu is the winter capital. The Kashmir valley is famous for its beautiful mountainous landscape, and Jammu's numerous shrines attract tens of thousands of Hindu pilgrims every year. Ladakh, also known as "Little Tibet", is renowned for its remote mountain beauty and Buddhist culture. It is the only state in India with a Muslim-majority population.
History.
Hari Singh had ascended the throne of Kashmir in 1925 and was the reigning monarch at the conclusion of British rule in the subcontinent in 1947. One of the conditions of the partition of India imposed by Britain was that the rulers of princely states would have the right to opt for either Pakistan or India or remain independent. In 1947, Kashmir's population was 77% Muslim and it shared a boundary with both Dominion of Pakistan and Union of India.
On 22 October 1947, locals and tribesmen backed by Pakistan invaded Kashmir. The Maharaja initially fought back but appealed for assistance to the Governor-General Louis Mountbatten, who agreed on the condition that the ruler accede to India. Maharaja Hari Singh signed the Instrument of Accession on 26 October 1947, which was accepted by the Governor General of India the next day. Once the Instrument of Accession was signed, Indian soldiers entered Kashmir with orders to evict the raiders. India took the matter to the United Nations. The UN resolution asked both India and Pakistan to vacate the areas they had occupied and hold a referendum under UN observation. The holding of this plebiscite, which India initially supported, was dismissed by India because the 1952 elected Constituent assembly of Jammu and Kashmir voted in favour of confirming the Kashmir region's accession to India. The United Nations Military Observer Group in India and Pakistan (UNMOGIP) was deployed to supervise the ceasefire between India and Pakistan. UNMOGIP's functions were to investigate complaints of ceasefire violations and submit findings to each party and to the U.N. secretary-general. Under the terms of the ceasefire, it was decided that both armies would withdraw and a plebiscite would be held in Kashmir to give Kashmiris the right to self-determination. The primary argument for the continuing debate over the ownership of Kashmir is that India did not hold the promised plebiscite. In fact, neither side has adhered to the U.N. resolution of 13 August 1948; while India chose not to hold the plebiscite, Pakistan failed to withdraw its troops from Kashmir as was required under the resolution. India gives the following reasons for not holding the plebiscite:
In response Pakistan holds that: 
Diplomatic relations between India and Pakistan soured for many other reasons and eventually resulted in three further wars in Kashmir the Indo-Pakistani War of 1965, the Indo-Pakistan War of 1971 and the Kargil War in 1999. India has control of 60% of the area of the former Princely State of Jammu and Kashmir (Jammu, Kashmir Valley, and Ladakh); Pakistan controls 30% of the region (Gilgit–Baltistan and Azad Kashmir). China occupied 10% (Aksai Chin and Trans-Karakoram Tract) of the state in 1962.
The Chenab formula was a compromise proposed in the 1960s, in which the Kashmir valley and other Muslim-dominated areas north of the Chenab river would go to Pakistan, and Jammu and other Hindu-dominated regions would go to India.
The eastern region of the erstwhile princely state of Kashmir has also been beset with a boundary dispute. In the late 19th- and early 20th centuries, although some boundary agreements were signed between Great Britain, Tibet, Afghanistan and Russia over the northern borders of Kashmir, China never accepted these agreements, and the official Chinese position did not change with the communist revolution in 1949. By the mid-1950s the Chinese army had entered the northeast portion of Ladakh.
By 1956–57 they had completed a military road through the Aksai Chin area to provide better communication between Xinjiang and western Tibet. India's belated discovery of this road led to border clashes between the two countries that culminated in the Sino-Indian war of October 1962. China has occupied Aksai Chin since 1962 and, in addition, an adjoining region, the Trans-Karakoram Tract was ceded by Pakistan to China in 1963.
For intermittent periods between 1957, when the state approved its own Constitution, and the death of Sheikh Abdullah in 1982, the state had alternating spells of stability and discontent. In the late 1980s, however, simmering discontent over the high-handed policies of the Union Government and allegations of the rigging of the 1987 assembly elections triggered a violent uprising which was backed by Pakistan.
Since then, the region has seen a prolonged, bloody conflict between separatists and the Indian Army, both of whom have been accused of widespread human rights abuses, including abductions, massacres, rapes and lootings. The army has officially denied these allegations.
However, violence in the state has been on the decline since 2004 with the peace process between India and Pakistan. The situation has become increasingly tense politically in recent years.
Geography and climate.
Jammu and Kashmir is home to several valleys such as the Kashmir Valley, Tawi Valley, Chenab Valley, Poonch Valley, Sind Valley and Lidder Valley. The main Kashmir valley is 100 km wide and 15520.3 km2 in area. The Himalayas divide the Kashmir valley from Ladakh while the Pir Panjal range, which encloses the valley from the west and the south, separates it from the Great Plains of northern India. Along the northeastern flank of the Valley runs the main range of the Himalayas. This densely settled and beautiful valley has an average height of 1850 m above sea-level but the surrounding Pir Panjal range has an average elevation of 5000 m.
Because of Jammu and Kashmir's wide range of elevations, its biogeography is diverse. Northwestern thorn scrub forests and Himalayan subtropical pine forests are found in the low elevations of the far southwest. These give way to a broad band of western Himalayan broadleaf forests running from northwest-southeast across the Kashmir Valley. Rising into the mountains, the broadleaf forests grade into western Himalayan subalpine conifer forests. Above the tree line are found northwestern Himalayan alpine shrub and meadows. Much of the northeast of the state is covered by the Karakoram-West Tibetan Plateau alpine steppe. Around the highest elevations, there is no vegetation, simply rock and ice.
The Jhelum River is the only major Himalayan river which flows through the Kashmir valley. The Indus, Tawi, Ravi and Chenab are the major rivers flowing through the state. Jammu and Kashmir is home to several Himalayan glaciers. With an average altitude of 5753 m above sea-level, the Siachen Glacier is 76 km long making it the longest Himalayan glacier.
The climate of Jammu and Kashmir varies greatly owing to its rugged topography. In the south around Jammu, the climate is typically monsoonal, though the region is sufficiently far west to average 40 to 50 mm (1.6 to 2 inches) of rain per month between January and March. In the hot season, Jammu city is very hot and can reach up to 40 °C (104 °F) whilst in July and August, very heavy though erratic rainfall occurs with monthly extremes of up to 650 millimetres (25.5 inches). In September, rainfall declines, and by October conditions are hot but extremely dry, with minimal rainfall and temperatures of around 29 °C (84 °F).
Across from the Pir Panjal range, the South Asian monsoon is no longer a factor and most precipitation falls in the spring from southwest cloudbands. Because of its closeness to the Arabian Sea, Srinagar receives as much as 635 mm of rain from this source, with the wettest months being March to May with around 85 millimetres (3.3 inches) per month. Across from the main Himalaya Range, even the southwest cloudbands break up and the climate of Ladakh and Zanskar is extremely dry and cold. Annual precipitation is only around 100 mm (4 inches) per year and humidity is very low. In this region, almost all above 3,000 metres (9,750 ft) above sea level, winters are extremely cold. In Zanskar, the average January temperature is −20 °C (−4 °F) with extremes as low as −40 °C (−40 °F). All the rivers freeze over and locals make river crossings during this period because their high levels from glacier melt in summer inhibits crossing. In summer in Ladakh and Zanskar, days are typically a warm 20 °C (68 °F), but with the low humidity and thin air nights can still be cold.
Administrative divisions.
Jammu and Kashmir consists of three divisions: Jammu, Kashmir Valley and Ladakh, and is further divided into 22 districts. The Siachen Glacier, although under Indian military control, does not lie under the administration of the state of Jammu and Kashmir. Kishtwar, Ramban, Reasi, Samba, Bandipora, Ganderbal, Kulgam and Shopian are newly formed districts, and their areas are included with those of the districts from which they were formed.
Major cities.
Municipal corporations: 2 – Srinagar, Jammu
Municipal councils: 6 – Udhampur, Kathua, Poonch, Anantnag, Baramulla, Sopore
Municipal boards: 21 – Samba, Ranbirsinghpora, Akhnoor, Reasi, Ramban, Doda, Bhaderwah, Kishtwar, Kargil, Duru-Verinag, Bijbehara, Pulwama, Tral, Badgam, Kulgam, Shopian, Ganderbal, Pattan, Sumbal, Kupwara, Handwara
Population of ten major cities:
Demographics.
Jammu and Kashmir has a Muslim majority population. Though Islam is practised by about 67% of the population of the state and by 97% of the population of the Kashmir valley, the state has large communities of Buddhists, Hindus (inclusive of Megh Bhagats) and Sikhs.
In Jammu, Hindus constitute 66% of the population, Muslims 30% and Sikhs, 4%; In Ladakh(comprises Buddhists-dominated Leh and Muslim-dominated Kargil), Muslims constitute about 47% of the population, the remaining being Buddhists(46%) and Hindus(6%). The people of Ladakh are of Indo-Tibetan origin, while the southern area of Jammu includes many communities tracing their ancestry to the nearby Indian states of Haryana and Punjab, as well as the city of Delhi. In totality, the Muslims constitute 67% of the population, the Hindus about 30%, the Buddhists 1%, and the Sikhs 2% of the population.
Buddhists, Hindus, Sikhs and a few Christian, Jain, and Zoroastrian communities were once natives and made up a vast majority of the whole Kashmir province, as well as neighbouring states, and ancient and modern northern half of what is today India and Pakistan, but because of economic changes, political tension, military involvement, and foreign extremists resulted in vast majority of the followers of these religions to settle in the growing and advancing neighbouring regions and major cities in India over the years, often during no present borders or records. Hindu pandits were specifically affected in this region due to their status in the local society.
According to political scientist Alexander Evans, approximately 95% of the total population of 160,000–170,000 of Kashmiri Brahmins, also called Kashmiri Pandits, ("i.e." approximately 150,000 to 160,000) left the Kashmir Valley in 1990 as militancy engulfed the state. According to an estimate by the Central Intelligence Agency, about 300,000 Kashmiri Pandits from the entire state of Jammu and Kashmir have been internally displaced due to the ongoing violence.
In Jammu and Kashmir, the principal spoken languages are Kashmiri, Urdu, Dogri, Pahari, Balti, Ladakhi, Gojri, Shina and Pashto. However, Urdu written in the Persian script is the official language of the state. Many speakers of these languages use Hindi or English as a second language.
The Kashmir Valley is dominated by ethnic Kashmiris, who have largely driven the Azadi campaign. Non-Kashmiri Muslim ethnic groups (Paharis, Sheenas, Gujjars and Bakarwalas), who dominate areas along the Line of Control, have remained indifferent to the separatist campaign. Jammu province region has a 70:30 Hindu-Muslim ratio. Parts of the region were hit by militants, but violence has ebbed there, along with the Valley, after India and Pakistan started a peace process in 2004.
Dogras (67%) are the single largest group in the multi-ethnic region of Jammu living with Punjabis, Paharis, Bakerwals and Gujjars. Statehood is demanded in Hindu-dominated districts. Ladakh is the largest region in the state with over 200,000 people. Its two districts are Leh (77% Buddhist) and Kargil (80% Muslim population). Union territory status has been the key demand of Leh Buddhists for many years.
Politics and government.
Jammu and Kashmir is the only state in India which enjoys special autonomy under Article 370 of the Constitution of India, according to which no law enacted by the Parliament of India, except for those in the field of defence, communication and foreign policy, will be extendable in Jammu and Kashmir unless it is ratified by the state legislature of Jammu and Kashmir. Subsequently, jurisdiction of the Supreme Court of India over Jammu and Kashmir has been extended.
Jammu and Kashmir is the only Indian state to have its own official state flag along with national flag and constitution. Indians from other states cannot purchase land or property in the state. Designed by the then ruling National Conference, the flag of Jammu and Kashmir features a plough on a red background symbolising labour; it replaced the Maharaja's state flag. The three stripes represent the three distinct administrative divisions of the state, namely Jammu, Valley of Kashmir, and Ladakh.
Since 1990, the Armed Forces Act, which gives special powers to the Indian security forces, has been enforced in Jammu and Kashmir. The decision to invoke this act was criticised by the Human Rights Watch.
Like all the states of India, Jammu and Kashmir has a multi-party democratic system of governance with a bicameral legislature. At the time of drafting the Constitution of Jammu and Kashmir, 100 seats were earmarked for direct elections from territorial constituencies. Of these, 25 seats were reserved for the areas of Jammu and Kashmir State that came under Pakistani occupation; this was reduced to 24 after the 12th amendment of the Constitution of Jammu and Kashmir:
"The territory of the State shall comprise all the territories which on the fifteenth day of August 1947, were under the sovereignty or suzerainty of the Ruler of the State" and Section 48 therein states that, "Notwithstanding anything contained in section 47, until the area of the State under the occupations of Pakistan ceases to so occupied and the people residing in that area elect their representatives (a) twenty-five seats in the Legislative Assembly shall remain vacant and shall not be taken into account for reckoning the total member-ship of the Assembly; and the said area shall be excluded in delimiting the territorial Constituencies Under Section 47".
After a delimitation in 1988, the total number of seats increased to 111, of which 87 were within Indian-administered territory. The Jammu & Kashmir Assembly is the only state in India to have a 6-year term, in contrast to the norm of a 5-year term followed in every other state's Assembly. There was indication from the previous INC Government to bring parity with the other states, but this does not seem to have received the required support to pass into law.
Influential political parties include the Jammu & Kashmir National Conference (NC), the Indian National Congress (INC), the Jammu and Kashmir People's Democratic Party (PDP), the Bharatiya Janata Party (BJP) and other smaller regional parties. After dominating Kashmir's politics for years, the National Conference's influence waned in 2002, when INC and PDP formed a political alliance and rose to power. Under the power-sharing agreement, INC leader Ghulam Nabi Azad replaced PDP's Mufti Mohammad Sayeed as the Chief Minister of Jammu and Kashmir in late 2005. However, in 2008, PDP withdrew its support from the government on the issue of temporary diversion of nearly 40 acre of land to the Sri Amarnath Shrine Board. In the 2008 Kashmir Elections that were held from 17 November to 24 December, the National Conference party and the Congress party together won enough seats in the state assembly to form a ruling alliance. In the 2014 election, the voter turnout was recorded at 65% - the highest in the history of the state. The results gave a fractured mandate to either parties — the PDP won 28 seats, BJP 25, NC 15 and INC 12. After 2 months of deliberations and president's rule, the BJP and the PDP announced an agreement for a coalition government, and PDP patron Mufti Mohammad Sayeed was sworn-in as CM for a second term, with Nirmal Singh of the BJP sworn-in as deputy CM. This also marked the first time in 35 years that the BJP was a coalition partner in the state government.
Some Kashmiris, especially those residing in the Kashmir Valley, demand greater autonomy, sovereignty and even independence from India. Due to the economic integration of Jammu and Kashmir with the rest of India, separatist movements across the Kashmir Valley declined. However, following the unrest in 2008, which included more than 500,000 protesters at a rally on 18 August, secessionist movements gained a boost.
The 2009 edition of the Freedom in the World (report) by Freedom House rated Jammu and Kashmir as "Partly Free", while in comparison, the same report rated Pakistan-administered Kashmir as "Not Free" (both reports available on UNHCR refworld).
Economy.
Jammu and Kashmir's economy is predominantly dependent on agriculture and allied activities. The Kashmir valley is known for its sericulture and cold-water fisheries. Wood from Kashmir is used to make high-quality cricket bats, popularly known as "Kashmir Willow". Kashmiri saffron is very famous and brings the state a handsome amount of foreign exchange. Agricultural exports from Jammu and Kashmir include apples, barley, cherries, corn, millet, oranges, rice, peaches, pears, saffron, sorghum, vegetables, and wheat, while manufactured exports include handicrafts, rugs, and shawls.
Horticulture plays a vital role in the economic development of the state. With an annual turnover of over ₹3 billion (), apart from foreign exchange of over ₹800 million (), this sector is the next biggest source of income in the state's economy. The region of Kashmir is known for its horticulture industry and is the wealthiest region in the state. Horticultural produce from the state includes apples, apricots, cherries, pears, plums, almonds and walnuts.
The Doda district has deposits of high-grade sapphire. Though small, the manufacturing and services sector is growing rapidly, especially in the Jammu division. In recent years, several consumer goods companies have opened manufacturing units in the region. The Associated Chambers of Commerce and Industry of India (ASSOCHAM) has identified several industrial sectors which can attract investment in the state, and accordingly, it is working with the union and the state government to set up industrial parks and special economic zones. In the fiscal year 2005–06, exports from the state amounted to ₹11.5 billion (). However, industrial development in the state faces several major constraints including extreme mountainous landscape and power shortage. The Jammu & Kashmir Bank, which is listed as a S&P CNX 500 conglomerate, is based in the state. It reported a net profit of ₹598 million () in 2008.
The Government of India has been keen to economically integrate Jammu and Kashmir with the rest of India. The state is one of the largest recipients of grants from New Delhi, totalling US$812 million per year. It has a mere 4% incidence of poverty, one of the lowest in the country.
In an attempt to improve the infrastructure in the state, Indian Railways is constructing the ambitious Kashmir Railway project at a cost of more than US$2.5 billion. Trains run on the 130 km Baramula-Banihal section. The 17.5 km Qazigund-Banihal section through the 11 km long Pir Panjal Railway Tunnel was commissioned. Udhampur-Katra section of the track was commissioned early in July 2014. The Katra-Banihal section is under construction. The route crosses major earthquake zones, and is subjected to extreme temperatures of cold and heat, as well as inhospitable terrain, making it an extremely challenging engineering project. It is expected to increase tourism and travel to Kashmir. Three other railway lines, the Bilaspur–Mandi–Leh railway, Srinagar-Kargil-Leh railway and the Jammu-Poonch railway have been proposed.
Tourism.
Before the insurgency intensified in 1989, tourism formed an important part of the Kashmiri economy. The tourism economy in the Kashmir valley was worst hit. However, the holy shrines of Jammu and the Buddhist monasteries of Ladakh continue to remain popular pilgrimage and tourism destinations. Every year, thousands of Hindu pilgrims visit holy shrines of Vaishno Devi and Amarnath, which has had significant impact on the state's economy. It was estimated in 2007 that the Vaishno Devi yatra contributed ₹4.75 billion () to the local economy annually a few years ago. The contribution would be much more now as the numbers of visitors have increased considerably. Foreign tourists have been slower to return. The British government still advises against all travel to Jammu and Kashmir with the exception of the cities of Jammu and Srinagar, travel between these two cities on the Jammu-Srinagar highway, and the region of Ladakh.
Besides Kashmir, Jammu region too has a lot of tourism potential. There are various places in Jammu which are worth seeing. Bhau Fort in Jammu city is the major attraction centre for the tourists visiting the city. Bage-e-Bahu is the another tourist destination. Aquarium established by the fisheries department is being visited by many these days.
Jammu is being majorly visited by the tourist from across the India as a pilgrimage to Mata Vaishno Devi. Mata Vaishno Devi is located on the trikuta hills which is around 40 to 45 km away from Jammu City. Approximately 10 million Pilgrims visit this holy place every year.
Tourism in the Kashmir valley has rebounded in recent years, and in 2009, the state became one of the top tourist destinations of India. Gulmarg, one of the most popular ski resort destinations in India, is also home to the world's highest green golf course. However, the decrease in violence in the state has boosted the state's economy and tourism. It was reported that more than a million tourists visited Kashmir in 2011.
Culture.
Ladakh is famous for its unique Indo-Tibetan culture. Chanting in Sanskrit and Tibetan language forms an integral part of Ladakh's Buddhist lifestyle. Annual masked dance festivals, weaving and archery are an important part of traditional life in Ladakh. Ladakhi food has much in common with Tibetan food, the most prominent foods being thukpa, noodle soup; and tsampa, known in Ladakhi as "Ngampe", roasted barley flour. Typical garb includes gonchas of velvet, elaborately embroidered waistcoats and boots, and gonads or hats. People adorned with gold and silver ornaments and turquoise headgears throng the streets during Ladakhi festivals.
The "Dumhal" is a famous dance in the Kashmir Valley, performed by men of the Wattal region. The women perform the Rouff, another traditional folk dance. Kashmir has been noted for its fine arts for centuries, including poetry and handicrafts. "Shikaras", traditional small wooden boats, and houseboats are a common feature in lakes and rivers across the Valley.
The Constitution of India does not allow people from regions other than Jammu and Kashmir to purchase land in the state. As a consequence, houseboats became popular among those who were unable to purchase land in the Valley and has now become an integral part of the Kashmiri lifestyle.
"Kawa", traditional green tea with spices and almond, is consumed all through the day in the chilly winter climate of Kashmir. Most of the buildings in the Valley and Ladakh are made from softwood and are influenced by Indian, Tibetan, and Islamic architecture.
Jammu's Dogra culture and tradition is very similar to that of neighbouring Punjab and Himachal Pradesh. Traditional Punjabi festivals such as Lohri and Vaisakhi are celebrated with great zeal and enthusiasm throughout the region, along with Accession Day, an annual holiday which commemorates the accession of Jammu & Kashmir to the Dominion of India. After "Dogras", "Gujjars" form the second-largest ethnic group in Jammu. Known for their semi-nomadic lifestyle, Gujjars are also found in large numbers in the Kashmir Valley. Similar to Gujjars, "Gaddis" are primarily herdsmen who hail from the Chamba region in Himachal Pradesh. Gaddis are generally associated with emotive music played on the flute. The "Bakkarwala"s found both in Jammu and the Valley of Kashmir are wholly nomadic pastoral people who move along the Himalayan slopes in search for pastures for their huge flocks of goats and sheep.
Education.
In 1970, the state government of Jammu and Kashmir established its own education board and university. Education in the state is divided into primary, middle, high secondary, college and university level. Jammu and Kashmir follows the 10+2 pattern for education of children. This is handled by Jammu and Kashmir State Board of School Education (abbreviated as JKBOSE). Private and public schools are recognised by the board to impart education to students. Board examinations are conducted for students in class VIII, X and XII. In addition, there are "Kendriya Vidyalayas" (run by the Government of India) and Indian Army schools that impart secondary school education. These schools follow the Central Board of Secondary Education pattern.
Notable higher education or research institutes in Jammu and Kashmir include Sher-e-Kashmir Institute of Medical Sciences, Soura, Srinagar, Shri Mata Vaishno Devi University, National Institute of Technology, Srinagar, Government College of Engineering and Technology, Jammu and the Government Medical College of Jammu. University-level education is provided by University of Jammu, University of Kashmir, Sher-e-Kashmir University of Agricultural Sciences and Technology of Jammu, Sher-e-Kashmir University of Agricultural Sciences and Technology of Kashmir, Islamic University of Science & Technology, Baba Ghulam Shah Badhshah University, Institution of Technicians and Engineers (Kashmir), [[Government Degree College Kathua] central university of Jammu located at samba and central university of Kashmir located at Ganderbal, IIT jammu.
Sports.
Sports like cricket, football are famous along with sports like golf, skiing, water sports and adventure sports. Srinagar is home to the [[Sher-i-Kashmir Stadium]], a stadium where international cricket matches have been played.
The first international match was played in 1983 in which West Indies defeated India and the last international match was played in 1986 in which Australia defeated India by six wickets. Since then no international match have taken place in the stadium due to the prevailing security situation.
[[Maulana Azad Stadium]] is a stadium in [[Jammu]] and is one of the home venues for the Jammu and Kashmir cricket team. Stadium has hosted home games for Jammu and Kashmir in domestic tournaments since 1966. It has also hosted one One Day International in 1988 between India and New Zealand, which was abandoned due to rain without a ball being bowled. The stadium has played host to one women's test match where India lost to West Indies and one Women's One Day International where India beat New Zealand in 1985.
Srinagar has an outdoor stadium namely [[Bakshi Stadium]] for hosting football matches. It is named after [[Bakshi Ghulam Mohammad]].
The city has a golf course named [[Royal Springs Golf Course, Srinagar]] located on the banks of [[Dal lake]], which is considered as one of the best golf courses of India.
External links.
[[Category:Commons category template with no category set]]
[[Category:Commons category without a link on Wikidata]]
[[Category:2005 Kashmir earthquake]]
[[Category:Disputed territories in Asia]]
[[Category:Jammu and Kashmir| ]]
[[Category:Independent India]]
[[Category:States and territories established in 1947]]
[[Category:1947 establishments in India]]
[[Category:Urdu-speaking countries and territories]]
[[Category:Territorial disputes of Pakistan]]
[[Category:Kashmiri-speaking countries and territories]]

</doc>
<doc id="51301" url="http://en.wikipedia.org/wiki?curid=51301" title="Pope Pontian">
Pope Pontian

Pope St. Pontian (Latin: "Pontianus"; died October 235), was the Bishop of Rome from 21 July 230 to 28 September 235. In 235, during the persecution of Christians in the reign of the Emperor Maximinus the Thracian, Pontian was arrested and sent to the island of Sardinia. He resigned to make the election of a new pope possible.
A little more is known of Pontian than his predecessors, apparently from a lost papal chronicle that was available to the compiler of the "Liberian Catalogue" of Bishops of Rome, written in the 4th century.
Pontian's pontificate was relatively peaceful under the reign of the Emperor Severus Alexander, and noted for the condemnation of Origen by a Roman synod, over which Pontian likely presided. According to early church historian Eusebius of Caesarea, the next emperor, Maximinus, overturned his predecessor's policy of toleration towards Christianity. Both Pope Pontian and the Antipope Hippolytus of Rome were arrested and exiled to labor in the mines of Sardinia, generally regarded as a death sentence.
In light of his sentence, Pontian resigned as bishop on 28 September 235, so as to allow an orderly transition in the Church of Rome. This action ended a schism that had existed in the Roman Church for eighteen years. Neither Hippolytus nor Pontian survived, reconciling with one another there before their deaths. Pontian died in October 235.
Remembered.
Pope Fabian had the bodies of both Pontian and Hippolytus brought back to Rome in 236 or 237 and buried in the papal crypt in the Catacomb of Callixtus on the Appian Way. The slab covering his tomb was discovered in 1909. On it is inscribed in Greek: "Ποντιανός Επίσκ" ("Pontianus Episk"; in English "Pontianus Bish"). The inscription "MARTUR" had been added in another hand.
Pontian's feast day was previously celebrated on 19 November, but since 1969 both he and Hippolytus are commemorated jointly on 13 August.

</doc>
<doc id="51303" url="http://en.wikipedia.org/wiki?curid=51303" title="Cowpox">
Cowpox

Cowpox is an infectious disease caused by the cowpox virus. The virus, part of the orthopoxvirus family, is so closely related to the "vaccinia" virus that the two are often spoken of interchangeably . The virus is zoonotic, meaning that it is transferable between species, such as from animal to human. The transferral of the disease was first observed in dairymaids who touched the udders of infected cows and consequently developed the signature pustules on their hands. Cowpox is more commonly found in animals other than bovines, such as rodents. Cowpox is similar to, but much milder than, the highly contagious and often deadly smallpox disease. Its close resemblance to the mild form of smallpox inspired the first smallpox vaccine, created and administered by English physician Edward Jenner.
The word “vaccination,” coined by Jenner in 1796[2], is derived from the Latin root "vaccinus", meaning of or from the cow. Once vaccinated, a patient develops antibodies that make him/her immune to cowpox, but they also develop immunity to the smallpox virus, or "Variola virus". The cowpox vaccinations and later incarnations proved so successful that in 1980, the World Health Organization announced that smallpox was the first disease to be eradicated by vaccination efforts worldwide. Other orthopox viruses remain prevalent in certain communities, such as the cowpox virus (CPXV) in Europe, vaccinia in Brazil, monkeypox virus, and continue to infect humans.
Origin.
Discovery.
In the years from 1770 to 1790, at least six people who had contact with a cow had independently tested the possibility of using the cowpox vaccine as an immunization for smallpox in humans. Amongst them were the English farmer Benjamin Jesty, in Dorset in 1774 and the German teacher Peter Plett in 1791. Jesty inoculated his wife and two young sons with cowpox, in a successful effort to immunize them to smallpox, an epidemic of which had arisen in their town. His patients who had contracted and recovered from the similar but milder cowpox (mainly milkmaids), seemed to be immune not only to further cases of cowpox, but also to smallpox. By scratching the fluid from cowpox lesions into the skin of healthy individuals, he was able to immunize those people against smallpox. Reportedly, farmers and people working regularly with cattle and horses were often spared during smallpox outbreaks. Investigations by the British Army in 1790 showed that horse-mounted troops were less infected by smallpox than infantry, due to probable exposure to the similar horse pox virus (Variola equina). By the early 19th century, more than 100,000 people in Great Britain had been vaccinated. The arm-to-arm method of transfer of the cowpox vaccine was also used to distribute Jenner's vaccine throughout the Spanish Empire. Spanish king Charles IV's daughter had been stricken with smallpox in 1798, and after she recovered, he arranged for the rest of his family to be vaccinated. In 1803, the king, convinced of the benefits of the vaccine, ordered his personal physician Francis Xavier de Balmis, to deliver it to the Spanish dominions in North and South America. To maintain the vaccine in an available state during the voyage, the physician recruited 22 young boys who had never had cowpox or smallpox before, aged three to nine years, from the orphanages of Spain. During the trip across the Atlantic, de Balmis vaccinated the orphans in a living chain. Two children were vaccinated immediately before departure, and when cowpox pustules had appeared on their arms, material from these lesions was used to vaccinate two more children.
Jesty did not publicize his findings, and Jenner, who performed his first inoculation 22 years later and publicized his findings, assumed credit. It is said that Jenner made this discovery by himself, possibly without knowing previous accounts 20 years earlier. Although Jesty may have been the first to discover it, Jenner made vaccination widely accessible and has therefore been credited for its invention.
Implementation.
Naturally occurring cases of cowpox were not common, but it was discovered that the vaccine could be “carried” in humans and reproduced and disseminated human-to-human. Jenner’s original vaccination used lymph from the cowpox pustule on a milkmaid, and subsequent “arm-to-arm” vaccinations applied the same principle. As this transfer of human fluids came with its own set of complications, a safer manner of producing the vaccine was first introduced in Italy, The new method used cows to manufacture the vaccine using a process called “retrovaccination,” in which a heifer was inoculated with humanized cowpox virus, and it was passed from calf to calf to produce massive quantities efficiently and safely. This then lead to the next incarnation, “true animal vaccine,” which used the same process but began with naturally-occurring cowpox virus, and not the humanized form.
This method of production proved to be lucrative and was taken advantage of by many entrepreneurs needing only calves and seed lymph from an infected cow to manufacture crude versions of the vaccine. W. F. Elgin of the National Vaccine Establishment presented his slightly refined technique to the Conference of State and Provincial Boards of Health of North America. A tuberculosis-free calf, stomach shaved, would be bound to an operating table, where incisions would be made on its lower body. Glycerinated lymph from a previously inoculated calf was spread along the cuts. After a few days, the cuts would have scabbed or crusted over. The crust was softened with sterilized water and mixed with glycerin, which disinfected it, then stored hermetically-sealed in capillary tubes for later use.
At some point, the virus in use was no longer cowpox, but vaccinia. Scientists have not determined exactly when the change or mutation occurred, but the effects of vaccinia and cowpox virus as vaccine are nearly the same.
The virus is found in Europe, and mainly in the UK. Human cases today are very rare and most often contracted from domestic cats. The virus is not commonly found in cattle; the reservoir hosts for the virus are woodland rodents, particularly voles. From these rodents, domestic cats contract the virus. Symptoms in cats include lesions on the face, neck, forelimbs, and paws, and less commonly upper respiratory tract infections. Symptoms of infection with cowpox virus in humans are localized, pustular lesions generally found on the hands and limited to the site of introduction. The incubation period is 9 to 10 days. The virus is prevalent in late summer and autumn.
Kinepox.
Kinepox is an alternate term for the smallpox vaccine used in early 19th-century America. Popularized by Jenner in the late 1790s, kinepox was a far safer method for inoculating people against smallpox than the previous method, variolation, which had a 3% fatality rate.
In a famous letter to Meriwether Lewis in 1803, Thomas Jefferson instructed the Lewis and Clark expedition to "carry with you some matter of the kine-pox; inform those of them with whom you may be, of its efficacy as a preservative from the smallpox; & encourage them in the use of it..." Jefferson had developed an interest in protecting Native Americans from smallpox, having been aware of epidemics along the Missouri River during the previous century. A year before his special instructions to Lewis, Jefferson had persuaded a visiting delegation of North American Indian Chieftains to be vaccinated with kinepox during the winter of 1801-2. Unfortunately, Lewis never got the opportunity to use kinepox during the pair's expedition, as it had become inadvertently inactive — a common occurrence in a time before vaccines were stabilized with preservatives such as glycerol or kept at refrigeration temperatures.
Historical use.
After inoculation, vaccination using the cowpox virus became the primary defense against smallpox. After infection by the cowpox virus, the body (usually) gains the ability to recognize the similar smallpox virus from its antigens and is able to fight the smallpox disease much more efficiently.
The cowpox virus contains 186 thousand base pairs of DNA, which contains the information for about 187 genes. This makes cowpox one of the most complicated viruses known. Some 100 of these genes give instructions for key parts of the human immune system, giving a clue as to why the closely related smallpox is so lethal. The vaccinia virus now used for smallpox vaccination is sufficiently different from the cowpox virus found in the wild as to be considered a separate virus.
Prevention.
Today, the virus is found in Europe, mainly in the UK. Human cases are very rare (though in 2010 a laboratory worker contracted cowpox.)
and most often contracted from domestic cats. Human infections usually remain localized and self-limiting, but can become fatal in immunosuppressed patients. The virus is not commonly found in cattle; the reservoir hosts for the virus are woodland rodents, particularly voles. Domestic cats contract the virus from these rodents. Symptoms in cats include lesions on the face, neck, forelimbs, and paws, and, less commonly, upper respiratory tract infections.[8] Symptoms of infection with cowpox virus in humans are localized, pustular lesions generally found on the hands and limited to the site of introduction. The incubation period is 9 to 10 days. The virus is most prevalent in late summer and autumn.
Immunity to cowpox is gained when the smallpox vaccine is administered. Though the vaccine now uses vaccinia virus, the poxviruses are similar enough that the body becomes immune to both cow- and smallpox.

</doc>
<doc id="51304" url="http://en.wikipedia.org/wiki?curid=51304" title="White Rose">
White Rose

The White Rose (German: "die Weiße Rose") was a non-violent, intellectual resistance group in Nazi Germany, consisting of students from the University of Munich and their philosophy professor. The group became known for an anonymous leaflet and graffiti campaign, lasting from June 1942 until February 1943, that called for active opposition to dictator Adolf Hitler's regime.
The six most recognized members of the German resistance group were arrested by the Gestapo, tried for treason and beheaded in 1943. The text of their sixth leaflet was smuggled by Helmuth James Graf von Moltke out of Germany through Scandinavia to the United Kingdom, and in July 1943, copies of it were dropped over Germany by Allied planes, retitled "The Manifesto of the Students of Munich".
Another member, Hans Conrad Leipelt, who helped distribute Leaflet 6 in Hamburg, was executed on 29 January 1945, for his participation.
Today, the members of the White Rose are honoured in Germany amongst its greatest heroes, since they opposed the Third Reich in the face of almost certain death.
Background.
White Rose survivor Jürgen Wittenstein described what it was like to live in Hitler's Germany: "The government – or rather, the party – controlled everything: the news media, arms, police, the armed forces, the judiciary system, communications, travel, all levels of education from kindergarten to universities, all cultural and religious institutions. Political indoctrination started at a very early age, and continued by means of the Hitler Youth with the ultimate goal of complete mind control. Children were exhorted in school to denounce even their own parents for derogatory remarks about Hitler or Nazi ideology."
Members.
Members and actions.
Students from the University of Munich comprised the core of the White Rose: Hans Scholl, Sophie Scholl, Alex Schmorell, Willi Graf, Christoph Probst, Traute Lafrenz, Katharina Schüddekopf, Lieselotte (Lilo) Berndl, Jürgen Wittenstein, Marie-Luise Jahn, Falk Harnack, Hubert Furtwängler, Wilhelm Geyer, Manfred Eickemeyer, Josef Söhngen, Heinrich Guter, Heinrich Bollinger, Helmut Bauer, Harald Dohrn, Rudi Alt and later Wolfgang Jaeger. Most were in their early twenties. A professor of philosophy and musicology, Kurt Huber, was also associated with their cause. Wilhelm Geyer taught Alexander Schmorell how to make the tin templates used in the graffiti campaign. Eugen Grimminger of Stuttgart funded their operations. Grimminger's secretary Tilly Hahn contributed her own funds to the cause, and acted as go-between for Grimminger and the group in Munich. She frequently carried supplies such as envelopes, paper, and an additional duplicating machine from Stuttgart to Munich. In addition, a group of students in the city of Ulm distributed a number of the group's leaflets. Among this group were Sophie Scholl's childhood friend Susanne Hirzel and her teenage brother Hans Hirzel and Franz Josef Müller.
Between June 1942 and February 1943, the group prepared and distributed six leaflets, in which they called for the active opposition of the German people to Nazi oppression and tyranny. Huber wrote the final leaflet. A draft of a seventh leaflet, designed by Christoph Probst, was found in the possession of Hans Scholl at the time of his arrest by the Gestapo. While Sophie Scholl got rid of incriminating evidence on her person before being taken into custody, Hans did not do the same with Probst's leaflet draft or cigarette coupons given to him by Geyer, an act that cost Probst his life and nearly undid Geyer.
Hans did try to destroy the draft of the last leaflet by ripping it into pieces and stuffing into his mouth to try save Probst from detection but the Gestapo recovered enough to match with written, signed statements from Probst found later in Hans's apartment.
Influences and vision.
The White Rose was influenced by the German Youth Movement, of which Christoph Probst was a member. Hans Scholl was a member of the Hitler Youth until 1937, and Sophie was a member of the Bund Deutscher Mädel. Membership of both groups was compulsory for young Germans, although many—such as Willi Graf, Otl Aicher, and Heinz Brenner—refused to join. The ideas of Deutsche Jungenschaft vom 1.11.1929 (dj.1.11.) had strong influence on Hans Scholl and his colleagues. dj.1.11. was a youth group of the German Youth Movement, founded by Eberhard Koebel in 1929. Willi Graf was a member of Neudeutschland, a Catholic youth association, and the Grauer Orden.
The group was motivated by ethical and moral considerations. They came from various religious backgrounds. Willi and Katharina were devout Catholics. The Scholls, Lilo, and Falk were just as devoutly Lutheran. Alexander Schmorell was Orthodox, the grandson of a priest and eventually glorified as an Orthodox Christian saint. Traute adhered to the concepts of anthroposophy, while Eugen Grimminger considered himself Buddhist. Christoph Probst was baptized a Catholic shortly before his execution. His father Hermann was nominally a Catholic, but for some time studied Eastern thought and wisdom, the reason why his son Christoph was not baptized as a baby.
In summer 1942, several members of the White Rose had to serve for three months on the Russian front alongside many other male medical students from the University of Munich. There, they observed the horrors of war, saw beatings and other mistreatment of Jews by the Germans, and heard about the persecution of the Jews from reliable sources. Some witnessed atrocities of the war on the battlefield and against civilian populations in the East. Willi Graf saw the Warsaw and Łódź Ghettos and could not get the images of brutality out of his mind. 
Alexander Schmorell spoke perfect Russian and this allowed him to have better contact and understanding from the local Russians and other Slavic populations and their plight, supposed to be removed -as Jews were earlier- for their government Lebensraum project of ethnic cleansing. This Russian insight proved invaluable during their time there, and he could convey to his fellow White Rose members what was not understood or even heard by other Germans coming from the Eastern front.
The students returned in November 1942. They rejected fascism and militarism and believed in a federated Europe that adhered to principles of tolerance and justice.
By February 1943, the young friends sensed the implications of the reversal of fortune the Wehrmacht suffered at Stalingrad, which eventually led to Germany's defeat. As the brutality of the regime became more and more apparent, when deportations of Jews began, and the remaining few were forced to wear the yellow Star of David, when German atrocities in occupied Poland and Russia became known, and when the copies of Bishop Galen's sermon condemning the killing of inmates in insane asylums were circulated in secret, detachment gave way to the conviction something had to be done. It was not enough to keep to oneself one's beliefs, and ethical standards, but the time had come to act.
Origin.
In 1941 Hans Scholl read a copy of a sermon by an outspoken critic of the Nazi regime, Bishop August von Galen, decrying the euthanasia policies expressed in Action T4 (and extended that same year to the Nazi concentration camps by Action 14f13) which the Nazis maintained would protect the German gene pool. Horrified by the Nazi policies, Sophie obtained permission to reprint the sermon and distribute it at the University of Munich as the group's first leaflet prior to their formal organization.
Under Gestapo interrogation, Hans Scholl gave several explanations for the origin of the name "The White Rose," and suggested he may have chosen it while he was under the emotional influence of a 19th-century poem with the same name by German poet Clemens Brentano. Most scholars, as well as the German public, have taken this answer at face value. Earlier, before these Gestapo transcripts surfaced, Annette Dumbach and Jud Newborn speculated briefly that the origin might have come from a German novel "Die Weiße Rose"- "The White Rose," published in Berlin in 1929 and written by B. Traven, the German author of "The Treasure of the Sierra Madre". Dumbach and Newborn said there was a chance that Hans Scholl and Alex Schmorell had read this. They also wrote that the symbol of the white rose was intended to represent purity and innocence in the face of evil.
In February 2006, however, Dr. Jud Newborn authored an essay entitled, "Solving Mysteries: The Secret of 'The White Rose'," originally intended as an Afterword to his co-authored book. In this essay he argues that Hans Scholl's response to the Gestapo was intentionally misleading in order to protect Josef Söhngen, the anti-Nazi bookseller who had provided the White Rose members with a safe meeting place for the exchange of information and to receive occasional financial contributions. Söhngen kept a stash of banned books hidden in his store. Dr. Newborn also looked into the content of B. Traven's "The White Rose," arguing that the novel, banned by the Nazis in 1933, provided evidence of origin of the group's name.
In the same essay, Newborn also revealed information about Hans Scholl's 1937-1938 arrest and trial for participation in a youth movement banned the end of 1936– one he had joined in 1934, when he and other Ulm Hitler Youth members considered membership in this group and the Hitler Youth to be compatible. Hans Scholl was also accused of transgressing Paragraph 175, the anti-homosexuality law, because of a same-sex teen relationship dating back to 1934-1935, when Hans was only 16 years old. Newborn built this argument partially on the work of Eckard Holler, a sociologist specializing in the German Youth Movement, as well as on the Gestapo interrogation transcripts from the 1937-1938 arrest, and with reference to historian George Mosse's discussion of the homoerotic aspects of the German "bündisch" Youth Movement. As Mosse indicated, idealized romantic attachments among male youths was not uncommon in Germany, especially among members of the "bündisch" associations. Newborn argued that this experience led both Hans and Sophie to identify with the victims of the Nazi state, providing an explanation for why Hans and Sophie Scholl made the transformation from avid Hitler Youth leaders to passionate opponents of National Socialism.
Leaflets.
Quoting extensively from the Bible, Aristotle and Novalis, as well as Goethe and Schiller, they appealed to what they considered the German intelligentsia, believing that they would be intrinsically opposed to Nazism. These leaflets were left in telephone books in public phone booths, mailed to professors and students, and taken by courier to other universities for distribution. At first, the leaflets were sent out in mailings from cities in Bavaria and Austria, since the members believed that southern Germany would be more receptive to their anti-militarist message.
Alexander Schmorell, who penned the words the White Rose has become most famous for, became an Orthodox saint after his martyrdom. Most of the more practical material—calls to arms and statistics of murder—came from Alex's pen. Hans Scholl wrote in a characteristically high style, exhorting the German people to action on the grounds of philosophy and reason.
At the end of July 1942, some of the male students in the group were deployed to the Eastern Front for military service (acting as medics) during the academic break. In late autumn, the men returned, and the White Rose resumed its resistance activities. In January 1943, using a hand-operated duplicating machine, the group is thought to have produced between 6,000 and 9,000 copies of their fifth leaflet, "Appeal to all Germans!", which was distributed via courier runs to many cities (where they were mailed). Copies appeared in Stuttgart, Cologne, Vienna, Freiburg, Chemnitz, Hamburg, Innsbruck and Berlin. The fifth leaflet was composed by Hans Scholl with improvements by Huber. These leaflets warned that Hitler was leading Germany into the abyss; with the gathering might of the Allies, defeat was now certain. The reader was urged to "Support the resistance movement!" in the struggle for "freedom of speech, freedom of religion and protection of the individual citizen from the arbitrary action of criminal dictator-states". These were the principles that would form "the foundations of a new Europe".
The leaflets caused a sensation, and the Gestapo began an intensive search for the publishers. On the nights of the 3rd, 8th and 15 February 1943, the slogans "Freedom" and "Down with Hitler" appeared on the walls of the university and other buildings in Munich. Alexander Schmorell, Hans Scholl and Willi Graf had painted them with tar-based paint. (Similar graffiti that appeared in the surrounding area at this time was painted by imitators).
The shattering German defeat at Stalingrad at the beginning of February provided the occasion for the group's sixth leaflet, written by Huber. Headed "Fellow students!" (the now-iconic "Kommilitoninnen! Kommilitonen!"), it announced that the "day of reckoning" had come for "the most contemptible tyrant our people has ever endured." "The dead of Stalingrad adjure us!"
Shortly after the capture of the members of the White Rose, Leaflet No. 6 was smuggled out of Germany and later copied by the Allies and dropped from aircraft as propaganda over Nazi Germany.
Capture and trial.
On 18 February 1943, coincidentally the same day that Nazi propaganda minister Joseph Goebbels called on the German people to embrace total war in his Sportpalast speech, the Scholls brought a suitcase full of leaflets to the university. They hurriedly dropped stacks of copies in the empty corridors for students to find when they flooded out of lecture rooms. Leaving before the class break, the Scholls noticed that some copies remained in the suitcase and decided it would be a pity not to distribute them. They returned to the atrium and climbed the staircase to the top floor, and Sophie flung the last remaining leaflets into the air. This spontaneous action was observed by a maintenance man Jakub Schmied. The police were called and Hans and Sophie Scholl were taken into Gestapo custody. Sophie and Hans were interrogated by Gestapo interrogator Robert Mohr, who initially thought Sophie was innocent. However, after Hans confessed, Sophie assumed full responsibility in an attempt to protect other members of the White Rose. Despite this, the other active members were soon arrested, and the group and everyone associated with them were brought in for interrogation.
The Scholls and Probst were the first to stand trial before the "Volksgericht"—the People's Court that tried political offenses against the Nazi German state—on 22 February 1943. They were found guilty of treason and Roland Freisler, head judge of the court, sentenced them to death. The three were executed the same day by guillotine at Stadelheim Prison. All three were noted for the courage with which they faced their deaths, particularly Sophie, who remained firm despite intense interrogation. (Reports that she arrived at the trial with a broken leg from torture were false.) She said to Freisler during the trial, "You know as well as we do that the war is lost. Why are you so cowardly that you won't admit it?" When Hans was executed, he said "Let freedom live" as the blade fell.
The second White Rose trial took place on 19 April 1943. Only eleven had been indicted before this trial. At the last minute, the prosecutor added Traute Lafrenz (who was considered so dangerous that she was to have had a trial all to herself), Gisela Schertling and Katharina Schüddekopf. Others tried were Hans Hirzel, Susanne Hirzel, Franz Josef Müller, Heinrich Guter, Eugen Grimminger, Heinrich Bollinger, Helmut Bauer and Falk Harnack. None had an attorney. One was assigned after the women appeared in court with their friends. Prior to their deaths, several members of the White Rose believed that their execution would stir university students and other anti-war citizens into activism against Hitler and the war.
Huber had counted on the good services of his friend, attorney Justizrat Roder, a high-ranking Nazi. Roder had not bothered to visit Huber before the trial and had not read Huber's leaflet. Another attorney had carried out all the pre-trial paperwork. When Roder realized how damning the evidence was against Huber, he resigned. The junior attorney took over.
Grimminger initially was to receive the death sentence for funding their operations, but escaped with a sentence of ten years in a penitentiary.
The third White Rose trial was to have taken place on 20 April 1943 (Hitler's birthday), because Freisler anticipated death sentences for Wilhelm Geyer, Harald Dohrn, Josef Söhngen and Manfred Eickemeyer. He did not want too many death sentences at a single trial, and had scheduled those four for the next day. However, the evidence against them was lost, and the trial was postponed until 13 July 1943.
At that trial, Gisela Schertling—who had betrayed most of the friends, even fringe members like Gerhard Feuerle—changed her mind and recanted her testimony against all of them. Since Freisler did not preside over the third trial, the judge acquitted all but Söhngen (who got only six months in prison) for lack of evidence.
Alexander Schmorell and Kurt Huber were beheaded on 13 July 1943, and Willi Graf on 12 October 1943. Huber's widow was sent a bill for 600 marks (twice her husband's monthly salary) for "wear of the guillotine." Friends and colleagues of the White Rose, who had helped in the preparation and distribution of leaflets and in collecting money for the widow and young children of Probst, were sentenced to prison terms ranging from six months to ten years.
After her release for the sentence handed down on 19 April, Traute Lafrenz was rearrested. She spent the last year of the war in prison. Trials kept being postponed and moved to different locations because of Allied air raids. Her trial was finally set for April 1945, after which she probably would have been executed. Three days before the trial, however, the Allies liberated the town where she was held prisoner, thereby saving her life.
The White Rose had the last word. Their last leaflet was smuggled to the Allies, who edited it and air-dropped millions of copies over Germany. The members of the White Rose, especially Sophie, became icons of the new post-war Germany.
Commemoration.
With the fall of Nazi Germany, the White Rose came to represent opposition to tyranny in the German psyche and was lauded for acting without interest in personal power or self-aggrandizement. Their story became so well known that the composer Carl Orff claimed (falsely by some accounts) to his Allied interrogators that he was a founding member of the White Rose and was released. He was personally acquainted with Huber, but there is no evidence that Orff was ever involved in the movement.
On February 5, 2012, Alexander Schmorell was canonized as a New Martyr by the Orthodox Church.
The square where the central hall of Munich University is located has been named "Geschwister-Scholl-Platz" after Hans and Sophie Scholl; the square opposite to it is "Professor-Huber-Platz". Two large fountains are in front of the university, one on either side of Ludwigstraße. The fountain in front of the university is dedicated to Hans and Sophie Scholl. The other, across the street, is dedicated to Professor Huber. Many schools, streets, and other places across Germany are named in memory of the members of the White Rose.
One of Germany's leading literary prizes is called the "Geschwister Scholl" prize (the "Scholl Siblings" prize). Likewise, the asteroid 7571 Weisse Rose is named after the group.
The White Rose has also received artistic treatments, including the acclaimed opera "Weiße Rose" by Udo Zimmermann, "In memoriam: die weisse Rose" by Hans Werner Henze and "Kommilitonen!", an opera by Peter Maxwell Davies.
In the media.
The following is a non-exhaustive chronological account of some of the more notable treatments of the White Rose in media, book and artistic form.
"The New York Times" published articles on the first White Rose trials on 29 March 1943 and 25 April 1943, entitled "Nazis Execute 3 Munich Students For Writing Anti-Hitler Pamphlets" and "Germans Clinging to Victory Hope in Fear of Reprisals," respectively. Though they did not correctly record all of the information about the resistance, the trials, and the execution, they were the first acknowledgement of the White Rose in the United States.
Beginning in the 1970s, three film accounts of the White Rose resistance were produced. The first was a film financed by the Bavarian state government entitled "Das Versprechen" ("The Promise") and released in the 1970s. The film is not well known outside Germany, and to some extent even within the country. It was particularly notable in that unlike most films, it showed the White Rose from its inception and how it progressed. In 1982, Percy Adlon's "Fünf letzte Tage" ("The Last Five Days") presented Lena Stolze as Sophie in her last days from the point of view of her cellmate Else Gebel. In the same year, Stolze repeated the role in Michael Verhoeven's "Die Weiße Rose" ("The White Rose").
A book, "Sophie Scholl and the White Rose", was published in English in February 2006. An account by Annette Dumbach and Dr. Jud Newborn tells the story behind the film "Sophie Scholl: The Final Days", focusing on the White Rose movement while setting the group's resistance in the broader context of German culture and politics and other forms of resistance during the Nazi era.
As mentioned earlier, Udo Zimmermann composed a chamber opera about the White Rose ("Weiße Rose") in 1986. Premiering in Hamburg, it went on to earn acclaim and a series of international performances.
Lillian Garrett-Groag's play, "The White Rose", premiered at the Old Globe Theatre in 1991. Several plays have also been written by teachers in the USA for performance by students.
In "Fatherland", an alternate history novel by Robert Harris, there is passing reference to the White Rose still remaining active in supposedly Nazi-ruled Germany in 1964.
In an extended German national TV competition held in the autumn of 2003 to choose "the ten greatest Germans of all time" (ZDF TV), Germans under the age of 40 placed Hans and Sophie Scholl in fourth place, selecting them over Bach, Goethe, Gutenberg, Willy Brandt, Bismarck, and Albert Einstein. Not long before, women readers of the mass-circulation magazine "Brigitte" had voted Sophie Scholl as "the greatest woman of the twentieth century".
In 2003, a group of students at the University of Texas at Austin, Texas established "The White Rose Society" dedicated to Holocaust remembrance and genocide awareness. Every April, the White Rose Society hands out 10,000 white roses on campus, representing the approximate number of people killed in a single day at Auschwitz. The date corresponds with Yom Hashoah, Holocaust Memorial Day. The group organizes performances of "The Rose of Treason", a play about the White Rose, and has rights to show the movie "Sophie Scholl – Die letzten Tage" ("Sophie Scholl: The Final Days"). The White Rose Society is affiliated with and the Anti-Defamation League.
In February 2005, a movie about Sophie Scholl's last days, "Sophie Scholl – Die letzten Tage" ("Sophie Scholl: The Final Days"), featuring actress Julia Jentsch as Sophie, was released. Drawing on interviews with survivors and transcripts that had remained hidden in East German archives until 1990, it was nominated for an Academy Award for Best Foreign Language Film in January 2006. An American film project about the White Rose continues to be under development by co-author Jud Newborn of the 2006 book "Sophie Scholl and the White Rose."
White Rose has inspired many people around the world, including many anti-war activists in recent years. Scattered throughout 2007-2008, 5 hoax pipe bombs were placed at various military recruitment centers with the words "Die Weisse Rose" written upon them.
In February 2009, a biography of Sophie Scholl, "Sophie Scholl: The Real Story of the Woman Who Defied Hitler", was published in English by the History Press. The book, by the Oxford-educated British historian Frank McDonough.
The UK-based genocide prevention student network Aegis Students uses a white rose as their symbol in commemoration of the White Rose movement. There are numerous study guides to the White Rose, notably one available from the University of Minnesota's Holocaust Center.
In 2009, Dan Fesperman published a novel entitled "The Arms Maker of Berlin" in which activities by real and fictional White Rose characters play a significant role in the story.
In 2011, a documentary film by André Bossuroy addressing the memory of the victims of Nazism and of Stalinism , with the support from the Fondation Hippocrène and from the EACEA Agency of the European Commission (programme Europe for Citizens – An active European remembrance), RTBF, VRT. Four young Europeans meet with historians and witnesses of our past… They investigate the events of the Second World War in Germany (the student movement of the White Rose in Munich), in France (the Vel' d'Hiv Roundup in Paris, the resistance in Vercors) and in Russia (Katyn Forest massacre). They examine the impact of these events; curious as to how the European peoples are creating their identities today.
Further reading.
Primary Source Materials in English Translation:

</doc>
<doc id="51306" url="http://en.wikipedia.org/wiki?curid=51306" title="Battle of Lake Benacus">
Battle of Lake Benacus

The Battle of Lake Benacus was fought along the banks of Lake Garda in northern Italy, which was known to the Romans as Benacus, in 268 or early 269 AD, between the army under the command of the Roman Emperor Claudius II and the Germanic tribes of the Alamanni and Juthungi.
Background.
Ιn 268, the Alamanni, who had been making incursions into Roman territory since the reign of Marcus Aurelius, had broken through the Roman frontier at the Danube and crossed the Alps. The power struggles in Mediolanum due to Aureolus' revolt, the murder of Emperor Gallienus and the resulting confrontation between Aureolus and Claudius, who had been nominated as emperor by Gallienus on his death bed, forced the Romans to denude the frontier of troops. Having defeated and killed Aureolus in the Siege of Mediolanum Claudius led his army, together with the remants of Aureolus' force, north to confront the Germans.
Battle and aftermath.
Details of the battle are unknown but future emperor Aurelian certainly played a part. After what was described as a complete victory, Claudius assumed the title Germanicus Maximus. Much of the German army was slaughtered on the field with the remainder retreating beyond the bounds of the empire. Claudius returned to Rome after the battle to attend to affairs of state. The Alemanni returned to Italy in 271 and won a victory against Emperor Aurelian at the Battle of Placentia before their ultimate defeat in the Battle of Fano.

</doc>
<doc id="51307" url="http://en.wikipedia.org/wiki?curid=51307" title="Miklós Horthy">
Miklós Horthy

Miklós Horthy de Nagybánya (Hungarian: "Vitéz nagybányai Horthy Miklós"; ]; English: Nicholas Horthy German: "Ritter Nikolaus Horthy von Nagybánya"; 18 June 1868 – 9 February 1957) was a Hungarian admiral and statesman who served as Regent of the Kingdom of Hungary between World Wars I and II and throughout most of World War II, from 1 March 1920 to 15 October 1944. He was styled "His Serene Highness the Regent of the Kingdom of Hungary" (Hungarian: "Ő Főméltósága a Magyar Királyság Kormányzója").
Horthy started his career as a Frigate Lieutenant in the Austro-Hungarian Navy in 1896 and attained the Admiralty in 1918. He served in the Otranto Raid and at the Battle of the Strait of Otranto and became Commander-in-Chief of the "k.u.k." Navy in the last year of the First World War. In 1919, following a series of revolutions and interventions in Hungary involving the short-lived Hungarian Soviet Republic, Romania, Czechoslovakia, and Yugoslavia, Horthy returned to Budapest with the National Army and established a regency government.
Horthy led a conservative nationalist government through the interwar period, banning the communist party and pursuing an irredentist foreign policy in the face of the Treaty of Trianon. Charles IV unsuccessfully attempted to regain his throne twice from Horthy until, in 1921, the parliament formally nullified the Pragmatic Sanction, effectively dethroning the Habsburgs.
In the later 1930s, Horthy's foreign policy led him into an alliance with Nazi Germany. With Adolf Hitler's support, Horthy was able to reclaim ethnically Hungarian lands lost after World War I on four separate occasions. Under Horthy's leadership Hungary participated in the invasion of the Soviet Union and of Yugoslavia. However, Horthy's reticence to contribute to the German war effort and to the deportation of Hungarian Jews, coupled with attempts to strike a secret deal with the Allies, eventually led the Germans to invade and take control of the country in March 1944. In October 1944, Horthy announced that Hungary would surrender and withdraw from the Axis. He was forced to resign, placed under arrest and taken to Bavaria. At the end of the war, he came under the custody of American troops.
After appearing as a witness at the Nuremberg war-crimes trials in 1948, Horthy settled and lived out his remaining years in exile in Portugal. His memoirs, "Ein Leben für Ungarn" ("A Life for Hungary"), were first published in 1953. He remains a very controversial and divisive historical figure in contemporary Hungary.
Early life and naval career.
Miklós Horthy was born at Kenderes an old Calvinist noble family descended from István Horti, ennobled by King Ferdinand II in 1635. Miklós Horthy, Sr. (1830—1904), a member of the House of Magnates and lord of a 1,500 acre estate, had wed Paula Halassy (1839—1895) in 1857. Miklós was the fourth of their eight children; after István, Zoltán, and Paula and before Erzsébet, Szabolcs, Jenő, and Jenő.
Horthy entered the Austro-Hungarian naval academy at Fiume (now Rijeka, Croatia) at age 14. Because the naval academy's official language was German, for the rest of his life Horthy spoke Hungarian with a slight, but noticeable, Austro-German accent. He also spoke Italian, Croatian, English, and French.
As a young man, Horthy travelled around the world and served as a diplomat for the Austro-Hungarian Empire in Turkey and other countries. Horthy married Magdolna Purgly in Arad in 1901. They had four children: Magdolna (1902), Paula (1903), István (1904) and Miklós (1907). From 1911 until 1914 he was a naval aide-de-camp to Emperor Franz Joseph, for whom he had a great respect.
At the beginning of the war Horthy was commanding the pre-dreadnought battleship . In 1915 he earned a reputation for boldness while commanding the new light cruiser SMS "Novara". He planned the 1917 attack on the Otranto Barrage, which resulted in the largest naval engagement of the war in the Adriatic. A consolidated British, French, and, Italian Mediterranean fleet met with the Austro-Hungarian force. Despite the numerical superiority of the Entente fleet, the Austrian force victoriously emerged from the battle. The Austrian fleet remained relatively unscathed, however Horthy was wounded. After the February 1918 Cattaro mutiny, Emperor Charles selected Horthy over many more senior commanders as the new Commander in Chief of the Imperial Fleet in March 1918. In June, Horthy planned another attack on Otranto, and in a departure from the cautious strategy of his predecessors, he committed the empire's battleships to the mission. While sailing through the night, the dreadnought met Italian MAS torpedo boats and was sunk, causing Horthy to abort the mission. He managed however to preserve the rest of the empire's fleet in being until he was ordered by Emperor Charles to surrender it to the new State of Slovenes, Croats and Serbs on 31 October.
The end of the war saw Hungary turned into a landlocked nation, and hence the new government had little need for Horthy's services. He retired with his family to his private estate at Kenderes, but his role as a Hungarian leader was far from over.
Interwar period, 1919–1939.
Commander of the National Army.
Two national traumas immediately following the First World War profoundly shaped the spirit and future of the Hungarian nation. The first was the loss, as dictated by the Entente powers, of large portions of Hungarian territory that had bordered other countries. These were lands which had been Hungary's as part of the Austro-Hungarian Empire; they were ceded to the nations of Czechoslovakia, Romania, Austria and Yugoslavia. The excisions, eventually ratified in the Treaty of Trianon at Versailles, cost Hungary two-thirds of its territory and one-third of its native Hungarian speakers, and dealt the population a terrible psychological blow. The second trauma in some sense sprang from the first: in March 1919, after the first proto-democratic efforts at government in Hungary faltered, Communist Béla Kun seized power in the capital of Budapest.
Kun and his colleagues proclaimed the Hungarian Soviet Republic, and promised the restoration of Hungary's former grandeur. Instead, his efforts at reconquest failed, and Hungarians were treated to a Soviet-style repression in the form of armed gangs who intimidated or murdered enemies of the regime. This period of violence came to be known as the Red Terror. Tibor Szamuely, a close collaborator of Bela Kun, even boasted that, "Terror is the principal weapon of our regime." Figures vary, but one generally accepted number of victims of the Red Terror is around 500 killed.
Within weeks of his coup, Kun's popularity plummeted. On 30 May 1919, anti-Communist politicians formed a counter-revolutionary government in the southern city of Szeged, occupied by French forces at the time. There, Gyula Károlyi asked former admiral Horthy, still considered a war hero, to be the Minister of War in the new government and take command of a counter-revolutionary force which would be named the National Army (Hungarian: "Nemzeti Hadsereg"). Horthy consented, and arrived in Szeged on 6 June. Soon after, because of orders from the Entente, the cabinet was reformed, and Horthy was not given a seat in it. Undaunted, Horthy managed to retain control of the National Army by detaching the Army command from the War ministry.
On 6 August French-supported Romanian forces entered Budapest. The Communist government collapsed and its leaders fled. In retaliation for the Red Terror, reactionary crews now exacted revenge in a two-year wave of violent repression known today as the White Terror. These reprisals were organized and carried out by officers of Horthy's National Army, particularly Pál Prónay, Gyula Ostenburg-Moravek and Iván Héjjas. Their victims were primarily Communists, Social Democrats, and Jews. Most Hungarian Jews were not supporters of the Bolsheviks, but much of the leadership of the Hungarian Soviet Republic had been young Jewish intellectuals, and anger about the Communist revolution easily translated into anti-Semitic hostility.
In Budapest, Prónay installed his unit in Hotel Britannia, where the group swelled to battalion size. Their program of vicious attacks continued; they planned a city-wide pogrom until Horthy found out and put a stop to it. In his diary, Prónay reported that Horthy
..."reproached me for the many Jewish corpses found in the various parts of the country, especially in the Transdanubia. This, he emphasized, gave the foreign press extra ammunitions against us. He told me that we should stop harassing small Jews; instead, we should kill some big (Kun government) Jews such as Somogyi or Vazsonyi – these people deserve punishment much more... in vain, I tried to convince him that the liberal papers would be against us anyway, and it did not matter that we killed only one Jew or we killed them all"...
Horthy's liability for Prónay's excesses is controversial. On several occasions, Horthy reached out to stop Prónay from a particularly excessive burst of anti-Jewish cruelty and the Jews of Pest went on record absolving Horthy of the White Terror as early as the fall of 1919, when they released a statement disavowing the Kun revolution, and blaming the terror on a few units within the National Army. Horthy has never been found to have personally engaged in White Terror atrocities. But his American biographer, Thomas Sakmyster, concluded that he "tacitly supported the right wing officer detachments" who carried out the terror; Horthy called them "my best men". The admiral also had practical reasons for overlooking the terror his officers wrought: he needed the dedicated officers to stabilize and reclaim Hungary. Nevertheless, it was at least another year before the terror died down. In the summer of 1920, Horthy's government took measures to rein in and eventually disperse the reactionary battalions. Prónay managed to undermine these measures, but only for a short time. Prónay was put on trial for extorting a wealthy Jewish politician, and for "insulting the President of the Parliament" by trying to cover up the extortion. Found guilty on both charges, Prónay was now a liability and an embarrassment. His command was revoked, and he was denounced as a common criminal on the floor of the Hungarian parliament.
After serving short jail sentences, Prónay tried to convince Horthy to restore his battalion command. The Prónay Battalion lingered for a few months more under the command of a junior officer, but the government officially dissolved the unit in January 1922 and expelled its members from the army. Prónay entered politics as a member of the government's right-wing opposition. In the 1930s, he sought and failed to emulate the Nazis by generating a Hungarian fascist mass movement. In 1932, he was charged with incitement, sentenced to six months in prison and stripped of his rank of lieutenant colonel. Prónay would support the pro-Nazi Arrow Cross and lead attacks on Jews before being killed by Soviet troops sometime during or after the siege of Budapest.
Precisely how much Horthy knew or approved of the White Terror is not known. Horthy himself declined to apologize for the savagery of his officer detachments, writing later: "I have no reason to gloss over deeds of injustice and atrocities committed when an iron broom alone could sweep the country clean." He endorsed Edgar von Schmidt-Pauli's poetic justification of the White reprisals ("Hell let loose on earth cannot be subdued by the beating of angels' wings") remarking, "the Communists in Hungary, willing disciples of the Russian Bolshevists, had indeed let hell loose."
The International Committee of the Red Cross (ICRC) in an internal report by delegate George Burnier, said in April 1920:
"There are two distinct military organizations in Hungary: the national army and a kind of civil guard which was formed when the communist régime fell. It is the latter which has been responsible for all the reprehensible acts committed. The Government managed to regain control of these organizations only a few weeks ago. They are now well-disciplined and collaborate with the municipal police forces".
This deep hostility and fear towards Communism would be the more lasting legacy of Kun's abortive revolution: a conviction shared by Horthy and his country's ruling elite that would help drive Hungary into what might have been a fatal alliance with Adolf Hitler.
 The Romanian army retreated from Budapest on 14 November, leaving Horthy to enter the city, where in a fiery speech he accused the capital's citizens of betraying Hungary by supporting Bolshevism.
..."The nation of the Hungarians loved and admired Budapest, which became its polluter in the last years. Here, on the banks of the Danube, I arraign her. This city has disowned her thousand years of tradition, she has dragged the Holy Crown and the national colours in the dust, she has clothed herself in red rags. The finest of the nation she threw into dungeons or drove into exile. She laid in ruin our property and wasted our wealth. Yet the nearer we approached to this city, the more rapidly did the ice in our hearts melt. We are now ready to forgive her.""
Following the orders of the Entente, Romanian troops finally evacuated Hungary on 25 February 1920.
Regent.
On 1 March 1920, the National Assembly of Hungary re-established the Kingdom of Hungary. However, it was apparent that the Entente powers would not accept any return of King Charles IV (Karoly IV of Hungary) from exile. Instead, with National Army officers controlling the parliament building, the assembly voted to install Horthy as Regent; he defeated Count Albert Apponyi by a vote of 131 to 7.
Bishop Ottokár Prohászka then led a small delegation to meet Horthy, announcing, "Hungary's Parliament has elected you Regent! Would it please you to accept the office of Regent of Hungary?" To their astonishment, Horthy declined, unless the powers of the office were expanded. As Horthy stalled, the politicians folded, and granted him "the general prerogatives of the King, with the exception of the right to name titles of nobility and of the patronage of the Church." The prerogatives he was given included the power to appoint and dismiss prime ministers, to convene and dissolve parliament, and to command the armed forces. With those sweeping powers guaranteed, Horthy took the oath of office. (Charles I did try to regain his throne twice; see "Charles I of Austria's attempts to retake the throne of Hungary" for more details.)
The Hungarian state was legally a kingdom, but it had no king, as the Entente powers would not have tolerated any return of the Habsburgs. The country retained its parliamentary system following the dissolution of Austria-Hungary, with a Prime Minister appointed as head of government. As head of state, Horthy retained significant influence through his constitutional powers and the loyalty of his ministers to the crown. Although his involvement in drafting legislation was minuscule, he nevertheless had the ability to ensure that laws passed by the Hungarian parliament conformed to his political preferences.
Seeking redress for Trianon.
The first decade of Horthy's reign was primarily consumed by stabilizing the Hungarian political system and economy. Horthy's chief partner in these efforts was his prime minister, István Bethlen. The British political and economical support for the commonly known anglophile Horthy played a significant role in the stabilization and consolidation of the early Horthy era in the Kingdom of Hungary.
Bethlen sought to stabilize the economy while building alliances with weaker nations which could advance Hungary's cause. That cause was, primarily, reversing the losses of the Treaty of Trianon. The humiliations of Trianon continued to occupy the central place in Hungarian foreign policy, and in the popular imagination; the indignant anti-Trianon slogan "Nem, nem soha!" ("No, no never!") became a ubiquitous motto of Hungarian outrage. When in 1927 the British newspaper magnate Lord Rothermere denounced, in the pages of his "Daily Mail", the partitions ratified at Trianon, an official letter of gratitude was eagerly signed by 1.2 "million" Hungarians.
But Hungary's stability was precarious, and the Great Depression derailed much of Bethlen's economic balance. Horthy replaced him with an old reactionary confederate from his Szeged days: Gyula Gömbös. Gömbös was an outspoken anti-Semite and a budding fascist. And although he agreed to Horthy's demands that he temper his anti-Jewish rhetoric and work amicably with Hungary's large Jewish professional class, Gömbös's tenure began swinging Hungary's political mood powerfully rightward. He strengthened Hungary's ties to Benito Mussolini's Italian fascist state. And most fatefully, when Adolf Hitler took power in Germany in 1933, he found in Gömbös an admiring and obliging colleague. John Gunther in 1936 stated that Horthy,
though sublimely reactionary as far as social or economic ideas are concerned, is in effect the guardian of constitutionalism and the vestigial democracy that remains in the country, because it is largely his influence that prevents [Gömbös] from abolishing parliament and setting up overt dictatorial rule.
Gömbös rescued the failing economy by securing trade guarantees from Germany – a strategy which positioned Germany as Hungary's primary trading partner and tied Hungary's future even more tightly to Hitler's. He also assured Hitler that Hungary would quickly become a one-party state modelled on the Nazi party control of Germany. Gömbös died in 1936, before he realized his most extreme goals, but he left his nation headed into firm partnership with the German dictator.
World War II and the Holocaust.
Uneasy alliance.
Hungary now entered into an intricate dance of influence with Hitler's regime, and Horthy began to play a greater and more public role in navigating Hungary along this dangerous path.
For Horthy, Hitler served as a bulwark against Soviet encroachment or invasion. Horthy was, in the eyes of observers, obsessed with the Communist threat. One American diplomat remarked that Horthy's anti-Communist tirades were so common and ferocious that diplomats "discounted it as a phobia."
Horthy clearly saw his country as trapped between two stronger powers, both of them dangerous; evidently he considered Hitler to be the more manageable of the two. Hitler was also able to wield great influence over Hungary not only as the country's major trading partner; he also fed several of Horthy's key ambitions: maintaining Hungarian sovereignty and satisfying the nationwide yearning to reclaim former Hungarian lands. Horthy's strategy was one of cautious, sometimes even grudging, alliance. How the regent granted or resisted Hitler's demands, especially with regard to Hungarian military action and the treatment of Hungary's Jews, remains the central topic by which his career has been judged.
Horthy's relationship with Hitler was, by his own account, a tense one – largely due, he said, to his unwillingness to bend his nation's policies to the German dictator's desires. On a state visit by Horthy to Germany in August 1938, Hitler asked Horthy for troops and materiel to participate in Germany's planned invasion of Czechoslovakia. In exchange, Horthy later reported, "He gave me to understand that as a reward we should be allowed to keep the territory we had invaded." Horthy said he declined, insisting to Hitler that Hungary's claims on the disputed lands should be settled by peaceful means.
Three months later, after the Munich Agreement put control of Czechoslovakia's Sudeten in Hitler's hands, Hitler allowed Hungary to annex nearly one-fourth of Slovakia. Horthy enthusiastically rode into the re-acquired territory (which was according to 1910 census predominantly populated by Hungarians to about 88%) at the head of his troops, greeted by emotional ethnic Hungarians: "As I passed along the roads, people embraced one another, fell upon their knees, and wept with joy because liberation had come to them at last, without war, without bloodshed." But as "peaceful" as this annexation was, and as just as it may have seemed to many Hungarians, it was a dividend of Hitler's brinksmanship and threats of war, in which Hungary was now inextricably complicit.
Hungary was now committed to the Axis agenda: on 24 February 1939, it joined the Anti-Comintern pact, and on 11 April withdrew from the League of Nations. American journalists began to refer to Hungary as "the jackal of Europe."
This combination of menace and reward drifted Hungary closer to a Nazi client state. In March 1939, when Hitler took what remained of Czechoslovakia by force, Hungary was allowed to annex Carpathian Ruthenia, as well after a conflict with the First Slovak Republic during the Slovak-Hungarian War Hungary gained further territories. Regarding Transcarpathia, minor conflicts had occurred between Ukrainian nationalist groups and the Hungarian military before it was secured.
In August 1940, Hitler intervened on Hungary's behalf once again, taking Northern Transylvania away from Romania, and awarding it to Hungary.
But in spite of their cooperation with the Nazi regime, Horthy and his government would be better described as "conservative authoritarian" than "fascist". Certainly Horthy was as hostile to the home-grown fascist and ultra-nationalist movements which emerged in Hungary between the wars (particularly the Arrow Cross Party) as he was to Communism. The Arrow Cross leader, Ferenc Szálasi, was repeatedly imprisoned at Horthy's command.
John F. Montgomery, who served in Budapest as U.S. ambassador from 1933 to 1941, openly admired this side of Horthy's character and reported the following incident in his memoir: in March 1939, Arrow Cross supporters disrupted a performance at the Budapest opera house by chanting "Justice for Szálasi!" loud enough for the regent to hear. A fight broke out, and when Montgomery went to take a closer look, he discovered that
..."two or three men were on the floor and he [Horthy] had another by the throat, slapping his face and shouting what I learned afterward was: "So you would betray your country, would you?" The Regent was alone, but he had the situation in hand... The whole incident was typical not only of the Regent's deep hatred of alien doctrine, but of the kind of man he is. Although he was around seventy two years of age, it did not occur to him to ask for help; he went right ahead like a skipper with a mutiny on his hands".
And yet, by the time of this episode, Horthy had allowed his government to give in to Nazi demands that the Hungarians enact laws restricting the lives of the country's Jews. The first Hungarian anti-Jewish Law, in 1938, limited the number of Jews in the professions, the government and commerce to twenty percent, and the second reduced it to five percent the following year; 250,000 Hungarian Jews lost their jobs as a result. A "Third Jewish Law" of August 1941 prohibited Jews from marrying non-Jews, and defined anyone having two Jewish grandparents as "racially Jewish." A Jewish man who had non-marital sex with a "decent non-Jewish woman resident in Hungary" could be sentenced to three years in prison.
Horthy's personal views on Jews and their role in Hungarian society are the subject of some debate. In an October 1940 letter to prime minister Pál Teleki, Horthy echoed a widespread national sentiment: that Jews enjoyed too much success in commerce, the professions, and industry – success which needed to be curtailed:
"As regards the Jewish problem, I have been an anti-Semite throughout my life. I have never had contact with Jews. I have considered it intolerable that here in Hungary everything, every factory, bank, large fortune, business, theatre, press, commerce, etc. should be in Jewish hands, and that the Jew should be the image reflected of Hungary, especially abroad. Since, however, one of the most important tasks of the government is to raise the standard of living, i.e., we have to acquire wealth, it is impossible, in a year or two, to replace the Jews, who have everything in their hands, and to replace them with incompetent, unworthy, mostly big-mouthed elements, for we should become bankrupt. This requires a generation at least".
War.
The Kingdom of Hungary was gradually drawn into the war itself. In 1939 and 1940, volunteer units fought in Finland's Winter War. In April 1941, Hungary became, in effect, a member of the Axis. Hungary permitted Hitler to send troops across Hungarian territory for the invasion of Yugoslavia and ultimately sent its own troops to claim its share of the dismembered Kingdom of Yugoslavia. Prime Minister Pál Teleki, horrified that he had failed to prevent this collusion with the Nazis against a former ally, committed suicide.
In June 1941, the Hungarian government finally yielded to Hitler's demands that the nation contribute to the Axis war effort. On 27 June, Hungary became part of Operation Barbarossa and declared war on the Soviet Union. The Hungarians sent in troops and materiel only four days after Hitler began his invasion of the Soviet Union.
Eighteen months later, more poorly equipped and less motivated than their German allies, the 200,000 troops of the Hungarian Second Army ended up holding the front on the Don River west of Stalingrad.
The first massacre of Jewish people from Hungarian territory took place in August 1941, when government officials ordered the deportation of Jews without Hungarian citizenship (principally refugees from other Nazi-occupied countries) to Ukraine. Roughly 18,000–20,000 of these deportees were slaughtered by Friedrich Jeckeln and his SS troops; only 2,000–3,000 survived. These killings are known as the Kamianets-Podilskyi Massacre. This event, in which the slaughter of Jews numbered for the first time in the tens of thousands, is considered the first large-scale massacre of the Holocaust. Because of the objections of Hungary's leadership, the deportations were halted.
By early 1942, Horthy was already seeking to put some distance between himself and Hitler's regime. That March, he dismissed the pro-German prime minister László Bárdossy, and replaced him with Miklós Kállay, a moderate whom Horthy expected to loosen Hungary's ties to Germany.
In September 1942, personal tragedy struck the Hungarian Regent. 37-year-old István Horthy, Horthy's eldest son, was killed. István Horthy was the Deputy Regent of Hungary and a Flight Lieutenant in the reserves, 1/1 Fighter Squadron of the Royal Hungarian Air Force. He was killed when his Hawk ("Héja") fighter crashed at an air field near Ilovskoye.
Then, in January 1943, Hungary's enthusiasm for the war effort, never especially high, suffered a tremendous blow. The Soviet army, in the full momentum of its triumphant turnaround after the Battle of Stalingrad, punched through Romanian troops at a bend in the Don River and virtually obliterated the Second Hungarian Army in a few days' fighting. In this single action, Hungarian combat fatalities jumped by 80,000. Jew and non-Jew suffered together in this defeat as the Hungarian troops had been accompanied by some 40,000 Jews and political prisoners in forced-labour units whose job had been to clear minefields.
German officials blamed Hungary's Jews for the nation's "defeatist attitude." In the wake of the Don Bend disaster, Hitler demanded at an April 1943 meeting that Horthy punish the 800,000 Jews still living in Hungary, who according to Hitler were responsible for this defeat. In response Horthy and his government supplied 10,000 Jewish deportees for labour battalions. However with the growing awareness that the Allies might well win the war, it became more expedient not to comply with further German requests. Cautiously, the Hungarian government began to explore contacts with the Allies in hopes of negotiating a surrender.
Occupation.
By 1944, the Axis was losing the war, and the Red Army was at Hungary's borders. Fearing that the Soviets would overrun the country, Kállay, with Horthy's approval, put out numerous feelers to the Allies. He even promised to surrender unconditionally to them once they reached Hungarian territory. An enraged Hitler summoned Horthy to a conference in Klessheim (today in Austria). He pressured Horthy to make greater contributions to the war effort, and again commanded him to assist in the killing of more of Hungary's Jews. Horthy now permitted the deportation of a large number of Jews (the generally accepted figure is 100,000), but would not go further.
The conference was a ruse. As Horthy was returning home on 19 March the Wehrmacht invaded and occupied Hungary. Horthy was told he could only stay in office if he fired Kállay and appointed a new government that would fully cooperate with Hitler and his plenipotentiary in Budapest, Edmund Veesenmayer. Knowing the likely alternative was a gauleiter who would treat Hungary in the same manner as the other countries under Nazi occupation, Horthy acquiesced and appointed his ambassador to Germany, General Döme Sztójay, as prime minister. The Germans originally wanted Horthy to reappoint Béla Imrédy (who had been prime minister from 1938 to 1939), but Horthy had enough influence to get Veesenmayer to accept Sztójay instead. Contrary to Horthy's hopes, Sztójay's government eagerly proceeded to participate in the Holocaust.
The chief agents of this collaboration were Andor Jaross, the Minister of the Interior, and his two rabidly anti-Semitic state secretaries, László Endre and László Baky (later to be known as the "Deportation Trio"). On 9 April, Prime Minister Sztójay and the Germans obligated Hungary to place 300,000 Jewish people at the "disposal" of the Reich—in effect, sentencing most of Hungary's remaining Jews to death. Five days later, on 14 April Endre, Baky, and SS Colonel Adolf Eichmann commenced the deportation of the remaining Hungarian Jews. The Yellow Star and Ghettoization laws, and deportation were accomplished in less than 8 weeks with the help of the new Hungarian government and authorities, particularly the gendarmerie ("csendőrség"). The deportation of Hungarian Jews to Auschwitz began on 15 May 1944 and continued at a rate of 12,000 a day until 9 July.
Upon learning about the deportations, Horthy wrote the following letter to the Prime Minister:
"Dear Sztójay: I was aware that the Government in the given forced situation has to take many steps that I do not consider correct, and for which I can not take responsibility. Among these matters is the handling of the Jewish question in a manner that does not correspond to the Hungarian mentality, Hungarian conditions, and, for the matter, Hungarian interests. It is clear to everyone that what among these were done by Germans or by the insistence of the Germans was not in my power to prevent, so in these matters I was forced into passivity. As such, I was not informed in advance, or I am not fully informed now, however, I have heard recently that in many cases in inhumaneness and brutality we exceeded the Germans. I demand that the handling of the Jewish affairs in the Ministry of Interior be taken out of the hands of Deputy Minister László Endre. Further more, László Baky's assignment to the management of the police forces should be terminated as soon as possible".
Just before the deportations began, two Slovakian Jewish prisoners, Rudolf Vrba and Alfréd Wetzler, escaped from Auschwitz and passed details of what was happening inside the camps to officials in Slovakia. This document, known as the Vrba-Wetzler Report, was quickly translated into German and passed among Jewish groups and then to Allied officials. Details from the report were broadcast by the BBC on 15 June and printed in "The New York Times" on 20 June. World leaders, including Pope Pius XII (25 June), President Franklin D. Roosevelt on 26 June, and King Gustaf V of Sweden on 30 June, subsequently pleaded with Horthy to use his influence to stop the deportations. Roosevelt specifically threatened military retaliation if the transports were not ceased. On 2 July, Allied bombers executed the heaviest bombings inflicted on Hungary during the war. Hungarian radio accused Jews of guiding the bombers to their targets with radio transmissions and light signals, but on 7 July Horthy at last ordered the transports halted. By that time, 437,000 Jews had been sent to Auschwitz, most of them to their deaths. Horthy was informed about the number of the deported Jews some days later: "approximately 400,000". By many estimates, one of every three people murdered at Auschwitz between May and July 1944 was a Hungarian Jew.
There remains some uncertainty over how much Horthy could have known about the number of Hungarian Jews being deported, their destination, and their intended fate – and when he knew it as well as what he could have done about it. According to historian Péter Sipos, the Hungarian government had already known about the Jewish genocide since 1943. Some historians have argued that Horthy believed that the Jews were being sent to the camps to work, and that they would be returned to Hungary after the war. Horthy himself could not have been clearer in his memoirs: "Not before August," he wrote, "did secret information reach me of the horrible truth about the extermination camps." The Vrba-Wetzler statement is believed to have been passed to Hungarian Zionist leader Rudolf Kastner no later than 28 April 1944, however, Kastner did not make it public. He made an agreement with the SS to remain silent in order to save the Jews who escaped on the Kastner train. The "Kastner train" left Budapest on the 30 June 1944.
On 15 July 1944 Anne McCormick, a foreign correspondent for "The New York Times" wrote in defence of Hungary as the last refuge of Jews in Europe, declaring that "as long as they exercised any authority in their own house, the Hungarians tried to protect the Jews."
Deposition and arrest.
In August 1944, the Nazis were distracted by their failing war effort, and Romania withdrew from the Axis and turned on Hitler and his allies. In Budapest, Horthy moved to reconsolidate his influence. He ousted Sztójay and the other Nazi-friendly ministers installed in the spring, replacing them with a new government under Géza Lakatos. He stopped the mass deportations of Jews, and ordered the police to use deadly force if the Germans attempted to resume them. While some smaller groups continued to be deported by train, the Germans did not press Horthy to ramp the pace back up to pre-August levels. Indeed, when Horthy turned down Eichmann's request to restart the deportations. Himmler ordered Eichmann to return to Germany.
Horthy also reopened the peace feelers to the Allies, and began considering strategies for surrendering to the Allied force he deeply distrusted: the Red Army. As bitterly anti-Communist as Horthy was, his dealings with the Nazis led him to conclude that the Communists were the far lesser evil. Working through his trustworthy General Béla Miklós who was in contact with Soviet forces in eastern Hungary, Horthy sought to surrender to the Soviets while preserving the Hungarian government's autonomy. The Soviets willingly promised this, and on 11 October Horthy and the Soviets finally agreed to surrender terms. On 15 October 1944, Horthy told his government ministers that Hungary had signed an armistice with the Soviet Union. He said, "It is clear today that Germany has lost the war... Hungary has accordingly concluded a preliminary armistice with Russia, and will cease all hostilities against her." Horthy "...informed a representative of the German Reich that we were about to conclude a military armistice with our former enemies and to cease all hostilities against them."
The Nazis had anticipated Horthy's move. On 15 October, after Horthy announced the armistice in a nationwide radio address, Hitler initiated Operation Panzerfaust, sending commando Otto Skorzeny to Budapest with instructions to remove Horthy from power. Horthy's son Miklós Horthy, Jr., was meeting with Soviet representatives to finalize the surrender when Skorzeny and his troops forced their way into the meeting and kidnapped the younger Horthy at gunpoint. Trussed up in a carpet, Miklós Jr. was immediately driven to the airport and flown to Germany to serve as a hostage. Skorzeny then brazenly led a convoy of German troops and four Tiger II tanks to the Vienna Gates of Castle Hill, where the Hungarians had been ordered not to resist. Though one unit had not received the order, the Germans quickly captured Castle Hill with minimal bloodshed: only seven soldiers were killed and twenty-six wounded.
Horthy was captured by Veesenmayer and his staff later on the 15th and taken to the Waffen SS office, where he was held overnight. Vessenmayer told Horthy that unless he recanted the armistice and abdicated, his son would be killed the next morning. The fascist Arrow Cross swiftly took over Budapest. With his son's life in the balance, Horthy consented to sign a document officially abdicating his office and naming Ferenc Szálasi, leader of the Arrow Cross, as his successor. Horthy understood that the Germans merely wanted the stamp of his prestige on a Nazi-sponsored Arrow Cross coup—but he signed anyway. As he later explained his capitulation: "I neither resigned nor appointed Szálasi Premier. I merely exchanged my signature for my son's life. A signature wrung from a man at machine-gun point can have little legality."
Horthy met Skorzeny three days later at Pfeffer-Wildenbruch's apartment and was told he would be transported to Germany in his own special train. Skorzeny told Horthy that he would be a "guest of honour" in a secure Bavarian castle. On 17 October, Horthy was personally escorted by Skorzeny into captivity at Schloss Hirschberg in Bavaria, where he was guarded closely, but allowed to live in comfort.
With the help of the SS, the Arrow Cross leadership moved swiftly to take command of the Hungarian armed forces, and to prevent the surrender that Horthy had arranged even though Soviet troops were now deep inside the country. Szálasi resumed persecution of Jews and other "undesirables". In the three months between November 1944 and January 1945, Arrow Cross death squads shot 10,000 to 15,000 Jews on the banks of the Danube. The Arrow Cross also welcomed Adolf Eichmann back to Budapest, where he began the deportation of the city's surviving Jews (Eichmann never successfully completed this phase of his plans, thwarted in large measure by the efforts of Swedish diplomat Raoul Wallenberg). Out of a pre-war Hungarian Jewish population estimated at 825,000, only 260,000 survived.
By December 1944, Budapest was under siege by Soviet forces. The Arrow Cross leadership retreated across the Danube into the hills of Buda in late January, and by February the city surrendered to the Soviet forces.
Horthy remained under house arrest in Bavaria until the war in Europe ended. On 29 April, his SS guardians fled in the face of the Allied advance. On 1 May, Horthy was first liberated, and then arrested, by elements of the U.S. 7th Army.
Exile.
After his arrest, Horthy was moved between a variety of detention locations before finally arriving at the prison facility at Nuremberg in late September 1945. There he was asked to provide evidence to the International Military Tribunal in preparation for the trial of the Nazi leadership. Although he was interviewed repeatedly about his contacts with some of the defendants, he did not testify in person. In Nuremberg he was reunited with his son, Miklós.
Horthy gradually came to believe that his arrest had been arranged and choreographed by the Americans in order to protect him from the Russians. Indeed, the former regent reported being told that Josip Broz Tito, the new ruler of Yugoslavia, asked that Horthy be charged with complicity with the 1942 massacre of Serbian and Jewish civilians by Hungarian troops in the Bačka region of Vojvodina. Serbian historian Zvonimir Golubović has claimed that not only was Horthy aware of these genocidal massacres, but had approved of them. However, American trial officials did not indict Horthy for war crimes. The former ambassador John Montgomery, who had some influence in Washington, also contributed to Horthy's release in Nuremberg.
According to the memoirs of Ferenc Nagy, who served for a year as prime minister in post-war Hungary, the Hungarian Communist leadership was also interested in extraditing Horthy for trial. Nagy said that Joseph Stalin was more forgiving: that Stalin told Nagy during a diplomatic meeting in April 1945 not to judge Horthy, because he was old and had offered an armistice in 1944.
On 17 December 1945, Horthy was released from Nuremberg prison and allowed to rejoin his family in the German town of Weilheim, Bavaria. The Horthys lived there for four years, supported financially by ambassador John Montgomery, his successor, Herbert Pell, and by Pope Pius XII, whom he knew personally.
In March 1948, Horthy returned to testify at the Ministries Trial, the last of the twelve U.S.-run Nuremberg Trials; he testified against Edmund Veesenmayer, the Nazi administrator who had controlled Hungary during the deportations to Auschwitz in the spring of 1944. Veesenmayer was sentenced to 20 years imprisonment, but was released in 1951.
For Horthy, returning to Hungary was impossible; it was now firmly in the hands of a Soviet-sponsored Communist government. In an extraordinary twist of fate, the chief of Hungary's post-war Communist apparatus was Mátyás Rákosi, one of Béla Kun's colleagues from the ill-fated Communist coup of 1919. Kun had been executed during Stalin's purges of the late 1930s, but Rákosi had survived in a Hungarian prison cell; in 1940 Horthy had permitted Rákosi to emigrate to the Soviet Union in exchange for a series of highly-symbolic Hungarian battle-flags from the 19th century, which were in Russian hands.
In 1950, the Horthy family managed to find a home in Portugal, thanks to Miklós Jr.'s contacts with Portuguese diplomats in Switzerland. Horthy and members of his family were relocated to the seaside town of Estoril, in the house address: Rua Dom Afonso Henriques, 1937 2765.573 Estoril.
His American supporter, John Montgomery, recruited a small group of wealthy Hungarians to raise funds for their upkeep in exile. According to Horthy's daughter-in-law, Countess Ilona Edelsheim Gyulai, Hungarian Jews also supported Horthy's family in exile covering their living expenses, including industrialist Ferenc Chorin and lawyer László Pathy.
In exile, Horthy wrote his memoirs, "Ein Leben für Ungarn" (English: "A Life for Hungary"), published under the name of Nikolaus von Horthy, in which he narrated many personal experiences from his youth until the end of World War II. He claimed that he had distrusted Hitler for much of the time he knew him and tried to perform the best actions and appoint the best officials in his country. He also highlighted Hungary's mistreatment by many other countries since the end of World War I. Horthy was one of the few Axis heads of state to survive the war, and thus to write post-war memoirs.
Horthy never lost his deep contempt for communism, and in his memoirs he blamed Hungary's alliance with the Axis on the threat posed by the "Asiatic barbarians" of the Soviet Union. He railed against the influence that the Allies' victory had given to Stalin's totalitarian state. "I feel no urge to say 'I told you so,' " Horthy wrote, "nor to express bitterness at the experiences that have been forced upon me. Rather, I feel wonder and amazement at the vagaries of humanity."
He died in 1957 in Estoril.
Horthy married Magdolna Purgly de Jószáshely in 1901; they were married for just over 56 years, until his death. He had two sons, Miklós Horthy, Jr. (often rendered in English as "Nicholas" or "Nikolaus") and István Horthy, who served as his political assistants; and two daughters, Magda and Paula. Of his four children, only Miklós outlived him.
According to footnotes in his memoirs, Horthy was very distraught about the failure of the Hungarian Revolution of 1956. In his will, Horthy asked that his body not be returned to Hungary "until the last Russian soldier has left." His heirs honoured the request. In 1993, two years after the Soviet troops left Hungary, Horthy's body was returned to Hungary and he was buried in his home town of Kenderes. The reburial in Hungary was the subject of some controversy on part of the left.
Titles, styles, honours and arms.
Full title as Regent.
His Serene Highness Miklós Horthy, Regent of Hungary.
Legacy.
 What did the average Hungarian learn in the good old days? He learned that the Horthy regime was restorationist, fascist, fascistoid, half-fascist, dictatorial, militaristic, nationalist, selfish, exploitative, power-hungry, servile, and so on. And primarily: counterrevolutionary. Now he hears and reads: conservative, autocratic, authoritarian, undemocratic, patriotic, self-interested, dynamic, reformist, abandoned, deceived, etc. Now I think that here and now a new, considered synthesis has not yet seen the light of day, even if it has in most of the world. For the Hungarian obviously does not wish to surrender his own history when he finally, and rightly, believes it could be his.
 — historian Mária Ormos
 I consider Miklós Horthy a patriot, who also must be found a respected place in the national memory.
 — József Antall
The interwar period dominated by Horthy's government is known in Hungarian as the "Horthy-kor" ("Horthy age") or "Horthy-rendszer" ("Horthy system"). Its legacy, and that of Horthy himself, remain among the most controversial political topics in Hungary today, tied inseparably to the Treaty of Trianon and the Holocaust. According to one school of thought, Horthy was a strong, conservative, but not undemocratic leader and patriot who only entered into an alliance with Hitler's Germany in order to restore lands Hungary lost after the First World War and was reluctant, or even defiant, in the face of Germany's demands to deport the Hungarian Jewry. Others see Horthy's alliance with Germany as foolhardy, or think that a positive view of Horthy serves a revisionist historical agenda, pointing to Horthy's passage of various anti-Jewish laws — the earliest in Europe, in 1920 — as a sign of his anti-Semitism and willing collaboration in the Holocaust.
The historiography and reception of Horthy has changed throughout the course of modern Hungarian history. He was officially denounced by the state during the communist era, while during his own time and in the 21st century, his reception has been more nuanced.
During the Horthy era.
During his own reign, Horthy's reception was fairly positive, though by no means monolithic. Opponents of the short-lived Soviet Republic saw him as a "national saviour," in contrast to the communist "losers of the nation."<ref name="Rubicon Történelmi Magazin/A Horthy-kép változásai/Országmentő 6. oldal">Rubicon Történelmi Magazin/A Horthy-kép változásai/Országmentő 6. oldal</ref> Because Horthy distanced himself from everyday politics, he was able to cultivate the image of the nationally governing admiral. The peaceful re-acquisition of Hungarian-majority lands lost after Trianon greatly bolstered this image.<ref name="Rubicon Történelmi Magazin/A Horthy-kép változásai/Hongyarapító 10. oldal">Rubicon Történelmi Magazin/A Horthy-kép változásai/Hongyarapító 10. oldal</ref> The regime's efforts at economic development and modernization also improved contemporaries' opinions, and although the Great Depression initially hurt his image, Horthy's wide-ranging social programs saved face for the most part.
On the other hand, Horthy's right-wing tendencies were not without their critics even in his time. Bourgeois liberals, among them Sándor Márai, criticized Horthy's authoritarian style as much as they disdained the violent tendencies of the far-left. He was also criticized by monarchists and elements of the aristocracy and clergy. While the harshest opposition to Horthy initially came from the communist parties he had overthrown and outlawed, the later 1930s saw him come under increasing criticism from the far-right. After the Arrow Cross took control of the country in 1945, Horthy was denounced as a "traitor" and "Jew-lover."
Horthy's reception in the West was positive until the outbreak of the Second World War, and while Hitler initially backed Horthy, relations between the two leaders were soured by Horthy's denial of involvement in the invasions of Poland and Czechoslovakia. Horthy likewise viewed the Nazis as "brigands and clowns." The Little Entente criticized Horthy, mainly for his irredentist policy goals.
During the communist era.
Under the Marxism mandated during the communist era, the Horthy era was depicted extremely negatively. Scholars agree that due to political pressure, Horthy's positive achievements were unmentioned while his negative aspects were exaggerated to the point of total distortion.
The communist takeover in 1945 saw the same powers that had denounced Horthy as an "executioner" and a "murdering monster" assume control of the state. The government systematically disseminated, through propaganda and state education, the idea that the Horthy era constituted the "lowest point in Hungarian history." Most of these views were supported by socialist or communist activists persecuted under the Horthy administration. Especially critical in this campaign was the 1950 publication of the textbook "The Story of the Hungarian People", which denounced Horthy's military as a "genocidal band" consisting of "sociopathic officers, kulaks, and the dregs of society." It further characterized Horthy himself as a "slave of the Habsburgs," a "red-handed dictator" who "spoke broken Hungarian" and was known for his "hatred of workers and soviets." "The Story of the Hungarian People" was required reading in middle schools throughout the 1950s.
The situation only slowly improved. While the professionalization of Hungarian history and historiography coupled with the loosening of state ideological controls inevitably led to a fairer assessment of Horthy's life, popular volumes still painted him negatively. Influential biographies openly leveled "ad hominem" attacks at Horthy, accusing him of bastardy, lechery, sadism, greed, nepotism, bloodthirst, warmongering, and cowardice, among other vices.
Reburial and contemporary politics.
The downfall of the communist regime and the rebirth of a free press and academia in Hungary vastly improved Hungarian understanding of the Horthy era. In 1993, only a few years after the first democratic elections, Horthy's body was returned from Portugal to his hometown of Kenderes. Tens of thousands of people, as well as almost the entirety of József Antall's MDF cabinet, attended the ceremony. Antall had prefaced the burial with a series of interviews praising Horthy as a "patriot." The reburial was broadcast on state television and was accompanied by large-scale protests in Budapest.
In contemporary Hungary, hagiography of Horthy is associated with the far-right Jobbik and its allies. Since 2012, Horthy statues, squares, or memorials have been erected in numerous villages and cities including Csókakő, Kereki, Gyömrő, and Debrecen. In November 2013, a Horthy statue's unveiling at a Calvinist church in Budapest drew international attention and criticism.
"Der Spiegel" has written about the resurgence of what its writers call "the Horthy cult," claiming that Horthy's popularity indicates returning irredentist, reactionary, and ultranationalistic elements. Critics have more specifically connected Horthy's popularity to the "Magyar Gárda", a paramilitary group that uses Árpád dynasty imagery and to recent incidents of antiziganist and antisemitic vandalism in Hungary. The ruling Fidesz party has, according to reporters, "hedged its bets" on the Horthy controversy, refusing to outright condemn Horthy statues and other commemorations for fear of losing far-right voters to Jobbik, although some Fidesz politicians have labeled Horthy memorials "provocative." This tension has led some to label Fidesz as "implicitly anti-semitic" and to accuse Prime Minister Viktor Orbán of a "revisionist" agenda.
Left-wing groups such as the Hungarian Socialist Party have condemned positive historiography of Horthy. Attila Mesterházy, the socialist leader, has condemned the Orbán government's position as "inexcusable," claiming that Fidesz was "“openly associating itself with the ideology of the regime that collaborated with the fascists." Words have led to actions in some instances, for example when leftist activist Péter Dániel vandalized a rural bust of Horthy by dousing it in red paint and hanging a sign that read "Mass Murderer — War Criminal" around its neck. Right-wing activists responded by vandalizing a Jewish cemetery in Székesfehérvár.
Film and television portrayals.
In the 1985 NBC TV film "", the role of Horthy was taken by Hungarian-born actor Guy Deghy, who appeared bearded although Horthy (as photographs bore out) appeared consistently clean-shaven throughout his life.
In the 2011 Spanish TV film series, "El ángel de Budapest" (The angel of Budapest), also set during Wallenberg's time in Hungary in 1944, he is portrayed by actor László Agárdi. In the 2014 American action drama film "Walking with the Enemy", Regent Horthy is portrayed by Ben Kingsley. The movie depicts a story of a young man during the Arrow Cross Party takeover in Hungary.

</doc>
<doc id="51310" url="http://en.wikipedia.org/wiki?curid=51310" title="Citroën">
Citroën

Citroën (]) is a major French automobile manufacturer, part of the PSA Peugeot Citroën group since 1976.
Founded in 1919 by French industrialist André-Gustave Citroën (1878–1935), Citroën was the first mass-production car company outside the USA and pioneered the modern concept of creating a sales and services network that complements the motor car.
Within eight years Citroën had become Europe's largest car manufacturer and the 4th largest in the world.
André-Gustave Citroën introduced the first industrial mass production of vehicles outside the United States, a technique he developed while mass-producing armaments for the French military in World War I. In 1924, Citroën produced Europe’s first all-steel-bodied car, the B10. In 1934, Citroën secured its reputation for innovation with the Traction Avant, not only the world's first mass-produced front-wheel drive car, but also one of the first cars to feature a unitary-type body, with no chassis frame holding the mechanical components.
In 1954 Citroën produced the world's first hydropneumatic self-levelling suspension system, then in 1955 the revolutionary DS, the first European production car with disc brakes.<ref name="vintagecars.about.com/internet archive"></ref> In 1967, Citroën introduced swiveling headlights in several models, allowing for greater visibility on winding roads. Citroën cars have received various international and national-level awards, including three European Car of the Year.
Citroën has a successful history in motorsport, and is the only automobile manufacturer to have won three different official championships from the International Automobile Federation: the World Rally Raid Championship (five times), the World Rally Championship (eight times ), and the World Touring Car Championship.
Citroën has been selling vehicles in China since 1984, and it represents a major market for the brand today, largely via the Dongfeng Peugeot-Citroën joint venture. In 2014, when PSA Peugeot Citroën ran into severe financial difficulties, the Dongfeng Motor Corporation took an ownership stake.
The brand celebrated its 90th Anniversary in 2009. The Citroën's C_42 showroom, on the Champs Elysées in Paris, is a place where Citroën organises some exhibitions, shows its cars and concept cars.
History.
Early years.
André Citroën built armaments for France during World War I; after the war, however, unless he planned ahead he knew he would have a modern factory without a product. There was nothing automatic about the decision to become an automobile manufacturer once the war was finished, but the auto-business was one that Citroën knew well, thanks to a successful six-year stint working with Mors between 1908 and the outbreak of war. The decision to switch to automobile manufacturing was evidently taken as early as 1916 which is when Citroën asked the engineer Louis Dufresne, previously with Panhard, to design a technically sophisticated 18HP automobile for which he could use his factory once peace broke out. Long before that happened, however, he had modified his vision, and decided, (like Henry Ford), that the best post war opportunities in auto-making would involve a lighter car of good quality, but made in sufficient quantities to be priced enticingly. In February 1917 Citroën contacted another engineer, , who already had a considerable reputation within the French automotive sector as the creator, in 1909, of a little car called Le Zèbre. André Citroën's mandate was characteristically demanding and characteristically simple: to produce an all-new design for a 10 HP car that would be better equipped, more robust and less costly to produce than any rival product at the time. The result was the Type A, announced to the press, just four months after the guns fell silent, in March 1919. The first "production" Type A emerged from the factory at the end of May, and in June it was exhibited at a show room in the Champs-Élysées which normally sold Alda cars. Citroën persuaded the owner of the Alda business, Fernand Charron, to lend him the show-room (just as a few years later Charron would be persuaded to become a major investor in Citroën business). On 7 July 1919 the first customer took delivery of a new Citroën 10HP "Type A".
That same year, André Citroën briefly negotiated with General Motors on a proposed sale of the Citroën company to GM. The deal nearly closed, but GM ultimately decided that its management and capital would be too overstretched by the takeover. Citroën thus remained independent till 1935.
Citroën was a keen marketer—he used the Eiffel Tower as the world's largest advertising sign, as recorded in "Guinness World Records". He also sponsored expeditions in Asia (Croisière Jaune), North America, (Croisière Blanche) and Africa (Croisière Noire) intended to demonstrate the potential for motor vehicles equipped with the Kégresse track system to cross inhospitable regions. The expeditions conveyed scientists and journalists.
Demonstrating extraordinary toughness, a 1923 Citroën that had already travelled 48000 km was the first car to be driven around Australia. The car, a 1923 Citroën 5CV Type C Torpedo, was driven by Neville Westwood from Perth, Western Australia, on a round trip from August to December 1925. The car is now fully restored and in the collection of the National Museum of Australia.
In 1924, Citroën began a business relationship with American engineer Edward G. Budd. From 1899, Budd had worked to develop stainless steel bodies for railroad cars, for the Pullman in particular. Budd went on to manufacture steel bodies for many automakers, Dodge being his first big auto client. At the Paris Motor Show in October 1924, Citroën introduced the Citroën B10, the first all-steel body in Europe. The cars were initially successful in the marketplace, but soon competitors (who were still using a wooden structure for their bodies) introduced new body designs. Citroën did not redesign the bodies of his cars. Citroëns still sold in large quantities in spite of not changing the body design, but the car's low price was the main selling point and Citroën experienced heavy losses.
In 1927, the bank Lazard helped Citroën by bringing new, much-needed funds as well as by renegotiating its debt—for example, by buying out the SOVAC. It went even further by entering in its capital and being represented at the board. The three directors sent by Lazard were Raymond Philippe, Andre Meyer, and Paul Frantzen.
André Citroën perceived the need to differentiate his product, to avoid the low price competition surrounding his conventional rear drive models in the late 1920s/early 1930s. In 1933, Citroën introduced the Rosalie, the first commercially available passenger car with a diesel engine, developed with Harry Ricardo.
Traction Avant and Michelin ownership.
Traction Avant.
The "Traction Avant" is a car that pioneered mass production of three revolutionary features that are still in use today: a unitary body with no separate frame, four wheel independent suspension, and front wheel drive. The vast majority of cars for many decades were similar in conception to the Ford Model T - a body bolted onto a ladder frame which held all the mechanical elements of the car, a solid rear axle that rigidly connected the rear wheels, and rear wheel drive. The "Model T school" of automobile engineering proved popular, because it was thought to be cheap to build, but it did pose dynamic defects as cars became more capable, and resulted in a heavier car, which is why cars today are more like the Traction Avant than the Model T under the skin.
Citroën commissioned the American Budd Company to create a prototype, which evolved into the 7-horsepower (CV), 32 HP Traction Avant of 1934.
Achieving quick development of the Traction Avant, tearing down and rebuilding the factory (in five months), and the extensive marketing efforts were investments that were too costly for Citroën to do all at once, causing the financial ruin of the company. In December 1934, despite the assistance of the Michelin company, Citroën filed for bankruptcy. Within the month, Michelin, already the car manufacturer's largest creditor, became its principal shareholder. Fortunately for Michelin, the technologically advanced Traction Avant met with market acceptance, and the basic philosophy of cutting edge technology used as a differentiator continued until the late 1990s. Pierre Michelin became the chairman of Citroën. Pierre-Jules Boulanger became the vice-president of Citroën and chief of the engineering and design department.
In 1935, founder André Citroën died from stomach cancer.
Research breakthroughs.
Pierre-Jules Boulanger had been a First World War air reconnaissance photography specialist with the French Air Force. He was capable and effective and finished the war having risen to the rank of captain. He was also courageous, having been decorated with the Military Cross and the Legion of Honour. He started working for Michelin in 1918, reporting directly to Édouard Michelin, co-director and founder of the business. Boulanger joined the Michelin board in 1922. He became president of Citroën in 1937 after the death of Édouard and kept his position until his death in 1950. In 1938, he also became Michelin's joint managing director.
During the German occupation of France in World War II Boulanger refused to meet Dr. Ferdinand Porsche or communicate with the German authorities except through intermediaries. He organized a "go slow" on production of trucks for the Wehrmacht, many of which were sabotaged at the factory, by putting the notch on the oil dipstick in the wrong place, resulting in engine seizure. In 1944 when the Gestapo headquarters in Paris was sacked by the French Resistance, his name was prominent on a Nazi blacklist of the most important "enemies of the Reich" to be arrested in the event of an allied invasion of France.
Citroën researchers, including Paul Magès, continued their work in secret, against the express orders of the Germans, and developed the concepts that were later brought to market in three remarkable vehicles - a small car (2CV), a delivery van (Type H), and a large, swift family car (DS). These were widely regarded by contemporary journalists as avant garde, even radical, solutions to automotive design.
This began a decades long period of unusual brand loyalty normally seen in the automobile industry only in niche brands, like Porsche and Ferrari.
The Deux Chevaux.
Citroën unveiled the 2CV—signifying two fiscal horsepower, initially only 12 HP—at the Paris Salon in 1948. The car became a bestseller, achieving the designer's aim of providing rural French people with a motorized alternative to the horse. It was unusually inexpensive to purchase and with its tiny two cylinder engine, inexpensive to run as well. The 2CV pioneered a very soft, interconnected suspension, but did not have the more complex self-levelling feature. This car remained in production, with only minor changes, until 1990 and was a common sight on French roads until recently. 8.8 Million 2CV variants were produced in the period 1948-1990.
The Goddess.
1955 saw the introduction of the DS, the first full usage of Citroën's now legendary hydropneumatic self-levelling suspension system that was tested on the rear suspension of the last of the Tractions. The DS was the first European production car with disc brakes.
The DS featured power steering, power brakes, and power suspension, and—from 1968—directional headlights. A single high-pressure system was used to activate the steering and brakes, with pistons in the gearbox cover to shift the gears in the transmission and to operate the clutch on the Citromatic, Citroën's semi-automatic transmission.
The car was remarkable for its era, and had a remarkable sounding name - "DS" is pronounced "DayEss" which in French means "Goddess."
High pressure hydraulics.
This high-pressure hydraulic system would form the basis of over 9 million Citroën cars, including the DS, SM, GS, CX, BX, XM, Xantia, C5, and C6. Self-levelling suspension is the principal user benefit - the car maintains a constant ride height above the road regardless of passenger and cargo load, despite the very soft suspension. This type of suspension is uniquely able to absorb road irregularities without disturbing the occupants. It is often compared to riding on a "magic carpet" for this reason.
These vehicles shared the distinguishing feature of rising to operating ride height when the engine was turned on, like a "mechanical camel" (per "Car & Driver" magazine). A lever beside the driver's seat allowed the driver to adjust the height of the car, later replaced with an electronic switch. The height adjustability of the suspension allows for clearing obstacles, fording shallow (slow-moving) streams, and changing tires.
Citroën was undercapitalised, so its vehicles had a tendency to be underdeveloped at launch, with limited distribution and service networks outside of France. As a result, early DS models experienced teething issues with the novel suspension. The hydropneumatics were sorted out and became reliable.
Licensing such a technological leap forward was pursued to a limited extent - in 1965, the Rolls-Royce Silver Shadow included this suspension. The 1963 Mercedes-Benz 600 and Mercedes-Benz 300SEL 6.3 tried to replicate the advantages with a costly, complex, and expensive to maintain air suspension that avoided the Citroën patented technology. By 1975, the Mercedes-Benz 450SEL 6.9 could finally be produced with the proven hydropneumatic suspension, and Mercedes-Benz continues to offer variations on this technology today.
During Citroën's 1968-75 venture with Maserati, the Citroën high-pressure hydraulic system was used on several Maserati models, for power clutch operation (Bora), power pedal adjustment (Bora), pop-up headlights (Bora, Merak), brakes (Bora, Merak, Khamsin), steering (Khamsin), and the entire Quattroporte II prototype, which was a four-door Citroën SM under the skin.
Aerodynamic pioneer.
Citroën was one of the early pioneers of the now widespread trend of aerodynamic automobile design, which helps to reduce fuel consumption and improve high-speed performance by reducing wind resistance. The cruising speed was the same as the top speed because of these efforts - the DS could happily run at 100 mph without disturbing the occupants. The firm began using a wind tunnel in the 1950s, enabling them to create highly streamlined cars such as the DS that were years ahead of their time. So good were the aerodynamics of the CX that it took its name from the term used to measure drag coeffient: formula_1.
Expansion and financial challenges.
In the 1960s, Citroën undertook a series of financial and development maneuvers, aiming to build on its strength in the 1950s with the successful, 2CV, Type H, and DS models. Since Citroën went bankrupt in 1974, the effectiveness of these maneuvers is not clear.
The maneuvers were to address two key gaps facing the company.
The first was the lack of a midsize car between its own range of very small, cheap cars (2CV/Ami) and large, expensive cars (DS/ID). In today's terms, this would be similar to a brand consisting only of the Tata Nano and Mercedes-Benz E-Class. Because of the potential volume, the midsize segment was the most profitable part of the car market, and in 1965 the "Citroënesque" Renault 16 stepped in to address it.
The second large issue was the lack of a powerful engine suitable for export markets. The post-WW2 Tax horsepower system in France was steeply progressive, and vehicles over 2.0 (later 2.8) liters of engine displacement faced a heavy, annual tax. The result was that cars made in France were considered underpowered outside of France.
For both the 1955 DS and 1974 CX models, development of the original engine around which the design was planned proved too expensive for the finances available, and the actual engine used in both cases was a modest and outdated four-cylinder design.
In 1963, Citroën negotiated with Peugeot to cooperate in the purchase of raw materials and equipment. Talks were broken off in 1965.
In 1964, Citroën partnered with NSU Motorenwerke to develop the Wankel engine via the Comobil (later Comotor) subsidiary. For Citroën, this represented the chance for a technological end run around the French Tax horsepower system - producing a more powerful car while maintaining a small engine. The first production car developed 106 hp with a 1-liter engine, while the standard GS delivered 55 hp with a 1-liter engine.
In 1965, Citroën took over the French carmaker Panhard in the hope of using Panhard's expertise in midsize cars. Cooperation between the two companies had begun 12 years earlier, and they had agreed to a partial merger of their sales networks in 1953. Panhard ceased making vehicles in 1967.
In 1967, Citroën purchased the truck manufacturer Berliet.
In 1968, Citroën purchased the Italian sports car maker Maserati, again with an eye to producing a more powerful car while maintaining a small engine in line with the French tax horsepower system. The first production car developed 170 hp with a 2.7 liter engine.
This was the 1970 SM, which featured a V6 Maserati engine, hydropneumatic suspension, and a fully powered, self-centering steering system called DIRAVI. The SM was engineered as if it were replacing the DS family car, a level of investment the small luxury Grand Touring car sector alone would never be able to support, even in the best of circumstances.
The year 1968 also saw a restructuring of Citroën's worldwide operations under a new holding company, Citroën SA. Michelin, Citroën's longtime controlling shareholder, sold a 49% stake to Fiat in what was referred to as the "PARDEVI" agreement (Participation et Développement Industriels).
From a model range perspective, the 1970s started well for Citroën, supported by the successful launch of the long-awaited midsize Citroën GS, finally filling the huge gap between the 2CV and the DS - with a 1-liter, hydropneumatically suspended car. The GS went on to sell 2.5 million units. 601,918 cars were produced just in 1972, up from 526,443 in 1971, and enough to lift the company past Peugeot into second place among French auto-makers when ranked by volume of units.
Circumstances became more unfavorable for Citroën as the 1970s progressed.
In 1973, Fiat sold its 49% stake in the "PARDEVI" holding company (that owned Citroën) back to Michelin. The Citroën and Fiat joint announcement indicated that benefits foreseen for their union in 1968 had failed to materialise. This was not in line with the tire company's long term strategy of ending involvements in the car manufacturing business, creating a very unstable ownership situation.
Citroën suffered a financial blow with the 1973 energy crisis. While some models sold well (peak production of the 2CV and its derivatives was 1974), the bets on Comotor and Maserati both had what was now clearly a flaw - high fuel consumption engines.
In 1974, the carmaker withdrew from North America due to U.S. design regulations that outlawed core features of Citroën cars (see Citroën SM).
Huge losses at Citroën were caused by failure of the Comotor rotary engine venture, plus the strategic error of going the from 1955 to 1970 without a model in the profitable middle range of the European market, and the massive development costs for the GS, GS Birotor, CX, SM, Maserati Bora, Maserati Merak, Maserati Quattroporte II, and Maserati Khamsin models— each a technological marvel in its own right.
40 years after the bankruptcy related to the Traction Avant, Citroën went bankrupt again, losing its existence as an independent entity, selling Berliet and Maserati, and closing Comotor.
PSA Peugeot Citroën era.
The French government feared large job losses, due to the poor cash flow situation at Citroën and the unstable ownership structure. It arranged talks between Michelin and Citroën and decided to merge Automobiles Citroën and Automobiles Peugeot into a single company. A year after the break with Fiat, on 24 June 1974, Citroën announced their new partnership, this time with Peugeot. Michelin agreed to transfer control of the business to Peugeot.
In December 1974 Peugeot S.A. acquired a 38.2% share of Citroën. On 9 April 1976 they increased their stake of the then bankrupt company to 89.95%, thus creating the "PSA Group" (where PSA is short for Peugeot Société Anonyme), becoming PSA Peugeot Citroën.
Citroën sold off Maserati to De Tomaso in May 1975, and the Italian firm was able to exploit the sales potential of the models and technology developed by Citroën, as well as exploit the image of the Maserati brand in a downward brand extension to sell 40,000 of the newly designed Bi-Turbo models. Berliet was sold to Renault.
The PSA venture was a financial success from 1976 to 1979. Citroën had two successful new designs in the market at this time (the GS and CX), a resurgent Citroën 2CV, and the Citroën Dyane in the wake of the oil crisis, and Peugeot was typically prudent in its own finances, launching the Peugeot 104 based Citroën Visa and Citroën LNA.
PSA then purchased the aging assets and substantial liabilities of Chrysler Europe for $1, leading to losses from 1980 to 1985. Since PSA needed a new brand to for the Chrysler cars, it resurrected the Talbot name.
Trade union problems.
In the early 1980s Citroën was targeted by union action. Events led to a mass demonstration in the streets of Paris on 25 May 1982. Approximately 27,000 Citroën workers demonstrated in affirmation of their wish to work at a company which was being picketed by striking workers who had been blocking access to the factories for four weeks. The demonstrations were successful and six days later work at the plants resumed. Jacques Lombard, one of the company’s senior managers, had gone public with his concerns criticising the strikes.
Taming the innovative spirit.
PSA gradually diluted Citroën's ambitious attitude to engineering and styling in an effort to rebrand the marque to appeal to a wider market. In the 1980s, Citroën models became increasingly Peugeot-based, following the worldwide motor industry trend called "platform sharing." The 1982 BX used the hydropneumatic suspension system and still had a "Citroën-esque" appearance, while being powered by Peugeot-derived engines and using the floorpan later seen on the Peugeot 405. By the late 1980s, many of the distinctive features of the marque had been removed or diluted—conventional Peugeot's switchgear replaced Citroën's quirky but ergonomic "Lunule" designs, complete with self-cancelling indicators that Citroën had previously refused to adopt on ergonomic grounds. While the cars were more banal, sales continued steadily.
Geographical expansion.
Citroën expanded into many new geographic markets. In the late 1970s, the firm developed a small car for production in Romania known as the Oltcit, which it sold in Western Europe as the Citroën Axel. That joint venture has ended, but a new one between PSA and Toyota is now producing cars like the Citroën C1 in the Czech Republic.
In China, Citroën began selling cars in 1984 - today it is a major overseas market. The current range of family cars that includes the C3 and Xsara and locally designed cars like the Fukang and Elysée models. The Citroën brand recently increased its Chinese sales by 30% in an overall market growth of 11% and ranks highest in the 2014 customer satisfaction survey by JD Power in China.
Citroën is a global brand except in North America, where the company has not returned since the SM was effectively banned in 1974 for not meeting U.S. National Highway Traffic Safety Administration (NHTSA) bumper height regulations.
Recent decades.
From 2003-10, Citroën produced the C3 Pluriel, an unusual convertible with allusions to the 1948-90 2CV model, both in body style (such as the bonnet) and in its all-round practicality.
In 2001, Citroën acknowledged its history of innovation when it opened a museum of its many significant vehicles - the 'Conservatoire,' with 300 cars.
With the severe decline in European auto sales after 2009, worldwide sales of vehicles declined from 1,460,373 in 2010 to 1,435,688 in 2011, with 961,156 of these sold in Europe.
Dongfeng Peugeot-Citroën continues growing rapidly, and ranks highest in the 2014 customer satisfaction survey by JD Power in China, above luxury brands like Mercedes and BMW, and above mass market brands like Volkswagen ranking only thirteenth and seventeenth. On the 10 first months of 2014 in China, the sales of Donfeng Citroen cars increased by 30% in an overall market growth of 11%. The launch of the Citroën C3-XR in December 2014 will enable Citroën to continue its growth in 2015.
Despite the near death financial experience of PSA Peugeot Citroën in 2014, and financial rescue by Dongfeng Motors, the "Citroën" and "DS" brands are both planned to grow.
Since 2013, the model Carolina "Pampita" Ardohaín represents Citroën and its life style in some fashion films.
DS brand.
Citroën announced in early 2009 the development of a premium sub-brand DS, for Different Spirit or Distinctive Series (although the reference to the historical Citroën DS is evident), to run in parallel to its mainstream cars. The slogan of the DS car marque is "Spirit of avant-garde".
This new series of cars started with the Citroën DS3 in early 2010, a small car based on the floor plan of the new C3. The Citroën DS3 is based on the concept of the produced Citroën C3 Pluriel model and the Citroën DS Inside concept car. The Citroën DS3 is customisable with various roof colours that can contrast with the body panels. The Citroën DS3 was named 2010 "Car of the Year" by "Top Gear Magazine", awarded first supermini four times in a row by the JD Power Satisfaction Survey UK and second most efficient supermini (Citroën DS3 1.6 e-HDi 115 Airdream : True MPG 63.0mpg) by "What car ?" behind the Citroën C3. In 2013, the Citroën DS3 was again the most sold premium subcompact car with 40% of these market shares in Europe, validating the business model of this product development.
The DS series is deeply connected to Citroën, as the DS4, launched in 2010, is based on the 2008 Citroën Hypnos concept car and the DS5, following in 2011, is based on the 2005 Citroën C-SportLounge concept car.
Their rear badge is a new DS logo rather than the familiar Citroën double chevron, and all will have markedly different styling from their equivalent sister car. Citroën has produced several dramatic-looking concept sports cars of late with the fully working Citroën Survolt being badged as a DS. Indeed, the 2014 DS Divine concept car develops the Citroën Survolt prototype as the future sport coupé of the DS range.
In China, Citroën has "stand alone" DS sale rooms, including vehicle plants built for the production of these vehicles. Since 2014, Citroën sells the Chinese built DS 5LS and DS 6WR in China.
Awards.
Citroën was recognized in the 1999 Car of the Century competition as producing the third most influential car of the 20th century, it's Citroën DS, which trailed only the Ford Model T and BMC Mini.
Citroën has produced three winners of the 50 year old European Car of the Year award, and many rated second or third place.
Citroën has produced one winner of the United States Motor Trend Car of the Year award - the original Car of the Year designation, which began in 1949. This was especially significant because this award previously was only given to cars designed and built in the United States.
Citroën has produced eight "Auto Europa" winners in 28 years, since 1987. "Auto Europa" is the prize awarded by the jury of the Italian Union of Automotive Journalists (UIGA), which annually celebrates the best car produced at least at 10,000 units in the 27 countries of the European Union: Citroën XM(1990), Citroën ZX (1992), Citroën Xantia (1994), Citroën Xsara Picasso (2001), Citroën C5 (2002), Citroën C3 (2003), Citroën C4 (2005) and Citroën DS4 (2012).
Citroën Racing.
Citroën Racing, previously known as Citroën Sport and before that as Citroën Competitions, is the team responsible for Citroën's sporting activities. It is a successful winning competitor in the World Rally Championship and in the World Touring Car Championship.
Early rally wins for Citroën vehicles.
Citroën vehicles were entered in endurance rally driving events beginning in 1956, with the introduction of the DS. The brand was successful and won many key events over a decades long period, with what was essentially the same production car design.
Racing the 2CV.
Citroën discovered that while racing the uniquely slow 2CV against other cars made little sense, they could be interesting to watch racing against each other. Citroën Competitions sponsored three long distance competitions - Paris-Kaboul-Paris in 1970, Paris-Persepolis-Paris in 1972, and Raid Afrique in 1973.
Enthusiasts carried on the tradition with "2CV Cross" - a group of 2CV's racing around a dirt track - a sport that continues today.
Rebuilding the competition group.
The Citroën Competitions division was impacted negatively by the firm's 1974 bankruptcy.
Competitive rallying was also changing - away from standard production cars to specially developed low volume models. In response to the entry of the competitive short wheel base Group B 4 wheel drive Audi Quattro into rallying, Citroën developed the heavily modified Group B Citroën BX 4TC in 1986.
The team returned successfully with the Citroën ZX Rally Raid to win the Rally Raid Manufacturer's Championship five times (1993, 1994, 1995, 1996, and 1997) with Pierre Lartigue and Ari Vatanen. Citroën Racing won the Dakar Rally four times, in 1991, continuing the serial of four victories of Peugeot sport, and then again in 1994, 1995, and 1996.
From 2001, the Citroën Racing team returned successfully to the World Rally Championship, winning eight times the Manufacturer's Title, continuing the serial of three WRC Championships victories of Peugeot sport, in 2003, 2004, 2005, 2008, 2009, 2010, 2011 and 2012. The Citroën WRC Team pilot Sébastien Loeb also won nine Drivers' Championships. In 2004, 2005, and 2006, the French pilot won the Drivers' Championship, driving the Citroën Xsara WRC, in 2007, 2008, 2009 and 2010 with the Citroën C4 WRC, and in 2011 and 2012 with the new Citroën DS3 WRC.
The Citroën World Rally Team has a record of 97 victories in the World Rally Championship. In 2014, Citroën was the automaker that won the most world championship titles: 14 World Champion titles in 15 appearances. Citroën won the World Rally Raid Championship 5 times, the World Rally Championship 8 times, and the World Touring Car Championship in its first participation.
New competition division for touring cars.
In 2013, Citroën Racing created a new sub-division, the Citroën World Touring Car Team, in order to attempt the 2014 World Touring Car Championship. The name "Citroën C-Elysée WTCC" has been chosen for the race car running in this world competition. It was developed in a few months, thanks to the experience of the Citroën World Rally Team. Citroën revealed a thirty-minute film on its Internet channel, to show the different steps to the C-Elysée project development : Projet M43 WTCC, Citroën WTCC 2014.
The Citroën World Touring Car Team won fourteen victories out of the fifteen first races of the 2014 WTCC season, in spite of the handicap of the 60 kg Compensation Weight put to the leading cars. The Citroën/Total WTCC Team won the "Manufacturer's WTCC Championship", 5 races before the end of the season, after the 2014 Shanghai race, where Citroën won first, second, third and fourth place, and recorded the fastest lap time. The Citroën World Touring Car Team pilots also got the three first ranks of the Drivers' World Touring Car Championship.
Concept cars.
Citroen has produced numerous concept cars over the decades, previewing future design trends or technologies. Notable concepts include the Citroën Karin (1980), Citroën Activa (1988), Citroën C-Métisse (2006), GT by Citroën (2008) and Citroën Survolt (2010).
Logo.
The origin of the logo may be traced back to a trip made by the 22-year-old André Citroën to Łódź city, Poland, where he discovered an innovative design for a chevron-shaped gear used in milling. He bought the patent for its application in steel. Mechanically a gear with helical teeth produces an axial force. By adding a second helical gear in opposition, this force is cancelled. The two chevrons of the logo represent the intermeshing contact of the two.
The presentation of the logo has evolved over time. Before the war, it was rendered in yellow on a blue background. After the war, the chevrons became more subtle herringbones, usually on a white background. With the company searching for a new image during the 1980s, the logo became white on red to give an impression of dynamism, emphasized by publicity slogan.
In February 2009 Citroën launched a new brand identity to celebrate its 90th anniversary, replacing the 1985 design. The new logo was designed by Landor Associates — a 3D metallic variation of the double chevron logo accompanied by a new font for the Citroën name and the new slogan "Créative Technologie". A TV campaign reminiscing over of Citroën was commissioned to announce the new identity to the public. The new look is currently being rolled out to dealers globally and is expected to take three to five years.
Factories.
Some joint venture models are manufactured in third party or joint venture factories, including:
References.
Bibliography.
</dl>

</doc>
<doc id="51314" url="http://en.wikipedia.org/wiki?curid=51314" title="History of Antarctica">
History of Antarctica

"For the natural history of the Antarctic continent, see Antarctica".
The history of Antarctica emerges from early Western theories of a vast continent, known as Terra Australis, believed to exist in the far south of the globe. The term "Antarctic", referring to the opposite of the Arctic Circle, was coined by Marinus of Tyre in the 2nd century CE.
The rounding of the Cape of Good Hope and Cape Horn in the 15th and 16th centuries proved that "Terra Australis Incognita" ("Unknown Southern Land"), if it existed, was a continent in its own right. In 1773 James Cook and his crew crossed the Antarctic Circle for the first time but although they discovered nearby islands, they did not catch sight of Antarctica itself. It is believed he was as close as 150 miles from the mainland.
In 1820, several expeditions claimed to have been the first to have sighted the ice shelf or the continent. A Russian expedition was led by Fabian Gottlieb von Bellingshausen and Mikhail Lazarev, a British expedition was captained by Edward Bransfield and an American sealer Nathaniel Palmer participated. The first landing was probably just over a year later when American Captain John Davis, a sealer, set foot on the ice.
Several expeditions attempted to reach the South Pole in the early 20th century, during the 'Heroic Age of Antarctic Exploration'. Many resulted in injury and death. Norwegian Roald Amundsen finally reached the Pole on December 14, 1911, following a dramatic race with the Englishman Robert Falcon Scott.
Early exploration.
The search for "Terra Australis Incognita".
In the Western world, belief in a Cold Land—a vast continent located in the far south of the globe to "balance" out the northern lands of Europe, Asia and North Africa—had existed for centuries. Aristotle had postulated a symmetry of the earth, which meant that there would be equally habitable lands south of the known world. The Greeks suggested that these two hemispheres, north and south, were divided by a 'belt of fire', due to the general observation that the climate got warmer and warmer the further south someone travelled, and no Europeans had gone past the equator to see that this was not the case..
It was not until Prince Henry the Navigator began in 1418 to encourage the penetration of the torrid zone in the effort to reach India by circumnavigating Africa that European exploration of the southern hemisphere began. In 1473 Portuguese navigator Lopes Gonçalves proved that the equator could be crossed, and cartographers and sailors began to assume the existence of another, temperate continent to the south of the known world.
The doubling of the Cape of Good Hope in 1487 by Bartolomeu Dias first brought explorers within touch of the Antarctic cold, and proved that there was an ocean separating Africa from any Antarctic land that might exist.
Ferdinand Magellan, who passed through the Straits of Magellan in 1520, assumed that the islands of Tierra del Fuego to the south were an extension of this unknown southern land, and it appeared as such on a map by Ortelius: "Terra australis recenter inventa sed nondum plene cognita" ("Southern land recently discovered but not yet fully known").
European geographers connected the coast of Tierra del Fuego with the coast of New Guinea on their globes, and allowing their imaginations to run riot in the vast unknown spaces of the south Atlantic, south Indian and Pacific oceans they sketched the outlines of the "Terra Australis Incognita" ("Unknown Southern Land"), a vast continent stretching in parts into the tropics. The search for this great south land or Third World was a leading motive of explorers in the 16th and the early part of the 17th centuries.
It has been argued that the Spaniard Gabriel de Castilla claimed to have sighted "snow-covered mountains" beyond the 64° S in 1603, but this claim is not generally recognized.
Quirós in 1606 took possession for the king of Spain all of the lands he had discovered in Australia del Espiritu Santo (the New Hebrides) and those he would discover "even to the Pole".
Francis Drake like Spanish explorers before him had speculated that there might be an open channel south of Tierra del Fuego. Indeed, when Schouten and Le Maire discovered the southern extremity of Tierra del Fuego and named it Cape Horn in 1615, they proved that the Tierra del Fuego archipelago was of small extent and not connected to the southern land.
Finally, in 1642 Tasman showed that even New Holland (Australia) was separated by sea from any continuous southern continent. Voyagers round the Horn frequently met with contrary winds and were driven southward into snowy skies and ice-encumbered seas; but so far as can be ascertained none of them before 1770 reached the Antarctic Circle, or knew it, if they did.
South of the Antarctic Convergence.
The visit to South Georgia by the English merchant Anthony de la Roché in 1675 was the first ever discovery of land south of the Antarctic Convergence . Soon after the voyage cartographers started to depict ‘Roché Island’, honouring the discoverer.
James Cook was aware of la Roché's discovery when surveying and mapping the island in 1775.
Edmond Halley's voyage in HMS "Paramour" for magnetic investigations in the South Atlantic met the pack ice in 52° S in January 1700, but that latitude (he reached 140 mi off the north coast of South Georgia) was his farthest south. A determined effort on the part of the French naval officer Jean-Baptiste Charles Bouvet de Lozier to discover the "South Land" – described by a half legendary "sieur de Gonneyville" – resulted in the discovery of Bouvet Island in 54°10′ S, and in the navigation of 48° of longitude of ice-cumbered sea nearly in 55° S in 1730 .
In 1771, Yves Joseph Kerguelen sailed from France with instructions to proceed south from Mauritius in search of "a very large continent." He lighted upon a land in 50° S which he called South France, and believed to be the central mass of the southern continent. He was sent out again to complete the exploration of the new land, and found it to be only an inhospitable island which he renamed the Isle of Desolation, but which was ultimately named after him.
The Antarctic Circle.
The obsession of the undiscovered continent culminated in the brain of Alexander Dalrymple, the brilliant and erratic hydrographer who was nominated by the Royal Society to command the Transit of Venus expedition to Tahiti in 1769. The command of the expedition was given by the admiralty to Captain James Cook. Sailing in 1772 with the "Resolution", a vessel of 462 tons under his own command and the "Adventure" of 336 tons under Captain Tobias Furneaux, Cook first searched in vain for Bouvet Island, then sailed for 20 degrees of longitude to the westward in latitude 58° S, and then 30° eastward for the most part south of 60° S, a higher southern latitude than had ever been voluntarily entered before by any vessel. On 17 January 1773 the Antarctic Circle was crossed for the first time in history and the two ships reached 67° 15' S by 39° 35' E, where their course was stopped by ice.
Cook then turned northward to look for French Southern and Antarctic Lands, of the discovery of which he had received news at Cape Town, but from the rough determination of his longitude by Kerguelen, Cook reached the assigned latitude 10° too far east and did not see it. He turned south again and was stopped by ice in 61° 52′ S by 95° E and continued eastward nearly on the parallel of 60° S to 147° E. On 16 March, the approaching winter drove him northward for rest to New Zealand and the tropical islands of the Pacific. In November 1773, Cook left New Zealand, having parted company with the "Adventure", and reached 60° S by 177° W, whence he sailed eastward keeping as far south as the floating ice allowed. The Antarctic Circle was crossed on 20 December and Cook remained south of it for three days, being compelled after reaching 67° 31′ S to stand north again in 135° W.
A long detour to 47° 50′ S served to show that there was no land connection between New Zealand and Tierra del Fuego. Turning south again, Cook crossed the Antarctic Circle for the third time at 109° 30′ W before his progress was once again blocked by ice four days later at 71° 10′ S by 106° 54′ W. This point, reached on 30 January 1774, was the farthest south attained in the 18th century. With a great detour to the east, almost to the coast of South America, the expedition regained Tahiti for refreshment. In November 1774, Cook started from New Zealand and crossed the South Pacific without sighting land between 53° and 57° S to Tierra del Fuego; then, passing Cape Horn on 29 December, he rediscovered Roché Island renaming it Isle of Georgia, and discovered the South Sandwich Islands (named "Sandwich Land" by him), the only ice-clad land he had seen, before crossing the South Atlantic to the Cape of Good Hope between 55° and 60°. He thereby laid open the way for future Antarctic exploration by exploding the myth of a habitable southern continent. Cook's most southerly discovery of land lay on the temperate side of the 60th parallel, and he convinced himself that if land lay farther south it was practically inaccessible and of no economic value.
First sighting.
The first land south of the parallel 60° south latitude was discovered by the Englishman William Smith, who sighted Livingston Island on 19 February 1819. A few months later Smith returned to explore the other islands of the South Shetlands archipelago, landed on King George Island, and claimed the new territories for Britain.
The first confirmed sighting of mainland Antarctica cannot be accurately attributed to one single person. It can, however, be narrowed down to three individuals. According to various sources, three men all sighted the ice shelf or the continent within days or months of each other: Fabian Gottlieb von Bellingshausen, a captain in the Russian Imperial Navy; Edward Bransfield, a captain in the Royal Navy; and Nathaniel Palmer, an American sealer out of Stonington, Connecticut.
It is certain that the expedition, led by von Bellingshausen and Lazarev on the ships "Vostok" and "Mirny", reached on 28 January 1820 a point within 32 km from Princess Martha Coast and recorded the sight of an ice shelf at that became known as the Fimbul ice shelf. On 30 January 1820, Bransfield sighted Trinity Peninsula, the northernmost point of the Antarctic mainland, while Palmer sighted the mainland in the area south of Trinity Peninsula in November 1820. Von Bellingshausen's expedition also discovered Peter I Island and Alexander I Island, the first islands to be discovered south of the circle.
Exploration.
Only slightly more than a year later, the first landing on the Antarctic mainland was arguably by the American Captain John Davis, a sealer, who claimed to have set foot there on 7 February 1821, though this is not accepted by all historians.
In December 1821, Nathaniel Palmer, an American sealer looking for seal breeding grounds, sighted what is now known as the Antarctic Peninsula, located between 55 and 80 degrees west. In 1823, James Weddell, a British sealer, sailed into what is now known as the Weddell Sea.
Charles Wilkes, as commander of a United States Navy expedition in 1840, discovered what is now known as Wilkes Land, a section of the continent around 120 degrees East.
After the North Magnetic Pole was located in 1831, explorers and scientists began looking for the South Magnetic Pole. One of the explorers, James Clark Ross, a British naval officer, identified its approximate location, but was unable to reach it on his trip in 1841. Commanding the British ships "Erebus" and "Terror", he braved the pack ice and approached what is now known as the Ross Ice Shelf, a massive floating ice shelf over 100 ft high. His expedition sailed eastward along the southern Antarctic coast discovering mountains which were since named after his ships: Mount Erebus, the most active volcano on Antarctica, and Mount Terror.
The first documented landing on the mainland of East Antarctica was at Victoria Land by the American sealer Mercator Cooper on 26 January 1853.
These explorers, despite their impressive contributions to South Polar exploration, were unable to penetrate the interior of the continent and, rather, formed a broken line of discovered lands along the coastline of Antarctica. What followed this period of Antarctic interest is what historian H.R. Mill called 'the age of averted interest'. Following the expedition South by the ships "Erebus" and "Terror" under James Clark Ross (January, 1841), he suggested that there were no scientific discoveries, or 'problems', worth exploration in the far South.
It is considered that Ross' influence, as well as the loss of the Franklin expedition in the Arctic, lead to disinterest in polar interest, particularly by the Royal Society - the organization that helped oversee many Arctic explorations, including those that would be made by Shackleton and Scott. However, in the following twenty years after Ross' return, there was a general lull internationally in Antarctic exploration.
Heroic Age of Antarctic Exploration.
The Heroic Age of Antarctic Exploration began at the end of the 19th century and closed with Ernest Shackleton's Imperial Trans-Antarctic Expedition in 1917.
During this period the Antarctic continent became the focus of an international effort that resulted in intensive scientific and geographical exploration and in which 17 major Antarctic expeditions were launched from ten countries.
Origins.
The initial impetus for the Heroic Age of Antarctic Exploration was a lecture given by Dr. John Murray entitled "The Renewal of Antarctic Exploration", given to the Royal geographical Society in London, November 27, 1893. Murray advocated that research into the Antarctic should be organised to "resolve the outstanding geographical questions still posed in the south". Furthermore, the Royal Geographic Society instated an Antarctic Committee shortly prior to this, in 1887, which successfully encouraged many whalers to explore the Southern regions of the world and laid the groundwork for the lecture given by Murray.
In August 1895 the Sixth International Geographical Congress in London passed a general resolution calling on scientific societies throughout the world to promote the cause of Antarctic exploration "in whatever ways seem to them most effective". Such work would "bring additions to almost every branch of science". The Congress had been addressed by the Norwegian Carsten Borchgrevink, who had just returned from a whaling expedition during which he had become one of the first to set foot on the Antarctic mainland. During his address, Borchgrevink outlined plans for a full-scale pioneering Antarctic expedition, to be based at Cape Adare.
The Heroic Age was inaugurated by an expedition launched by the Belgian Geographical Society in 1897; Borchgrevink followed a year later with a privately sponsored British expedition. (Some histories consider the "Discovery" expedition, which departed in 1901, as the first proper expedition of the Heroic Age.)
The Belgian Antarctic Expedition was led by Belgian Adrian de Gerlache. In 1898, they became the first men to spend winter on Antarctica, when their ship Belgica became trapped in the ice. They became stuck on 28 February 1898, and only managed to get out of the ice on 14 March 1899.
During their forced stay, several men lost their sanity, not only because of the Antarctic winter night and the endured hardship, but also because of the language problems between the different nationalities. This was the first expedition to overwinter within the Antarctic Circle.
Early British expeditions.
The Southern Cross Expedition began in 1898 and lasted for two years. This was the first expedition to overwinter on the Antarctic mainland (Cape Adare) and was the first to make use of dogs and sledges. It made the first ascent of The Great Ice Barrier, (The Great Ice Barrier later became formally known as the Ross Ice Shelf). The expedition set a Farthest South record at 78°30'S. It also calculated the location of the South Magnetic Pole.
The Discovery Expedition was then launched, from 1901–04 and was led by Robert Falcon Scott. It made the first ascent of the Western Mountains in Victoria Land, and discovered the polar plateau. Its southern journey set a new Farthest South record, 82°17'S. Many other geographical features were discovered, mapped and named. This was the first of several expeditions based in McMurdo Sound.
A year later, the Scottish National Antarctic Expedition was launched, headed by William Speirs Bruce. 'Ormond House' was established as a meteorological observatory on Laurie Island in the South Orkneys and was the first permanent base in Antarctica. The Weddell Sea was penetrated to 74°01'S, and the coastline of Coats Land was discovered, defining the sea's eastern limits.
Ernest Shackleton, who had been a member of Scott's expedition, organized and led the Nimrod Expedition from 1907 to 1909. The expedition's primary objective was of reaching the South Pole. Based in McMurdo Sound, the expedition pioneered the Beardmore Glacier route to the South Pole, and the (limited) use of motorised transport. Its southern march reached 88°23'S, a new Farthest South record 97 geographical miles from the Pole before having to turn back. During the expedition, Shackleton was the first to reach the polar plateau. Parties led by T. W. Edgeworth David also became the first to climb Mount Erebus and to reach the South Magnetic Pole.
Expeditions from other countries.
The First German Antarctic Expedition was sent to investigate eastern Antarctica in 1901. It discovered the coast of Kaiser Wilhelm II Land, and Mount Gauss. The expedition's ship became trapped in ice, however, which prevented more extensive exploration.
The Swedish Antarctic Expedition, operating at the same time worked in the east coastal area of Graham Land, and was marooned on Snow Hill Island and Paulet Island in the Weddell Sea, after the sinking of its expedition ship. It was rescued by the Argentinian naval vessel "Uruguay".
The French organized their first expedition in 1903 under the leadership of Jean-Baptiste Charcot. Originally intended as a relief expedition for the stranded Nordenskiöld party, the main work of this expedition was the mapping and charting of islands and the western coasts of Graham Land, on the Antarctic peninsula. A section of the coast was explored, and named Loubet Land after the President of France.
A follow up trip was organized from 1908–10 which continued the earlier work of the French expedition with a general exploration of the Bellingshausen Sea, and the discovery of islands and other features, including Marguerite Bay, Charcot Island, Renaud Island, Mikkelsen Bay, Rothschild Island.
Race to the Pole.
The prize of the Heroic age was to reach the South Pole. Two expeditions set off in 1910 to attain this goal; a party led by Norwegian polar explorer Roald Amundsen from the ship "Fram" and Robert Falcon Scott's British group from the "Terra Nova".
Amundsen succeeded in reaching the Pole on 14 December 1911 using a route from the Bay of Whales to the polar plateau via the Axel Heiberg Glacier.
Scott and his four companions reached the South Pole via the Beardmore route on 17 January 1912, 33 days after Amundsen. All five died on the return journey from the Pole, through a combination of starvation and cold. The Amundsen-Scott South Pole Station was later named after these two men.
Further expeditions.
The Australasian Antarctic Expedition took place between 1911-1914 and was led by Sir Douglas Mawson. It concentrated on the stretch of Antarctic coastline between Cape Adare and Mount Gauss, carrying out mapping and survey work on coastal and inland territories.
Discoveries included Commonwealth Bay, Ninnis Glacier, Mertz Glacier, and Queen Mary Land. Major accomplishments were made in geology, glaciology and terrestrial biology.
The Imperial Trans-Antarctic Expedition of 1914-1917 was led by Ernest Shackleton and set out to cross the continent via the South pole. However, their ship, the "Endurance", was trapped and crushed by pack ice in the Weddell Sea before they were able to land. The expedition members survived after a journey on sledges over pack ice, a prolonged drift on an ice-floe, and a voyage in three small boats to Elephant Island. Then Shackleton and five others crossed the Southern Ocean in an open boat called "James Caird" and made the first crossing of South Georgia to raise the alarm at the whaling station Grytviken.
A related component of the Trans-Antarctic Expedition was the Ross Sea party, led by Aeneas Mackintosh. Its objective was to lay depots across the Great Ice Barrier, in order to supply Shackleton's party crossing from the Weddell Sea. All the required depots were laid, but in the process three men, including the leader Mackintosh, lost their lives.
Shackleton's last expedition and the one that brought the 'Heroic Age' to a close, was the Shackleton–Rowett Expedition from 1921-1922 on board the ship "Quest". Its vaguely defined objectives included coastal mapping, a possible continental circumnavigation, the investigation of sub-Antarctic islands, and oceanographic work. After Shackleton's death on 5 January 1922, "Quest" completed a shortened programme before returning home.| 
Further exploration.
By air.
After Shackleton's last expedition, there was a hiatus in Antarctic exploration for about seven years. From 1929, aircraft and mechanized transportation were increasingly used, earning this period the sobriquet of the 'Mechanical Age'. Hubert Wilkins first visited Antarctica in 1921-1922 as an ornithologist attached to the Shackleton-Rowett Expedition. From 1927, Wilkins and pilot Carl Ben Eielson began exploring the Arctic by aircraft.
On 15 April 1928, only a year after Charles Lindbergh's flight across the Atlantic, Wilkins and Eielson made a trans-Arctic crossing from Point Barrow, Alaska, to Spitsbergen, arriving about 20 hours later on 16 April, touching along the way at Grant Land on Ellesmere Island. For this feat and his prior work, Wilkins was knighted.
With financial backing from William Randolph Hearst, Wilkins returned to the South Pole and flew over Antarctica in the "San Francisco". He named the island of Hearst Land after his sponsor.
US Navy Rear Admiral Richard Evelyn Byrd led five expeditions to Antarctica during the 1930s, 1940s, and 1950s. He overflew the South Pole with pilot Bernt Balchen on November 28 and 29, 1929, to match his overflight of the North Pole in 1926. Byrd's explorations had science as a major objective and extensively used the aircraft to explore the continent.
Captain Finn Ronne, Byrd's executive officer, returned to Antarctica with his own expedition in 1947–1948, with Navy support, three planes, and dogs. Ronne disproved the notion that the continent was divided in two and established that East and West Antarctica was one single continent, i.e. that the Weddell Sea and the Ross Sea are not connected. The expedition explored and mapped large parts of Palmer Land and the Weddell Sea coastline, and identified the Ronne Ice Shelf, named by Ronne after his wife Edith Ronne. Ronne covered 3,600 miles by ski and dog sled—more than any other explorer in history.
Overland crossing.
The 1955–58 Commonwealth Trans-Antarctic Expedition successfully completed the first overland crossing of Antarctica, via the South Pole. Although supported by the British and other Commonwealth governments, most of the funding came from corporate and individual donations.
It was headed by British explorer Dr Vivian Fuchs, with New Zealander Sir Edmund Hillary leading the New Zealand Ross Sea Support team. After spending the winter of 1957 at Shackleton Base, Fuchs finally set out on the transcontinental journey in November 1957, with a twelve-man team travelling in six vehicles; three Sno-Cats, two Weasels and one specially adapted "Muskeg" tractor. On route, the team were also tasked with carrying out scientific research including seismic soundings and gravimetric readings.
In parallel Hillary's team had set up Scott Base – which was to be Fuchs' final destination – on the opposite side of the continent at McMurdo Sound on the Ross Sea. Using three converted Massey Ferguson TE20 tractors and one Weasel (abandoned part-way), Hillary and his three men (Ron Balham, Peter Mulgrew and Murray Ellis), were responsible for route-finding and laying a line of supply depots up the Skelton Glacier and across the Polar Plateau on towards the South Pole, for the use of Fuchs on the final leg of his journey. Other members of Hillary's team carried out geological surveys around the Ross Sea and Victoria Land areas.
Hillary's party reached the South Pole on January 3, 1958, and was just the third (preceded by Amundsen in 1911 and Scott in 1912) to reach the Pole overland. Fuchs' team reached the Pole from the opposite direction on 19 January 1958, where they met up with Hillary. Fuchs then continued overland, following the route that Hillary had laid and on 2 March succeeded in reaching Scott Base, completing the first overland crossing of the continent by land via the South Pole.
Political history.
British claims.
The United Kingdom reasserted sovereignty over the Falkland Islands in the far South Atlantic in 1833 and maintained a continuous presence there. In 1908, the British government extended its territorial claim by declaring sovereignty over "South Georgia, the South Orkneys, the South Shetlands, and the Sandwich Islands, and Graham's Land, situated in the South Atlantic Ocean and on the Antarctic continent to the south of the 50th parallel of south latitude, and lying between the 20th and the 80th degrees of west longitude". All these territories were administered as Falkland Islands Dependencies from Stanley by the Governor of the Falkland Islands. The motivation for this declaration lay in the need for regulating and taxing the whaling industry effectively. Commercial operators would hunt whales in areas outside of the official boundaries of the Falkland Islands and its dependencies and there was a need to close this loophole.
In 1917, the wording of the claim was modified, so as to, among other things, unambiguously include all the territory in the sector stretching to the South Pole (thus encompassing all of the present-day British Antarctic Territory). The new claim covered "all islands and territories whatsoever between the 20th degree of west longitude and the 50th degree of west longitude which are situated south of the 50th parallel of south latitude; and all islands and territories whatsoever between the 50th degree of west longitude and the 80th degree of west longitude which are situated south of the 58th parallel of south latitude".
Under the ambition of Leopold Amery, the Under-Secretary of State for the Colonies, Britain attempted to incorporate the entire continent into the Empire. In a memorandum to the governor-generals for Australia and New Zealand, he wrote that 'with the exception of Chile and Argentina and some barren islands belonging to France... it is desirable that the whole of the Antarctic should ultimately be included in the British Empire.'
The first step was taken on 30 July 1923, when the British government passed an Order in Council under the British Settlements Act 1887, defining the new borders for the Ross Dependency - "that part of His Majesty's Dominions in the Antarctic Seas, which comprises all the islands and territories between the 160th degree of East Longitude and the 150th degree of West Longitude which are situated south of the 60th degree of South Latitude shall be named the Ross Dependency."
The Order in Council then went on to appoint the Governor-General and Commander-in Chief of New Zealand as the Governor of the territory.
In 1930, the United Kingdom claimed Enderby Land. In 1933, a British imperial order transferred territory south of 60° S and between meridians 160° E and 45° E to Australia as the Australian Antarctic Territory.
Following the passing of the Statute of Westminster in 1931, the government of the United Kingdom relinquished all control over the government of New Zealand and Australia. This however had no bearing on the obligations of the Governor-General of both countries in their capacity as Governor of the Antarctic territories.
Other European claims.
Meanwhile, alarmed by these unilateral declarations, the French government laid claim to a strip of the continent in 1924. The basis for their claim to Adélie Land lay on the discovery of the coastline in 1840 by the French explorer Jules Dumont d'Urville, who named it after his wife, Adèle. The British eventually decided to recognize this claim and the border between Adélie Land and Australian Antarctic Territory was fixed definitively in 1938.
These developments also concerned Norwegian whaling interests, who wished to avoid the British taxation of whaling stations in the Antarctic and were concerned that they would be commercially excluded from the continent. The whale-ship owner Lars Christensen financed several expeditions to the Antarctic with the view to claim land for Norway and establish stations on Norwegian territory to gain better privileges. The first expedition, led by Nils Larsen and Ola Olstad, landed on Peter I Island in 1929 and claimed the island for Norway. On 6 March 1931, a Norwegian royal proclamation declared the island under Norwegian sovereignty and on 23 March 1933 the island was declared a dependency.
The 1929 expedition led by Hjalmar Riiser-Larsen and Finn Lützow-Holm named the continental land mass near the island as Queen Maud Land, named after the Norwegian queen Maud of Wales. The territory was explored further during the "Norvegia" expedition of 1930–31. Negotiations with the British government in 1938 resulted in the western border of Queen Maud Land being set at 20°W.
Norway's claim was disputed by Nazi Germany, which in 1938 dispatched the German Antarctic Expedition, led by Alfred Ritscher, to fly over as much of it as possible. The ship "Schwabenland" reached the pack ice off Antarctica on 19 January 1939. During the expedition, an area of about 350000 sqkm was photographed from the air by Ritscher, who dropped darts inscribed with swastikas every 26 km. Germany eventually attempted to claim the territory surveyed by Ritscher under the name New Swabia, but lost any claim to the land following its defeat in the Second World War.
On 14 January 1939, five days prior to the German arrival, Queen Maud Land was annexed by Norway, after a royal decree announced that the land bordering the Falkland Islands Dependencies in the west and the Australian Antarctic Dependency in the east was to be brought under Norwegian sovereignty. The primary basis for the annexation was to secure the Norwegian whaling industry's access to the region. In 1948, Norway and the United Kingdom agreed to limit Queen Maud Land to from 20°W to 45°E, and that the Bruce Coast and Coats Land were to be incorporated into Norwegian territory.
South American involvement.
This encroachment of foreign powers was a matter of immense disquiet to the nearby South American countries, Argentina and Chile. Taking advantage of a European continent plunged into turmoil with the onset of the Second World War, Chile's president, Pedro Aguirre Cerda declared the establishment of a Chilean Antarctic Territory in areas already claimed by Britain.
Argentina had an even longer history of involvement in the Continent. Already in 1904 the Argentine government began a permanent occupation in the area with the purchase of a meteorological station on Laurie Island established in 1903 by Dr William S. Bruce's Scottish National Antarctic Expedition. Bruce offered to transfer the station and instruments for the sum of 5.000 pesos, on the condition that the government committed itself to the continuation of the scientific mission. British officer William Haggard also sent a note to the Argentine Foreign Minister, Jose Terry, ratifying the terms of Bruce proposition.
In 1906, Argentina communicated to the international community the establishment of a permanent base on South Orkney Islands. However, Haggard responded by reminding Argentina that the South Orkneys were British. The British position was that Argentine personnel was granted permission only for the period of one year. The Argentine government entered into negotiations with the British in 1913 over the possible transfer of the island. Although these talks were unsuccessful, Argentina attempted to unilaterally establish their sovereignty with the erection of markers, national flags and other symbols.
 Finally, with British attention elsewhere, Argentina declared the establishment of Argentine Antarctica in 1943, claiming territory that overlapped with British ( 20°W to 80°W) and the earlier Chilean (53°W to 90°W) claims.
In response to this and earlier German explorations, the British Admiralty and Colonial Office launched Operation Tabarin in 1943 to reassert British territorial claims against Argentine and Chilean incursion and establish a permanent British presence in the Antarctic. The move was also motivated by concerns within the Foreign Office about the direction of United States post-war activity in the region.
A suitable cover story was the need to deny use of the area to the enemy. The "Kriegsmarine" was known to use remote islands as rendezvous points and as shelters for commerce raiders, U-boats and supply ships. Also, in 1941, there existed a fear that Japan might attempt to seize the Falkland Islands, either as a base or to hand them over to Argentina, thus gaining political advantage for the Axis and denying their use to Britain.
In 1943, British personnel from HMS "Carnarvon Castle" removed Argentine flags from Deception Island. The expedition was led by Lieutenant James Marr and left the Falkland Islands in two ships, HMS "William Scoresby" (a minesweeping trawler) and "Fitzroy", on Saturday January 29, 1944.
Bases were established during February near the abandoned Norwegian whaling station on Deception Island, where the Union Flag was hoisted in place of Argentine flags, and at Port Lockroy (on February 11) on the coast of Graham Land. A further base was founded at Hope Bay on February 13, 1945, after a failed attempt to unload stores on February 7, 1944. Symbols of British sovereignty, including post offices, signposts and plaques were also constructed and postage stamps were issued.
Operation Tabarin provoked Chile to organize its First Chilean Antarctic Expedition in 1947-48, where the Chilean president Gabriel González Videla personally inaugurated one of its bases.
Following the end of the war in 1945, the British bases were handed over to civilian members of the newly created Falkland Islands Dependencies Survey (subsequently the British Antarctic Survey) the first such national scientific body to be established in Antarctica.
Post war developments.
Friction between Britain and the Latin American states continued into the post war period. Royal Navy warships were despatched in 1948 to prevent naval incursions and in 1952, an Argentine shore party at Hope Bay (the British Base "D", established there in 1945, came up against the Argentine Esperanza Base, est. 1952) fired a machine gun over the heads of a British Antarctic Survey team unloading supplies from the "John Biscoe". The Argentines later extended a diplomatic apology, saying that there had been a misunderstanding and that the Argentine military commander on the ground had exceeded his authority.
The United States became politically interested in the Antarctic continent before and during WWII. The United States Antarctic Service Expedition, from 1939-1941, was sponsored by the government with additional support came from donations and gifts by private citizens, corporations and institutions. The objectives of the Expedition, outlined by President Franklin D. Roosevelt, was to establish two bases: East Base, in the vicinity of Charcot Island, and West Base, in the vicinity of King Edward VII Land. After operating successfully for two years, but with international tensions on the rise, it was considered wise to evacuate the two bases. However,
Immediately after the war, American interest was rekindled with an explicitly geopolitical motive. Operation Highjump, from 1946-1947 was organized by Rear Admiral Richard E. Byrd Jr. and included 4,700 men, 13 ships, and multiple aircraft. The primary mission of Operation Highjump was to establish the Antarctic research base Little America IV, for the purpose of training personnel and testing equipment in frigid conditions and amplifying existing stores of knowledge of hydrographic, geographic, geological, meteorological and electromagnetic propagation conditions in the area. The mission was also aimed at consolidating and extending United States sovereignty over the largest practicable area of the Antarctic continent, although this was publicly denied as a goal even before the expedition ended.
Towards an international treaty.
Meanwhile, in an attempt at ending the impasse, Britain submitted an application to the International Court of Justice in 1955 to adjudicate between the territorial claims of Britain, Argentina and Chile. This proposal failed, as both Latin American countries rejected submitting to an international arbitration procedure.
Negotiations towards the establishment of an international condominium over the continent first began in 1948, involving the 7 claimant powers (Britain, Australia, New Zealand, France, Norway, Chile and Argentina) and the US. This attempt was aimed at excluding the Soviet Union from the affairs of the continent and rapidly fell apart when the USSR declared an interest in the region, refused to recognize any claims of sovereignty and reserved the right to make its own claims in 1950.
An important impetus toward the formation of the Antarctic Treaty System in 1959, was the International Geophysical Year, 1957-1958. This year of international scientific cooperation triggered an 18-month period of intense Antarctic science. More than 70 existing national scientific organizations then formed IGY committees, and participated in the cooperative effort. The British established Halley Research Station in 1956 by an expedition from the Royal Society. Sir Vivian Fuchs headed the Commonwealth Trans-Antarctic Expedition, which completed the first overland crossing of Antarctica in 1958. In Japan, the Japan Maritime Safety Agency offered ice breaker Sōya as the South Pole observation ship and Showa Station was built as the first Japanese observation base on Antarctica.
France contributed with Dumont d'Urville Station and Charcot Station in Adélie Land. The ship "Commandant Charcot" of the French Navy spent nine months of 1949/50 at the coast of Adelie Land, performing ionospheric soundings. The US erected the Amundsen–Scott South Pole Station as the first permanent structure directly over the South Pole in January 1957.
Finally, to prevent the possibility of military conflict in the region, the United States, United Kingdom, the Soviet Union and 9 other countries with significant interests negotiated and signed the Antarctic Treaty in 1959. The treaty entered into force in 1961 and sets aside Antarctica as a scientific preserve, established freedom of scientific investigation and banned military activity on that continent. The treaty was the first arms control agreement established during the Cold War.
Recent history.
A baby, named Emilio Marcos de Palma, was born near Hope Bay on 7 January 1978, becoming the first baby born on the continent. He also was born farther south than anyone in history.
On 28 November 1979, an Air New Zealand DC-10 on a sightseeing trip crashed into Mount Erebus on Ross Island, killing all 257 people on board.
Børge Ousland, a Norwegian explorer, finished the first unassisted Antarctic solo crossing on January 18, 1997.
On 23 November 2007, the MS "Explorer" struck an iceberg and sank, but all on board were rescued by nearby ships, including a passing Norwegian cruise ship, the MS "Nordnorge".

</doc>
<doc id="51319" url="http://en.wikipedia.org/wiki?curid=51319" title="Intellectual history">
Intellectual history

Intellectual history refers to the historiography of major ideas and thinkers. This history cannot be considered without the knowledge of the men and women who created, discussed, wrote about, and in other ways were concerned with ideas. Intellectual history as practiced by historians is parallel to the history of philosophy as done by philosophers, and is more akin to the history of ideas. Its central premise is that ideas do not develop in isolation from the people who create and use them, and that one must study ideas not as abstract propositions but in terms of the culture, lives, and historical contexts that produced them.
Intellectual history aims to understand ideas from the past by understanding them in context. The term "context" in the preceding sentence is ambiguous: it can be political, cultural, intellectual, and social. One can read a text both in terms of a chronological context (for example, as a contribution to a discipline or tradition as it extended over time) or in terms of a contemporary intellectual moment (for example, as participating in a debate particular to a certain time and place). Both of these acts of contextualization are typical of what intellectual historians do, nor are they exclusive. Generally speaking, intellectual historians seek to place concepts and texts from the past in multiple contexts.
It is important to realize that intellectual history is not just the history of intellectuals. It studies ideas as they are expressed in texts, and as such is different from other forms of cultural history which deal also with visual and other non-verbal forms of evidence. Any written trace from the past can be the object of intellectual history. The concept of the "intellectual" is relatively recent, and suggests someone professionally concerned with thought. Instead, anyone who has put pen to paper to explore his or her thoughts can be the object of intellectual history. A famous example of an intellectual history of a non-canonical thinker is Carlo Ginzburg's study of a 16th-century Italian miller, Menocchio, in his seminal work "The Cheese and the Worms".
Although the field emerged from European disciplines of "Kulturgeschichte" and "Geistesgeschichte", the historical study of ideas has engaged not only western intellectual traditions but others as well, including those in other parts of the world. Increasingly, historians are calling for a Global intellectual history that will show the parallels and interrelations in the history of thought of all human societies. Another important trend has been the history of the book and of reading, which has drawn attention to the material aspects of how books were designed, produced, distributed, and read.
Intellectual historiography.
Intellectual history as a self-conscious discipline is a relatively recent phenomenon. It has precedents, however, in the history of philosophy, the history of ideas, and in cultural history as practiced since Burckhardt or indeed since Voltaire. The history of the human mind, as it was called in the eighteenth century, was of great concern to scholars and philosophers, and their efforts can in part be traced to Francis Bacon’s call for what he termed a literary history in his The Advancement of Learning. However, the discipline of intellectual history as it is now understood emerged only in the immediate postwar period, in its earlier incarnation as "the history of ideas" under the leadership of Arthur Lovejoy, the founder of the Journal of the History of Ideas. Since that time, Lovejoy's formulation of "unit-ideas" has been discredited and replaced by more nuanced and more historically sensitive accounts of intellectual activity, and this shift is reflected in the replacement of the phrase history of ideas by "intellectual history".
In the United Kingdom, the history of political thought has been a particular focus since the late 1960s and is associated especially with the Faculty of History at the University of Cambridge, where until recently such scholars as John Dunn and Quentin Skinner studied European political thought in its historical context, emphasizing the emergence and development of such concepts as the state and freedom. Skinner in particular is renowned for his provocative methodological essays, which were and are widely read by philosophers and practitioners of other humanistic disciplines, and did much to give prominence to the practice of intellectual history. The University of Sussex in the UK has also achieved a reputation in this field of study, and the Sussex emphasis on broad interdisciplinary study has been particularly useful in relevant teaching and research.
In the United States, intellectual history is understood more broadly to encompass many different forms of intellectual output, not just the history of political ideas, and it includes such fields as the history of historical thought, associated especially with Anthony Grafton of Princeton University and J.G.A. Pocock of Johns Hopkins University. Formalized in 2010, the History and Culture Ph.D. at Drew University is one of a few graduate programs in the US currently specializing in intellectual history, both in its American and European contexts. Despite the prominence of early modern intellectual historians (those studying the age from the Renaissance to the Enlightenment), the intellectual history of the modern period has also been the locus of intense and creative output on both sides of the Atlantic. Prominent examples of such work include Louis Menand's "" and Martin Jay's "The Dialectical Imagination".
In continental Europe, equivalents of intellectual history can be found. An example is Reinhart Koselleck’s "Begriffsgeschichte" (history of concepts), though there are methodological differences between the work of Koselleck and his followers and the work of Anglo-American intellectual historians.
Further reading.
Samuel Moyn and Andrew Sartori (editors), "Global intellectual history" (2013)
References.
</dl>

</doc>
<doc id="51320" url="http://en.wikipedia.org/wiki?curid=51320" title="Ancient history">
Ancient history

Ancient history is the aggregate of past events from the beginning of recorded human history to the Early Middle Ages or the Postclassical Era. The span of recorded history is roughly 5,000 years, beginning with Sumerian Cuneiform script, the oldest discovered form of coherent writing from the protoliterate period around the 30th century BC.
The term classical antiquity is often used to refer to history in the Old World from the beginning of recorded Greek history in 776 BC (First Olympiad). This roughly coincides with the traditional date of the founding of Rome in 753 BC, the beginning of the history of ancient Rome, and the beginning of the Archaic period in Ancient Greece. Although the ending date of ancient history is disputed, some Western scholars use the fall of the Western Roman Empire in 476 AD (the most used), the closure of the Platonic Academy in 529 AD, the death of the emperor Justinian I in 565 AD, the coming of Islam or the rise of Charlemagne as the end of ancient and Classical European history.
In India, ancient history includes the early period of the Middle Kingdoms, and, in China, the time up to the Qin Dynasty.
Study.
Historians have two major avenues which they take to better understand the ancient world: archaeology and the study of source texts. Primary sources are those sources closest to the origin of the information or idea under study. Primary sources have been distinguished from secondary sources, which often cite, comment on, or build upon primary sources.
Archaeology.
Archaeology is the excavation and study of artefacts in an effort to interpret and reconstruct past human behavior. Archaeologists excavate the ruins of ancient cities looking for clues as to how the people of the time period lived. Some important discoveries by archaeologists studying ancient history include:
Source text.
Most of what is known of the ancient world comes from the accounts of antiquity's own historians. Although it is important to take into account the bias of each ancient author, their accounts are the basis for our understanding of the ancient past. Some of the more notable ancient writers include Herodotus, Thucydides, Arrian, Plutarch, Polybius, Sima Qian, Sallust, Livy, Josephus, Suetonius, and Tacitus.
A fundamental difficulty of studying ancient history is that recorded histories cannot document the entirety of human events, and only a fraction of those documents have survived into the present day. Furthermore, the reliability of the information obtained from these surviving records must be considered. Few people were capable of writing histories, as literacy was not widespread in almost any culture until long after the end of ancient history.
The earliest known systematic historical thought emerged in ancient Greece, beginning with Herodotus of Halicarnassus (484–c. 425 BC). Thucydides largely eliminated divine causality in his account of the war between Athens and Sparta, establishing a rationalistic element which set a precedent for subsequent Western historical writings. He was also the first to distinguish between cause and immediate origins of an event.
The Roman Empire was one of the ancient world's most literate cultures, but many works by its most widely read historians are lost. For example, Livy, a Roman historian who lived in the 1st century BC, wrote a history of Rome called "Ab Urbe Condita" ("From the Founding of the City") in 144 volumes; only 35 volumes still exist, although short summaries of most of the rest do exist. Indeed, only a minority of the work of any major Roman historian has survived.
Chronology.
Prehistory.
Prehistory is the period before written history. The early human migrations in the Lower Paleolithic saw Homo erectus spread across Eurasia 1.8 million years ago. The controlled use of fire occurred 800 thousand years ago in the Middle Paleolithic. 250 thousand years ago, Homo sapiens (modern humans) emerged in Africa. 60–70 thousand years ago, Homo sapiens migrated out of Africa along a coastal route to South and Southeast Asia and reached Australia. 50 thousand years ago, modern humans spread from Asia to the Near East. Europe was first reached by modern humans 40 thousand years ago. Humans migrated to the Americas about 15 thousand years ago in the Upper Paleolithic,
The 10th millennium BC is the earliest given date for the invention of agriculture and the beginning of the ancient era. Göbekli Tepe was erected by hunter-gatherers in the 10th millennium BC (c. 11,500 years ago), before the advent of sedentism. Together with Nevalı Çori, it has revolutionized understanding of the Eurasian Neolithic. In the 7th millennium BC, Jiahu culture began in China. By the 5th millennium BC, the late Neolithic civilizations saw the invention of the wheel and the spread of proto-writing. In the 4th millennium BC, the Cucuteni-Trypillian culture in the Ukraine-Moldova-Romania region develops. By 3400 BC, "proto-literate" cuneiform is spread in the Middle East. The 30th century BC, referred to as the Early Bronze Age II, saw the beginning of the literate period in Mesopotamia and Ancient Egypt. Around the 27th century BC, the Old Kingdom of Egypt and the First Dynasty of Uruk are founded, according to the earliest reliable regnal eras.
Timeline of ancient history.
Middle to Late Bronze Age.
The Bronze Age forms part of the three-age system. It follows the Neolithic Age in some areas of the world.
In the 24th century BC, the Akkadian Empire was founded in Mesopotamia.
The First Intermediate Period of Egypt of the 22nd century BC was followed by the Middle Kingdom of Egypt between the 21st to 17th centuries BC. The Sumerian Renaissance also developed c. the 21st century BC in Ur. Around the 18th century BC, the Second Intermediate Period of Egypt began.
By 1600 BC, Mycenaean Greece developed. The beginning of the Shang Dynasty emerged in China in this period, and there was evidence of a fully developed Chinese writing system. The beginning of Hittite dominance of the Eastern Mediterranean region is also seen in the 1600s BC. The time from the 16th to the 11th centuries BC around the Nile is called the New Kingdom of Egypt. Between 1550 BC and 1292 BC, the Amarna Period developed in Egypt.
Early Iron Age.
The Iron Age is the last principal period in the three-age system, preceded by the Bronze Age. Its date and context vary depending on the country or geographical region.
During the 13th to 12th centuries BC, the Ramesside Period occurred in Egypt. Around 1200 BC, the Trojan War was thought to have taken place. By c. 1180 BC, the disintegration of the Hittite Empire was underway.
In 1046 BC, the Zhou force, led by King Wu of Zhou, overthrows the last king of the Shang Dynasty. The Zhou Dynasty is established in China shortly thereafter.
Pirak is an early iron-age site in Balochistan, Pakistan, going back to ca 1200 BC. This period is believed to be the beginning of Iron Age India and the subcontinent.
In 1000 BC, the Mannaeans Kingdom begins in Western Asia. Around the 10th to 7th centuries BC, the Neo-Assyrian Empire forms in Mesopotamia. In 800 BC, the rise of Greek city-states begins. In 776 BC, the first recorded Olympic Games are held.
Classical Antiquity.
"Classical antiquity" is a broad term for a long period of cultural history centered around the Mediterranean Sea, which begins roughly with the earliest-recorded Greek poetry of Homer (9th century BC), and continues through the rise of Christianity and the fall of the Western Roman Empire (5th century AD), ending in the dissolution of classical culture with the close of Late Antiquity.
Such a wide sampling of history and territory covers many rather disparate cultures and periods. "Classical antiquity" typically refers to an idealized vision of later people, of what was, in Edgar Allan Poe's words, "the glory that was Greece, the grandeur that was Rome!" In the 18th and 19th centuries AD, reverence for classical antiquity was much greater in Europe and the United States than it is today. Respect for the ancients of Greece and Rome affected politics, philosophy, sculpture, literature, theatre, education, and even architecture and sexuality.
In politics, the presence of a Roman Emperor was felt to be desirable long after the empire fell. This tendency reached its peak when Charlemagne was crowned "Roman Emperor" in the year 800, an act which led to the formation of the Holy Roman Empire. The notion that an emperor is a monarch who outranks a mere king dates from this period. In this political ideal, there would always be a Roman Empire, a state whose jurisdiction extended to the entire civilized world.
Epic poetry in Latin continued to be written and circulated well into the 19th century. John Milton and even Arthur Rimbaud received their first poetic educations in Latin. Genres like epic poetry, pastoral verse, and the endless use of characters and themes from Greek mythology left a deep mark on Western literature.
In architecture, there have been several Greek Revivals, (though while apparently more inspired in retrospect by Roman architecture than Greek). Still, one needs only to look at Washington, DC to see a city filled with large marble buildings with façades made out to look like Roman temples, with columns constructed in the classical orders of architecture.
In philosophy, the efforts of St Thomas Aquinas were derived largely from the thought of Aristotle, despite the intervening change in religion from paganism to Christianity. Greek and Roman authorities such as Hippocrates and Galen formed the foundation of the practice of medicine even longer than Greek thought prevailed in philosophy. In the French theatre, tragedians such as Molière and Racine wrote plays on mythological or classical historical subjects and subjected them to the strict rules of the classical unities derived from Aristotle's "Poetics". The desire to dance like a latter-day vision of how the ancient Greeks did it moved Isadora Duncan to create her brand of ballet. The Renaissance was partly caused by the rediscovery of classic antiquity.
Classical ancient history end.
The transition period from Classical Antiquity to the Early Middle Ages is known as Late Antiquity. Some key dates marking that transition are:
The beginning of the post-classical age (known generally as the Middle Ages) is a period in the history of Europe following the fall of the Western Roman Empire spanning roughly five centuries from AD 500 to 1000. Aspects of continuity with the earlier classical period are discussed in greater detail under the heading "Late Antiquity". Late Antiquity is the transitional centuries from Classical Antiquity to the Middle Ages in both mainland Europe and the Mediterranean world: generally from the end of the Roman Empire's Crisis of the 3rd century (c. 284) to the Islamic conquests and the re-organization of the Byzantine Empire under Heraclius.
Prominent civilizations.
Southwest Asia (Near East).
The Ancient Near East is considered the cradle of civilization. It was the first to practice intensive year-round agriculture; created the first coherent writing system, invented the potter's wheel and then the vehicular- and mill wheel, created the first centralized governments, law codes and empires, as well as introducing social stratification, slavery and organized warfare, and it laid the foundation for the fields of astronomy and mathematics.
Mesopotamia.
Mesopotamia is the site of some of the earliest known civilizations in the world. Early settlement of the alluvial plain lasted from the Ubaid period (late 6th millennium BC) through the Uruk period (4th millennium BC) and the Dynastic periods (3rd millennium BC) until the rise of Babylon in the early 2nd millennium BC. The surplus of storable foodstuffs created by this economy allowed the population to settle in one place instead of migrating after crops and herds. It also allowed for a much greater population density, and in turn required an extensive labor force and division of labor. This organization led to the necessity of record keeping and the development of writing (c. 3500 BC).
Babylonia was an Amorite state in lower Mesopotamia (modern southern Iraq), with Babylon as its capital. Babylonia emerged when Hammurabi (fl. c. 1728–1686 BC, according to the short chronology) created an empire out of the territories of the former kingdoms of Sumer and Akkad. The Amorites being a Semitic people, Babylonia adopted the written Semitic Akkadian language for official use; they retained the Sumerian language for religious use, which by that time was no longer a spoken language. The Akkadian and Sumerian cultures played a major role in later Babylonian culture, and the region would remain an important cultural center, even under outside rule. The earliest mention of the city of Babylon can be found in a tablet from the reign of Sargon of Akkad, dating back to the 23rd century BC.
The Neo-Babylonian Empire, or Chaldea, was Babylonia under the rule of the 11th ("Chaldean") dynasty, from the revolt of Nabopolassar in 626 BC until the invasion of Cyrus the Great in 539 BC. Notably, it included the reign of Nebuchadrezzar II who conquered Judah and Jerusalem.
Akkad was a city and its surrounding region in central Mesopotamia. Akkad also became the capital of the Akkadian Empire. The city was probably situated on the west bank of the Euphrates, between Sippar and Kish (in present-day Iraq, about 50 km southwest of the center of Baghdad). Despite an extensive search, the precise site has never been found. Akkad reached the height of its power between the 24th and 22nd centuries BC, following the conquests of king Sargon of Akkad. Because of the policies of the Akkadian Empire toward linguistic assimilation, Akkad also gave its name to the predominant Semitic dialect: the Akkadian language, reflecting use of "akkadû" ("in the language of Akkad") in the Old Babylonian period to denote the Semitic version of a Sumerian text.
Assyria was originally (in the Middle Bronze Age) a region on the Upper Tigris river, named for its original capital, the ancient city of Assur. Later, as a nation and empire that came to control all of the Fertile Crescent, Egypt and much of Anatolia, the term "Assyria proper" referred to roughly the northern half of Mesopotamia (the southern half being Babylonia), with Nineveh as its capital. The Assyrian kings controlled a large kingdom at three different times in history. These are called the "Old" (20th to 15th centuries BC), "Middle" (15th to 10th centuries BC), and "Neo-Assyrian" (911–612 BC) kingdoms, or periods, of which the last is the most well known and best documented. Assyrians invented excavation to undermine city walls, battering rams to knock down gates, as well as the concept of a corps of engineers, who bridged rivers with pontoons or provided soldiers with inflatable skins for swimming.
Mitanni was an Indo-Iranian empire in northern Mesopotamia from c. 1500 BC. At the height of Mitanni power, during the 14th century BC, it encompassed what is today southeastern Turkey, northern Syria and northern Iraq, centered around its capital, Washukanni, whose precise location has not been determined by archaeologists.
Ancient Persia.
Elam is the name of an ancient civilization located in what is now southwest Iran. Archaeological evidence associated with Elam has been dated to before 5000 BC. According to available written records, it is known to have existed from around 3200 BC – making it among the world's oldest historical civilizations – and to have endured up until 539 BC. Its culture played a crucial role in the Gutian Empire, especially during the Achaemenid dynasty that succeeded it, when the Elamite language remained among those in official use. The Elamite period is considered a starting point for the history of Iran.
The Medes were an ancient Iranian people. They had established their own empire by the 6th century BC, having defeated the Neo-Assyrian Empire with the Chaldeans. The Medes are credited with the foundation of the first Iranian empire, the largest of its day until Cyrus the Great established a unified Iranian empire of the Medes and Persian, often referred to as the Achaemenid Persian Empire, by defeating his grandfather and overlord, Astyages the king of Media.
The Achaemenid Empire was the first of the Persian Empires to rule over significant portions of Greater Persia, and followed the Median Empire as the second great empire of the Persian people. It is noted in western history as the foe of the Greek city states in the Greco-Persian Wars, for freeing the Israelites from their Babylonian captivity, and for instituting Aramaic as the empire's official language. Because of the Empire's vast extent and long endurance, Persian influence upon the language, religion, architecture, philosophy, law and government of nations around the world lasts to this day. At the height of its power, the Achaemenid dynasty encompassed approximately 8.0 million square kilometers, held the greatest percentage of world population to date, and was territorially the largest empire of classical antiquity.
Parthia was an Iranian civilization situated in the northeastern part of modern Iran. Their power was based on a combination of the guerrilla warfare of a mounted nomadic tribe, with organizational skills to build and administer a vast empire – even though it never matched in power and extent the Persian empires that preceded and followed it. The Parthian empire was led by the Arsacid dynasty, which reunited and ruled over the Iranian plateau, after defeating and disposing the Hellenistic Seleucid Empire, beginning in the late 3rd century BC, and intermittently controlled Mesopotamia between 150 BC and 224 AD. It was the third native dynasty of ancient Iran (after the Median and the Achaemenid dynasties). Parthia had many wars with the Roman Empire.
The Sassanid Empire, lasting the length of the Late Antiquity period, is considered to be one of Iran's most important and influential historical periods. In many ways the Sassanid period witnessed the highest achievements of Persian civilization and constituted the last great Iranian Empire before the Muslim conquest and the adoption of Islam. During Sassanid times, Persia influenced Roman civilization considerably, and the Romans reserved for the Sassanid Persians alone the status of equals. Sassanid cultural influence extended far beyond the empire's territorial borders, reaching as far as Western Europe, Africa, China, and India, playing a role, for example, in the formation of both European and Asiatic medieval art.
Armenia.
The early history of the Hittite empire is known through tablets that may first have been written in the 17th century BC but survived only as copies made in the 14th and 13th centuries BC. These tablets, known collectively as the Anitta text, begin by telling how Pithana the king of Kussara or Kussar (a small city-state yet to be identified by archaeologists) conquered the neighbouring city of Neša (Kanesh). However, the real subject of these tablets is Pithana's son Anitta, who conquered several neighbouring cities, including Hattusa and Zalpuwa (Zalpa).
Assyrian inscriptions of Shalmaneser I (c. 1270 BC) first mention "Uruartri" as one of the states of Nairi – a loose confederation of small kingdoms and tribal states in the Armenian Highland from the 13th to 11th centuries BC. Uruartri itself was in the region around Lake Van. The Nairi states were repeatedly subjected to attacks by the Assyrians, especially under Tukulti-Ninurta I (c. 1240 BC), Tiglath-Pileser I (c. 1100 BC), Ashur-bel-kala (c. 1070 BC), Adad-nirari II (c. 900), Tukulti-Ninurta II (c. 890), and Ashurnasirpal II (883-859 BC).
The Kingdom of Armenia was an independent kingdom from 190 BC to 387 АD, and a client state of the Roman and Persian empires until 428. Between 95 BC - 55 BC under the rule of King Tigranes the Great, the kingdom of Armenia became a large and powerful empire stretching from the Caspian to the Mediterranean Seas. During this short time it was considered to be the most powerful state in the Roman East.
Arabia.
The history of Pre-Islamic Arabia before the rise of Islam in the 630s is not known in great detail. Archaeological exploration in the Arabian peninsula has been sparse; indigenous written sources are limited to the many inscriptions and coins from southern Arabia. Existing material consists primarily of written sources from other traditions (such as Egyptians, Greeks, Persians, Romans, etc.) and oral traditions later recorded by Islamic scholars.
The first known inscriptions of the Kingdom of Hadhramaut are known from the 8th century BC. It was first referenced by an outside civilization in an Old Sabaic inscription of Karab'il Watar from the early 7th century BC, in which the King of Hadramaut, Yada`'il, is mentioned as being one of his allies.
Dilmun appears first in Sumerian cuneiform clay tablets dated to the end of 4th millennium BC, found in the temple of goddess Inanna, in the city of Uruk. The adjective "Dilmun" refers to a type of axe and one specific official; in addition, there are lists of rations of wool issued to people connected with Dilmun.
The Sabaeans were an ancient people speaking an Old South Arabian language who lived in what is today Yemen, in south west Arabian Peninsula; from 2000 BC to the 8th century BC. Some Sabaeans also lived in D'mt, located in northern Ethiopia and Eritrea, due to their hegemony over the Red Sea. They lasted from the early 2nd millennium to the 1st century BC. In the 1st century BC it was conquered by the Himyarites, but after the disintegration of the first Himyarite empire of the Kings of Saba' and dhu-Raydan the Middle Sabaean Kingdom reappeared in the early 2nd century. It was finally conquered by the Himyarites in the late 3rd century.
The ancient Kingdom of Awsan with a capital at Hagar Yahirr in the wadi Markha, to the south of the wadi Bayhan, is now marked by a tell or artificial mound, which is locally named Hagar Asfal. Once it was one of the most important small kingdoms of South Arabia. The city seems to have been destroyed in the 7th century BC by the king and mukarrib of Saba Karib'il Watar, according to a Sabaean text that reports the victory in terms that attest to its significance for the Sabaeans.
The Himyar was a state in ancient South Arabia dating from 110 BC. It conquered neighbouring Saba (Sheba) in c. 25 BC, Qataban in c. 200 AD and Hadramaut c. 300 AD. Its political fortunes relative to Saba changed frequently until it finally conquered the Sabaean Kingdom around 280 AD. It was the dominant state in Arabia until 525 AD. The economy was based on agriculture.
Foreign trade was based on the export of frankincense and myrrh. For many years it was also the major intermediary linking East Africa and the Mediterranean world. This trade largely consisted of exporting ivory from Africa to be sold in the Roman Empire. Ships from Himyar regularly traveled the East African coast, and the state also exerted a considerable amount of political control of the trading cities of East Africa.
The Nabataean origins remain obscure. On the similarity of sounds, Jerome suggested a connection with the tribe Nebaioth mentioned in "Genesis", but modern historians are cautious about an early Nabatean history. The Babylonian captivity that began in 586 BC opened a power vacuum in Judah, and as Edomites moved into Judaean grazing lands, Nabataean inscriptions began to be left in Edomite territory (earlier than 312 BC, when they were attacked at Petra without success by Antigonus I). The first definite appearance was in 312 BC, when Hieronymus of Cardia, a Seleucid officer, mentioned the Nabateans in a battle report. In 50 BC, the Greek historian Diodorus Siculus cited Hieronymus in his report, and added the following: "Just as the Seleucids had tried to subdue them, so the Romans made several attempts to get their hands on that lucrative trade."
Petra or Sela was the ancient capital of Edom; the Nabataeans must have occupied the old Edomite country, and succeeded to its commerce, after the Edomites took advantage of the Babylonian captivity to press forward into southern Judaea. This migration, the date of which cannot be determined, also made them masters of the shores of the Gulf of Aqaba and the important harbor of Elath. Here, according to Agatharchides, they were for a time very troublesome, as wreckers and pirates, to the reopened commerce between Egypt and the East, until they were chastised by the Ptolemaic rulers of Alexandria.
The Lakhmid Kingdom was founded by the Lakhum tribe that immigrated out of Yemen in the 2nd century and ruled by the Banu Lakhm, hence the name given it. It was formed of a group of Arab Christians who lived in Southern Iraq, and made al-Hirah their capital in (266). The founder of the dynasty was 'Amr and the son Imru' al-Qais converted to Christianity. Gradually the whole city converted to that faith. Imru' al-Qais dreamt of a unified and independent Arab kingdom and, following that dream, he seized many cities in Arabia.
The Ghassanids were a group of South Arabian Christian tribes that emigrated in the early 3rd century from Yemen to the Hauran in southern Syria, Jordan and the Holy Land where they intermarried with Hellenized Roman settlers and Greek-speaking Early Christian communities. The Ghassanid emigration has been passed down in the rich oral tradition of southern Syria. It is said that the Ghassanids came from the city of Ma'rib in Yemen. There was a dam in this city, however one year there was so much rain that the dam was carried away by the ensuing flood. Thus the people there had to leave. The inhabitants emigrated seeking to live in less arid lands and became scattered far and wide. The proverb "They were scattered like the people of Saba" refers to that exodus in history. The emigrants were from the southern Arab tribe of Azd of the Kahlan branch of Qahtani tribes.
Levant.
Though the Ugaritic site is thought to have been inhabited earlier, Neolithic Ugarit was already important enough to be fortified with a wall early on. The first written evidence mentioning the city comes from the nearby city of Ebla, c. 1800 BC. Ugarit passed into the sphere of influence of Egypt, which deeply influenced its art.
Israel.
Israel and Judah were related Iron Age kingdoms of the ancient Levant and had existed during the Iron Ages and the Neo-Babylonian, Persian and Hellenistic periods.
The name Israel first appears in the stele of the Egyptian pharaoh Merneptah c. 1209 BC, "Israel is laid waste and his seed is no more." This "Israel" was a cultural and probably political entity of the central highlands, well enough established to be perceived by the Egyptians as a possible challenge to their hegemony, but an ethnic group rather than an organised state; Archaeologist Paula McNutt says: "It is probably ... during Iron Age I [that] a population began to identify itself as 'Israelite'," differentiating itself from its neighbours via prohibitions on intermarriage, an emphasis on family history and genealogy, and religion.
Israel had emerged by the middle of the 9th century BC, when the Assyrian king Shalmaneser III names "Ahab the Israelite" among his enemies at the battle of Qarqar (853). Judah emerged somewhat later than Israel, probably during the 9th century BC, but the subject is one of considerable controversy. Israel came into increasing conflict with the expanding neo-Assyrian empire, which first split its territory into several smaller units and then destroyed its capital, Samaria (722). A series of campaigns by the Neo-Babylonian Empire between 597 and 582 led to the destruction of Judah.
Followed by the fall of Babylon to the Persian empire, Jews were allowed, by Cyrus the Great, to return to Judea. The Hasmonean Kingdom (followed by the Maccabean revolt) had existed during the Hellenistic period and then the Herodian kingdom during the Roman period.
Phoenicians.
Phoenicia was an ancient civilization centered in the north of ancient Canaan, with its heartland along the coastal regions of modern day Lebanon, Syria and Israel. Phoenician civilization was an enterprising maritime trading culture that spread across the Mediterranean between the period of 1550 to 300 BC.
A written reference, Herodotus's account (written c. 440 BC) refers to a memory from 800 years earlier, which may be subject to question in the fullness of genetic results. ("History," I:1). This is a legendary introduction to Herodotus' brief retelling of some mythical Hellene-Phoenician interactions. Though few modern archaeologists would confuse this myth with history, a grain of truth may yet lie therein.
Africa.
Egypt.
Ancient Egypt was a long-lived civilization geographically located in north-eastern Africa. It was concentrated along the middle to lower reaches of the Nile River reaching its greatest extension during the 2nd millennium BC, which is referred to as the New Kingdom period. It reached broadly from the Nile Delta in the north, as far south as Jebel Barkal at the Fourth Cataract of the Nile. Extensions to the geographical range of ancient Egyptian civilization included, at different times, areas of the southern Levant, the Eastern Desert and the Red Sea coastline, the Sinai Peninsula and the Western Desert (focused on the several oases).
Ancient Egypt developed over at least three and a half millennia. It began with the incipient unification of Nile Valley polities around 3500 BC and is conventionally thought to have ended in 30 BC when the early Roman Empire conquered and absorbed Ptolemaic Egypt as a province. (Though this last did not represent the first period of foreign domination, the Roman period was to witness a marked, if gradual transformation in the political and religious life of the Nile Valley, effectively marking the termination of independent civilisational development).
The civilization of ancient Egypt was based on a finely balanced control of natural and human resources, characterised primarily by controlled irrigation of the fertile Nile Valley; the mineral exploitation of the valley and surrounding desert regions; the early development of an independent writing system and literature; the organisation of collective projects; trade with surrounding regions in east / central Africa and the eastern Mediterranean; finally, military ventures that exhibited strong characteristics of imperial hegemony and territorial domination of neighbouring cultures at different periods. Motivating and organizing these activities were a socio-political and economic elite that achieved social consensus by means of an elaborate system of religious belief under the figure of a (semi)-divine ruler (usually male) from a succession of ruling dynasties and which related to the larger world by means of polytheistic beliefs.
Nubia.
The Kushite state was formed before a period of Egyptian incursion into the area. The Kushite civilization has also been referred to as Nubia. The first cultures arose in Sudan before the time of a unified Egypt, and the most widespread is known as the Kerma civilization. It is through Egyptian, Hebrew, Roman and Greek records that most of our knowledge of Kush (Cush) comes.
It is also referred to as Ethiopia in ancient Greek and Roman records. According to Josephus and other classical writers, the Kushite Empire covered all of Africa, and some parts of Asia and Europe at one time or another. The Kushites are also famous for having buried their monarchs along with all their courtiers in mass graves. The Kushites also built burial mounds and pyramids, and shared some of the same gods worshipped in Egypt, especially Amon and Isis.
Axum.
The Axumite Empire was an important trading nation in northeastern Africa, growing from the proto-Aksumite period c. 4th century BC to achieve prominence by the 1st century AD. Its ancient capital is found in northern Ethiopia, the Kingdom used the name "Ethiopia" as early as the 4th century. Aksum is mentioned in the 1st century AD "Periplus of the Erythraean Sea" as an important market place for ivory, which was exported throughout the ancient world, and states that the ruler of Aksum in the 1st century AD was Zoscales, who, besides ruling in Aksum also controlled two harbours on the Red Sea: Adulis (near Massawa) and Avalites (Assab). He is also said to have been familiar with Greek literature. It is also the alleged resting place of the Ark of the Covenant and the home of the Queen of Sheba. Aksum was also the first major empire to convert to Christianity.
Land of Punt.
The Land of Punt, also called Pwenet, or Pwene by the ancient Egyptians, was a trading partner known for producing and exporting gold, aromatic resins, African blackwood, ebony, ivory, slaves and wild animals. Information about Punt has been found in ancient Egyptian records of trade missions to this region.
The exact location of Punt remains a mystery. The mainstream view is that Punt was located to the south-east of Egypt, most likely on the coast of the Horn of Africa. The earliest recorded Egyptian expedition to Punt was organized by Pharaoh Sahure of the Fifth Dynasty (25th century BC) although gold from Punt is recorded as having been in Egypt in the time of king Khufu of the Fourth Dynasty of Egypt. Subsequently, there were more expeditions to Punt in the Sixth Dynasty of Egypt, the Eleventh dynasty of Egypt, the Twelfth dynasty of Egypt and the Eighteenth dynasty of Egypt. In the Twelfth dynasty of Egypt, trade with Punt was celebrated in popular literature in "Tale of the Shipwrecked Sailor".
Nok culture.
The Nok culture appeared in Nigeria around 1000 BC and mysteriously vanished around 200 AD. The civilization’s social system is thought to have been highly advanced. The Nok civilization was considered to be the earliest sub-Saharan producer of life-sized Terracotta which have been discovered by archaeologists. A Nok sculpture resident at the Minneapolis Institute of Arts, portrays a sitting dignitary wearing a "Shepherds Crook" on the right arm, and a "hinged flail" on the left. These are symbols of authority associated with ancient Egyptian pharaohs, and the god Osiris, which suggests that an ancient Egyptian style of social structure, and perhaps religion, existed in the area of modern Nigeria during the late Pharonic period. (Informational excerpt copied from Nigeria and Nok culture articles)
Carthage.
Carthage was founded in 814 BC by Phoenician settlers from the city of Tyre, bringing with them the city-god Melqart. Ancient Carthage was an informal hegemony of Phoenician city-states throughout North Africa and modern Spain from 575 BC until 146 BC. It was more or less under the control of the city-state of Carthage after the fall of Tyre to Babylonian forces. At the height of the city's influence, its empire included most of the western Mediterranean. The empire was in a constant state of struggle with the Roman Republic, which led to a series of conflicts known as the Punic Wars. After the third and final Punic War, Carthage was destroyed then occupied by Roman forces. Nearly all of the territory held by Carthage fell into Roman hands.
South Asia.
The earliest evidence of human civilization in South Asia is from the Mehrgarh region (7000 BC to 3200 BC) of Pakistan. Located near the Bolan Pass, to the west of the Indus River valley and between the present-day Pakistani cities of Quetta, Kalat and Sibi, Mehrgarh was discovered in 1974 by an archaeological team directed by French archaeologist Jean-François Jarrige, and was excavated continuously between 1974 and 1986. The earliest settlement at Mehrgarh—in the northeast corner of the 495 acre site—was a small farming village dated between 7000 BC–5500 BC.
Early Mehrgarh residents lived in mud brick houses, stored their grain in granaries, fashioned tools with local copper ore, and lined their large basket containers with bitumen. They cultivated six-row barley, einkorn and emmer wheat, jujubes and dates, and herded sheep, goats and cattle. Residents of the later period (5500 BC to 2600 BC) put much effort into crafts, including flint knapping, tanning, bead production, and metal working. The site was occupied continuously until about 2600 BC.[2]
In April 2006, it was announced in the scientific journal Nature that the oldest evidence in human history for the drilling of teeth in vivo (i.e. in a living person) was found in Mehrgarh. Mehrgarh is sometimes cited as the earliest known farming settlement in South Asia, based on archaeological excavations from 1974 (Jarrige et al.). The earliest evidence of settlement dates from 7000 BC. It is also cited for the earliest evidence of pottery in South Asia. Archaeologists divide the occupation at the site into several periods. Mehrgarh is now seen as a precursor to the Indus Valley Civilization.
Indus Valley Civilization.
The Indus Valley Civilization (c. 3300–1700 BC, flourished 2600–1900 BC), abbreviated IVC, was an ancient civilization that flourished in the Indus and Ghaggar-Hakra river valleys primarily in what is now Pakistan, although scattered settlements linked to this ancient civilization have been found in eastern Afghanistan, Bahrain, eastern Iran, western India and Turkmenistan. Another name for this civilization is the Harappan Civilization, after the first of its cities to be excavated, Harappa in the Pakistani province of Punjab. The IVC might have been known to the Sumerians as the Meluhha, and other trade contacts may have included Egypt, Africa, however the modern world discovered it only in the 1920s as a result of archaeological excavations and rail road building. Prominent historians of Ancient India would include Ram Sharan Sharma and Romila Thapar.
Mahajanapadas.
The births of Mahavira and Buddha in the 6th century BC mark the beginning of well-recorded history in the region. Around the 5th century BC, the ancient region of Pakistan was invaded by the Achaemenid Empire under Darius in 522 BC forming the easternmost satraps of the Persian Empire. The provinces of Sindh and Panjab were said to be the richest "satraps" of the Persian Empire and contributed many soldiers to various Persian expeditions. It is known that an "Indian" contingent fought in Xerxes' army on his expedition to Greece. Herodotus mentions that the Indus satrapy supplied cavalry and chariots to the Persian army. He also mentions that the Indus people were clad in armaments made of cotton, carried bows and arrows of cane covered with iron. Herodotus states that in 517 BC Darius sent an expedition under Scylax to explore the Indus. Under Persian rule, much irrigation and commerce flourished within the vast territory of the empire. The Persian empire was followed by the invasion of the Greeks under Alexander's army. Since Alexander was determined to reach the eastern-most limits of the Persian Empire he could not resist the temptation to conquer Pakistan, which at this time was parcelled out into small chieftain-ships, who were feudatories of the Persian Empire. Alexander amalgamated the region into the expanding Hellenic empire. The "Rigveda", in Sanskrit, goes back to about 1500 BC. The Indian literary tradition has an oral history reaching down into the Vedic period of the later 2nd millennium BC.
"Ancient India" is usually taken to refer to the "golden age" of classical Hindu culture, as reflected in Sanskrit literature, beginning around 500 BC with the sixteen monarchies and 'republics' known as the Mahajanapadas, stretched across the Indo-Gangetic plains from modern-day Afghanistan to Bangladesh. The largest of these nations were Magadha, Kosala, Kuru and Gandhara. Notably, the great epics of Ramayana and Mahabharata are rooted in this classical period.
Amongst the sixteen Mahajanapadas, the kingdom of Magadha rose to prominence under a number of dynasties that peaked in power under the reign of Ashoka Maurya, one of India's most legendary and famous emperors. During the reign of Ashoka, the four dynasties of Chola, Chera, and Pandya were ruling in the South, while the King Devanampiya Tissa was controlling the Anuradhapura Kingdom (now Sri Lanka). These kingdoms, while not part of Ashoka's empire, were in friendly terms with the Maurya Empire. There was a strong alliance existed between Devanampiya Tissa (250–210 BC) and Ashoka of India, who sent Arahat Mahinda, four monks, and a novice being sent to Sri Lanka. They encountered Devanampiya Tissa at Mihintale. After this meeting, Devanampiya Tissa embraced Buddhism the order of monks was established in the country. Devanampiya Tissa, guided by Arahat Mahinda, took steps to firmly establish Buddhism in the country.
The Satavahanas started out as feudatories to the Mauryan Empire, and declared independence soon after the death of Ashoka (232 BC). Other notable ancient South Indian dynasties include the Kadambas of Banavasi, western Ganga dynasty, Badami Chalukyas, Western Chalukyas, Hoysalas, Kakatiya dynasty, Pallavas, Rashtrakutas of Manyaketha and Satavahanas.
Middle kingdoms.
The period between AD 320–550 is known as the Classical Age, when most of North India was reunited under the Gupta Empire (c. AD 320–550). This was a period of relative peace, law and order, and extensive achievements in religion, education, mathematics, arts, Sanskrit literature and drama. Grammar, composition, logic, metaphysics, mathematics, medicine, and astronomy became increasingly specialized and reached an advanced level. The Gupta Empire was weakened and ultimately ruined by the raids of Hunas (a branch of the Hephthalites emanating from Central Asia). Under Harsha (r. 606–47), North India was reunited briefly.
The educated speech at that time was Sanskrit, while the dialects of the general population of northern India were referred to as Prakrits. The South Indian Malabar Coast and the Tamil people of the Sangam age traded with the Graeco-Roman world. They were in contact with the Phoenicians, Romans, Greeks, Arabs, Syrians, Jews, and the Chinese.
The regions of South Asia, primarily present-day Pakistan and India, were estimated to have had the largest economy of the world between the 1st and 15th centuries AD, controlling between one third and one quarter of the world's wealth up to the time of the Mughals, from whence it rapidly declined during British rule.
East Asia.
China.
Ancient era.
Written records of China's past dates from the Shang Dynasty (商朝) in perhaps the 13th century BC, and takes the form of inscriptions of divination records on the bones or shells of animals—the so-called "oracle bones" (甲骨文). Archaeological findings providing evidence for the existence of the Shang Dynasty, c. 1600–1046 BC is divided into two sets. The first, from the earlier Shang period (c. 1600–1300) comes from sources at Erligang (二里崗), Zhengzhou (鄭州) and Shangcheng. The second set, from the later Shang or Yin (殷) period, consists of a large body of oracle bone writings. Anyang (安陽) in modern day Henan has been confirmed as the last of the nine capitals of the Shang (c. 1300–1046 BC).
By the end of the 2nd millennium BC, the Zhou Dynasty (周朝) began to emerge in the Yellow River valley, overrunning the Shang. The Zhou appeared to have begun their rule under a semi-feudal system. The ruler of the Zhou, King Wu, with the assistance of his brother, the Duke of Zhou, as regent managed to defeat the Shang at the Battle of Muye. The king of Zhou at this time invoked the concept of the Mandate of Heaven to legitimize his rule, a concept that would be influential for almost every successive dynasty. The Zhou initially moved their capital west to an area near modern Xi'an, near the Yellow River, but they would preside over a series of expansions into the Yangtze River valley. This would be the first of many population migrations from north to south in Chinese history.
Spring and Autumn.
In the 8th century BC, power became decentralized during the Spring and Autumn period (春秋時代), named after the influential Spring and Autumn Annals. In this period, local military leaders used by the Zhou began to assert their power and vie for hegemony. The situation was aggravated by the invasion of other peoples from the northwest, such as the Quanrong, forcing the Zhou to move their capital east to Luoyang. This marks the second large phase of the Zhou dynasty: the Eastern Zhou. In each of the hundreds of states that eventually arose, local strongmen held most of the political power and continued their subservience to the Zhou kings in name only. Local leaders for instance started using royal titles for themselves. The Hundred Schools of Thought (諸子百家) of Chinese philosophy blossomed during this period, and such influential intellectual movements as Confucianism (儒家), Taoism (道家), Legalism (法家) and Mohism (墨家) were founded, partly in response to the changing political world. The Spring and Autumn Period is marked by a falling apart of the central Zhou power. China now consists of hundreds of states, some only as large as a village with a fort.
Warring States.
After further political consolidation, seven prominent states remained by the end of 5th century BC, and the years in which these few states battled each other is known as the Warring States period (戰國時代). Though there remained a nominal Zhou king until 256 BC, he was largely a figurehead and held little power. As neighboring territories of these warring states, including areas of modern Sichuan (四川) and Liaoning (遼寧), were annexed, they were governed under the new local administrative system of commandery and prefecture (郡縣). This system had been in use since the Spring and Autumn Period and parts can still be seen in the modern system of Sheng & Xian (province and county, 省縣). The final expansion in this period began during the reign of Ying Zheng (嬴政), the king of Qin. His unification of the other six powers, and further annexations in the modern regions of Zhejiang (浙江), Fujian (福建), Guangdong (廣東) and Guangxi (廣西) in 214 BC enabled him to proclaim himself the First Emperor (Qin Shi Huangdi, 秦始皇帝).
Japan.
Japan first appeared in written records in AD 57 with the following mention in China's "Book of the Later Han": "Across the ocean from Luoyang are the people of Wa. Formed from more than one hundred tribes, they come and pay tribute frequently." According to the Kojiki, Emperor Jimmu, in 660 BC, unified the many peoples of the Japanese archipelago and established order. The "Book of Wei", written in the 3rd century, noted the country was the unification of some 30 small tribes or states and ruled by a shaman queen named Himiko of Yamataikoku.
During the Han Dynasty and Wei Dynasty, Chinese travelers to Kyūshū recorded its inhabitants and claimed that they were the descendants of the Grand Count (Tàibó) of the Wu. The inhabitants also show traits of the pre-sinicized Wu people with tattooing, teeth-pulling and baby-carrying. The "Book of Wei" records the physical descriptions which are similar to ones on "Haniwa" statues, such men with braided hair, tattooing and women wearing large, single-piece clothing.
Korea.
According to the Samguk Yusa and other Korean medieval-era Folklore collection, Gojoseon was the first Korean kingdom. Gojoseon was founded in 2333 BC by the legendary ruler Dangun, said to be descended from the Lord of Heaven. Then, Korea was governed for Jizi and the 40th generation descendant. According to Records of the Grand Historian, Korea was founded by Wiman from China in 197 BC. In 105 BC, Han Dynasty China ruined Korea and ruled for about 400 years.
The Three Kingdoms (Baekje, Goguryeo, and Silla) conquered other successor states of Gojoseon and came to dominate the peninsula and much of Manchuria. The three kingdoms competed with each other both economically and militarily; Goguryeo and Baekje were the more powerful states for much of the three kingdoms era. At times more powerful than the neighboring Sui Dynasty, Goguryeo was a regional power that defeated massive Chinese invasions multiple times. As one of the Three Kingdoms of Korea, Silla gradually extended across Korea and eventually became the first state since Gojoseon to cover most of Korean peninsula in 676. In 698, former Goguryeo general Dae Jo-yeong founded Balhae as the successor to Goguryeo.
Unified Silla itself fell apart in the late 9th century, giving way to the tumultuous Later Three Kingdoms period (892-936), which ended with the establishment of the Goryeo Dynasty. After the fall of Balhae in 926 to the Khitan, much of its people were absorbed into Goryeo Dynasty.
Vietnam.
Around 3000 BC, the 15 different Lạc Việt ethnic tribes lived together in many areas with other inhabitants. Due to increasing needs to control floods, fights against invaders, and culture and trade exchanges, these tribes living near each other tended to gather together and integrate into a larger mixed group. Among these Lac Viet tribes was the Van Lang, which was the most powerful tribe. The leader of this tribe later joined all the tribes together to found the Hồng Bàng Dynasty in 2897 BC. He became the first in a line of earliest Vietnamese kings, collectively known as the Hùng kings (Hùng Vương). The Hùng kings called the country, which was then located on the Red River delta in present-day northern Vietnam, Văn Lang. The people of Văn Lang were referred to as the Lạc Việt. The next generations followed in their father's footsteps and kept this appellation. Based on historical documents, researchers correlatively delineated the location of Văn Lang Nation to the present day regions of North and north of Central Vietnam, as well as the south of present-day Kwangsi (China).
The Đông Sơn culture was a prehistoric Bronze Age culture that was centered at the Red River Valley of northern Vietnam. Its influence flourished to other parts of Southeast Asia, including the Indo-Malayan Archipelago from about 2000 BC to 200 AD. The theory based on the assumption that bronze casting in eastern Asia originated in northern China; however, this idea has been discredited by archaeological discoveries in north-eastern Thailand in the 1970s. In the words of one scholar, "Bronze casting began in Southeast Asia and was later borrowed by the Chinese, not vice versa as the Chinese scholars have always claimed. Evidence of early kingdoms of Vietnam other than the Đông Sơn culture in Northern Vietnam was found in Cổ Loa, the ancient city situated within present-day Hà Nội.
Mongols.
North-western Mongolia was Turkic while south-western Mongolia had come under Indo-European (Tocharian and Scythian) influence. In antiquity, the eastern portions of both Inner and Outer Mongolia were inhabited by Mongolic peoples descended from the Donghu people, including the Xianbei, Wuhuan, Rouran, Tuoba, Murong, Shiwei, Kumo Xi and Khitan. These were Tengriist horse-riding pastoralist kingdoms that had close contact with the Chinese. The Donghu are first mentioned by Sima Qian as already existing in Inner Mongolia north of the state of Yan in 699-632 BC. The Mongolic-speaking Xianbei (208 BC-234 AD) originally formed a part of the Donghu confederation, but existed even before that time, as evidenced by a mention in the Guoyu "晉語八" section which states that during the reign of King Cheng of Zhou (reigned 1042-1021 BC) the Xianbei came to participate at a meeting of Zhou subject-lords at Qiyang (岐阳) (now Qishan County) but were only allowed to perform the fire ceremony under the supervision of Chu (楚), since they were not vassals by covenant (诸侯). As a nomadic confedation composed of the Xianbei and Wuhuan, the Donghu were prosperous in the 4th century BC, forcing surrounding tribes to pay tribute and constantly harassing the State of Zhao (325 BC, during the early years of the reign of Wuling) and the State of Yan (in 304 BC General Qin Kai was given as a hostage to the Donghu).
In 208 BC Xiongnu emperor Modu Chanyu, in his first major military campaign, defeated the formerly superior Donghu, who split into the Xianbei and Wuhuan. The Xianbei fled east all the way to Liaodong. In 49 AD the Xianbei ruler Bianhe attacked the Xiongnu and killed 2000 people after having received generous gifts from Emperor Guangwu of Han. In 54 AD the Xianbei rulers Yuchoupen and Mantu presented themselves to the Han emperor and received the titles of wang and gou. Until 93 AD the Xianbei were quietly protecting the Chinese border from Wuhuan and Xiongnu attacks and received ample rewards. From 93 AD the Xianbei began to occupy the lands of the Xiongnu. 100,000 Xiongnu families changed their name to Xianbei. In 97 AD Feijuxian in Liaodong was attacked by the Xianbei, and the governor Qi Sen was dismissed for inaction. Other Xianbei rulers who were active before the rise of the Xianbei emperor Tanshihuai (141-181) were Yanzhiyang, Lianxu and Cizhiqian. The Xianbei gave rise to different Mongolic branches, for example the Rouran (330-555), Khitan (388-1218) and Shiwei (444-present day). The Khitans developed the Khitan scripts in 920-925 AD. The Rouran king Shelun was the first major leader of the steppes to adopt (in 402 AD) the title of Khagan (可汗) or Qiudoufa Khan (丘豆伐可汗) (which was originally a title used by Xianbei nobles).
The Mongols of Genghis Khan were the Menggu sub-tribe of the Shiwei Xianbei. The first surviving Mongolian text is the Stele of Yisüngge, a report on sports in Mongolian script on stone, that is most often dated at the verge of 1224 and 1225. Other early sources are written in Mongolian, Phagspa (decrets), Chinese (the Secret history), Arabic (dictionaries) and a few other western scripts.
Huns.
The Huns left practically no written records. There is no record of what happened between the time they left Mongolian Plateau and arrived in Europe 150 years later. The last mention of the northern Xiongnu was their defeat by the Chinese in 151 at the lake of Barkol, after which they fled to the western steppe at Kangju (centered on the city of Turkistan in Kazakhstan). Chinese records between the 3rd and 4th centuries suggest that a small tribe called Yueban, remnants of northern Xiongnu, was distributed about the steppe of Kazakhstan.
Americas.
In pre-Columbian times, several large, centralized ancient civilizations developed in the Western Hemisphere, both in Mesoamerica and western South America.
Andean civilizations.
The Central Andes in South America has the largest ancient civilization register, spanning 4,500 years from Norte chico to the latest civilization, the Inca empire.
Mesoamerica.
Mesoamerican ancient civilizations included the Olmecs and Mayans. Between 2000 and 300 BC, complex cultures began to form and many matured into advanced Mesoamerican civilizations such as the: Olmec, Izapa, Teotihuacan, Maya, Zapotec, Mixtec, Huastec, Tarascan, "Toltec" and Aztec, which flourished for nearly 4,000 years before the first contact with Europeans. These civilizations' progress included pyramid-temples, mathematics, astronomy, medicine, and theology.
The Zapotec emerged around 1500 years BC. They left behind the great city Monte Alban. Their writing system had been thought to have influenced the Olmecs but, with recent evidence, the Olmec may have been the first civilization in the area to develop a true writing system independently. At the present time, there is some debate as to whether or not Olmec symbols, dated to 650 BC, are actually a form of writing preceding the oldest Zapotec writing dated to about 500 BC.
Olmec symbols found in 2002 and 2006 date to 650 BC and 900 BC respectively, preceding the oldest Zapotec writing. The Olmec symbols found in 2006, dating to 900 BC, are known as the Cascajal Block.
The earliest Mayan inscriptions found which are identifiably Maya date to the 3rd century BC in San Bartolo, Guatemala.
Europe.
Etruria.
The history of the Etruscans can be traced relatively accurately, based on the examination of burial sites, artifacts, and writing. Etruscans culture that is identifiably and certainly Etruscan developed in Italy in earnest by 800 BC approximately over the range of the preceding Iron Age Villanovan culture. The latter gave way in the 7th century to a culture that was influenced by Greek traders and Greek neighbors in Magna Graecia, the Hellenic civilization of southern Italy.
From the descendants of the Villanovan people in Etruria in central Italy, a separate Etruscan culture emerged in the beginning of the 7th century BC, evidenced by around 7,000 inscriptions in an alphabet similar to that of Euboean Greek, in the non-Indo-European Etruscan language. The burial tombs, some of which had been fabulously decorated, promotes the idea of an aristocratic city-state, with centralized power structures maintaining order and constructing public works, such as irrigation networks, roads, and town defenses.
Greece.
Ancient Greece is the period in Greek history lasting for close to a millennium, until the rise of Christianity. It is considered by most historians to be the foundational culture of Western Civilization. Greek culture was a powerful influence in the Roman Empire, which carried a version of it to many parts of Europe.
The civilization of the ancient Greeks has been immensely influential on the language, politics, educational systems, philosophy, science, art, and architecture of the modern world, fueling the Renaissance in Europe and again resurgent during various neo-Classical revivals in 18th- and 19th-century Europe and the Americas.
Ancient Greece was the Greek-speaking world in ancient times. It includes not only to the geographical peninsula of modern Greece, but also to areas of Hellenic culture that were settled in ancient times by Greeks: Cyprus and the Aegean islands, the Aegean coast of Anatolia (then known as Ionia), Sicily and southern Italy (known as Magna Graecia), and the scattered Greek settlements on the coasts of Colchis, Illyria, Thrace, Egypt, Cyrenaica, southern Gaul, east and northeast of the Iberian peninsula, Iberia, Taurica and further to the east in exotic Asian cities such as Taxila, Sagala and Jhelum in modern day Pakistan.
During its twelve-century existence, the Roman civilization shifted from a monarchy to an oligarchic republic to a vast empire. It came to dominate Europe and the entire area surrounding the Mediterranean Sea through conquest and assimilation. However, a number of factors led to the eventual decline of the Roman Empire. The western half of the empire, including Hispania, Gaul, and Italy, eventually broke into independent kingdoms in the 5th century; the Eastern Roman Empire, governed from Constantinople, is referred to as the Byzantine Empire after AD 476, the traditional date for the "fall of Rome" and subsequent onset of the Middle Ages.
Rome.
Ancient Rome was a civilization that grew out of the city-state of Rome, originating as a small agricultural community founded on the Italian Peninsula in the 9th century BC. In its twelve centuries of existence, Roman civilization shifted from a monarchy to an oligarchic republic to an increasingly autocratic empire.
Roman civilization is often grouped into "classical antiquity" with ancient Greece, a civilization that inspired much of the culture of ancient Rome. Ancient Rome contributed greatly to the development of law, war, art, literature, architecture, and language in the Western world, and its history continues to have a major influence on the world today. The Roman civilization came to dominate Europe and the Mediterranean region through conquest and assimilation.
Throughout the territory under the control of ancient Rome, residential architecture ranged from very modest houses to country villas. A number of Roman founded cities had monumental structures. Many contained fountains with fresh drinking-water supplied by hundreds of miles of aqueducts, theatres, gymnasiums, bath complexes sometime with libraries and shops, marketplaces, and occasionally functional sewers.
Late Antiquity.
The Roman Empire underwent considerable social, cultural and organizational change starting with reign of Diocletian, who began the custom of splitting the Empire into Eastern and Western halves ruled by multiple emperors. Beginning with Constantine the Great the Empire was Christianized, and a new capital founded at Constantinople. Migrations of Germanic tribes disrupted Roman rule from the late 4th century onwards, culminating in the eventual collapse of the Empire in the West in 476, replaced by the so-called barbarian kingdoms. The resultant cultural fusion of Greco-Roman, Germanic and Christian traditions formed the cultural foundations of Europe.
Germanic tribes.
Migration of Germanic peoples to Britain from what is now northern Germany and southern Scandinavia is attested from the 5th century (e.g. Undley bracteate). Based on Bede's "Historia ecclesiastica gentis Anglorum", the intruding population is traditionally divided into Angles, Saxons, and Jutes, but their composition was likely less clear-cut and may also have included ancient Frisians and Franks. The "Anglo-Saxon Chronicle" contains text that may be the first recorded indications of the movement of these Germanic Tribes to Britain. The Angles and Saxons and Jutes were noted to be a confederation in the Greek Geographia written by Ptolemy in around AD 150.
Anglo-Saxon is the term usually used to describe the peoples living in the south and east of Great Britain from the early 5th century AD. Benedictine monk Bede identified them as the descendants of three Germanic tribes: the Angles, the Saxons, and the Jutes, from the Jutland peninsula and Lower Saxony (German: "Niedersachsen", Germany). The Angles may have come from Angeln, and Bede wrote their nation came to Britain, leaving their land empty. They spoke closely related Germanic dialects. The Anglo-Saxons knew themselves as the "Englisc," from which the word "English" derives.
The Celts were a diverse group of tribal societies in Iron Age Europe. Proto-Celtic culture formed in the Early Iron Age in Central Europe (Hallstatt period, named for the site in present-day Austria). By the later Iron Age (La Tène period), Celts had expanded over wide range of lands: as far west as Ireland and the Iberian Peninsula, as far east as Galatia (central Anatolia), and as far north as Scotland. By the early centuries AD, following the expansion of the Roman Empire and the Great Migrations of Germanic peoples, Celtic culture had become
restricted to the British Isles (Insular Celtic), with the Continental Celtic languages extinct by the mid-1st millennium AD.
Viking refers to a member of the Norse (Scandinavian) peoples, famous as explorers, warriors, merchants, and pirates, who raided and colonized wide areas of Europe beginning in the late 8th. These Norsemen used their famed longships to travel. The Viking Age forms a major part of Scandinavian history, with a minor, yet significant part in European history.
Developments.
Religion and philosophy.
New philosophies and religions arose in both east and west, particularly about the 6th century BC. Over time, a great variety of religions developed around the world, with some of the earliest major ones being Hinduism, Buddhism, and Jainism in India, and Zoroastrianism in Persia. The Abrahamic religions trace their origin to Judaism, around 1800 BC.
The ancient Indian philosophy is a fusion of two ancient traditions: Sramana tradition and Vedic tradition. Indian philosophy begins with the "Vedas" where questions related to laws of nature, the origin of the universe and the place of man in it are asked. Jainism and Buddhism are continuation of the Sramana school of thought. The Sramanas cultivated a pessimistic world view of the samsara as full of suffering and advocated renunciation and austerities. They laid stress on philosophical concepts like Ahimsa, Karma, Jnana, Samsara and Moksa. While there are ancient relations between the Indian Vedas and the Iranian Avesta, the two main families of the Indo-Iranian philosophical traditions were characterized by fundamental differences in their implications for the human being's position in society and their view on the role of man in the universe.
In the east, three schools of thought were to dominate Chinese thinking until the modern day. These were Taoism, Legalism and Confucianism. The Confucian tradition, which would attain dominance, looked for political morality not to the force of law but to the power and example of tradition. Confucianism would later spread into the Korean peninsula and Goguryeo and toward Japan.
In the west, the Greek philosophical tradition, represented by Socrates, Plato, and Aristotle, was diffused throughout Europe and the Middle East in the 4th century BC by the conquests of Alexander III of Macedon, more commonly known as Alexander the Great. After the Bronze and Iron Age religions formed, the rise and spread of Christianity through the Roman world marked the end of Hellenistic philosophy and ushered in the beginnings of Medieval philosophy.
Science and technology.
In the history of technology and ancient science during the growth of the ancient civilizations, ancient technological advances were produced in engineering. These advances stimulated other societies to adopt new ways of living and governance.
The characteristics of Ancient Egyptian technology are indicated by a set of artifacts and customs that lasted for thousands of years. The Egyptians invented and used many basic machines, such as the ramp and the lever, to aid construction processes. The Egyptians also played an important role in developing Mediterranean maritime technology including ships and lighthouses.
The history of science and technology in India dates back to ancient times. The Indus Valley civilization yields evidence of hydrography, metrology and sewage collection and disposal being practiced by its inhabitants. Among the fields of science and technology pursued in India were Ayurveda, metallurgy, astronomy and mathematics. Some ancient inventions include plastic surgery, cataract surgery, Hindu-Arabic numeral system and Wootz steel.
The history of science and technology in China show significant advances in science, technology, mathematics, and astronomy. The first recorded observations of comets and supernovae were made in China. Traditional Chinese medicine, acupuncture and herbal medicine were also practiced.
Ancient Greek technology developed at an unprecedented speed during the 5th century BC, continuing up to and including the Roman period, and beyond. Inventions that are credited to the ancient Greeks such as the gear, screw, bronze casting techniques, water clock, water organ, torsion catapult and the use of steam to operate some experimental machines and toys. Many of these inventions occurred late in the Greek period, often inspired by the need to improve weapons and tactics in war. Roman technology is the engineering practice which supported Roman civilization and made the expansion of Roman commerce and Roman military possible over nearly a thousand years. The Roman Empire had the most advanced set of technology of their time, some of which may have been lost during the turbulent eras of Late Antiquity and the Early Middle Ages. Roman technological feats of many different areas, like civil engineering, construction materials, transport technology, and some inventions such as the mechanical reaper went unmatched until the 19th century.
Qanats which likely emerged on the Iranian plateau and possibly also in the Arabian peninsula sometime in the early 1st millennium BC spread from there slowly west- and eastward.
Maritime activity.
The history of ancient navigation began in earnest when men took to the sea in planked boats and ships propelled by sails hung on masts, like the Ancient Egyptian Khufu ship from the mid-3rd millennium BC. According to the Greek historian Herodotus, Necho II sent out an expedition of Phoenicians, which in three years sailed from the Red Sea around Africa to the mouth of the Nile. Many current historians tend to believe Herodotus on this point, even though Herodotus himself was in disbelief that the Phoenicians had accomplished the act.
Hannu was an ancient Egyptian explorer (around 2750 BC) and the first explorer of whom there is any knowledge. He made the first recorded exploring expedition, writing his account of his exploration in stone. Hannu travelled along the Red Sea to Punt, and sailed to what is now part of eastern Ethiopia and Somalia. He returned to Egypt with great treasures, including precious myrrh, metal and wood.
Warfare.
Ancient warfare is war as conducted from the beginnings of recorded history to the end of the ancient period. In Europe, the end of antiquity is often equated with the fall of Rome in 476. In China, it can also be seen as ending in the 5th century, with the growing role of mounted warriors needed to counter the ever-growing threat from the north.
The difference between prehistoric warfare and ancient warfare is less one of technology than of organization. The development of first city-states, and then empires, allowed warfare to change dramatically. Beginning in Mesopotamia, states produced sufficient agricultural surplus that full-time ruling elites and military commanders could emerge. While the bulk of military forces were still farmers, the society could support having them campaigning rather than working the land for a portion of each year. Thus, organized armies developed for the first time.
These new armies could help states grow in size and became increasingly centralized, and the first empire, that of the Sumerians, formed in Mesopotamia. Early ancient armies continued to primarily use bows and spears, the same weapons that had been developed in prehistoric times for hunting. Early armies in Egypt and China followed a similar pattern of using massed infantry armed with bows and spears.
Artwork and music.
Ancient music is music that developed in literate cultures, replacing prehistoric music. Ancient music refers to the various musical systems that were developed across various geographical regions such as Persia, India, China, Greece, Rome, Egypt and Mesopotamia (see music of Mesopotamia, music of ancient Greece, music of ancient Rome, Music of Iran). Ancient music is designated by the characterization of the basic audible tones and scales. It may have been transmitted through oral or written systems. Arts of the ancient world refers to the many types of art that were in the cultures of ancient societies, such as those of ancient China, Egypt, Greece, India, Persia, Mesopotamia and Rome.

</doc>
<doc id="51329" url="http://en.wikipedia.org/wiki?curid=51329" title="Famine">
Famine

A famine is a widespread scarcity of food, caused by several factors including crop failure, population unbalance, or government policies. This phenomenon is usually accompanied or followed by regional malnutrition, starvation, epidemic, and increased mortality. Nearly every continent in the world has experienced a period of famine throughout history. Some countries, particularly in sub-Sahara Africa, continue to have extreme cases of famine.
History.
The cyclical occurrence of famine has been a mainstay of societies engaged in subsistence agriculture since the dawn of agriculture itself. The frequency and intensity of famine has fluctuated throughout history, depending on changes in food demand, such as population growth, and supply-side shifts caused by changing climatic conditions. Famine was first eliminated in Holland and England during the 17th century, due to the commercialization of agriculture and the implementation of improved techniques to increase crop yields.
Decline of famine.
The feudal system of the Middle Ages, in which subsistence peasants worked on the land of a lord in return for protection, was not conducive to improvement or change, as neither the peasants nor the landlords had much economic incentive to increase the land's productivity.
In the 16th and 17th century, the feudal system began to break down, and more prosperous farmers began to enclose their own land and improve their yields to sell the surplus crops for a profit. These capitalist landowners paid their labourers with money, thereby increasing the commercialization of rural society. In the emerging competitive labour market, better techniques for the improvement of labour productivity were increasingly valued and rewarded. It was in the farmer's interest to produce as much as possible on their land in order to sell it to areas that demanded that product. They produced guaranteed surpluses of their crop every year if they could.
Subsistence peasants were also increasingly forced to commercialize their activities because of increasing taxes. Taxes that had to be paid to central governments in money forced the peasants to produce crops to sell. Sometimes they produced industrial crops, but they would find ways to increase their production in order to meet both their subsistence requirements as well as their tax obligations. Peasants also used the new money to purchase manufactured goods. The agricultural and social developments encouraging increased food production were gradually taking place throughout the 16th century, but took off in the early 17th century.
By the 1590s, these trends were sufficiently developed in the rich and commercialized province of Holland to allow its population to withstand a general outbreak of famine in Western Europe at that time. By that time, the Netherlands had one of the most commercialized agricultural systems in Europe. They grew many industrial crops such as flax, hemp and hops. Agriculture became increasingly specialized and efficient. The efficiency of Dutch agriculture allowed for much more rapid urbanization in the late sixteenth and early seventeenth centuries than anywhere else in Europe. As a result, productivity and wealth increased, allowing the Netherlands to maintain a steady food supply.
By 1650, English agriculture had also become commercialized on a much wider scale. The last peace-time famine in England was in 1623-24. There were still periods of hunger, as in the Netherlands, but no more famines ever occurred. Common areas for pasture were enclosed for private use and large scale, efficient farms were consolidated. Other technical developments included the draining of marshes, more efficient field use patterns, and the wider introduction of industrial crops. These agricultural developments led to wider prosperity in England and increasing urbanization. By the end of the 17th century, English agriculture was the most productive in Europe. In both England and the Netherlands, the population stabilized between 1650 and 1750, the same time period in which the sweeping changes to agriculture occurred. Famine still occurred in other parts of Europe, however. In East Europe, famines occurred as late as the twentieth century.
Attempts at famine alleviation.
Because of the severity of famine, it was a chief concern for governments and other authorities. In pre-industrial Europe, preventing famine, and ensuring timely food supplies, was one of the chief concerns of many governments, although they were severely limited in their options due to limited levels of external trade and an infrastructure and bureaucracy generally too rudimentary to affect real relief. Most governments were concerned by famine because it could lead to revolt and other forms of social disruption.
By the mid-19th century and the onset of the Industrial Revolution, it became possible for governments to alleviate the effects of famine through price controls, large scale importation of food products from foreign markets, stockpiling, rationing, regulation of production and charity. The Great Famine of 1845 in Ireland was one of the first famines to feature such intervention, although the government response was often lacklustre. The initial response of the British government to the early phase of the famine was "prompt and relatively successful," according to F. S. L. Lyons. Confronted by widespread crop failure in the autumn of 1845, Prime Minister Sir Robert Peel purchased £100,000 worth of maize and cornmeal secretly from America. Baring Brothers & Co initially acted as purchasing agents for the Prime Minister. The government hoped that they would not "stifle private enterprise" and that their actions would not act as a disincentive to local relief efforts. Due to weather conditions, the first shipment did not arrive in Ireland until the beginning of February 1846. The maize corn was then re-sold for a penny a pound.
In 1846, Peel moved to repeal the Corn Laws, tariffs on grain which kept the price of bread artificially high. The famine situation worsened during 1846 and the repeal of the Corn Laws in that year did little to help the starving Irish; the measure split the Conservative Party, leading to the fall of Peel's ministry. In March, Peel set up a programme of public works in Ireland.
Despite this promising start, the measures undertaken by Peel's successor, Lord John Russell, proved comparatively "inadequate" as the crisis deepened. Russell's ministry introduced public works projects, which by December 1846 employed some half million Irish and proved impossible to administer. The government was influenced by a laissez-faire belief that the market would provide the food needed. It halted government food and relief works, and turned to a mixture of "indoor" and "outdoor" direct relief; the former administered in workhouses through the Poor Law, the latter through soup kitchens.
A systematic attempt at creating the necessary regulatory framework for dealing with famine was developed by the British Raj in the 1880s. In order to comprehensively address the issue of famine, the British created an Indian Famine commission to recommend steps that the government would be required to take in the event of a famine. The Famine Commission issued a series of government guidelines and regulations on how to respond to famines and food shortages called the Famine Code. The famine code was also one of the first attempts to scientifically predict famine in order to mitigate its effects. These were finally passed into law in 1883 under Lord Ripon.
The Code introduced the first famine scale: three levels of food insecurity were defined: near-scarcity, scarcity, and famine. "Scarcity" was defined as three successive years of crop failure, crop yields of one-third or one-half normal, and large populations in distress. "Famine" further included a rise in food prices above 140% of "normal", the movement of people in search of food, and widespread mortality. The Commission identified that the loss of wages from lack of employment of agricultural labourers and artisans were the cause of famines. The Famine Code applied a strategy of generating employment for these sections of the population and relied on open-ended public works to do so.
20th century.
During the 20th century, an estimated 70 million people died from famines across the world, of whom an estimated 30 million died during the famine of 1958–61 in China. The other most notable famines of the century included the 1942–1945 disaster in Bengal, famines in China in 1928 and 1942, and a sequence of famines in the Soviet Union, including the Soviet famine of 1932-1933, caused by the policies of Stalin.
A few of the great famines of the late 20th century were: the Biafran famine in the 1960s, the Khmer Rouge-caused famine in Cambodia in the 1970s, the North Korean famine of the 1990s and the Ethiopian famine of 1984–85.
The latter event was reported on television reports around the world, carrying footage of starving Ethiopians whose plight was centered around a feeding station near the town of Korem. This stimulated the first mass movements to end famine across the world.
BBC newsreader Michael Buerk gave moving commentary of the tragedy on 23 October 1984, which he described as a "biblical famine". This prompted the Band Aid single, which was organized by Bob Geldof and featured more than 20 pop stars. The Live Aid concerts in London and Philadelphia raised even more funds for the cause. An estimated 900,000 people died within one year as a result of the famine, but the tens of millions of pounds raised by Band Aid and Live Aid are widely believed to have saved the lives of Ethiopians who were in danger of dying.
Regional history.
Africa.
See also: .
In the mid-22nd century BC, a sudden and short-lived climatic change that caused reduced rainfall resulted in several decades of drought in Upper Egypt. The resulting famine and civil strife is believed to have been a major cause of the collapse of the Old Kingdom.
An account from the First Intermediate Period states, "All of Upper Egypt was dying of hunger and people were eating their children." In 1680s, famine extended across the entire Sahel, and in 1738 half the population of Timbuktu died of famine. In Egypt, between 1687 and 1731, there were six famines. The famine that afflicted Egypt in 1784 cost it roughly one-sixth of its population. The Maghreb experienced famine and plague in the late 18th century and early 19th century. There was famine in Tripoli in 1784, and in Tunis in 1785.
According to John Iliffe, "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys."
The first documentation of weather in West-Central Africa occurs around the mid-16th to 17th centuries in areas such as Luanda Kongo, however, not much data was recorded on the issues of weather and disease except for a few notable documents. The only records obtained are of violence between Portuguese and Africans during the Battle of Mbilwa in 1665. In these documents the Portuguese wrote of African raids on Portuguese merchants solely for food, giving clear signs of famine. Additionally, instances of cannibalism by the African Jaga were also more prevalent during this time frame, indicating an extreme deprivation of a primary food source.
A notable period of famine occurred around the turn of the 20th century in the Congo Free State. In forming this state, Leopold used mass labor camps to finance his empire. This period resulted in the death of up to 10 million Congolese from brutality, disease and famine. Some colonial "pacification" efforts often caused severe famine, notably with the repression of the Maji Maji revolt in Tanganyika in 1906. The introduction of cash crops such as cotton, and forcible measures to impel farmers to grow these crops, sometimes impoverished the peasantry in many areas, such as northern Nigeria, contributing to greater vulnerability to famine when severe drought struck in 1913.
A large scale famine occurred in Ethiopia in 1888 and succeeding years, as the rinderpest epizootic, introduced into Eritrea by infected cattle, spread southwards reaching ultimately as far as South Africa. In Ethiopia it was estimated that as much as 90 percent of the national herd died, rendering rich farmers and herders destitute overnight. This coincided with drought associated with an el Nino oscillation, human epidemics of smallpox, and in several countries, intense war. The Ethiopian Great famine that afflicted Ethiopia from 1888 to 1892 cost it roughly one-third of its population. In Sudan the year 1888 is remembered as the worst famine in history, on account of these factors and also the exactions imposed by the Mahdist state.
Records compiled for the Himba recall two droughts from 1910-1917. They were recorded by the Himba through a method of oral tradition. From 1910-1911 the Himba described the drought as "drought of the omutati seed" also called "omangowi", which means the fruit of an unidentified vine that people ate during the time period. From 1914-1916 droughts brought "katur' ombanda" or "kari' ombanda" which means "the time of eating clothing".
For the middle part of the 20th century, agriculturalists, economists and geographers did not consider Africa to be famine prone (they were much more concerned about Asia). There were notable counter-examples, such as the famine in Rwanda during World War II and the Malawi famine of 1949, but most famines were localized and brief food shortages. Although the drought was brief the main cause of death in Rwanda was due to Belgian prerogatives to acquisition grain from their colony (Rwanda). The increased grain acquisition was related to WW2. This and the drought caused 300,000 Rwandans to perish.
From 1967-1969 large scale famine occurred in Biafra and Nigeria due to a government blockade of the Breakaway territory. It is estimated that 1.5 million people died of starvation due to this famine. Additionally, drought and other government interference with the food supply caused 500 thousand Africans to perish in Central and West Africa.
Famine recurred in the early 1970s, when Ethiopia and the west African Sahel suffered drought and famine. The Ethiopian famine of that time was closely linked to the crisis of feudalism in that country, and in due course helped to bring about the downfall of the Emperor Haile Selassie. The Sahelian famine was associated with the slowly growing crisis of pastoralism in Africa, which has seen livestock herding decline as a viable way of life over the last two generations.
Famines occurred in Sudan in the late-1970s and again in 1990 and 1998. The 1980 famine in Karamoja, Uganda was, in terms of mortality rates, one of the worst in history. 21% of the population died, including 60% of the infants. In the 1980s, large scale multilayer drought occurred in the Sudan and Sahelian regions of Africa. This caused famine because even though the Sudanese Government believed there was a surplus of grain, there were local deficits across the region.
In October 1984, television reports describing the Ethiopian famine as "biblical", prompted the Live Aid concerts in London and Philadelphia, which raised large sums to alleviate the suffering. A primary cause of the famine (one of the largest seen in the country) is that Ethiopia (and the surrounding Horn) was still recovering from the droughts which occurred in the mid-late 1970s. Compounding this problem was the intermittent fighting due to civil war, the government's lack of organization in providing relief, and hoarding of supplies to control the population. Ultimately, over 1 million Ethiopians died and over 22 million people suffered due to the prolonged drought, which lasted roughly 2 years.
In 1992 Somalia became a war zone with no effective government, police, or basic services after the collapse of the dictatorship led by Siad Barre and the split of power between warlords. This coincided with a massive drought, causing over 300,000 Somalians to perish.
Since the start of the 21st century, more effective early warning and humanitarian response actions have reduced the number of deaths by famine markedly. That said, many African countries are not self-sufficient in food production, relying on income from cash crops to import food. Agriculture in Africa is susceptible to climatic fluctuations, especially droughts which can reduce the amount of food produced locally. Other agricultural problems include soil infertility, land degradation and erosion, swarms of desert locusts, which can destroy whole crops, and livestock diseases. Desertification is increasingly problematic: the Sahara reportedly spreads up to 30 mi per year. The most serious famines have been caused by a combination of drought, misguided economic policies, and conflict. The 1983–85 famine in Ethiopia, for example, was the outcome of all these three factors, made worse by the Communist government's censorship of the emerging crisis. In Sudan at the same date, drought and economic crisis combined with denials of any food shortage by the then-government of President Gaafar Nimeiry, to create a crisis that killed perhaps 250,000 people—and helped bring about a popular uprising that overthrew Nimeiry.
Numerous factors make the food security situation in Africa tenuous, including political instability, armed conflict and civil war, corruption and mismanagement in handling food supplies, and trade policies that harm African agriculture. An example of a famine created by human rights abuses is the 1998 Sudan famine. AIDS is also having long-term economic effects on agriculture by reducing the available workforce, and is creating new vulnerabilities to famine by overburdening poor households. On the other hand, in the modern history of Africa on quite a few occasions famines acted as a major source of acute political instability. In Africa, if current trends of population growth and soil degradation continue, the continent might
be able to feed just 25% of its population by 2025, according to United Nations University (UNU)'s Ghana-based Institute for Natural Resources in Africa.
Recent famines in Africa include the 2005–06 Niger food crisis, the 2010 Sahel famine and the 2011 East Africa drought, where two consecutive missed rainy seasons precipitated the worst drought in East Africa in 60 years. An estimated 50,000 to 150,000 people are reported to have died during the period. In 2012, the Sahel drought put more than 10 million people in the western Sahel at risk of famine (according to a Methodist Relief & Development Fund (MRDF) aid expert), due to a month-long heat wave.
Against a backdrop of conventional interventions through the state or markets, alternative initiatives have been pioneered to address the problem of food security. An example is the "Community Area-Based Development Approach" to agricultural development ("CABDA"), an NGO programme with the objective of providing an alternative approach to increasing food security in Africa. CABDA proceeds through specific areas of intervention such as the introduction of drought-resistant crops and new methods of food production such as agro-forestry. Piloted in Ethiopia in the 1990s it has spread to Malawi, Uganda, Eritrea and Kenya. In an analysis of the programme by the Overseas Development Institute, CABDA's focus on individual and community capacity-building is highlighted. This enables farmers to influence and drive their own development through community-run institutions, bringing food security to their household and region.
Far East.
Chinese scholars had kept count of 1,828 instances of famine from 108 BC to 1911 in one province or another — an average of close to one famine per year. From 1333 to 1337 a terrible famine killed 6 million Chinese. The four famines of 1810, 1811, 1846, and 1849 are said to have killed no fewer than 45 million people.
Japan experienced more than 130 famines between 1603 and 1868.
The period from 1850 to 1873 saw, as a result of the Taiping Rebellion, drought, and famine, the population of China drop by over 30 million people. China's Qing Dynasty bureaucracy, which devoted extensive attention to minimizing famines, is credited with averting a series of famines following El Niño-Southern Oscillation-linked droughts and floods. These events are comparable, though somewhat smaller in scale, to the ecological trigger events of China's vast 19th-century famines. (Pierre-Etienne Will, "Bureaucracy and Famine") Qing China carried out its relief efforts, which included vast shipments of food, a requirement that the rich open their storehouses to the poor, and price regulation, as part of a state guarantee of subsistence to the peasantry (known as "ming-sheng").
When a stressed monarchy shifted from state management and direct shipments of grain to monetary charity in the mid-19th century, the system broke down. Thus the 1867–68 famine under the Tongzhi Restoration was successfully relieved but the Great North China Famine of 1877–78, caused by drought across northern China, was a catastrophe. The province of Shanxi was substantially depopulated as grains ran out, and desperately starving people stripped forests, fields, and their very houses for food. Estimated mortality is 9.5 to 13 million people. (Mike Davis,
The largest famine of the 20th century, and almost certainly of all time, was the 1958–61 Great Leap Forward famine in China. The immediate causes of this famine lay in Mao Zedong's ill-fated attempt to transform China from an agricultural nation to an industrial power in one huge leap. Communist Party cadres across China insisted that peasants abandon their farms for collective farms, and begin to produce steel in small foundries, often melting down their farm instruments in the process. Collectivisation undermined incentives for the investment of labor and resources in agriculture; unrealistic plans for decentralized metal production sapped needed labor; unfavorable weather conditions; and communal dining halls encouraged overconsumption of available food. Such was the centralized control of information and the intense pressure on party cadres to report only good news—such as production quotas met or exceeded—that information about the escalating disaster was effectively suppressed. When the leadership did become aware of the scale of the famine, it did little to respond, and continued to ban any discussion of the cataclysm. This blanket suppression of news was so effective that very few Chinese citizens were aware of the scale of the famine, and the greatest peacetime demographic disaster of the 20th century only became widely known twenty years later, when the veil of censorship began to lift.
The exact number of famine deaths during 1958–61 is difficult to determine, and estimates range from 18 to at least 42 million people, with a further 30 million cancelled or delayed births. It was only when the famine had wrought its worst that Mao was forced to reverse agricultural collectivisation policies, which were effectively dismantled in 1978. China has not experienced a famine of the proportions of the Great Leap Forward since 1961.
In 1975, the Khmer Rouge entered the capital of Phnom Penh and took control of Cambodia. With the application of the fundamental ideals of communism, the new government under Pol Pot drove all urban residents into the countryside to work on communal farm and civil work projects. Without external assistance, with 75% of the necessary draft animals dead from the previous four years of war, agricultural guidelines written by idealists, and work overseen by zealous cadre, the country soon sunk into the depths of famine.
No international relief would come until the Vietnamese army invaded in 1979 and liberated the country. While Pol Pot was in power, between one and three million people died out of a total population of eight million. Many were executed. Most died from malnourishment and exhaustion as a result of the famine caused by inept and negligent government officials.
Famine struck North Korea in the mid-1990s, set off by unprecedented floods. This Autarkic urban, industrial state depended on massive inputs of subsidised goods, including fossil fuels, primarily from the Soviet Union and the People's Republic of China. When the Soviet collapse and China's marketization switched trade to a hard currency, full price basis, North Korea's economy collapsed. The vulnerable agricultural sector experienced a massive failure in 1995–96, expanding to full-fledged famine by 1996–99.
An estimated 600,000 died of starvation (other estimates range from 200,000 to 3.5 million). North Korea has not yet regained food self-sufficiency and relies on external food aid from China, Japan, South Korea, Russia and the United States. While Woo-Cumings have focused on the FAD side of the famine, Moon argues that FAD shifted the incentive structure of the authoritarian regime to react in a way that forced millions of disenfranchised people to starve to death (Moon, 2009).
Various famines have occurred in Vietnam. Japanese occupation during World War II caused the Vietnamese Famine of 1945, which caused 2 million deaths, or 10% of the population
then. Following the unification of the country after the Vietnam War, Vietnam experienced a food shortage in the 1980s, which prompted many people to flee the country.
India.
Owing to its almost entire dependence upon the monsoon rains, India is vulnerable to crop failures, which upon occasion deepen into famine. There were 14 famines in India between the 11th and 17th centuries (Bhatia, 1985). For example, during the 1022–1033 Great famines in India entire provinces were depopulated. Famine in Deccan killed at least 2 million people in 1702-1704. B.M. Bhatia believes that the earlier famines were localised, and it was only after 1860, during the British rule, that famine came to signify general shortage of foodgrains in the country. There were approximately 25 major famines spread through states such as Tamil Nadu in the south, and Bihar and Bengal in the east during the latter half of the 19th century.
Romesh Chunder Dutt argued as early as 1900, and present-day scholars such as Amartya Sen agree, that some historic famines were a product of both uneven rainfall and British economic and administrative policies, which since 1857 had led to the seizure and conversion of local farmland to foreign-owned plantations, restrictions on internal trade, heavy taxation of Indian citizens to support British expeditions in Afghanistan (see The Second Anglo-Afghan War), inflationary measures that increased the price of food, and substantial exports of staple crops from India to Britain. (Dutt, 1900 and 1902; Srivastava, 1968; Sen, 1982; Bhatia, 1985.) Some British citizens, such as William Digby, agitated for policy reforms and famine relief, but Lord Lytton, the governing British viceroy in India, opposed such changes in the belief that they would stimulate shirking by Indian workers. The first, the Bengal famine of 1770, is estimated to have taken around 10 million lives — one-third of Bengal's population at the time. Other notable famines include the Great Famine of 1876–78, in which 6.1 million to 10.3 million people died and the Indian famine of 1899–1900, in which 1.25 to 10 million people died. The famines were ended by the 20th century with the exception of the Bengal Famine of 1943–44— even though there were no crop failures —killing 1.5 million to 3 million Bengalis during World War II.
The observations of the Famine Commission of 1880 support the notion that food distribution is more to blame for famines than food scarcity. They observed that each province in British India, including Burma, had a surplus of foodgrains, and the annual surplus was 5.16 million tons (Bhatia, 1970). At that time, annual export of rice and other grains from India was approximately one million tons.
In 1966, there was a close call in Bihar, when the United States allocated 900,000 tons of grain to fight the famine. Three years of drought in India resulted in an estimated 1.5 million deaths from starvation and disease.
Middle East.
The Great Persian Famine of 1870–1871 is believed to have caused the death of 1.5 million persons (20–25 percent of the population) in Persia (present–day Iran).
In the early 20th century an Ottoman blockade of food being exported to Lebanon caused a famine which killed up to 450,000 Lebanese Maronite Christians (about one-third of the population). The famine killed more people than the Lebanese Civil War. The blockade was caused by varies Maronite and Christian uprisings in the Syrian region of the Empire including one which occurred in the 1860s which lead to the massacre of thousands of Lebanese and Syrian Christians (Both Maronite and Orthodox) by Ottoman Turks and local Sunni Muslims and Druze 
Europe.
The Great Famine of 1315–1317 (or to 1322) was the first major food crisis to strike Europe in the 14th century. Millions in northern Europe died over an extended number of years, marking a clear end to the earlier period of growth and prosperity during the 11th and 12th centuries. Starting with bad weather in the spring of 1315, widespread crop failures lasted until the summer of 1317, from which Europe did not fully recover until 1322. It was a period marked by extreme levels of criminal activity, disease and mass death, infanticide, and cannibalism. It had consequences for Church, State, European society and future calamities to follow in the 14th century. There were 95 famines in medieval Britain, and 75 or more in medieval France. More than 10% of England's population, or at least 500,000 people, may have died during the famine of 1315–1316.
Famine was a very destabilizing and devastating occurrence. The prospect of starvation led people to take desperate measures. When scarcity of food became apparent to peasants, they would sacrifice long-term prosperity for short-term survival. They would kill their draught animals, leading to lowered production in subsequent years. They would eat their seed corn, sacrificing next year's crop in the hope that more seed could be found. Once those means had been exhausted, they would take to the road in search of food. They migrated to the cities where merchants from other areas would be more likely to sell their food, as cities had a stronger purchasing power than did rural areas. Cities also administered relief programs and bought grain for their populations so that they could keep order. With the confusion and desperation of the migrants, crime would often follow them. Many peasants resorted to banditry in order to acquire enough to eat.
One famine would often lead to difficulties in the following years because of lack of seed stock or disruption of routine, or perhaps because of less-available labour. Famines were often interpreted as signs of God's displeasure. They were seen as the removal, by God, of His gifts to the people of the Earth. Elaborate religious processions and rituals were made to prevent God's wrath in the form of famine.
During the Little Ice Age from the 15th century to the 18th century, famines in Europe became more frequent. This often led to a rise in conspiracy theories concerning the causes behind these famines, such as the Pacte de Famine in France.
The 1590s saw the worst famines in centuries across all of Europe. Famine had been relatively rare during the 16th century. The economy and population had grown steadily as subsistence populations tend to when there is an extended period of relative peace (most of the time). Subsistence peasant populations will almost always increase when possible since the peasants will try to spread the work to as many hands as possible. Although peasants in areas of high population density, such as northern Italy, had learned to increase the yields of their lands through techniques such as promiscuous culture, they were still quite vulnerable to famines, forcing them to work their land even more intensively.
The great famine of the 1590s began the period of famine and decline in the 17th century. The price of grain, all over Europe was high, as was the population. Various types of people were vulnerable to the succession of bad harvests that occurred throughout the 1590s in different regions. The increasing number of wage labourers in the countryside were vulnerable because they had no food of their own, and their meager living was not enough to purchase the expensive grain of a bad-crop year. Town labourers were also at risk because their wages would be insufficient to cover the cost of grain, and, to make matters worse, they often received less money in bad-crop years since the disposable income of the wealthy was spent on grain. Often, unemployment would be the result of the increase in grain prices, leading to ever-increasing numbers of urban poor.
All areas of Europe were badly affected by the famine in these periods, especially rural areas. The Netherlands was able to escape most of the damaging effects of the famine, though the 1590s were still difficult years there. Amsterdam's grain trade with the Baltic, guaranteed a food supply.
The years around 1620 saw another period of famine sweep across Europe. These famines were generally less severe than the famines of twenty-five years earlier, but they were nonetheless quite serious in many areas. Perhaps the worst famine since 1600, the great famine in Finland in 1696, killed one-third of the population.  PDF (589 KiB)
Devastating harvest failures afflicted the northern Italian economy from 1618 to 1621, and it did not recover fully for centuries. There were serious famines in the late-1640s and less severe ones in the 1670s throughout northern Italy.
Over two million people died in two famines in France between 1693 and 1710. Both famines were made worse by ongoing wars.
As late as the 1690s, Scotland experienced famine which reduced the population of parts of Scotland by at least 15%.
The Great Famine of 1695–1697 may have killed a third of the Finnish population. and roughly 10% of Norway's population. Death rates rose in Scandinavia between 1740 and 1800 as the result of a series of crop failures. For instance, the Finnish famine of 1866–1868 killed 15% of the population.
The period of 1740–43 saw frigid winters and summer droughts, which led to famine across Europe and a major spike in mortality. The winter 1740-41 was unusually cold, possibly because of volcanic activity.
According to Scott and Duncan (2002), "Eastern Europe experienced more than 150 recorded famines between AD 1500 and 1700 and there were 100 hunger years and 121 famine years in Russia between AD 971 and 1974."
The Great Famine, which lasted from 1770 until 1771, killed about one tenth of Czech lands’ population, or 250,000 inhabitants, and radicalised countrysides leading to peasant uprisings.
There were sixteen good harvests and 111 famine years in northern Italy from 1451 to 1767. According to Stephen L. Dyson and Robert J. Rowland, "The Jesuits of Cagliari [in Sardinia] recorded years during the late 1500s "of such hunger and so sterile that the majority of the people could sustain life only with wild ferns and other weeds" ... During the terrible famine of 1680, some 80,000 persons, out of a total population of 250,000, are said to have died, and entire villages were devastated..."
According to Bryson (1974), there were thirty-seven famine years in Iceland between 1500 and 1804. In 1783 the volcano Laki in south-central Iceland erupted. The lava caused little direct damage, but ash and sulphur dioxide spewed out over most of the country, causing three-quarters of the island's livestock to perish. In the following famine, around ten thousand people died, one-fifth of the population of Iceland. [Asimov, 1984, 152-153]
Other areas of Europe have known famines much more recently. France saw famines as recently as the 19th century. The Great Famine in Ireland, 1846–1851, caused by the failure of the potato crop over a few years, resulted in 1,000,000 dead and another 2,000,000 refugees fleeing to Britain, Australia and the United States.
Iceland was also hit by a potato famine between 1862 and 1864. Lesser known than the Irish potato famine, the Icelandic potato famine was caused by the same blight that ravaged most of Europe during the 1840s. About 5 percent of Iceland's population died during the famine.
Famine still occurred in Eastern Europe during the 20th century. Droughts and famines in Imperial Russia are known to have happened every 10 to 13 years, with average droughts happening every 5 to 7 years. Russia experienced eleven major famines between 1845 and 1922, one of the worst being the famine of 1891–2.
Famines continued in the Soviet era, the most notorious being the "Holodomor" in various parts of the country, especially the Volga, and the Ukrainian and northern Kazakh SSR's during the winter of 1932–1933. The Soviet famine of 1932–1933 is nowadays reckoned to have cost an estimated 6 million lives. The last major famine in the USSR happened in 1947 due to the severe drought and the mismanagement of grain reserves by the Soviet government.
The Hunger Plan, i.e. the Nazi plan to starve large sections of the Soviet population, caused the deaths of many. The Russian Academy of Sciences in 1995 reported civilian victims in the USSR at German hands, including Jews, totalled 13.7 million dead, 20% of the 68 million persons in the occupied USSR. This included 4.1 million famine and disease deaths in occupied territory. There were an additional estimated 3 million famine deaths in areas of the USSR not under German occupation.
The 872 days of the Siege of Leningrad (1941–1944) caused unparalleled famine in the Leningrad region through disruption of utilities, water, energy and food supplies. This resulted in the deaths of about one million people.
Famine even struck in Western Europe during the Second World War. In the Netherlands, the "Hongerwinter" of 1944 killed approximately 30,000 people. Some other areas of Europe also experienced famine at the same time.
Latin America.
The pre-Columbian Americans often dealt with severe food shortages and famines. The persistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico.
Brazil's 1877–78 "Grande Seca" (Great Drought), the worst in Brazil's history, caused approximately half a million deaths. The one from 1915 was devastating too.
Oceania.
Easter Island was hit by a great famine between the 15th and 18th centuries. Hunger and subsequent cannibalism was caused by overpopulation and depletion of natural resources as a result of deforestation, partly because work on megalithic monuments required a lot of wood.
There are other documented episodes of famine in various islands of Polynesia, such as occurred in Ka'u, Hawai'i in 1868.
According to Daniel Lord Smail, "'Famine cannibalism' was until recently a regular feature of life in the islands of the Massim near New Guinea and of some other societies of Southeast Asia and the Pacific."
Famine today.
Today, famine is most widespread in Sub-Saharan Africa, but with exhaustion of food resources, overdrafting of groundwater, wars, internal struggles, and economic failure, famine continues to be a worldwide problem with hundreds of millions of people suffering. These famines cause widespread malnutrition and impoverishment; The famine in Ethiopia in the 1980s had an immense death toll, although Asian famines of the 20th century have also produced extensive death tolls. Modern African famines are characterized by widespread destitution and malnutrition, with heightened mortality confined to young children.
Relief technologies, including immunization, improved public health infrastructure, general food rations and supplementary feeding for vulnerable children, has provided temporary mitigation to the mortality impact of famines, while leaving their economic consequences unchanged, and not solving the underlying issue of too large a regional population relative to food production capability. Humanitarian crises may also arise from genocide campaigns, civil wars, refugee flows and episodes of extreme violence and state collapse, creating famine conditions among the affected populations.
Despite repeated stated intentions by the world's leaders to end hunger and famine, famine remains a chronic threat in much of Africa and Asia. In July 2005, the Famine Early Warning Systems Network labelled Niger with emergency status, as well as Chad, Ethiopia, South Sudan, Somalia and Zimbabwe. In January 2006, the United Nations Food and Agriculture Organization warned that 11 million people in Somalia, Kenya, Djibouti and Ethiopia were in danger of starvation due to the combination of severe drought and military conflicts. In 2006, the most serious humanitarian crisis in Africa was in Sudan's region Darfur.
The Green Revolution was widely viewed as an answer to famine in the 1970s and 1980s. Between 1950 and 1984, hybrid strains of high-yielding crops transformed agriculture around the globe and world grain production increased by 250%. Some criticize the process, stating that these new high-yielding crops require more chemical fertilizers and pesticides, which can harm the environment. Although these high-yielding crops make it technically possible to feed more people, there are indications that regional food production has peaked in many world sectors, due to certain strategies associated with intensive agriculture such as groundwater overdrafting and overuse of pesticides and other agricultural chemicals.
Frances Moore Lappé, later co-founder of the Institute for Food and Development Policy (Food First) argued in "Diet for a Small Planet" (1971) that vegetarian diets can provide food for larger populations, with the same resources, compared to omnivorous diets.
Noting that modern famines are sometimes aggravated by misguided economic policies, political design to impoverish or marginalize certain populations, or acts of war, political economists have investigated the political conditions under which famine is prevented. Economist Amartya Sen states that the liberal institutions that exist in India, including competitive elections and a free press, have played a major role in preventing famine in that country since independence. Alex de Waal has developed this theory to focus on the "political contract" between rulers and people that ensures famine prevention, noting the rarity of such political contracts in Africa, and the danger that international relief agencies will undermine such contracts through removing the locus of accountability for famines from national governments.
The demographic impacts of famine are sharp. Mortality is concentrated among children and the elderly. A consistent demographic fact is that in all recorded famines, male mortality exceeds female, even in those populations (such as northern India and Pakistan) where there is a male longevity advantage during normal times. Reasons for this may include greater female resilience under the pressure of malnutrition, and possibly female's naturally higher percentage of body fat. Famine is also accompanied by lower fertility. Famines therefore leave the reproductive core of a population—adult women—lesser affected compared to other population categories, and post-famine periods are often characterized a "rebound" with increased births.
Even though the theories of Thomas Malthus would predict that famines reduce the size of the population commensurate with available food resources, in fact even the most severe famines have rarely dented population growth for more than a few years. The mortality in China in 1958–61, Bengal in 1943, and Ethiopia in 1983–85 was all made up by a growing population over just a few years. Of greater long-term demographic impact is emigration: Ireland was chiefly depopulated after the 1840s famines by waves of emigration.
Risk of future famine.
"The Guardian" reports that in 2007 approximately 40% of the world's agricultural land is seriously degraded. If current trends of soil degradation continue in Africa, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa. As of late 2007, increased farming for use in biofuels, along with world oil prices at nearly $100 a barrel, has pushed up the price of grain used to feed poultry and dairy cows and other cattle, causing higher prices of wheat (up 58%), soybean (up 32%), and maize (up 11%) over the year. In 2007 Food riots have taken place in many countries across the world. An epidemic of stem rust, which is destructive to wheat and is caused by race Ug99, has in 2007 spread across Africa and into Asia.
Beginning in the 20th century, nitrogen fertilizers, new pesticides, desert farming, and other agricultural technologies began to be used to increase food production, in part to combat famine. Between 1950 and 1984, as the Green Revolution influenced agriculture, world grain production increased by 250%. However, as early as 1995, there were signs that these new developments may contribute to the decline of arable land (e.g. persistence of pesticides leading to soil contamination and decline of area available for farming). Developed nations have shared these technologies with developing nations with a famine problem.
David Pimentel, professor of ecology and agriculture at Cornell University, and Mario Giampietro, senior researcher at the National Research Institute on Food and Nutrition (INRAN), place in their study "Food, Land, Population and the U.S. Economy" the maximum U.S. population for a sustainable economy at 200 million.
According to geologist Dale Allen Pfeiffer, coming decades could see rising food prices without relief and massive starvation on a global level. Water deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China or India. The water tables are falling in many countries (including Northern China, the US, and India) due to widespread overconsumption. Other countries affected include Pakistan, Iran, and Mexico. This will eventually lead to water scarcity and cutbacks in grain harvest. Even with the overpumping of its aquifers, China has developed a grain deficit, contributing to the upward pressure on grain prices. Most of the three billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages.
After China and India, there is a second tier of smaller countries with large water deficits — Algeria, Egypt, Iran, Mexico, and Pakistan. Four of these already import a large share of their grain. Only Pakistan remains marginally self-sufficient. But with a population expanding by 4 million a year, it will also soon turn to the world market for grain. According to a UN climate report, the Himalayan glaciers that are the principal dry-season water sources of Asia's biggest rivers - Ganges, Indus, Brahmaputra, Yangtze, Mekong, Salween and Yellow - could disappear by 2350 as temperatures rise and human demand rises. Approximately 2.4 billion people live in the drainage basin of the Himalayan rivers. India, China, Pakistan, Afghanistan, Bangladesh, Nepal and Myanmar could experience floods followed by severe droughts in coming decades. In India alone, the Ganges provides water for drinking and farming for more than 500 million people.
Evan Fraser, a geographer at the University of Guelph in Ontario Canada, explores the ways in which climate change may affect future famines. To do this he draws on a range of historic cases where relatively small environmental problems triggered famines as a way of creating theoretical links between climate and famine in the future. Drawing on situations as diverse as the Great Irish Potato Famine, a series of weather induced famines in Asia during the late 19th century, and famines in Ethiopia during the 1980s, he concludes there are three “lines of defense” that protect a community’s food security from environmental change. The first line of defense is the agro-ecosystem on which food is produced: diverse ecosystems with well managed soils high in organic matter tend to be more resilient. The second line of defense is the wealth and skills of individual households: if those households affected by bad weather such as drought have savings or skills they may be able to do all right despite the bad weather. The final line of defense is created by the formal institutions present in a society. Governments, churches, or NGOs must be willing and able to mount effective relief efforts. Pulling this together, Evan Fraser argues that if an ecosystem is resilient enough, it may be able to withstand weather-related shocks. But if these shocks overwhelm the ecosystem’s line of defense, it is necessary for the household to adapt using its skills and savings. If a problem is too big for the family or household, then people must rely on the third line of defense, which is whether or not the formal institutions present in a society are able to provide help. Evan Fraser concludes that in almost every situation where an environmental problem triggered a famine you see a failure in each of these three lines of defense. Hence, understanding how climate change may cause famines in the future requires combining both an assessment of local socio-economic and environmental factors along with climate models that predict where bad weather may occur in the future
Causes.
Definitions of famines are based on three different categories – these include food supply-based, food consumption-based and mortality-based definitions. Some definitions of famines are:
Food shortages in a population are caused either by a lack of food or by difficulties in food distribution; it may be worsened by natural climate fluctuations and by extreme political conditions related to oppressive government or warfare. The conventional explanation until 1981 for the cause of famines was the Food availability decline (FAD) hypothesis. The assumption was that the central cause of all famines was a decline in food availability. However, FAD could not explain why only a certain section of the population such as the agricultural laborer was affected by famines while others were insulated from famines. Based on the studies of some recent famines, the decisive role of FAD has been questioned and it has been suggested that the causal mechanism for precipitating starvation includes many variables other than just decline of food availability. According to this view, famines are a result of entitlements, the theory being proposed is called the "failure of exchange entitlements" or FEE. A person may own various commodities that can be exchanged in a market economy for the other commodities he or she needs. The exchange can happen via trading or production or through a combination of the two. These entitlements are called trade-based or production-based entitlements. Per this proposed view, famines are precipitated due to a breakdown in the ability of the person to exchange his entitlements. An example of famines due to FEE is the inability of an agricultural laborer to exchange his primary entitlement, i.e., labor for rice when his employment became erratic or was completely eliminated.
According to the Physicians for Social Responsibility (PSR), global climate change is additionally challenging the Earth's ability to produce food, potentially leading to famine.
Some elements make a particular region more vulnerable to famine. These include poverty, population growth, an inappropriate social infrastructure, a suppressive political regime, and a weak or under-prepared government.
Climate and population pressure.
Many famines are caused by imbalance of food production compared to the large populations of countries whose population exceeds the regional carrying capacity . Historically, famines have occurred from agricultural problems such as drought, crop failure, or pestilence. Changing weather patterns, the ineffectiveness of medieval governments in dealing with crises, wars, and epidemic diseases such as the Black Death helped to cause hundreds of famines in Europe during the Middle Ages, including 95 in Britain and 75 in France. In France, the Hundred Years' War, crop failures and epidemics reduced the population by two-thirds.
The failure of a harvest or change in conditions, such as drought, can create a situation whereby large numbers of people continue to live where the carrying capacity of the land has temporarily dropped radically. Famine is often associated with subsistence agriculture. The total absence of agriculture in an economically strong area does not cause famine; Arizona and other wealthy regions import the vast majority of their food, since such regions produce sufficient economic goods for trade.
Famines have also been caused by volcanism. The 1815 eruption of the Mount Tambora volcano in Indonesia caused crop failures and famines worldwide and caused the worst famine of the 19th century. The current consensus of the scientific community is that the aerosols and dust released into the upper atmosphere causes cooler temperatures by preventing the sun's energy from reaching the ground. The same mechanism is theorized to be caused by very large meteorite impacts to the extent of causing mass extinctions.
State-sponsored famines.
In certain cases, such as the Great Leap Forward in China (which produced the largest famine in absolute numbers), North Korea in the mid-1990s, or Zimbabwe in the early-2000s, famine can occur because of government policy.
In 1932, under the USSR, Ukraine experienced one of their largest famines when between 2.4 and 7.5 millions peasants died as a result of state sponsored famine. It was termed Holodomor, suggesting that it was a deliberate repression to eliminate protesters of collectivization. Forced grain quotas imposed upon the rural peasants and brutal terror contributed to the widespread famine. The Soviet government continued to deny the problem and did not provide for victims nor accept foreign aid.
In 1958 in China, Mao Zedong's Communist Government began the Great Leap Forward campaign, aimed at rapidly industrializing the country. The government forcibly took control of agriculture. Barely enough grain was left for the peasants, and starvation set in many rural areas. Exportation of grain continued despite the famine to conceal the problem. While the famine is attributed to unintended consequences, it is believed that the government refused to acknowledge the problem, thereby further contributing to the deaths. In many instances, peasants were persecuted. Between 20 to 45 million people perished in this famine, making it one of the most deadly famines to date.
Malawi ended its famine by subsidizing farmers against the strictures of the World Bank. During the 1973 Wollo Famine in Ethiopia, food was shipped out of Wollo to the capital city of Addis Ababa, where it could command higher prices. In the late-1970s and early-1980s, residents of the dictatorships of Ethiopia and Sudan suffered massive famines, but the democracy of Botswana avoided them, despite also having a severe drop in national food production. In Somalia, famine occurred because of a failed state.
Famine prevention.
Food security.
Long term measures to improve food security, include investment in modern agriculture techniques, such as fertilizers and irrigation, but can also include strategic national food storage. In addition, recent work has indicated that the entire human population could be fed in the event of global catastrophe if alternative foods were used. In the book "Feeding Everyone No Matter What" methods of converting natural gas and wood to edible human food was described.
World Bank strictures restrict government subsidies for farmers, and increasing use of fertilizers is opposed by some environmental groups because of its unintended consequences: adverse effects on water supplies and habitat.
The effort to bring modern agricultural techniques found in the Western world, such as nitrogen fertilizers and pesticides, to Asia, called the Green Revolution, resulted in decreases in malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available. Supporting farmers in areas of food insecurity, through such measures as free or subsidized fertilizers and seeds, increases food harvest and reduces food prices.
The World Bank and some rich nations press nations that depend on them for aid to cut back or eliminate subsidized agricultural inputs such as fertilizer, in the name of privatization even as the United States and Europe extensively subsidized their own farmers.
Relief.
There is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable. The United Nations' World Food Program (WFP), the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a "revolution" in food aid. The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.
However, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help. Fred Cuny stated that "the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died." US Law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport. Fred Cuny further pointed out "studies of every recent famine have shown that food was available in-country — though not always in the immediate food deficit area" and "even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad."
Deficient micronutrients can be provided through fortifying foods. Fortifying foods such as peanut butter sachets (see Plumpy'Nut) have revolutionized emergency feeding in humanitarian emergencies because they can be eaten directly from the packet, do not require refrigeration or mixing with scarce clean water, can be stored for years and, vitally, can be absorbed by extremely ill children.
WHO and other sources recommend that malnourished children - and adults who also have diarrhea - drink rehydration solution, and continue to eat, in addition to antibiotics, and zinc supplements. There is a special oral rehydration solution called ReSoMal which has less sodium and more potassium than standard solution. However, if the diarrhea is severe, the standard solution is preferable as the person needs the extra sodium. Obviously, this is a judgment call best made by a physician, and using either solution is better than doing nothing. Zinc supplements often can help reduce the duration and severity of diarrhea, and Vitamin A can also be helpful. The World Health Organization underlines the importance of a person with diarrhea continuing to eat, with a 2005 publication for physicians stating: “Food should "never" be withheld and the child's usual foods should "not" be diluted. Breastfeeding should "always" be continued."
Ethiopia has been pioneering a program that has now become part of the World Bank's prescribed recipe for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food.
Levels of food insecurity.
In modern times, local and political governments and non-governmental organizations that deliver famine relief have limited resources with which to address the multiple situations of food insecurity that are occurring simultaneously. Various methods of categorizing the gradations of food security have thus been used in order to most efficiently allocate food relief. One of the earliest were the Indian Famine Codes devised by the British in the 1880s. The Codes listed three stages of food insecurity: near-scarcity, scarcity and famine, and were highly influential in the creation of subsequent famine warning or measurement systems. The early warning system developed to monitor the region inhabited by the Turkana people in northern Kenya also has three levels, but links each stage to a pre-planned response to mitigate the crisis and prevent its deterioration
The experiences of famine relief organizations throughout the world over the 1980s and 1990s resulted in at least two major developments: the "livelihoods approach" and the increased use of nutrition indicators to determine the severity of a crisis. Individuals and groups in food stressful situations will attempt to cope by rationing consumption, finding alternative means to supplement income, etc., before taking desperate measures, such as selling off plots of agricultural land. When all means of self-support are exhausted, the affected population begins to migrate in search of food or fall victim to outright mass starvation. Famine may thus be viewed partially as a social phenomenon, involving markets, the price of food, and social support structures. A second lesson drawn was the increased use of rapid nutrition assessments, in particular of children, to give a quantitative measure of the famine's severity.
Since 2003, many of the most important organizations in famine relief, such as the World Food Programme and the U.S. Agency for International Development, have adopted a five-level scale measuring intensity and magnitude. The intensity scale uses both livelihoods' measures and measurements of mortality and child malnutrition to categorize a situation as food secure, food insecure, food crisis, famine, severe famine, and extreme famine. The number of deaths determines the magnitude designation, with under 1000 fatalities defining a "minor famine" and a "catastrophic famine" resulting in over 1,000,000 deaths.
Society and culture.
Famine personified as an allegory is found in some cultures, e.g. one of the Four Horsemen of the Apocalypse in Christian tradition, the fear gorta of Irish folklore, or the Wendigo of Algonquian tradition.

</doc>
<doc id="51330" url="http://en.wikipedia.org/wiki?curid=51330" title="Babington Plot">
Babington Plot

The Babington Plot was a plot in 1586 to assassinate Queen Elizabeth, a Protestant, and put the rescued Mary, Queen of Scots, her Roman Catholic cousin, on the English throne. It led to the execution of Queen Mary Stuart of Scotland as a direct result of a letter sent by Queen Mary (who had been imprisoned for 18 years since 1568 in England at the behest of Queen Elizabeth) in which she consented directly to the assassination of Elizabeth.
The long-term goal of the plot was the invasion of England by the Spanish forces of King Philip II and the Catholic League in France, leading to the restoration of the old religion. The plot was discovered by Sir Francis Walsingham and used to entrap Queen Mary for the purpose of removing her as a claimant to the English throne.
The chief conspirators were Sir Anthony Babington, a young recusant nobleman targeted by Ballard; John Ballard, a Jesuit priest who desired to rescue the Scottish Queen; Robert Poley; Gilbert Gifford, and Thomas Phelippes, a Walsingham spy agent and code decypherer. The fallen priest Gifford had been in Walsingham's service since the end of 1585 or 1586. Gifford obtained a letter of introduction to Queen Mary from Morgan. Walsingham then placed double agent Gifford and spy decipherer Phelippes inside Chartley Castle, where Queen Mary was imprisoned. Gifford organised the Walsingham plan to place Babington's and Queen Mary's coded communications into a beer barrel cork which were then intercepted by Phelippes, decoded and sent to Walsingham.
Ballard was attempting to recruit Babington in an undeveloped scheme to rescue Queen Mary and place her on the throne of England by killing Queen Elizabeth. Babington sent a coded letter to the imprisoned Queen Mary, which gave his name to the complicated multiple-sided plot.
On 7 July 1586, the only Babington letter that was sent to Queen Mary was decoded by the spy Phelippes. Queen Mary responded in code on 17 July ordering the would-be rescuers to assassinate Queen Elizabeth. The response letter also included decyphered phrases indicating her desire to be rescued: "The affairs being thus prepared" and "I may suddenly be transported out of this place". At the Fotheringay trial in October 1586, Queen Elizabeth's agents William Cecil and Walsingham used the letter against Queen Mary who refused to admit that she was guilty. But she was betrayed by her secretaries Nau and Curle who confessed under pressure that the letter was mainly truthful, a fact not denied by Antonia Fraser, the most important modern biographer of Mary. (Fraser is in general a big defender of Mary Stuart but not in this case). To understand Mary's decision to accept the murder of Elizabeth, a few facts should be taken into account. First there was a conflict of 20 years between the two women, a conflict that was both political, religious and personal. It was not the first time that Mary had conspired against Elizabeth who in return treated Mary in a very harsh way. Second, Mary was the queen of France and Scotland, had had a court of more than 1000 servants, was considered the most beautiful woman in Europe and was the darling of the Renaissance Period, all of which she lost because of her English captivity. In 1586, Mary was a prisoner for 20 years who had lost her freedom, her son, her kingdom and her social life. She was a sick invalid, a very large overweight woman with a double chin who was unable to move without help; she was cut from any contact with her son who had betrayed her; her social life was mainly confined to her bed and room, where she spent almost all her time because of her health problems, under heavy guard with no outside contact; finally, if she was taken to another prison, it was in a closed litter under heavy guard to cut her from any interaction with the people of England. To understand Mary's frame of mind, we must consider, as Antonia Fraser put it, that a woman who lost everything saw a chance not only to escape her intolerant captivity, which could have gone for another 20 years, but also to achieve her ideal of a Catholic Restoration in England. Such a restoration was Mary's main aim in life, at least in her captive years.
Mary's imprisonment.
Mary, Queen of Scots, a Roman Catholic, was a legitimate heir to the throne of England. In 1568 she escaped imprisonment by Scottish rebels and sought the promised aid of her cousin, Queen Elizabeth I, a year after her forced abdication from the throne of Scotland. The issuance of the papal bull "Regnans in Excelsis" by Pope Pius V on 25 February 1570, granted English Catholics authority to overthrow the English queen. Queen Mary became the focal point of numerous plots and intrigues to restore England to its former religion, Catholicism, and to depose Elizabeth and even to take her life. Rather than the promised aid, Queen Elizabeth imprisoned Queen Mary for nineteen years in the charge of a succession of jailers, principally the Earl of Shrewsbury.
Queen Elizabeth ordered Queen Mary transferred back to the ruined Tutbury Castle in the wintry weather of Christmas Eve 1584. Queen Mary became ill due to the bad conditions of her captivity, imprisoned in a very damp cold room with closed windows and with no access to the sun.
In 1585, Queen Elizabeth ordered Queen Mary to be transferred in a coach and under heavy guard and placed under the strictest confinement at Chartley Hall in Staffordshire, under the control of Sir Amias Paulet. She was prohibited any correspondence with the outside world. Puritan Paulet was chosen by Queen Elizabeth in part because he abhorred Queen Mary's Catholic faith.
 Gifford a supporter of Mary, went to Paris to obtain the confidence of Morgan, then locked in the Bastille. Morgan previously worked for George Talbot, 6th Earl of Shrewsbury, an earlier jailor of Queen Mary. Through Shrewsbury, Queen Mary became acquainted with Morgan. Queen Mary sent Morgan to Paris to deliver letters to the French court. While in Paris Morgan became involved in a previous plot designed by William Parry, which resulted in Morgan's incarceration in the Bastille. In 1585 Gifford was arrested returning to England through Rye in Sussex with letters of introduction from Morgan to Queen Mary. Walsingham released Gifford to work as a double agent, in the Babington Plot.
The plot.
The Babington plot was related to several separate plans:
Infiltration.
Walsingham and Cecil realised that the July 1584 decree by Queen Elizabeth after the Throckmorton plot that prevented all communication to and from Queen Mary, also impaired their ability to entrap her in another plot. They needed evidence of another plot for which she could be executed based on their Bond of Association tenets. Thus Walsingham established a new line of communication, one which he could carefully control without incurring any suspicion from Queen Mary. Gifford approached Guillaume de l'Aubespine, Baron de Châteauneuf-sur-Cher, the French ambassador to England and described the new correspondence arrangement that had been designed by Walsingham. Gifford and jailor Paulet had arranged for a local brewer to facilitate the movement of messages between Queen Mary and her supporters by placing them in a watertight casing that could be placed inside the stopper of the barrel. Phelippes was then quartered at Chartley Hall to receive the messages, decode them and send them to Walsingham. Gifford submitted a code table that had been supplied by Walsingham to Chateauneuf and then requested the first message be sent to Queen Mary.
All subsequent messages to Queen Mary would be sent via diplomatic packets to Chateauneuf, who then passed them on to Gifford. Gifford would pass them on to Walsingham, who would confide them to Thomas Phelippes, a cipher and language expert in his employ. Phelippes was previously employed by Amias Paulet when the latter was Elizabeth's ambassador to France. The cipher used was a nomenclator cipher. Phelippes would decode and make a copy of the letter. The letter was then resealed and given back to Gifford, who would pass it on to the brewer. The brewer would then "smuggle" the letter to Queen Mary. If Queen Mary sent a letter to her supporters, it would go through the reverse process. In short order, every message coming to and from Chartley Hall was intercepted and read by Walsingham, who became aware of every plot.
Firmer plans and a developing plot: John Ballard and Anthony Babington.
At the behest of Mary's French supporters, John Ballard, a Jesuit priest and agent of the Roman Church, went to England on various occasions in 1585 to secure promises of aid from the northern Catholic gentry of the imprisoned Queen who would accept an insurrection against Elizabeth and replace her with Mary. In March 1586, he met with John Savage, an ex-soldier who was involved in a separate plot against Elizabeth and who had sworn an oath to assassinate the queen. Later that same year, he reported to Charles Paget and Don Bernardino de Mendoza and told them that English Catholics were prepared to mount an insurrection against Elizabeth, provided that they would be assured of foreign support. While it was uncertain whether Ballard's report of the extent of Catholic opposition was accurate, what was certain that he was able to secure assurances that support would be forthcoming. After this he returned to England, where he persuaded a member of the Catholic gentry, Anthony Babington to lead and organise the English Catholics against Elizabeth. Ballard informed Babington about all the plans that had been so far proposed. But Babington's confession made it clear that Ballard was sure of the support of the Catholic League: 
Despite this assurance of foreign support, Babington was hesitant as he thought that no foreign invasion would succeed for as long as Elizabeth remained, to which Ballard answered that the plans of John Savage would take care of that. After a lengthy discussion with friends and soon to be fellow conspirators, Babington consented to join and to lead the conspiracy.
Unfortunately for the conspirators, Walsingham was certainly aware of some of the aspects of the plot, based on reports by his spies, most notably Gilbert Gifford, who kept tabs on all the major participants. While he could have shut down some part of the plot and arrested some of those involved within reach, he still lacked any piece of evidence that would prove Queen Mary's active participation in the plot and he feared to commit any mistake which might cost Elizabeth her life. They did not manage to assassinate queen Elizabeth I of England.
The Fatal Correspondence.
Despite his assent in his participation in the plot, Babington's conscience was troubled at the prospect of assassinating the English queen. On 28 June 1586, encouraged by a letter received from Thomas Morgan, Queen Mary wrote a letter to Babington that assured him of his status as a trusted friend. In a reply in 7 July 1586, Babington wrote to Mary about all the details of the plot. He informed Mary about the foreign plans for invasion as well as the planned insurrection by English Catholics:
He also mentioned plans on rescuing Mary from Chartley as well as dispatching Savage to assassinate Elizabeth: 
The letter was received by Mary, who was in a dark mood at that period of time because she received the news that her son betrayed her in favour of Elizabeth, on 14 July 1586 — after being intercepted and deciphered — and on 17 July she replied to Babington in a long letter in which she outlined the components of a successful rescue and the need to assassinate Elizabeth if her rescue had any chance to be successful. She also stressed the necessity of foreign aid if the rescue attempt was to succeed: 
Queen Mary in her response letter, advised the would-be rescuers to confront the puritans and to link her case to the Queen of England as her heir.
The letter was again intercepted and deciphered by Phelippes. But this time, Phelippes, who was also an excellent forger, kept the original and made a copy of the letter in which Mary supported the murder of Elizabeth. In the letter Mary not only consented to the assassination but also made clear( because of her hate of Elizabeth, her ambition and her desire to be free from her intolerable captivity which was a living hell for her), that she is prepared to take an active part in the assassination . Phelippes who have seen Mary many times describing her in his letters as a very sad, angry and desperate woman who was very big, obese and very tall [she was almost 6 feet tall without heels, 20 to 30 cm taller than the average man living in the 16th century]; with a double chin giving her a massive presence which combined with her legend as a martyred queen and the remains of her beauty explain Elizabeth jealousy, fears and her determination not to allow Mary any social or political interaction with any person. Mary was taken by her guards in a coach, a litter or in a big chair because of her invalidity and inability to walk alone and who in addition was not allowed by her guards to approach Phelippes or anybody else when she saw him sometimes when she was outside her room where she usually spent almost all her time in bed due to her invalidity with no social contact in a cold damp room with barred windows which even kept the sun from reaching her; finally the privies stench system was directly operated below her room leading to the destruction of her health which explain her decision to be rescued even if it meant the killing of Elizabeth. Phelippes added a forged part focusing on the name of the conspirators:
Phelippes then made another copy of the letter and sent it to Walsingham with a small picture of the gallows as a seal.
Arrests, trials and executions.
John Ballard was arrested on 4 August 1586, and under torture he confessed and implicated Babington. Although Babington was able to receive the letter with the postscript, he was not able to reply with the names of the conspirators, as he was arrested. Others were taken prisoner by 15 August 1586. Mary's two secretaries, Claude Nau de la Boisseliere (died 1605) and Gilbert Curle (died 1609), were likewise taken into custody and interrogated.
The conspirators were sentenced to death for treason and conspiracy against the crown, and were sentenced to be hanged, drawn, and quartered. This first group included Babington, Ballard, Chidiock Tichborne, Sir Thomas Salusbury, Robert Barnewell, John Savage and Henry Donn. A further group of seven men, Edward Havington, Charles Tilney, Edward Jones, John Charnock, John Travers, Jerome Bellamy, and Robert Gage, were tried and convicted shortly afterward. Ballard and Babington were executed on 20 September 1586 along with the other men who had been tried with them. Such was the public outcry at the horror of their execution that Queen Elizabeth changed the order for the second group to be allowed to hang until dead before being disembowelled.
In October 1586 Queen Mary of Scotland was sent to trial at Fotheringhay Castle in Northamptonshire by 46 English Lords, Bishops and Earls. She was not permitted legal counsel, not permitted to review the evidence against her and not permitted to provide witnesses. Portions of spy Phellipes' letter translations were read at the trial. As the Scottish Queen, Mary was convicted of treason against the foreign country of England. One English Lord voted not guilty. Elizabeth signed her cousin's death warrant, and on 8 February 1587, in front of 300 witnesses, Mary, Queen of Scots, was executed by beheading with an axe in a very painful manner since it took three strikes to cut her head; her hands put forcefully behind her back like a traitor who sponsored the Babington Plot.
In literature.
"Mary Stuart" (German: "Maria Stuart"), a dramatised version of the last days of Mary, Queen of Scots, including the Babington Plot, was written by Friedrich Schiller and performed in Weimar, Germany in 1800. This in turn formed the basis for "Maria Stuarda", an opera by Donizetti, in 1835: although the Babington Plot occurs before the events of the opera, and is only referenced twice during the opera, the second such occasion being Mary admitting her own part in it, in private, to her final confessor (a role taken by Lord Talbot in the opera, although not in real life.)
The story of the Babington Plot is dramatised in the novel "Conies in the Hay" by Jane Lane. (ISBN 0-7551-0835-3), and features prominently in Anthony Burgess's "A Dead Man in Deptford". Episode Four of the television series "Elizabeth R" (titled "Horrible Conspiracies") is devoted to the Babington Plot, and the movie "" deals substantially with the Plot as well. A more fictional account is given in the "My Story" book series, "The Queen's Spies" (retitled "To Kill A Queen" 2008) told in diary format by a fictional Elizabethan girl, Kitty.
The Babington plot is also the subject of the children's novel "A Traveller in Time" by Alison Uttley, who grew up near the Babington family home in Derbyshire.

</doc>
<doc id="51331" url="http://en.wikipedia.org/wiki?curid=51331" title="Dimensionless quantity">
Dimensionless quantity

In dimensional analysis, a dimensionless quantity is a quantity to which no physical dimension is applicable. It is thus a bare number, and is therefore also known as a quantity of dimension one. Dimensionless quantities are widely used in many fields, such as mathematics, physics, engineering, and economics. Numerous well-known quantities, such as π, e, and φ, are dimensionless. By contrast, examples of quantities with dimensions are length, time, and speed, which are measured in dimensional units, such as meter, second and meter/second.
Dimensionless quantities are often obtained as products or ratios of quantities that are not dimensionless, but whose dimensions cancel in the mathematical operation. This is the case, for instance, with the engineering strain, a measure of deformation. It is defined as change in length, divided by initial length, but because these quantities both have dimensions "L" (length), the result is a dimensionless quantity.
Buckingham π theorem.
Another consequence of the Buckingham π theorem of dimensional analysis is that the functional dependence between a certain number (say, "n") of variables can be reduced by the number (say, "k") of independent dimensions occurring in those variables to give a set of "p" = "n" − "k" independent, dimensionless quantities. For the purposes of the experimenter, different systems that share the same description by dimensionless quantity are equivalent.
Example.
The power consumption of a stirrer with a given shape is a function of the density and the viscosity of the fluid to be stirred, the size of the stirrer given by its diameter, and the speed of the stirrer. Therefore, we have "n" = 5 variables representing our example.
Those "n" = 5 variables are built up from "k" = 3 dimensions:
According to the π-theorem, the "n" = 5 variables can be reduced by the "k" = 3 dimensions to form "p" = "n" − "k" = 5 − 3 = 2 independent dimensionless numbers, which are, in case of the stirrer:
Standards efforts.
The International Committee for Weights and Measures contemplated defining the unit of 1 as the 'uno', but the idea was dropped.
Dimensionless physical constants.
Certain fundamental physical constants, such as the speed of light in a vacuum, the universal gravitational constant, Planck's constant and Boltzmann's constant can be normalized to 1 if appropriate units for time, length, mass, charge, and temperature are chosen. The resulting system of units is known as the natural units. However, not all physical constants can be normalized in this fashion. For example, the values of the following constants are independent of the system of units and must be determined experimentally:
List of dimensionless quantities.
All numbers are dimensionless quantities. Certain dimensionless quantities of some importance are given below:

</doc>
<doc id="51332" url="http://en.wikipedia.org/wiki?curid=51332" title="Power number">
Power number

The power number "N"p (also known as Newton number) is a commonly used dimensionless number relating the resistance force to the inertia force. 
The power-number has different specifications according to the field of application. E.g., for
stirrers the power number is defined as:
formula_1
with 

</doc>
<doc id="51333" url="http://en.wikipedia.org/wiki?curid=51333" title="Miloš Forman">
Miloš Forman

Jan Tomáš Forman (]; born February 18, 1932), known as Miloš Forman (], ), is a Czech film director, screenwriter, actor, and professor, who until 1968 lived and worked primarily in the former Czechoslovakia.
Forman was one of the most important directors of the Czechoslovak New Wave. His 1967 film "The Fireman's Ball", on the surface a naturalistic representation of an ill-fated social event in a provincial town, was seen by both movie scholars and authorities in Czechoslovakia as a biting satire on Eastern European Communism, resulting in it being banned for many years in Forman's home country.
Since Forman left Czechoslovakia, two of his films, "One Flew Over the Cuckoo's Nest" and "Amadeus", have acquired particular renown, both gaining him an Academy Award for Best Director. "One Flew Over the Cuckoo's Nest" was the second to win all five major Academy Awards (Best Picture, Actor in Leading Role, Actress in Leading Role, Director, and Screenplay) following "It Happened One Night" in 1934, an accomplishment not repeated until 1991 by "The Silence of the Lambs". Forman was also nominated for a Best Director Oscar for "The People vs. Larry Flynt". He has also won Golden Globe, Cannes, Berlinale, BAFTA, Cesar, David di Donatello, European Film Academy, and Czech Lion awards.
Personal life.
Forman was born in Čáslav, Czechoslovakia (present-day Czech Republic), the son of Anna (née Švábová), who ran a summer hotel. When young, he believed his biological father to be Rudolf Forman, a professor. Both Anna and Rudolf Forman were Protestants. During the Nazi occupation, a member of the anti-Nazi Underground named Rudolf Forman as a member of the Underground while being interrogated by the Gestapo. Rudolf was arrested for distributing banned books and died in Buchenwald in 1944. Forman's mother died in Auschwitz in 1943. Forman has stated that he did not fully understand what had happened to them until he saw footage of the concentration camps when he was 16.
Forman lived with relatives during World War II and later discovered that his biological father was in fact a Jewish architect, Otto Kohn, a survivor of the Holocaust. He has a brother, Pavel Forman, 12 years older, a Czech painter who, after the 1968 invasion, emigrated to Australia. In his youth, he wanted to become a theatrical producer, bypassing theater. Through his biological father, he is a half-brother of mathematician Joseph J. Kohn.
After the war, Forman attended the elite King George boarding school in the spa town Poděbrady, where his fellow students included Václav Havel, the Mašín brothers and future film-makers Ivan Passer and Jerzy Skolimowski. He later studied screenwriting at the Academy of Performing Arts in Prague. He was assistant of Alfred Radok, creator of Laterna Magika. During the Warsaw Pact invasion of Czechoslovakia in summer 1968, he left Europe for the United States.
Forman's first wife was Czech movie star Jana Brejchová. They met during the making of the movie "Štěňata" (1957). They divorced in 1962. Forman has twin sons with his second wife, Czech actress Věra Křesadlová-Formanová. Both sons, Petr Forman and Matěj Forman, born 1964, are involved in the theatre. That marriage lasted for thirty-five years, spanning 1964 to 1999. Then Forman married Martina Zbořilová on November 28, 1999. They also have twin sons, Jim and Andy (born 1999, named for comics Jim Carrey and Andy Kaufman), and reside in Connecticut, USA.
In 2006, he received the Hanno R. Ellenbogen Citizenship Award presented by the Prague Society for International Cooperation.
He is a professor emeritus at Columbia University.
The asteroid 11333 Forman was named after Forman.
In 2009 a documentary film about Forman directed by Miloš Šmídmajer was produced – "Miloš Forman: Co te nezabije...".
Forman has written poems and published an autobiography called "My Two Worlds".
Career.
Along with future favorite cinematographer Miroslav Ondříček and longtime schoolfriend Ivan Passer, Forman filmed the silent documentary "Semafor" about Semafor theater. Forman's first important production was the documentary "Audition" whose subject was competing singers. He directed several Czech comedies in Czechoslovakia. However, during the Prague Spring and the ensuing 1968 invasion, he was in Paris negotiating the production of his first American film. His employer, a Czech studio, fired him, claiming that he had been out of the country illegally. He moved to New York, where he later became a professor of film at Columbia University and co-chair (with his former teacher František Daniel) of Columbia's film department. One of his protégés was future director James Mangold, whom Forman had advised about scriptwriting.
In 1977, he became a naturalized citizen of the United States.
In 1985 he headed the Cannes film festival and in 2000 did the same for the Venice festival. He presided over a ceremony of Caesar in 1988.
In 1997, he received the Crystal Globe award for outstanding artistic contribution to world cinema at the Karlovy Vary International Film Festival. Forman performed alongside actor Edward Norton in Norton's directorial debut, "Keeping the Faith" (2000), as the wise friend to Norton's conflicted priest.
In April 2007 the jazz opera "Dobře placená procházka" premiered at the Prague National Theatre, directed by Forman's son, Petr Forman.
Forman received an honorary degree in 2009 from Emerson College in Boston, USA.
He regularly collaborates with cinematographer Miroslav Ondříček.
Films.
"Loves of a Blonde".
"Loves of a Blonde" is one of the best–known movies of Czechoslovak New Wave and has won awards at the Venice and Locarno film festivals. It was also nominated for the Academy Award for Best Foreign Language Film in 1967.
"The Fireman's Ball".
A 1967 Czechoslovak-Italian co-production, this was Forman's first color film. It is one of the best–known movies of Czechoslovak New Wave. On the face of it a naturalistic representation of an ill-fated social event in a provincial town, the film has been seen by both movie scholars and the then-authorities in Czechoslovakia as a biting satire on East European Communism, which resulted in it being banned for many years in Forman's home country.
It was nominated for the Academy Award for Best Foreign Film.
"Taking Off".
The first movie Forman made in the United States, "Taking Off" won the Grand Prix at the 1971 Cannes Film Festival. The film starred Lynn Carlin and Buck Henry, and also featured Linnea Heacock as Jeannie.
"One Flew Over the Cuckoo's Nest".
In spite of initial difficulties, he started directing in the United States, and achieved success in 1975 with the adaptation of Ken Kesey's novel "One Flew Over the Cuckoo's Nest" starring Jack Nicholson and Louise Fletcher. The film won Oscars in the five most important categories: Best Director, Best Actor, Best Actress, Best Picture and Best Adapted Screenplay, one of only three films in history to do so, along with "It Happened One Night" and "The Silence of the Lambs", and firmly established Forman's reputation.
"Hair".
The success of "One Flew Over the Cuckoo's Nest" allowed Forman to direct the long-planned film "Hair" (a rock musical) in 1979, based on the Broadway musical by James Rado, Gerome Ragni, and Galt MacDermot. The film starred Treat Williams, John Savage and Beverly D'Angelo.
"Amadeus".
Forman's next important achievement was the adaption of Peter Shaffer's "Amadeus" in 1984—retelling the story of Wolfgang Amadeus Mozart and Antonio Salieri. The internationally acclaimed film starred Tom Hulce, Elizabeth Berridge and F. Murray Abraham. The movie won eight Oscars, including Best Picture, Best Director and Best Actor (Abraham).
"Valmont".
His adaptation of Pierre Choderlos de Laclos's novel "Les Liaisons dangereuses", it had its premiere on November 17, 1989. Another film adaptation by Stephen Frears had been released the previous year and received much acclaim. The film starred Colin Firth, Meg Tilly and Annette Bening.
"The People vs. Larry Flynt".
The 1996 biographical film of pornographic publisher Larry Flynt brought Forman another Oscar nomination. The film starred Woody Harrelson, Courtney Love and Edward Norton.
"Man on the Moon".
The biography of famous actor and avant-garde comic Andy Kaufman (Jim Carrey, who won a Golden Globe for his performance) premiered on December 22, 1999. The film also starred Danny DeVito, Courtney Love and Paul Giamatti.
"Goya's Ghosts".
This free biography of Spanish painter Francisco Goya (American-Spanish co-production) premiered on November 8, 2006. The film starred Natalie Portman, Javier Bardem, Stellan Skarsgård and Randy Quaid.
Influence on the Czech language.
Forman's early films are popular among Czechs. Many situations and phrases from his movies have passed into common use. For example, the Czech term "zhasnout" ("to switch lights off") from "The Fireman's Ball", associated with petty theft in the film, has been used to describe the large-scale asset stripping that occurred in the country during the 1990s.
Awards, nominations and honours.
Academy Awards
Golden Globe
Cannes
Berlinale
BAFTA
César Award
David di Donatello
European Film Academy
Czech Lion
List of Greatest Czechs

</doc>
<doc id="51334" url="http://en.wikipedia.org/wiki?curid=51334" title="Antigua (disambiguation)">
Antigua (disambiguation)

Antigua can refer to:

</doc>
