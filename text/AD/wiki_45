<doc id="50649" url="http://en.wikipedia.org/wiki?curid=50649" title="Family and consumer science">
Family and consumer science

Family and Consumer Sciences (FCS), also known as home economics, is the profession and field of study that deals with the economics and management of the home and community. The field deals with the relationship between individuals, families, and communities, and the environment in which they live.
As a subject of study, FCS is taught in secondary schools, colleges and universities, vocational schools, and in adult education centers; students include women and men. It prepares students for homemaking or professional careers, or to assist in preparing to fulfill real-life responsibilities at home. As a profession, it includes educators in the field and human services professionals.
The field represents many disciplines including consumer science, nutrition, food preparation, parenting, early childhood education, family economics and resource management, human development, interior design, textiles, apparel design, as well as other related subjects. Family and Consumer Sciences education focuses on individuals and families living in society throughout the life span, thus dealing not only with families but also with their interrelationships with the communities. Other topics such as sexual education, food management, and fire prevention might also be covered.
Name.
Family & Consumer Sciences was previously known as Home Economics (often abbreviated "home ec" or HE"). In 1994, various organizations including the American Association of Family and Consumer Sciences adopted the new term "family and consumer science" to reflect the fact that the field covers aspects outside of home life and wellness.
The field is also known by other names, including human sciences, home science, and domestic economy. In addition, home economics has a strong historical relationship to the field of human ecology, and since the 1960s a number of university-level home economics programs have been renamed "human ecology" programs, including Cornell University's program, among others.
History.
One of the first to champion the economics of running a home was Catherine Beecher, sister to Harriet Beecher Stowe. Catherine and Harriet both were leaders in mid-19th century North America in talking about domestic science. They came from a very religious family that valued education especially for women.
The Morrill Act of 1862 propelled domestic science further ahead as land grant colleges sought to educate farm wives in running their households as their husbands were being educated in agricultural methods and processes. Iowa, Kansas, Nebraska, Illinois, Minnesota and Michigan were early leaders offering programs for women. There were women graduates of these institutions several years before the Lake Placid Conferences which gave birth to the home economics movement.
The home economics movement started with Ellen Swallow Richards, who was the first woman to attend Massachusetts Institute of Technology and later became the first female instructor. Through her chemistry research, she became an expert in water quality and later began to focus on applying scientific principles to domestic situations. At the Chicago World’s Fair in 1893, she designed the Rumford Kitchen, which was a tiny kitchen that served nutritious meals to thousands of fair goers, along with a healthy dose of nutrition education. She shunned an invitation to participate in the Women’s Building as she said none of her research was just women’s work, but rather information for all.
Late in the 19th century, Richards convened a group of contemporaries to discuss the essence of domestic science and how the elements of this discipline would ultimately improve the quality of life for many individuals and families. They met at pristine Lake Placid, New York at the invitation of Melvil Dewey. Over the course of the next 10 years, these educators worked tirelessly to elevate the discipline, which was to become home economics, to a legitimate profession. Richards wanted to call this oekology or the science of right living. Euthenics, the science of controllable environment, was also a name of her choice, but "home economics" was ultimately chosen as the official term in 1899. Richards then founded the American Home Economics Association (now called the American Association of Family and Consumer Sciences) in 1909.
In the 1910s the AHEA won passage of two crucial pieces of legislation that allowed home economists to establish formal niches for research and teaching within institutions of higher education. The Smith-Lever Act of 1914 and the Smith-Hughes Act of 1917 provided funding to expand demonstration work in rural communities and to develop and teach a home economics curriculum on the campuses of most state land-grant colleges. 
Home economics emerged at the turn of the twentieth century as a movement to train women to be more efficient household managers. At the same moment, American families began to consume many more goods and services than they produced. To guide women in this transition, professional home economics had two major goals: to teach women to assume their new roles as modern consumers and to communicate homemakers’ needs to manufacturers and political leaders. The development of the profession with its origins as an educational movement to its identity as a source of consumer expertise in the interwar period to its virtual disappearance by the 1970s [is the subject of Carolyn M. Goldstein’s book CREATING CONSUMERS: HOME ECONOMISTS IN TWENTIETH-CENTURY AMERICA.]. 
In the 19th century, home economics classes were intended to ready young women for their duties in the home in healthy environments. Classes were first offered in the United States, Canada, Germany and Great Britain, followed by Latin America, Asia, and Africa. International organizations such as those associated with the United Nations have been involved in starting home economics programs around the world.
Through the 1940s Iowa State College (later University) was the only program granting a master of science in household equipment however this program was centered on the ideals that women should acquire practical skills but also a scientifically based understanding of how technology in the household works. For example, women were required to disassemble and then reassemble kitchen machinery so they could understand basic operations as well as repair the equipment. In doing so, Iowa State effectively created a culturally acceptable form of physics and engineering for within the boundaries of what constituted the boundaries of woman's knowledge at the time.
Home economists … found a receptive audience among many of its young female members who expressed interest in learning about how to improve their homes, spend their leisure time, and make decisions about what to buy, what to make at home, and even what books to read. 
A Standards Committee’s proposal in 1924 changed the AHEA constitution allowing for a special voting membership section clearing the way for HEIB (Home Economists in Business) section. The largest group of HEIB members worked for food companies such as Washburn Crosby (later General Mills), Kellogg’s, ..or trade associations charged with promoting particular foods such as the Institute of American Meat Packers and the National Dairy Council. [Some] worked as journalists for women’s magazines such as.. the Ladies’ Home Journal. [Others] worked for utility companies. 
The function that home economists have been performing for more than a century remains key to our experiences as consumers, but the professional identity has disappeared. Indeed, efforts to reposition the discipline ultimately led to its renaming in the late twentieth century. In 1994, “home economics” was replaced by “family and consumer science.”…This renaming signaled a formal break from the field’s association with domesticity, highlighting instead issues of “family” and “consumption.” 
A new actor that rose to prominence in the 1980s was the charismatic celebrity unaligned with home economists or any other professional group. Martha Stewart and her fellow “domestic celebrities,” in television programs, books, magazines, and websites, attest to the continued importance of independent experts and commercial mass-media organizations in facilitating technological and cultural change in consumer products and services industries. 
To a great degree, Americans today tap into values once enshrined in a home economics curriculum to express a national identity and assert a standard of living that is distinct not only from Third World cultures but also from the cultures of other industrialized nations... Even as luxury consumption exerts a cultural pull for many, middle-class culture still celebrates consumers who make sensible, controlled choices in the marketplace, resisting pure pleasure, impulse purchases, and cheap or shoddy goods. 
By country.
FCS is taught worldwide, both as an elective or as a required course in secondary education, and in many tertiary and continuing education institutions. Sometimes it is also taught in primary education. International cooperation in the field is cooordinated by the International Federation for Home Economics, established in 1908.
South Korea.
In South Korea, the field is most commonly known as "family studies" or "family science" (가정과학, gajeong-gwahak). The field began in schools taught by Western missionaries in the late 19th century. The first college-level department of family science was established at Ewha Womans University in Seoul in 1929.
United States.
In the United States, most states still require Family & Consumer Sciences as a required course for middle school courses, while high school students choose to take it as an elective. Approximately 5 million students in US secondary education take FCS each year.
Content.
Situated in the human sciences, home economics draws from a range of disciplines to achieve optimal and sustainable living for individuals, families, and communities. Historically, home economics has been in the context of the home and household, but this has extended in the 21st century to include the wider living environments as we better understand that the capacities, choices, and priorities of individuals and families impact at all levels, ranging from the household to the local and the global community. Home economists are concerned with promoting and protecting the well-being of individuals, families, and communities; they facilitate the development of attributes for lifelong learning for paid, unpaid, and voluntary work. Home economics professionals are advocates for individuals, families, and communities.
The content of home economics comes from the synthesis of multiple disciplines. This interdisciplinary knowledge is essential because the phenomena and challenges of everyday life are not typically one-dimensional. The content of home economics courses vary, but may include: food, nutrition, and health; personal finance; family resource management and planning; textiles and clothing; shelter and housing; consumerism and consumer science; household management; design and technology; food science and hospitality; human development and family studies; education and community services, among others. The capacity to draw from such disciplinary diversity is a strength of the profession, allowing for the development of specific interpretations of the field, as relevant to the context.
Cleaning.
Home cleaning tasks can be separated into four categories: litter removal, storage of belongings, dusting, and washing of surfaces. Washing of surfaces is the most dangerous and complicated part because of the cleaning solutions. For example, hard water deposits are cleaned with acid solutions and grease is cleaned with alkaline solutions; they can both harm the skin and are reactive toward each other, potentially producing unwanted by-products. Mixing together chlorine bleach and strong acids (e.g. limescale remover containing HCl) forms chlorine gas, which is toxic. Solvents such as paint thinner and rubbing alcohol are toxic and flammable. Some disinfectants are toxic. Even dish water can require rubber gloves.
Finances.
Dealing with finances is a key element of home economics. Understanding concepts such as debt, credit, loaning, interest rates and so on are important to having a healthy economic home. Life skills such as making a budget, keeping financial records and making frugal purchasing choices are all part of this subject area. As home economics is a very holistic field, one must also incorporate concepts like purchasing power and how consumers can affect the global world.
Professional associations.
The AAFCS (American Association of Family & Consumer Sciences) represents teachers, educators, cooperatives, business, designers and nutritionists.The American Association of Family & Consumer Sciences (AAFCS) is the only national forum where K-12 teachers, university educators, and corporate executives collaborate to improve the quality of individual and family life.
The Association for Career and Technical Education (ACTE) is the largest American national education association dedicated to the advancement of education that prepared youth and adults for successful careers. ACTE's core purpose is to provide leadership in developing an educated, prepared, and competitive workforce. The ACTE division of Family and Consumer Sciences Education includes three sections (1) NATFACS - National Association Teachers of Family and Consumer Sciences (2) NATEFACS - National Association Teacher Educators of Family andy Consumer Sciences, andy (3) NASAFACS - National Association State Administrators of Family and Consumer Sciences.
The National Council on Family Relations, (NCFR) founded in 1938, is the oldest multidisciplinary, nonpartisan professional organization focused solely on family research, practice and education. The premier professional association for the multidisciplinary understanding of families. The members’ interests—as diverse as their careers and backgrounds—are focused on topics and efforts that yield a common benefit: …understanding and strengthening families. NCFR members are professionals dedicated to understanding and strengthening families. Our 3,400-plus members come from more than 35 countries and all 50 U.S. states, and include: researchers, demographers, marriage and family therapists, parent/family educators, university faculty, students, social workers, public health workers, extension specialists and faculty, ECFE teachers, clergy, counselors, K-12 teachers, and more.
Areas of practice.
Home Economics is also called Human sciences based on everyday work where the setting is the house.
Home economics has four dimensions of practice:
To be successful in these four dimensions of practice means that the profession is constantly evolving, and there will always be new ways of performing the profession.

</doc>
<doc id="50650" url="http://en.wikipedia.org/wiki?curid=50650" title="Astronomy">
Astronomy

Astronomy is a natural science which is the study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma ray bursts, and cosmic microwave background radiation. A related but distinct subject, physical cosmology, is concerned with studying the universe as a whole.
Astronomy is one of the oldest sciences. The early civilizations in recorded history, such as the Babylonians, Greeks, Indians, Egyptians, Nubians, Iranians, Chinese, and Maya performed methodical observations of the night sky. However, the invention of the telescope was required before astronomy was able to develop into a modern science. Historically, astronomy has included disciplines as diverse as astrometry, celestial navigation, observational astronomy and the making of calendars, but professional astronomy is nowadays often considered to be synonymous with astrophysics.
During the 20th century, the field of professional astronomy split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects, which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other, with theoretical astronomy seeking to explain the observational results and observations being used to confirm theoretical results.
Astronomy is one of the few sciences where amateurs can still play an active role, especially in the discovery and observation of transient phenomena and Amateur astronomers have made and contributed to many important astronomical discoveries.
Etymology.
"Astronomy" (from the Greek ἀστρονομία from ἄστρον "astron", "star" and -νομία "-nomia" from νόμος "nomos", "law" or "culture") means "law of the stars" (or "culture of the stars" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin they are now entirely distinct.
Use of terms "astronomy" and "astrophysics".
Generally, either the term "astronomy" or "astrophysics" may be used to refer to this subject. Based on strict dictionary definitions, "astronomy" refers to "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties" and "astrophysics" refers to the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". In some cases, as in the introduction of the introductory textbook "The Physical Universe" by Frank Shu, "astronomy" may be used to describe the qualitative study of the subject, whereas "astrophysics" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Few fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use "astronomy" and "astrophysics," partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. One of the leading scientific journals in the field is the European journal named "Astronomy and Astrophysics". The leading American journals are "The Astrophysical Journal" and "The Astronomical Journal".
History.
In early times, astronomy only comprised the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops, as well as in understanding the length of the year.
Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, India, China, Egypt, and Central America, astronomical observatories were assembled, and ideas on the nature of the universe began to be explored. Most of early astronomy actually consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the universe were explored philosophically. The Earth was believed to be the center of the universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the universe, or the Ptolemaic system, named after Ptolemy.
A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros.
Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and was the first to propose a heliocentric model of the solar system. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.
During the Middle Ages, astronomy was mostly stagnant in medieval Europe, at least until the 13th century. However, astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was discovered by the Persian astronomer Azophi and first described in his "Book of Fixed Stars". The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and the Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Azophi, Albumasar, Biruni, Arzachel, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars. It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed an astronomical observatory. Europeans had previously believed that there had been no astronomical observation in pre-colonial Middle Ages sub-Saharan Africa but modern discoveries show otherwise.
The Roman Catholic Church gave more financial and social support to the study of astronomy for over six centuries, from the recovery of ancient learning during the late Middle Ages into the Enlightenment, than any other, and, probably, all other, institutions. Among the Church's motives was finding the date for Easter.
Scientific revolution.
During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended, expanded upon, and corrected by Galileo Galilei and Johannes Kepler. Galileo used telescopes to enhance his observations.
Kepler was the first to devise a system that described correctly the details of the motion of the planets with the Sun at the center. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was left to Newton's invention of celestial dynamics and his law of gravitation to finally explain the motions of the planets. Newton also developed the reflecting telescope.
Further discoveries paralleled the improvements in the size and quality of the telescope. More extensive star catalogues were produced by Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. The distance to a star was first announced in 1838 when the parallax of 61 Cygni was measured by Friedrich Bessel.
During the 18–19th centuries, attention to the three body problem by Euler, Clairaut, and D'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Lagrange and Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.
Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.
The existence of the Earth's galaxy, the Milky Way, as a separate group of stars, was only proved in the 20th century, along with the existence of "external" galaxies, and soon after, the expansion of the Universe, seen in the recession of most galaxies from us. Modern astronomy has also discovered many exotic objects such as quasars, pulsars, blazars, and radio galaxies, and has used these observations to develop physical theories which describe some of these objects in terms of equally exotic objects such as black holes and neutron stars. Physical cosmology made huge advances during the 20th century, with the model of the Big Bang heavily supported by the evidence provided by astronomy and physics, such as the cosmic microwave background radiation, Hubble's law, and cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere.
Observational astronomy.
In astronomy, the main source of information about celestial bodies and other objects is visible light or more generally electromagnetic radiation. Observational astronomy may be divided according to the observed region of the electromagnetic spectrum. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.
Radio astronomy.
Radio astronomy studies radiation with wavelengths greater than approximately one millimeter. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.
Although some radio waves are produced by astronomical objects in the form of thermal emission, most of the radio emission that is observed from Earth is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths.
A wide variety of objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.
Infrared astronomy.
Infrared astronomy is founded on the detection and analysis of infrared radiation (wavelengths longer than red light). The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. Longer infrared wavelengths can penetrate clouds of dust that block visible light, allowing the observation of young stars in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters.
With the exception of wavelengths close to visible light, infrared radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places or in space. Some molecules radiate strongly in the infrared. This allows the study the chemistry of space; more specifically it can detect water in comets.
Optical astronomy.
Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Optical images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly detectors using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.
Ultraviolet astronomy.
Ultraviolet astronomy refers to observations at ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at these wavelengths is absorbed by the Earth's atmosphere, so observations at these wavelengths must be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an appropriate adjustment of ultraviolet measurements is necessary.
X-ray astronomy.
X-ray astronomy is the study of astronomical objects at X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or spacecraft. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.
X-rays were first observed and documented in 1895 by Wilhelm Conrad Röntgen, a German scientist who found them when experimenting with vacuum tubes. Through a series of experiments, Röntgen was able to discover the beginning elements of radiation. The "X", in fact, holds its own significance, as it represents Röntgen's inability to identify exactly the type of radiation.
Gamma-ray astronomy.
Gamma ray astronomy is the study of astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not actually detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.
Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.
Fields not based on the electromagnetic spectrum.
In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth.
In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of particles which can be detected by current observatories. Additionally, some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.
Gravitational wave astronomy is an emerging new field of astronomy which aims to use gravitational wave detectors to collect observational data about compact objects. A few observatories have been constructed, such as the "Laser Interferometer Gravitational Observatory" LIGO, but gravitational waves are extremely difficult to detect.
Combining observations made using electromagnetic radiation, neutrinos or gravitational waves with those made using a different means, which shall give complementary information, is known as multi-messenger astronomy.
Astrometry and celestial mechanics.
One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars.
Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters, and potential collisions, with the Earth.
The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of radial velocity and proper motion plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of dark matter in the galaxy.
During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting nearby stars.
Theoretical astronomy.
Theoretical astronomers use several tools including analytical models (for example, polytropes to approximate the behaviors of a star) and computational numerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.
Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
Topics studied by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale structure of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole ("astro")physics and the study of gravitational waves.
Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics.
A few examples of this process:
Dark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.
Specific subfields.
Solar astronomy.
At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year fluctuation in sunspot numbers. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.
The Sun has steadily increased in luminosity over the course of its life, increasing by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.
The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona.
At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that are the outer layers that form a convection zone where the gas material transports energy primarily through physical displacement of the gas. It is believed that this convection zone creates the magnetic activity that generates sunspots.
A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. This solar wind interacts with the magnetosphere of the Earth to create the Van Allen radiation belts about the Earth, as well as the aurora where the lines of the Earth's magnetic field descend into the atmosphere.
Planetary science.
Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of this planetary system, although many new discoveries are still being made.
The Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper Belt, and finally the Oort Cloud, which may extend as far as a light-year.
The planets were formed in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided, the leading hypothesis for how the Moon was formed.
Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer surface. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.
A planet or moon's interior heat is produced from the collisions that created the body, radioactive materials ("e.g." uranium, thorium, and 26Al), or tidal heating. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.
Stellar astronomy.
The study of stars and stellar evolution is fundamental to our understanding of the universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.
Almost all elements heavier than hydrogen and helium were created inside the cores of stars.
The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it expends the hydrogen fuel in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature, so that the star both expands in size, and increases in core density. The resulting red giant enjoys a brief life span, before the helium fuel is in turn consumed. Very massive stars can also undergo a series of decreasing evolutionary phases, as they fuse increasingly heavier elements.
The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars form a white dwarf as it ejects matter that forms a planetary nebulae. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Close binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae are necessary for the distribution of metals to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.
Galactic astronomy.
Our solar system orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view.
In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at the center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.
Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.
As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.
Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.
Extragalactic astronomy.
The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies; their morphology (description) and classification; and the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos.
Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.
As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust; few star-forming regions; and generally older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies.
A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation where massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and our nearest galaxy neighbor, the Andromeda Galaxy, are spiral galaxies.
Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction.
An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material.
A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.
The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.
Cosmology.
Cosmology (from the Greek κόσμος ("kosmos") "world, universe" and λόγος ("logos") "word, study" or literally "logic") could be considered the study of the universe as a whole.
Observations of the large-scale structure of the universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the big bang, wherein our universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the big bang can be traced back to the discovery of the microwave background radiation in 1965.
In the course of this expansion, the universe underwent several evolutionary stages. In the very early moments, it is theorized that the universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early universe. (See also nucleocosmochronology.)
When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding universe then underwent a Dark Age due to the lack of stellar energy sources.
A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.
Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.
Fundamental to the structure of the universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the universe. For this reason, much effort is expended in trying to understand the physics of these components.
Interdisciplinary studies.
Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data.
The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of law and history.
Amateur astronomy.
Astronomy is one of the sciences to which amateurs can contribute the most.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschal 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them.
Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs ("e.g." the One-Mile Telescope).
Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.
Unsolved problems in astronomy.
Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics.

</doc>
<doc id="50652" url="http://en.wikipedia.org/wiki?curid=50652" title="Uniform convergence">
Uniform convergence

In the mathematical field of analysis, uniform convergence is a type of convergence stronger than pointwise convergence. A sequence {"f""n"} of functions converges uniformly to a limiting function "f" if the speed of convergence of "f""n"("x") to "f"("x") does not depend on "x".
The concept is important because several properties of the functions "f""n", such as continuity and Riemann integrability, are transferred to the limit "f" if the convergence is uniform, but not necessarily if the convergence is not.
Uniform convergence to a function on a given interval can be defined in terms of the uniform norm.
History.
In 1821 Augustin Louis Cauchy published a proof that a convergent sum of continuous functions is always continuous, to which Niels Henrik Abel in 1826 found purported counterexamples in the context of Fourier series, arguing that Cauchy's proof had to be incorrect. Completely standard notions of convergence did not exist at the time, and Cauchy handled convergence using infinitesimal methods. When put into the modern language, what Cauchy proved is that a uniformly convergent sequence of continuous functions has a continuous limit. The failure of a merely pointwise-convergent limit of continuous functions to converge to a continuous function illustrates the importance of distinguishing between different types of convergence when handling sequences of functions.
The term uniform convergence was probably first used by Christoph Gudermann, in an 1838 paper on elliptic functions, where he employed the phrase "convergence in a uniform way" when the "mode of convergence" of a series formula_1 is independent of the variables formula_2 and formula_3 While he thought it a "remarkable fact" when a series converged in this way, he did not give a formal definition, nor use the property in any of his proofs.
Later Gudermann's pupil Karl Weierstrass, who attended his course on elliptic functions in 1839–1840, coined the term "gleichmäßig konvergent" (German: "uniformly convergent") which he used in his 1841 paper "Zur Theorie der Potenzreihen", published in 1894. Independently a similar concept was used by Philipp Ludwig von Seidel and George Gabriel Stokes but without having any major impact on further development. G. H. Hardy compares the three definitions in his paper "Sir George Stokes and the concept of uniform convergence" and remarks: "Weierstrass's discovery was the earliest, and he alone fully realized its far-reaching importance as one of the fundamental ideas of analysis."
Under the influence of Weierstrass and Bernhard Riemann this concept and related questions were intensely studied at the end of the 19th century by Hermann Hankel, Paul du Bois-Reymond, Ulisse Dini, Cesare Arzelà and others.
Definition.
Suppose formula_4 is a set and formula_5 is a real-valued function for every natural number formula_6. We say that the sequence formula_7 is uniformly convergent with limit formula_8 if for every formula_9, there exists a natural number formula_10 such that for all formula_11 and all formula_12 we have formula_13.
Consider the sequence formula_14 where the supremum is taken over all formula_11. Then formula_16 converges to formula_17 uniformly if and only if formula_18 tends to 0.
The sequence formula_19 is said to be locally uniformly convergent with limit formula_17 if for every formula_21 in some metric space formula_22, there exists an formula_23 such that formula_24 converges uniformly on formula_25.
Notes.
Note that interchanging the order of "there exists "N"" and "for all "x"" in the definition above results in a statement equivalent to the pointwise convergence of the sequence. That notion can be defined as follows: the sequence ("f""n") converges pointwise with limit "f" : "S" → R if and only if 
Here the order of the universal quantifiers for "x" and for ε is not important, but the order of the former and the existential quantifier for "N" is.
In the case of uniform convergence, "N" can only depend on ε, while in the case of pointwise convergence "N" may depend on both ε and "x". It is therefore plain that uniform convergence implies pointwise convergence. The converse is not true, as the following example shows: take "S" to be the unit interval [0,1] and define for every natural number "n". Then ("f""n") converges pointwise to the function "f" defined by if "x" < 1 and . This convergence is not uniform: for instance for , there exists no "N" as required by the definition. This is because solving for "n" gives "n" > log ε / log "x". This depends on "x" as well as on ε. Also note that it is impossible to find a suitable bound for "n" that does not depend on "x" because for any nonzero value of ε, log ε / log "x" grows without bounds as "x" tends to 1.
Generalizations.
One may straightforwardly extend the concept to functions "S" → "M", where ("M", "d") is a metric space, by replacing |"f""n"("x") − "f"("x")| with "d"("f""n"("x"), "f"("x")).
The most general setting is the uniform convergence of nets of functions "S" → "X", where "X" is a uniform space. We say that the net ("f"α) "converges uniformly" with limit "f" : "S" → "X" if and only if
The above-mentioned theorem, stating that the uniform limit of continuous functions is continuous, remains correct in these settings.
Definition in a hyperreal setting.
Uniform convergence admits a simplified definition in a hyperreal setting. Thus, a sequence formula_26 converges to "f" uniformly if for all "x" in the domain of "f*" and all infinite "n", formula_27 is infinitely close to formula_28 (see microcontinuity for a similar definition of uniform continuity).
Examples.
Given a topological space "X", we can equip the space of bounded real or complex-valued functions over "X" with the uniform norm topology. Then uniform convergence simply means convergence in the uniform norm topology.
The sequence formula_29 with formula_30 converges pointwise but not uniformly:
In this example one can easily see that pointwise convergence does not preserve differentiability or continuity. While each function of the sequence is smooth, that is to say that for all "n", formula_32, the limit formula_33 is not even continuous.
Exponential function.
The series expansion of the exponential function can be shown to be uniformly convergent on any bounded subset S of formula_34 using the Weierstrass M-test.
Here is the series:
Any bounded subset is a subset of some disc formula_36 of radius R, centered on the origin in the complex plane. The Weierstrass M-test requires us to find an upper bound formula_37 on the terms of the series, with formula_37 independent of the position in the disc:
This is trivial:
If formula_42 is convergent, then the M-test asserts that the original series is uniformly convergent.
The ratio test can be used here:
which means the series over formula_37 is convergent.
Thus the original series converges uniformly for all formula_45, and since formula_46, the series is also uniformly convergent on S.
Applications.
To continuity.
If formula_47 is a real interval (or indeed any topological space), we can talk about the continuity of the functions formula_48 and formula_49. The following is the more important result about uniform convergence:
This theorem is proved by the "formula_55 trick", and is the archetypal example of this trick: to prove a given inequality (formula_56), one uses the definitions of continuity and uniform convergence to produce 3 inequalities (formula_57), and then combines them via the triangle inequality to produce the desired inequality.
This theorem is important, since pointwise convergence of continuous functions is not enough to guarantee continuity of the limit function as the image illustrates.
More precisely, this theorem states that the uniform limit of "uniformly continuous" functions is uniformly continuous; for a locally compact space, continuity is equivalent to local uniform continuity, and thus the uniform limit of continuous functions is continuous.
To differentiability.
If formula_47 is an interval and all the functions formula_48 are differentiable and converge to a limit formula_49, it is often desirable to differentiate the limit function formula_49 by taking the limit of the derivatives of formula_48. This is however in general not possible: even if the convergence is uniform, the limit function need not be differentiable, and even if it is differentiable, the derivative of the limit function need not be equal to the limit of the derivatives. Consider for instance formula_63 with uniform limit 0, but the derivatives do not approach 0. In order to ensure a connection between the limit of a sequence of differenctiable functions and the limit of the sequence of derivatives, the uniform convergence of the sequence of derivatives plus the convergence of the sequence of functions at at least one point is required. The precise statement covering this situation is as follows: 
To integrability.
Similarly, one often wants to exchange integrals and limit processes. For the Riemann integral, this can be done if uniform convergence is assumed:
Much stronger theorems in this respect, which require not much more than pointwise convergence, can be obtained if one abandons the Riemann integral and uses the Lebesgue integral instead.
To analyticity.
If a sequence of analytic functions converges uniformly in a region S of the complex plane, then the limit is analytic in S. This demonstrates an example that complex functions are more well-behaved than real functions, since the uniform limit of analytic functions on a real interval need not even be differentiable.
To series.
We say that formula_85 converges:
i) pointwise on E if and only if the sequence "s""n" converges where "s""n"(x) is the sequence of partial sums.
ii) uniformly on E if and only if "s""n"(x) converges uniformly as n goes to infinity.
iii) absolutely on E if and only if formula_86 converges for each x in E.
With this definition comes the following result:
Theorem: Let "x"0 be contained in the set E and for each "f""n" is continuous at "x""0". If f = formula_87 converges uniformly on E then f is continuous at "x""0" in E. 
Suppose that E = [a, b] and each "f""n" is integrable on [a, b]. If formula_87 converges uniformly on [a, b] then f is integrable on [a, b] and the series of integrals of "f""n" is equal to integral of the series of "f""n". This is known as term by term integration.
Almost uniform convergence.
If the domain of the functions is a measure space E then the related notion of almost uniform convergence can be defined. We say a sequence of functions formula_89 converges almost uniformly on "E" if for every formula_90 there exists a measurable set formula_91 with measure less than formula_92 such that the sequence of functions formula_89 converges uniformly on formula_94. In other words, almost uniform convergence means there are sets of arbitrarily small measure for which the sequence of functions converges uniformly on their complement. 
Note that almost uniform convergence of a sequence does not mean that the sequence converges uniformly almost everywhere as might be inferred from the name.
Egorov's theorem guarantees that on a finite measure space, a sequence of functions that converges almost everywhere also converges almost uniformly on the same set.
Almost uniform convergence implies almost everywhere convergence and convergence in measure.

</doc>
<doc id="50662" url="http://en.wikipedia.org/wiki?curid=50662" title="Norman Bates">
Norman Bates

Norman Bates is a fictional character created by writer Robert Bloch as the main character in his novel "Psycho", and portrayed by Anthony Perkins as the primary antagonist of the 1960 film of the same name directed by Alfred Hitchcock and its sequels. The character was inspired by murderer Ed Gein.
Character overview.
Both the novel and Alfred Hitchcock's 1960 film adaptation explain that Bates suffered severe emotional abuse as a child at the hands of his mother, Norma, who preached to him that sexual intercourse is sinful and that all women (except herself) are whores. After Bates' father died, Bates and his mother lived alone together until Bates reached adolescence, when his mother took a lover, Joe Considine (named Chet Rudolph in ""). Driven over the edge with jealousy, Bates murdered both of them with strychnine. After committing the murders, Bates forged a suicide note to make it look as if Norma had killed her lover and then herself. After a brief hospitalization for shock, he developed dissociative identity disorder, assuming her personality to repress his awareness of her death and to escape the feelings of guilt for murdering her. He inherited his mother's house — where he kept her corpse — and the family motel in Fairview, California.
Bloch sums up Bates' multiple personalities in his stylistic form of puns: "Norman", a child dependent on his mother; "Norma", a possessive mother who kills anyone who threatens the illusion of her existence; and "Normal", a functional adult who goes through the motions of day-to-day life. "Norma" dominates "Norman" much as she had when she was alive, forbidding him to have friends and flying into violent rages whenever he feels attracted to a woman. "Norma" and "Norman" carry on conversations through Bates talking to himself in his mother's voice, and Bates dresses in his mother's clothes whenever "Norma" takes hold completely.
In Bloch's novels.
In Bloch's novel, Mary Crane (called Marion in the film), a young woman on the run after stealing from her employer, checks into the motel one night. Bates is smitten with her, and shyly asks her to have dinner with him in the house, provoking "Mother's" jealousy; she flies into a rage and threatens to kill her if he lets her in the house. Bates defies her and eats dinner with Mary anyway, but lashes out at her when she suggests that he institutionalize his mother. When Mary goes to her room to shower, Bates spies on her through a peephole he drilled in the wall, and drinks until he passes out. While he is unconscious, "Mother" takes control and beheads Mary (she stabs her to death in the film). When Bates awakes to discover what he believes his mother has done, he sinks Mary's car — with her corpse in the trunk — into a nearby swamp. As "Mother", he also murders Milton Arbogast, a private detective hired by Mary's employer, days later.
Bates is finally caught when Mary's sister Lila and boyfriend, Sam Loomis, arrive at the hotel looking for her. When Bates figures out what they want, he knocks Sam out and goes running after Lila, who has reached the house and found Mrs. Bates' corpse. He attacks her as "Mother", but Sam overpowers him, and he is finally arrested. Bates is declared insane and sent to an institution, where "Mother" takes complete, and permanent, control of Bates' mind: he "becomes" his mother.
In Bloch's 1982 sequel to his novel, Bates escapes from the psychiatric hospital by killing a nun and donning her habit. Picked up as a hitchhiker, Bates tries to attack the driver with a tire iron, but the driver overpowers him. This in turn causes a fiery accident where the driver escapes, but Bates dies. Bates's psychiatrist, Dr. Adam Claiborne, discovers Bates' body and assumes his personality. In the next book, "Psycho House", Norman appears only as a novelty animatronic on display in the Bates Hotel, which has been converted into a tourist attraction.
Film and television sequels.
In "Psycho II", the first sequel to the original film, Bates is released from the institution 22 years after his arrest, seemingly cured, and he meets Mary Loomis — Marion Crane's niece — with whom he falls in love. However, a series of mysterious murders occurs, as well as strange appearances and messages from "Mother", and Bates slowly loses his grip on sanity. The mysterious appearances and messages turn out to be a plot by Lila Crane to drive him insane again in order to get him recommitted. The actual murders turn out to be the work of his aunt — Norma's sister, Emma Spool — who shares the family's history of mental illness and claims to be Norman's "real" mother. Before Bates discovers this, however, Mary Loomis is shot dead by the police during a confrontation with Bates, and Spool murders Lila. When Spool tells Bates that she is his mother, he kills her and embalms her body while assuming the "Mother" personality once again.
In "Psycho III", Bates continues to struggle, unsuccessfully, against "Mother"'s dominion. He also finds another love interest named Maureen Coyle, who eventually dies at "Mother"'s hand. In the film Mrs. Spool's body is first discovered by sleazy musician Duane Duke, whom Bates kills when Duke tries to use the discovery to blackmail Bates. Tracy Venable, a reporter interested in Bates' case, finds out the truth about Spool. "Mother" orders Bates to kill Venable, but in the end he attacks "Mother"'s corpse violently, attempting to break free of her control, as well as getting revenge at "Mother" for killing Maureen. He is again institutionalized. During the last few minutes of the movie, Venable tells Bates that Emma Spool was his aunt, not his mother, and had killed his father. Apparently, she had fallen for Bates' father and, when Norma Bates had given birth to Norman, kidnapped the child, believing he was her son.
"", the final film in the series, retcons the revelations of the third film, however, supplying that Bates' father was stung to death by bees and removing all references to Emma Spool. In this film, Bates has been released from an institution, and is married to one of the hospital's psychologists. When his wife becomes pregnant, however, he lures her to his mother's house and tries to kill her, wanting to prevent another of his "cursed" line from being born into the world. (The film implies that Norma Bates suffered from schizophrenia and passed the illness on to her son.) He relents at the last minute, however, when his wife professes her love for him. He then burns the house down in an attempt to free himself of his past. During the attempt, he is tormented by hallucinations of "Mother" and several of his victims. He almost dies in the flames before willing himself to get out, apparently defeating his illness at long last, while the ghost of his mother demands to be let out.
In the television movie and series pilot "Bates Motel (film)", Bates is never released from the institution after his first incarceration. He befriends Alex West, a fellow inmate who had murdered his stepfather, and wills ownership of the titular motel to him before dying of old age. This film was made before the film "". Anthony Perkins refused involvement with it.
The spinoff TV series "Bates Motel" premiered on March 18, 2013, on A&E. Set in the present day, it depicts the young Norman Bates' life with his mother before the events of the first film. This series also introduces Norman's maternal half-brother, Dylan Massett.
Characterization.
The character Norman Bates in "Psycho" was loosely based on two people. First was the real-life murderer Ed Gein, about whom Bloch later wrote a fictionalized account, "The Shambles of Ed Gein", in 1962. (The story can be found in "Crimes and Punishments: The Lost Bloch, Volume 3"). Second, it has been indicated by several people, including Noel Carter (wife of Lin Carter) and Chris Steinbrunner, as well as allegedly by Bloch himself, that Norman Bates was partly based on Calvin Beck, publisher of "Castle of Frankenstein".
The characterization of Bates in the novel and the movie differ in some key areas. In the novel, Bates is in his mid-to-late 40s, short, overweight and homely. In the movie, he is in his early-to-mid-20s, tall, slender, and handsome. Reportedly, when working on the film, Hitchcock decided that he wanted audiences to be able to sympathize with Bates and genuinely like the character, so he made him more of a "boy next door." In the novel, Bates becomes "Mother" after getting drunk and passing out; in the movie, he remains sober before switching personalities. 
In the novel, Bates is well-read in occult and esoteric authors such as P.D. Ouspensky and Aleister Crowley. He is aware that "Mother" disapproves of these authors as being against religion.
Portrayals.
Bates was portrayed by Anthony Perkins in Hitchcock's seminal 1960 film adaptation of Bloch's novel and its three sequels. Perkins hosted an episode of "Saturday Night Live" in 1976 in which he performed numerous sketches portraying Norman Bates, including the instructional video "The Norman Bates School of Motel Management." He also portrayed Norman, albeit more lightheartedly, in a 1990 commercial for Oatmeal Crisp cereal. Vince Vaughn portrayed Bates in Gus Van Sant's 1998 remake, while Kurt Paul took on the role in "Bates Motel". Henry Thomas played a younger version of the character in "". Freddie Highmore portrays a younger version of Bates in the TV series "Bates Motel".
Bates was also portrayed by Ezio Greggio in the 1994 movie "Silence of the Hams" as a would-be serial killer named Anthony Motel who runs the Cemetery Motel.
Comic books.
Norman appears in the 1992 three-issue comic book adaptation of the first "Psycho" film released by Innovation Publishing. Despite being a colorized adaptation of the Hitchcock film, the version of Norman present in the comics resembles the one from Bloch's original novel: an older, overweight, balding man. Comic artist Felipe Echevarria has explained that this was due to Perkins' refusal to allow his likeness to be replicated for the books, wanting to disassociate himself with Norman Bates.
Reception.
Norman Bates is ranked as the second greatest villain on the American Film Institute's list of the top 100 film heroes and villains, behind Hannibal Lecter and before Darth Vader. His line "A boy's best friend is his mother" also ranks as number 56 on the institute's list of the 100 greatest movie quotes.
In 2008, Norman Bates was selected by "Empire Magazine" as one of "The 100 Greatest Movie Characters". Bates also ranked number 4 on Premiere Magazine's list of "The 100 Greatest Movie Characters of All Time".

</doc>
<doc id="50666" url="http://en.wikipedia.org/wiki?curid=50666" title="Frederick, Prince of Wales">
Frederick, Prince of Wales

Frederick Louis, Prince of Wales (1 February 1707 – 31 March 1751) was heir apparent to the British throne from 1727 until his death. He was the eldest but estranged son of King George II and Caroline of Ansbach, as well as the father of King George III.
Under the Act of Settlement passed by the English Parliament in 1701, Frederick was high in line of succession to the British throne. He moved to Great Britain following the accession of his father, and was created Prince of Wales. He predeceased his father, however, and upon the latter's death on 25 October 1760, the throne passed to Prince Frederick's eldest son, George III.
Early life.
Prince Frederick Louis was born on 1 February 1707 in Hanover, Germany, as Duke Friedrich Ludwig of Brunswick-Lüneburg, to Prince George, son of George, Elector of Hanover, who was also one of Frederick's two godfathers. The Elector was the son of Sophia of Hanover, granddaughter of James VI and I and first cousin and heiress-presumptive to the English Queen Anne. However, Sophia died before Anne at age 83 in June 1714, which elevated the Elector to heir-presumptive; Queen Anne died on August 1 of the same year, and Sophia's son became King George I. This made Frederick's father the new Prince of Wales and first-in-line to the British throne and Frederick himself second-in-line. Frederick's other godfather was his grand-uncle Frederick I, King in Prussia and Elector of Brandenburg-Prussia. Frederick was nicknamed "Griff" within the family.
In the year of Anne's death and the coronation of George I, Frederick's parents, George, Prince of Wales (later George II), and Caroline of Ansbach, were called upon to leave Hanover for Great Britain when their eldest son was only seven years old. He was left in the care of his grand-uncle Ernest Augustus, Prince-Bishop of Osnabrück, and did not see his parents again for 14 years.
In 1722, Frederick was inoculated against smallpox by Charles Maitland on the instructions of his mother Caroline. His grandfather, George I, created him Duke of Edinburgh, Marquess of the Isle of Ely, Earl of Eltham in the county of Kent, Viscount of Launceston in the county of Cornwall, and Baron of Snaudon in the county of Carnarvon, on 26 July 1726. The latter two titles have been interpreted differently since – the "of"s are omitted and "Snaudon" rendered as "Snowdon".
Frederick arrived in England in 1728 as a grown man, the year after his father had become King George II. By then, George and Caroline had had several younger children, and Frederick, himself now Prince of Wales, was a high-spirited youth fond of drinking, gambling and women. The long separation damaged their relationship, and they would never be close.
Prince of Wales.
The motives for the ill-feeling between Frederick and his parents may include the fact that he had been set up by his grandfather, even as a small child, as the representative of the House of Hanover, and was used to presiding over official occasions in the absence of his parents. He was not permitted to go to Great Britain until after his father took the throne as George II on 11 June 1727. Frederick had continued to be known as Prince Friedrich Ludwig of Hanover (with his British HRH style) even after his father had been created Prince of Wales.
In 1728, Frederick (his name now anglicised) was finally brought to Britain and was created Prince of Wales on 8 January 1729. He served as the tenth Chancellor of the University of Dublin from 1728 to 1751, and a portrait of the him still enjoys a commanding position in the Hall of the Trinity College, Dublin.
He sponsored a court of 'opposition' politicians. Frederick and his group supported the Opera of the Nobility in Lincoln's Inn Fields as a rival to Handel's royally-sponsored opera at the King's Theatre in the Haymarket. Frederick was a lover of music who played the viola and cello; he is depicted playing a cello in a portrait by Philip Mercier of Frederick and his sisters, now part of the National Portrait Gallery collection. He enjoyed the natural sciences and the arts, and became a thorn in the side of his parents, making a point of opposing them in everything, according to the court gossip Lord Hervey. At court, the favourite was Frederick's younger brother, Prince William, Duke of Cumberland, to the extent that the king looked into ways of splitting his domains so that Frederick would succeed only in Britain, while Hanover would go to William.
Hervey and Frederick (using a pseudonym "Captain Bodkin") wrote a theatrical comedy together which was staged at the Drury Lane Theatre in October 1731. It was panned by the critics, and even the theatre's manager thought it so bad that it was unlikely to play out even the first night. He had soldiers stationed in the audience to maintain order, and when the play flopped the audience was given their money back. Hervey and Frederick also shared a mistress, Anne Vane, who had a son called FitzFrederick Vane in June 1732. Either of them or William Stanhope, 1st Earl of Harrington, another of her lovers, could have been the father. Jealousy between them may have contributed to a breach, and their friendship ended. Hervey later wrote bitterly that Frederick was "false ... never having the least hesitation in telling any lie that served his present purpose."
Patron of the arts.
A permanent result of Frederick's patronage of the arts is "Rule, Britannia!", one of the best-known British patriotic songs. It was composed by the English composer Thomas Arne and written by the Scottish poet and playwright James Thomson as part of the masque "Alfred" which was first performed on 1 August 1740 at Cliveden, the country home of the Prince and Princess of Wales. Thomas Arne was also one of Frederick's favourite artists. A masque linking the Prince with both the ancient hero-king Alfred the Great's victories over the Vikings and with the contemporary issue of building up the British sea power obviously went well with Frederick's political plans and aspirations. Later the song got a life of its own regardless of the masque. Thomson, who supported the Prince of Wales politically, also dedicated to him an earlier major work, "Liberty" (1734).
Unlike the king, Frederick was a knowledgeable amateur of painting, who patronised immigrant artists like Jacopo Amigoni and Jean Baptiste Vanloo, who painted the portraits of the prince and his consort for Frederick's champion William Pulteney, 1st Earl of Bath. The list of other artists he employed—Philip Mercier, John Wootton, George Knapton and the French engraver Joseph Goupy—represents some of the principal figures of the English Rococo. William Kent's neo-Palladian state barge of 1732 is still preserved, though Sir William Chambers' palace at Kew for his widow Augusta (1757) was demolished in 1802.
Domestic life.
Negotiations between George II and his brother-in-law Frederick William I of Prussia on a proposed marriage between the Prince of Wales and Frederick William's daughter Wilhelmine were welcomed by Frederick even though the couple had never met. George II was not keen on the proposal but continued talks for diplomatic reasons. Frustrated by the delay, Frederick sent an envoy of his own to the Prussian court. When the King discovered the plan, he immediately arranged for Frederick to leave Hanover for England. The marriage negotiations foundered when Frederick William demanded that Frederick be made Regent in Hanover.
Frederick also almost married Lady Diana Spencer, daughter of Charles Spencer, 3rd Earl of Sunderland and Lady Anne Churchill. Lady Diana was the favourite grandchild of the powerful Sarah, Duchess of Marlborough. The duchess sought for a royal alliance by marrying Lady Diana with the Prince of Wales with a massive dowry of £100,000. The prince who was in great debt also agreed to the proposal but the plan was vetoed by Robert Walpole and the king. Lady Diana soon married John Russell, 4th Duke of Bedford.
Although in his youth he was undoubtedly a spendthrift and womaniser, Frederick settled down following his marriage to the seventeen-year-old Augusta of Saxe-Gotha in 1736. The wedding was held at St James's Palace and was presided over by the Bishop of London.
In May 1736, George II returned to Hanover, which resulted in unpopularity in England; a satirical notice was even pinned to the gates of St James's Palace decrying his absence. "Lost or strayed out of this house", it read, "a man who has left a wife and six children on the parish." The King made plans to return in the face of inclement weather; when his ship was caught in a storm, gossip swept London that he had drowned. Eventually, in January 1737, he arrived back in England. Immediately he fell ill, with piles and a fever, and withdrew to his bed. The Prince of Wales put it about that the King was dying, with the result that George insisted on getting up and attending a social event to disprove the gossip-mongers.
Quickly accumulating large debts, Frederick relied for an income on his wealthy friend, George Bubb Dodington. The Prince's father refused to make him the financial allowance that the Prince considered should have been his.
Frederick's public opposition to his father's government continued; he opposed the unpopular Gin Act 1736, which tried to control the Gin Craze. Frederick applied to Parliament for an increased financial allowance which had hitherto been denied him by the King, and public disagreement over the payment of the money drove a further wedge between parents and son. Frederick's allowance was raised but by less than he had asked for.
In June 1737, Frederick informed his parents that Augusta was pregnant, and due to give birth in October. In fact, Augusta's due date was earlier and a peculiar episode followed in July in which the Prince, on discovering that his wife had gone into labour, sneaked her out of Hampton Court Palace in the middle of the night, to ensure that the King and Queen could not be present at the birth. George and Caroline were horrified. Traditionally, royal births were witnessed by members of the family and senior courtiers to guard against supposititious children, and Augusta had been forced by her husband to ride in a rattling carriage while heavily pregnant and in pain. With a party including two of her daughters and Lord Hervey, the Queen raced over to St James's Palace, where Frederick had taken Augusta. Caroline was relieved to discover that Augusta had given birth to a "poor, ugly little she-mouse" rather than a "large, fat, healthy boy" which made a supposititious child unlikely since the baby was so pitiful. The circumstances of the birth deepened the estrangement between mother and son.
Frederick was banished from the King's court, and a rival court grew up at Frederick's new residence, Leicester House. His mother fell fatally ill at the end of the year, but the King refused Frederick permission to see her. He became a devoted family man, taking his wife and eight children (his youngest daughter was born posthumously) to live in the countryside at Cliveden, where he fished, shot and rowed. In 1742, Robert Walpole left office and the realignment of the government led to a reconciliation between father and son, as Frederick's friends gained influence.
After the Jacobite Rising of 1745, Frederick met Flora MacDonald, who had been imprisoned in the Tower of London for aiding the escape of the Rising's leader Charles Edward Stuart. He helped in securing her eventual release. In 1747, Frederick rejoined the political opposition, and the King responded by calling an early general election, which Frederick's party lost.
Cricket.
By the time Frederick arrived in Great Britain, cricket had developed into the country's most popular team sport and it thrived on gambling. Perhaps because he wished to Anglicise and so fit in with his new society, Frederick developed an academic interest in cricket and soon became a genuine enthusiast. He began to make wagers and then to patronise and play the sport, even forming his own team on several occasions.
The earliest mention of Frederick in cricket annals is in a contemporary report that concerns a major match on 28 September 1731 between Surrey and London, played on Kennington Common. No post-match report was found despite advance promotion as "likely to be the best performance of this kind that has been seen for some time". The records show that "for the convenience of the gamesters, the ground is to be staked and roped out" – a new practice in 1731 and possibly done partly for the benefit of a royal visitor. The advertisement refers to "the whole county of Surrey" as London's opponents and states that the Prince of Wales is "expected to attend".
In August 1732, the "Whitehall Evening Post" reported that Frederick attended "a great cricket match" at Kew on Thursday 27 July.
By the 1733 season, Frederick was seriously involved in the game, in effect as a county cricketer for Surrey. He was said to have given a guinea to each player in a Surrey v Middlesex game at Moulsey Hurst. Then he awarded a silver cup to a combined Surrey & Middlesex team which had just beaten Kent, arguably the best county team at the time, at Moulsey Hurst on Wed 1 August. This is the first reference in cricket history to any kind of trophy (other than hard cash) being contested. On Friday 31 August, the Prince of Wales' XI played Sir William Gage's XI on Moulsey Hurst. The result is unknown but the teams were said to be of county standard, so presumably it was in effect a Surrey v Sussex match.
In the years following 1733, there are frequent references to the Prince of Wales as a patron of cricket and as an occasional player.
When he died on 20 March 1751, cricket suffered a double blow as his death closely followed that of Charles Lennox, 2nd Duke of Richmond, the game's greatest financial patron at the time. Accordingly the number of top-class matches declined for several years, although economic difficulties and priorities from the wars of the period certainly inhibited many potential investors.
Death and legacy.
His political ambitions unfulfilled, Frederick died at Cliveden House at the age of 44 in 1751 from a burst abscess in the lung. The abscess has been commonly attributed to a blow by a cricket or a real tennis ball, but this is unproven. He was buried at Westminster Abbey on 13 April 1751.
Fredericksburg, Virginia; Prince Frederick, Maryland; and Fort Frederica, Georgia, USA, were named after him.
The Prince of Wales' epigram (quoted by William Makepeace Thackeray, "Four Georges"):
Titles, styles, honours and arms.
Titles and styles.
In Britain:
He was given the title Duke of Gloucester on 10 January 1717.
Arms.
Between his creation as Duke of Edinburgh in 1726 and his creation as Prince of Wales, he bore the arms of the kingdom, differentiated by a "label argent of three points, the centre point bearing a cross gules". As Prince of Wales, the difference changed to simply a "label argent of three points". Frederick never succeeded his father as Treasurer of the Holy Roman Empire and so the red escutcheon in the center of his Hanover quarter is empty.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="50670" url="http://en.wikipedia.org/wiki?curid=50670" title="Marcy Playground">
Marcy Playground

Marcy Playground is an American alternative rock band consisting of three members: John Wozniak (lead vocals, guitar), Dylan Keefe (bass), and Shlomi Lavie (drums). The band is best known for their 1997 hit "Sex and Candy".
History.
Early years.
The band is named after the Marcy Open grade school in Minneapolis, which is the alternative school John Wozniak attended. He chose the name because many of his songs were inspired by his childhood. Marcy Playground emerged in the late 1990s. Influences include David Bowie, Paul Simon, Neil Young, Van Morrison, Jimi Hendrix, Syd Barrett of Pink Floyd, Nirvana, Wham! and the Beatles. The influences are quite clear on Marcy Playground's self-titled album, with songs like "Shadow Of Seattle" and "Saint Joe On The School Bus". Frontman John Wozniak's first effort, "Zog BogBean – From the Marcy Playground", was self-produced, recorded in his bedroom studio with some help from his then-girlfriend Sherry Fraser and her brother Scott in the early nineties. A limited run of CDs were self-released by Wozniak. "Our Generation" and "Dog and His Master", two songs found on Wozniak's "Zog BogBean" project, would appear on later Marcy Playground albums. As of April 2009, "Zog BogBean" is available for download at Marcy Playground's official site as well as other outlets such as iTunes.
"Marcy Playground" (1997–1998).
After attending the Evergreen State College for two years, Wozniak moved east to New York, in order to work with multi-instrumentalist, Jared Kotler. who John had known from suburban Philadelphia. Jared believed in John's songwriting talent and put together money with his cousin Jeff White in order to pay for the duo to record at Sabella Recording Studios in nearby Roslyn, NY. After hearing the two records worth of material John had recorded of John's songs, Capitol Records became interested in the music. Kurt Rosenwinkel, a mutual friend, introduced bassist Dylan Keefe to John Wozniak and Marcy Playground began performing in NYC. The band was signed to Capitol in 1995 and they performed a series of NYC club dates that would be the first incarnation of Marcy Playground as a band. Personal problems between John and Jared reached a peak after a year of playing the New York music scene and drummer Dan Rieser was brought in to replace Jared. The self-titled album was released in 1997, with the first single, "Poppies", released soon after. Marcy Playground emerged into the mainstream with the success of the single "Sex and Candy". The song spent 15 weeks at No. 1 on the Billboard Modern Rock Tracks chart. The album went platinum and managed to spawn two other singles, "Saint Joe on the School Bus" and "Sherry Fraser". "Marcy Playground" is quiet and minimalist in tone. Wozniak's songs run in different styles: some are modern folk music; many have undertones reminiscent of children's songs; the blurred sound of psychedelia makes appearances; and then there are the songs with a clear rock sound.
"Shapeshifter" (1999–2000).
Marcy Playground's next outing was 1999's "Shapeshifter". Shapeshifter was released on November 2, 1999.
A minor controversy came to light when Paul Leary of the Butthole Surfers revealed on Marcy Playground's website forum that the cover art for "Shapeshifter" had originally been conceptualized and commissioned by Leary for his band's aborted "After the Astronaut" album. He admitted to being a fan of Marcy Playground and Leary's outrage was eased once he learned that Capitol Records, former home of the Butthole Surfers and then-current home of Marcy Playground, had pitched the artwork to Wozniak as original work from its own art department and that Marcy Playground had no knowledge of the work's origins. Upon learning the truth, Wozniak proclaimed that he was "honored" to have an album cover designed by Paul Leary. "It's Saturday", the album's lead single, managed to hit No. 23 on the Modern Rock Charts, and followed by a second single, "Bye Bye".
After "Shapeshifter," drummer Dan Rieser left the band to pursue other interests. The position was eventually filled on his recommendation by Gonzalo "Gonz" Martinez de la Cotera, a friend whose previous band Lincoln had opened for Marcy Playground.
"MP3" (2004–2006).
After a considerable hiatus; Marcy Playground recorded a follow-up to "Shapeshifter". Marcy Playground's third album, "MP3," was released in 2004.
Marcy Playground did some touring in support of "MP3". The album's first single, "Deadly Handsome Man" was a song featured on the "Jay and Silent Bob Strike Back" movie soundtrack a few years earlier under its original title, "The Devil's Song". The second single, "Punk Rock Superstar" surprised many fans while bringing in some new ones, when it was featured on the Xbox 360's playlist upon its re-release in 2006. Other singles from the album include "Blood in Alphabet Soup" and "No One's Boy", both were featured on promotional ads for "MP3". One album track, "Paper Dolls", was co-written by Jimi Haha of the alternative rock band Jimmie's Chicken Shack.
On the track titled, "Hotter Than the Sun", Wozniak reflects on the band's one-time success and remains positive about the band's future.
"Leaving Wonderland...in a fit of rage" (2009).
Marcy Playground's fourth album, "Leaving Wonderland...in a fit of rage", was released on July 7, 2009. Originally conceived as a solo record by John Wozniak, he brought Dylan Keefe into the project and the record was released under the band's name. "Leaving Wonderland" features 12 songs, including the singles "Good Times" and "Blackbird". Also included on the record is a version of "Memphis", a Marcy Playground b-side that Wozniak's wife felt deserved a second breath of life. Sherry Fraser, Wozniak's longtime friend and lead vocalist of Two Ton Boa, designed the album cover. "Leaving Wonderland" was produced, recorded and mixed by producer Jeff Dawson.
After Gonzalo Martinez's departure from the band, family friend Shlomi Lavie stepped in as drummer for the band's 2009 tour. After a successful 4-day Carnival Cruise show in the Bahamas alongside Post-grunge band Sponge, it was announced that the band would tour for the Spring of 2009, playing in venues along the California coastline, including the Key Club, as well as cities in Canada and North Carolina.
"Indaba Remixes from Wonderland" (2010-2011).
Marcy Playground's fifth album, "Indaba Remixes from Wonderland," was released on September 28, 2010. "Indaba Remixes from Wonderland" is a new CD and digital collection of remixes from Marcy Playground's 2009 critically acclaimed album "Leaving Wonderland...in a fit of rage". Marcy Playground partnered with Indaba Music to offer fans the opportunity to remix songs from the "Leaving Wonderland" album. Marcy Playground then selected its favorite mixes from hundreds submitted to be included on "Indaba". This is the first time Marcy Playground had an album produced exclusively from fan submitted remixes to Indaba Music in which the winners will receive royalties from album sales.
Marcy Playground lyricist/vocalist/guitarist John Wozniak stated: "I wanted to make this record to show people what the online music community is capable of. I believe the talent reflected in these tracks speaks for itself. This community is thriving. It's thriving because musicians feed on inspiration they get from other musicians. Music is not meant to be a solitary endeavor; it's something we like to do with other people."
Marcy Playground was to tour many venues in support of the album through 2010.
"Lunch, Recess & Detention" (2012).
Marcy Playground announced in mid-June 2012 that they will be releasing a compilation of rarities, b-sides, and new material on July 17, 2012 entitled "Lunch, Recess & Detention". The album’s first single, "Mr. Fisher", was released on June 26, 2012. Marcy Playground joined Everclear, Sugar Ray, Lit, and the Gin Blossoms on the Summerland Tour 2012.
Band members.
Current:
Past:

</doc>
<doc id="50674" url="http://en.wikipedia.org/wiki?curid=50674" title="Kill Rock Stars">
Kill Rock Stars

Kill Rock Stars is an independent record label founded in 1991 by Slim Moon and Tinuviel Sampson, and based in both Olympia, Washington and Portland, Oregon. The label has released a variety of work in different genres, making it difficult to pigeonhole as having any one artistic mission. Overall, though, the political sensibilities of the label can be said to be left-wing, feminist, and anti-war, and the label initially showed a commitment towards underground punk bands and to representing artists in the Olympia area music scene.
History.
Sampson and Moon initially started the label because in his words, "I just wanted to put out my friends’ records because nobody was putting out my friends’ records. And to put out spoken word 7" records." KRS-101 (the label's first release) was in fact a split 7" spoken-word record with Kathleen Hanna and Slim Moon; other "Wordcore" releases followed. The first major release was a compilation of Olympia-area bands simply titled "Kill Rock Stars" ("Stars Kill Rock" and "Rock Stars Kill" would follow in the same compilation series) and featured Bikini Kill, Bratmobile, Unwound, Nirvana, The Melvins, as well as singer-songwriter Elliott Smith. 
Although the label's music has never reflected just a single genre or underground music movement, it is arguably most notable for releasing the work of various riot grrrl bands during the mid-'90s, some of which, especially Bikini Kill, generated a good deal of press attention. Other Kill Rock Stars releases in this genre includes albums by Bratmobile, Huggy Bear, Heavens to Betsy and Excuse 17.
The label continued its tradition of spoken word by releasing their first full-length spoken word LP Big Broad by Juliana Lueking in 1995. This was also the year that Elliott Smith released his self-titled solo LP on the label. Another milestone was the 1997 release of Sleater-Kinney's third LP (and first on Kill Rock Stars) "Dig Me Out", which garnered national press attention in "Spin" and "Rolling Stone" magazines.
In 1997/98, the 5RC label was formed as a sister label to Kill Rock Stars; it released generally harsher-sounding and more challenging experimental rock than Kill Rock Stars. The 5RC roster includes Xiu Xiu, Deerhoof, Need New Body, The Mae Shi, The Robot Ate Me, and Metalux among others. 1998 also marked the first-ever Mailorder Freak Singles Club and featured Quasi, Small Stars, Sta-Prest and Rock*A*Teens among others.
Another popular band on Kill Rock Stars was The Decemberists, who released three full-length albums on the label between 2001 and 2005. Colin Meloy, singer for The Decemberists, also released a solo album on the label in April 2008. Other notable releases by Kill Rock Stars include albums by bands such as The Paper Chase, Jeff Hanson, Unwound, Marnie Stern, Gossip, Mecca Normal, Two Ton Boa and Comet Gain; spoken word albums by Kathy Acker and Miranda July; and reissues of work by earlier punk/post-punk bands such as Kleenex/Liliput, Essential Logic, and Delta 5. 
In October 2006 Slim Moon, the owner, announced he would be departing Kill Rock Stars to work as an A&R representative at Nonesuch Records, a Warner Music Group subsidiary. Slim's wife Portia Sabin took over ownership of Kill Rock Stars and in 2007 the label released 11 records, including "New Moon", a collection of songs recorded by Elliott Smith between 1994 and 1997.

</doc>
<doc id="50677" url="http://en.wikipedia.org/wiki?curid=50677" title="Laser printing">
Laser printing

Laser printing is an electrostatic digital printing process. It produces high-quality text and graphics (and moderate-quality photographs) by repeatedly passing a laser beam back and forth over a negatively charged cylindrical drum to define a differentially-charged image. The drum then selectively collects electrically charged powdered ink (toner), and transfers the image to paper, which is then heated in order to permanently fuse the text and/or imagery. As with digital photocopiers and multifunction/all-in-one inkjet printers, laser printers employ a xerographic printing process. However, laser printing differs from analog photocopiers in that the image is produced by the direct scanning of the medium across the printer's photoreceptor. This enables laser printing to copy images more quickly than most photocopiers.
History.
In the 1960s, the Xerox Corporation held a dominant position in the photocopier market. In 1969, Gary Starkweather, who worked in Xerox's product development department, had the idea of using a laser beam to 'draw' an image of what was to be copied directly onto the copier drum. After transferring to the recently formed Palo Alto Research Center (Xerox PARC) in 1971, Starkweather adapted a Xerox 7000 copier to create SLOT (Scanned Laser Output Terminal). In 1972, Starkweather worked with Butler Lampson and Ronald Rider to add a control system and character generator, resulting in a printer called EARS (Ethernet, Alto Research character generator, Scanned laser output terminal) -- which later became the Xerox 9700 laser printer.
The first commercial implementation of a laser printer was the IBM 3800 in 1976. It was designed for data centers, where it replaced line printers attached to mainframe computers. The IBM 3800 was used for high-volume printing on continuous stationery, and achieved speeds of 215 pages per minute (ppm), at a resolution of 240 dots per inch (dpi). Over 8,000 of these printers were sold. The Xerox 9700 was brought to market in 1977. Unlike the IBM 3800, the Xerox 9700 was not targeted to replace any particular existing printers; but, it did have limited support for the loading of fonts. The Xerox 9700 excelled at printing high-value documents on cut-sheet paper with varying content (e.g., insurance policies).
In 1979, inspired by the Xerox 9700's commercial success, Japanese camera and optics company, Canon, developed a low-cost, "desktop" laser printer: the Canon LBP-10. Canon then began work on a much-improved print engine, the Canon CX, resulting in the LBP-CX printer. Lacking experience in selling to computer users, Canon sought partnerships with three Silicon Valley companies: Diablo Data Systems (who turned them down), Hewlett-Packard (HP), and Apple Computer.
The first laser printer designed for office use reached market in 1981: the Xerox Star 8010. The system used a desktop metaphor that was unsurpassed in commercial sales, until the Apple Macintosh. Although it was innovative, the Star workstation was a prohibitively expensive (US$) system, affordable only to a fraction of the businesses and institutions at which it was targeted.
The first laser printer intended for mass-market sales was the HP LaserJet, released in 1984; it used the Canon CX engine, controlled by HP software. The LaserJet was quickly followed by printers from Brother Industries, IBM, and others. First-generation machines had large photosensitive drums, of circumference greater than the loaded paper's length. Once faster-recovery coatings were developed, the drums could touch the paper multiple times in a pass, and therefore be smaller in diameter.
In 1985, Apple introduced the LaserWriter (also based on the Canon CX engine), but used the newly released PostScript page-description language. Up until this point, each manufacturer used its own page-description language, making the supporting software complex and expensive. PostScript allowed the use of text, fonts, graphics, images, and color largely independent of the printer's brand or resolution. PageMaker, written by Aldus for the Macintosh and LaserWriter, was also released in 1985 and the combination became very popular for desktop publishing.:13/23:364 Laser printers brought exceptionally fast and high-quality text printing, with multiple fonts on a page, to the business and consumer markets. No other commonly-available printer during this era could also offer this combination of features.
Printing process.
A laser beam (typically, an aluminium gallium arsenide semiconductor laser) projects an image of the page to be printed onto an electrically-charged, selenium-coated, rotating, cylindrical drum (or, more commonly in subsequent versions, organic photoconductors). Photoconductivity allows the charged electrons to fall away from the areas exposed to light. Powdered ink (toner) particles are then electrostatically attracted to the charged areas of the drum that have not been laser-beamed. The drum then transfers the image onto paper (which is passed through the machine) by direct contact. Finally the paper is passed onto a finisher, which uses intense heat to instantly fuse the toner/image onto the paper.
There are typically seven steps involved in the process:
Raster image processing.
The document to be printed is encoded in a page description language such as PostScript, Printer Command Language (PCL), or Open XML Paper Specification (OpenXPS). The raster image processor converts the page description into a bitmap which is stored in the printer's raster memory. Each horizontal strip of dots across the page is known as a raster line or scan line. 
Laser printing differs from other printing technologies in that each page is always rendered in a single continuous process without any pausing in the middle, while other technologies like inkjet can pause every few lines. To avoid a buffer underrun (where the laser reaches a point on the page before it has the dots to draw there), a laser printer typically needs enough raster memory to hold the bitmap image of an entire page. 
Memory requirements increase with the square of the dots per inch, so 600 dpi requires a minimum of 4 megabytes for monochrome, and 16 megabytes for color at 600 dpi. For fully graphical output using a page description language, a minimum of 1 megabyte of memory is needed to store an entire monochrome letter/A4 sized page of dots at 300 dpi. At 300 dpi, there are 90,000 dots per square inch (300 dots per linear inch). A typical 8.5 × 11 sheet of paper has 0.25 in margins, reducing the printable area to 8.0 x, or 84 square inches. 84 sq/in × 90,000 dots per sq/in = 7,560,000 dots. 1 megabyte = 1,048,576 bytes, or 8,388,608 bits, which is just large enough to hold the entire page at 300 dpi, leaving about 100 kilobytes to spare for use by the raster image processor. 
In a color printer, each of the four CMYK toner layers is stored as a separate bitmap, and all four layers are typically preprocessed before printing begins, so a minimum of 4 megabytes is needed for a full-color letter-size page at 300 dpi.
During the 1980s, memory chips were still very expensive, which is why entry-level laser printers in that era always came with four-digit suggested retail prices in U.S. dollars. Memory prices later plunged, and 1200 dpi printers have been widely available in the consumer market since 2008. 2400 dpi electrophotographic printing plate makers, essentially laser printers that print on plastic sheets, are also available.
Charging.
In older printers, a corona wire positioned parallel to the drum, or in more recent printers, a primary charge roller, projects an electrostatic charge onto the photoreceptor (otherwise named the photo conductor unit), a revolving photosensitive drum or belt, which is capable of holding an electrostatic charge on its surface while it is in the dark.
An AC bias is applied to the primary charge roller to remove any residual charges left by previous images. The roller will also apply a DC bias on the drum surface to ensure a uniform negative potential.
Numerous patents describe the photosensitive drum coating as a silicon sandwich with a photocharging layer, a charge leakage barrier layer, as well as a surface layer. One version uses amorphous silicon containing hydrogen as the light receiving layer, Boron nitride as a charge leakage barrier layer, as well as a surface layer of doped silicon, notably silicon with oxygen or nitrogen which at sufficient concentration resembles machining silicon nitride.
Exposing.
The laser is aimed at a rotating polygonal mirror, which directs the laser beam through a system of lenses and mirrors onto the photoreceptor. The cylinder continues to rotate during the sweep and the angle of sweep compensates for this motion. The stream of rasterized data held in memory turns the laser on and off to form the dots on the cylinder. Lasers are used because they generate a narrow beam over great distances. The laser beam neutralizes (or reverses) the charge on the black parts of the image, leaving a static electric negative image on the photoreceptor surface to lift the toner particles.
Some non-laser printers ("LED printers") expose by an array of light emitting diodes spanning the width of the page, rather than by a laser ("exposing" is also known as "writing" in some documentation).
Developing.
The surface with the latent image is exposed to toner, fine particles of dry plastic powder mixed with carbon black or coloring agents. The toner particles are given a negative charge, and are electrostatically attracted to the photoreceptor's latent image, the areas touched by the laser. Because like charges repel, the negatively charged toner will not touch the drum where the negative charge remains.
Transferring.
The photoreceptor is pressed or rolled over paper, transferring the image. Higher-end machines use a positively charged transfer roller on the back side of the paper to pull the toner from the photoreceptor to the paper.
Fusing.
The paper passes through rollers in the fuser assembly where heat of up to 200 C and pressure bond the plastic powder to the paper.
One roller is usually a hollow tube (heat roller) and the other is a rubber backing roller (pressure roller). A radiant heat lamp is suspended in the center of the hollow tube, and its infrared energy uniformly heats the roller from the inside. For proper bonding of the toner, the fuser roller must be uniformly hot.
Some printers use a very thin flexible metal fuser roller, so there is less mass to be heated and the fuser can more quickly reach operating temperature. If paper moves through the fuser more slowly, there is more roller contact time for the toner to melt, and the fuser can operate at a lower temperature. Smaller, inexpensive laser printers typically print slowly, due to this energy-saving design, compared to large high speed printers where paper moves more rapidly through a high-temperature fuser with a very short contact time.
Cleaning.
When the print is complete, an electrically neutral soft plastic blade cleans any excess toner from the photoreceptor and deposits it into a waste reservoir, and a discharge lamp removes the remaining charge from the photoreceptor.
Toner may occasionally be left on the photoreceptor when unexpected events such as a paper jam occur. The toner is on the photoconductor ready to apply, but the operation failed before it could be applied. The toner must be wiped off and the process restarted.
Multiple steps occurring at once.
Once the raster image generation is complete all steps of the printing process can occur one after the other in rapid succession. This permits the use of a very small and compact unit, where the photoreceptor is charged, rotates a few degrees and is scanned, rotates a few more degrees and is developed, and so forth. The entire process can be completed before the drum completes one revolution.
Different printers implement these steps in distinct ways. LED printers actually use a linear array of light-emitting diodes to "write" the light on the drum. The toner is based on either wax or plastic, so that when the paper passes through the fuser assembly, the particles of toner melt. The paper may or may not be oppositely charged. The fuser can be an infrared oven, a heated pressure roller, or (on some very fast, expensive printers) a xenon flash lamp. The warmup process that a laser printer goes through when power is initially applied to the printer consists mainly of heating the fuser element.
Performance.
As with most electronic devices, the cost of laser printers has fallen markedly over the years. In 1984, the HP LaserJet sold for $3500, had trouble with even small, low resolution graphics, and weighed 32 kg. s of 2008[ [update]], low-end monochrome laser printers often sell for less than $75. These printers tend to lack onboard processing and rely on the host computer to generate a raster image, but outperform the 1984 LaserJet in nearly all situations.
Laser printer speed can vary widely, and depends on many factors, including the graphic intensity of the job being processed. The fastest models can print over 200 monochrome pages per minute (12,000 pages per hour). The fastest color laser printers can print over 100 pages per minute (6000 pages per hour). Very high-speed laser printers are used for mass mailings of personalized documents, such as credit card or utility bills, and are competing with lithography in some commercial applications.
The cost of this technology depends on a combination of factors, including the cost of paper, toner, drum replacement, as well as the replacement of other items such as the fuser assembly and transfer assembly. Often printers with soft plastic drums can have a very high cost of ownership that does not become apparent until the drum requires replacement.
Duplex printing (printing on both sides of the paper) can halve paper costs and reduce filing volumes. Formerly only available on high-end printers, duplexers are now common on mid-range office printers, though not all printers can accommodate a duplexing unit. Duplexing can also give a slower page-printing speed, because of the longer paper path.
Color laser printers.
Color laser printers use colored toner (dry ink), typically cyan, magenta, yellow, and black (CMYK). While monochrome printers only use one laser scanner assembly, color printers often have two or more.
Color printing adds complexity to the printing process because very slight misalignments known as registration errors can occur between printing each color, causing unintended color fringing, blurring, or light/dark streaking along the edges of colored regions. To permit a high registration accuracy, some color laser printers use a large rotating belt called a "transfer belt". The transfer belt passes in front of all the toner cartridges and each of the toner layers are precisely applied to the belt. The combined layers are then applied to the paper in a uniform single step.
Color printers usually have a higher cost per page than monochrome printers (even if printing monochrome-only pages).
Comparison with inkjet printers.
Manufacturers use a similar business model for both low-cost color laser printers and inkjet printers: the printers are sold cheaply while replacement toners and inks are relatively expensive. Color laser printers are much quicker than inkjet printers and their running cost per page is usually slightly less. The print quality of color lasers is limited by their resolution, typically 600–1200 dpi, and their use of just four color toners. They often have trouble printing large areas of the same or gradually changing color. Inkjet printers designed for printing photos can produce much higher quality color images.
Anti-counterfeiting marks.
Many modern color laser printers mark printouts by a nearly invisible dot raster, for the purpose of identification.
The dots are yellow and about 0.1 mm in size, with a raster of about 1 mm. This is purportedly the result of a deal between the U.S. government and printer manufacturers to help track counterfeiters.
The dots encode data such as printing date, time, and printer serial number in binary-coded decimal on every sheet of paper printed, which allows pieces of paper to be traced by the manufacturer to identify the place of purchase, and sometimes the buyer.
Digital rights advocacy groups such as the Electronic Frontier Foundation are concerned about this erosion of the privacy and anonymity of those who print.
Smart chips in toner cartridges.
Similar to inkjet printers, toner cartridges may contain smart chips that reduce the number of pages that can be printed with it (reducing the amount of usable ink in the cartridge to sometimes only 50%), in an effort to increase sales of the toner cartridges. Besides being more expensive to the consumer, this technique also increases waste, and thus increases pressure on the environment. For these toner cartridges (as with inkjet cartridges), reset devices can be used to override the limitation set by the smart chip. Also, for some printers, online walk-throughs have been posted to demonstrate how to use up all the ink in the cartridge.
Safety hazards, health risks, and precautions.
Toner clean-up.
Toner particles are designed to have electrostatic properties and can develop static electric charges when they rub against other particles, objects, or the interiors of transport systems and vacuum hoses. Static discharge from charged toner particles can ignite dust in a vacuum cleaner bag or create a small explosion if sufficient toner is airborne. Toner particles are so fine that they are poorly filtered by conventional household vacuum cleaner filter bags and blow through the motor or back into the room.
If toner spills into the laser printer, a special type of vacuum cleaner with an electrically conductive hose and a high efficiency (HEPA) filter may be needed for effective cleaning. These are called ESD-safe (Electrostatic Discharge-safe) or toner vacuums. Similar HEPA-filter equipped vacuums should be used for clean-up of larger toner spills.
Ozone hazards.
As a normal part of the printing process, the high voltages inside the printer can produce a corona discharge that generates a small amount of ionized oxygen and nitrogen, forming ozone and nitrogen oxides. In larger commercial printers and copiers, a carbon filter in the air exhaust stream breaks down these oxides to prevent pollution of the office environment.
However, some ozone escapes the filtering process in commercial printers, and ozone filters are not used in many smaller consumer printers. When a laser printer or copier is operated for a long period of time in a small, poorly ventilated space, these gases can build up to levels at which the odor of ozone or irritation may be noticed. A potential for creating a health hazard is theoretically possible in extreme cases.
Respiratory health risks.
According to a recent study conducted in Queensland, Australia, some printers emit sub-micrometre particles which some suspect may be associated with respiratory diseases. Of 63 printers evaluated in the Queensland University of Technology study, 17 of the strongest emitters were made by HP and one by Toshiba. The machine population studied, however, was only those machines already in place in the building and was thus biased toward specific manufacturers. The authors noted that particle emissions varied substantially even among the same model of machine. According to Professor Morawska of Queensland University, one printer emitted as many particles as a burning cigarette:
 The health effects from inhaling ultrafine particles depend on particle composition, but the results can range from respiratory irritation to more severe illness such as cardiovascular problems or cancer.
—Queensland University of Technology
A 2006 study in Japan found that laser printers increase concentrations of styrene, xylenes, and ozone, and that inkjet printers emitted pentanol.
Muhle et al. (1991) reported that the responses to chronically inhaled copying toner, a plastic dust pigmented with carbon black, titanium dioxide and silica were also similar qualitatively to titanium dioxide and diesel exhaust.
In December 2011, the Australian government agency Safe Work Australia reviewed existing research and concluded that "no epidemiology studies directly associating laser printer emissions with adverse health outcomes were located" and that several assessments conclude that "risk of direct toxicity and health effects from exposure to laser printer emissions is negligible". The review also observes that, because the emissions have been shown to be volatile or semi-volatile organic compounds, "it would be logical to expect possible health effects to be more related to the chemical nature of the aerosol rather than the physical character of the ‘particulate’ since such emissions are unlikely to be or remain as ‘particulates’ after they come into contact with respiratory tissue."
Air transport ban.
After the 2010 cargo plane bomb plot, in which shipments of laser printers with explosive-filled toner cartridges were discovered on separate cargo airplanes, the US Transportation Security Administration prohibited pass-through passengers from carrying toner or ink cartridges weighing over 1 lb on inbound flights, in both carry-on and checked luggage. "PC Magazine" noted that the ban would not impact most travelers, as the majority of cartridges do not exceed the proscribed weight.

</doc>
<doc id="50678" url="http://en.wikipedia.org/wiki?curid=50678" title="Matthias Jakob Schleiden">
Matthias Jakob Schleiden

Matthias Jakob Schleiden (5 April 1804 – 23 June 1881) was a German botanist and co-founder of the cell theory, along with Theodor Schwann and Rudolf Virchow.
Born in Hamburg, Schleiden was educated at Heidelberg, then practiced law in Hamburg, but soon developed his love for the botany into a full-time pursuit. Schleiden preferred to study plant structure under the microscope. While a professor of botany at the University of Jena, he wrote "Contributions to Phytogenesis" (1838), in which he stated that the different parts of the plant organism are composed of cells. Thus, Schleiden and Schwann became the first to formulate what was then an informal belief as a principle of biology equal in importance to the atomic theory of chemistry. He also recognized the importance of the cell nucleus, discovered in 1831 by the Scottish botanist Robert Brown, and sensed its connection with cell division.
Schleiden was one of the first German biologists to accept Charles Darwin's theory of evolution. He became professor of botany at the University of Dorpat in 1863.
He concluded that all plant parts are made of cells and that an embryonic plant organism arises from the one cell.
He died in Frankfurt am Main on 23 June 1881.
The standard author abbreviation Schleid. is used to indicate this individual as the author when citing a botanical name.

</doc>
<doc id="50679" url="http://en.wikipedia.org/wiki?curid=50679" title="Dot matrix printing">
Dot matrix printing

Dot matrix printing or impact matrix printing is a type of computer printing which uses a print head that moves back and forth, or in an up and down motion, on the page and prints by impact, striking an ink-soaked cloth ribbon against the paper, much like the print mechanism on a typewriter. However, unlike a typewriter or daisy wheel printer, letters are drawn out of a dot matrix, and thus, varied fonts and arbitrary graphics can be produced.
Design.
Each dot is produced by a tiny metal rod, also called a "wire" or "pin", which is driven forward by the power of a tiny electromagnet or solenoid, either directly or through small levers (pawls). Facing the ribbon and the paper is a small guide plate pierced with holes to serve as guides for the pins. This plate may be made of hard plastic or an artificial jewel such as sapphire or ruby.
The portion of the printer containing the pins is called the print head. When running the printer, it generally prints one line of text at a time. There are two approaches to achieve this:
The common "serial dot matrix printers" use a horizontally moving print head. The print head can be thought of featuring a single vertical column of seven or more pins approximately the height of a character box. In reality, the pins are arranged in up to four vertically or/and horizontally slightly displaced columns in order to increase the dot density and print speed through interleaving without causing the pins to jam. Thereby, up to 48 pins can be used to form the characters of a line while the print head moves horizontally.
In a considerably different configuration, so called "line dot matrix printers" use a fixed print head almost as wide as the paper path utilizing a horizontal line of thousands of pins for printing. Sometimes two horizontally slightly displaced rows are used to improve the effective dot density through interleaving. While still line-oriented, these printers for the professional heavy-duty market effectively print a whole line at once while the paper moves forward below the print head.
The printing speed of "serial dot matrix printers" with moving heads varies from 50 to 550 cps. In contrast to this, "line matrix printers" are capable of printing much more than 1000 cps, resulting in a throughput of up to 800 pages/hour.
Because the printing involves mechanical pressure, both of these types of printers can create carbon copies and carbonless copies.
These machines can be highly durable. When they do wear out, it is generally due to ink invading the guide plate of the print head, causing grit to adhere to it; this grit slowly causes the channels in the guide plate to wear from circles into ovals or slots, providing less and less accurate guidance to the printing wires. Eventually, even with tungsten blocks and titanium pawls, the printing becomes too unclear to read.
A variation on the dot matrix printer was the "cross hammer dot printer", patented by Seikosha in 1982. The smooth cylindrical roller of a conventional printer was replaced by a spinning, fluted cylinder. The print head was a simple hammer, with a vertical projecting edge, operated by an electromagnet. Where the vertical edge of the hammer intersected the horizontal flute of the cylinder, compressing the paper and ribbon between them, a single dot was marked on the paper. Characters were built up of multiple dots.
Although nearly all inkjet, thermal, and laser printers also print closely spaced dots rather than continuous lines or characters, it is not customary to call them dot matrix printers.
Early history.
The "LA30" was a 30 character/second dot matrix printer introduced in 1970 by Digital Equipment Corporation of Maynard, Massachusetts. It printed 80 columns of uppercase-only 5×7 dot matrix characters across a unique-sized paper. The printhead was driven by a stepper motor and the paper was advanced by a somewhat-unreliable and definitely noisy solenoid ratchet drive. The LA30 was available with both a parallel interface and a serial interface; however, the serial LA30 required the use of fill characters during the carriage-return
The LA30 was followed in 1974 by the "LA36", which achieved far greater commercial success, becoming for a time the standard dot matrix computer terminal. The LA36 used the same print head as the LA30 but could print on forms of any width up to 132 columns of mixed-case output on standard green bar fanfold paper. The carriage was moved by a much-more-capable servo drive using a DC electric motor and an optical encoder / tachometer. The paper was moved by a stepper motor. The LA36 was only available with a serial interface but unlike the earlier LA30, no fill characters were required. This was possible because, while the printer never communicated at faster than 30 characters per second, the mechanism was actually capable of printing at 60 characters per second. During the carriage return period, characters were "buffered" for subsequent printing at full speed during a "catch-up" period. The two-tone buzz produced by 60 character-per-second catch-up printing followed by 30 character-per-second ordinary printing was a distinctive feature of the LA36.
Digital then broadened the basic LA36 line onto a wide variety of dot matrix printers including:
In 1970, Centronics (then of Hudson, New Hampshire) introduced a dot matrix printer, the Centronics 101. The search for a reliable printer mechanism led it to develop a relationship with Brother Industries, Ltd of Japan, and the sale of Centronics-badged Brother printer mechanisms equipped with a Centronics print head and Centronics electronics. Unlike Digital, Centronics concentrated on the low-end line printer marketplace with their distinctive units. In the process, they designed the parallel electrical interface that was to become standard on most printers until it began to be replaced by the Universal Serial Bus (USB) in the late 1990s.
Printer head positioning.
The printer head is attached to a metal bar that ensures correct alignment, but horizontal positioning is controlled by a rubber band that attaches to sprockets on two wheels at each side which is then driven with an electric motor. Actual position can be found out either by dead count using a stepper motor, rotary encoder attached to one wheel or a transparent plastic band with markings that is read by an optical sensor on the printer head (common on inkjets).
Uses.
Personal computers.
In the 1970s and 1980s, dot matrix impact printers were generally considered the best combination of expense and versatility, and until the 1990s they were by far the most common form of printer used with personal and home computers.
The Epson MX-80, introduced in 1979, was the groundbreaking model that sparked the initial popularity of impact printers in the personal computer market. The MX-80 combined affordability with good-quality text output (for its time). Early impact printers (including the MX) were notoriously loud during operation, a result of the hammer-like mechanism in the print head. The MX-80 even inspired the name of a noise rock band. The MX-80's low dot density (60 dpi horizontal, 72 dpi vertical) produced printouts of a distinctive "computerized" quality. When compared to the crisp typewriter quality of a daisy-wheel printer, the dot-matrix printer's legibility appeared especially bad. In office applications, output quality was a serious issue, as the dot-matrix text's readability would rapidly degrade with each photocopy generation. IBM sold the MX-80 as IBM 5125.
Initially, third-party software (such as the Bradford printer enhancement program) offered a quick fix to the quality issue. The software utilized a variety of software techniques to increase print quality; general strategies were doublestrike (print each line twice), and double-density mode (slow the print head to allow denser and more precise dot placement). Such add-on software was inconvenient to use, because it required the user to remember to run the enhancement program "before" each printer session (to activate the enhancement mode). Furthermore, not all enhancement software was compatible with all programs.
Early personal computer software focused on the processing of text, but as graphics displays became ubiquitous throughout the personal computer world, users wanted to print both text and images. Ironically, whereas the daisy-wheel printer and pen-plotter struggled to reproduce bitmap images, the first dot-matrix impact printers (including the MX-80) lacked the ability to print graphics. Yet the dot-matrix print head was well-suited to this task, and the capability, referred to as "dot-addressable" quickly became a standard feature on all dot-matrix printers intended for the personal and home computer markets. In 1981, Epson offered a retrofit EPROM kit called Graftrax to add the capability to many early MX series printers. Banners and signs produced with software that used this ability, such as Broderbund's Print Shop, became ubiquitous in offices and schools throughout the 1980s.
Progressive hardware improvements to impact printers boosted the carriage speed, added more (typeface) font options, increased the dot density (from 60 dpi up to 240 dpi), and added pseudo-color printing. Faster carriage speeds meant faster (and sometimes louder) printing. Additional typefaces allowed the user to vary the text appearance of printouts. Proportional-spaced fonts allowed the printer to imitate the non-uniform character widths of a typesetter. Increased dot density allowed for more detailed, darker printouts. The impact pins of the printhead were constrained to a minimum size (for structural durability), and dot densities above 100 dpi merely caused adjacent dots to overlap. While the pin diameter placed a lower limit on the smallest reproducible graphic detail, manufacturers were able to use higher dot density to great effect in improving text quality.
Several dot-matrix impact printers (such as the Epson FX series) offered 'user-downloadable fonts'. This gave the user the flexibility to print with different typefaces. PC software uploaded a user-defined fontset into the printer's memory, replacing the built-in typeface with the user's selection. Any subsequent text printout would use the downloaded font, until the printer was powered off or soft-reset. Several third-party programs were developed to allow easier management of this capability. With a supported word-processor program (such as WordPerfect 5.1), the user could embed up to 2 NLQ custom typefaces in addition to the printer's built-in (ROM) typefaces. (The later rise of WYSIWYG software philosophy rendered downloaded fonts obsolete.)
"Single-strike" and "Multi-strike" ribbons were an attempt to address issues in the ribbon's ink quality. Standard printer ribbons used the same principles as typewriter ribbons. The printer would be at its darkest with a newly installed ribbon cartridge, but would gradually grow fainter with each successive printout. The variation in darkness over the ribbon cartridge's lifetime prompted the introduction of alternative ribbon formulations. "Single-strike" ribbons used a carbon-like substance in typewriter ribbons transfer. As the ribbon was only usable for a single loop (rated in terms of 'character count'), the blackness was of consistent, outstanding darkness. "Multi-strike" ribbons gave an increase in ribbon life, at the expense of quality.
The high quality of single-strike ribbons had two side effects:
Pseudo-color.
Several manufacturers implemented color dot-matrix impact printing through a multi-color ribbon. Color was achieved through a multi-pass composite printing process. During each pass, the print head struck a different section of the ribbon (one primary color). For a 4-color ribbon, each printed line of output required a total of 4 passes. In some color printers, such as the Apple ImageWriter II, the printer moved the ribbon relative to the fixed print head assembly. In other models, the print head was tilted against a stationary ribbon.
Due to their poor color quality and increased operating expense, color impact models never replaced their monochrome counterparts. As the color ribbon was used in the printer, the black ink section would gradually contaminate the other 3 colors, changing the consistency of printouts over the life of the ribbon. Hence, the color dot-matrix was suitable for abstract illustrations and piecharts, but not for "photo-realistic" reproduction. Dot-matrix thermal-transfer printers offered more consistent color quality, but consumed printer film, still more expensive. Color printing in the home would only become ubiquitous much later, with the ink-jet printer.The speed is usually 30-550 cps
Near Letter Quality (NLQ).
Text quality was a recurring issue with dot-matrix printers. "Near Letter Quality" mode—informally specified as almost good enough to be used in a business letter—endowed dot-matrix printers with a simulated typewriter-like quality. By using multiple passes of the carriage, and higher dot density, the printer could increase the effective resolution. For example, the Epson FX-86 could achieve a theoretical "addressable" dot-grid of 240 by 216 dots/inch using a print head with a vertical dot density of only 72 dots/inch, by making multiple passes of the print head for each line. For 240 by 144 dots/inch, the print head would make one pass, printing 240 by 72 dots/inch, then the printer would advance the paper by half of the vertical dot pitch (1/144 inch), then the print head would make a second pass. For 240 by 216 dots/inch, the print head would make three passes with smaller paper movement (1/3 vertical dot pitch, or 1/216 inch) between the passes. To cut hardware costs, some manufacturers merely used a "double strike" (doubly printing each line) to increase the printed text's boldness, resulting in bolder but still jagged text. In all cases, NLQ mode incurred a severe speed penalty. Not surprisingly, all printers retained one or more 'draft' modes for high-speed printing.
NLQ became a standard feature on all dot-matrix printers. While NLQ was well received in the IBM PC market, the Apple Macintosh market did not use NLQ mode at all, as it did not rely on the printer's own fonts. Mac word-processing applications used fonts stored in the computer. For non-PostScript (raster) printers, the final raster image was produced by the computer and sent to the printer, which meant dot-matrix printers on the Mac platform exclusively used raster ("graphics") printing mode. For near-letter-quality output, the Mac would simply double the resolution used by the printer, to 144 dpi, and use a screen font twice the point size desired. Since the Mac's screen resolution (72 dpi) was exactly half of the ImageWriter's maximum, this worked perfectly, creating text at exactly the desired size.
Due to the extremely precise alignment required for dot alignment between NLQ passes, typically the paper needed to be held somewhat taut in the tractor feed sprockets, and the continuous paper stack must be perfectly aligned behind or below the printer. Loosely held paper or skewed supply paper could cause misalignments between passes, rendering the NLQ text illegible.
24-pin printers.
By the mid-1980s, manufacturers had increased the pincount of the impact printhead from 7, 8, 9 or 12 pins to 18, 24, 27 or 48, with 24 pins being most common. The increased pin-count permitted superior print-quality which was necessary for success in Asian markets to print legible CJK characters. In the PC market, nearly all 9-pin printers printed at a de facto-standard vertical pitch of 9/72 inch (per printhead pass, i.e. 8 lpi). Epson's 24-pin LQ-series rose to become the new de facto standard, at 24/180 inch (per pass - 7.5 lpi). Not only could a 24-pin printer lay down a denser dot-pattern in a single-pass, it could simultaneously cover a larger area.
Compared to the older 9-pin models, a new 24-pin impact printer not only produced better-looking NLQ text, it printed the page more quickly (largely due to the 24-pin's ability to print NLQ with a single pass). 24-pin printers repeated this feat in bitmap graphics mode, producing higher-quality graphics in reduced time. While the text-quality of a 24-pin was still visibly inferior to a true letter-quality printer—the daisy wheel or laser-printer, the typical 24-pin impact printer printed more quickly than most daisy-wheel models.
As manufacturing costs declined, 24-pin printers gradually replaced 9-pin printers. Twenty-four pin printers reached a dot-density of 360×360 dpi, a marketing figure aimed at potential buyers of competing ink-jet and laser-printers. 24-pin NLQ fonts generally used a dot-density of 360x180, the highest allowable with single-pass printing. Multipass NLQ was abandoned, as most manufacturers felt the marginal quality improvement did not justify the tradeoff in speed. Most 24-pin printers offered 2 or more NLQ typefaces, but the rise of WYSIWYG software and GUI environments such as Microsoft Windows ended the usefulness of NLQ.
Contemporary use.
The desktop impact printer was gradually replaced by the inkjet printer. When Hewlett-Packard's patents expired on steam-propelled photolithographically produced ink-jet heads, the inkjet mechanism became available to the printer industry. For applications that did not require impact (e.g., carbon-copy printing), the inkjet was superior in nearly all respects: comparatively quiet operation, faster print speed, and output quality almost as good as a laser printer. By the mid-1990s, inkjet technology had surpassed dot-matrix in the mainstream market.
As of 2005, dot matrix impact technology remains in use in devices such as cash registers, ATMs, fire alarm systems, and many other point-of-sales terminals. Thermal printing is gradually supplanting them in these applications. Full-size dot-matrix impact printers are still used to print multi-part stationery, for example at bank tellers and auto repair shops, and other applications where use of tractor feed paper is desirable such as data logging and aviation. Some are even fitted with USB interfaces as standard to aid connection to modern computers without legacy ports. Dot matrix printers are also more tolerant of the hot and dirty operating conditions found in many industrial settings. The simplicity and durability of the design, as well as its similarity to older typewriter technology, allows users who are not "computer literate" to easily perform routine tasks such as changing ribbons and correcting paper jams.
One often overlooked application for dot-matrix printers is in the field of IT security. Various system and server activity logs are typically stored on the local filesystem, where a remote attacker - having achieved their primary goals - can then alter or delete the contents of the logs, in an attempt to "cover their tracks" or otherwise thwart the efforts of system administrators and security experts. However, if the log entries are simultaneously output to a printer, line-by-line, a local hard-copy record of system activity is created - and this cannot be remotely altered or otherwise manipulated. Dot-matrix printers are ideal for this task, as they can sequentially print each log entry, one entry at a time, as they are added to the log. The usual dot-matrix printer support for continuous stationery also prevents incriminating pages from being surreptitiously removed or altered without evidence of tampering.
Some companies, such as Printek, DASCOM, WeP Peripherals, Epson, Okidata, Olivetti, Lexmark, and TallyGenicom still produce serial printers. Printronix is now the only manufacturer of line printers. Today, a new dot matrix printer actually costs more than most inkjet printers and some entry level laser printers. However, not much should be read into this price difference as the printing costs for inkjet and laser printers are a great deal higher than for dot matrix printers, and the inkjet/laser printer manufacturers effectively use their monopoly over arbitrarily priced printer cartridges to subsidize the initial cost of the printer itself. Dot matrix ribbons are a commodity and are not monopolized by the printer manufacturers themselves.
Advantages and disadvantages.
Dot matrix printers, like any impact printer, can print on multi-part stationery or make carbon-copies. Impact printers have one of the lowest printing costs per page. As the ink is running out, the printout gradually fades rather than suddenly stopping partway through a job. They are able to use continuous paper rather than requiring individual sheets, making them useful for data logging. They are good, reliable workhorses ideal for use in situations where low printing cost is more important than quality. The ink ribbon also does not easily dry out, including both the ribbon stored in the casing as well as the portion that is stretched in front of the print head; this unique property allows the dot-matrix printer to be used in environments where printer duty can be rare, for instance, as with a Fire Alarm Control Panel's output.
Impact printers create noise when the pins or typeface strike the ribbon to the paper. Sound-damping enclosures may have to be used in quiet environments. They can only print lower-resolution graphics, with limited color performance, limited quality, and lower speeds compared to non-impact printers. While they support fanfold paper with tractor holes well, single-sheet paper may have to be wound in and aligned by hand, which is relatively time-consuming, or a sheet feeder may be utilized which can have a lower paper feed reliability. When printing labels on release paper, they are prone to paper jams when a print wire snags the leading edge of the label while printing at its very edge. For text-only labels (e.g., mailing labels), a daisy wheel printer or band printer may offer better print quality and a lower risk of damaging the paper.
The advantages are:
low purchase cost, can handle multipart forms, cheap to operate, only needs fresh ribbons, rugged, low repair cost and the ability to print on continuous paper. This makes it possible to print long banners that span across several sheets of paper.
The disadvantages are:
noisy, low resolution (you can see the dots making up each character), not all can do color,
color looks faded and streaky, slowness and more prone to jamming - with jams that are more difficult to clear. This is because paper is fed in using two sprockets engaging with holes in the paper. A small tear on the side of a sheet can cause a jam, with paper debris that is tedious to remove.

</doc>
<doc id="50680" url="http://en.wikipedia.org/wiki?curid=50680" title="Funicular">
Funicular

A funicular, also known as an inclined plane or cliff railway, is a cable railway in which a cable attached to a pair of tram-like vehicles on rails moves them up and down a steep slope; the ascending and descending vehicles counterbalance each other.
Operation.
The basic idea of funicular operation is that two cars are always attached to each other by a cable, which runs through a pulley at the top of the slope. Counterbalancing of the two cars, with one going up and one going down, minimizes the energy needed to lift the car going up. Winching is normally done by an electric drive that turns the pulley. Sheave wheels guide the cable to and from the drive mechanism and the slope cars.
Track layout.
Early funiculars used two parallel straight tracks, four rails, with separate station platforms for each vehicle. The tracks are laid with sufficient space between them for the two cars to pass at the midpoint. The wheels of the cars are usually single-flanged, as on standard railway vehicles. Examples of this type of track layout are the Duquesne Incline in Pittsburgh, Pennsylvania, and most cliff railways in the UK.
Layouts that require less width have been developed, with only two or three rails for the most part of the slope and four rails only at the passing section.
The Swiss engineer Carl Roman Abt invented the method that allows cars to be used with a two-rail configuration: the outboard wheels have flanges on both sides, which keeps them aligned with the outer rail, thus holding each car in position, whereas the inboard wheels are unflanged and ride on top of the opposite rail, thereby easily crossing over the rails (and cable) at the passing track.
Two-rail configurations of this type avoid the need for switches and crossings, since the cars have the flanged wheels on opposite sides and will automatically follow different tracks, and in general, significantly reduce costs (especially when the funicular runs in a tunnel, such as the Funicular de Bulnes, in Asturias).
In layouts using three rails, the middle rail is shared by both cars. The three-rail layout is wider than the two-rail layout, but the passing section is simpler to build. If a rack for braking is used, that rack can be mounted higher in a three-rail layout, making it less sensitive to choking in snowy conditions.
Some four-rail funiculars have the upper and lower sections interlaced and a single platform at each station. The Hill Train at Legoland, Windsor, is an example of this configuration.
The track layout can also be changed during the renovation of a funicular, and often four-rail layouts have been rebuilt as two- or three-rail layouts; e.g., the Wellington Cable Car in New Zealand was rebuilt with two rails.
History of different track layouts.
Until the end of the 1870s, the four-rail parallel-track funicular was the normal configuration. Carl Roman Abt developed the Abt Switch allowing the two-rail layout, which was used for the first time in 1879 when the Giessbachbahn funicular opened in Switzerland. In the United States, the first funicular to use a two-rail layout was the Telegraph Hill Railroad in San Francisco, which was in operation from 1884 until 1886. The Mount Lowe Railway in Altadena, California, was the first mountain railway in the United States to use the three-rail layout. Three- and two-rail layouts considerably reduced the space required for building a funicular, reducing grading costs on mountain slopes and property costs for urban funiculars. These layouts enabled a funicular boom in the latter 19th century.
Bottom towrope.
The cars can be attached to a second cable running through a pulley at the bottom of the incline in case the gravity force acting on the vehicles is too low to operate them on the slope. One of the pulleys must be designed as a tensioning wheel to avoid slack in the ropes. In this case, the winching can also be done at the lower end of the incline. This practice is used for funiculars with gradients below 6%, funiculars using sledges instead of cars, or any other case where it is not ensured that the descending car is always able to pull out the cable from the pulley in the station on the top of the incline.
Gravity plane.
Funiculars used in mines were sometimes unpowered gravity planes, also known as self-acting inclines or brake inclines. The weight of descending loaded wagons was used to pull the empty mine wagons.
Water counterbalancing.
A few funiculars have been built using water tanks under the floor of each car that are filled or emptied until just sufficient imbalance is achieved to allow movement. The car at the top of the hill is loaded with water until it is heavier than the car at the bottom, causing it to descend the hill and pulling up the other car. The water is drained at the bottom, and the process repeats with the cars exchanging roles. The movement is controlled by a brakeman.
The oldest funicular in the world moving by water counterbalancing is the Bom Jesus funicular built in 1882. The funicular track in Bom Jesus do Monte near Braga, Portugal is 274 m long and descends 116 m. The funicular of Fribourg is special since it utilizes waste water, coming from the upper part of the city, for counterbalancing.
Inclined lift.
 The inclined lift, or inclined elevator, occasionally inclinator is a special version of the funicular, since it has only one car carrying payload on the slope. The car is either winched up to the station on the top of the incline where the cable is collected on a winch drum, or the single car is balanced by a counterweight and operated the same way as a funicular with two cars. Many inclined lifts were constructed along the pressure lines of storage power plants for transporting building materials. Examples are the "Gelmerbahn" leading to the Gelmersee and the "Funicolare Piora–Ritom" leading to Lago Ritom, both in Switzerland.
The steepest funicular in the world is the incline lift Katoomba Scenic Railway in Australia.
Modern versions resembling an elevator are used in some installations, such as at the Cityplace Station in Dallas, Texas, the Huntington Metro Station in Huntington, Virginia, the San Diego Convention Center in San Diego, California, the Luxor Hotel in Las Vegas, Nevada, and the Eiffel Tower in Paris. The London Millennium Funicular provides an alternative to staircase access to London's Millennium Bridge.
A mixture between an inclined lift and a funicular with two cars was the second Angels Flight in Los Angeles. The funicular closed in 1969 and was reinstalled in 1996 using separate cables for each car, which were winched on separate winch drums in the station at the top. The winch drums were connected to the drive motor and the service brake by a gear train. The system failed because of gear train breakage, causing a fatal accident in 2001. The funicular was then closed until 2010, and since 5 September 2013.
History.
The oldest funicular is the Reisszug, a private line providing goods access to Hohensalzburg Castle at Salzburg in Austria. It was first documented in 1515 by Cardinal Matthäus Lang, who became Archbishop of Salzburg. The line originally used wooden rails and a hemp haulage rope and was operated by human or animal power. Today, steel rails, steel cables and an electric motor have taken over, but the line still follows the same route through the castle's fortifications.
The first railway in England with wooden rails was probably made for James Clifford, lord of the manor of Broseley. He was working coal mines there by 1575 and had a wagonway delivering coal to barges on the River Severn by 1606. This is after the first "record" of a railway in England, the Wollaton Wagonway, but seems to be earlier.
In the 18th century, funiculars were used to allow barge traffic on canals to ascend and descend steep hills. An early example were the three inclined planes on the Tyrone Canal in County Tyrone that were in use as early as 1777. They were used primarily in the early 19th century, especially during the height of the canal-building era in the 1830s in the United States. Such railways operated by allowing water in feeder canals at the top of the plane to drive a turbine, raising or lowering a canal barge along a steep slope.
Examples of hydropower inclined-plane railroads in the United States included the Morris Canal in New Jersey, which connected the Delaware River to the Passaic River using 23 planes, as well as a series of locks along the gentler gradients. The Allegheny Portage Railroad, part of the Pennsylvania Main Line Canal, built in 1834 with ten planes as the first railroad across the Allegheny Mountains of Pennsylvania, was steam powered.
Modern funicular railways operating in urban areas date from the 1860s. The first line of the Funiculars of Lyon (Funiculaires de Lyon) opened in 1862, followed by other lines in 1878, 1891 and 1900. The Budapest Castle Hill Funicular was built in 1868-69, with the first test run on 23 October 1869. In Istanbul, Turkey, the Tünel has been in continuous operation since 1875 and is both the first underground funicular and the second-oldest underground railway. The oldest funicular railway operating in Britain dates from 1875 and is in Scarborough, North Yorkshire.
In Quebec City, Canada, the Old Quebec Funicular has been operating since 1879, connecting the Haute-Ville (Upper Town) to the Basse-Ville (Lower Town). The Dresden Funicular Railway was opened in 1895.
One of the most famous funiculars was the Great Incline of the Mount Lowe Railway in Altadena, California, designed by Andrew Smith Hallidie of San Francisco cable car fame. The Mount Lowe Railway combined its funicular, raising passengers 2800 ft up the steep side of Mount Echo (elevation 3500 ft, with electric narrow-gauge trolley systems at each end (the Rubio Canyon line was standard-gauged after being acquired by Henry Huntington's Pacific Electric Railway). The Incline had three grade changes, the lower end at 62% easing to a 48% at the top, and the cars were designed to adjust to the grade changes for the comfort of their passengers. It had three rails to reduce the width of the formation and the materials required, though a complicated cable routing system was needed at the passing track.
The eastern United States had several incline railways, most engineered by the Otis Elevator Company of Yonkers, New York (today a subsidiary of UTC in Connecticut). Perhaps the best example was the Mount Beacon Incline Railway in Beacon, New York, the steepest funicular Otis built in the northeast. It had an average gradient of 64% and a maximum gradient of 74% and operated for over 75 years. It was destroyed by fire in 1983, and a not-for-profit society is currently working toward its restoration.
The funicular on Mount Vesuvius inspired the song "Funiculì, Funiculà", composed in 1880. That funicular was wrecked repeatedly by volcanic eruptions and abandoned after the eruption of 1944.
World.
Fløibanen is a funicular in Bergen, Norway, which runs up the mountain of Fløyen. It is one of Bergen's major tourist attractions and one of Norway's most visited attractions.
Hong Kong's Peak Tram was one of the first funiculars in Asia, opened in 1888, with a maximum grade of 48%, 1.4 km long, and is now one of Hong Kong's major tourist attractions.
Another funicular in Asia is located on Penang Hill, Penang, Malaysia. Located 6 km from George Town, Penang Hill (Bukit Bendera) is one of the most popular destinations in Penang. Penang Hill is actually a complex of hills and spurs, and the highest point is Western Hill, which is 830 m above sea level. The most convenient way up to Penang Hill is by means of a funicular railway, which is in Air Itam. There is a tunnel that measures 258 ft long and 10 ft wide starting at steepness of 35 ft high, which is the steepest tunnel in the world. The funicular train leaves every half an hour and can carry up to 100 passengers. It takes about 10 minutes to get to the top. Prior to the latest train deployed, journey can take up to 30 minutes and requires passengers to change trains halfway up.
In Spain, the Bulnes funicular is an unusual two-rail installation that runs in a tunnel. The passenger cars are augmented by trailers used for carrying goods and/or animals.
Valparaiso, Chile, has fifteen funiculars, the oldest dating from 1883. Some of them are inside the historic quarter, which has been declared a World Heritage area by UNESCO. Many are currently in disrepair and have been shut down by municipal authorities. There has been recent controversy regarding five of the elevators in the downtown area, where there have been protests about safety and operation. The Polanco Elevator, perhaps the most unusual, had been closed for repairs to the structure and recently re-entered service.
The Carmelit is an underground funicular railway in Haifa, Israel. It is one of the smallest metro systems in the world, having only four cars, six stations and a single tunnel 1.8 km (1.1 mi) long. It operated from its construction in 1959 until 1986 after showing signs of aging. It subsequently reopened in September 1992 after extensive renovations.
The Scenic Railway at Katoomba Scenic World, Blue Mountains, Australia (which supports multiple tourist attractions such as the Skyway and Cableway), is claimed to be the world's steepest passenger-carrying funicular railway, with a maximum incline of 52 degrees or 122%, with a total incline length of 310 m and a vertical lift of 206.5 m in a horizontal distance of 243.4 m. The railway is on the old mining track.
The Great Incline of the Mount Lowe Railway (above right) had multiple grades with cars that adjusted to the variations. The gentlest grade was 48%, the steepest 62%.
The Niesenbahn in the Swiss Kandertal is the longest continuous-cable funicular in Europe. In Lugano, a funicular connects the city centre with Lugano railway station on the hillside above.
In Ukraine, The Kiev Funicular serves the city of Kiev, connecting the historic Uppertown, and the lower neighborhood of Podil through the steep Volodymyrska Hill overseeing the Dnieper River. Funicular was constructed during 1902-05.
In Poland, the most popular is the Gubałówka Hill Funicular, operated by Polish Cable Lines (Polskie Koleje Linowe, PKL).
Water-powered funiculars include the Lynton and Lynmouth Cliff Railway in North Devon, England; the CAT Funicular at the Centre for Alternative Technology in Gwynedd, Wales; the Nerobergbahn in Wiesbaden, Germany; and the Bom Jesus funicular in Braga, Portugal (the oldest, still working, in the world).
The Great Orme Tramway is the only cable-hauled tramway still operating on British public roads. It runs from Church Walks in Llandudno. It first opened on 31 July 1902 and runs on a daily basis from late March to late October, taking visitors to the summit of The Great Orme, climbing 1 mi of track to the summit complex at a height of 679 ft. There are panoramic views of the Welsh mountains and as far as the Isle of Man, Blackpool and the Lake District. There is an exhibition of the history of this funicular tramway at the half-way station.
Pittsburgh, Pennsylvania has two operational funiculars, called "inclines". The Monongahela Incline travels between the top of the Mount Washington hillside to Station Square at the base of the mountain along the Monongahela River. It serves as a tourist attraction and mass transit system. The Duquesne Incline connects Duquesne Heights with the lower elevations of Pittsburgh.
Naples, Italy, has four funiculars. The Chiaia Funicular was built in 1889, followed within two years by the Montesanto Funicular, and after some years by Central Funicular and Mergellina. The most famous funicular in Naples was the Mount Vesuvius Funicular (1880–1944), the first railway track in the world built on an active volcano, which was destroyed various times by Vesuvius eruptions. Partially modified to became a rack railway in its last section, it was destroyed by the eruption in 1944. It became famous worldwide because the Neapolitan song "Funiculì Funiculà" was dedicated to it.
The Johnstown Inclined Plane (built in 1890) in Johnstown, Pennsylvania, United States, is claimed to be the world's steepest vehicular inclined plane, at 70.9%. In addition to passengers, it can carry one automobile in each direction. Chattanooga, Tennessee, is home to the Lookout Mountain Incline Railway (built in 1895), which travels from the base to the top of Lookout Mountain and is claimed to be the steepest funicular in the world, with a maximum grade of 72.7%.
In addition to the historic Angels Flight and Mount Lowe Railway, Southern California has two recently constructed funicular railways. Six Flags Magic Mountain in Valencia, California, has a funicular that takes guests up the Mountain from an area near the park entrance to a station near the Ninja coaster entrance. It was called "Funicular" for many years, introducing thousands of people to the word, but is now known as the "Orient Express" to fit in with the Far Eastern theme at the top of the Mountain. The Pacific Palms Resort in the City of Industry, California, formerly the Industry Hills Sheraton Resort, utilizes a funicular to transport golfers and their carts. The 400 ft line runs from the 9th Green of the "Ike" Course and 18th Green of the "Babe" Course to the St. Andrews Station, a replica of a Scottish station that houses concessions and eating areas with spectacular views of the two hillside courses. The railway was installed in 1979 as part of a 650 acre brownfield reclamation project that transformed a collection of hills containing a former refuse dump into a resort and convention and recreation center. The funicular was devised as a perfect solution for transporting golfers among the steep and dramatic terrain of the demanding and highly regarded golf courses. The railway is currently not in operation but remains fully intact awaiting necessary maintenance until it can once again ferry golfers up the 33% grade overlooking the San Gabriel Valley and San Bernardino Mountains.
The Falls Incline Railway, originally the Horseshoe Falls Incline, at Niagara Falls, Canada, gives access to hotels above the falls.
The funicular in Baku, Azerbaijan connects the hilltop Alley of Martyrs with Neftchilar Avenue on the Caspian seaside. The Baku Funicular is 455 m long and has been in operation since 1960.
Private funiculars.
Private funiculars on steep sections provide easier access from the street to a house than steep paths or steps. They are common in hilly cities, such as Wellington, New Zealand, which has about 300. These have a small car for two to four people permanently attached to a cable from a winch, which runs on an inclined pair of rails (beams) or a single rail at a low speed (0.3 to 1.0 metres/second). They are often called "cable cars" or "lifts" (elevators), e.g., in the New Zealand standard for private cable cars. Larger and faster models can improve access to commercial buildings.
Greenwood Forest Park in North Wales is home to the Dragon roller coaster. This coaster uses a funicular railway pulley system to lift the empty roller coaster using the weight of the people about to ride in it.
An apartment complex in San Francisco, California, has its own funicular shaped like the city's Cable Cars. 
Smallest funiculars.
The smallest funicular in the world is the Fisherman's Walk Cliff Railway in Bournemouth, England, at a length of 128 ft.
The smallest funicular in Croatia is the Zagreb Funicular with a length of 66.0 m.
The smallest funicular in Italy is the Ferata Gran Risa, located in La Ila in South Tyrol, with a length of 66.7 m.
The smallest funicular in Switzerland is located in Lucerne. It serves the guests of the 100-year-old hotel Montana and is of the same age as the hotel. One single cabin shuffles between the top station and the bottom station at the lake promenade (length: 85 m). The travel time for both directions is 60 seconds.
See also.
 

</doc>
<doc id="50682" url="http://en.wikipedia.org/wiki?curid=50682" title="Line matrix printer">
Line matrix printer

A line matrix printer is a computer printer that is a compromise between a line printer and a dot matrix printer. Basically, it prints a page-wide line of dots. It builds up a line of text by printing lines of dots.
Applications.
Line matrix printers are used for high-speed printing applications in industries such as manufacturing, banking, supply chain and back office environments. In these high-volume printing industries, line matrix printers are used to produce invoices, bank statements, product shipment and transportation documentation as well as product compliance labels.
Line matrix printers can print text, bar codes and graphics. Line matrix printers were originally popularized by the mini-computer craze and continued its growth as a bar code printer. Today they are sold in virtually every corner of the world and while they print as rapidly as lineprinters, they can print bar codes and other graphics as well. When implemented as impact printers, they can be the least expensive to operate per page.
How It Works.
Often considered the "backbone" of many industrial and back-office systems, there are several types of line matrix printers used today across diverse enterprise production environments including: manufacturing, supply chain, distribution & logistics, transportation, finance and banking. Dot matrix printers, also known also as impact printers, represent the oldest printing technology, are still widely used today, due to its lowest 'cost per page' ratio. Dot matrix printers are divided on two main groups: serial dot matrix printers and line matrix printers.
A serial dot matrix printer is a type of computer printer with a print head that runs back and forth, or in an up and down motion, on the page and prints by impact, striking an ink-soaked cloth ribbon against the paper, much like the print mechanism on a typewriter. However, unlike a typewriter or daisy wheel printer, letters are drawn out of a dot matrix, and thus, varied fonts and arbitrary graphics can be produced. Because the printing involves mechanical pressure, these printers can create carbon copies and carbonless copies.
Both line matrix and serial dot matrix printers, use pins to strike against the inked ribbon, making dots on the paper and forming the desired characters. The differences are that line matrix printers use a hammer bank (or print-shuttle) instead of print head, this print-shuttle has hammers instead of print wires, and these hammers are arranged in a horizontal row instead in vertical column. The hammer bank uses the same technology as the permanent magnet print head with the small difference that instead of print wires the print-shuttle has hammers.
The permanent magnetic field holds the hammer spring in stressed, ready to strike position. The driver sends electrical current to hammer coil, which then creates electromagnetic field opposite to the permanent magnetic field. When both fields equalize, the energy stored in the spring is released to strike the hammer against the ribbon and prints a dot on the paper.
During the printing process the print-shuttle vibrates in horizontal direction with high speed while the print hammers are fired selectively. Each hammer prints a series of dots in horizontal direction for one pass of the shuttle, then paper advances at one step and the shuttle prints the following row of dots
Line Matrix Printers versus Laser Printers.
For years laserprinters have been the popular choice, however line matrix printers have improved technological aspects and now claim to hold significant benefits over laser printers in terms of energy savings, cost per page, reliability in industrial environments and media flexibility (multipart forms, oversize media, peel off labels, cloth, or card stock). Every aspect of the line matrix printer is designed to deliver higher reliability, fast throughput, and greater resistance to rough handling and hazardous environmental conditions. The result is a product that provides a substantially lower cost over the life of the product. Industry today continues to use line matrix in the print production of mission critical business document production. These documents are critical to keeping the enterprise operation functioning. If these documents don't print the business workflow stops.
Emerging Technology.
Line matrix technology continues to advance based on the need to support modern IT environments. Examples of emerging technologies include:

</doc>
<doc id="50683" url="http://en.wikipedia.org/wiki?curid=50683" title="Page printer">
Page printer

A page printer is a computer printer which processes and prints a whole page at a time, as opposed to printers which print one line or character at a time such as line printers and dot-matrix printers. Page printers are often all incorrectly termed “laser printers”—although virtually all laser printers are page printers, other page printing technologies also exist.
Components.
The components of a page printer are:
Page printing technologies.
There are several page printing technologies, for example:

</doc>
<doc id="50684" url="http://en.wikipedia.org/wiki?curid=50684" title="Pulled rickshaw">
Pulled rickshaw

A pulled rickshaw (or ricksha) is a mode of human-powered transport by which a runner draws a two-wheeled cart which seats one or two people.
In recent times the use of human-powered rickshaws has been discouraged or outlawed in many countries due to concern for the welfare of rickshaw workers. Pulled rickshaws have been replaced mainly by cycle rickshaw and auto rickshaws.
Overview.
Rickshaws are commonly believed to have been invented in Japan in the 1860s, at the beginning of a rapid period of technical advancement. In the 19th century, rickshaw pulling became an inexpensive, popular mode of transportation across Asia.
Peasants who migrated to large Asian cities often worked first as a rickshaw runner. It was "the deadliest occupation in the East, [and] the most degrading for human beings to pursue."
The rickshaw's popularity in Japan declined by the 1930s with the advent of automated forms of transportation, like automobiles and trains. In China, the rickshaw's popularity began to decline in the 1920s. In Singapore, the rickshaw's popularity increased into the 20th century. There were approximately 50,000 rickshaws in 1920 and that number doubled by 1930.
Description.
The initial rickshaws rode on iron-shod wooden wheels and the passenger sat on hard, flat seats. In the late 19th century and early 20th century. Rubber or pneumatic rubber tires, spring cushions, and backrests improved the passenger's comfort. Other features, such as lights were also added.
In the city of Shanghai, public rickshaws were painted yellow to differentiate from the private vehicles of the wealthy citizens, which were described as: 
... always shiny, were carefully maintained, and sported 'a spotless white upholstered double seat, a clean plaid for one's lap, and a wide protective tarpaulin to protect the passenger (or passengers, since sometimes up to three people rode together) against the rain.'
The rickshaws were a convenient means of travel, able to traverse winding, narrow city streets. During monsoon season, passengers might be carried out of the carriage, above the flooded streets, to the door of their arrival. They offered door-to-door travel, unlike scheduled public bus and tram service.
Country overview.
Africa.
East Africa.
In the 1920s, it was used in Bagamoyo, Tanga, Tanzania and other areas of East Africa for short distances.
Madagascar.
Rickshaws, known as "pousse-pousse", were introduced by British missionaries. The intention was to eliminate the slavery-associated palanquin. Its name pousse-pousse, meaning "push-push", is reportedly gained from the need to have a second person to push the back of the rickshaw on Madagascar's hilly roads. They are a common form of transport in a number of Malagasy cities, especially Antsirabe, but are not found in the towns or cities with very hilly roads. They are similar to Chinese rickshaws and are often brightly decorated.
Nairobi.
Rickshaws operated in Nairobi in the beginning of the 20th century; pullers went on strike there in 1908.
South Africa.
Durban is famous for its iconic Zulu rickshaw pullers navigating throughout the city. These colorful characters are famous for their giant, vibrant hats and costumes. There were about 2,000 registered men who pulled rickshaws in Durban in 1904; Since displaced by motorised transport, there are approximately 25 rickshaws left whom mostly cater to tourists today.
Asia.
China.
In China, from the ancient times and until the 19th century, rich and important people, when traveling overland, were commonly transported in sedan chairs carried by bearers, rather than in wheeled vehicles. This was at least partly explained by road conditions. 
It is thought that it was from China (or East Asia in general) that sedan chair (a.k.a. "palanquin") designs were introduced into Western Europe in the 17th century.
 However, wheeled carts for one or two passengers, pushed (rather than pulled, like a proper rikshaw) by human servant, were attested as well.
The proper rickshaw (pronounced "renliche" in Chinese) was first seen in China in 1886, and was used for public transportation in 1898. It was commonly called "dongyangche" for Japanese vehicle or "east- foreign-vehicle."
Rickshaw transportation was an important element in urban development in 20th century China, as a mode of transportation, source of employment and facilitation of migration for workers. According to author David Strand:
Sixty thousand men took as many as a half million fares a day in a city of slightly more than one million. Sociologist Li Jinghan estimated that one out of six males in the city between the ages of sixteen and fifty was a puller. Rickshaw men and their dependents made up almost 20 percent of Beijing's population.
Shanghai's rickshaw industry began in 1874 with 1,000 rickshaws imported from Japan. By 1914 there were 9,718 vehicles. The pullers were a large group of the city's working poor: 100,000 men pulled rickshaws by the early 1940s, up from 62,000 in the mid-1920s.
Most manual rickshaws, a symbol of oppression of the working class, were eliminated in China after the founding of the People's Republic of China in 1949.
Hong Kong.
Rickshaws were first imported to Hong Kong from Japan in 1880. They were a popular form of transport for many years, peaking at more than 3,000 in the 1920s. However, their popularity waned after World War II. No new licenses for rickshaws have been issued since 1975, and only a few old men—four as of 2009—still bear a license. It is reported that only one of them still offer rickshaw rides on The Peak, mainly for tourists.
India.
Around 1880, rickshaws appeared in India, first in Simla. At the turn of the century it was introduced in Kolkata (Calcutta), India and in 1914 was a conveyance for hire.
Service availability.
Though most cities offer auto rickshaw service, hand-pulled rickshaws do exist in some areas, such as Kolkata, "the last bastion of human powered "tana" rickshaws". According to Trillin, most Kolkata rickshaws serve people "just a notch above poor" who tend to travel short distances. However, in a recent article by Hyrapiet and Greiner, the authors found that rickshaws also transport middle-class residents who use their services out of convenience and for short distance trips to the local marketplace. Rickshaws are used to transport goods, shoppers, and school children. It is also used as a "24 hour ambulance service." Also according to Hyrapiet and Greiner, rickshaw pullers have acted as peer-educators for the Calcutta Samaritans prodving critical information on HIV/AIDS because of their access to marginalized groups within Kolkata's red light districts.
Rickshaws are the most effective means of transportation through the flooded streets of the monsoon season. When Kolkata floods rickshaw business increases and prices rise.
The pullers live a life of poverty and many sleep under rickshaws. Rudrangshu Mukerjee, an academic, stated many people's ambivalent feelings about riding a rickshaw: he does not like about being carried in a rickshaw but does not like the idea of "taking away their livelihood."
Motor vehicles are banned in the eco-sensitive zone area of Matheran, India, a tourist hill station near Mumbai so man-pulled rickshaws are still one of the major forms of transport there.
Legislation.
In August 2005, the Communist government of West Bengal announced plans to completely ban pulled rickshaws, resulting in protests and strikes of the pullers. In 2006, the chief minister of West Bengal, Buddhadeb Bhattacharya, announced that pulled rickshaws would be banned and that rickshaw pullers would be rehabilitated.
Japan.
There are several theories about the invention of the rickshaw. Japan historian Seidensticker wrote of the theories:
Though the origins of the rickshaw are not entirely clear, they seem to be Japanese, and of Tokyo specifically. The most widely accepted theory offers the name of three inventors, and gives 1869 as the date of invention.
Starting in 1870, the Tokyo government issued a permission for Izumi Yosuke, Takayama Kosuke, and Suzuki Tokujiro to build and sell rickshaws. By 1872, they became the main mode of transportation in Japan, with about 40,000 rickshaws in service.
The rickshaw's popularity in Japan declined by the 1930s with the advent of automated forms of transportation, like automobiles and trains. After the World War II, when gasoline and automobiles were scarce, they made a temporary come-back. The rickshaw tradition has stayed alive in Kyoto and Tokyo's geisha districts only for tourists as well as in other tourist places. The tradition completely disappeared once, but a few people revived jinrikisha (human-powered rickshaws) for tourists in the 1970s-1980s and the rickshaws became popular as a tourism resource in the 2000s. The modern rickshaw men are a kind of tourist guide, who take their clients to some tourist spots and explain about them. Many of them are part-time working students and athletes who like running or exchanging cultures.
Malaysia.
Rickshaws were a common mode of transport in urban areas of Malaysia in the 19th and early 20th centuries until gradually replaced by cycle rickshaws.
Pakistan.
Pulled and Cycle rickshaw ("qinqi") have been banned in Pakistan since November 1949. Prior to the introduction of auto rickshaws in cities, horse-drawn carriages ("tongas") were a main source of public transportation.
Singapore.
Singapore had received its first rickshaws in 1880 and soon after they were prolific, making a "noticeable change in the traffic on Singapore's streets." Bullock carts and gharries were used prior to the introduction of rickshaws.
Many of the poorest individuals in Singapore in the late nineteenth century were poor, unskilled people of Chinese ancestry. Sometimes called coolies, the hardworking men found pulling rickshaws was a new means of employment. Rickshaw pullers experienced "very poor" living conditions, poverty and long hours of hard work. Income remained unchanged from 1876 to 1926, about $.60 per day.
Rickshaws popularity increased into the 20th century. There were approximately 50,000 rickshaws in 1920 and that number doubled by 1930. In or after the 1920s a union was formed, called the Rickshaw Association, protect the welfare of rickshaw workers.
North America.
United States of America.
From "A History of the Los Angeles City Market (1930-1950)", pulled rickshaws were operated in Los Angeles by high school teenagers during that time period.
Canada.
Foot-driven rickshaws have enjoyed several decades of popularity in Halifax, Nova Scotia; in addition to providing tours of the historic Waterfront, rickshaws are also occasionally used for transportation by local residents. The city is home to the oldest rickshaw company in Canada.
Rickshaws are a popular mode of transportation in downtown Ottawa, Ontario, providing tours of historical Byward Market, in the summer. Ottawa's rickshaws stay true to the traditional foot-driven rickshaw model, but feature modern sound-systems.

</doc>
<doc id="50685" url="http://en.wikipedia.org/wiki?curid=50685" title="Edmund Clerihew Bentley">
Edmund Clerihew Bentley

E. C. Bentley (full name "Edmund Clerihew Bentley"; 10 July 1875 – 30 March 1956) was a popular English novelist and humorist of the early twentieth century, and the inventor of the clerihew, an irregular form of humorous verse on biographical topics. One of the best known is this (1905):
Bentley was born in London and educated at St Paul's School and Merton College, Oxford. His father, John Edmund Bentley, was professionally a civil servant but was also a rugby union international having played in the first ever international match for England against Scotland in 1871. Bentley worked as a journalist on several newspapers, including the "Daily Telegraph". He also worked for the imperialist weekly called "The Outlook" during the editorship of James Louis Garvin. His first published collection of poetry, titled "Biography for Beginners" (1905), popularized the clerihew form; it was followed by two other collections, "More Biography" (1929) and "Baseless Biography" (1939). His detective novel, "Trent's Last Case" (1913), was much praised, numbering Dorothy L. Sayers among its admirers, and with its labyrinthine and mystifying plotting can be seen as the first truly modern mystery. It was adapted as a film in 1920, 1929, and 1952. The success of the work inspired him, after 23 years, to write a sequel, "Trent's Own Case" (1936). There was also a book of Trent short stories, "Trent Intervenes". Several of his books were reprinted in the early 2000s by House of Stratus.
From 1936 until 1949 Bentley was president of the Detection Club. He contributed to two crime stories for the club's radio serials broadcast in 1930 and 1931, which were published in 1983 as "The Scoop and Behind The Screen". In 1950 he contributed the introduction to a Constable & Co omnibus edition of Damon Runyon's "stories of the bandits of Broadway", which was republished by Penguin Books in 1990 as "On Broadway".
He died in 1956 in London at the age of 80. His son Nicolas Bentley was a famous illustrator.
Phonographic recordings of his work "Recordings for the Blind" are heard in the movie "Places in the Heart", by the character Mr. Will.
G. K. Chesterton dedicated his popular detective novel on anarchist terrorism, "The Man Who Was Thursday", to Edmund Clerihew Bentley, a schoolfriend.

</doc>
<doc id="50687" url="http://en.wikipedia.org/wiki?curid=50687" title="Inkjet printing">
Inkjet printing

Inkjet printing is a type of computer printing that recreates a digital image by propelling droplets of ink onto paper, plastic, or other substrates. Inkjet printers are the most commonly used type of printer, and range from small inexpensive consumer models to very large professional machines that can cost tens of thousands of dollars, or more.
The concept of inkjet printing originated in the 19th century, and the technology was first extensively developed in the early 1950s. Starting in the late 1970s inkjet printers that could reproduce digital images generated by computers were developed, mainly by Epson, Hewlett-Packard (HP), and Canon. In the worldwide consumer market, four manufacturers account for the majority of inkjet printer sales: Canon, HP, Epson, and Lexmark, a 1991 spin-off from IBM.
The emerging ink jet material deposition market also uses inkjet technologies, typically printheads using piezoelectric crystals, to deposit materials directly on substrates.
There are two main technologies in use in contemporary inkjet printers: continuous (CIJ) and Drop-on-demand (DOD).
Another emerging printing technology is EHD. Liquids can be printed at nanoscale by pyro-EHD.
Continuous inkjet.
The continuous inkjet (CIJ) method is used commercially for marking and coding of products and packages. In 1867 Lord Kelvin patented the syphon recorder, which recorded telegraph signals as a continuous trace on paper using an ink jet nozzle deflected by a magnetic coil. The first commercial devices (medical strip chart recorders) were introduced in 1951 by Siemens.
In CIJ technology, a high-pressure pump directs liquid ink from a reservoir through a gunbody and a microscopic nozzle, creating a continuous stream of ink droplets via the Plateau-Rayleigh instability. A piezoelectric crystal creates an acoustic wave as it vibrates within the gunbody and causes the stream of liquid to break into droplets at regular intervals: 64,000 to 165,000 droplets per second may be achieved. The ink droplets are subjected to an electrostatic field created by a charging electrode as they form; the field varies according to the degree of drop deflection desired. This results in a controlled, variable electrostatic charge on each droplet. Charged droplets are separated by one or more uncharged "guard droplets" to minimize electrostatic repulsion between neighbouring droplets.
The charged droplets pass through another electrostatic field and are directed (deflected) by electrostatic deflection plates to print on the receptor material (substrate), or allowed to continue on undeflected to a collection gutter for re-use. The more highly charged droplets are deflected to a greater degree. Only a small fraction of the droplets is used to print, the majority being recycled.
CIJ is one of the oldest ink jet technologies in use and is fairly mature. The major advantages are the very high velocity (~50 m/s) of the ink droplets, which allows for a relatively long distance between print head and substrate, and the very high drop ejection frequency, allowing for very high speed printing. Another advantage is freedom from nozzle clogging as the jet is always in use, therefore allowing volatile solvents such as ketones and alcohols to be employed, giving the ink the ability to "bite" into the substrate and dry quickly.
The ink system requires active solvent regulation to counter solvent evaporation during the time of flight (time between nozzle ejection and gutter recycling), and from the venting process whereby air that is drawn into the gutter along with the unused drops is vented from the reservoir. Viscosity is monitored and a solvent (or solvent blend) is added to counteract solvent loss.
Drop-on-demand.
Drop-on-demand (DOD) is divided into thermal DOD and piezoelectric DOD.
Most consumer inkjet printers, including those from Canon, Hewlett-Packard, and Lexmark, use the thermal inkjet process. The idea of using thermal excitation to move tiny drop of ink was developed independently by two groups at roughly the same time: John Vaught and a team at Hewlett-Packard's Corvallis Division, and Canon engineer Ichiro Endo. Initially, in 1977, Endo's team was trying to use the piezoelectric effect to move ink out of the nozzle but noticed that ink shot out of a syringe when it was accidentally heated with a soldering iron. Vaught's work started in late 1978 with a project to develop fast, low-cost printing. The team at HP found that thin-film resistors could produce enough heat to fire an ink droplet. Two years later the HP and Canon teams found out about each other's work. In the thermal inkjet process, the print cartridges consist of a series of tiny chambers, each containing a heater, all of which are constructed by photolithography. To eject a droplet from each chamber, a pulse of current is passed through the heating element causing a rapid vaporization of the ink in the chamber and forming a bubble, which causes a large pressure increase, propelling a droplet of ink onto the paper (hence Canon's trade name of "Bubble Jet"). The ink's surface tension, as well as the condensation and resultant contraction of the vapor bubble, pulls a further charge of ink into the chamber through a narrow channel attached to an ink reservoir. The inks involved are usually water-based and use either pigments or dyes as the colorant. The inks must have a volatile component to form the vapor bubble; otherwise droplet ejection cannot occur. As no special materials are required, the print head is generally cheaper to produce than in other inkjet technologies.
Most commercial and industrial inkjet printers and some consumer printers (those produced by Epson and Brother Industries) use a piezoelectric material in an ink-filled chamber behind each nozzle instead of a heating element. When a voltage is applied, the piezoelectric material changes shape, generating a pressure pulse in the fluid, which forces a droplet of ink from the nozzle. Piezoelectric (also called Piezo) inkjet allows a wider variety of inks than thermal inkjet as there is no requirement for a volatile component, and no issue with kogation (buildup of ink residue), but the print heads are more expensive to manufacture due to the use of piezoelectric material (usually PZT, lead zirconium titanate). A DOD process uses software that directs the heads to apply between zero to eight droplets of ink per dot, only where needed. Piezo inkjet technology is often used on production lines to mark products. For instance, the "use-before" date is often applied to products with this technique; in this application the head is stationary and the product moves past. Requirements of this application are high speed, a long service life, a relatively large gap between the print head and the substrate, and low operating cost.
Ink formulations.
The basic problem with inkjet inks is the conflicting requirements for a coloring agent that will stay on the surface vs. rapid dispersement of the carrier fluid.
Desktop inkjet printers, as used in offices or at home, tend to use aqueous inks based on a mixture of water, glycol and dyes or pigments. These inks are inexpensive to manufacture, but are difficult to control on the surface of media, often requiring specially coated media. HP inks contain sulfonated polyazo black dye (commonly used for dying leather), nitrates and other compounds. Aqueous inks are mainly used in printers with thermal inkjet heads, as these heads require water to perform.
While aqueous inks often provide the broadest color gamut and most vivid color, most are not waterproof without specialized coating or lamination after printing. Most Dye-based inks, while usually the least expensive, are subject to rapid fading when exposed to light. Pigment-based aqueous inks are typically more costly but provide much better long-term durability and ultraviolet resistance. Inks marketed as "Archival Quality" are usually pigment-based.
Some professional wide format printers use aqueous inks, but the majority in professional use today employ a much wider range of inks, most of which require piezo inkjet heads and extensive maintenance:
Head design.
There are two main design philosophies in inkjet head design: "fixed-head" and "disposable head". Each has its own strengths and weaknesses.
Fixed head.
The "fixed-head" philosophy provides an inbuilt print head (often referred to as a "gaiter- head") that is designed to last for the life of the printer. The idea is that because the head need not be replaced every time the ink runs out, consumable costs can be made lower and the head itself can be more precise than a cheap disposable one, typically requiring no calibration. On the other hand, if a fixed head is damaged, obtaining a replacement head can become expensive, if removing and replacing the head is even possible. If the printer's head cannot be removed, the printer itself will then need to be replaced.
Fixed head designs are available in consumer products, but are more likely to be found on industrial high-end printers and large format plotters. In the consumer space, fixed-head printers are manufactured primarily by Epson and Canon. Hewlett-Packard also offers a few fixed-head models, such as the HP OfficeJet Pro X576dw. Industrial fixed-head print heads are manufactured by these companies: Kodak Versamark, Trident, Xaar, Spectra (Dimatix), Hitachi / Ricoh, HP Scitex, Brother, Konica Minolta, Seiko Epson, and ToshibaTec (a licensee of Xaar).
Disposable head.
The "disposable head" philosophy uses a print head which is supplied as a part of a replaceable ink cartridge. Every time a cartridge is exhausted, the entire cartridge and print head are replaced with a new one. This adds to the cost of consumables and makes it more difficult to manufacture a high-precision head at a reasonable cost, but also means that a damaged or clogged print head is only a minor problem: the user can simply buy a new cartridge. Hewlett-Packard has traditionally favoured the disposable print head, as did Canon in its early models. This type of construction can also be seen as an effort by printer manufacturers to stem third party ink cartridge assembly replacements, as these would-be suppliers don't have the ability to manufacture specialized print heads.
An intermediate method does exist: a disposable ink tank connected to a disposable head, which is replaced infrequently (perhaps every tenth ink tank or so). Most high-volume Hewlett-Packard inkjet printers use this setup, with the disposable print heads used on lower volume models. A similar approach is used by Kodak, where the printhead intended for permanent use is nevertheless inexpensive and can be replaced by the user. Canon now uses (in most models) replaceable print heads which are designed to last the life of the printer, but can be replaced by the user should they become clogged.
Cleaning mechanisms.
The primary cause of inkjet printing problems is due to ink drying on the printhead's nozzles, causing the pigments and dyes to dry out and form a solid block of hardened mass that plugs the microscopic ink passageways. Most printers attempt to prevent this drying from occurring by covering the printhead nozzles with a rubber cap when the printer is not in use. Abrupt power losses, or unplugging the printer before it has capped the printhead, can cause the printhead to be left in an uncapped state. Even when the head is capped, this seal is not perfect, and over a period of several weeks the moisture (or other solvent) can still seep out, causing the ink to dry and harden. Once ink begins to collect and harden, the drop volume can be affected, drop trajectory can change, or the nozzle can completely fail to jet ink.
To combat this drying, nearly all inkjet printers include a mechanism to reapply moisture to the printhead. Typically there is no separate supply of pure ink-free solvent available to do this job, and so instead the ink itself is used to remoisten the printhead. The printer attempts to fire all nozzles at once, and as the ink sprays out, some of it wicks across the printhead to the dry channels and partially softens the hardened ink. After spraying, a rubber wiper blade is swept across the printhead to spread the moisture evenly across the printhead, and the jets are again all fired to dislodge any ink clumps blocking the channels.
Some printers use a supplemental air-suction pump, utilizing the rubber capping station to suck ink through a severely clogged cartridge. The suction pump mechanism is frequently driven by the page feed stepper motor: it is connected to the end of the shaft. The pump only engages when the shaft turns backwards, hence the rollers reversing while head cleaning. Due to the built-in head design, the suction pump is also needed to prime the ink channels inside a new printer, and to reprime the channels between ink tank changes.
Professional solvent- and UV-curable ink wide-format inkjet printers generally include a "manual clean" mode that allows the operator to manually clean the print heads and capping mechanism and to replace the wiper blades and other parts used in the automated cleaning processes. The volume of ink used in these printers often leads to "overspray" and therefore buildup of dried ink in many places that automated processes are not capable of cleaning.
The ink consumed in the cleaning process needs to be collected to prevent ink from leaking in the printer. The collection area is called the spittoon, and in Hewlett Packard printers this is an open plastic tray underneath the cleaning/wiping station. In Epson printers, there is typically a large absorption pad in a pan underneath the paper feed platen. For printers several years old, it is common for the dried ink in the spittoon to form a pile that can stack up and touch the printheads, jamming the printer. Some larger professional printers using solvent inks may employ a replaceable plastic receptacle to contain waste ink and solvent which must be emptied or replaced when full.
There is a second type of ink drying that most printers are unable to prevent. For ink to spray from the cartridge, air must enter to displace the removed ink. The air enters via an extremely long, thin labyrinth tube, up to 10 cm long, wrapping back and forth across the ink tank. The channel is long and narrow to reduce moisture evaporation through the vent tube, but some evaporation still occurs and eventually the ink cartridge dries up from the inside out. To combat this problem, which is especially acute with professional fast-drying solvent inks, many wide-format printer cartridge designs contain the ink in an airtight, collapsible bag that requires no vent. The bag merely shrinks until the cartridge is empty.
The frequent cleaning conducted by some printers can consume quite a bit of ink and has a great impact on cost-per-page determinations.
Clogged nozzles can be detected by printing a standard test pattern on the page. Some software workaround methods are known for re-routing printing information from a clogged nozzle to a working nozzle.
Advantages.
Compared to earlier consumer-oriented color printers, inkjets have a number of advantages. They are quieter in operation than impact dot matrix or daisywheel printers. They can print finer, smoother details through higher printhead resolution, and many consumer inkjets with photographic-quality printing are widely available.
In comparison to more expensive technologies like thermal wax, dye sublimation, and laser printing, inkjets have the advantage of practically no warm up time, and lower cost per page. However, low-cost laser printers can have lower per-page costs, at least for black-and-white printing, and possibly for color.
For some inkjet printers, monochrome ink sets are available either from the printer manufacturer or from third-party suppliers. These allow the inkjet printer to compete with the silver-based photographic papers traditionally used in black-and-white photography, and provide the same range of tones: neutral, "warm" or "cold". When switching between full-color and monochrome ink sets, it is necessary to flush out the old ink from the print head with a cleaning cartridge. Special software or at least a modified device driver are usually required, to deal with the different color mapping.
Some types of inkjet printers are capable of very high speed printing. One commercial high speed ink jet printer can print on 30 in wide web at 200 m per minute.
Disadvantages.
Many "intelligent" ink cartridges contain a microchip that communicates the estimated ink level to the printer; this may cause the printer to display an error message, or incorrectly inform the user that the ink cartridge is empty. In some cases, these messages can be ignored, but some inkjet printers will refuse to print with a cartridge that declares itself empty, to prevent consumers from refilling cartridges. For example, Epson embeds a chip which prevents printing when the chip claims the cartridge is empty, although a researcher who over-rode the system found that in one case he could print up to 38% more good quality pages, even though the chip stated that the cartridge was empty. Third-party ink suppliers sell ink cartridges at significant discounts (at least 10−30% off OEM cartridge prices, sometimes up to 95%), and also bulk ink and cartridge self-refill kits at even lower prices. Many vendors' "intelligent" ink cartridges have been reverse-engineered. It is now possible to buy inexpensive devices to reliably reset such cartridges to report themselves as full, so that they may be refilled many times.
Long-term durability of early inkjet prints was quite poor, though improved ink formulations have greatly improved this attribute. See the section on durability for more information.
The very narrow inkjet nozzles are prone to clogging. The ink consumed cleaning them—either during cleaning invoked by the user, or in many cases, performed automatically by the printer on a routine schedule—can account for a significant proportion of the ink used in the machine. Inkjet printing head nozzles can be cleaned using specialized solvents; or by soaking in warm distilled water for short periods of time, for water-soluble inks.
Third-party ink and cartridges.
The high cost of OEM ink cartridges and the intentional obstacles to refilling them have been addressed by the growth of third-party ink suppliers. Many printer manufacturers discourage customers from using third-party inks, stating that they can damage the print heads due to not being the same formulation as the OEM inks, cause leaks, and produce inferior-quality output (e.g. of incorrect color gamut). "Consumer Reports" has noted that some third-party cartridges may contain less ink than OEM cartridges, and thus yield no cost savings, while Wilhelm Imaging Research claims that with third-party inks the lifetime of prints may be considerably reduced. However, an April 2007 review showed that, in a double-blind test, reviewers generally "preferred" the output produced using third-party ink over OEM ink. In general, OEM inks have undergone significant system reliability testing with the cartridge and print-head materials, whereas R&D efforts on third-party ink material compatibility is likely to be significantly less. Some inkjet manufacturers have tried to prevent cartridges being refilled using various schemes including fitting smart chips to the cartridges that can detect when the cartridge has run out of ink and prevent the operation of a refilled cartridge.
The warranty on a printer may not apply if the printer is damaged by the use of non-approved supplies. In the US the Magnuson–Moss Warranty Act is a federal law which states that warrantors cannot require that only brand name parts and supplies be used with their products, as some printer manufacturers imply. However, this would not apply if non-approved items cause damage. In the UK, a printer manufacturer cannot lawfully impose such conditions as part of its warranty (Regina Vs Ford Motor Company refers) although many attempt to do so illegally. As long as the product used was sold as being for the printer it was used in then the sale of goods act applies - anything so sold must be "of merchandisable quality and fit for purpose" in any case under UK law the retailer not the manufacturer is legally liable for 2 years on electrically operated items and is therefore where one must seek redress.
Durability.
Inkjet documents can have poor to excellent archival durability, depending on the quality of the inks and paper used. If low-quality paper is used, it can yellow and degrade due to residual acid in the untreated pulp; in the worst case, old prints can literally crumble into small particles when handled. High-quality inkjet prints on acid-free paper can last as long as typewritten or handwritten documents on the same paper.
Because the ink used in many low-cost consumer inkjets is water-soluble, care must be taken with inkjet-printed documents to avoid even the smallest drop of moisture, which can cause severe "blurring" or "running". In extreme cases, even sweaty fingertips during hot humid weather could cause low-quality inks to smear. Similarly, water-based highlighter markers can blur inkjet-printed documents and discolor the highlighter's tip. The lifetime of inkjet prints produced using aqueous inks is generally shorter (although UV-resistant inks are available) than those produced with solvent-based inkjets; however, so-called "archival inks" have been produced for use in aqueous-based machines which offer extended life.
In addition to smearing, gradual fading of many inks can be a problem over time. Print lifetime is highly dependent on the quality and formulation of the ink. The earliest inkjet printers, intended for home and small office applications, used dye-based inks. Even the best dye-based inks are not as durable as pigment-based inks, which are now available for many inkjet printers. Many inkjet printers now utilize pigment based inks which are highly water resistant: at least the black ink is often pigment-based. Resin or silicone protected photopaper is widely available at low cost, introducing complete water and mechanical rub resistance for dye and pigment inks. The photopaper itself must be designed for pigment or for dye inks, as pigment particles are too large to be able to penetrate through dye-only photopaper protection layer.
The highest-quality inkjet prints are often called "giclée" prints, to distinguish them from less-durable and lower-cost prints. However, the use of the term is no guarantee of quality, and the inks and paper used must be carefully investigated before an archivist can rely on their long-term durability.
Operating cost tradeoffs.
Inkjets use solvent-based inks which have much shorter expiration dates compared to laser toner, which has an indefinite shelf life. Inkjet printers tend to clog if not used regularly, whereas laser printers are much more tolerant of intermittent use. Inkjet printers require periodical head cleaning, which consumes a considerable amount of ink, and will drive printing costs higher especially if the printer is unused for long periods.
If an inkjet head becomes clogged, third-party ink solvents/head cleaners and replacement heads are available in some cases. The cost of such items may be less expensive compared to a transfer unit for a laser printer, but the laser printer unit has a much longer lifetime between required maintenance. Many inkjet printer models now have permanently installed heads, which cannot be economically replaced if they become irreversibly clogged, resulting in scrapping of the entire printer. On the other hand, inkjet printer designs which use a disposable printhead usually cost significantly more per page than printers using permanent heads. By contrast, laser printers do not have printheads to clog or replace frequently, and usually can produce many more pages between maintenance intervals.
Inkjet printers have traditionally produced better quality output than color laser printers when printing photographic material. Both technologies have improved dramatically over time, although the best quality giclee prints favored by artists use what is essentially a high-quality specialized type of inkjet printer.
Business model.
A common business model for inkjet printers involves selling the actual printer at or below production cost, while dramatically marking up the price of the (proprietary) ink cartridges (a profit model called "Freebie marketing"). Most current inkjet printers attempt to enforce this product tying by anticompetitive measures such as microchips in the cartridges to hinder the use of third-party or refilled ink cartridges. The microchips monitor usage and report the ink remaining to the printer. Some manufacturers also impose "expiration dates". When the chip reports that the cartridge is empty (or out of date) the printer stops printing. Even if the cartridge is refilled, the microchip will indicate to the printer that the cartridge is depleted. For many models (especially from Canon), the 'empty' status can be overridden by entering a 'service code' (or sometimes simply by pressing the 'start' button again). For some printers, special circuit "flashers" are available that reset the quantity of remaining ink to the maximum.
Some manufacturers, most notably Epson and Hewlett Packard, have been accused of indicating that a cartridge is depleted while a substantial amount of ink remains. A 2007 study found that most printers waste a significant quantity of ink when they declare a cartridge to be empty. Single-ink cartridges were found to have on average 20% of their ink remaining, though actual figures range from 9% to 64% of the cartridge's total ink capacity, depending on the brand and model of printer. This problem is further compounded with the use of one-piece multi-ink cartridges, which are declared empty as soon as one color runs low. Of great annoyance to many users are those printers that will refuse to print documents requiring only black ink, just because one or more of the color ink cartridges is depleted.
In recent years, many consumers have begun to challenge the business practices of printer manufacturers, such as charging up to US$8,000 per gallon (US$2,100 per liter) for printer ink. Alternatives for consumers are cheaper copies of cartridges, produced by third parties, and the refilling of cartridges, using refill kits. Due to the large differences in price caused by OEM markups, there are many companies selling third-party ink cartridges. Most printer manufacturers discourage refilling disposable cartridges or using aftermarket copy cartridges, and say that use of incorrect inks may cause poor image quality due to differences in viscosity, which can affect the amount of ink ejected in a drop, and color consistency, and can damage the printhead. Nonetheless, the use of alternative cartridges and inks has been gaining in popularity, threatening the business model of printer manufacturers. Printer companies such as HP, Lexmark, and Epson have used patents and the DMCA to launch lawsuits against third-party vendors. An anti-trust class-action lawsuit was launched in the US against HP and office supply chain Staples Inc, alleging that HP paid Staples $100 million to keep inexpensive third-party ink cartridges off the shelves.
In Lexmark Int'l v. Static Control Components, the United States Court of Appeals for the Sixth Circuit ruled that circumvention of this technique does not violate the Digital Millennium Copyright Act. The European Commission also ruled this practice anticompetitive: it will disappear in newer models sold in the European Union. While the DMCA case dealt with copyright protection, companies also rely on patent protection to prevent copying and refilling of cartridges. For example, if a company devises all of the ways in which their microchips can be manipulated and cartridges can be refilled and patents these methods, they can prevent anyone else from refilling their cartridges. Patents protecting the structure of their cartridges prevent the sale of cheaper copies of the cartridges. For some printer models (notably those from Canon) the manufacturer's own microchip can be removed and fitted to a compatible cartridge thereby avoiding the need to replicate the microchip (and risk prosecution). Other manufacturers embed their microchips deep within the cartridge in an effort to prevent this approach.
In 2007 Eastman Kodak entered the inkjet market with its own line of All-In-One printers based on a marketing model that differed from the prevailing practice of selling the printer at a loss while making large profits on replacement ink cartridges. Kodak claimed that consumers could save up to 50 percent on printing by using its lower cost cartridges filled with the company’s proprietary pigmented colorants while avoiding the potential problems associated with off-brand inks.
Professional inkjet printers.
In addition to the widely used small inkjet printers for home and office, there are professional inkjet printers, some for "page-width" format printing and many for wide format printing. "Page-width format" means that the print width ranges from about 8.5" to 37" (about 20 cm to 100 cm). "Wide format" means print width ranging from 24" up to 15' (about 75 cm to 5 m). The most common application of page-width printers is in printing high-volume business communications that do not need high-quality layout and color. Particularly with the addition of variable data technologies, the page-width printers are important in billing, tagging, and individualized catalogs and newspapers. The application of most wide format printers is in printing advertising graphics; a lower-volume application is printing of design documents by architects or engineers. But nowadays there are inkjet printers for digital textile printing up to 64" wide with good High Definition image 1440x720 dpi.
Another specialty application for inkjets is producing prepress color proofs for printing jobs created digitally. Such printers are designed to give accurate color rendition of how the final image will look (a "proof") when the job is finally produced on a large volume press such as a four-colour offset lithography press. An example is an Iris printer, whose output is what the French term Giclée was coined for.
The largest-volume supplier is Hewlett-Packard, which supply over 90 percent of the market for printers for printing technical drawings. The major products in their Designjet series are the Designjet 500/800, the Designjet T Printer series (including the T1100 and T610), the Designjet 1050 and the Designjet 4000/4500. They also have the HP Designjet 5500, a six-color printer that is used especially for printing graphics as well as the new Designjet Z6100 which sits at the top of the HP Designjet range and features an eight colour pigment ink system.
Epson, Kodak and Canon also manufacture wide-format printers, sold in much smaller numbers than standard printers. Epson has a group of 3 Japanese companies around it that predominantly use Epson piezo printheads and inks: Mimaki, Roland, and Mutoh.
Scitex Digital Printing developed high-speed, variable-data, inkjet printers for production printing, but sold its profitable assets associated with the technology to Kodak in 2005 who now market the printers as Kodak Versamark™ VJ1000, VT3000, and VX5000 printing systems. These roll-fed printers can print at up to 1000 feet per minute.
Professional high-volume inkjet printers are made by a range of companies. These printers can range in price from US$35,000 to $2 million. Carriage widths on these units can range from 54" to 192" (about 1.4 to 5 m), and ink technologies tend toward solvent, eco-solvent and UV-curing as opposed to water-based (aqueous) ink sets. Major applications where these printers are used are for outdoor settings for billboards, truck sides and truck curtains, building graphics and banners, while indoor displays include point-of-sales displays, backlit displays, exhibition graphics and museum graphics.
The major suppliers for professional wide- and grand-format printers include: Agfa Graphics, EFI, LexJet, Grapo, Inca, Durst, Océ, NUR (now part of Hewlett-Packard), Lüscher, VUTEk, Scitex Vision (now part of Hewlett-Packard), Mutoh, Mimaki, Roland DG, Seiko I Infotech, Sun Innovations, Leggett and Platt, Agfa, Raster Printers, DGI and MacDermid ColorSpan (now part of Hewlett-Packard).
Professional inkjet photo printers.
Inkjet printers for professional photo printing use up to 10 different inks (photo magenta, photo cyan, yellow, magenta, red, cyan, photo black, matte black, gray and one chroma optimiser for black density and uniform glossiness), with 4800x2400 dpi. They can print an image of 36 megapixels on A3 borderless photo paper with 444 ppi, and can also print to CD/DVD discs. Other professional photo printers have twelve different tanks (the ten colors above plus Light Gray and Dark Gray for monchrome or low-light photos).
Alternative trade names.
Images produced on inkjet printers are sometime sold under other names since the term is associated with words like "digital", "computers", and "everyday printing", which can have negative connotations in some contexts. These "trade names" or "coined names" are usually used in the fine arts reproduction field. They include Digigraph, Iris prints or Giclée, and Cromalin.

</doc>
<doc id="50690" url="http://en.wikipedia.org/wiki?curid=50690" title="Pseudotsuga macrocarpa">
Pseudotsuga macrocarpa

Pseudotsuga macrocarpa, commonly called the bigcone spruce or bigcone Douglas-fir, is an evergreen conifer native to the mountains of southern California, It is notable for having the largest (by far) cones in its "Pseudotsuga" genus, hence the name.
The tree occurs from the San Rafael Mountains in central Santa Barbara County and the Tehachapi Mountains of southwestern Kern County, south through the Transverse Ranges, to the Cuyamaca Mountains in San Diego County. The tree is shade-tolerant and prefers to grow on slopes.
Name.
"Pseudotsuga macrocarpa" is a Douglas-fir. The name "bigcone spruce", though confusing as it is not a spruce species, is often still used, and occurs in place names.
Description.
"Pseudotsuga macrocarpa" typically grows from 15–30 m (50–100 feet) in height and 0.5-1.5 m (2–5 feet) in trunk diameter. The growth form is straight, with a conical crown from 12–30 m (40–100 feet) broad, and a strong and spreading root system. The bark is deeply ridged, composed of thin, woodlike plates separating heavy layers of cork; bark of trees over 1 m (3 feet) in diameter is from 15–20 cm (6-8 inches) thick. The main branches are long and spreading with pendulous side shoots.
The leaves are needle-like, 2.5–5 cm (1-2 inches) long, are shed when about 5 years old. The female cones are from 10–18 cm (4-7 inches) long, larger and with thicker scales than those of other douglas-firs, and with exserted tridentine bracts. The seeds are large and heavy, 10 mm long and 8 mm broad, with a short rounded wing 12 mm long; they may be bird or mammal dispersed as the wing is too small to be effective for wind dispersal. Trees start producing seeds at about 20 years of age.
The largest known individual of this species is 53 m (173 feet) tall, 231 cm (91 inches) in diameter, and is estimated to be from 600 to 700 years of age.
Distribution.
"Pseudotsuga macrocarpa" is restricted to the California montane chaparral and woodlands and California coastal sage and chaparral ecoregions of California. It prefers a Mediterranean climate, characterized by hot dry summers and wet, mild winters. Annual rainfall during a 30-year period on a Bigcone Douglas-fir site in the San Gabriel Mountains averaged 75 cm (30 inches) and ranged from 25–125 cm (10-50 inches).
Bigcone Douglas-fir occurs between 300-2,700 m (1,000-8,000 feet). At low elevation, it occurs near streams in moist, shaded canyons and draws where aspects are mostly north and east. At elevations from 1,350-1,700 m (4,440-5,600 feet), aspects include south- and east-facing slopes. At these elevations, it also grows on sloping hillsides, ridges, and benches. At higher elevations, it occurs on south and west aspects on all types of terrain. The average angle of slope on which it grows is 35 degrees, ranging from level to 90 degrees, although these extremes are uncommon.
Ranges the tree is found in, south to north, include:
Ecology.
"Pseudotsuga macrocarpa", has several features to tolerate and survive wildfire, notably the very thick bark, and the presence of numerous adventitious buds on the upper side of the branches; this enables the trees to survive even crown fires which burn off all the branchlets, the apparently dead trees becoming green again the following spring. Wildfire frequencies in the chaparral habitats in which it often grows typically range from 15-50 year intervals. Bigcone Douglas-fir is closely associated with canyon live oak ("Quercus chrysolepis") and often establishes itself in its shade; after about 50 years, it emerges above the oak canopy.
The number of plant associates in Bigcone Douglas-fir communities is usually small. 
Fire ecology.
"Pseudotsuga macrocarpa" populations are suspected to be declining due to possibly larger and more extreme wildland fires with greater frequencies. Major wildfires within its range, since 2003, have clearly proven a reduced extent when compared to early 1930s extents derived from historical aerial photos. Although historical information has provided the opportunity to detect stand level patch changes, post-fire resprouting of older more mature trees and natural regeneration and recruitment of the species into higher canopy has yet to be adequately quantified.
One tree species in direct competition with Bigcone is "Calocedrus decurrens", with preliminary post-fire regeneration of this species exponentially greater than Bigcone. After 1 or 2 years after the Station fire in 2009 on the Angeles National Forest, there was an estimated 20:1 cedar:bigcone seedling density in fixed radius plots on Mount Wilson. It may be more appropriate to perform population stability estimates up to 5 yrs or much later (i.e. 20 yrs) after a large conflagration due to the potential for immediate and delayed post-fire sprouting and regeneration and interplant competition, as well as the well-noted strategy of seed germination in shrub understories, which is likely to escape detection by surveyors until much later in its life.
Research related to the role of mychorizae and its relationship to seed establishment needs evaluated in these vegetation communities due to the suspected role it has with the relationship with water, especially in water-limited systems such as those in the wildlands of southern California. In addition, an aggressive seed cone collection strategy should be drafted for this species which includes extensive collection during large cone production years such as 2013, and should include a tracking system to determine correlations to climatic conditions in order to develop a foundation from which to perform species viability assessments w/ varying future climate scenarios.
This tree is being considered for more extensive plantings in semiarid locales. Its favorable qualities include resistance to drought, fire, insects, decay, and damage from ozone, and its aggressive rooting system and tolerance to variable growing medium. The needles of older trees sometimes fade to yellow, drop, and trees appear dead only to sprout with renewed vigor within 2 years. The reason is unknown, although drought or insects may be possible causes.
Uses.
Wildlife.
Bigcone Douglas-fir stands provide habitat for Black-tailed Deer, Black Bear, and various small animals. These trees provide preferred spring habitat for Black Bear in the San Bernardino Mountains.
The seeds are eaten by various rodents and birds.
Restoration species.
Bigcone Douglas-fir is used for watershed and habitat restoration. The Los Angeles County Department of Forestry has extensively planted the tree over a 50-year period for that purpose.
Some hybrids of "Pseudotsuga macrocarpa" (Bigcone Douglas-fir) X "Pseudotsuga menziesii" (Douglas-fir) show promise for planting on drier restoration sites within the "P. menziesii"−Douglas-fir natural range. These hybrids produce wood of comparable quality to that of Douglas-fir and have the drought tolerance of Bigcone Douglas-fir.
Timber.
There is no current commercial market for Bigcone Douglas-fir wood due to limited distribution and access. It is heavy, hard, and fine grained but not durable. There is less sapwood than heartwood, and the latter contains pockets of resin.
In the past, the wood was used locally for fuel and lumber.

</doc>
<doc id="50691" url="http://en.wikipedia.org/wiki?curid=50691" title="Douglas fir">
Douglas fir

Douglas fir, with the scientific name Pseudotsuga menziesii, also known as Oregon pine or Douglas spruce, is an evergreen conifer species native to western North America.
Naming.
The common name honors David Douglas, a Scottish botanist and collector who first reported the extraordinary nature and potential of the species. The common name is misleading since it is not a true fir, i.e., not a member of the genus "Abies". For this reason the name is often written as Douglas-fir (a name also used for the genus "Pseudotsuga" as a whole).
The specific epithet, "menziesii", is after Archibald Menzies, a Scottish physician and rival naturalist to David Douglas. Menzies first documented the tree on Vancouver Island in 1791. Colloquially, the species is also known simply as Doug-fir or as Douglas pine (although the latter common name may also refer to "Pinus douglasiana").
Distribution.
One variety, coast Douglas fir ("Pseudotsuga menziesii" var. "menziesii"), grows in the coastal regions, from west-central British Columbia southward to central California. In Oregon and Washington, its range is continuous from the eastern edge of the Cascades west to the Pacific Coast Ranges and Pacific Ocean. In California, it is found in the Klamath and California Coast Ranges as far south as the Santa Lucia Range, with a small stand as far south as the Purisima Hills in Santa Barbara County. In the Sierra Nevada, it ranges as far south as the Yosemite region. It occurs from near sea level along the coast to 1800 m above sea level in the California Mountains.
Further inland, coast Douglas fir is replaced by another variety, Rocky Mountain or interior Douglas fir ("P. menziesii" var. "glauca"). Interior Douglas fir intergrades with coast Douglas fir in the Cascades of northern Washington and southern British Columbia, and from there ranges northward to central British Columbia and southeastward to the Mexican border, becoming increasingly disjunct as latitude decreases and its altitudinal limits increase. Mexican Douglas fir ("P. lindleyana"), which ranges as far south as Oaxaca, is often considered part of "P. menziesii".
Description.
Coast Douglas fir is currently the second-tallest conifer in the world (after coast redwood). Extant coast Douglas fir trees 60 - or more in height and 1.5 - in diameter are common in old growth stands, and maximum heights of 100 - and diameters up to 4.5 - have been documented. The tallest living specimen is the "Doerner Fir", previously known as the Brummit Fir, 99.4 m tall, at East Fork Brummit Creek in Coos County, Oregon, the stoutest is the "Queets Fir", 4.85 m in diameter, in the Queets River valley of Olympic National Park in Washington. Douglas firs commonly live more than 500 years and occasionally more than 1,000 years.
The bark on young trees is thin, smooth, gray, and contains numerous resin blisters. On mature trees, it is thick and corky. The shoots are brown to olive-green, turning gray-brown with age, smooth, though not as smooth as fir shoots, and finely pubescent with short, dark hairs. The buds are a very distinctive, narrow, conic shape, 4 – long, with red-brown bud scales. The leaves are spirally arranged, but slightly twisted at the base to lie flattish on either side of the shoot, needle-like, 2 - long, green above with no stomata, and with two whitish stomatal bands below. Unlike the Rocky Mountain Douglas fir, coast Douglas fir foliage has a noticeable sweet fruity-resinous scent, particularly if crushed.
The mature female seed cones are pendent, 5 - long, 2 – broad when closed, opening to 4 cm broad. They are produced in spring, green at first, maturing orange-brown in the autumn 6–7 months later. The seeds are 5 – long and 3 – broad, with a 12–15-mm wing. The male (pollen) cones are 2 – long, dispersing yellow pollen in spring.
In forest conditions, old individuals typically have a narrow, cylindric crown beginning 20 - above a branch-free trunk. Self-pruning is generally slow and trees retain their lower limbs for a long period. Young, open-grown trees typically have branches down to near ground level. It often takes 70–80 years for the trunk to be clear to a height of 5 m and 100 years to be clear to a height of 10 m.
Appreciable seed production begins at 20–30 years in open-grown coast Douglas fir. Seed production is irregular; over a 5–7 year period, stands usually produce one heavy crop, a few light or medium crops, and one crop failure. Even during heavy seed-crop years, only about 25% of trees in closed stands produce an appreciable number of cones. Each cone contains around 25 to 50 seeds. Seed size varies; average number of cleaned seeds varies from 70 to 88/g (32,000–40,000/lb). Seeds from the northern portion of its range tend to be larger than seeds from the south.
Ecology.
The rooting habit of coast Douglas fir is not particularly deep, with the roots tending to be shallower than those of same-aged ponderosa pine, sugar pine, or California incense-cedar, though deeper than Sitka spruce. Some roots are commonly found in organic soil layers or near the mineral soil surface. However, Douglas fir exhibits considerable morphological plasticity, and on drier sites coast Douglas fir will generate deeper taproots. Interior Douglas fir exhibits even greater plasticity, occurring in stands of interior temperate rainforest in British Columbia, as well as at the edge of semi-arid sagebrush steppe throughout much of its range, where it generates even deeper taproots than coast Douglas fir is capable.
Douglas fir snags are abundant in forests older than 100–150 years and provide cavity-nesting habitat for numerous forest birds. Mature or "old-growth" Douglas fir forest is the primary habitat of the red tree vole ("Arborimus longicaudus") and the spotted owl ("Strix occidentalis"). Home range requirements for breeding pairs of spotted owls are at least 400 ha (4 km2 of old-growth. Red tree voles may also be found in immature forests if Douglas fir is a significant component. This animal nests almost exclusively in the foliage of Douglas fir trees. Nests are located 2 - above the ground. The red vole's diet consists chiefly of Douglas fir needles. A parasitic plant sometimes utilizing "P. menziesii" is Douglas-fir dwarf mistletoe ("Arceuthobium douglasii").
Its seedlings are not a preferred browse of black-tailed deer ("Odocoileus hemionus columbianus") and elk ("Cervus canadensis"), but can be an important food source for these animals during the winter when other preferred forages are lacking. In many areas, Douglas fir needles are a staple in the spring diet of blue grouse ("Dendragapus"). In the winter, New World porcupines primarily eat the inner bark of young conifers, among which they prefer Douglas fir.
The leaves are also used by the woolly conifer aphid "Adelges cooleyi"; this 0.5 mm long sap-sucking insect is conspicuous on the undersides of the leaves by the small white "fluff spots" of protective wax that it produces. It is often present in large numbers, and can cause the foliage to turn yellowish from the damage in causes. Exceptionally, trees may be partially defoliated by it, but the damage is rarely this severe. Among Lepidoptera, apart from some that feed on "Pseudotsuga" in general (see there) the gelechiid moths "Chionodes abella" and "C. periculella" as well as the cone scale-eating tortrix moth "Cydia illutana" have been recorded specifically on "P. menziesii".
Douglas fir seeds are an extremely important food for small mammals. Mice, voles, shrews, and chipmunks consumed an estimated 65 percent of a Douglas fir seed crop following dispersal in western Oregon. The Douglas squirrel ("Tamiasciurus douglasii") harvests and caches great quantities of Douglas fir cones for later use. They also eat mature pollen cones, developing inner bark, terminal shoots, and tender young needles. The seeds are also important in the diets of several seed-eating birds. These include most importantly American sparrows (Emberizidae) – dark-eyed junco ("Junco hyemalis"), song sparrow ("Melospiza melodia"), golden-crowned sparrow ("Zonotrichia atricapilla") and white-crowned sparrow ("Z. leucophrys") – and true finches (Fringillidae) – pine siskin ("Carduelis pinus"), purple finch (""Carpodacus" purpureus"), and the Douglas fir red crossbill ("Loxia curvirostra neogaea") which is uniquely adapted to foraging for "P. menziesii" seeds.
The coast Douglas fir variety is the dominant tree west of the Cascade Mountains in the Pacific Northwest, occurring in nearly all forest types, competes well on most parent materials, aspects, and slopes. Adapted to a moist, mild climate, it grows larger and faster than Rocky Mountain Douglas fir. Associated trees include western hemlock, Sitka spruce, sugar pine, western white pine, ponderosa pine, grand fir, coast redwood, western redcedar, California incense-cedar, Lawson's cypress, tanoak, bigleaf maple and several others. Pure stands are also common, particularly north of the Umpqua River in Oregon.
Shrub associates in the central and northern part of coast Douglas fir's range include vine maple ("Acer circinatum"), salal ("Gaultheria shallon"), Pacific rhododendron ("Rhododendron macrophyllum"), Oregon-grape ("Mahonia aquifolium"), red huckleberry ("Vaccinium parvifolium"), and salmonberry ("Rubus spectabilis"). In the drier, southern portion of its range shrub associates include California hazel ("Corylus cornuta" var. "californica"), oceanspray ("Holodiscus discolor"), creeping snowberry ("Symphoricarpos mollis"), western poison-oak ("Toxicodendron diversilobum"), ceanothus ("Ceanothus" spp.), and manzanita ("Arctospaphylos" spp.). In wet coastal forests, nearly every surface of old-growth coast Douglas fir is covered by epiphytic mosses and lichens.
Poriol is a flavanone, a type of flavonoid, produced by "P. menziesii" in reaction to infection by "Poria weirii".
Forest succession.
The shade-intolerance of Douglas fir plays a large role in the forest succession of lowland old growth rainforest communities of the Pacific Northwest. While mature stands of lowland old-growth rainforest contain many western hemlock ("Tsuga heterophylla") seedlings, and some western redcedar ("Thuja plicata") seedlings, Douglas fir dominated stands contain almost no Douglas fir seedlings. This seeming contradiction occurs because Douglas firs are intolerant of deep shade and rarely survive for long within the shaded understory. When a tree dies in a mature forest the canopy opens up and sunlight becomes available as a source of energy for new growth. The shade-tolerant western hemlock seedlings that sprout beneath the canopy have a head-start on other seedlings. This competitive advantage allows the western hemlock to grow rapidly into the sunlight, while other seedlings still struggle to emerge from the soil. The boughs of the growing western hemlock limit the sunlight for smaller trees and severely limit the chances of shade-intolerant trees, such as the Douglas fir. Over the course of centuries, western hemlock typically come to dominate the canopy of an old-growth lowland rainforest.
Douglas firs are seral trees in temperate rainforest, and possess thicker bark and a somewhat faster growth rate than most other climax trees of the area, such as the western hemlock and western redcedar. This quality often gives Douglas firs a competitive advantage when the forest experiences a major disturbance such as fire. Periodically, portions of a Pacific Northwest lowland forest may be burned by wildfire, may be logged, or may be blown down by a wind storm. These types of disturbances allow Douglas fir to regenerate in openings, and low-intensity fires often leave Douglas fir trees standing on drier sites, while less drought- and fire-tolerant species are unable to get established.
Conifers dominate the climax forests of the coast Douglas fir. All of the climax conifers that grow alongside it can live for centuries, with a few species capable of living for over a millennium. Forests that exist on this time scale experience the type of sporadic disturbances that allow mature stands of Douglas fir to establish themselves as a persistent element within a mature old-growth forest. When old growth forests survive in a natural state, they often look like a patchwork quilt of different forest communities. Western hemlock typically dominate oldgrowth rainforests, but contain sections of Douglas firs, redcedar, alder, and even redwood forests on their southern extent, near the Oregon and California border, while Sitka spruce increases in frequency with latitude.
The logging practices of the last 200 years created artificial disturbances that caused Douglas firs to thrive. The Douglas fir's useful wood and its quick growth make it the crop of choice for many timber companies, which typically replant a clear-cut area with Douglas fir seedlings. The high-light conditions that exist within a clear-cut also naturally favor the regeneration of Douglas fir. Because of clear-cut logging, almost all the Pacific Northwest forests not strictly set aside for protection are today dominated by Douglas fir, while the normally dominant climax species, such as western hemlock and western redcedar are less common. On drier sites in California, where Douglas fir behaves as a climax species in the absence of fire, the Douglas fir has become somewhat invasive following fire suppression practices of the twentieth and twenty-first centuries; it is becoming a dominant species in many oak woodlands, in which it was previously a minor component.
Uses.
Douglas fir is one of the world's best timber producers and yields more timber than any other tree in North America. The wood is used for dimensional lumber, timbers, pilings, and plywood. Creosote treated pilings and decking are used in marine structures. The wood is also made into railroad ties, mine timbers, house logs, posts and poles, fencing, flooring, pulp, and furniture. Douglas fir is used extensively in landscaping. It is planted as a specimen tree or in mass screenings. It is also a popular Christmas tree.
This plant has ornamental value in large parks and gardens, and has gained the Royal Horticultural Society's Award of Garden Merit.
Away from its native area, it is also extensively used in forestry as a plantation tree for timber in Europe, New Zealand, Chile and elsewhere. It is also naturalised throughout Europe, Argentina and Chile (called "Pino Oregón"), and in New Zealand sometimes to the extent of becoming an invasive species (termed a wilding conifer) subject to control measures.
The buds have been used to flavor eau de vie, a clear, colorless fruit brandy.
Native Hawaiians built "waʻ kaulua" (double-hulled canoes) from coast Douglas fir logs that had drifted ashore.

</doc>
<doc id="50692" url="http://en.wikipedia.org/wiki?curid=50692" title="Pseudotsuga menziesii var. glauca">
Pseudotsuga menziesii var. glauca

Pseudotsuga menziesii" var. "glauca, or Rocky Mountain Douglas-fir, is an evergreen conifer native to the interior mountainous regions of western North America, from central British Columbia and southwest Alberta in Canada southward through the United States to the far north of Mexico. The range is continuous in the northern Rocky Mountains south to eastern Washington, eastern Oregon, Idaho, western and south-central Montana and western Wyoming, but becomes discontinuous further south, confined to "sky islands" on the higher mountains in Utah, Colorado, Arizona and New Mexico, with only very isolated small populations in eastern Nevada, westernmost Texas, and northern Mexico. It occurs from 600 m altitude in the north of the range, up to 3,000 m, rarely 3,200 m, in the south. Further west towards the Pacific coast, it is replaced by the related coast Douglas-fir ("Pseudotsuga menziesii" var. "menziesii"), and to the south, it is replaced by Mexican Douglas-fir in high mountains as far south as Oaxaca. Some botanists have grouped Mexican Douglas-fir with "P. menziesii" var. "glauca", but genetic and morphological evidence suggest that Mexican populations should be considered a different variety.
Rocky Mountain Douglas-fir is most commonly treated as a variety ("Pseudotsuga menziesii" var. "glauca"), but has also been called a subspecies ("Pseudotsuga menziesii" subsp. "glauca") or more rarely (mainly in the past) a distinct species ("Pseudotsuga glauca"). The strong ecological and genetic differentiation with intergradation limited primarily to postglacial contact zones in British Columbia supports infraspecific groupings. Some botanists have further split Rocky Mountain Douglas-fir into two varieties, but these are not widely acknowledged and have only limited support from genetic testing.
Characteristics.
Rocky Mountain Douglas-fir is a large tree, typically reaching 35–45 m in height and 1 m in diameter, with exceptional specimens known to 67 m tall, and 2 m diameter. It commonly lives more than 500 years and occasionally more than 1,200 years. The bark on young trees is thin, smooth, gray, and covered with resin blisters. On mature trees, it is moderately thick (3–6 cm), furrowed and corky though much less so than coast Douglas-fir.
The shoots are brown to gray-brown, smooth, though not as smooth as fir shoots, and finely pubescent with scattered short hairs. The buds are a distinctive narrow conic shape, 3–6 mm long, with red-brown bud scales. The leaves are spirally arranged but slightly twisted at the base to be upswept above the shoot, needle-like, 2–3 cm long, gray-green to blue-green above with a single broad stomatal patch, and with two whitish stomatal bands below.
The male (pollen) cones are 2–3 cm long, and are typically restricted to or more abundant on lower branches. Pollen cones develop over 1 year and wind-dispersed pollen is released for several weeks in the spring.
The mature female seed cones are pendent, 4–7 cm long, 2 cm broad when closed, opening to 3–4 cm broad. They are produced in spring, purple (sometimes green) at first, maturing orange-brown in the autumn 5–7 months later. The seeds are 5–6 mm long and 3–4 mm broad, with a 12–15 mm wing. Both coast Douglas-fir and Rocky Mountain Douglas-fir produce abundant crops of seed approximately every 2–11 years. Seed is produced annually except for about 1 year in any 4-to-5-year period.
Growth.
Rocky Mountain Douglas-fir grows more slowly than coast Douglas-fir and is also much more cold tolerant. Tolerance of different environmental conditions varies among populations of Rocky Mountain Douglas-fir, especially among populations from the northern and southern Rockies. However, even nearby populations can differ in cold hardiness.
Root morphology is variable, but when unimpeded, a taproot forms within several years. "Platelike" root morphologies occur where growth is impeded. The most prominent lateral roots begin in the 1st or 2nd year of growth. Most roots in surface soil are "long ropelike laterals of secondary and tertiary origin". Fine-root production is episodic in response to changing environmental conditions; the average lifespan of fine roots is usually between several days and several weeks.
Rocky Mountain Douglas-fir reaches reproductive maturity at 12–15 years. It has winged seeds that are dispersed primarily by wind and gravity. In western Montana clearcuts, seeds were dispersed up to 250 m (800 feet) uphill from their source, but seedfall between 180–250 m (600–800 feet) was only 7% of that found in uncut stands. Other studies determined that seedfall in clearcuts beyond 80 m (265 feet) from seed trees was about 3% of seedfall in uncut stands where seed trees are close together. Well-stocked stands have resulted from seedfall from sources 1–2 km (0.6–1.2 miles) distant, but most Douglas-fir seeds fall within 100 m (330 feet) of their source. Small amounts of seed are dispersed by mice, chipmunks, and squirrels. Rocky Mountain Douglas-fir seeds are disseminated about twice as far as seeds of Ponderosa pine.
Longevity.
The oldest accurately-dated Rocky Mountain Douglas-fir, 1275 years old, is in New Mexico. This longevity is apparently uncommon; growing on a relatively barren lava field has protected it from fire, animals, and humans. Growth typically slows dramatically between 90 and 140 years of age.
In the dry-belt forests of central British Columbia, ages can exceed 500 years on sites normal for the region. The oldest accurately-dated growth ring available for the region is 1475; dates in the 1500s and 1600s are more common for remnant patches that have escaped logging, fire, and other disturbances.
Ecology.
Rocky Mountain Douglas-fir grows on a variety of sites across its wide geographic range. It grows at lower elevations adjacent to and within bunchgrass communities and is also found in upper-elevation subalpine forests. It tends to be most abundant in low- and middle-elevation forests, where it grows over a wide range of aspects, slopes, landforms, and soils.
In spring and winter (in British Columbia, Idaho, and Montana) elk browse on south- and southwest-facing Rocky Mountain Douglas-fir and Ponderosa pine stands, particularly when shrubs and/or grasses are productive. In summer, elk generally are found at higher elevations (outside the Rocky Mountain Douglas-fir and Pacific Ponderosa Pine zones). During fall elk use stands of Rocky Mountain lodgepole pine, subalpine fir, western larch, or grand fir with high canopy cover.
In parts of Yellowstone National Park, elk browsing is so intensive that young Rocky Mountain Douglas-fir are stunted at 1–1.5 m (3–4.5 feet) in height, with live branches trailing very close to the ground, and branches on the upper two thirds of the tree dead. Low-elevation and south-facing open-structure Rocky Mountain Douglas-fir types are often important winter range for white-tailed deer and mule deer. Moose winter in low-elevation Rocky Mountain Douglas-fir types in areas where willow thickets, the preferred winter habitat, are lacking; in such areas Rocky Mountain Douglas-fir is an important moose food.
Chipmunks, mice, voles, and shrews eat large quantities of conifer seeds from the forest floor, and clipped cones are a staple and major part of storage of red squirrels. These animals store a large amount of Rocky Mountain Douglas-fir cones or seeds. American marten commonly den in hollow logs.
Numerous species of songbirds extract seeds from Douglas-fir cones or forage for seeds on the ground. The most common are the Clark's nutcracker, black-capped chickadee, mountain chickadee, boreal chickadee, red-breasted nuthatch, pygmy nuthatch, red crossbill, white-winged crossbill, dark-eyed junco, and pine siskin. Migrating flocks of dark-eyed juncos may consume vast quantities of seeds and freshly germinated seedlings. Woodpeckers commonly feed in the bark of Rocky Mountain Douglas-fir. Blue grouse forage on needles and buds in winter; they and other birds rely heavily on Rocky Mountain Douglas-fir communities for cover.
The Douglas-fir is vulnerable to infestation by a woolly aphid, "Adelges cooleyi" that also infects the Engelmann spruce to complete its lifecycle.
Uses.
Rocky Mountain Douglas-fir is a valuable timber tree. The wood is exceptionally strong and is used for structural timber as well as poles, plywood, pulp, dimensional lumber, railroad ties, mine timbers, log cabins, posts and poles, fencing, and firewood. Other uses listed include "machine-stress-rated lumber", glued-laminated (Glulam) beams, pallets, furniture, cabinets, doors, flooring, window frames, and other miscellaneous woodwork and millwork. Rocky Mountain Douglas-firs are also cut and sold as Christmas trees.

</doc>
<doc id="50693" url="http://en.wikipedia.org/wiki?curid=50693" title="Mortimer Wheeler">
Mortimer Wheeler

Sir Robert Eric Mortimer Wheeler (10 September 1890 – 22 July 1976) was a British archaeologist and officer in the British Army. Over the course of his career, he served as Director of both the National Museum of Wales and London Museum, Director-General of the Archaeological Survey of India, and the founder and Honorary Director of the Institute of Archaeology in London, further writing twenty-four books on archaeological subjects.
Born in Glasgow to a middle-class family, Wheeler was raised largely in Yorkshire before relocating to London in his teenage years. After studying Classics at University College London (UCL), he began working professionally in archaeology, specialising in the Romano-British period. During World War I he volunteered for service in the Royal Artillery, being stationed on the Western Front, where he rose to the rank of major and was awarded the Military Cross. Returning to Britain, he obtained his doctorate from UCL before taking on a position at the National Museum of Wales, first as Keeper of Archaeology and then as Director, during which time he oversaw excavation at the Roman forts of Segontium, Y Gaer, and Isca Augusta with the aid of his first wife, Tessa Wheeler. Influenced by the archaeologist Augustus Pitt Rivers, Wheeler argued that excavation and the recording of stratigraphic context required an increasingly scientific and methodical approach, developing the "Wheeler Method". In 1926, he was appointed Keeper of the London Museum; there, he oversaw a reorganisation of the collection, successfully lobbied for increased funding, and began lecturing at UCL.
In 1934, he established the Institute of Archaeology as part of the federal University of London, adopting the position of Honorary Director. In this period, he oversaw excavations of the Roman sites at Lydney Park and Verulamium and the Iron Age hillfort of Maidan Castle. During World War II, he re-joined the armed forces and rose to the position of brigadier, serving in the North African Campaign and then the Allied invasion of Italy. In 1944 he was appointed Director-General of the Archaeological Survey of India, through which he oversaw excavations of sites at Harappa, Arikamedu, and Brahmagiri, and implemented reforms to the subcontinent's archaeological establishment. Returning to Britain in 1948, he divided his time between lecturing for the Institute of Archaeology and acting as archaeological adviser to Pakistan's government. In later life, his popular books, cruiseline lectures, and appearances on radio and television, particularly the BBC series "Animal, Vegetable, Mineral?", helped to bring archaeology to a mass audience. Appointed Honorary Secretary of the British Academy, he raised large sums of money for archaeological projects, and was appointed British representative for several UNESCO projects.
Wheeler is recognised as one of the most important British archaeologists of the twentieth century, responsible for successfully encouraging British public interest in the discipline and advancing methodologies of excavation and recording. Further, he is widely acclaimed as a major figure in the establishment of South Asian archaeology. However, many of his specific interpretations of archaeological sites have been discredited or reinterpreted, and in his personal life he was often criticised for bullying colleagues and sexually harassing young women.
Early life.
Childhood: 1890–1907.
Mortimer Wheeler was born on 10 September 1890 in the city of Glasgow, Scotland. He was the first child of the journalist Robert Mortimer Wheeler and his second wife Emily Wheeler ("née" Baynes). The son of a tea merchant based in Bristol, in youth Robert had considered becoming a Baptist minister, but instead became a staunch freethinker while studying at the University of Edinburgh. Initially working as a lecturer in English literature, Robert turned to journalism after his first wife died in childbirth. His second wife, Emily, shared her husband's interest in English literature, and was the niece of a Shakespearean scholar at St. Andrews University, Thomas Spencer Baynes. Their marriage was emotionally strained, a situation exacerbated by their financial insecurity. Within two years of their son's birth, the family moved to Edinburgh, where a daughter named Amy was born. The couple gave their two children nicknames, with Mortimer being "Boberic" and Amy being "Totsy".
When Wheeler was four, his father was appointed chief lead writer for the "Bradford Observer". The family relocated to Saltaire, a village northwest of Bradford, a cosmopolitan city in Yorkshire, northeast England, which was then in the midst of the wool trade boom. Wheeler was inspired by the moors surrounding Saltaire and fascinated by the area's archaeology, later describing discovering a late prehistoric cup-marked stone, searching for lithics on Ilkley Moor, and digging into a barrow on Baildon Moor. Although suffering from ill health, Emily Wheeler taught her two children with the help of a maid up to the age of seven or eight. Mortimer remained emotionally distant from his mother, instead being far closer to his father, whose company he favoured over that of other children. His father had a keen interest in natural history and a love of fishing and shooting, rural pursuits which he encouraged Mortimer to take part in. Robert acquired many books for his son, particularly on the subject of art history, with Wheeler loving to both read and paint.
In 1899, Wheeler joined Bradford Grammar School shortly before his ninth birthday, where he proceeded straight to the second form. In 1902 Robert and Emily had a second daughter, whom they named Betty; Mortimer showed little interest in this younger sister. In 1905, Robert agreed to take over as head of the London office of his newspaper, by then renamed the "Yorkshire Daily Observer", and so the family relocated to the southeast of the city in December, settling into a house named Carlton Lodge in South Croydon Road, West Dulwich. In 1908 they relocated to 14 Rollescourt Avenue in nearby Herne Hill. Rather than being sent for a conventional education, when he was 15 Wheeler was instructed to educate himself by spending time in London, where he frequented The National Gallery and the Victoria and Albert Museum.
University and early career: 1907–14.
After passing the entrance exam on his second attempt, in 1907 Wheeler was awarded a scholarship to read classical studies at University College London (UCL), commuting daily from his parental home to the university campus in Bloomsbury, central London. At UCL, he was taught by the prominent classicist A. E. Housman. During his undergraduate studies, he became editor of the "Union Magazine", for which he produced a number of illustrated cartoons. Increasingly interested in art, he decided to switch from classical studies to a course at UCL's art school, the Slade School of Fine Art; he returned to his previous subject after coming to the opinion that – in his words – he never became more than "a conventionally accomplished picture maker". This interlude had adversely affected his classical studies, and he received a second class BA on graduating.
Wheeler began studying for a Master of Arts degree in classical studies, which he attained in 1912. During this period, he also gained employment as the personal secretary of the UCL Provost Gregory Foster, although he later criticised Foster for transforming the university from "a college in the truly academic sense [into] a hypertrophied monstrosity as little like a college as a plesiosaurus is like a man". It was also at this time of life that he met Tessa Verney, a student then studying history at UCL, when they were both serving on the committee of the University College Literary Society. They entered into a relationship, resulting in Wheeler's first marriage.
During his studies, Wheeler had developed his love of archaeology, having joined an excavation of Viroconium Cornoviorum, a Romano-British settlement in Wroxeter, in 1913. Considering a profession in the discipline, he won a studentship that had been established jointly by the University of London and the Society of Antiquaries in memory of Augustus Wollaston Franks. The prominent archaeologist Sir Arthur Evans doubled the amount of money that went with the studentship. Wheeler's proposed project had been to analyse Romano-Rhenish pottery, and with the grant he funded a trip to the Rhineland in Germany, there studying the Roman pottery housed in local museums; his research into this subject was never published.
At this period, there were very few jobs available within British archaeology; as the later archaeologist Stuart Piggott related, "the young Wheeler was looking for a professional job where the profession had yet to be created." In 1913 Wheeler secured a position as junior investigator for the English Royal Commission on Historical Monuments, who were embarking on a project to assess the state of all structures in the nation that pre-dated 1714. As part of this, he was first sent to Stebbing in Essex to assess Late Medieval buildings, although once that was accomplished he focused on studying the Romano-British remains of that county. In summer 1914 he married Tessa in a low-key, non-religious wedding ceremony, before they moved into Wheeler's parental home in Herne Hill.
First World War: 1914–18.
"I cannot attempt to describe the conditions under which we are fighting. Anything I could write about them would seem exaggeration but would in reality be miles below the truth. The whole battlefield for miles is a congested mess of sodden, rain-filled shell-holes, which are being added to every moment. The mud is not so much mud as fathomless sticky morass... If it were not for the cement pill boxes left by the Boche, not a thing could live many hours."
— Wheeler, in a letter to his wife, October 1917.
After the United Kingdom's entry into World War I in 1914, Wheeler volunteered for the armed forces. Although preferring solitary to group activities, Wheeler found that he greatly enjoyed soldiering. For the next seven months, he was posted as an instructor in the University of London Officer Training Corps. It was during this period, in January 1915, that a son was born to the Wheelers, and named Michael. Michael was their only child, something that was a social anomaly at the time, although it is unknown if this was by choice or not. In May 1915, Wheeler was moved to the Royal Field Artillery (Territorial Force) and shortly thereafter was appointed captain. In this position he was stationed at various bases across Britain, often bringing his wife and child with him; his responsibility was as a battery commander, initially of field guns and subsequently of howitzers.
In October 1917 Wheeler was posted to the 76th Army Brigade RFA, which was then stationed in Belgium, where it had been engaged in the Battle of Passchendaele against German troops along the Western Front. There, he was immediately placed in charge of A Battery, replacing a major who had been poisoned by mustard gas. Being promoted to the position of acting major, he was part of the Left Group of artillery covering the advancing Allied infantry in the battle. Throughout, he continued a correspondence with his wife, sister, and parents. After the Allied victory in the battle, the brigade was transferred to Italy.
Wheeler and the brigade arrived in Italy on 20 November, and proceeded through the Italian Riviera to reach Caporetto, where it had been sent to bolster the Italian troops against a German and Austro-Hungarian advance. As the Russian Republic removed itself from the war, the German Army refocused its efforts on the Western Front, and so in March 1918 Wheeler's brigade was ordered to leave Italy, getting a train from Castelfranco to Vieux Rouen in France. Back on the Western Front, the brigade was assigned to the Second Division of Julian Byng's Third Army, reaching a stable area of the front in April. Here, Wheeler was engaged in artillery fire for several months, before the British went on the offensive in August. On 24 August, in between the ruined villages of Achiet and Sapignies, he led an expedition which captured two German field guns while under heavy fire from a castle mound; he was subsequently awarded the Military Cross for this action. Wheeler continued as part of the British forces pushing westward until the German surrender in November 1918. He was not demobilised for several months, instead being stationed at Pulheim in Germany until March; during this time he wrote up his earlier research on Romano-Rhenish poetry, making use of access to local museums, before returning to London in July 1919.
Career.
National Museum of Wales: 1919–26.
On returning to London, Wheeler moved into a top-floor flat near Gordon Square with his wife and child. He returned to working for the Royal Commission, examining and cataloguing the historic structures of Essex. In doing so, he produced his first publication, an academic paper on Colchester's Roman Balkerne Gate which was published in Essex Archaeological Society's "Transactions" in 1920. He soon followed this with two papers in the "Journal of Roman Studies"; the first offered a wider analysis of Roman Colchester, while the latter outlined his discovery of the vaulting for the city's Temple of Claudius which was destroyed by Boudica's revolt. In doing so, he developed a reputation as a Roman archaeologist in Britain. He then submitted his research on Romano-Rhenish pots to the University of London, on the basis of which he was awarded his Doctorate of Letters; thenceforth until his knighthood he styled himself as Dr. Wheeler. He was unsatisfied with his job in the Commission, unhappy that he was receiving less pay and a lower status than he had had in the army, and so began to seek out alternative employment.
He obtained a post as the Keeper of Archaeology at the National Museum of Wales, a job that also entailed becoming a lecturer in archaeology at the University College of South Wales and Monmouthshire. Taking up this position, he moved to Cardiff with his family in August 1920, although he initially disliked the city. The museum was in disarray; prior to the war, construction had begun on a new purpose-built building to house the collections. This had ceased during the conflict and the edifice was left abandoned during Cardiff's post-war economic slump. Wheeler recognised that Wales was very regionally divided, with many Welsh people having little loyalty to Cardiff; thus, he made a point of touring the country, lecturing to local societies about archaeology. The Wheelers' work for the cause of the museum has been seen as part of a wider "cultural-nationalist movement" linked to growing Welsh nationalism during this period; for instance, the Welsh nationalist party Plaid Cymru was founded in 1925.
Wheeler was impatient to start excavations, and in July 1921 started a six-week project to excavate at the Roman fort of Segontium; accompanied by his wife, he used up his holiday to oversee the project. A second season of excavation at the site followed in 1922. Greatly influenced by the writings of the archaeologist Augustus Pitt-Rivers, Wheeler emphasised the need for a strong, developed methodology when undertaking an archaeological excavation, believing in the need for strategic planning, or what he termed "controlled discovery", with clear objectives in mind for a project. Further emphasising the importance of prompt publication of research results, he wrote full seasonal reports for "Archaeologia Cambrensis" before publishing a full report, "Segontium and the Roman Occupation of Wales". Wheeler was keen on training new generations of archaeologists, and two of the most prominent students to excavate with him at Segontium were Victor Nash-Williams and Ian Richmond.
Over the field seasons of 1924 and 1925, Wheeler then ran excavations of the Roman fort of Y Gaer near Brecon, a project aided by his wife and two archaeological students, Nowell Myres and Christopher Hawkes. During this project, he was visited by the prominent Egyptologist Flinders Petrie and his wife Hilda Petrie; Wheeler greatly admired Petrie's emphasis on strong archaeological methodologies. Wheeler published the results of his excavation in "The Roman Fort Near Brecon". He then began excavations at Isca Augusta, a Roman site in Caerleon, where he focused on revealing the Roman amphitheatre. Intent on attracting press attention to both raise public awareness of archaeology and attract new sources of funding, he contacted the press and organised a sponsorship of the excavation by the middle-market newspaper, the "Daily Mail". In doing so, he emphasised the folkloric and legendary associations that the site had with King Arthur. In 1925, Oxford University Press published Wheeler's first book for a general audience, "Prehistoric and Roman Wales"; he later expressed the opinion that it was not a good book.
In 1924, the Director of the National Museum of Wales, William Evans Hoyle, resigned amid ill health. Wheeler applied to take on the role of his replacement, providing supportive testimonials from Charles Reed Peers, Robert Bosanquet, and H. J. Fleure. Although he had no prior museum experience, he was successful in his application and was appointed Director. He then employed a close friend, Cyril Fox, to take on the vacated position of Keeper of Archaeology. Wheeler's proposed reforms included extending the institution's reach and influence throughout Wales by building affiliations with regional museums, and focusing on fundraising to finance the completion of the new museum premises. He obtained a £21,367 donation from the wealthy shipowner William Reardon Smith and appointed Smith to be the museum's treasurer, and also travelled to Whitehall, London, where he successfully urged the British Treasury to provide further funding for the museum. As a result, construction on the museum's new building was able to continue, and it was officially opened by King George V in 1927.
London Museum: 1926–33.
Upon the retirement of the Keeper of the London Museum, Harmon Oates, Wheeler was invited to fill the vacancy. Having been considering a return to London for some time, he eagerly agreed, taking on the post, which was based at Lancaster House in the St James's area, in July 1926. This move caused much ill feeling in Wales, where many felt that Wheeler had simply taken the directorship of the National Museum to advance his own career prospectives, and that he had abandoned them when a better offer came along. Wheeler himself disagreed, believing that he had left Fox at the Museum as his obvious successor, and that the reforms he had implemented would therefore continue. The position initially provided Wheeler with an annual salary of £600, which resulted in a decline in living standards for his family, who moved into a flat near to Victoria Station.
Tessa's biographer L.C. Carr later commented that together, the Wheelers "professionalized the London Museum." Wheeler expressed his opinion that the museum "had to be cleaned, expurgated, and catalogued; in general, turned from a junk shop into a tolerably rational institution." Focusing on reorganising the exhibits and developing a more efficient method of cataloguing the artefacts, he also authored "A Short Guide to the Collections", before using the items in the museum to write three books: "London and the Vikings", "London and the Saxons", and "London and the Romans". Upon his arrival, the Treasury allocated the museum an annual budget of £5000, which Wheeler deemed insufficient for its needs. In 1930, Wheeler persuaded them to increase that budget, as he highlighted increasing visitor numbers, publications, and acquisitions, as well as a rise in the number of educational projects. With this additional funding, he was able to employ more staff and increase his own salary to £900.
Soon after joining the museum, Wheeler was elected to the council of the Society of Antiquaries. Through the Society, he became involved in the debate as to who should finance archaeological supervision of building projects in Greater London; his argument was that the City of London Corporation should provide the funding, although in 1926 it was agreed that the Society itself would employ a director of excavation based in Lancaster House to take on the position. Also involved in the largely moribund Royal Archaeological Institute, Wheeler organised its relocation to Lancaster House. In 1927, Wheeler took on an unpaid lectureship at University College London, where he established a graduate diploma course on archaeology; one of the first to enroll was Stuart Piggott. In 1928, Wheeler curated an exhibit at UCL on "Recent Work in British Archaeology", for which he attracted much press attention.
Wheeler was keen to continue archaeological fieldwork outside of London, undertaking excavations every year from 1926 to 1939. After completing his excavation of the Carlaeon amphitheatre in 1928, he began fieldwork at the Roman settlement and temple in Lydney Park, Gloucestershire, having been invited to do so by the aristocratic landowner, Charles Bathurst. It was during these investigations that Wheeler personally discovered the Lydney Hoard of coinage. Wheeler and his wife jointly published their excavation report in 1932 as "Report on the Excavation of the Prehistoric, Roman and Post-Roman Site in Lydney Park, Gloucestershire", which Piggott noted had "set the pattern" for all Wheeler's future excavation reports.
From there, Wheeler was invited to direct a Society of Antiquaries excavation at the Roman settlement of Verulamium, which existed on land recently acquired by the Corporation of St. Albans. He took on this role for four seasons from 1930 to 1933, before leaving a fifth season of excavation under the control of the archaeologist Kathleen Kenyon and the architect A.W.G. Lowther. Wheeler enjoyed the opportunity to excavate at a civilian as opposed to military site, and also liked its proximity to his home in London. He was particularly interested in searching for a pre-Roman Iron Age oppidum at the site, noting that the existence of a nearby Catuvellauni settlement was attested to in both classical texts and numismatic evidence. With Wheeler focusing his attention on potential Iron Age evidence, Tessa concentrated on excavating the inside of the city walls; Wheeler had affairs with at least three assistants during the project. After Tessa wrote two interim reports, the final excavation report was finally published in 1936 as "Verulamium: A Belgic and Two Roman Cities", jointly written by Wheeler and his wife. The report resulted in the first major published criticism of Wheeler, produced by the young archaeologist Nowell Myres in a review for "Antiquity"; although stating that there was much to praise about the work, he critiqued Wheeler's selective excavation, dubious dating, and guesswork. Wheeler responded with a piece in which he defended his work and launched a personal attack on both Myres and Myres's employer, Christ Church, Oxford.
Institute of Archaeology: 1934–39.
Wheeler had long desired to establish an academic institution devoted to archaeology that could be based in London. He hoped that it could become a centre in which to establish the professionalisation of archaeology as a discipline, with systematic training of students in methodological techniques of excavation and conservation and recognised professional standards; in his words, he hoped "to convert archaeology into a discipline worthy of that name in all senses." He further described his intention that the Institute should become "a laboratory: a laboratory of archaeological science".
Many archaeologists shared his hopes, and to this end Petrie had donated much of his collection of Near Eastern artefacts to Wheeler, in the hope that it would be included in such an institution. Wheeler was subsequently able to convince the University of London, a federation of institutions across the capital, to support the venture, and both he and Tessa began raising funds from wealthy backers. In 1934, the Institute of Archaeology was officially opened, albeit at this point it only existed on paper, with no premises or academic staff; the first students to enroll were Rachel Clay and Barbara Parker, who went on to have careers in the discipline. While Wheeler – who was still Keeper of the London Museum – took on the role of Honorary Director of the Institute, he installed the archaeologist Kathleen Kenyon as secretary of the Management Committee, describing her as "a level-headed person, with useful experience".
After ending his work at Verulamium, Wheeler turned his attention to the late Iron Age hill-fort of Maidan Castle near to Dorchester, Dorset, where he excavated for four seasons from 1934 to 1937. Co-directed by Wheeler, Tessa, and the Curator of Dorset County Museum, Charles Drew, the project was carried out under the joint auspices of the Society of Antiquaries and the Dorset Field Club. With around 100 assistants per season, the dig constituted the largest excavation that had been conducted in Britain up to that point, with Wheeler organising weekly meetings with the press to inform them about any discoveries. His excavation report was be published in 1943 as "Maidan Castle, Dorset". The report's publication allowed further criticism to be voiced of Wheeler's approach and interpretations; in his review of the book, the archaeologist W.F. Grimes criticised the highly selective nature of the excavation, noting that Wheeler had not asked questions regarding the socio-economic issues of the community at Maidan Castle, aspects of past societies that had come to be of increasing interest to British archaeology. Over coming decades, as further excavations were carried out at the site and archaeologists developed a greater knowledge of Iron Age Britain, much of Wheeler's interpretation of the site and its development was shown to be wrong, in particular by the work of the archaeologist Niall Sharples.
In 1936, Wheeler embarked on a visit to the Near East, sailing from Marseilles to Port Said, where he visited the Old Kingdom tombs of Sakkara. From there he went via Sinai to Palestine, Lebanon, and Syria. During this trip, he visited various archaeological projects, but was dismayed by the quality of their excavations; in particular, he noted that the American-run excavation at Tel Megiddo was adopting standards that had been rejected in Britain twenty-five years previously. He was away for six weeks, and upon his return to Europe discovered that his wife Tessa had died of a pulmonary embolism after a minor operation on her toe. According to Tessa's biographer, for Wheeler this discovery was "the peak of mental misery, and marked the end of his ability to feel a certain kind of love." He received further bad news when his father then died that winter. By the summer of 1937, he had embarked on a new romance, with a young woman named Mavis de Vere Cole, who had first met Wheeler when visiting the Maidan Castle excavations with her then-lover, the painter Augustus John. After she eventually agreed to his repeated requests for marriage, the two were wedded in an early 1939 ceremony held at Caxton Hall, before their wedding reception took place at Shelley House. They proceeded on a honeymoon to the Middle East.
After a search that had taken several years, Wheeler was able to secure a premises for the Institute of Archaeology: St. John's Lodge in Regent's Park, central London. Left empty since its use as a hospital during the First World War, the building was owned by the Crown and was controlled by the First Commissioner of Works, William Ormsby-Gore; Ormsby-Gore was very sympathetic to archaeology, and leased the building to the Institute at a low rent. The St. John's Lodge premises was officially opened on 29 April 1937. During his speech at the ceremony, the University of London's Vice-Chancellor Charles Reed Peers made it clear that the building was only intended as a temporary home for the Institute, which it was hoped would be able to move to Bloomsbury, the city's academic hub. In his speech, the university's Chancellor, Alexander Cambridge, 1st Earl of Athlone, compared the new institution to both the Institute of Historical Research and the Courtauld Institute of Art.
Wheeler had also become President of the Museums Association, and in a presidential address given in Belfast talked on the topic of preserving museum collections in war time, believing that Britain's involvement in a second European conflict was imminent. In anticipation of this event, in August 1939 he arranged for the London Museum to place many of its most important collections into safe keeping. He was also awarded an honorary doctorate from Bristol University, and at the award ceremony met the Conservative Party politician Winston Churchill, who was then engaged in writing his multi-volume "History of the English-Speaking Peoples"; Churchill asked Wheeler to aid him in writing about late prehistoric and early medieval Britain, to which the latter agreed.
After Maidan Castle, Wheeler turned his attention to France, where the archaeological investigation of Iron Age sites had lagged behind developments in Britain. There, he oversaw a series of surveys and excavations with the aid of Leslie Scott, beginning with a survey tour of Brittany in the winter of 1936–37. After this, Wheeler decided to excavate the oppidum at Camp d'Artus, near Huelgoat, Finistère. Alongside bringing a number of British archaeologists to work on the site, he hired six local Breton workmen to assist the project, coming to the belief that the oppidum had been erected by local Iron Age tribes to defend themselves from the Roman invasion led by Julius Caesar. Meanwhile, Scott had been placed in charge of an excavation at the smaller nearby hillfort of Kercaradec, near Quimper. In July 1939, the project focused its attention on Normandy, with excavations beginning at the Iron Age hillforts of Camp de Canada and Duclair. They were brought to an abrupt halt in September 1939 as the Second World War broke out in Europe, and the team evacuated back to Britain. Wheeler's excavation report, co-written with Katherine Richardson, was eventually published as "Hill-forts of Northern France" in 1957.
Second World War: 1939–45.
Wheeler had been expecting and openly hoping for war with Nazi Germany for several years; he believed that the United Kingdom's involvement in the conflict would remedy the shame that he thought had been brought upon the country by its signing of the Munich Agreement in September 1938. Volunteering for the armed services, he was assigned to raise the 48th Light Anti-Aircraft Battery at Enfield, where he set about recruiting volunteers, including his son. As the 48th swelled in size, Wheeler's unit was transferred to the 42nd Mobile Light Anti-Aircraft Regiment in the Royal Artillery. Given the nickname of "Flash Alf" by those serving under him, he was recognised as a ruthless disciplinarian and was blamed by many for the death of one of his soldiers from influenza during training. Having been appointed secretary of the Society of Antiquaries in 1939 and then director in 1940, he travelled to London to deal with society affairs on various occasions. Cole had meanwhile entered into an affair with a man named Clive Entwistle, who lambasted Wheeler as "that whiskered baboon". When Wheeler discovered Entwistle in bed with his wife in May 1941, he initiated divorce proceedings, with the divorce being finalised in March 1942. Meanwhile, it was in 1941 that Wheeler was awarded a Fellowship of the British Academy.
In the summer of 1941, Wheeler and three of his batteries were assigned to fight against German and Italian forces in the North African Campaign. In September, they set sail from Glasgow aboard the "Empress of Russia" battleship; because the Mediterranean was controlled largely by enemy naval forces, they were forced to travel via the Cape of Good Hope, before taking shore leave in Durban. There, Wheeler visited the local kraals to compare them with the settlements of Iron Age Britain. The ship subsequently docked in Aden, where Wheeler and his men again took shore leave. They soon reached the British-controlled Suez, where they disembarked and were stationed on the shores of the Great Bitter Lake. There, Wheeler took a brief leave of absence to travel to Jerusalem, where he visited Petrie on his hospital deathbed. Back in Egypt, he gained permission to fly as a front gunner in a Wellington bomber on a bombing raid against Axis forces, to better understand what it was like to be against an anti-aircraft battery.
Serving with the Eighth Army, Wheeler was present in North Africa when the Axis armies pushed the Allies back to El Alamein. He was also part of the Allied counter-push, taking part in the Second Battle of El Alamein and the advance on Axis-held Tripoli. On the way he became concerned that the archaeological sites of North Africa were being threatened both by the fighting and the occupying forces. After the British secured control of Libya, Wheeler visited Tripoli and Leptis Magna, where he found that Roman remains had been damaged and vandalised by British troops; he brought about reforms to prevent this, lecturing to the troops on the importance of preserving archaeology, making many monuments out-of-bounds, and ensuring that the Royal Air Force changed its plans to construct a radar station in the midst of a Roman settlement. Aware that the British were planning to invade and occupy the Italian island of Sicily, he insisted that measures be introduced to preserve the historic and archaeological monuments on the island.
Promoted to the position of brigadier, after the German surrender in North Africa Wheeler was sent to Algiers where he was part of the staff committee planning the invasion of Italy. There, he learned that the India Office had requested that the army relieve him of his duties to permit him to be appointed Director General of Archaeology in India. Although he had never been to the country, he agreed that he would take the job on the condition that he be permitted to take part in the invasion of Italy first. As intended, Wheeler and his 12th Bridage then took part in the invasion of Sicily and then mainland Italy, where they were ordered to use their anti-aircraft guns to protect the British 10th Corps. As the Allies advanced north through Italy, Wheeler spent time in Naples and then Capri, where he met various aristocrats who had anti-fascist sympathies.
Wheeler left Italy in November 1943 and returned to London. There, he resigned as the director of London Museum and focused on organising the Institute of Archaeology, preparing it for its adoption of a new director, V. Gordon Childe, after the war. He also resigned as director of the Society of Antiquaries, but was appointed the group's representative to the newly formed Council for British Archaeology. He developed a relationship with a woman named Kim Collingridge, and asked her to marry him. As she was a devout Roman Catholic, he officially converted to the religion, something which shocked many of his friends, who believed that he was being dishonest because he did not genuinely believe in the doctrines of the faith. He then set sail for Bombay aboard the "City of Exeter" ship in February 1944.
Archaeological Survey of India: 1944–48.
Wheeler arrived in Bombay in the spring of 1944. There, he was welcomed by the city's governor, John Colville, before heading by train to Delhi and then Simla, where the headquarters of the Archaeological Survey of India were located. Wheeler had been suggested for the job by Archibald Wavell, the Viceroy of India, who had been acting on the recommendations of the archaeologist Leonard Woolley, who had authored a report lamenting the state of the archaeological establishment in the British-controlled subcontinent. Wheeler recognised this state of affairs, in a letter to a friend complaining about the lack of finances and equipment, commenting that "We're back in 1850". He initially found much to dislike in India, and in his letters to friends in Britain expressed derogatory and racist sentiments toward Indians: he stated that "they feed wrongly and think wrongly and live wrongly... I already find myself regarding them as ill-made clockwork toys rather than as human beings, and I find myself bullying them most brutally." He expelled those staff members whom he deemed too idle, and physically beat others in an attempt to motivate them.
From the beginning of his tenure, he sought to distance himself from previous Director-Generals and their administrations, by criticising them in print and attempting to introduce new staff who had no loyalty to his predecessors. Assigned with a four-year contract, Wheeler attempted to recruit two archaeologists from Britain, Glyn Daniel and Stuart Piggott, to aid him in reforming the Archaeological Survey, although they declined the offer. He then toured the subcontinent, seeking to meet all of the Survey's staff members. He had drawn up a prospectus containing research questions that he wanted the Survey to focus on; these included understanding the period between the Bronze Age Indus Valley Civilization and the Achaemenid Empire, discerning the socio-cultural background to the Vedas, dating the Aryan invasion, and establishing a dating system for southern India prior to the sixth century CE. During his time in office he also achieved a 25 per cent budget increase for the Archaeological Survey, and convinced the government to agree to the construction of a National Museum of Archaeology, to be built in New Delhi.
In October 1944, he opened his six-month archaeological field school in Taxila, where he instructed various students from across India in the methodologies of the discipline. Wheeler became very fond of his students, with one of them, B. B. Lal, commenting that "behind the gruff exterior, Sir Mortimer had a very kind and sympathetic heart." Throughout his period in India, his students were some of the only individuals whom Wheeler warmed to; more widely, he was annoyed by what he saw as the idleness, incompetence and corruption of Indian society. Initially focusing on the northwest of the subcontinent, Wheeler was particularly fascinated by the Bronze Age Indus Valley Civilization. On his initial inspection of the Indus Valley sites of Mohenjo-daro and Harappa, he organised a very brief excavation which revealed fortifications around both settlements. He subsequently led a more detailed excavation at Harappa, where he exposed further fortifications and established a stratigraphy for the settlement.
Turning his attention to southern India, Wheeler discovered remnants of a Roman amphora in a museum, and began excavations at Arikamedu, revealing a port from the first century CE which had traded in goods from the Roman Empire. The excavation had been plagued by severe rains and tropical heat, although it was during the excavation that World War II ended; in celebration, Wheeler gave all his workers an extra rupee for the day. It has since been alleged that while Wheeler took credit for discovering the significance of this sight, it had previously been established by A. Aiyappan, the Superintendent of the Government Museum in Madras, and the French archaeologist Jouveau Dubreuil, with Wheeler intentionally ignoring their contribution. He later undertook excavations of six megalithic tombs in Brahmagiri, Mysore, which enabled him to gain a chronology for the archaeology of much of southern India.
Wheeler established a new archaeological journal, "Ancient India", planning for it to be published twice a year. He had trouble securing paper and faced various delays; the first issue was released in January 1946, and he would release three further volumes during his stay. Wheeler married Kim Collingridge in Simla, before he and his wife took part in an Indian Cultural Mission to Iran. The Indian government had deemed Wheeler ideal to lead the group, which departed via train to Zahidan before visiting Persepolis, Tehran, Isfahan, Shiraz, Pasargadae, and Kashan. Wheeler enjoyed the trip, and was envious of Tehran's archaeological museum and library, which was far in advance of anything then found in India. Crossing into Iraq, in Baghdad the team caught a flight back to Delhi. In 1946, he was involved in a second cultural mission, this time to Afghanistan, where he expressed a particular interest in the kingdom of ancient Bactria and visited the archaeology of Balkh.
Wheeler was present during the 1947 Partition of India into the Dominion of Pakistan and the Union of India and the accompanying ethnic violence between Hindu and Muslim communities. He was unhappy with how these events had affected the Archaeological Survey, complaining that some of his finest students and staff were now citizens of Pakistan and no longer able to work for him. He was based in New Delhi when the city was rocked by sectarian violence, and attempted to help many of his Muslim staff members escape from the Hindu-majority city unharmed. He further helped smuggle Muslim families out of the city hospital, where they had taken refuge from a violent Hindu mob. As India neared independence from the British Empire, the political situation had changed significantly; by October 1947 he was one of the last British individuals in a high-up position within the country's governing establishment, and recognised that many Indian nationalists wanted him to also leave.
As their relationship had become increasingly strained, his wife had left and returned to Britain. Although hoping to leave his post in India several months early, he was concerned for his economic prospects, and desperately searched for a new job position. Through friends in the British archaeological community, he was offered a job as the Secretary of the Royal Commission on Ancient Monuments for Wales, although was upset that this would mean a drop in his professional status and income and decided to turn it down. Instead, he agreed to take up a chair in the Archaeology of the Roman Provinces at the Institute of Archaeology. In addition, the Pakistani Minister of Education invited him to become the Archaeological Adviser to the Pakistani government; he agreed to also take up this position, on the agreement that he would only spend several months in the country each year over the next three.
Later life.
Between Britain and Pakistan: 1948–52.
Returning to London, Wheeler moved into the Hallam Street flat where his son and daughter-in-law were living. Wheeler and the latter disliked each other, and so in summer 1950 he moved out and began renting an apartment in Mount Street. A year later he moved into his wife's house in Mallord Street, in an unsuccessful hope of reigniting their relationship.
Taking up his part-time professorship at the Institute of Archaeology, he began to lecture to students almost every day. There, he found that he developed a relationship of mutual respect with the director, Childe, despite their strong personal and theoretical differences. After the retirement of Cyril Fox, in April 1949 Wheeler was nominated for the Presidency of the Society of Antiquaries, but lost to James Mann; many archaeologists, including Childe and O. G. S. Crawford, resigned from the Society in protest, deeming Wheeler to have been a far more appropriate candidate for the position. Wheeler was nevertheless elected director of the Society. In 1950, he was then awarded the Petrie Medal, and in 1952 was knighted. That same year he was invited to give the Norton lectures for the Archaeological Institute of America, and while in the United States was also awarded the Lucy Wharton Drexel medal at Pennsylvania. He nevertheless disliked the country, and in later life exhibited anti-Americanism.
Wheeler spent three months in Pakistan during early 1949, where he was engaged in organising the fledgling Pakistani Archaeological Department with the aid of former members of the Archaeological Survey and new students whom he recruited. The Minister of Education, Falzur Rahman, was sympathetic to Wheeler's plans, and the government agreed to establish a National Museum of Pakistan in Karachi, which opened in April 1950. Wheeler himself was appointed the first President of the Pakistani Museums Association, and found himself as a mediator in the arguments between India and Pakistan over the redistribution of archaeological and historic artefacts following the partition. He also wrote a work of archaeological propaganda for the newly formed state, "Five Thousand Years of Pakistan" (1950).
To instruct new Pakistani students in the methods of archaeology, in early 1950 Wheeler ran a training excavation at Mohenjo-daro; there, he was joined by the British student Leslie Alcock, who spoke both Punjabi and Urdu and who was appointed a site supervisor by Wheeler. This excavation proved to be the only one for which Wheeler would not write and publish a full excavation report. Instead, he made reference to its findings in his book "The Indus Civilization", published as part of the series on The Cambridge History of India. His relationship with the Pakistani government had become strained, and so he declined to return to work for them for his the third year.
Wheeler had been keen to return to excavation in Britain. Based on that which he had organised in India, Wheeler developed an archaeological training course, which he ran at Verulamium in the summer of 1949 to instruct British students in the methodologies of excavation. In summer 1950, he was invited by the Royal Commission on Historical Monuments to direct a trial excavation at Bindon Hill in Dorset. It was a leisurely project which he treated as a seaside holiday. He was subsequently invited by the Ancient Monuments Department of the Ministry of Works to excavate the Stanwick Iron Age Fortifications in North Riding, Yorkshire, which he proceeded to do over the summers of 1951 and 1952. Aided by many old friends and colleagues from within the British archaeological scene, among those who joined him were Alcock and Alcock's wife. Wheeler published his report on the site in 1954.
In 1949 Wheeler was appointed Honorary Secretary of the British Academy after Frederic G. Kenyon stepped down from the position. According to Piggott, the institution had "unhappily drifted into senility without the excuse of being venerable", and Wheeler devoted much time attempting to revitalise the organisation and ensured that Charles Webster was appointed President. Together, Wheeler and Webster sought to increase the number of younger members of the Academy, increasing the number of Fellows who were permitted to join and proposing that those over 75 years of age not be permitted to serve on the organisation's council; this latter measure was highly controversial, and though defeated in 1951, Wheeler and Webster were able to push it through in 1952. In doing so, Piggott stated, Wheeler helped rid the society of its "self-perpetuating gerontocracy." To aid him in these projects, Wheeler employed a personal assistant, Molly Myers, who remained with him for the rest of his life.
Popular fame: 1952–69.
In 1956, Wheeler retired from his part-time professorship at the Institute of Archaeology. Childe was also retiring from his position of director that year, and Wheeler involved himself in the arguments surrounding who should replace him. Wheeler vocally opposed the nomination of W.F. Grimes, deeming his career undistinguished; instead, he championed Glyn Daniel as a candidate, although ultimately Grimes was selected. That year, Wheeler's marriage broke down, and he moved from his wife's house to a former brothel at 27 Whitcomb Street in central London. From 1954 to 1959, he served as the President of the Society of Antiquaries, and after resigning supporting Ian Richmond as his replacement; however, Joan Evans was selected. From 1964 to 1966 he served as Chairman of the Ancient Monuments Board, stepping down when he concluded that he was too old for the role.
In December 1963, Wheeler underwent a prostate operation that went wrong, resulting in him being hospitalised for over a month.
In November 1967, Wheeler became a Companion of Honour, and in 1968 he became a Fellow of the Royal Society.
Media fame and public archaeology.
Wheeler became famous in Britain as "the embodiment of popular archaeology through the medium of television." In 1952, Wheeler was invited to be a panelist on the new BBC television series, "Animal, Vegetable, Mineral?". Based on the American quiz programme "What in the World?", the show was hosted by Glyn Daniel and featured three experts in archaeology, anthropology, and natural history being asked to identify artefacts which had been selected from various museums. The show proved popular with British audiences, and would air for six more years. The series brought Wheeler to public attention, resulting in his being awarded the Television Personality of the Year award in 1954. He subsequently appeared in an episode of "Buried Treasure", an archaeology show also hosted by Daniel, in which the pair travelled to Denmark to discuss Tollund Man. In 1957, he appeared in a second episode of "Buried Treasure", for which he travelled to Pakistan to discuss that nation's archaeology, and in 1958 again appeared in an episode, this time on the site of Great Zimbabwe in Southern Rhodesia. In 1959 he presented his own three-part series on "The Grandeur That Was Rome", for which he travelled to Hadrian's Wall, Pompeii, and Leptis Magna; the show failed to secure high ratings, and was Wheeler's last major foray into television. Meanwhile, he also made appearances on BBC radio, initially featuring on the John Irving series "The Archaeologist", but later presenting his own eight-part series on Roman Britain and also appearing on the series "Asian Club", which was aimed primarily at newly arrived migrants from the Indian subcontinent.
From 1954 onward, Wheeler began to devote an increasing amount of his time to encouraging greater public interest in archaeology, and it was in that year that he obtained an agent. That year, Oxford University Press published two of his books. The first was a book on archaeological methodologies, "Archaeology from the Earth", which was translated into various languages. The second was "Rome Beyond the Imperial Frontier", discussing evidence for Roman activity at sites like Arikamedu and Segontium. In 1955 Wheeler released his episodic autobiography, "Still Digging", which had sold over 70,000 copies by the end of the year. In 1959, Wheeler wrote "Early India and Pakistan", which was published as part as Daniel's "Ancient Peoples and Places" series for Thames and Hudson; as with many earlier books, he was criticised for rushing to conclusions.
He authored the section on "Ancient India" for Piggott's edited volume, "The Dawn of Civilisation", which was published by Thames and Hudson in 1961, before writing an introduction for Roger Wood's photography book, "Roman Africa in Colour", which was also published by Thames and Hudson. He then agreed to edit a series for the publisher, known as "New Aspects of Antiquity", through which they released a variety of archaeological works. The rival publisher Weidenfeld & Nicolson had also persuaded Wheeler to work for them, securing him to write many sections of their book, "Splendours of the East". They also published his 1968 book "Flames Over Persopolis", in which Wheeler discussed Persopolis and the Persian Empire in the year that it was conquered by Alexander the Great.
In 1954, the tour company R.K. Swan invited Wheeler to provide lectures on the archaeology of ancient Greece aboard their Hellenic cruise line, which he did in 1955. In 1957, he then gave a guided tour of the archaeology of the Indian subcontinent for the rival tour company Fairways and Swinford. Subsequently, Swans appointed him as one of their paid directors, being chairman of their Hellenic Cruise division; thenceforth, he made two fortnight tours a year, in spring and summer. In late 1969 he then conducted the Swans tour to the Indian subcontinent, visiting the south and east of the republic as well as Ceylon. During this period, Wheeler had kept in contact with many of his friends and colleagues in India and Pakistan, helping to secure them work and funding where possible.
Wheeler had continued his archaeological investigations, and in 1954 led an expedition to the Somme and Pas de Calais where he sought to obtain more information on the French Iron Age to supplement that gathered in the late 1930s. Pakistan's Ministry of Education invited Wheeler to return to their country in October 1956. Here, he undertook test excavations at Charsada to determine a chronology of the site. In 1965, he agreed to take on the position of President of the Camelot Research Committee, which had been established to promote the findings of excavations at Cadbury Castle in Somerset run by his friends Ralegh Radford and Alcock; the project ended in 1970. He also agreed to sit as Chairman of the Archaeological Committee overseeing excavations at York Minster, work which occupied him into the 1970s. Wheeler had also continued his work with museums, campaigning for greater state funding for them. While he had become a trustee of the institution in 1963, he achieved publicity for vocally criticising the British Museum as "a mountainous corpse", lambasting it as being poorly managed and overcrowded with artefacts. The BBC staged a public debate with the museum director Frank Francis.
British Academy and UNESCO.
As Honorary Secretary of the British Academy, Wheeler focused on increasing the organisation's revenues, thus enabling it to expand its remit. He developed personal relationships with various employees at the British Treasury, and offered the Academy's services as an intermediary in dealing with the Egypt Exploration Society, the British School at Athens, the British School at Rome, the British School at Ankara, the British School in Iraq, and the British School at Jerusalem, all of which were then directly funded independently by the Treasury. Accepting this offer, the Treasury agreed to double its funding of the Academy to £5000 a year. Approaching various charitable foundations, from 1955 Wheeler also secured funding from both the Pilgrim Trust and the Nuffield Foundation, and in 1957 then secured additional funding from the Rockefeller Foundation.
With this additional money, the Academy was able to organise a survey of the state of the humanities and social sciences in the United Kingdom, authoring a report that was published by Oxford University Press in 1961 as "Research in the Humanities and the Social Sciences". On the basis of this report, Wheeler was able to secure a dramatic rise in funding from the British Treasury; they increased their annual grant to £25,000, and promised that this would increase to £50,000 shortly after. According to his later biographer Jacquetta Hawkes, in doing so Wheeler raised the position of the Academy to that of "the main source of official patronage for the humanities" within the United Kingdom, while Piggott stated that he set the organisation upon its "modern course".
To improve Britain's cultural influence abroad, Wheeler had been urging the establishment of a British Institute of History and Archaeology in East Africa, touring East Africa itself in August 1955. In 1956 the Academy requested £6000 from the Treasury to fund this new institution, to which they eventually agreed in 1959. The Institute was initially established in Dar es Salaam in 1961, although later relocated to Nairobi. Meanwhile, Wheeler had also been campaigning for the establishment of a British Institute of Persian Studies, a project which was supported by the British Embassy in Tehran; they hoped that it would rival the successful French Institute in the city. In 1960, the Treasury agreed, with the new institution being housed on the premises of the University of Tehran. He further campaigned for the establishment of a British Institute in Japan, although these ideas were scrapped amid the British financial crisis of 1967.
Wheeler retained an active interest in the running of these British institutions abroad; in 1967 he visited the British School in Jerusalem amid the Six-Day War between Israel and its Arab neighbours, and in January 1968 visited the Persian institute with the archaeologist Max Mallowan and Mallowan's wife Agatha Christie, there inspecting the excavations at Siraf. In 1969 he proceeded to the Italian city of Rome to inspect the British School there. That year, he resigned as Honorary Secretary of the Academy. The position subsequently became a salaried, professional one, with the numismatist Derek Allen taking on the position.
Recognising his stature within the archaeological establishment, the government appointed Wheeler as the British representative on a UNESCO project to undertake a programme of rescue archaeology in the Nile Valley ahead of the construction of the Aswan Dam, which was going to flood large areas of Egypt and Sudan. Personally securing UK funding for the project, he deemed it an issue of national and personal shame when he was unable to persuade the British government to supply additional funding for the relocation of the Abu Simbel temples. In October 1968, he then took part in a UNESCO visit to Pakistan to assess the state of Mohenjo-daro, writing the project's report on how the archaeological site could best be preserved. His involvement with the international organisation continued for the rest of his life, for in March 1973 he was invited to a UNESCO conference in Paris.
Final years: 1970–76.
During his final years, Wheeler remained involved in various activities, for instance sitting on the advisory panel of the "Antiquity" journal and the Management Committee of the Royal Archaeological Institute. In March 1971, the archaeologist Barry Cunliffe and a number of his undergraduate students at the University of Southampton organised a conference on the subject of "The Iron Age and its Hillforts" to celebrate Wheeler's eightieth birthday. Wheeler attended the event, with the conference proceedings subsequently being published as a festschrift for the octogenarian. In spring 1973, Wheeler returned to BBC television for two episodes of the archaeology-themed series "Chronicle" in which he discussed his life and career. The episodes were well received, and Wheeler became a close friend of the show's producer, David Collison.
In the 1970s, Wheeler became increasingly forgetful and came to rely largely on his assistant, Molly Myres, to organise his affairs. Amid increasing ill health, in September 1973 he moved full-time into Myres's house in Leatherhead, Surrey, although he continued to use his central London flat during day-trips to the city. There, he authored a final book, "My Archaeological Mission to India and Pakistan", although much of the text was culled from his previous publications; it was published by Thames and Hudson in 1976. After suffering a stroke, Wheeler died at Myers' home on 22 July 1976. In memoriam, the British Academy, Royal Academy, and Royal Society flew their flags at half-mast. Wheeler's funeral was held with military trappings at a local crematorium, while a larger memorial service was held in St James's Church, Piccadilly in November.
Personal life.
Wheeler was known as "Rik" among friends. He divided opinion among those who knew him, with some loving and others despising him, and during his lifetime he was often criticised on both scholarly and moral grounds. The archaeologist Max Mallowan asserted that he "was a delightful, light-hearted and amusing companion, but those close to him knew that he could be a dangerous opponent if threatened with frustration." 
His charm offensives were often condemned as being insincere. During excavations, he was known as an authoritarian leader, but favoured those whom he thought exhibited bravery by standing up to his authority. Hence, he has been termed "a benevolent dictator". He was meticulous in his writings, and would repeatedly revise and re-write both pieces for publication and personal letters. Throughout his life, he was a heavy smoker.
Wheeler expressed the view that he was "the least political of mortals". Despite not taking a strong interest in politics, Wheeler was described by his biographer as "a natural conservative"; for instance, during his youth he was strongly critical of the Suffragettes and their cause of greater legal rights for women. Nevertheless, he was "usually happy to advance young women professionally", something that may have been based largely on his sexual attraction toward them. He expressed little interest in his relatives; in later life he saw no reason to have a social relationship with people purely on the basis of family ties.
Wheeler was married three times. In May 1914, Wheeler married Tessa Verney. Tessa became an accomplished archaeologist, and they collaborated until she died in 1936. Their only child, a son Michael, was born in January 1915; he became a barrister. Following Tessa's death, in 1939, Wheeler married Mavis de Vere Cole, although their relationship was strained; Cole's diaries revealed that Wheeler physically hit her when she annoyed him. In 1945 Mortimer Wheeler married his third wife, Margaret "Kim" Collingridge, although they became estranged in 1956; they never divorced as a result of her devout Catholicism. Meanwhile, Wheeler was well known for his conspicuous promiscuity, favouring young women for one night stands, many of whom were his students. He was further known for having casual sex in public places. This behaviour led to much emotional suffering among his various wives and mistresses, of which he was aware. As a result of this behaviour, later archaeologist Gabriel Moshenska informed a reporter from the "Daily Mail" that Wheeler had developed a reputation as "a bit of a groper and a sex pest and an incredible bully as well".
Reception and legacy.
"He was a true innovator in archaeology, an inspired teacher, [and] had the dramatic gifts to enable him to spread his own enthusiasm among multitudes. He developed powers of command and creative administration that brought him extraordinary successes in energizing feeble institutions and creating new ones."
— Jacquetta Hawkes, 1982.
Wheeler has been termed "the most famous British archaeologist of the twentieth century" by archaeologists Gabriel Moshenska and Tim Schadla-Hall. Highlighting his key role in encouraging interest in archaeology throughout British society, they stated that his "mastery of public archaeology was founded on his keen eye for value and a showman's willingness to package
and sell the past." This was an issue that Wheeler felt very strongly about; writing his obituary for the "Biographical Memoirs of Fellows of the Royal Society", the English archaeologist Stuart Piggott noted that Wheeler placed "great importance to the archaeologist's obligation to the public, on whose support the prosecution of his subject ultimately depended."
Piggott believed that Wheeler's greatest impact was as "the great innovator in field techniques", comparing him in this respect to Pitt-Rivers. Piggott stated that the "importance of Wheeler's contribution to archaeological technique, enormous and far-reaching, lies in the fact that in the early 1920s he not only appreciated and understood what Pitt-Rivers had done, but saw that his work could be used as a basis for adaptation, development and improvement." L.H. Carr stated that it was for his methodological developments, oft termed "the Wheeler Method", that Wheeler was best known; in this she contrasted him with those archaeologists who were best known for their associations with a specific archaeological site, such as Arthur Evans and Knossos or Leonard Woolley and Ur.
Wheeler was well known for his publications on archaeological matters; Carr stated that both Wheeler and his first wife emphasised "technical rigour and a full presentation of materials unearthed, as well as a literary discussion of their meaning calculated to appeal to a larger audience." Focusing on Wheeler's publications regarding South Asian archaeology, Sudeshna Guha noted that he "produced an assemblage of image-objects that embodied the precision he demanded from excavation photography."
Mallowan noted that "Immediate and swift presentation of results was more important to him than profound scholarship, although his critical sense made him conscious that it was necessary to maintain high standards and he would approve of nothing that was slipshod." Jacquetta Hawkes commented that he made errors in his interpretation of the archaeological evidence because he was "sometimes too sure of being right, too ready to accept his own authority". She asserted that while Wheeler was not an original thinker, he had "a vision of human history that enabled him to see each discovery of its traces, however small, in its widest significance."
"Despite his very short stay as Director General, [Wheeler] infused an element of urgency into the Indian archaeological scene. With him archaeology in India became exciting, worth doing for its own sake. This excitement is apparent in the articles that he wrote, and still affects those who know the scene."
— Dilip K. Chakrabarti, 1982 
Piggott claimed that Wheeler's appointment as Director-General of the Archaeological Survey of India represented "the most remarkable archaeological achievement of his career, an enormous challenge accepted and surmounted in the autocratic and authoritarian terms within which he could best deploy his powers as administrator and excavator. No other archaeologist of the time, it seems fair to remark, could have come near to attaining his command of incisive strategy and often ruthless tactics which won him the bewildered admiration and touching devotion of his Indian staff." The Indian archaeologist Dilip K. Chakrabarti later stated that Wheeler's accomplishments while in India were "considerable", particularly given the socio-political turmoil of independence and partition. Chakrabarti stated that Wheeler had contributed to South Asian archaeology in various ways: by establishing a "total view" of the region's development from the Palaeolithic onward, by introducing new archaeological techniques and methodologies to the subcontinent, and by encouraging Indian universities to begin archaeological research. Ultimately, Chakrabarti was of the opinion that Wheeler had "prepared the archaeology of the subcontinent for its transition to modernity in the post-Partition period." Similarly, Peter Johansen praised Wheeler for systematising and professionalising Indian archaeology and for "instituting a clearly defined body of techniques and methods for field and laboratory work and training."
On Wheeler's death, H.D. Sankalia of Deccan College, Pune, described him as "well known among Old World archaeologists in the United States", particularly for his book "Archaeology from the Earth" and his studies of the Indus Valley Civilisation. In its 2013 obituary of the English archaeologist Mick Aston, "British Archaeology" magazine – the publication of the Council for British Archaeology – described Aston as "the Mortimer Wheeler of our times" because despite the strong differences between their personalities, both had done much to bring archaeology to the British public. However, writing in 2011, Moshenska and Schadla-Hall asserted that Wheeler's reputation has not undergone significant revision among archaeologists, but that instead he had come to be remembered as "a cartoonish and slightly eccentric figure" whom they termed "Naughty Morty".
Carr described the Institute of Archaeology as "one of the [Wheeler] couple's most permanent memorials."
Biographies and studies.
In 1960, Ronald William Clark published a biography titled "Sir Mortimer Wheeler". FitzRoy Somerset, 4th Baron Raglan reviewed the volume for the journal "Man", describing "this very readable little book" as being "adulatory" in tone, "but hardly more so than its subject deserves." In 1982, the archaeologist Jacquetta Hawkes published a second biography, "Mortimer Wheeler: Adventurer in Archaeology". Hawkes admitted she had developed "a very great liking" for Wheeler, having first met him when she was an archaeology student at the University of Cambridge. She believed that he had "a daemonic energy", with his accomplishments in India being "almost superhuman". Ultimately, she thought of him as being "an epic hero in an anti-heroic age" in which growing social egalitarianism had stifled and condemned aspects of his greatness.
In the 2000 film "Hey Ram", the lead character, Saket Ram (played by actor Kamal Haasan) and his friend, Amjad Khan (played by Shah Rukh Khan) are shown as employees of Wheeler before the 1947 Hindu-Muslim riots. In a 2003 volume of the "South Asian Studies" journal, Sudeshna Gusha published a research article examining Wheeler's use of photography in his excavations and publications in the Indian subcontinent.
In 2011, the academic journal "Public Archaeology" published a research paper by Moshenska and Schadla-Hall that analysed Wheeler's role in presenting archaeology to the British public. Two years later, the "Papers from the Institute of Archaeology" issued a short comic strip by Moshenska and Alex Salamunovich depicting Wheeler's activities in studying the archaeology of Libya during World War II.
Bibliography.
A bibliography of Wheeler's published books was included by Piggott in his obituary, and again by Hawkes in her biography.

</doc>
<doc id="50695" url="http://en.wikipedia.org/wiki?curid=50695" title="Augustus Pitt Rivers">
Augustus Pitt Rivers

Augustus Henry Lane-Fox Pitt Rivers (14 April 1827 – 4 May 1900) was an English army officer, ethnologist, and archaeologist. He was noted for innovations in archaeological methodology, and in the museum display of archaeological and ethnological collections. His international collection of about 22,000 objects was the founding collection of the Pitt Rivers Museum at the University of Oxford while his collection of English archaeology from the area around Stonehenge forms the basis of the collection at The Salisbury Museum in Wiltshire. 
Throughout most of his life he used the surname Lane Fox, under which his early archaeological reports are published. In 1880 he adopted the Pitt Rivers name on inheriting from Lord Rivers an estate of more than 32000 acres in Cranborne Chase. His name will ever be a prominent landmark in the history of the progress of archaeology and ethnology.
Early life and family.
Born Augustus Henry Lane-Fox at Bramham cum Oglethorpe near Wetherby in Yorkshire, he was the son of William Lane-Fox and Lady Caroline Douglas, sister of George Douglas, 17th Earl of Morton. George Lane-Fox and Sackville Lane-Fox were his uncles.
In 1880, Lane-Fox inherited the estates of his cousin, Horace Pitt-Rivers, 6th Baron Rivers and with it the remainder of the Richard Rigby fortune. It was "an event that transformed his life." He was required to adopt the surname Pitt Rivers as part of the bequest. Pitt Rivers married Alice Stanley (1828–1910), daughter of the politician Edward Stanley, 2nd Baron Stanley of Alderley and women's education campaigner Henrietta Stanley, Baroness Stanley of Alderley. Three notable descendants of his are his grandson, the anthropologist, eugenicist, anti-Semite and detainee in 1940 under Defence Regulation 18B George Pitt-Rivers, his great-grandson, the anthropologist and ethnographer, Julian A. Pitt-Rivers, and his great-great-grandson, William Fox-Pitt, the equestrian. Another grandson was Michael Pitt-Rivers who gained notoriety in Britain in the 1950s when he was put on trial charged with "buggery".
Military career.
Lane-Fox had a long and successful military career as a staff officer. He was educated at the Royal Military College, Sandhurst, for six months at the age of fourteen and was commissioned into the Grenadier Guards on 16 May 1845 as an ensign. In the course of a thirty-two year military career, albeit much leave-interrupted, he only once saw major front line action, at Alma in 1854. In 1851 he became a member of the committee to experiment and report on the respective merits of the army’s smoothbore muskets. He was appointed to Woolwich to instruct in the use of the new Minié rifle in 1852. Subsequently, he was largely responsible for founding the Hypthe school of Musketry in Kent and became its principle instructor, revising its "Instruction of Musketry Manuel". The remainder of his service career revolved around musketry instruction and in 1858 he published a paper‘On the improvement of the rifle as a weapon for general use’. He bought a promotion to Captain on 2 August 1850. He was promoted to the brevet rank of lieutenant-colonel of the army "for distinguished Service in the Field" during the Crimean War. On 15 May 1857, he bought the rank of lieutenant-colonel in the Grenadier Guards. The then Brevet-Major Lane-Fox, was appointed a member of the Fifth Class of the Order of the Medjidie in 1858 for "distinguished services before the enemy during the [Crimean War]". He was promoted to colonel on 22 January 1867. Pitt Rivers retired in 1882 and was accorded the honorary rank of Lt General.
Archaeological career.
Pitt Rivers' interests in archaeology and ethnology began in the 1850s, during postings overseas, and he became a noted scientist while he was a serving military officer. He was elected, in the space of five years, to the Ethnological Society of London (1861), the Society of Antiquaries of London (1864) and the Anthropological Society of London (1865). By the time he retired he had amassed ethnographic collections numbering tens of thousands of items from all over the world. Influenced by the evolutionary writings of Charles Darwin and Herbert Spencer, he arranged them typologically and (within types) chronologically. He viewed archaeology as an extension of anthropology and, as consequence, built up matching collections of archaeological and ethnographic objects to show longer developmental sequences – to support his views on cultural evolution. This style of arrangement, designed to highlight evolutionary trends in human artefacts, was a revolutionary innovation in museum design. Pitt Rivers' ethnological collections form the basis of the Pitt Rivers Museum which is still one of Oxford's attractions. His researches and collections cover periods from the Lower Paleolithic to Roman and Medieval times, and extend all over the world. The Pitt Rivers Museum curates more than half a million ethnographic and archaeological artifacts, photographic and manuscript collections from all parts of the world. The museum was founded in 1884 when the university accepted the gift of more than 20,000 artifacts from Pitt Rivers. The collections continue to grow, and the museum has been described as one of the “six great ethnological museums of the world”. Pitt Rivers' Wessex Collection is housed in The Salisbury Museum in the city of Salisbury near Stonehenge. The new Wessex Gallery of archaeology opened there in 2014, funded by the Heritage Lottery Fund, and other sources. Pitt Rivers and other early archaeologists such as William Stukeley who first investigated the prehistory of Wiltshire, Cranborne Chase, Avebury and Stonehenge, are celebrated in the new Wessex Gallery.
The estates Pitt Rivers inherited in 1880 contained a wealth of archaeological material from the Roman and Saxon periods. He excavated these over seventeen seasons, from the mid-1880s until his death. His approach was highly methodical by the standards of the time, and he is widely regarded as the first scientific archaeologist to work in Britain. His most important methodological innovation was his insistence that "all" artefacts, not just beautiful or unique ones, be collected and catalogued. This focus on everyday objects as the key to understanding the past broke decisively with past archaeological practice, which verged on treasure hunting. It is Pitt Rivers' most important, and most lasting, scientific legacy. His work inspired Mortimer Wheeler among others to add to the scientific approach of archaeological excavation techniques.
Pitt Rivers created the Larmer Tree Gardens, a public pleasure garden, on the Rushmore estate near Tollard Royal in Wiltshire.
From 1882 Pitt Rivers was Britain's first Inspector of Ancient Monuments: a post created by anthropologist and parliamentarian John Lubbock who married Pitt Rivers' daughter, Alice. Charged with cataloguing archaeological sites and protecting them from destruction, he worked with his customary methodical zeal but was hampered by the limitations of the law, which gave him little real power over the landowners on whose property the sites stood.
In 1884 he served as High Sheriff of Dorset.
Advocate for cremation.
Pitt Rivers was an advocate for cremation at a time when such a practice was illegal in England. Even though many people believed that it was immoral to destroy a corpse, the cremation movement favored a practical way to dispose of bodies. Pitt Rivers was cremated after his death in 1900.
References.
Published by: Royal Anthropological Institute of Great Britain and Ireland
Article Stable URL: http://www.jstor.org/stable/2793146

</doc>
<doc id="50702" url="http://en.wikipedia.org/wiki?curid=50702" title="Environmental engineering">
Environmental engineering

Environmental engineering is the integration of sciences and engineering principles to improve the natural environment, to provide healthy water, air, and land for human habitation and for other organisms, and to clean up pollution sites. Environmental engineering can also be described as a branch of applied science and technology that addresses the issue of energy preservation, production asset and control of waste from human and animal activities. Furthermore, it is concerned with finding plausible solutions in the field of public health, such as waterborne diseases, implementing laws which promote adequate sanitation in urban, rural and recreational areas. It involves waste water management and air pollution control, recycling, waste disposal, radiation protection, industrial hygiene, environmental sustainability, and public health issues as well as a knowledge of environmental engineering law. It also includes studies on the environmental impact of proposed construction projects.
Environmental engineers study the effect of technological advances on the environment. To do so, they conduct studies on hazardous-waste management to evaluate the significance of such hazards, advise on treatment and containment, and develop regulations to prevent mishaps. Environmental engineers also design municipal water supply and industrial wastewater treatment systems as well as address local and worldwide environmental issues such as the effects of acid rain, global warming, ozone depletion, water pollution and air pollution from automobile exhausts and industrial sources.
At many universities, environmental engineering programs follow either the department of civil engineering or the department of chemical engineering at engineering faculties. Environmental "civil" engineers focus on hydrology, water resources management, bioremediation, and water treatment plant design. Environmental "chemical" engineers, on the other hand, focus on environmental chemistry, advanced air and water treatment technologies and separation processes.
Additionally, engineers are more frequently obtaining specialized training in law (J.D.) and are utilizing their technical expertise in the practices of environmental engineering law.
Most jurisdictions also impose licensing and registration requirements.
Development.
Ever since people first recognized that their health and well-being were related to the quality of their environment, they have applied thoughtful principles to attempt to improve the quality of their environment. The ancient Harappan civilization utilized early sewers in some cities. The Romans constructed aqueducts to prevent drought and to create a clean, healthful water supply for the metropolis of Rome. In the 15th century, Bavaria created laws restricting the development and degradation of alpine country that constituted the region's water supply.
The field emerged as a separate environmental discipline during the middle third of the 20th century in response to widespread public concern about water and pollution and increasingly extensive environmental quality degradation. However, its roots extend back to early efforts in public health engineering. Modern environmental engineering began in London in the mid-19th century when Joseph Bazalgette designed the first major sewerage system that reduced the incidence of waterborne diseases such as cholera. The introduction of drinking water treatment and sewage treatment in industrialized countries reduced waterborne diseases from leading causes of death to rarities.
In many cases, as societies grew, actions that were intended to achieve benefits for those societies had longer-term impacts which reduced other environmental qualities. One example is the widespread application of the pesticide DDT to control agricultural pests in the years following World War II. While the agricultural benefits were outstanding and crop yields increased dramatically, thus reducing world hunger substantially, and malaria was controlled better than it ever had been, numerous species were brought to the verge of extinction due to the impact of the DDT on their reproductive cycles. The story of DDT as vividly told in Rachel Carson's "Silent Spring" (1962) is considered to be the birth of the modern environmental movement and the development of the modern field of "environmental engineering."
Conservation movements and laws restricting public actions that would harm the environment have been developed by various societies for millennia. Notable examples are the laws decreeing the construction of sewers in London and Paris in the 19th century and the creation of the U.S. national park system in the early 20th century.
Scope.
Solid waste management.
Solid waste management is the collection, transport, processing or disposal, managing, and monitoring of solid waste materials. The term usually relates to materials produced by direct or indirect human activity, and the process is generally undertaken to reduce their effect on health, the environment, or aesthetics. Waste management is a distinct practice from resource recovery, which focuses on delaying the rate of consumption of natural resources. The management of wastes treats all materials as a single class, whether solid, liquid, gaseous, or radioactive substances, and the objective is to reduce the harmful environmental impacts of each through different methods.
Environmental impact assessment and mitigation.
Scientists have air pollution dispersion models to evaluate the concentration of a pollutant at a receptor or the impact on overall air quality from vehicle exhausts and industrial flue gas stack emissions. To some extent, this field overlaps the desire to decrease carbon dioxide and other greenhouse gas emissions from combustion processes.
They apply scientific and engineering principles to evaluate if there are likely to be any adverse impacts to water quality, air quality, habitat quality, flora and fauna, agricultural capacity, traffic impacts, social impacts, ecological impacts, noise impacts, visual (landscape) impacts, etc. If impacts are expected, they then develop mitigation measures to limit or prevent such impacts. An example of a mitigation measure would be the creation of wetlands in a nearby location to mitigate the filling in of wetlands necessary for a road development if it is not possible to reroute the road.
In the United States, the practice of environmental assessment was formally initiated on January 1, 1970, the effective date of the National Environmental Policy Act (NEPA). Since that time, more than 100 developing and developed nations either have planned specific analogous laws or have adopted procedure used elsewhere. NEPA is applicable to all federal agencies in the United States.
Water supply and treatment.
Engineers and scientists work to secure water supplies for potable and agricultural use. They evaluate the water balance within a watershed and determine the available water supply, the water needed for various needs in that watershed, the seasonal cycles of water movement through the watershed and they develop systems to store, treat, and convey water for various uses. Water is treated to achieve water quality objectives for the end uses. In the case of a potable water supply, water is treated to minimize the risk of infectious disease transmission, the risk of non-infectious illness, and to create a palatable water flavor. Water distribution systems are designed and built to provide adequate water pressure and flow rates to meet various end-user needs such as domestic use, fire suppression, and irrigation.
Wastewater treatment.
There are numerous wastewater treatment technologies. A wastewater treatment train can consist of a primary clarifier system to remove solid and floating materials, a secondary treatment system consisting of an aeration basin followed by flocculation and sedimentation or an activated sludge system and a secondary clarifier, a tertiary biological nitrogen removal system, and a final disinfection process. The aeration basin/activated sludge system removes organic material by growing bacteria (activated sludge). The secondary clarifier removes the activated sludge from the water. The tertiary system, although not always included due to costs, is becoming more prevalent to remove nitrogen and phosphorus and to disinfect the water before discharge to a surface water stream or ocean outfall.
Air pollution management.
Scientists have developed air pollution dispersion models to evaluate the concentration of a pollutant at a receptor or the impact on overall air quality from vehicle exhausts and industrial flue gas stack emissions. To some extent, this field overlaps the desire to decrease carbon dioxide and other greenhouse gas emissions from combustion processes.
Environmental Protection Agency.
The U.S. Environmental Protection Agency (EPA) is one of the many agencies that work with environmental engineers to solve key issues. An important component of EPA’s mission is to protect and improve air, water, and overall environmental quality in order to avoid or mitigate the consequences of harmful effects.
Ecological engineering for sustainable agriculture in arid and semiarid West African regions.
Ecological engineering offers new alternatives for the management of agricultural systems that are more tailored to the ever-changing social and environmental necessities in these regions. This requires managing the complexity of agrosystems, while striving to mimic the functioning of natural ecosystems of West African drylands and taking advantage of traditional practices and local know-how resulting from a long process of adaptation to environmental constraints.
Education.
Courses aimed at developing graduates with specific skills in environmental systems or environmental technology are becoming more common and fall into broad classes:

</doc>
<doc id="50705" url="http://en.wikipedia.org/wiki?curid=50705" title="Construction engineering">
Construction engineering

Construction engineering is a professional discipline that deals with the designing, planning, construction, and management of infrastructures such as highways, bridges, airports, railroads, buildings, dams, and utilities. These Engineers are unique such that they are a cross between civil engineers and construction managers. Construction engineers learn the designing aspect much like civil engineers and construction site management functions much like construction managers. 
The primary difference between a construction engineer and a construction manager is that the construction engineer has the ability to sit for the Professional Engineer license (PE) whereas a construction manager cannot. At the educational level, construction managers are not as focused on design work as they are on construction procedures, methods, and people management. Their primary concern is to deliver a project on time, within budget, and of the desired quality. 
The difference between a construction engineer and civil engineer is only at the educational level as both disciplines are able to sit for the PE exam giving them the same title of engineer. Civil engineering students concentrate more on the design work, gearing them toward a career as a design professional. This essentially requires them to take a multitude of design courses. Construction engineering students take design courses as well as construction management courses. This allows them to understand both the design functions as well as the building requirements needed to design and build today's infrastructures.
Work activities.
Depending on which career the construction engineer has chosen to follow, an entry-level design engineer normally provides support to project managers and assist with creating conceptual designs, scopes, and cost estimates for the planning and construction of approved projects. It should be noted that a career in design work does require a professional engineer license (PE). Individuals who pursue this career path are strongly advised to sit for the Engineer In Training exam (EIT) while in college as it takes five years (4 years in USA) post graduate to obtain the PE license. 
Entry-level construction manager positions are typically called project engineers or assistant project engineers. They are responsible for preparing purchasing requisitions, processing change orders, preparing monthly budgeting reports, and handling meeting minutes. The construction management position does not necessarily require a PE license; however possessing one does make the individual more marketable, as the PE license allows the individual to sign off on temporary structure designs.
Abilities.
Construction engineers are problem solvers, they help create infrastructure that best meets the unique demands of its environment. They must be able to understand infrastructure life cycles and have the perspective to solve technical challenges with clarity and imagination. Therefore individuals should have a strong understanding of maths and science, but many other skills are required, including critical and analytical thinking, time management, people management and good communication skills.
Educational requirements.
Individuals looking to obtain a construction engineering degree must first ensure that the program is accredited by EAC or Technology Accreditation Commission (TAC) of the Accreditation Board for Engineering and Technology (ABET). ABET accreditation is assurance that a college or university program meets the quality standards established by the profession for which it prepares its students. In the US there are currently twenty-five programs that exist in the entire country so careful college consideration is advised.
A typical construction engineering curriculum is a mixture of engineering mechanics, engineering design, construction management and general science and mathematics. This usually leads to a Bachelor of Science degree. The B.S. degree along with some design or construction experience is sufficient for most entry level positions. Graduate schools may be an option for those who want to go further in depth of the construction and engineering subjects taught at the undergraduate level. In most cases construction engineering graduates look to either civil engineering, engineering management, or business administration as a possible graduate degree.
Job prospects.
Job prospects for construction engineers generally have a strong cyclical variation. For example, starting in 2008 - continuing until at least 2011 - job prospects have been poor due to the collapse of housing bubbles in many parts of the world. This sharply reduced demand for construction, forced construction professionals towards infrastructure construction and therefore increased the competition faced by established and new construction engineers. This increased competition, and a core reduction in quantity demand is in parallel with a possible shift in the demand for construction engineers due to the automation of many engineering tasks, overall resulting in reduced prospects for construction engineers. In early 2010 the United States construction industry had a 27% unemployment rate, this is nearly three times higher than the 9.7% national average unemployment rate. The construction unemployment rate (including tradesmen) is comparable to the United States 1933 unemployment rate - the lowest point of the Great Depression - of 25%.
Remuneration.
The average salary for a civil engineer in the UK depends on the sector, and more specifically the level of experience of the individual. A 2010 survey of the remuneration and benefits of those occupying jobs in construction and the built environment industry showed that the average salary of a civil engineer in the UK is £29,582. In the United States, as of May 2013, the average was $85,640. The average salary varies depending on experience, for example the average annual salary for a civil engineer with between 3 and 6 years experience is £23,813. For those with between 14 and 20 years experience the average is £38,214.
References.
 technology. Retrieved from http://catalog.njit.edu/undergraduate/programs/constructioneng.php
 Engineering and Technology. Retrieved from http://www.abet.org/ 

</doc>
<doc id="50709" url="http://en.wikipedia.org/wiki?curid=50709" title="Gundam">
Gundam

Gundam (Japanese: ガンダム, Hepburn: Gandamu), also referred to as the Gundam Series (ガンダムシリーズ, Gandamu Shirīzu) is a science fiction media franchise created by Sunrise that feature giant robots (or "mecha") called "mobile suits", with titular mobile suits that carry the name "Gundam." 
The franchise started on April 7, 1979, as an anime TV series called "Mobile Suit Gundam", which was revolutionary in that it defined the real robot genre of anime by featuring giant robots in a militaristic war setting. The popularity of the first TV series and the merchandising that followed spawned a franchise that has come to include works released in numerous media. It is regarded as the "Star Wars" or "Star Trek" of Japan with respect of its popularity since its creation. Titles have appeared in the form of multiple television series and OVAs, movies, manga, novels, and video games. The franchise has also led to the creation of one of the biggest toy and hobby franchises in the Japanese toy industry.
As of 2014, the "Gundam" franchise generated a total of 80 billion yen in revenues. The 2014 retail sales of Gundam toy and hobby items totaled 18.4 billion yen. In the 2008 ranking of average sales figures for anime copies sold in Japan (1970-2008 total sales figures averaged by episode), "Gundam" series were in four of the top five places: "Mobile Suit Gundam" ranked second, with "Mobile Suit Gundam SEED Destiny" third, "Mobile Suit Gundam SEED" fourth, and "Mobile Suit Zeta Gundam" fifth. Also, "Mobile Suit Gundam Wing" ranked 18th and "Mobile Suit Gundam ZZ" ranked 20th. Gunpla (Gundam Plastic model) holds 90% of the Japan character plastic model market.
Academics in Japan have also viewed the series as inspiration, with the International Gundam Society being the first academic institution based on an animated TV series.
Overview.
Concept.
"Mobile Suit Gundam" was principally developed by renowned animator Yoshiyuki Tomino, along with a changing group of Sunrise creators who went under the collective pseudonym of "Hajime Yatate".
During its conceptual phase, the series was titled "Freedom Fighter Gunboy", or simply "Gunboy" for the gun the robot was armed with, and the primary target demographic were shōnen (boys). In the early production stages, there were numerous references to the word "freedom": the White Base was originally "Freedom's Fortress", the "Core Fighter" was the "Freedom Wing", and the "Gunperry" was the "Freedom Cruiser". The Yatate team combined the English word "gun" with the last syllable of the word "freedom" to form the name Gundom. Tomino then changed it to the current title, suggesting that "Gundam" signified a powerful unit wielding a gun powerful enough to hold back enemies, like a hydroelectric dam holding back floods. In keeping with this concept, Gundam in all media that followed are often depicted as singularly unique or limited-production, with much higher capabilities than mass-produced units.
Most Gundam are large, bipedal, humanoid-shaped vehicles controlled from cockpits by a human pilot. The majority of these "mobile suits" have a cockpit in the "torso" of the machine, with a camera built into the "head" to transmit images to the cockpit and are non-sentient machines.
Innovation.
"Mobile Suit Gundam" is said to have pioneered the real robot subgenre of mecha anime.
Unlike its super robot cousins, "Mobile Suit Gundam" attempted a realism in the robot design and weaponry, by running out of energy and ammunition or breaking and malfunctioning. The technology is practical and is either derived from true science (such as Lagrange points in space and the O'Neill cylinder as a living environment) or at least well-explained, feasible technology, requiring only a few fictional elements to function (such as Minovsky Physics as a means of energy production from helium-3).
The necessity of developing humanoid robots is also explained, albeit fictional. The fictional Minovsky particle pervasive in Universal Century is depicted as interfering with radar-guided long-distance cruise missiles, anti-aircraft guns, missiles, and all early warning systems, with weapons systems having to rely on human eyes. In Universal Century, the space-based Principality of Zeon rebels against Earth Federation, requiring a weapons system that could function in zero and normal gravity and be able to open and close air locks, plant demolition charges, and engage with enemy tanks and planes; with a robotic giant being an excellent choice. Once mobile suits have been developed by one side, the opposing force had to develop a similar system, just as British invention of tanks lead to the development of tanks in Germany, and eventually led to tank-to-tank battles.
Narrative.
The general narratives of "Gundam" shows classify as war drama. They revolve around the mobile suits and their pilots fighting in a war, in which destruction and dehumanization are inherent, through multiple sides; each faction has their own heroes and villains, all of which have their own unique motives, failings, and virtues. "Gundam" features political battles and debates on important philosophical issues and political ideals on the nature and meaning of war, the ideal of pacifism, and the continuing evolution — natural or engineered — of humanity and its consequences. These are often framed in the series as a debate between the protagonist and antagonist over the course of a duel, as they try to convince each other of the righteousness of their causes. Most of the stories are structured as "coming-of-age" dramas, where the main cast's personalities, points of view, allegiances, goals, and actions may or may not change dramatically as events unfold. This makes the plot seem more realistic than earlier super robot animated series where the hero and cast usually act in the same predictable manner, with little connection between the episodes. The best example of this is how the personalities of longtime rivals, Amuro Ray and Char Aznable, are influenced by their experiences in the "Gundam" saga.
Timelines.
The majority of "Gundam" animation, including the earliest series, occur in the Universal Century (UC) calendar era, with later series set in alternate calendars or timelines mostly unrelated to the UC system that have begun to be connected in the canon with Correct Century. The creation of the separate timelines were originally stand-alone works that did not require prior knowledge of the Universal Century timeline to understand or appreciate the story's background. These timelines define Gundam differently and portray conflicts in entirely different settings and circumstances to other entries, including the definition of Gundam. For example, the original Gundam was considered a military general-purpose prototype mobile suit and a "Gundam" from G Gundam is considered a name for a mobile fighter whose purpose is to compete against other Gundams.
Definition of Gundam.
Within the Gundam franchise, the titular term "Gundam" generally refers to a specific category of mobile suits that tend to feature design cues from Kunio Okawara's original Gundam design. Nonetheless, producer Masahiko Asano wrote in his notes on the production of "Gundam Sentinel" that everyone seemed to have their own idea of Gundam and, in the meeting, they were trying to find the asymptotic view for those there. The variations in naming and identification impacts the definition of the term throughout the "Gundam" timelines:
Spinoffs.
Over the years, other types of narratives and settings have been developed as the franchise grew and diversified. "SD Gundam", a sub-franchise of "Gundam" that started in the mid-1980s, features Gundam mecha and characters expressed in super deformed and anthropomorphic style while placing them in historical, fantastical or science fiction settings, with greater emphasis on comedy and adventure. In addition, a recent development in "Gundam" animated works is to feature a more contemporary setting and use Gunpla as a central plot element, as seen in shows like "Model Suit Gunpla Builders Beginning G" and "Gundam Build Fighters".
Franchise.
TV series, films, and video.
MSG Dates
Manga and novels.
The manga narration of the original series is published in English in North America by a variety of companies, such as Viz Media, Del Rey Manga, and TOKYOPOP, among others, and in Singapore by Chuang Yi.
Video games.
Following the popularity of "Gundam", various video games feature original characters previously not found in other media. Over 80 different Gundam games have been created for arcade, computer and console platforms. A video game series based on the popular Dynasty Warriors video game resulted in the games. Some of the video games would go on to have spinoff novels and manga. Most Gundam video games can only be found in Japan with little release to the world market ( series is an exception), however Gundam video games are region-lock free - making them accessible to users outside Japan .
Gundam model.
Models of the Gundam robots are a major reason for the franchise's enduring success, which began in the 1980s. Hundreds of models, primarily plastic but sometimes featuring resin, metal and other types of detail parts, have been released. These range in quality from children's toy kits, to hobbyist and museum-grade models. Most models are of 1:35, 1:48, 1:60, 1:100 or 1:144 scale. Special promotional models of 1:6 or 1:12 scale are targeted to retailers and are not commercially available. One full size model was also constructed and displayed in Tokyo's Odaiba, and later in Shizuoka.
Other Merchandise.
Bandai, the primary licensee of the "Gundam" trademark, makes a variety of products for the Gundam fan. Other companies produce unofficial merchandise such as toys, models, and T-shirts. Categories of products include the "Mobile Suit In Action" ("MSiA") action figures, and Gundam Model Kits in several scales and design complexity. Generally, each series listed above will have its own set of products, although the MSiA and models lines, such as Master Grade and High Grade Universal Century, may extend across series. The most popular line of action figure in recent year; however, is the "Gundam Fix" series. This line of figures include the mecha shown in the animated series/manga/novels, but also included new accessories to create a more updated version. In addition to Master Grade and High Grade Gundams, Bandai released yet another series of Gundam model in 2010 for the 30th anniversary of Gundam. The release of the real grade Gundam series let to an evolutionary way to building Gundam kits; real grade Gundam series combined the detailed inner structures of master grade versions and added an additional colour separation making the tiny 1/144 scaled real grade series complex in design and compact in size. After the introduction of the RG Gundam series, Bandai released the Metal Build series in March 2011 beginning with the 00 (double 0) Gundam. The Metal Build Gundam is more than an expensive action figure capable of multiple poses made available through the fixed joints, it's a game changer. Each Metal Build release features the best that Bandai has to offer in toy engineering, design, sculpt, posability, durability, and what many Gundam enthusiasts wish for, heavy on diecast content. Gundam model kits and action figures coexist with the Gundam animated series, Perfect grade Gundams and 1/64 series are also continuous products of Bandai's production line.
Internet.
Bandai maintains a number of sites to promote various Gundam projects. Most prominent amongst these is "Gundam Perfect Web", the official Japanese site. Its English language counterpart is the US maintained "Gundam Official". For a brief trial period in 2005, the site hosted the "Gundam Official User Forum". On July 2, 2013 another "Gundam Official" site will be launched. These forums were based on the existing fan forum, "Gundam Watch", and made use of many of its staff. When the project was retired, Gundam Watch was reborn, before passing the torch onto "Gundam Evolution", which maintained many of the same traditions and staff.
A number of series specific websites have been produced. These are often available for a limited time, usually to promote a DVD release. Common content includes character and mecha listings, lists of related merchandise and pay-for-download content. "Special" pages are also frequent, often presenting downloadable wallpaper or a small game. The "Superior Defender Gundam Force" site, for example, offers a game where players take the role of the villain Commander Sazabi, attempting to blast his subordinate with his weapons. After completion, users are rewarded with a papercraft of the Ark fans featured frequently in the show's second half.
Global Debut.
Since 1980, Gundam has been seen all over the world, here's a list of countries that Gundam has debuted in:
Impact.
Gundam is a popular cultural icon of Japan; it is a 50 billion yen business of Bandai Namco (projected 50 billion yen income of the company and reached a highest number of 54.5 billion yen in 2006). Not only were stamps published, an employee of the Agriculture Ministry was reprimanded for contribution to Japanese Wikipedia Gundam related pages, the Japanese Self Defense Forces code-named its developing advance personal combat system as Gundam, and the Fire department used Gundam to promote the future of fire fighting developments. A tram station stood a monument of the original Gundam and used the main theme of the first Gundam anime as its departure melody and other businesses like Mitsubishi not only created a test-type simulator for concept cars with a version of Gundam cockpit, it also held recruitment seminars using "How to make a Gundam" as a demo of what their development process is and based their Lancer Evolution design on Gundam. Isuzu also used a Gundam to model the VX2. A conference as a preparation for the "International Gundam Society" (国際ガンダム学会) was held on the August 24 in Hiroshima, using Gundam as the main topic to discuss about the relationship of the science and technology in science fiction anime and the real world. The "Gundam" metaverse makes regular appearances in the "Super Robot Wars" series by Banpresto.
"Gundam"'s realistic scientific setting has gained a reputation in the field itself as well. On July 18, 2007, when MIT's Astronautics Department's Professor Dava Newman displayed a biosuit, the suit was referenced as "Mobile Suit Gundam's Normal Suit is now real" by various news agencies. On February 14, 2008, when NASA proposed research into nuclear thermal rockets, "Technobahn", a scientific journal in Japan, referred to the usage of nuclear thermal rocket engines on mobile suits in the "Gundam" universe.

</doc>
<doc id="50711" url="http://en.wikipedia.org/wiki?curid=50711" title="Middle English">
Middle English

Middle English (ME) refers to the dialects of the English language spoken in parts of the British Isles after the Norman conquest until the late 15th century. This stage of the development of the English language roughly followed the High to the Late Middle Ages.
Middle English developed out of Late Old English seeing many dramatic changes in its grammar, pronunciation and writing customs. The Middle English period ended about 1470, when a London-based dialect became the main standard (Chancery Standard) aided by the invention of the printing press. Unlike Old English which adopted similar writing customs, written Middle English displays a wide variety of scribal forms. The language of England as used after 1470 and up to 1650 is known as Early Modern English. By that time the variant of the Northumbrian dialect (prevalent in Northern England) and spoken in southeast Scotland was developing into the Scots language.
During the Middle English period many Old English grammatical features were simplified or disappeared. This includes the reduction (and eventual elimination) of some grammatical cases, the simplification of noun and adjective inflection and the simplification of verb conjugations. Middle English also saw a mass adoption of Norman-French vocabulary, especially words related to politics, law, the arts, religion and other courtly language. Much of this adoption was due to the emulation of the French-speaking Normans occupiers of England at the time. Everyday English vocabulary remained mostly Germanic. Pronunciation changed dramatically during the middle period especially vowel sounds and diphthongs with the beginning of the great vowel shift.
Little survives of early Middle English literature most likely due to the occupation of French speaking Normans and the prestige that came with writing in French rather than English. During the 14th century a new style of literature emerged with the works of notable poets such as Chaucer and John Wycliffe. Poets wrote both in the vernacular and courtly English. Chaucer's "Canterbury Tales" remains the most studied and read work of the period.
It is popularly believed that William Shakespeare wrote in Middle English, but he actually wrote in Early Modern English.
History.
Middle English developed out of Late Old English in Norman England (1066–1154) and was spoken throughout the Plantagenet era (1154–1485). The Middle English period ended about 1470, when the Chancery Standard, a form of London-based English, began to become widespread, a process aided by the introduction of the printing press to England by William Caxton in the late 1470s. By that time the variant of the Northumbrian dialect (prevalent in Northern England) spoken in southeast Scotland was developing into the Scots language.
The language of England as used after 1470 and up to 1650 is known as Early Modern English.
Important texts for the reconstruction of the evolution of Middle English out of Old English are the "Ormulum" (12th century), the "Ancrene Wisse" and the Katherine Group (early 13th century, see AB language) and "Ayenbite of Inwyt" (ca. 1340).
The second half of the 11th century was the transitional period from Late Old English to Early Middle English.
Early Middle English was the language of the 12th and 13th centuries.
Middle English was fully developed as a literary language by the second half of the 14th century. Late Middle English and the transition to Early Modern English took place from the early 15th century and is taken to have been complete by the beginning of the Tudor period in 1485.
Transition from Old English.
The Norman conquest of England in 1066 resulted in only limited culture shock. However, the conquest saw the replacement of top levels of English-speaking political and ecclesiastical hierarchies by the Norman-speaking rulers who used Latin for administrative purposes. Thus Norman came into use as a language of polite discourse and literature, and this fundamentally altered the role of Old English in education and administration, even though many Normans of the early period were illiterate and depended on the clergy for written communication and record-keeping. Even now, after nearly a thousand years, the Norman influence on the English language is still apparent, though it did not begin to affect Middle English until somewhat later.
Consider these pairs of Modern English words. The first of each pair is derived from Old English and the second is of Anglo-Norman origin: pig/ pork, chicken/ poultry, calf/ veal, cow/ beef, wood/ forest, sheep/ mutton, house/ mansion, worthy/ valuable, bold/ courageous, freedom/ liberty.
The role of Anglo-Norman as the language of government and law can be seen in the abundance of Modern English words for the mechanisms of government that derive from Anglo-Norman: "court", "judge", "jury", "appeal", "parliament". Also prevalent in Modern English are terms relating to the chivalric cultures that arose in the 12th century, an era of feudalism and crusading. Early on, this vocabulary of refined behavior began to work its way into English, imports of the Normans who made their mark on the English language as much as on the territory of England itself.
This period of trilingual activity developed much of the flexible triplicate "synonymy" of modern English. For instance, English has three words meaning roughly "of or relating to a king":
Likewise, Norman – and, later, French – influences led to some interesting word pairs in English, such as the following, which both mean "someone who defends":
The end of Anglo-Saxon rule did not of course change the language immediately. Although the most senior offices in the church were filled by Normans, Old English continued in use in chronicles such as the Peterborough Chronicle until the middle of the 12th century. The non-literate would have spoken the same dialects as before the Conquest, though these changed slowly until written records of them became available for study, which varies in different regions. Once the writing of Old English comes to an end, Middle English has no standard language, only dialects that derive from the dialects of the same regions in the Anglo-Saxon period.
Early Middle English.
Early Middle English (1100–1300) has a largely Anglo-Saxon vocabulary (with many Norse borrowings in the northern parts of the country), but a greatly simplified inflectional system. The grammatical relations that were expressed in Old English by the dative and locative cases are replaced in Early Middle English with prepositional constructions. This replacement is, however, incomplete: the Old English genitive "-es" survives in the modern Saxon genitive—it is now called the "possessive": e.g., the form "dog's" for the longer "of the dog". But most of the other case endings disappeared in the Early Middle English period, including most of the roughly one dozen forms of the definite article ("the"). The dual grammatical number (expressing exactly two of a thing) also disappeared from English during the Early Middle English period (apart from personal pronouns), further simplifying the language.
Deeper changes occurred in the grammar. Gradually, the wealthy and the government Anglicised again, although Norman (and subsequently French) remained the dominant language of literature and law until the 14th century, even after the loss of the majority of the continental possessions of the English monarchy. The new English language did not sound the same as the old; for, as well as undergoing changes in vocabulary, the complex system of inflected endings Old English had, was gradually lost or simplified in the dialects of spoken Middle English. This change was gradually reflected in its increasingly diverse written forms as well. The loss of case endings was part of a general trend from inflections to fixed word order that also occurred in other Germanic languages, and therefore cannot be attributed simply to the influence of French-speaking sections of the population: English did, after all, remain the language of the vast majority. It is also argued that Norse immigrants to England had a great impact on the loss of inflectional endings in Middle English. One argument is that, although Norse- and English-speakers were somewhat comprehensible to each other, the Norse-speakers' inability to reproduce the ending sounds of English words influenced Middle English's loss of inflectional endings. Another argument is that the morphological simplifications were caused by Romano-Britons who were bilingual in Old English and either Brittonic (which lacks noun case) or British Latin (which may have lacked noun case, like most modern Romance languages).
Late Middle English.
The Late Middle English period was a time of upheaval in England.
After the deposition of Richard II of England in 1399, the House of Plantagenet split into the House of Lancaster and the House of York, whose antagonism culminated in the Wars of the Roses (1455–1487). Stability came only gradually with the Tudor dynasty under Henry VII.
During this period of social change, with new rulers coming into positions of power, some of them from other parts of the country or from lower levels in society, many linguistic changes occurred.
Towards the end of the 15th century a more modern English began to emerge. Printing began in England in the 1470s, which helped stabilise the language. With a standardised, printed English Bible and Prayer Book being read to church congregations from the 1540s onward, a wider public became familiar with a uniform language, and the era of Modern English began.
Chancery Standard.
Chancery Standard was largely based on the London and East Midland dialects, since those areas were both political and demographic centers of English society. However, it used other dialect forms where they made meanings clearer; for example, the northern "they", "their" and "them" (derived from Scandinavian forms) were used rather than the London "hi/they", "hir" and "hem". This was perhaps because the London forms could be confused with words such as "he", "her" and "him". (However, the colloquial form written as "'em", as in "up and at 'em", may well represent a spoken survival of "hem" rather than a shortening of the Norse-derived "them".)
The clerks who used Chancery Standard would have been familiar with French and Latin, which must have influenced the forms they chose. Chancery Standard was not the only influence on later forms of English—its level of influence is disputed and a variety of spoken dialects continued to exist—but it provided a core around which Early Modern English could crystallize.
By the mid-15th century, Chancery Standard was used for most official purposes except by the Church, which still used Latin, and for some legal purposes, for which Law French and some Latin were used. It was disseminated around England by bureaucrats on official business and slowly gained prestige.
In the late 1490s and early 1500s, the early printer Richard Pynson favored Chancery Standard in his published works, and consequently pushed the English spelling further towards standardization.
Grammar.
With its simplified case-ending system, the grammar of Middle English is much closer to that of modern English than that of Old English. Compared with other Germanic languages, it is probably the most similar to that of modern West Frisian, one of English's closest relatives.
Nouns.
Middle English retains only two distinct noun-ending patterns from the more complex system of inflection in Old English. The early Modern English words "engel" (angel) and "name" (name) demonstrate the two patterns:
Some nouns of the "engel" type have an "-e" in the nominative/accusative singular, like the weak declension, but otherwise strong endings. Often these are the same nouns that had an "-e" in the nominative/accusative singular of Old English. (These in turn inherited from Proto-Germanic "ja"-stem and "i"-stem nouns.)
The strong -(e)s plural form has survived into Modern English. The weak -(e)n form is now rare in the standard language, used only in "oxen", "children", "brethren"; and it is slightly less rare in some dialects, used in "eyen" for "eyes", "shoon" for "shoes", "hosen" for "hose(s)", "kine" for "cows", and "been" for "bees".
Verbs.
As a general rule (and all these rules are general), the indicative first person singular of verbs in the present tense ends in -e ("ich here" — "I hear"), the second person in -(e)st ("þou spekest" — "thou speakest"), and the third person in -eþ ("he comeþ" — "he cometh/he comes"). ("þ" (the letter ‘thorn’) is pronounced, in this case, like the unvoiced "th" in "think", but, under certain circumstances, may be like the voiced "th" in "that").
Plural forms vary strongly by dialect, with southern dialects preserving the Old English -eþ, Midland dialects showing -en from about 1200 onward and northern forms using -es in the third person singular as well as the plural.
The past tense of weak verbs is formed by adding an -ed(e), -d(e) or -t(e) ending. The past-tense forms, without their personal endings, also serve as past participles, together with past-participle prefixes derived from Old English: i-, y- and sometimes bi-.
Strong verbs, by contrast, form their past tense by changing their stem vowel (binden → bound), as in Modern English.
Pronouns.
After the Conquest, English retained Old English pronouns, with the exception of the third person plural, a borrowing from Old Norse (the original Old English form clashed with the third person singular and was eventually dropped):
Many other variations are noted in Middle English sources due to difference in spellings and pronunciations. See {{Citation
 |publisher = Oxford University Press
 |publication-place = [London]
 |title = A Middle-English dictionary
 |url = http://openlibrary.org/books/OL7114246M/A_Middle_English_dictionary
 |author = Francis Henry Stratmann
 |edition = A Middle English dictionary
 |publication-date = 1891
 }} and "", A. L. Mayhew, Walter W. Skeat, Oxford, Clarendon Press, 1888.
Here are the Old English pronouns.
The first and second person pronouns in Old English survived into Middle English largely unchanged, with only minor spelling variations. In the third person, the masculine accusative singular became 'him'. The feminine form was replaced by a form of the demonstrative that developed into 'sche', but unsteadily—'heyr' remained in some areas for a long time. The lack of a strong standard written form between the 13th and the 15th centuries makes these changes hard to map.
The overall trend was the gradual reduction in the number of different case endings. The accusative case disappeared, but the six other cases were partly retained in personal pronouns, as in "he", "him", "his".
Orthography.
Unlike Old English, which tended largely to adopt Late West Saxon scribal conventions in the period immediately before the Norman conquest of England, written Middle English displays a wide variety of scribal (and presumably dialectal) forms. This diversity suggests the gradual end of the role of Wessex as a focal point and trend-setter for writers and scribes, the emergence of more distinct local scribal styles and written dialects, and a general pattern of transition of activity over the centuries that followed, as Northumbria, East Anglia, and London successively emerged as major centers of English literature, each with their own particular interests.
Pronunciation.
Generally, all letters in Middle English words were pronounced. (Silent letters in Modern English generally come from pronunciation shifts, which means that pronunciation is no longer closely reflected by the written form because of fixed spelling constraints imposed by the invention of dictionaries and printing.) Therefore 'knight' was pronounced ] (with a pronounced ⟨k⟩ and the ⟨gh⟩ as the ⟨ch⟩ in German 'Knecht'), not [ˈnaɪt] as in Modern English.
In earlier Middle English all written vowels were pronounced. By Chaucer's time, however, the final ⟨e⟩ had become silent in normal speech, but could optionally be pronounced in verse as the meter required (but was normally silent when the next word began with a vowel). Chaucer followed these conventions: -e is silent in 'kowthe' and 'Thanne', but is pronounced in 'straunge', 'ferne', 'ende', etc. (Presumably, the final ⟨y⟩ is partly or completely dropped in 'Canterbury', so as to make the meter flow.)
An additional rule in speech, and often in poetry as well, was that a non-final unstressed ⟨e⟩ was dropped when adjacent to only a single consonant on either side if there was another short 'e' in an adjoining syllable. Thus, 'every' sounds like "evry" and 'palmeres' like "palmers".
Toward latter part of Middle English, the Great Vowel Shift was changing the pronunciation of most long vowels from a Continental sound to a distinctly English sound. While Middle English was essentially written and spelled the way it sounded, the Great Vowel Shift caused many words to be pronounced differently from the Middle English spellings.
Archaic characters.
The following characters can be found in Middle English text, direct holdovers from the Old English Latin alphabet.
Other glyphs.
The English language was in flux due to the influence from other languages.
The letter "J", for example, was a foreign glyph used for multiple purposes. The letter originated as a simple typographical swash or variation on the letter "i", and so it is often identical to it in sound. (For example, the word "wife" is often spelled "wijf" and "paradise" is "paradijs" in Middle English.) Illiterate, native speakers of Middle English would have known of no difference between instances of "i" and "j". Sometimes, the written "j" was to be pronounced like a modern "y" rather than a homophone of "jay". Many Hebrew names and words translated into English (via the Latin Vulgate, the Greek Septuagint, or the Greek New Testament) used the letter "J" for the Hebrew Yodh, which has a sound similar to "y" (as in "Hallelujah"). The yodh was commonly transliterated as the Greek letter iota (ɩ), which looks and sounds a lot like the Middle English "i", although the yodh is really a voiced palatal approximant. Middle English words include Jerusalem, Juda, Jordan, Joseph, Joon, all of which are spelled similar in Modern English, however, the pronunciation would have followed the centuries-old Latin pronunciation, which used the voiced palatal approximant, similar to the "y" sound for each "J" (e.g. "Yer-oo-sa-lem").
There were certain foreign words, notably from Old French, that used the letter "j" for a different sound. The word "joie" (modern "joy"), derived from Old French and used in Wycliffe's Bible, was pronounced with a ; sound. The "j" from French sounded like the Old English sound, which was spelled "cȝ". As Middle English turned into Modern English, both "j" and "dg" would be used to represent these sounds as the "ȝ" glyph lost favor.
Just as Latin used both the "U" and the "V" for both the vowel "u" and the consonant "v," Middle English continued this diversity in writing. The letter "v" appeared at the beginning of a word whether it was the vowel or consonant, like vpon" for "upon" and vois" for "voice." The letter "u" served for either sound in the middle or at the end of a word, like "euentid" for "eventide" and "þou" for "thou."
Because Middle English was written primarily by scribes, clergymen, and educated laymen, many scribal abbreviations from Latin were used. Abbreviations were used for frequently used words and names. It was common for the Lollards to abbreviate the name of Jesus (as in Latin manuscripts) to "ihc". The letters "n" and "m" were often omitted by placing a mark above an adjacent letter, so the word "in" was written as the letter "i" with a horizontal mark above it, like ī. The word "þt" supplanted "that" due to space concerns. Various forms of the ampersand replaced the word "and."
Arabic numerals were not used in Middle English, so all notation was in Roman numerals.
Sample texts.
Ormulum, 12th century.
This passage explains the background to the Nativity:
Epitaph of John the smyth, died 1371.
An epitaph from a monumental brass in an Oxfordshire parish church:
Wycliffe's Bible, 1384.
From the Wycliffe's Bible, (1384):<br>
First version
1And it was don aftirward, and Jhesu made iorney by citees and castelis, prechinge and euangelysinge þe rewme of God, 2and twelue wiþ him; and summe wymmen þat weren heelid of wickide spiritis and syknessis, Marie, þat is clepid Mawdeleyn, of whom seuene deuelis wenten 3out, and Jone, þe wyf of Chuse, procuratour of Eroude, and Susanne, and manye oþere, whiche mynystriden to him of her riches.—Luke ch.8, v.1–3
Second version
1And it was don aftirward, and Jhesus made iourney bi citees and castels, prechynge and euangelisynge þe rewme of 2God, and twelue wiþ hym; and sum wymmen þat weren heelid of wickid spiritis and sijknessis, Marie, þat is clepid Maudeleyn, of whom seuene deuelis 3wenten out, and Joone, þe wijf of Chuse, þe procuratoure of Eroude, and Susanne, and many oþir, þat mynystriden to hym of her ritchesse.—Luke ch.8, v.1–3
And it came to pass afterward, that he went throughout every city and village, preaching and showing the glad tidings of the kingdom of God: and the twelve were with him, and certain women, which had been healed of evil spirits and infirmities, Mary called Magdalene, out of whom went seven devils, and Joanna the wife of Chuza Herod's steward, and Susanna, and many others, which ministered unto him of their substance.—Translation of Luke ch.8 v.1–3, from the New Testament
Chaucer, 1390s.
The following is the beginning of the general Prologue from "The Canterbury Tales" by Geoffrey Chaucer. The text was written in a dialect associated with London and spellings associated with the then-emergent Chancery Standard.
In modern prose:
When April with its sweet showers has pierced March's drought to the root, bathing every vein in such liquid by whose virtue the flower is engendered, and when Zephyrus with his sweet breath has also enlivened the tender plants in every wood and field, and the young sun is halfway through Aries, and small birds that sleep all night with an open eye make melodies (their hearts pricked by Nature), then people long to go on pilgrimages, and palmers seek foreign shores and distant shrines known in sundry lands, and especially they wend their way to Canterbury from every shire of England in order to seek the holy blessed martyr, who has helped them when they were sick.
Gower, 1390.
The following is the beginning of the Prologue from "Confessio Amantis" by John Gower.

</doc>
<doc id="50712" url="http://en.wikipedia.org/wiki?curid=50712" title="Regional science">
Regional science

Regional science is a field of the social sciences concerned with analytical approaches to problems that are specifically urban, rural, or regional. Topics in regional science include, but are not limited to location theory or spatial economics, location modeling, transportation, migration analysis, land use and urban development, interindustry analysis, environmental and ecological analysis, resource management, urban and regional policy analysis, geographical information systems, and spatial data analysis. In the broadest sense, any social science analysis that has a spatial dimension is embraced by regional scientists.
Origins.
Regional science was founded in the late 1940s when some economists began to become dissatisfied with the low level of regional economic analysis and felt an urge to upgrade it. But even in this early era, the founders of regional science expected to catch the interest of people from a wide variety of disciplines. Regional science's formal roots date to the aggressive campaigns by Walter Isard and his supporters to promote the "objective" and "scientific" analysis of settlement, industrial location, and urban development. Isard targeted key universities and campaigned tirelessly. Accordingly, the Regional Science Association was founded in 1954, when the core group of scholars and practitioners held its first meetings independent from those initially held as sessions of the annual meetings of the American Economics Association. A reason for meeting independently undoubtedly was the group's desire to extend the new science beyond the rather restrictive world of economists and have natural scientists, psychologists, anthropologists, lawyers, sociologists, political scientists, planners, and geographers join the club. Now called the Regional Science Association International, it maintains subnational and international associations, journals, and a conference circuit (notably in North America, continental Europe, Japan, and South Korea). Membership in the RSAI continues to grow.
Germinal publications.
Topically speaking, regional science took off in the wake of Walter Christaller's book "Die Zentralen Orte in Sűddeutschland" (Verlag von Gustav Fischer, Jena, 1933; transl. "Central Places in Southern Germany", 1966), soon followed by Tord Palander's (1935) "Beiträge zur Standortstheorie"; August Lösch's "Die räumliche Ordnung der Wirtschaft" (Verlag von Gustav Fischer, Jena, 1940; 2nd rev. edit., 1944; transl. "The Economics of Location", 1954) ; and Edgar M. Hoover's two books--"Location Theory and the Shoe and Leather Industry" (1938) and "The Location of Economic Activity" (1948). Other important early publications include: Edward H. Chamberlin's (1950) "The Theory of Monopolistic Competition" ; François Perroux's (1950) "Economic Spaces: Theory and Application"; Torsten Hägerstrand's (1953) "Innovationsförloppet ur Korologisk Synpunkt"; Edgar S. Dunn's (1954)"The Location of Agricultural Production" ; Martin J. Beckmann, C.B McGuire, and Clifford B. Winston's (1956) "Studies in the Economics of Transportation"; Melvin L. Greenhut's (1956) "Plant Location in Theory and Practice"; Gunnar Myrdal's (1957) "Economic Theory and Underdeveloped Regions"; Albert O. Hirschman's (1958) "The Strategy of Economic Development"; and Claude Ponsard's (1958) "Histoire des Théorie Économique Spatiales". Nonetheless, Walter Isard's first book in 1956, "Location and Space Economy", apparently captured the imagination of many, and his third, "Methods of Regional Analysis", published in 1960, only sealed his position as the father of the field.
As is typically the case, the above works were built on the shoulders of giants. Much of this predecessor work is documented well in Walter Isard's "Location and Space Economy" as well as Claude Ponsard's "Histoire des Théorie Économique Spatiales". Particularly important was the contribution by 19th century German economists to location theory. The early German hegemony more or less starts with Johann Heinrich von Thünen and runs through both Wilhelm Launhardt and Alfred Weber to Walter Christaller and August Lösch.
Core journals.
If an academic discipline is identified by its journals, then technically regional science began in 1955 with the publication of the first volume of the "Papers and Proceedings, Regional Science Association" (now "Papers in Regional Science" published by Springer). In 1958, the "Journal of Regional Science" followed.
Most recently the journal "Spatial Economic Analysis" has been published by the RSAI British and Irish Section with the "Regional Studies Association". The latter is a separate and growing organisation involving economists, planners, geographers, political scientists, management academics, policymakers, and practitioners.
Academic programs.
Walter Isard's efforts culminated in the creation of a few academic departments and several university-wide programs in regional science. At Walter Isard's suggestion, the University of Pennsylvania started the Regional Science Department in 1956. It featured as its first graduate William Alonso and was looked upon by many to be the international academic leader for the field. Another important graduate and faculty member of the department is Masahisa Fujita. The core curriculum of this department was microeconomics, input-output analysis, location theory, and statistics. Faculty also taught courses in mathematical programming, transportation economics, labor economics, energy and ecological policy modeling, spatial statistics, spatial interaction theory and models, benefit/cost analysis, urban and regional analysis, and economic development theory, among others. But the department's unusual multidisciplinary orientation undoubtedly encouraged its demise, and it lost its department status in 1993.
With a few exceptions, such as Cornell University, which awards graduate degrees in Regional Science, most practitioners hold positions in departments such as economics, geography, civil engineering, agricultural economics, rural sociology, urban planning, public policy, or demography. The diversity of disciplines participating in regional science have helped make it one of the most interesting and fruitful fields of academic specialization, but it has also made it difficult to fit the many perspectives into a curriculum for an academic major. It is even difficult for authors to write regional science textbooks, since what is elementary knowledge for one discipline might be entirely novel for another.
Public policy impact.
Part of the movement was, and continues to be, associated with the political and economic realities of the role of the local community. On any occasion where public policy is directed at the sub-national level, such as a city or group of counties, the methods of regional science can prove useful. Traditionally, regional science has provided policymakers with guidance on the following issues:
By targeting federal resources to specific geographic areas the Kennedy administration realized that political favors could be bought. This is also evident in Europe and other places where local economic areas do not coincide with political boundaries. In the more current era of devolution knowledge about "local solutions to local problems" has driven much of the interest in regional science. Thus, there has been much political impetus to the growth of the discipline.
Developments after 1980.
Regional science has enjoyed mixed fortunes since the 1980s. While it has gained a larger following among economists and public policy practitioners, the discipline has fallen out of favor among more radical and post-modernist geographers. In an apparent effort to secure a larger share of research funds, geographers had the National Science Foundation's Geography and Regional Science Program renamed "Geography and Spatial Sciences".
New economic geography.
In 1991, Paul Krugman, as a highly regarded international trade theorist, put out a call for economists to pay more attention to economic geography in a book entitled "Geography and Trade", focusing largely on the core regional science concept of agglomeration economies. Krugman's call renewed interest by economists in regional science and, perhaps more importantly, founded what some term the "new economic geography", which enjoys much common ground with regional science. Broadly trained "new economic geographers" combine quantitative work with other research techniques, for example at the London School of Economics. The unification of Europe and the increased internationalization of the world's economic, social, and political realms has further induced interest in the study of regional, as opposed to national, phenomena. The new economic geography appears to have garnered more interest in Europe than in America where amenities, notably climate, have been found to better predict human location and re-location patterns, as emphasized in recent work by Mark Partridge. In 2008 Krugman won the Nobel Memorial Prize in Economic Sciences and his Prize Lecture has references both to work in regional science's location theory as well as economic's trade theory.
Criticisms.
Today there are dwindling numbers of regional scientists from academic planning programs and mainstream geography departments. Attacks on regional science's practitioners by radical critics began as early as the 1970s, notably David Harvey who believed it lacked social and political commitment. Regional science's founder, Walter Isard, never envisioned regional scientists would be political or planning activists. In fact, he suggested that they will seek to be sitting in front of a computer and surrounded by research assistants. Trevor J. Barnes suggests the decline of regional science practice among planners and geographers in North America could have been avoided. He says "It is unreflective, and consequently inured to change, because of a commitment to a God’s eye view. It is so convinced of its own rightness, of its Archimedean position, that it remained aloof and invariant, rather than being sensitive to its changing local context." 

</doc>
<doc id="50713" url="http://en.wikipedia.org/wiki?curid=50713" title="Zygnematales">
Zygnematales

The Zygnematales (Greek: ζυγός (zygos) + "νήμα" (nēma) (nom.), "νήματος" (nēmatos) (gen.)), also called the Conjugales, are an order of green algae, comprising several thousand different species in genera such as the well-known "Zygnema" and "Spirogyra". Most members of this group develop into unbranched filaments, one cell thick, which grow longer through normal cell division. Most live in freshwater, and form an important component of the algal scum that grows on or near plants, rocks, and various debris.
Systematically they fall within the division Charophyta, which includes the groups of algae that are most closely related to the higher plants. Charophyta are included with land plants (Embryophyta) in the clade Streptophyta.
Sexual reproduction in Zygnematales takes place through a process called "conjugation". Here filaments of opposite gender line up, and tubes form between corresponding cells. The male cells then become amoeboid and crawl across, or sometimes both cells crawl into the tube. The cells then meet and fuse to form a zygote, which later undergoes meiosis to produce new filaments. As in plants, only the female passes chloroplasts on to the offspring.
The only other group of conjugating algae are the desmids, which live as individual cells often with a striking symmetrical appearance. The desmids are sometimes placed in a separate order Desmidiales, which is closely related to Zygnematales, and the two are placed together in the class Zygnematophyceae.

</doc>
<doc id="50714" url="http://en.wikipedia.org/wiki?curid=50714" title="Sinclair Lewis">
Sinclair Lewis

Harry Sinclair Lewis (; February 7, 1885 – January 10, 1951) was an American novelist, short-story writer, and playwright. In 1930, he became the first writer from the United States to receive the Nobel Prize in Literature, which was awarded "for his vigorous and graphic art of description and his ability to create, with wit and humor, new types of characters." His works are known for their insightful and critical views of American capitalism and materialism between the wars. He is also respected for his strong characterizations of modern working women. H.L. Mencken wrote of him, "[If] there was ever a novelist among us with an authentic call to the trade ... it is this red-haired tornado from the Minnesota wilds."
He has been honored by the U.S. Postal Service with a Great Americans series postage stamp.
Biography.
Childhood and education.
Born February 7, 1885, in the village of Sauk Centre, Minnesota, Sinclair Lewis began reading books at a young age and kept a diary. He had two siblings, Fred (born 1875) and Claude (born 1878). His father, Edwin J. Lewis, was a physician and a stern disciplinarian who had difficulty relating to his sensitive, unathletic third son. Lewis's mother, Emma Kermott Lewis, died in 1891. The following year, Edwin Lewis married Isabel Warner, whose company young Lewis apparently enjoyed. Throughout his lonely boyhood, the ungainly Lewis—tall, extremely thin, stricken with acne and somewhat pop-eyed—had trouble gaining friends and pined after various local girls. At the age of 13 he unsuccessfully ran away from home, wanting to become a drummer boy in the Spanish–American War.
In late 1902 Lewis left home for a year at Oberlin Academy (the then-preparatory department of Oberlin College) to qualify for acceptance by Yale University. While at Oberlin, he developed a religious enthusiasm that waxed and waned for much of his remaining teenage years. He entered Yale in 1903 but did not receive his bachelor's degree until 1908, having taken time off to work at Helicon Home Colony, Upton Sinclair's cooperative-living colony in Englewood, New Jersey, and to travel to Panama. Lewis's unprepossessing looks, "fresh" country manners and seemingly self-important loquacity made it difficult for him to win and keep friends at Oberlin and Yale. He did initiate a few relatively long-lived friendships among students and professors, some of whom recognized his promise as a writer.
Early career.
Lewis's earliest published creative work—romantic poetry and short sketches—appeared in the "Yale Courant" and the "Yale Literary Magazine", of which he became an editor. After graduation Lewis moved from job to job and from place to place in an effort to make ends meet, write fiction for publication and to chase away boredom. While working for newspapers and publishing houses (and for a time at the Carmel-by-the-Sea, California writers' colony), he developed a facility for turning out shallow, popular stories that were purchased by a variety of magazines. He also earned money by selling plots to Jack London, including one for the latter's unfinished novel "The Assassination Bureau, Ltd".
Lewis's first published book was "Hike and the Aeroplane", a Tom Swift-style potboiler that appeared in 1912 under the pseudonym Tom Graham.
Sinclair Lewis's first serious novel, "Our Mr. Wrenn: The Romantic Adventures of a Gentle Man", appeared in 1914, followed by "The Trail of the Hawk: A Comedy of the Seriousness of Life" (1915) and "The Job" (1917). That same year also saw the publication of another potboiler, "", an expanded version of a serial story that had originally appeared in "Woman's Home Companion". "Free Air", another refurbished serial story, was published in 1919.
Marriage and family.
In 1914 Lewis married Grace Livingston Hegger (1887–1981), an editor at "Vogue" magazine. They had one son, Wells Lewis (1917–1944), named after British author H. G. Wells. Wells Lewis was killed in action while serving in the U.S. Army in World War II, specifically during the rescue of "The Lost Battalion" in the , near Germany, in France. Dean Acheson, the future Secretary of State, was a neighbor and family friend in Washington, and observed that Sinclair's literary "success was not good for that marriage, or for either of the parties to it, or for Lewis's work" and the family moved out of town.
Lewis divorced Grace in 1925. On May 14, 1928, he married Dorothy Thompson, a political newspaper columnist. Later in 1928, he and Dorothy purchased a second home in rural Vermont. They had a son, Michael Lewis, in 1930. Their marriage had virtually ended by 1937, and they divorced in 1942. Michael Lewis became an actor, also suffered with alcoholism, and died in 1975 of Hodgkin's lymphoma. Michael had two sons, John Paul and Gregory Claude, with wife Bernadette Nanse and a daughter Lesley with wife Valerie Cardew.
Commercial success.
Upon moving to Washington, D.C., Lewis devoted himself to writing. As early as 1916, he began taking notes for a realistic novel about small-town life. Work on that novel continued through mid-1920, when he completed "Main Street", which was published on October 23, 1920. As his biographer Mark Schorer wrote, the phenomenal success of "Main Street" "was the most sensational event in twentieth-century American publishing history." Lewis's agent had the most optimistic projection of sales at 25,000 copies. In its first six months, "Main Street" sold 180,000 copies, and within a few years, sales were estimated at two million. According to biographer Richard Lingeman, ""Main Street" made [Lewis] rich—earning him perhaps three million current [2005] dollars".
Lewis followed up this first great success with "Babbitt" (1922), a novel that satirized the American commercial culture and boosterism. The story was set in the fictional Midwestern town of Zenith, Winnemac, a setting to which Lewis would return in future novels, including "Gideon Planish" and "Dodsworth".
Lewis continued his success in the 1920s with "Arrowsmith" (1925), a novel about the challenges faced by an idealistic doctor. It was awarded the Pulitzer Prize (which Lewis refused). Adapted as a 1931 Hollywood film directed by John Ford and starring Ronald Colman, it was nominated for four Academy Awards.
Next Lewis published "Elmer Gantry" (1927), which depicted an evangelical minister as deeply hypocritical. The novel was denounced by many religious leaders and banned in some U.S. cities. Adapted for the screen more than a generation later, the novel was the basis of the 1960 movie starring Burt Lancaster, who earned a Best Actor Oscar for his performance.
Lewis closed out the decade with "Dodsworth" (1929), a novel about the most affluent and successful members of American society. He portrayed them as leading essentially pointless lives in spite of great wealth and advantages. The book was adapted for the Broadway stage in 1934 by Sidney Howard, who also wrote the screenplay for the 1936 film version. Directed by William Wyler and a great success at the time, the film is still highly regarded. In 1990, it was selected for preservation in the United States National Film Registry, and in 2005 "Time" magazine named it one of the "100 Best Movies" of the past 80 years.
During the late 1920s and 1930s, Lewis wrote many short stories for a variety of magazines and publications. "Little Bear Bongo" (1930), a tale about a bear cub who wanted to escape the circus in search of a better life in the real world, was published in "Cosmopolitan" magazine. The story was acquired by Walt Disney Pictures in 1940 for a possible feature film. World War II sidetracked those plans until 1947. Disney used the story (now titled "Bongo") as part of its feature "Fun and Fancy Free".
Nobel Prize.
In 1930, Lewis won the Nobel Prize in Literature, the first writer from the United States to receive the award. In the Swedish Academy's presentation speech, special attention was paid to "Babbitt". In his Nobel Lecture, Lewis praised Theodore Dreiser, Willa Cather, Ernest Hemingway, and other contemporaries, but also lamented that "in America most of us—not readers alone, but even writers—are still afraid of any literature which is not a glorification of everything American, a glorification of our faults as well as our virtues," and that America is "the most contradictory, the most depressing, the most stirring, of any land in the world today." He also offered a profound criticism of the American literary establishment: "Our American professors like their literature clear and cold and pure and very dead."
Later years.
After winning the Nobel Prize, Lewis wrote eleven more novels, ten of which appeared in his lifetime. The best remembered is "It Can't Happen Here" (1935), a novel about the election of a fascist to the American presidency.
After an alcoholic binge in 1937, Lewis checked into the Austen Riggs Center, a psychiatric hospital in Stockbridge, Massachusetts, for treatment. His doctors gave Lewis a blunt assessment that he needed to decide "whether he was going to live without alcohol or die by it, one or the other." Lewis checked out after ten days, lacking, one of his physicians wrote to a colleague, any "fundamental understanding of his problem."
In the 1940s, Lewis and rabbi-turned-popular author Lewis Browne frequently appeared on the lecture platform together, touring the United States and debating such questions as "Has the Modern Woman Made Good?", "The Country Versus the City", "Is the Machine Age Wrecking Civilization?" and "Can Fascism Happen Here?" before audiences of as many as 3,000 people. The pair was described as "the Gallagher and Shean of the lecture circuit" by Lewis biographer Richard Lingeman.
The novel "Kingsblood Royal" (1947) is set in the fictional city Grand Republic, Minnesota, an enlarged and updated version of Zenith. Based on the Sweet Trials in Detroit, in which an African-American doctor was denied the chance to purchase a house in a "white" section of the city, "Kingsblood Royal" was a powerful and very early contribution to the civil rights movement.
Lewis died in Rome on January 10, 1951, aged 65, from advanced alcoholism. His cremated remains were buried in Sauk Centre. A final novel, "World So Wide" (1951), was published posthumously.
William Shirer, a friend and admirer of Lewis, disputes accounts that Lewis died of alcoholism "per se". He reported that Lewis had a heart attack and that his doctors advised him to stop drinking if he wanted to live. Lewis did not, and perhaps could not, stop; he died when his heart stopped.
In summing up Lewis' career, Shirer concludes:It has become rather commonplace for so-called literary critics to write off Sinclair Lewis as a novelist. Compared to ... Fitzgerald, Hemingway, Dos Passos, and Faulkner ... Lewis lacked style. Yet his impact on modern American life ... was greater than all of the other four writers together.
Works.
Short stories.
"The Short Stories of Sinclair Lewis (1904–1949)".
Samuel J. Rogal edited "The Short Stories of Sinclair Lewis (1904–1949)", a seven-volume set published in 2007 by Edwin Mellen Press. The work is the first attempt to collect all of Lewis's short stories.

</doc>
<doc id="50715" url="http://en.wikipedia.org/wiki?curid=50715" title="Piracy">
Piracy

Piracy is typically an act of robbery or criminal violence at sea. The term can include acts committed on land, in the air, or in other major bodies of water or on a shore. It does not normally include crimes committed against persons traveling on the same vessel as the perpetrator (e.g. one passenger stealing from others on the same vessel). The term has been used throughout history to refer to raids across land borders by non-state agents.
Piracy or pirating is the name of a specific crime under customary international law and also the name of a number of crimes under the municipal law of a number of states. It is distinguished from privateering, which is authorized by national authorities and therefore a legitimate form of war-like activity by non-state actors. Privateering is considered commerce raiding, and was outlawed by the Peace of Westphalia (1648) for signatories to those treaties.
Those who engage in acts of piracy are called pirates.
In the 21st century, the international community is facing many problems in bringing pirates to justice.
Etymology.
The English "pirate" is derived from the Latin term "pirata" and that from Greek πειρατής ("peiratēs"), "brigand", in turn from πειράομαι (peiráomai), "I attempt", from πεῖρα ("peîra"), "attempt, experience". The word is also cognate to "peril".
Also, particularly in the 1600s and 1700s, spelling was haphazard due to variations by printers, and words such as "Pyrate" or "an act of Pyracy" are examples of some of the accepted ways of spelling in past years.
History by region.
It may be reasonable to assume that piracy has existed for as long as the oceans were plied for commerce. The following will examine the history of piracy in a few central regions.
Europe and Mediterranean.
Antiquity.
The earliest documented instances of piracy are the exploits of the Sea Peoples who threatened the Aegean and Mediterranean in the 14th century BC. In classical antiquity, the Illyrians and Tyrrhenians were known as pirates, as well as Greeks and Romans. During their voyages, the Phoenicians seem to have sometimes resorted to piracy, and specialized in kidnapping boys and girls to be sold as slaves.
In the 3rd century BC, pirate attacks on Olympos (city in Anatolia) brought impoverishment. Among some of the most famous ancient pirateering peoples were the Illyrians, populating the western Balkan peninsula. Constantly raiding the Adriatic Sea, the Illyrians caused many conflicts with the Roman Republic. It was not until 168 BC when the Romans finally conquered Illyria and made it a province that their threat was ended.
During the 1st century BC, there were pirate states along the Anatolian coast, threatening the commerce of the Roman Empire in the eastern Mediterranean. On one voyage across the Aegean Sea in 75 BC, Julius Caesar was kidnapped and briefly held by Cilician pirates and held prisoner in the Dodecanese islet of Pharmacusa. The Senate finally invested the general Gnaeus Pompeius Magnus with powers to deal with piracy in 67 BC (the "Lex Gabinia"), and Pompey, after three months of naval warfare, managed to suppress the threat.
As early as 258 AD, the Gothic-Herulic fleet ravaged towns on the coasts of the Black Sea and Sea of Marmara. The Aegean coast suffered similar attacks a few years later. In 264, the Goths reached Galatia and Cappadocia, and Gothic pirates landed on Cyprus and Crete. In the process, the Goths seized enormous booty and took thousands into captivity. In 286 AD, Carausius, a Roman military commander of Gaulish origins, was appointed to command the "Classis Britannica", and given the responsibility of eliminating Frankish and Saxon pirates who had been raiding the coasts of Armorica and Belgic Gaul. In the Roman province of Britannia, Saint Patrick was captured and enslaved by Irish pirates.
Middle Ages.
The most widely known and far-reaching pirates in medieval Europe were the Vikings, warriors and looters from Scandinavia who raided mainly between the 8th and 12th centuries, during the Viking Age in the Early Middle Ages. They raided the coasts, rivers and inland cities of all Western Europe as far as Seville, attacked by the Norse in 844. Vikings also attacked coasts of North Africa and Italy and plundered all the coasts of the Baltic Sea, ascending the rivers of Eastern Europe as far as the Black Sea and Persia. The lack of centralized powers all over Europe during the Middle Ages favoured pirates all over the continent.
In the Late Middle Ages, the Frisian pirates led by respectively Pier Gerlofs Donia and Wijerd Jelckama, fought against the troops of the Holy Roman Emperor Charles V with some success.
Toward the end of the 9th century, Moor pirate havens were established along the coast of southern France and northern Italy. In 846 Moor raiders sacked the "extra muros" Basilicas of Saint Peter and Saint Paul in Rome. In 911, the bishop of Narbonne was unable to return to France from Rome because the Moors from Fraxinet controlled all the passes in the Alps. Moor pirates operated out of the Balearic Islands in the 10th century. From 824 to 961 Arab pirates in the Emirate of Crete raided the entire Mediterranean. In the 14th century, raids by Moor pirates forced the Venetian Duke of Crete to ask Venice to keep its fleet on constant guard.
After the Slavic invasions of the former Roman province of Dalmatia in the 5th and 6th centuries, a tribe called the Narentines revived the old Illyrian piratical habits and often raided the Adriatic Sea starting in the 7th century. By 642 they invaded southern Italy and assaulted Siponto. Their raids in the Adriatic increased rapidly, until the whole Sea was no longer safe for travel.
The Narentines took more liberties in their raiding quests while the Venetian Navy was abroad, as when it was campaigning in Sicilian waters in 827–882. As soon as the Venetian fleet would return to the Adriatic, the Narentines temporarily abandoned their habits again, even signing a Treaty in Venice and baptising their Slavic pagan leader into Christianity. In 834 or 835 they broke the treaty and again they raided Venetian traders returning from Benevento, and all of Venice's military attempts to punish them in 839 and 840 utterly failed. Later, they raided the Venetians more often, together with the Arabs. In 846, the Narentines broke through to Venice itself and raided its lagoon city of Caorle. In the middle of March 870 they kidnapped the Roman Bishop's emissaries that were returning from the Ecclesiastical Council in Constantinople. This caused a Byzantine military action against them that finally brought Christianity to them. After the Arab raids on the Adriatic coast circa 872 and the retreat of the Imperial Navy, the Narentines continued their raids of Venetian waters, causing new conflicts with the Italians in 887–888. The Venetians futilely continued to fight them throughout the 10th and 11th centuries.
In 937, Irish pirates sided with the Scots, Vikings, Picts, and Welsh in their invasion of England. Athelstan drove them back.
The Slavic piracy in the Baltic Sea ended with the Danish conquest of the Rani stronghold of Arkona in 1168. In the 12th century the coasts of western Scandinavia were plundered by Curonians and Oeselians from the eastern coast of the Baltic Sea. In the 13th and 14th century, pirates threatened the Hanseatic routes and nearly brought sea trade to the brink of extinction. The Victual Brothers of Gotland were a companionship of privateers who later turned to piracy. Until about 1440, maritime trade in both the North Sea and the Baltic Sea was seriously in danger of attack by the pirates.
H. Thomas Milhorn mentions a certain Englishman named William Maurice, convicted of piracy in 1241, as the first person known to have been hanged, drawn and quartered, which would indicate that the then-ruling King Henry III took an especially severe view of this crime.
The ushkuiniks were Novgorodian pirates who looted the cities on the Volga and Kama Rivers in the 14th century.
As early as Byzantine times, the Maniots (one of Greece's toughest populations) were known as pirates. The Maniots considered piracy as a legitimate response to the fact that their land was poor and it became their main source of income. The main victims of Maniot pirates were the Ottomans but the Maniots also targeted ships of European countries.
Zaporizhian Sich was a pirate republic in Europe from the 16th through to the 18th century. Situated in Cossack territory in the remote Steppe of Eastern Europe, it was populated with Ukrainian peasants that had run away from their feudal masters, outlaws of every sort, destitute gentry, run-away slaves from Turkish galleys, etc. The remoteness of the place and the rapids at the Dnepr river effectively guarded the place from invasions of vengeful powers. The main target of the inhabitants of Zaporizhian Sich who called themselves "Cossacks" were rich settlements at the Black Sea shores of Ottoman Empire and Crimean Khanate. By 1615 and 1625, Zaporozhian Cossacks had even managed to raze townships on the outskirts of Istanbul, forcing the Ottoman Sultan to flee his palace. Don Cossacks under Stenka Razin even ravaged the Persian coasts.
Asia.
In East Asia by the ninth century, populations centered mostly around merchant activities in coastal Shandong and Jiangsu provinces. Wealthy benefactors, including Jang Bogo established Silla Buddhist temples in the region. Jang Bogo had become incensed at the treatment of his fellow countrymen, who in the unstable milieu of late Tang often fell victim to coastal pirates or inland bandits. After returning to Silla around 825, and in possession of a formidable private fleet headquartered at Cheonghae (Wando), Jang Bogo petitioned the Silla king Heungdeok (r. 826–836) to establish a permanent maritime garrison to protect Silla merchant activities in the Yellow Sea. Heungdeok agreed and in 828 formally established the Cheonghae (淸海, "clear sea") Garrison(청해진) at what is today Wando island off Korea's South Jeolla province. Heungdeok gave Jang an army of 10,000 men to establish and man the defensive works. The remnants of Cheonghae Garrison can still be seen on Jang islet just off Wando's southern coast. Jang's force, though nominally bequeathed by the Silla king, was effectively under his own control. Jang became arbiter of Yellow Sea commerce and navigation.
From the 13th century, Wokou based in Japan made their debut in East Asia, initiating invasions that would persist for 300 years.
In South East Asia, piracy began with the retreating Mongol Yuan fleet after the betrayal by their Javanese allies (who, incidentally, would found the empire of Majapahit after the Mongols left). They preferred the junk, a ship using a more robust sail layout. Marooned navy officers, consisting mostly of Cantonese and Hokkien tribesmen, set up their small gangs near river estuaries, mainly to protect themselves. They recruited locals as common foot-soldiers known as "lang" (Malay: "lanun", meaning 'pirate') to set up their fortresses. They survived by utilizing their well trained pugilists, as well as marine and navigation skills, mostly along Sumatran and Javanese estuaries. Their strength and ferocity coincided with the impending trade growth of the maritime silk and spice routes.
Pirates who accepted the Royal Pardon from the Chola Empire would get to serve in the Chola Navy as "Kallarani". They would be used as coast guards, or sent on recon missions to deal with Arab piracy in the Arabian Sea. Their function is similar to the 18th century privateers, used by the Royal Navy.
Starting in the 14th century, the Deccan (Southern Peninsular region of India) was divided into two entities: on the one side stood the Muslim Bahmani Sultanate and on the other stood the Hindu kings rallied around the Vijayanagara Empire. Continuous wars demanded frequent resupplies of fresh horses, which were imported through sea routes from Persia and Africa. This trade was subjected to frequent raids by thriving bands of pirates based in the coastal cities of Western India. One of such was Timoji, who operated off Anjadip Island both as a privateer (by seizing horse traders, that he rendered to the raja of Honavar) and as a pirate who attacked the Kerala merchant fleets that traded pepper with Gujarat.
During the 16th and 17th centuries, there was frequent European piracy against Mughal Indian merchants, especially those en route to Mecca for Hajj. The situation came to a head when the Portuguese attacked and captured the vessel "Rahimi" which belonged to Mariam Zamani the Mughal queen, which led to the Mughal seizure of the Portuguese town Daman. In the 18th century, the famous Maratha privateer Kanhoji Angre ruled the seas between Mumbai and Goa. The Marathas attacked British shipping and insisted that East India Company ships pay taxes if sailing through their waters.
The Buginese sailors of South Sulawesi were infamous as pirates who used to range as far west as Singapore and as far north as the Philippines in search of targets for piracy. The Orang laut pirates controlled shipping in the Straits of Malacca and the waters around Singapore, and the Malay and Sea Dayak pirates preyed on maritime shipping in the waters between Singapore and Hong Kong from their haven in Borneo. The Moro pirates of the southern Philippines harassed Spanish shipping and terrorized Christian Filipino settlements. David P. Forsythe wrote: "Of particular significance in Southeast Asia were the incursions of Moro raiders in the southern Philippines who may have captured around 2 million slaves in the first two centuries of Spanish rule after 1565."
During the Qing period, Chinese pirate fleets grew increasingly large. The effects large-scale piracy had on the Chinese economy were immense. They preyed voraciously on China's junk trade, which flourished in Fujian and Guangdong and was a vital artery of Chinese commerce. Pirate fleets exercised hegemony over villages on the coast, collecting revenue by exacting tribute and running extortion rackets. In 1802, the menacing Zheng Yi inherited the fleet of his cousin, captain Zheng Qi, whose death provided Zheng Yi with considerably more influence in the world of piracy. Zheng Yi and his wife, Zheng Yi Sao (who would eventually inherit the leadership of his pirate confederacy) then formed a pirate coalition that, by 1804, consisted of over ten thousand men. Their military might alone was sufficient to combat the Qing navy. However, a combination of famine, Qing naval opposition, and internal rifts crippled piracy in China around the 1820s, and it has never again reached the same status.
Barbary corsairs.
Pirates and privateers that operated from North African (the "Barbary Coast") ports of Algiers, Tunis, Tripoli and ports in Morocco were known as the Barbary corsairs. They preyed on shipping in the western Mediterranean Sea from the time of the Crusades, as well as on ships on their way to Asia around Africa until the early 19th century. The coastal villages and towns of Italy, Spain and Mediterranean islands were frequently attacked by them and long stretches of the Italian and Spanish coasts were almost completely abandoned by their inhabitants; after 1600 Barbary corsairs occasionally entered the Atlantic and struck as far north as Iceland. According to Robert Davis between 1 million and 1.25 million Europeans were captured by Barbary corsairs and sold as slaves in North Africa and the Ottoman Empire between the 16th and 19th centuries. The most famous corsairs were the Ottoman Hayreddin and his older brother Oruç Reis (Redbeard), Turgut Reis (known as Dragut in the West), Kurtoglu (known as Curtogoli in the West), Kemal Reis, Salih Reis and Koca Murat Reis. A few Barbary corsairs, such as the Dutch Jan Janszoon and the English John Ward (Muslim name Yusuf Reis), were renegade European privateers who had converted to Islam.
At one point, there were nearly 1,000 pirates located in Madagascar. Île Sainte-Marie was a popular base for pirates throughout the 17th and 18th centuries. The most famous pirate utopia is that of the probably fictional Captain Misson and his pirate crew, who allegedly founded the free colony of Libertatia in northern Madagascar in the late 17th century, until it was destroyed in a surprise attack by the island natives in 1694.
"The Golden Age of Piracy" in the Caribbean.
The classic era of piracy was in the Caribbean, circa 1650 until the mid-1720s. By 1650, France, England and the United Provinces began to develop their colonial empires. This involved considerable seaborne trade, and a general economic improvement: there was money to be made—or stolen—and much of it traveled by ship.
French buccaneers were established on northern Hispaniola as early as 1625, but lived at first mostly as hunters rather than robbers; their transition to full-time piracy was gradual and motivated in part by Spanish efforts to wipe out both the buccaneers and the prey animals on which they depended. The buccaneers' migration from Hispaniola's mainland to the more defensible offshore island of Tortuga limited their resources and accelerated their piratical raids. According to Alexandre Exquemelin, a buccaneer and historian who remains a major source on this period, the Tortuga buccaneer Pierre Le Grand pioneered the settlers' attacks on galleons making the return voyage to Spain.
The growth of buccaneering on Tortuga was augmented by the English capture of Jamaica from Spain in 1655. The early English governors of Jamaica freely granted letters of marque to Tortuga buccaneers and to their own countrymen, while the growth of Port Royal provided these raiders with a far more profitable and enjoyable place to sell their booty. In the 1660s, the new French governor of Tortuga, Bertrand d'Ogeron, similarly provided privateering commissions both to his own colonists and to English cutthroats from Port Royal. These conditions brought Caribbean buccaneering to its zenith.
A new phase of piracy began in the 1690s as English pirates began to look beyond the Caribbean for treasure. The fall of Britain's Stuart kings had restored the traditional enmity between Britain and France, thus ending the profitable collaboration between English Jamaica and French Tortuga. The devastation of Port Royal by an earthquake in 1692 further reduced the Caribbean's attractions by destroying the pirates' chief market for fenced plunder. Caribbean colonial governors began to discard the traditional policy of "no peace beyond the Line," under which it was understood that war would continue (and thus letters of marque would be granted) in the Caribbean regardless of peace treaties signed in Europe; henceforth, commissions would be granted only in wartime, and their limitations would be strictly enforced. Furthermore, much of the Spanish Main had simply been exhausted; Maracaibo alone had been sacked three times between 1667 and 1678, while Río de la Hacha had been raided five times and Tolú eight.
At the same time, England's less favored colonies, including Bermuda, New York, and Rhode Island, had become cash-starved by the Navigation Acts, which restricted trade with foreign ships. Merchants and governors eager for coin were willing to overlook and even underwrite pirate voyages; one colonial official defended a pirate because he thought it "very harsh to hang people that brings in gold to these provinces." Although some of these pirates operating out of New England and the Middle Colonies targeted Spain's remoter Pacific coast colonies well into the 1690s and beyond, the Indian Ocean was a richer and more tempting target. India's economic output was large during this time, especially in high-value luxury goods like silk and calico which made ideal pirate booty; at the same time, no powerful navies plied the Indian Ocean, leaving both local shipping and the various East India companies' vessels vulnerable to attack. This set the stage for the famous pirates, Thomas Tew, Henry Every, Robert Culliford and (although his guilt remains controversial) William Kidd.
Between 1713 and 1714, a succession of peace treaties was signed which ended the War of the Spanish Succession. With the end of this conflict, thousands of seamen, including Britain's paramilitary privateers, were relieved of military duty. The result was a large number of trained, idle sailors at a time when the cross-Atlantic colonial shipping trade was beginning to boom. In addition, Europeans who had been pushed by unemployment to become sailors and soldiers involved in slaving were often enthusiastic to abandon that profession and turn to pirating, giving pirate captains for many years a constant pool of trained European recruits to be found in west African waters and coasts.
In 1715, pirates launched a major raid on Spanish divers trying to recover gold from a sunken treasure galleon near Florida. The nucleus of the pirate force was a group of English ex-privateers, all of whom would soon be enshrined in infamy: Henry Jennings, Charles Vane, Samuel Bellamy, and Edward England. The attack was successful, but contrary to their expectations, the governor of Jamaica refused to allow Jennings and their cohorts to spend their loot on his island. With Kingston and the declining Port Royal closed to them, Jennings and his comrades founded a new pirate base at Nassau, on the island of New Providence in the Bahamas, which had been abandoned during the war. Until the arrival of governor Woodes Rogers three years later, Nassau would be home for these pirates and their many recruits.
Shipping traffic between Africa, the Caribbean, and Europe began to soar in the 18th century, a model that was known as triangular trade, and was a rich target for piracy. Trade ships sailed from Europe to the African coast, trading manufactured goods and weapons for slaves. The traders would then sail to the Caribbean to sell the slaves, and return to Europe with goods such as sugar, tobacco and cocoa. Another triangular trade saw ships carry raw materials, preserved cod, and rum to Europe, where a portion of the cargo would be sold for manufactured goods, which (along with the remainder of the original load) were transported to the Caribbean, where they were exchanged for sugar and molasses, which (with some manufactured articles) were borne to New England. Ships in the triangular trade made money at each stop.
As part of the peace settlement of the War of the Spanish succession, Britain obtained the "asiento", a Spanish government contract, to supply slaves to Spain's new world colonies, providing British traders and smugglers more access to the traditionally closed Spanish markets in America. This arrangement also contributed heavily to the spread of piracy across the western Atlantic at this time. Shipping to the colonies boomed simultaneously with the flood of skilled mariners after the war. Merchant shippers used the surplus of sailors' labor to drive wages down, cutting corners to maximize their profits, and creating unsavory conditions aboard their vessels. Merchant sailors suffered from mortality rates as high or higher than the slaves being transported (Rediker, 2004). Living conditions were so poor that many sailors began to prefer a freer existence as a pirate. The increased volume of shipping traffic also could sustain a large body of brigands preying upon it. Among the most infamous Caribbean pirates of the time, was Edward Teach or "Blackbeard", Calico Jack Rackham and Bartholomew Roberts. Most of these pirates were eventually hunted down by the Royal Navy and killed or captured; several battles were fought between the brigands and the colonial powers on both land and sea.
Piracy in the Caribbean declined for the next several decades after 1730, but by the 1810s many pirates roamed the waters though they were not as bold or successful as their predecessors. The most successful pirates of the era were Jean Lafitte and Roberto Cofresi. Lafitte is considered by many to be the last buccaneer due to his army of pirates and fleet of pirate ships which held bases in and around the Gulf of Mexico. Lafitte and his men participated in the War of 1812 battle of New Orleans. Cofresi's base was in Mona Island, Puerto Rico, from where he disrupted the commerce throughout the region. He became the last major target of the international anti-piracy operations.
North America.
River piracy, in late 18th-mid-19th century America, was primarily concentrated along the Ohio River and Mississippi River valleys. In 1803, at Tower Rock, the U.S. Army dragoons, possibly, from the frontier army post up river at Fort Kaskaskia, on the Illinois side opposite St. Louis, raided and drove out the river pirates.
Stack Island was also associated with river pirates and counterfeiters in the late 1790s. In 1809, the last major river pirate activity took place, on the Upper Mississippi River, and river piracy in this area came to an abrupt end, when a group of flatboatmen raided the island, wiping out the river pirates. From 1790–1834, Cave-In-Rock was the principal outlaw lair and headquarters of river pirate activity in the Ohio River region, from which Samuel Mason led a gang of river pirates on the Ohio River.
River piracy continued on the lower Mississippi River, from the early 1800s to the mid-1830s, declining as a result of direct military action and local law enforcement and regulator-vigilante groups that uprooted and swept out pockets of outlaw resistance.
Great Lakes piracy occurred, from 1900–1930, on Lake Michigan, through the exploits of "Roaring" Dan Seavey.
Suppression of piracy.
Caribbean.
The elimination of piracy from European waters expanded to the Caribbean in the 18th century, West Africa and North America by the 1710s and by the 1720s even the Indian Ocean was a difficult location for pirates to operate.
England began to strongly turn against piracy at the turn of the 18th century, as it was increasingly damaging to the country's economic and commercial prospects in the region. The Piracy Act of 1698 for the "more effectual suppression of Piracy" made it easier to capture, try and convict pirates by lawfully enabling acts of piracy to be “examined, inquired of, tried, heard and determined, and adjudged in any place at sea, or upon the land, in any of his Majesty’s islands, plantations, colonies, dominions, forts, or factories.” This effectively enabled admirals to hold a court session to hear the trials of pirates in any place they deemed necessary, rather than requiring that the trial be held in England. Commissioners of these vice-admiralty courts were also vested with “full power and authority” to issue warrants, summon the necessary witnesses, and “to do all thing necessary for the hearing and final determination of any case of piracy, robbery, or felony.” These new and faster trials provided no legal representation for the pirates; and ultimately led in this era to the execution of 600 pirates, which represented approximately 10 percent of the pirates active at the time in the Caribbean region. Being an accessory to piracy was also criminalised under the statute.
Piracy saw a brief resurgence between the end of the War of the Spanish Succession in 1713 and around 1720, as many unemployed seafarers took to piracy as a way to make ends meet when a surplus of sailors after the war led to a decline in wages and working conditions. At the same time, one of the terms of the Treaty of Utrecht that ended the war gave to Great Britain's Royal African Company and other British slavers a thirty-year asiento, or contract, to furnish African slaves to the Spanish colonies, providing British merchants and smugglers potential inroads into the traditionally closed Spanish markets in America and leading to an economic revival for the whole region. This revived Caribbean trade provided rich new pickings for a wave of piracy. Also contributing to the increase of Caribbean piracy at this time was Spain's breakup of the English logwood settlement at Campeche and the attractions of a freshly sunken silver fleet off the southern Bahamas in 1715. Fears over the rising levels of crime and piracy, political discontent, concern over crowd behaviour at public punishments, and an increased determination by parliament to suppress piracy, resulted in the Piracy Act of 1717 and of 1721. These established a seven-year penal transportation to North America as a possible punishment for those convicted of lesser felonies, or as a possible sentence that capital punishment might be commuted to by royal pardon.
After 1720, piracy in the classic sense became extremely rare as increasingly effective anti-piracy measures were taken by the Royal Navy making it impossible for any pirate to pursue an effective career for long. By 1718, the British Royal Navy had approximately 124 vessels and 214 by 1815; a big increase from the two vessels England had possessed in 1670. British Royal Navy warships tirelessly hunted down pirate vessels, and almost always won these engagements.
Many pirates did not surrender and were killed at the point of capture; notorious pirate Edward Teach, or "Blackbeard", was hunted down by Lieutenant Robert Maynard at Ocracoke Inlet off the coast of North Carolina on 22 November 1718 and killed. Captain Chaloner Ogle of the HMS "Swallow" cornered Bartholomew Roberts in 1722 at Cape Lopez, and a fatal broadside from the Swallow killed the pirate captain instantly. Roberts' death shocked the pirate world, as well as the Royal Navy. The local merchants and civilians had thought him invincible, and some considered him a hero. Roberts' death was seen by many historians as the end of the Golden Age of Piracy. Also crucial to the end of this era of piracy was the loss of the pirates' last Caribbean safe haven at Nassau.
In the early 19th century, piracy along the East and Gulf Coasts of North America as well as in the Caribbean increased again. Jean Lafitte was just one of hundreds of pirates operating in American and Caribbean waters between the years of 1820 and 1835. The United States Navy repeatedly engaged pirates in the Caribbean, Gulf of Mexico and in the Mediterranean. Cofresí's "El Mosquito" was disabled in a collaboration between Spain and the United States. After fleeing for hours, he was ambushed and captured inland. The United States landed shore parties on several islands in the Caribbean in pursuit of pirates; Cuba was a major haven. By the 1830s piracy had died out again, and the navies of the region focused on the slave trade.
About the time of the Mexican-American War in 1846, the United States Navy had grown strong and numerous enough to eliminate the pirate threat in the West Indies. By the 1830s, ships had begun to convert to steam propulsion, so the Age of Sail and the classical idea of pirates in the Caribbean ended. Privateering, similar to piracy, continued as an asset in war for a few more decades and proved to be of some importance during the naval campaigns of the American Civil War.
Privateering would remain a tool of European states until the mid-19th century's Declaration of Paris. But letters of marque were given out much more sparingly by governments and were terminated as soon as conflicts ended. The idea of "no peace beyond the Line" was a relic that had no meaning by the more settled late 18th and early 19th centuries.
Barbary pirates.
Piracy off the Barbary coast was often assisted by competition among European powers in the 17th century. France encouraged the corsairs against Spain, and later Britain and Holland supported them against France. However, by the second half of the 17th century the greater European naval powers began to initiate reprisals to intimidate the Barbary States into making peace with them. The most successful of the Christian states in dealing with the corsair threat was England. From the 1630s onwards England had signed peace treaties with the Barbary States on various occasions, but invariably breaches of these agreements led to renewed wars. A particular bone of contention was the tendency of foreign ships to pose as English to avoid attack. However, growing English naval power and increasingly persistent operations against the corsairs proved increasingly costly for the Barbary States. During the reign of Charles II a series of English expeditions won victories over raiding squadrons and mounted attacks on their home ports which permanently ended the Barbary threat to English shipping. In 1675 a bombardment from a Royal Navy squadron led by Sir John Narborough and further defeats at the hands of a squadron under Arthur Herbert negotiated a lasting peace (until 1816) with Tunis and Tripoli.
France, which had recently emerged as a leading naval power, achieved comparable success soon afterwards, with bombardments of Algiers in 1682, 1683 and 1688 securing a lasting peace, while Tripoli was similarly coerced in 1686. In 1783 and 1784 the Spaniards also bombarded Algiers in an effort to stem the piracy. The second time, Admiral Barceló damaged the city so severely that the Algerian Dey asked Spain to negotiate a peace treaty and from then on Spanish vessels and coasts were safe for several years.
Until the American Declaration of Independence in 1776, British treaties with the North African states protected American ships from the Barbary corsairs. Morocco, which in 1777 was the first independent nation to publicly recognize the United States, became in 1784 the first Barbary power to seize an American vessel after independence. While the United States managed to secure peace treaties, these obliged it to pay tribute for protection from attack. Payments in ransom and tribute to the Barbary states amounted to 20% of United States government annual expenditures in 1800, leading to the Barbary Wars that ended the payment of tribute. However, Algiers broke the 1805 peace treaty after only two years, and subsequently refused to implement the 1815 treaty until compelled to do so by Britain in 1816.
In 1815, the sacking of Palma on the island of Sardinia by a Tunisian squadron, which carried off 158 inhabitants, roused widespread indignation. Britain had by this time banned the slave trade and was seeking to induce other countries to do likewise. This led to complaints from states which were still vulnerable to the corsairs that Britain's enthusiasm for ending the trade in African slaves did not extend to stopping the enslavement of Europeans and Americans by the Barbary States.
In order to neutralise this objection and further the anti-slavery campaign, in 1816 Lord Exmouth was sent to secure new concessions from Tripoli, Tunis, and Algiers, including a pledge to treat Christian captives in any future conflict as prisoners of war rather than slaves and the imposition of peace between Algiers and the kingdoms of Sardinia and Sicily. On his first visit he negotiated satisfactory treaties and sailed for home. While he was negotiating, a number of Sardinian fishermen who had settled at Bona on the Tunisian coast were brutally treated without his knowledge. As Sardinians they were technically under British protection and the government sent Exmouth back to secure reparation. On August 17, in combination with a Dutch squadron under Admiral Van de Capellen, he bombarded Algiers. Both Algiers and Tunis made fresh concessions as a result.
However, securing uniform compliance with a total prohibition of slave-raiding, which was traditionally of central importance to the North African economy, presented difficulties beyond those faced in ending attacks on ships of individual nations, which had left slavers able to continue their accustomed way of life by preying on less well-protected peoples. Algiers subsequently renewed its slave-raiding, though on a smaller scale. Measures to be taken against the city's government were discussed at the Congress of Aix-la-Chapelle in 1818. In 1820 another British fleet under Admiral Sir Harry Neal again bombarded Algiers. Corsair activity based in Algiers did not entirely cease until its conquest by France in 1830.
China.
In the 1840s and 1850s, United States Navy and Royal Navy forces campaigned together against Chinese pirates. Several notable battles were fought though pirate junks continued operating off China for years more. However, some British and American individual citizens also volunteered to serve with Chinese pirates to fight against European forces. The British offered rewards for the capture of westerners serving with Chinese pirates. During the Second Opium War and the Taiping Rebellion, piratical junks were again destroyed in large numbers by British naval forces but ultimately it wasn't until the 1860s and 1870s that fleets of pirate junks ceased to exist.
Geography.
Narrow channels which funnel shipping into predictable routes can develop opportunities for piracy, as well as for privateering and commerce raiding. (For a land-based parallel, compare the association of bandits and brigands with mountain passes.) Historic examples include the waters of Gibraltar, the Strait of Malacca, Madagascar, the Gulf of Aden, and the English Channel, whose geographic strictures facilitated pirate attacks.
Persian Gulf.
The southern coast of the Persian Gulf was known to the British from the late 18th century as the "Pirate Coast, "where control of the seaways of the Persian Gulf was asserted by the Qawasim and other local maritime powers. Memories of the privations carried out on the coast by Portuguese raiders under Albuquerque were long and local powers antipathetic as a consequence to Christian powers asserting dominance of their coastal waters. Early British expeditions to protect the Imperial Indian Ocean trade from raiders, principally Al Qasimi from Ras al-Khaimah and Lingeh led to campaigns against those headquarters and other harbours along the coast in 1809 and then, after a relapse in raiding, again in 1819 and so to the signing of a first formal treaty of perpetual maritime peace between the British and coastal rulers in 1820, leading to the area becoming known as the Trucial Coast and several emirates recognised by the British as Trucial States.
Popular image.
In the popular modern imagination, pirates of the classical period were rebellious, clever teams who operated outside the restricting bureaucracy of modern life. Pirates were also depicted as always raising their Jolly Roger flag when preparing to hijack a vessel. The Jolly Roger is the traditional name for the flags of mainly English pirates and a symbol for piracy that has been adopted by film-makers and toy manufacturers.
Various claims and speculation about their overall image, attire, fashion, dress code, etc. have been made and contributed to their fanciful mystery and lore. Including, for example, men getting their ear pierced was popular with pirates; the value of the earring was meant to pay for their burial if they were lost at sea and their body washed ashore.
Pirate democracy.
Unlike traditional Western societies of the time, many Caribbean pirate crews of European descent operated as limited democracies. Pirate communities were some of the first to instate a system of checks and balances similar to the one used by the present-day United States and many other countries. The first record of such a government aboard a pirate sloop dates to the 17th century.
Sunken pirate ships.
To date two identifiable pirate shipwrecks have been discovered. One is the Whydah Gally, a former slave ship seized on its maiden voyage from Africa by the pirate captain "Black Sam" Bellamy. Since 2007 the Wydah collection has been touring as part of the exhibit "Real Pirates" sponsored by National Geographic. The other is the "Queen Anne's Revenge," the flagship of the infamous pirate Blackbeard. He used her for less than a year, but she was an effective tool in his prize-taking. In June of 1718, Blackbeard ran the ship aground at Beaufort Inlet, North Carolina. In late 1996, Intersal, a private firm working under a permit with the state of North Carolina, discovered the remains of the vessel.
Treasure.
Even though pirates raided many ships, few, if any, buried their treasure. Often, the "treasure" that was stolen was food, water, alcohol, weapons, or clothing. Other things they stole were household items like bits of soap and gear like rope and anchors, or sometimes they would keep the ship they captured (either to sell off or keep because it was better than their ship). Such items were likely to be needed immediately, rather than saved for future trade. For this reason, there was no reason for the pirates to bury these goods. Pirates tended to kill few people aboard the ships they captured; usually they would kill no one if the ship surrendered, because if it became known that pirates took no prisoners, their victims would fight to the last breath and make victory both very difficult and costly in lives. In contrast, ships would quickly surrender if they knew they would be spared. In one well-documented case 300 heavily armed soldiers on a ship attacked by Thomas Tew surrendered after a brief battle with none of Tew's 40-man crew being injured.
Rewards.
Pirates had a system of hierarchy on board their ships determining how captured money was distributed. However, pirates were more "egalitarian" than any other area of employment at the time. In fact pirate quartermasters were a counterbalance to the captain and had the power to veto his orders. The majority of plunder was in the form of cargo and ship's equipment with medicines the most highly prized. A vessel's doctor's chest would be worth anywhere from £300 to £400, or around $470,000 in today's values. Jewels were common plunder but not popular as they were hard to sell, and pirates, unlike the public of today, had little concept of their value. There is one case recorded where a pirate was given a large diamond worth a great deal more than the value of the handful of small diamonds given his crewmates as a share. He felt cheated and had it broken up to match what they received.
Spanish pieces of eight minted in Mexico or Seville were the standard trade currency in the American colonies. However, every colony still used the monetary units of pounds, shillings and pence for bookkeeping while Spanish, German, French and Portuguese money were all standard mediums of exchange as British law prohibited the export of British silver coinage. Until the exchange rates were standardised in the late 18th century each colony legislated its own different exchange rates. In England, 1 piece of eight was worth 4s 3d while it was worth 8s in New York, 7s 6d in Pennsylvania and 6s 8d in Virginia. One 18th-century English shilling was worth around $58 in modern currency so a piece of eight could be worth anywhere from $246 to $465. As such, the value of pirate plunder could vary considerably depending on who recorded it and where.
Ordinary seamen received a part of the plunder at the captain's discretion but usually a single share. On average, a pirate could expect the equivalent of a year's wages as his share from each ship captured while the crew of the most successful pirates would often each receive a share valued at around £1,000 ($1.17 million) at least once in their career. One of the larger amounts taken from a single ship was that by captain Thomas Tew from an Indian merchantman in 1692. Each ordinary seaman on his ship received a share worth £3,000 ($3.5 million) with officers receiving proportionally larger amounts as per the agreed shares with Tew himself receiving 2½ shares. It is known there were actions with multiple ships captured where a single share was worth almost double this.
By contrast, an ordinary seamen in the Royal Navy received 19s per month to be paid in a lump sum at the end of a tour of duty which was around half the rate paid in the Merchant Navy. However, corrupt officers would often "tax" their crews' wage to supplement their own and the Royal Navy of the day was infamous for its reluctance to pay. From this wage, 6d per month was deducted for the maintenance of Greenwich Hospital with similar amounts deducted for the Chatham Chest, the chaplain and surgeon. Six months' pay was withheld to discourage desertion. That this was insufficient incentive is revealed in a report on proposed changes to the RN Admiral Nelson wrote in 1803; he noted that since 1793 more than 42,000 sailors had deserted. Roughly half of all RN crews were pressganged and these not only received lower wages than volunteers but were shackled while the vessel was docked and were never permitted to go ashore until released from service.
Although the Royal Navy suffered from many morale issues, it answered the question of prize money via the 'Cruizers and Convoys' Act of 1708 which handed over the share previously gained by the Crown to the captors of the ship. Technically it was still possible for the Crown to get the money or a portion of it but this rarely happened. The process of condemnation of a captured vessel and its cargo and men was given to the High Court of the Admiralty and this was the process which remained in force with minor changes throughout the Revolutionary and Napoleonic Wars.
Even the flag officer's share was not quite straightforward; he would only get the full one-eighth if he had no junior flag officer beneath him. If this was the case then he would get a third share. If he had more than one then he would take one half while the rest was shared out equally.
There was a great deal of money to be made in this way. The record breaker was the capture of the Spanish frigate the "Hermione", which was carrying treasure in 1762. The value of this was so great that each individual seaman netted £485 ($1.4 million in 2008 dollars). The two captains responsible, Evans and Pownall, received £65,000 each ($188.4 million). In January 1807 the frigate Caroline took the Spanish San Rafael which brought in £52,000 for her captain, Peter Rainier (who had been only a Midshipman some thirteen months before). All through the wars there are examples of this kind of luck falling on captains. Another famous 'capture' was that of the Spanish frigates Thetis and Santa Brigada which were loaded with gold specie. They were taken by four British frigates who shared the money, each captain receiving £40,730. Each lieutenant got £5,091, the Warrant Officer group, £2,468, the midshipmen £791 and the individual seamen £182.
It should also be noted that it was usually only the frigates which took prizes; the ships of the line were far too ponderous to be able to chase and capture the smaller ships which generally carried treasure. Nelson always bemoaned that he had done badly out of prize money and even as a flag officer received little. This was not that he had a bad command of captains but rather that British mastery of the seas was so complete that few enemy ships dared to sail.
Punishment.
During the 17th and 18th centuries, once pirates were caught, justice was meted out in a summary fashion, and many ended their lives by "dancing the hempen jig", which meant hanging at the end of a rope. Public execution was a form of entertainment at the time, and people came out to watch them as they would to a sporting event today. Newspapers were glad to report every detail, such as recording the condemned men's last words, the prayers said by the priests for their immortal souls, and their final agonizing moments on the gallows. In England most of these executions took place at Execution Dock on the River Thames in London.
In the cases of more famous prisoners, usually captains, their punishments extended beyond death. Their bodies were enclosed in iron cages (gibbet) (for which they were measured before their execution) and left to swing in the air until the flesh rotted off them- a process that could take as long as two years. The bodies of captains such as William "Captain" Kidd, Charles Vane, William Fly, and Jack Rackham ("Calico Jack") were all treated this way.
Privateers.
A privateer or corsair used similar methods to a pirate, but acted under orders of the state while in possession of a commission or letter of marque and reprisal from a government or monarch authorizing the capture of merchant ships belonging to an enemy nation. For example, the United States Constitution of 1787 specifically authorized Congress to issue letters of marque and reprisal. The letter of marque and reprisal was recognized by international convention and meant that a privateer could not technically be charged with piracy while attacking the targets named in his commission. This nicety of law did not always save the individuals concerned, however, as whether one was considered a pirate or a legally operating privateer often depended on whose custody the individual found himself in—that of the country that had issued the commission, or that of the object of attack. Spanish authorities were known to execute foreign privateers with their letters of marque hung around their necks to emphasize Spain's rejection of such defenses. Furthermore, many privateers exceeded the bounds of their letters of marque by attacking nations with which their sovereign was at peace (Thomas Tew and William Kidd are notable alleged examples), and thus made themselves liable to conviction for piracy. However, a letter of marque did provide some cover for such pirates, as plunder seized from neutral or friendly shipping could be passed off later as taken from enemy merchants.
The famous Barbary Corsairs of the Mediterranean, authorized by the Ottoman Empire, were privateers, as were the Maltese Corsairs, who were authorized by the Knights of St. John, and the Dunkirkers in the service of the Spanish Empire. In the years 1626–1634 alone, the Dunkirk privateers captured 1,499 ships, and sank another 336. From 1609 to 1616, England lost 466 merchant ships to Barbary pirates, and 160 British ships were captured by Algerians between 1677 and 1680. One famous privateer was Sir Francis Drake. His patron was Queen Elizabeth I, and their relationship ultimately proved to be quite profitable for England.
Privateers constituted a large proportion of the total military force at sea during the 17th and 18th centuries. During the Nine Years War, the French adopted a policy of strongly encouraging privateers (French corsairs), including the famous Jean Bart, to attack English and Dutch shipping. England lost roughly 4,000 merchant ships during the war. In the following War of Spanish Succession, privateer attacks continued, Britain losing 3,250 merchant ships. During the War of Austrian Succession, Britain lost 3,238 merchant ships and France lost 3,434 merchant ships to the British.
During King George's War, approximately 36,000 Americans served aboard privateers at one time or another. During the American Revolution, about 55,000 American seamen served aboard the privateers. The American privateers had almost 1,700 ships, and they captured 2,283 enemy ships. Between the end of the Revolutionary War and 1812, less than 30 years, Britain, France, Naples, the Barbary States, Spain, and the Netherlands seized approximately 2,500 American ships. Payments in ransom and tribute to the Barbary states amounted to 20% of United States government annual revenues in 1800. Throughout the American Civil War, Confederate privateers successfully harassed Union merchant ships.
Privateering lost international sanction under the Declaration of Paris in 1856.
Modern age.
Overview.
Seaborne piracy against transport vessels remains a significant issue (with estimated worldwide losses of US$16 billion per year), particularly in the waters between the Red Sea and Indian Ocean, off the Somali coast, and also in the Strait of Malacca and Singapore, which are used by over 50,000 commercial ships a year. In the late 2000s, the emergence of piracy off the coast of Somalia spurred a multi-national effort led by the United States to patrol the waters near the Horn of Africa. In 2011, Brazil also created an anti-piracy unit on the Amazon river.
River piracy happens in Europe, with vessels suffering from pirate attacks on the Serbian and Romanian stretches of the international Danube river, i.e. inside the European Union's territory.
Modern pirates favor small boats and taking advantage of the small number of crew members on modern cargo vessels. They also use large vessels to supply the smaller attack/boarding vessels. Modern pirates can be successful because a large amount of international commerce occurs via shipping. Major shipping routes take cargo ships through narrow bodies of water such as the Gulf of Aden and the Strait of Malacca making them vulnerable to be overtaken and boarded by small motorboats. Other active areas include the South China Sea and the Niger Delta. As usage increases, many of these ships have to lower cruising speeds to allow for navigation and traffic control, making them prime targets for piracy.
Also, pirates often operate in regions of developing or struggling countries with smaller navies and large trade routes. Pirates sometimes evade capture by sailing into waters controlled by their pursuer's enemies. With the end of the Cold War, navies have decreased in size and patrol less frequently, whilst trade has increased, making organized piracy far easier. Modern pirates are sometimes linked with organized-crime syndicates, but often are small individual groups.
The International Maritime Bureau (IMB) maintains statistics regarding pirate attacks dating back to 1995. Their records indicate hostage-taking overwhelmingly dominates the types of violence against seafarers. For example in 2006, there were 239 attacks, 77 crew members were kidnapped and 188 taken hostage but only 15 of the pirate attacks resulted in murder. In 2007 the attacks rose by 10% to 263 attacks. There was a 35% increase on reported attacks involving guns. Crew members that were injured numbered 64 compared to just 17 in 2006. That number does not include instances of hostage taking and kidnapping where the victims were not injured.
The number of attacks from January to September 2009 had surpassed the previous year's total due to the increased pirate attacks in the Gulf of Aden and off Somalia. Between January and September the number of attacks rose to 306 from 293. The pirates boarded the vessels in 114 cases and hijacked 34 of them so far in 2009. Gun use in pirate attacks has gone up to 176 cases from 76 last year.
Rather than cargo, modern pirates have targeted the personal belongings of the crew and the contents of the ship's safe, which potentially contains large amounts of cash needed for payroll and port fees. In other cases, the pirates force the crew off the ship and then sail it to a port to be repainted and given a new identity through false papers purchased from corrupt or complicit officials.
Modern piracy can also take place in conditions of political unrest. For example, following the U.S. withdrawal from Vietnam, Thai piracy was aimed at the many Vietnamese who took to boats to escape. Further, following the disintegration of the government of Somalia, warlords in the region have attacked ships delivering UN food aid.
Environmental action groups such as Sea Shepherd Conservation Society have been accused of engaging in piracy and terrorism, when they ram and throw butyric acid on the decks of ships engaged in commercial fishing, shark poaching and finning, seal hunting, and whaling. In two instances, they boarded a Japanese whaling vessel.
The attack against the German built cruise ship the "Seabourn Spirit" offshore of Somalia in November 2005 is an example of the sophisticated pirates mariners face. The pirates carried out their attack more than 100 mi offshore with speedboats launched from a larger mother ship. The attackers were armed with automatic firearms and an RPG.
Since 2008, Somali pirates centered in the Gulf of Aden made about $120 million annually, reportedly costing the shipping industry between $900 million and $3.3 billion per year. By September 2012, the heyday of piracy in the Indian Ocean was reportedly over. Backers were now reportedly reluctant to finance pirate expeditions due to the low rate of success, and pirates were no longer able to reimburse their creditors. According to the International Maritime Bureau, pirate attacks had by October 2012 dropped to a six-year low. Only five ships were captured by the end of the year, representing a decrease from 25 in 2011 and 27 in 2010, with only 1 ship attacked in the third quarter compared to 36 during the same period in 2011. However, pirate incidents off on the West African seaboard increased to 34 from 30 the previous year, and attacks off the coast of Indonesia rose from 2011's total of 46 to 51.
Many nations forbid ships to enter their territorial waters or ports if the crew of the ships are armed, in an effort to restrict possible piracy. Shipping companies sometimes hire private armed security guards.
Modern definitions of piracy include the following acts:
For the United States, piracy is one of the offenses against which Congress is delegated power to enact penal legislation by the Constitution of the United States, along with treason and offenses against the law of nations. Treason is generally making war against one's own countrymen, and violations of the law of nations can include unjust war among other nationals or by governments against their own people.
In modern times, ships and airplanes are hijacked for political reasons as well. The perpetrators of these acts could be described as pirates (for instance, the French for "plane hijacker" is "pirate de l'air", literally "air pirate"), but in English are usually termed "hijackers". An example is the hijacking of the Italian civilian passenger ship "Achille Lauro" in 1985, which is generally regarded as an act of piracy.
Modern pirates also use a great deal of technology. It has been reported that crimes of piracy have involved the use of mobile phones, satellite phones, GPS, AK74 rifles, Sonar systems, modern speedboats, shotguns, pistols, mounted machine guns, and even RPGs and grenade launchers.
Anti-piracy measures.
Under a principle of international law known as the "universality principle", a government may "exercise jurisdiction over conduct outside its territory if that conduct is universally dangerous to states and their nationals." The rationale behind the universality principle is that states will punish certain acts "wherever they may occur as a means of protecting the global community as a whole, even absent a link between the state and the parties or the acts in question." Under this principle, the concept of "universal jurisdiction" applies to the crime of piracy. For example, the United States has a statute (section 1651 of title 18 of the United States Code) imposing a sentence of life in prison for piracy "as defined by the law of nations" committed anywhere on the high seas, regardless of the nationality of the pirates or the victims.
According to piracy experts, the goal is to "deter and disrupt" pirate activity, and pirates are often detained, interrogated, disarmed, and released. With millions of dollars at stake, pirates have little incentive to stop. In Finland, one case involved pirates who had been captured and whose boat was sunk. No prosecution of the pirates is forthcoming, as pirates attacked a vessel of Singapore and the pirates are not, themselves, EU or Finnish citizens. A further complication is that Singapore law allows the death penalty for piracy and Finland does not. Some countries have been reluctant to utilize the death penalty to stop pirates.
The Dutch are using a 17th-century law against "sea robbery" to prosecute. Warships that capture pirates have no jurisdiction to try them, and NATO does not have a detention policy in place. Prosecutors have a hard time assembling witnesses and finding translators, and countries are reluctant to imprison pirates because the countries would be saddled with the pirates upon their release.
George Mason University professor Peter Leeson has suggested that the international community appropriate Somali territorial waters and sell them, together with the international portion of the Gulf of Aden, to a private company which would then provide security from piracy in exchange for charging tolls to world shipping through the Gulf.
Self-defense.
The fourth volume of the handbook: "Best Management Practices to Deter Piracy off the Coast of Somalia and in the Arabian Sea Area" (known as BMP4) is the current authoritative guide for merchant ships on self-defense against pirates. The guide is issued and updated by a consortium of interested international shipping and trading organizations including the EU, NATO and the International Maritime Bureau. It is distributed primarily by the Maritime Security Centre – Horn of Africa (MSCHOA) – the planning and coordination authority for EU naval forces (EUNAVFOR). BMP4 encourages vessels to register their voyages through the region with MSCHOA as this registration is a key component of the operation of the International Recommended Transit Corridor (IRTC) (the navy-patrolled route through the Gulf of Aden).
BMP4 also contains a chapter entitled "Self-Protective Measures" which lays out a list of steps a merchant vessel can take on its own to make itself less of a target to pirates and make it better able to repel an attack if one occurs. This list includes doing things like ringing the deck of the ship with razor wire, rigging fire-hoses to spray sea-water over the side of the ship (to hinder boardings), having a distinctive pirate alarm, hardening the bridge against gunfire and creating a "citadel" where the crew can retreat in the event pirates get on board.
Other unofficial self-defense measures that can be found on merchant vessels include the setting up of mannequins posing as armed guards or firing flares at the pirates.
Though it varies by country, generally peacetime law in the 20th and 21st centuries has not allowed merchant vessels to carry weapons. As a response to the rise in modern piracy, however, the U.S. government changed its rules so that it is now possible for U.S.-flagged vessels to embark a team of armed private security guards. Other countries and organisations have similarly followed suit. This has given birth to a new breed of private security companies who provide training and protection for crew members and cargo and have proved effective in countering pirate attacks. The USCG leaves it to ship owners' discretion to determine if those guards will be armed. Seychelles has become a central location for international anti-piracy operations, hosting the Anti-Piracy Operation Center for the Indian Ocean. In 2008, VSOS became the first authorized armed maritime security company to operate in the Indian Ocean region.
With safety trials complete in the late 2000s, laser dazzlers have been developed for defensive purposes on super-yachts. They can be effective up to 4 km with the effects going from mild disorientation to flash blindness at closer range.
In February 2012, Italian Marines based on the tanker "Enrica Lexie" allegedly fired on an Indian fishing trawler off Kerala, killing two of her eleven crew. The Marines allegedly mistook the fishing vessel as a pirate vessel. The incident sparked a diplomatic row between India and Italy. "Enrica Lexie" was ordered into Kochi where her crew were questioned by officers of the Indian Police. The fact is still "sub juris" and its legal eventual outcome could influence future deployment of VPDs, since states will be either encouraged or discouraged to provide them depending on whether functional immunity is ultimately granted or denied to the Italians.Another similar incident has been reported to have happened in the Red Sea between the coasts of Somalia and Yemen, involving the death of a Yemeni fisherman allegedly at the hands of a Russian Vessel Protection Detachment (VPD) on board a Norwegian-flagged vessel.
However, despite VPD deployment being controversial because of these incidents, according to the Associated Press, during a United Nations Security Council conference about piracy "U.S. Ambassador Susan Rice told the council that no ship carrying armed guards has been successfully attacked by pirates" and "French Ambassador Gerard Araud stressed that private guards do not have the deterrent effect that government-posted marine and sailors and naval patrols have in warding off attacks".
Self protection measures and increased patrol.
First and foremost, the best protection against piracy is simply to avoid encountering them. This can be accomplished by using tools such as radar, or by using specialised systems that use shorter wavelengths (as small boats are not always picked up by radar). An example of a specialised system is WatchStander.
In addition, while the non-wartime 20th century tradition has been for merchant vessels not to be armed, the U.S. Government has recently changed the rules so that it is now "best practice" for vessels to embark a team of armed private security guards. In addition, the crew themselves can be given weapons training, and warning shots can be fired legally in international waters.
Other measures vessels can take to protect themselves against piracy are implementing a high freewall and vessel boarding protection systems (e.g., hot water wall, electricity-charged water wall, automated fire monitor, slippery foam).
Ships can also attempt to protect themselves using their Automatic Identification Systems (AIS). Every ship over 300 tons carries a transponder supplying both information about the ship itself and its movements. Any unexpected change in this information can attract attention. Previously this data could only be picked up if there was a nearby ship, thus rendering single ships vulnerable. However, special satellites have been launched recently that are now able to detect and retransmit this data. Large ships cannot therefore be hijacked without being detected. This can act as a deterrent to attempts to either hijack the entire ship or steal large portions of cargo with another ship since an escort can be sent more quickly than might otherwise have been the case.
Finally, in an emergency, warships can be called upon. In some areas such as near Somalia, naval vessels from different nations are present that are able to intercept vessels attacking merchant vessels. For patrolling dangerous coastal waters (and/or keeping financial expenses down), robotic or remote-controlled USVs are also sometimes used. Also, both shore-launched and vessel-launched UAVs are also used by the U.S. Army.
Commerce raiders.
A wartime activity similar to piracy involves disguised warships called commerce raiders or merchant raiders, which attack enemy shipping commerce, approaching by stealth and then opening fire. Commerce raiders operated successfully during the American Revolution. During the American Civil War, the Confederacy sent out several commerce raiders, the most famous of which was the CSS "Alabama". During World War I and World War II, Germany also made use of these tactics, both in the Atlantic and Indian Oceans. Since commissioned naval vessels were openly used, these commerce raiders should not be considered even privateers, much less pirates— although the opposing combatants were vocal in denouncing them as such.
National law.
United Kingdom.
Section 2 of the Piracy Act 1837 creates a statutory offence of aggravated piracy. See also the Piracy Act 1850.
In 2008 the British Foreign Office advised the Royal Navy not to detain pirates of certain nationalities as they might be able to claim asylum in Britain under British human rights legislation, if their national laws included execution, or mutilation as a judicial punishment for crimes committed as pirates.
Definition of piracy jure gentium
See section 26 of, and Schedule 5 to, the Merchant Shipping and Maritime Security Act 1997. These provisions replace the Schedule to the Tokyo Convention Act 1967. In Cameron v HM Advocate, 1971 SLT 333, the High Court of Justiciary said that that Schedule supplemented the existing law and did not seek to restrict the scope of the offence of piracy jure gentium.
See also:
Jurisdiction
See section 46(2) of the Senior Courts Act 1981 and of the Territorial Waters Jurisdiction Act 1878. See also R v Kohn (1864) 4 F & F 68.
Piracy committed by or against aircraft
See section 5 of the Aviation Security Act 1982.
Sentence
The book "Archbold" said that in a case that does not fall within section 2 of the Piracy Act 1837, the penalty appears to be determined by the Offences at Sea Act 1799, which provides that offences committed at sea are liable to the same penalty as if they had been committed upon the shore.
History
William Hawkins said that at common law, piracy by a subject was esteemed to be petty treason. The Treason Act 1351 provided that this was not petty treason.
In English admiralty law, piracy was classified as petit treason during the medieval period, and offenders were accordingly liable to be drawn and quartered on conviction. Piracy was redefined as a felony during the reign of Henry VIII. In either case, piracy cases were cognizable in the courts of the Lord High Admiral. English admiralty vice-admiralty judges emphasized that "neither Faith nor Oath is to be kept" with pirates; i.e. contracts with pirates and oaths sworn to them were not legally binding. Pirates were legally subject to summary execution by their captors if captured in battle. In practice, instances of summary justice and annulment of oaths and contracts involving pirates do not appear to have been common.
United States.
In the United States, criminal prosecution of piracy is authorized in the :
... To define and punish Piracies and Felonies committed on the high Seas, and Offences against the Law of Nations;
Title 18 U.S.C. § 1651 states:
Whoever, on the high seas, commits the crime of piracy as defined by the law of nations, and is afterwards brought into or found in the United States, shall be imprisoned for life.
Citing the United States Supreme Court decision in the year 1820 case of "United States v. Smith", a U.S. District Court ruled in 2010 in the case of "United States v. Said" that the definition of piracy under section 1651 is confined to "robbery at sea." The piracy charges (but not other serious federal charges) against the defendants in the "Said" case were dismissed by the Court.
International law.
Effects on international boundaries.
During the 18th century, the British and the Dutch controlled opposite sides of the Straits of Malacca. The British and the Dutch drew a line separating the Straits into two halves. The agreement was that each party would be responsible for combating piracy in their respective half. Eventually this line became the border between Malaysia and Indonesia in the Straits.
Law of nations.
Piracy is of note in international law as it is commonly held to represent the earliest invocation of the concept of universal jurisdiction. The crime of piracy is considered a breach of "jus cogens", a conventional peremptory international norm that states must uphold. Those committing thefts on the high seas, inhibiting trade, and endangering maritime communication are considered by sovereign states to be "hostis humani generis" (enemies of humanity).
For a different opinion on Pirates as Hostis Humani Generis see Caninas, Osvaldo Peçanha. . Paper presented at the annual meeting of the ISA – ABRI JOINT INTERNATIONAL MEETING, Pontifical Catholic University, Rio de Janeiro Campus (PUC-Rio), Rio de Janeiro, Brazil, Jul 22, 2009
Because of universal jurisdiction, action can be taken against pirates without objection from the flag state of the pirate vessel. This represents an exception to the principle "extra territorium jus dicenti impune non paretur" ("One who exercises jurisdiction out of his territory is not obeyed with impunity").
International conventions.
Articles 101 to 103 of UNCLOS.
Articles 101 to 103 of the United Nations Convention on the Law of the Sea (UNCLOS) (1982) contain a definition of "piracy iure gentium". They read:
Article 101
"Definition of piracy"
Piracy consists of any of the following acts:
Article 102
"Piracy by a warship, government ship or government aircraft whose crew has mutinied"
The acts of piracy, as defined in article 101, committed by a warship, government ship or government aircraft whose crew has mutinied and taken control of the ship or aircraft are assimilated to acts committed by a private ship or aircraft.
Article 103
"Definition of a pirate ship or aircraft"
A ship or aircraft is considered a pirate ship or aircraft if it is intended by the persons in dominant control to be used for the purpose of committing one of the acts referred to in article 101. The same applies if the ship or aircraft has been used to commit any such act, so long as it remains under the control of the persons guilty of that act.
This definition was formerly contained in articles 15 to 17 of the Convention on the High Seas signed at Geneva on April 29, 1958. It was drafted by the International Law Commission.
A limitation of article 101 above is that it confines piracy to the High Seas. As the majority of piratical acts occur within territorial waters, some pirates are able to go free as certain jurisdictions lack the resources to monitor their borders adequately.
IMB definition.
The International Maritime Bureau (IMB) defines piracy as:
Uniformity in Maritime Piracy Law.
Given the diverging definitions of piracy in international and municipal legal systems, some authors argue that greater uniformity in the law is required in order to strengthen anti-piracy legal instruments.
In popular culture.
Pirates are a frequent topic in fiction and, in their Caribbean incarnation, are associated with certain stereotypical manners of speaking and dress, some of them wholly fictional: "nearly all our notions of their behavior come from the golden age of fictional piracy, which reached its zenith in 1881 with the appearance of Robert Louis Stevenson's "Treasure Island"." Some inventions of pirate culture such as "walking the plank" were popularized by J. M. Barrie's novel, "Peter Pan", where Captain Hook's pirates helped define the fictional pirate archetype. Robert Newton's portrayal of Long John Silver in Disney's 1950 film adaptation also helped define the modern rendition of a pirate, including the stereotypical West Country "pirate accent". Other influences include "Sinbad the Sailor", and the recent "Pirates of the Caribbean" films have helped kindle modern interest in piracy and have performed well at the box office. "" also revolves around pirates during the Golden Age of Piracy.
The classic Gilbert and Sullivan comic opera "The Pirates of Penzance" focuses on The Pirate King and his hopeless band of pirates.
Many sports teams use "pirate" or a related term such as "raider" or "buccaneer" as their nickname, based on the popular stereotypes of pirates. Such teams include the Pittsburgh Pirates, a Major League Baseball team in Pittsburgh, Pennsylvania: they acquired their nickname in 1891 after "pirating" a player from another team. The Oakland Raiders and Tampa Bay Buccaneers, both in the National Football League, also use pirate-related nicknames.
In research.
Economics of piracy.
Key sources on the economics of piracy include an early study by Cyrus Karraker (1953: Piracy was a Business), in which the author discusses pirates in terms of contemporary racketeering. Patrick Crowhurst researched French piracy and David Starkey focused British 18th century piracy.
Piracy and entrepreneurship.
Recent research ventures embarked on links between entrepreneurship and piracy. In this context, the claim is made for a nonmoral approach to piracy as a source of inspiration for entrepreneurship education as well as for research in entrepreneurship and business model generation.
References.
Bibliography.
</dl>

</doc>
<doc id="50719" url="http://en.wikipedia.org/wiki?curid=50719" title="Quantum harmonic oscillator">
Quantum harmonic oscillator

The quantum harmonic oscillator is the quantum-mechanical analog of the classical harmonic oscillator. Because an arbitrary potential can usually be approximated as a harmonic potential at the vicinity of a stable equilibrium point, it is one of the most important model systems in quantum mechanics. Furthermore, it is one of the few quantum-mechanical systems for which an exact, analytical solution is known.
One-dimensional harmonic oscillator.
Hamiltonian and energy eigenstates.
The Hamiltonian of the particle is:
The functions "Hn" are the physicists' Hermite polynomials,
The corresponding energy levels are
This energy spectrum is noteworthy for three reasons. First, the energies are quantized, meaning that only discrete energy values (integer-plus-half multiples of "ħω") are possible; this is a general feature of quantum-mechanical systems when a particle is confined. Second, these discrete energy levels are equally spaced, unlike in the Bohr model of the atom, or the particle in a box. Third, the lowest achievable energy (the energy of the state, called the ground state) is not equal to the minimum of the potential well, but "ħω"/2 above it; this is called zero-point energy. Because of the zero-point energy, the position and momentum of the oscillator in the ground state are not fixed (as they would be in a classical oscillator), but have a small range of variance, in accordance with the Heisenberg uncertainty principle. This zero-point energy further has important implications in quantum field theory and quantum gravity.
Note that the ground state probability density is concentrated at the origin. This means the particle spends most of its time at the bottom of the potential well, as we would expect for a state with little energy. As the energy increases, the probability density becomes concentrated at the classical "turning points", where the state's energy coincides with the potential energy. This is consistent with the classical harmonic oscillator, in which the particle spends most of its time (and is therefore most likely to be found) at the turning points, where it is the slowest. The correspondence principle is thus satisfied. Moreover, special nondispersive wave packets, with minimum uncertainty, called coherent states in fact oscillate very much like classical objects, as illustrated in the figure; they are "not" eigenstates of the Hamiltonian.
Ladder operator method.
The spectral method solution, though straightforward, is rather tedious. The "ladder operator" method, developed by Paul Dirac, allows us to extract the energy eigenvalues without directly solving the differential equation. Furthermore, it is readily generalizable to more complicated problems, notably in quantum field theory. Following this approach, we define the operators a and its adjoint "a"†,
This leads to the useful representation of ∧"x" and ∧"p", 
The operator a is not Hermitian, since itself and its adjoint "a"† are not equal. Yet the energy eigenstates |"n">, when operated on by these ladder operators, give
It is then evident that "a"†, in essence, appends a single quantum of energy to the oscillator, while a removes a quantum. For this reason, they are sometimes referred to as "creation" and "annihilation" operators.
From the relations above, we can also define a number operator N, which has the following property:
The following commutators can be easily obtained by substituting the canonical commutation relation,
And the Hamilton operator can be expressed as
so the eigenstate of N is also the eigenstate of energy.
The commutation property yields
and similarly,
This means that a acts on |"n"⟩ to produce, up to a multiplicative constant, |"n"–1⟩, and "a"† acts on |"n"⟩ to produce |"n"+1⟩. For this reason, a is called a "lowering operator", and "a"† a "raising operator". The two operators together are called ladder operators. In quantum field theory, a and "a"† are alternatively called "annihilation" and "creation" operators because they destroy and create particles, which correspond to our quanta of energy.
Given any energy eigenstate, we can act on it with the lowering operator, a, to produce another eigenstate with "ħω" less energy. By repeated application of the lowering operator, it seems that we can produce energy eigenstates down to . However, since
the smallest eigen-number is 0, and
In this case, subsequent applications of the lowering operator will just produce zero kets, instead of additional energy eigenstates. Furthermore, we have shown above that
Finally, by acting on |0⟩ with the raising operator and multiplying by suitable normalization factors, we can produce an infinite set of energy eigenstates 
such that
which matches the energy spectrum given in the preceding section.
Arbitrary eigenstates can be expressed in terms of |0⟩, 
The ground state |0⟩ in the position representation is determined by "a" |0⟩ = 0, 
and hence
and so on, as in the previous section.
Natural length and energy scales.
The quantum harmonic oscillator possesses natural scales for length and energy, which can be used to simplify the problem. These can be found by nondimensionalization.
The result is that, if we measure "energy" in units of "ħω" and "distance" in units of √"ħ"/("mω"), then the Hamiltonian simplifies to
while the energy eigenfunctions and eigenvalues simplify to
where "H""n"("x") are the Hermite polynomials.
To avoid confusion, we will not adopt these "natural units" in this article. However, they frequently come in handy when performing calculations, by bypassing clutter.
For example, the fundamental solution () of "H−i∂t", the time-dependent Schroedinger operator for this oscillator, simply boils down to the Mehler kernel,
where . The most general solution for a given initial configuration "ψ"("x",0) then is simply
Phase space solutions.
In the phase space formulation of quantum mechanics, solutions to the quantum harmonic oscillator in several different representations of the quasiprobability distribution can be written in closed form. The most widely used of these is for the Wigner quasiprobability distribution, which has the solution
where
and "Ln" are the Laguerre polynomials.
This example illustrates how the Hermite and Laguerre polynomials are linked through the Wigner map.
"N"-dimensional harmonic oscillator.
The one-dimensional harmonic oscillator is readily generalizable to "N" dimensions, where "N" = 1, 2, 3, ... . In one dimension, the position of the particle was specified by a single coordinate, "x". In "N" dimensions, this is replaced by "N" position coordinates, which we label "x"1, ..., "x""N". Corresponding to each position coordinate is a momentum; we label these "p"1, ..., "p""N". The canonical commutation relations between these operators are
The Hamiltonian for this system is
As the form of this Hamiltonian makes clear, the "N"-dimensional harmonic oscillator is exactly analogous to "N" independent one-dimensional harmonic oscillators with the same mass and spring constant. In this case, the quantities "x"1, ..., "x""N" would refer to the positions of each of the "N" particles. This is a convenient property of the formula_33 potential, which allows the potential energy to be separated into terms depending on one coordinate each.
This observation makes the solution straightforward. For a particular set of quantum numbers {"n"} the energy eigenfunctions for the "N"-dimensional oscillator are expressed in terms of the 1-dimensional eigenfunctions as:
In the ladder operator method, we define "N" sets of ladder operators,
By a procedure analogous to the one-dimensional case, we can then show that each of the "a""i" and "a"†"i" operators lower and raise the energy by ℏω respectively. The Hamiltonian is
This Hamiltonian is invariant under the dynamic symmetry group "U"("N") (the unitary group in "N" dimensions), defined by
where formula_38 is an element in the defining matrix representation of "U"("N").
The energy levels of the system are
As in the one-dimensional case, the energy is quantized. The ground state energy is "N" times the one-dimensional energy, as we would expect using the analogy to "N" independent one-dimensional oscillators. There is one further difference: in the one-dimensional case, each energy level corresponds to a unique quantum state. In "N"-dimensions, except for the ground state, the energy levels are "degenerate", meaning there are several states with the same energy.
The degeneracy can be calculated relatively easily. As an example, consider the 3-dimensional case: Define "n" = "n"1 + "n"2 + "n"3. All states with the same "n" will have the same energy. For a given "n", we choose a particular "n"1. Then "n"2 + "n"3 = "n" − "n"1. There are "n" − "n"1 + 1 possible pairs {"n"2, "n"3}. "n"2 can take on the values 0 to "n" − "n"1, and for each "n"2 the value of "n"3 is fixed. The degree of degeneracy therefore is:
Formula for general "N" and "n" ["g""n" being the dimension of the symmetric irreducible "n"th power representation of the unitary group "U"("N")]:
The special case "N" = 3, given above, follows directly from this general equation. This is however, only true for distinguishable particles, or one particle in N dimensions (as dimensions are distinguishable). For the case of "N" bosons in a one-dimension harmonic trap, the degeneracy scales as the number of ways to partition an integer "n" using integers less than or equal to "N".
This arises due to the constraint of putting "N" quanta into a state ket where formula_44 and formula_45, which are the same constraints as in integer partition.
Example: 3D isotropic harmonic oscillator.
The Schrödinger equation of a spherically-symmetric three-dimensional harmonic oscillator can be solved explicitly by separation of variables; see this article for the present case. This procedure is analogous to the separation performed in the hydrogen-like atom problem, but with the spherically symmetric potential
where μ is the mass of the problem. (Because m will be used below for the magnetic quantum number, mass is indicated by μ, instead of m, as earlier in this article.)
The solution reads
where
are generalized Laguerre polynomials; The order k of the polynomial is a non-negative integer;
The energy eigenvalue is
The energy is usually described by the single quantum number
Because k is a non-negative integer, for every even n we have and for every odd n we have . The magnetic quantum number m is an integer satisfying −ℓ ≤ "m" ≤ ℓ, so for every n and ℓ there are 2"ℓ" + 1 different quantum states, labeled by m . Thus, the degeneracy at level n is
where the sum starts from 0 or 1, according to whether n is even or odd.
This result is in accordance with the dimension formula above, and amounts to the dimensionality of a symmetric representation of "SU"(3), the relevant degeneracy group.
Harmonic oscillators lattice: phonons.
We can extend the notion of a harmonic oscillator to a one lattice of many particles. Consider a one-dimensional quantum mechanical "harmonic chain" of "N" identical atoms. This is the simplest quantum mechanical model of a lattice, and we will see how phonons arise from it. The formalism that we will develop for this model is readily generalizable to two and three dimensions.
As in the previous section, we denote the positions of the masses by "x1,x2...", as measured from their equilibrium positions (i.e. "xi" = 0 if the particle i is at its equilibrium position.) In two or more dimensions, the "xi" are vector quantities. The Hamiltonian for this system is
where m is the (assumed uniform) mass of each atom, and "xi" and "pi" are the position and momentum operators for the "i" th atom and the sum is made over the nearest neighbors (nn). However, it is customary to rewrite the Hamiltonian in terms of the normal modes of the wavevector rather than in terms of the particle coordinates so that one can work in the more convenient Fourier space.
We introduce, then, a set of N "normal coordinates" "Qk", defined as the discrete Fourier transforms of the xs, and N "conjugate momenta" Π defined as the Fourier transforms of the ps,
The quantity "kn" will turn out to be the wave number of the phonon, i.e. 2"π" divided by the wavelength. It takes on quantized values, because the number of atoms is finite.
This preserves the desired commutation relations in either real space or wave vector space
From the general result
it is easy to show, through elementary trigonometry, that the potential energy term is
where
The Hamiltonian may be written in wave vector space as
Note that the couplings between the position variables have been transformed away; if the Qs and Πs were hermitian(which they are not), the transformed Hamiltonian would describe N "uncoupled" harmonic oscillators.
The form of the quantization depends on the choice of boundary conditions; for simplicity, we impose "periodic" boundary conditions, defining the ("N" + 1)th atom as equivalent to the first atom. Physically, this corresponds to joining the chain at its ends. The resulting quantization is
The upper bound to n comes from the minimum wavelength, which is twice the lattice spacing a, as discussed above.
The harmonic oscillator eigenvalues or energy levels for the mode "ωk" are 
If we ignore the zero-point energy then the levels are evenly spaced at 
So an exact amount of energy "ħω", must be supplied to the harmonic oscillator lattice to push it to the next energy level. In comparison to the photon case when the electromagnetic field is quantised, the quantum of vibrational energy is called a phonon.
All quantum systems show wave-like and particle-like properties. The particle-like properties of the phonon are best understood using the methods of second quantization and operator techniques described later.

</doc>
<doc id="50720" url="http://en.wikipedia.org/wiki?curid=50720" title="Buccaneer">
Buccaneer

The buccaneers were pirates who attacked Spanish shipping in the Caribbean Sea during the 17th century. The term "buccaneer" is now used generally as a synonym for "pirate". Originally, buccaneer crews were larger, more apt to attack coastal cities, and more localized to the Caribbean than later pirate crews who sailed to the Indian Ocean on the Pirate Round in the late 17th century.
History.
The term "buccaneer" derives from the Caribbean Arawak word "buccan", a wooden frame for smoking meat, preferably manatee. From this derived the French word "boucane" and hence the name "boucanier" for French hunters who used such frames to smoke meat from feral cattle and pigs on Hispaniola (now Haiti and the Dominican Republic). English colonists anglicised the word "boucanier" to "buccaneer".
About 1630, some Frenchmen who were driven away from the island of Hispaniola fled to nearby Tortuga. The Spaniards tried to drive them out of Tortuga, but the buccaneers were joined by many other French, Dutch and English and turned to piracy against Spanish shipping, generally using small craft to attack galleons in the vicinity of the Windward Passage. Finally they became so strong that they even sailed to the mainland of Spanish America and sacked cities.
English settlers occupying Jamaica began to spread the name "buccaneers" with the meaning of pirates. The name became universally adopted later in 1684 when the first English translation of Alexandre Exquemelin's book "The Buccaneers of America" was published. 
Viewed from London, buccaneering was a low-budget way to wage war on England's rival, Spain. So, the English crown licensed buccaneers with letters of marque, legalizing their operations in return for a share of their profits. The buccaneers were invited by Jamaica's Governor Thomas Modyford to base ships at Port Royal. The buccaneers robbed Spanish shipping and colonies, and returned to Port Royal with their plunder, making the city the most prosperous in the Caribbean. There even were Royal Navy officers sent to lead the buccaneers, such as Christopher Myngs. Their activities went on irrespective of whether England happened to be at war with Spain or France.
Among the leaders of the buccaneers were two Frenchmen, Jean-David Nau, better known as François l'Ollonais, and Daniel Montbars, who destroyed so many Spanish ships and killed so many Spaniards that he was called "the Exterminator".
Another noted leader was a Welshman named Henry Morgan, who sacked Maracaibo, Portobello, and Panama City, stealing a huge amount from the Spanish. Morgan became rich and went back to England, where he was knighted by Charles II. 
In the 1690s, the old buccaneering ways began to die out, as European governments began to discard the policy of "no peace beyond the Line." Buccaneers were hard to control and might embroil their colonies in unwanted wars. Notably, at the 1697 joint French-buccaneer siege of Cartagena, led by Bernard Desjean, Baron de Pointis, the buccaneers and the French regulars parted on extremely bitter terms. Less tolerated by local Caribbean officials, buccaneers increasingly turned to legal work or else joined regular pirate crews who sought plunder in the Indian Ocean, the east coast of North America, or West Africa as well as in the Caribbean.
Legal status.
The status of buccaneers as pirates or privateers was ambiguous. As a rule, the buccaneers called themselves privateers, and many sailed under the protection of a letter of marque granted by British, French or Dutch authorities. For example, Henry Morgan had some form of legal cover for all of his attacks, and expressed great indignation at being called a "corsair" by the governor of Panama. Nevertheless, these rough men had little concern for legal niceties, and exploited every opportunity to pillage Spanish targets, whether or not a letter of marque was available. Many of the letters of marque used by buccaneers were legally invalid, and any form of legal paper in that illiterate age might be passed off as a letter of marque. Furthermore, even those buccaneers who had valid letters of marque often failed to observe their terms; Morgan's 1671 attack on Panama, for instance, was not at all authorized by his commission from the governor of Jamaica. The legal status of buccaneers was still further obscured by the practice of the Spanish authorities, who regarded them as heretics and interlopers, and thus hanged or garrotted captured buccaneers entirely without regard to whether their attacks were licensed by French or English monarchs. 
Simultaneously, French and English governors tended to turn a blind eye to the buccaneers' depredations against the Spanish, even when unlicensed. But as Spanish power waned toward the end of the 17th century, the buccaneers' attacks began to disrupt France and England's merchant traffic with Spanish America. Merchants who had previously regarded the buccaneers as a defence against Spain now saw them as a threat to commerce, and colonial authorities grew hostile. This change in political atmosphere, more than anything else, put an end to buccaneering.
Lifestyle.
A hundred years before the French Revolution, the buccaneer companies were run on lines in which liberty, equality and fraternity were the rule. In a buccaneer camp, the captain was elected and could be deposed by the votes of the crew. The crew, and not the captain, decided whether to attack a particular ship, or a fleet of ships. Spoils were evenly divided into shares; the captain received an agreed amount for the ship, plus a portion of the share of the prize money, usually five or six shares.
Crews generally had no regular wages, being paid only from their shares of the plunder, a system called "no purchase, no pay" by Modyford or "no prey, no pay" by Exquemelin. There was a strong "esprit" among buccaneers. This, combined with overwhelming numbers, allowed them to win battles and raids. There was also, for some time, a social insurance system guaranteeing compensation for battle wounds at a worked-out scale.
Sexuality.
Arizona State historian Barry R. Burg argues that due to a variety of social circumstances, 17th century society produced men who rejected the landed lifestyle and preferred the company of men aboard a ship. In particular, Burg considered court records and historical writings of 17th century England and the Caribbean. Burg draws parallels between modern situational sexuality and analogous homosexual environments within the warrior classes of ancient Greece and Japan to conclude that pirates engaged almost exclusively in homosexual activities.
US Naval officer, pirate researcher and author Benerson Little, though, suggests Burg's view is "extremely speculative" and has dismissed many of the theories Burg drew from his research. Post anarchist author Peter Lamborn Wilson goes further and openly criticizes Burg for, "applying late 19th-century categories to his analysis of the 16th and 17th century pirates".
Warfare.
Naval.
Buccaneers initially used small boats to attack Spanish galleons surreptitiously, often at night, and climb aboard before the alarm could be raised. Buccaneers were expert marksmen and would quickly kill the helmsman and any officers aboard. Buccaneers' reputation as cruel pirates grew to the point that, eventually, most victims would surrender, hoping they would not be killed.
Land.
When buccaneers raided towns, they did not sail into port and bombard the defenses, as naval forces typically did. Instead, they secretly beached their ships out of sight of their target, marched overland, and attacked the towns from the landward side, which was usually less fortified. Their raids relied mainly on surprise and speed. The sack of Campeche was considered the first such raid and many others that followed replicated the same techniques including the attack on Veracruz in 1683 and the raid on Cartagena later that same year.

</doc>
<doc id="50722" url="http://en.wikipedia.org/wiki?curid=50722" title="Sully Prudhomme">
Sully Prudhomme

René François Armand (Sully) Prudhomme (]; 16 March 1839 – 6 September 1907) was a French poet and essayist, and was the first ever winner of the Nobel Prize in Literature, in 1901.
Born in Paris, Prudhomme originally studied to be an engineer, but turned to philosophy and later to poetry; he declared it as his intent to create scientific poetry for modern times. In character sincere and melancholic, he was linked to the Parnassus school, although, at the same time, his work displays characteristics of its own.
Early life.
Prudhomme attended the Lycée Bonaparte, but eye trouble interrupted his studies. He worked for a while in the Creusot region for the Schneider steel foundry, and then began studying law in a notary's office. The favourable reception of his early poems by the "Conférence La Bruyère" (a student society) encouraged him to begin a literary career.
Writing.
His first collection, "Stances et Poèmes" ("Stanzas and Poems", 1865), was praised by Sainte-Beuve. It included his most famous poem, "Le vase brisé". He published more poetry before the outbreak of the Franco-Prussian War. This war, which he discussed in "Impressions de la guerre" (1872) and "La France" (1874), permanently damaged his health.
During his career, Prudhomme gradually shifted from the sentimental style of his first books towards a more personal style which unified the formality of the Parnassus school with his interest in philosophical and scientific subjects. The inspiration was clearly Lucretius's "De rerum natura", for the first book of which he made a verse translation. His philosophy was expressed in "La Justice" (1878) and "Le Bonheur" (1888). The extreme economy of means employed in these poems has, however, usually been judged as compromising their poetical quality without advancing their claims as works of philosophy. He was elected to the Académie française in 1881. Another distinction, "Chevalier de la Légion d’honneur", was to follow in 1895.
After, "Le Bonheur", Prudhomme turned from poetry to write essays on aesthetics and philosophy. He published two important essays: "L'Expression dans les beaux-arts" (1884) and "Réflexions sur l'art des vers" (1892), a series of articles on Blaise Pascal in "La Revue des Deux Mondes" (1890), and an article on free will ("La Psychologie du Libre-Arbitre", 1906) in the "Revue de métaphysique et de morale".
Nobel Prize.
The first writer to receive the Nobel Prize for Literature (given "in special recognition of his poetic composition, which gives evidence of lofty idealism, artistic perfection and a rare combination of the qualities of both heart and intellect"), he devoted the bulk of the money he received to the creation of a poetry prize awarded by the "Société des gens de lettres". He also founded, in 1902, the "Société des poètes français" with Jose-Maria de Heredia and Leon Dierx.
Death.
At the end of his life, his poor health (which had troubled him ever since 1870) forced him to live almost as a recluse at Châtenay-Malabry, suffering attacks of paralysis while continuing to work on essays. He died suddenly on 6 September 1907, and was buried at Père-Lachaise in Paris.
Bibliography.
Poetry.
Each year links to its corresponding "[year] in poetry" article:
Prose.
Each year links to its corresponding "[year] in literature" article:

</doc>
<doc id="50723" url="http://en.wikipedia.org/wiki?curid=50723" title="Convergence of random variables">
Convergence of random variables

In probability theory, there exist several different notions of convergence of random variables. The convergence of sequences of random variables to some limit random variable is an important concept in probability theory, and its applications to statistics and stochastic processes. The same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behaviour that is essentially unchanging when items far enough into the sequence are studied. The different possible notions of convergence relate to how such a behaviour can be characterised: two readily understood behaviours are that the sequence eventually takes a constant value, and that values in the sequence continue to change but can be described by an unchanging probability distribution.
Background.
"Stochastic convergence" formalizes the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle into a pattern. The pattern may for instance be
Some less obvious, more theoretical patterns could be
These other types of patterns that may arise are reflected in the different types of stochastic convergence that have been studied.
While the above discussion has related to the convergence of a single series to a limiting value, the notion of the convergence of two series towards each other is also important, but this is easily handled by studying the sequence defined as either the difference or the ratio of the two series.
For example, if the average of "n" independent random variables "Y""i", "i" = 1, ..., "n", all having the same finite mean and variance, is given by
then as "n" tends to infinity, Xn converges "in probability" (see below) to the common mean, μ, of the random variables "Y""i". This result is known as the weak law of large numbers. Other forms of convergence are important in other useful theorems, including the central limit theorem.
Throughout the following, we assume that ("X""n") is a sequence of random variables, and "X" is a random variable, and all of them are defined on the same probability space formula_2.
Convergence in distribution.
 | header5 = Graphic example
 | data6 = Suppose {"Xi"} is an iid sequence of uniform "U"(−1, 1) random variables. Let formula_3 be their (normalized) sums. Then according to the central limit theorem, the distribution of Zn approaches the normal "N"(0, 1/3) distribution. This convergence is shown in the picture: as n grows larger, the shape of the pdf function gets closer and closer to the Gaussian curve.
With this mode of convergence, we increasingly expect to see the next outcome in a sequence of random experiments becoming better and better modeled by a given probability distribution.
Convergence in distribution is the weakest form of convergence, since it is implied by all other types of convergence mentioned in this article. However convergence in distribution is very frequently used in practice; most often it arises from application of the central limit theorem.
Definition.
A sequence "X"1, "X"2, ... of random variables is said to converge in distribution, or converge weakly, or converge in law to a random variable X if
for every number "x" ∈ R at which F is continuous. Here Fn and F are the cumulative distribution functions of random variables Xn and X, respectively.
The requirement that only the continuity points of F should be considered is essential. For example if Xn are distributed uniformly on intervals (0, 1/"n"), then this sequence converges in distribution to a degenerate random variable . Indeed, for all "n" when "x" ≤ 0, and for all "x" ≥ 1/"n" when "n" > 0. However, for this limiting random variable , even though for all n. Thus the convergence of cdfs fails at the point where F is discontinuous.
Convergence in distribution may be denoted as
where formula_6 is the law (probability distribution) of X. For example if X is standard normal we can write formula_7.
For random vectors {"X"1, "X"2, ...} ⊂ R"k" the convergence in distribution is defined similarly. We say that this sequence converges in distribution to a random k-vector X if
for every "A" ⊂ R"k" which is a continuity set of X.
The definition of convergence in distribution may be extended from random vectors to more general random elements in arbitrary metric spaces, and even to the “random variables” which are not measurable — a situation which occurs for example in the study of empirical processes. This is the “weak convergence of laws without laws being defined” — except asymptotically.
In this case the term weak convergence is preferable (see weak convergence of measures), and we say that a sequence of random elements {"Xn"} converges weakly to X (denoted as "Xn" ⇒ "X") if
for all continuous bounded functions h. Here E* denotes the "outer expectation", that is the expectation of a “smallest measurable function g that dominates "h"("Xn")”.
Convergence in probability.
The basic idea behind this type of convergence is that the probability of an “unusual” outcome becomes smaller and smaller as the sequence progresses.
The concept of convergence in probability is used very often in statistics. For example, an estimator is called consistent if it converges in probability to the quantity being estimated. Convergence in probability is also the type of convergence established by the weak law of large numbers.
Definition.
A sequence {"X""n"} of random variables converges in probability towards the random variable "X" if for all "ε" > 0
Formally, pick any "ε" > 0 and any "δ" > 0. Let Pn be the probability that Xn is outside the ball of radius "ε" centered at "X". Then for Xn to converge in probability to "X" there should exist a number "N" (which will depend on "ε" and "δ") such that for all "n" ≥ "N", Pn < δ.
Convergence in probability is denoted by adding the letter "p" over an arrow indicating convergence, or using the “plim” probability limit operator:
For random elements {"X""n"} on a separable metric space ("S", "d"), convergence in probability is defined similarly by
Properties.
or 
Almost sure convergence.
This is the type of stochastic convergence that is most similar to pointwise convergence known from elementary real analysis.
Definition.
To say that the sequence Xn converges almost surely or almost everywhere or with probability 1 or strongly towards "X" means that
This means that the values of Xn approach the value of "X", in the sense (see almost surely) that events for which Xn does not converge to "X" have probability 0. Using the probability space formula_18 and the concept of the random variable as a function from Ω to R, this is equivalent to the statement
Using the notion of the , almost sure convergence can also be defined as follows:
Almost sure convergence is often denoted by adding the letters "a.s." over an arrow indicating convergence:
For generic random elements {"Xn"} on a metric space ("S", "d"), convergence almost surely is defined similarly:
Sure convergence.
To say that the sequence of random variables ("X""n") defined over the same probability space (i.e., a random process) converges surely or everywhere or pointwise towards "X" means
where Ω is the sample space of the underlying probability space over which the random variables are defined.
This is the notion of pointwise convergence of sequence functions extended to sequence of random variables. (Note that random variables themselves are functions).
Sure convergence of a random variable implies all the other kinds of convergence stated above, but there is no payoff in probability theory by using sure convergence compared to using almost sure convergence. The difference between the two only exists on sets with probability zero. This is why the concept of sure convergence of random variables is very rarely used.
Convergence in mean.
Given a real number "r" ≥ 1, we say that the sequence Xn converges in the "r"-th mean (or in the "Lr"-norm) towards the random variable "X", if the r-th absolute moments E(|"Xn"|"r") and E(|"X"|"r") of Xn and "X" exist, and
where the operator E denotes the expected value. Convergence in r-th mean tells us that the expectation of the r-th power of the difference between "Xn" and "X" converges to zero.
This type of convergence is often denoted by adding the letter "Lr" over an arrow indicating convergence:
The most important cases of convergence in "r"-th mean are:
Convergence in the "r"-th mean, for "r" ≥ 1, implies convergence in probability (by Markov's inequality). Furthermore, if "r" > "s" ≥ 1, convergence in "r"-th mean implies convergence in "s"-th mean. Hence, convergence in mean square implies convergence in mean.
It is also worth noticing that if formula_27, then
Properties.
Provided the probability space is complete:
The chain of implications between the various notions of convergence are noted in their respective sections. They are, using the arrow notation:
These properties, together with a number of other special cases, are summarized in the following list:
References.
</dl>
"This article incorporates material from the Citizendium article "", which is licensed under the but not under the ."

</doc>
<doc id="50724" url="http://en.wikipedia.org/wiki?curid=50724" title="Strong convergence">
Strong convergence

In mathematics, strong convergence may refer to:

</doc>
<doc id="50725" url="http://en.wikipedia.org/wiki?curid=50725" title="Weak convergence">
Weak convergence

In mathematics, weak convergence may refer to: 

</doc>
<doc id="50732" url="http://en.wikipedia.org/wiki?curid=50732" title="Extreme value theory">
Extreme value theory

Extreme value theory or extreme value analysis (EVA) is a branch of statistics dealing with the extreme deviations from the median of probability distributions. It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. Extreme value analysis is widely used in many disciplines, such as structural engineering, finance, earth sciences, traffic prediction, and geological engineering. For example, EVA might be used in the field of hydrology to estimate the probability of an unusually large flooding event, such as the 100-year flood. Similarly, for the design of a breakwater, a coastal engineer would seek to estimate the 50-year wave and design the structure accordingly.
Data analysis.
Two approaches exist for practical extreme value analysis. The first method relies on deriving block maxima (minima) series as a preliminary step. In many situations it is customary and convenient to extract the annual maxima (minima), generating an "Annual Maxima Series" (AMS). The second method relies on extracting, from a continuous record, the peak values reached for any period during which values exceed a certain threshold (falls below a certain threshold). This method is generally referred to as the "Peak Over Threshold" method (POT) and can lead to several or no values being extracted in any given year.
For AMS data, the analysis may partly rely on the results of the Fisher–Tippett–Gnedenko theorem, leading to the generalized extreme value distribution being selected for fitting. However, in practice, various procedures are applied to select between a wider range of distributions. The theorem here relates to the limiting distributions for the minimum or the maximum of a very large collection of independent random variables from the same arbitrary distribution. Given that the number of relevant random events within a year may be rather limited, it is unsurprising that analyses of observed AMS data often lead to distributions other than the generalized extreme value distribution being selected.
For POT data, the analysis involves fitting two distributions: one for the number of events in a basic time period and a second for the size of the exceedances. A common assumption for the first is the Poisson distribution, with the generalized Pareto distribution being used for the exceedances. Some further theory needs to be applied in order to derive the distribution of the most extreme value that may be observed in a given period, which may be a target of the analysis. An alternative target may be to estimate the expected costs associated with events occurring in a given period. For POT analyses, a tail-fitting can be based on the Pickands–Balkema–de Haan theorem.
Applications.
Applications of extreme value theory include predicting the probability distribution of:
History.
The field of extreme value theory was pioneered by
Leonard Tippett (1902–1985). Tippett was employed by the British Cotton Industry Research Association, where he worked to make cotton thread stronger. In his studies, he realized that the strength of a thread was controlled by the strength of its weakest fibres. With the help of R. A. Fisher, Tippet obtained three asymptotic limits describing the distributions of extremes. Emil Julius Gumbel codified this theory in his 1958 book "Statistics of Extremes", including the Gumbel distributions that bear his name.
A summary of historically important publications relating to extreme value theory can be found on the article List of publications in statistics.
Univariate theory.
Let formula_1 be a sequence of independent and identically distributed variables with distribution function "F" and let formula_2 denote the maximum.
In theory, the exact distribution of the maximum can be derived:
The associated indicator function formula_4 is a Bernoulli process with a success probability formula_5 that depends on the magnitude formula_6 of the extreme event. The number of extreme events within formula_7 trials thus follows a binomial distribution and the number of trials until an event occurs follows a geometric distribution with expected value and standard deviation of the same order formula_8.
In practice, we might not have the distribution function formula_9 but the Fisher–Tippett–Gnedenko theorem provides an asymptotic result. If there exist sequences of constants formula_10 and formula_11 such that
as formula_13 then
where formula_15 depends on the tail shape of the distribution.
When normalized, "G" belongs to one of the following non-degenerate distribution families:
Weibull law: formula_16
when the distribution of formula_17 has a light tail with finite upper bound. Also known as Type 3.
Gumbel law: formula_18 when the distribution of formula_17 has an exponential tail. Also known as Type 1
Fréchet Law: formula_20 when the distribution of formula_17 has a heavy tail (including polynomial decay). Also known as Type 2.
In all cases, formula_22.

</doc>
<doc id="50735" url="http://en.wikipedia.org/wiki?curid=50735" title="Prabuty">
Prabuty

Prabuty (German: "Riesenburg") is a town in Kwidzyn County within the Pomeranian Voivodeship of northern Poland. In the period between 1975–98 Prabuty were part of the Elbląg Voivodeship.
Geographical location.
Prabuty is located approximately 18 kilometers east of Kwidzyn, 100 kilometers south-east of Gdańsk, 100 kilometers west of Olsztyn and 133 kilometers south-west of Kaliningrad.
Prabuty is an important rail junction on the Warszawa–Gdynia railway.
History.
In 1236, the Teutonic Knights under Henry III, Margrave of Meissen, destroyed an Old Prussian fortress between the lakes Sorgensee (jez. Dzierzgon) and Liwieniec. The town was first mentioned in 1250 as Riesenburg. The village growing around the castle and received Culm law city rights on 30 October 1330 from bishop Rudolf of Pomesania (1322–1332).
In 1451 the town council joined the Prussian Confederation that opposed the Teutonic Order, but bishop Kaspar Linke expelled the councilors and confiscated their property. After the Battle of Chojnice, in which Polish forces were defeated, the town sided with the Order again.
After the Thirteen Years' War and the Second Peace of Thorn (1466) the town became part of Ducal Prussia although Pomesanian bishops retained their rule over the area. A synod was held in the town in 1556.
Riesenburg suffered during the 17th century Polish-Swedish wars. In 1628 half of it burnt down, and in 1688 it burnt down completely. In 1722 fire caused great destructions.
In 1701,as part of Ducal Prussia, the town became a part of the Kingdom of Prussia and part of the newly created province of West Prussia in 1772. In 1871 the town became part of the German Empire in the framework of the Prussian-led unification of Germany. Until 1919 Riesenburg belonged to the administrative district of Regierungsbezirk Marienwerder in the Province of West Prussia.
After World War I, a referendum was held concerning the future nationality of the town, which remained part of Weimar Germany. From 1920 to 1939 Riesenburg belonged to the administrative district of Regierungsbezirk Westpreußen in the Province of East Prussia and from October 26, 1939, to 1945 to the district Regierungsbezirk Marienwerder in the province of Reichsgau Danzig-West Prussia.
The town was captured by the Soviet Red Army in 1945 during World War II. It then became part of Poland. Most of the native East-Prussian inhabitants were expelled and replaced by Poles from regions east of the Curzon Line, in particular from the former Polish "Kresy Wschodnie".
Heinz Heydrich (1905–44, suicide), brother of Reinhard Heydrich, is buried in a soldiers cemetery Riesenburg, according to the Deutsche Dienststelle (WASt).
External links.
<br>

</doc>
<doc id="50739" url="http://en.wikipedia.org/wiki?curid=50739" title="Iaido">
Iaido

Iaido (居合道, Iaidō), abbreviated with "iai" (居合), is a modern Japanese martial art/sport.
Iaido is associated with the smooth, controlled movements of drawing the sword from its scabbard or saya, striking or cutting an opponent, removing blood from the blade, and then replacing the sword in the scabbard. While new practitioners of iaido may start learning with a wooden sword ("bokken") depending on the teaching style of a particular instructor, most of the practitioners use the blunt edged sword, called iaitō. Few, more experienced, iaido practitioners use a sharp edged sword ("shinken").
Practitioners of iaido are often referred to as "iaidoka".
Origins of the name.
The term 'iaido' appear in 1932 and consists of the kanji characters 居合道. The origin of the first two characters, "iai" (居合), is believed to come from saying "Tsune ni ite, kyū ni awasu" (常に居て、急に合わす), that can be roughly translated as “being constantly (prepared), match/meet (the opposition) immediately”. Thus the primary emphasis in 'iai' is on the psychological state of being present (居). The secondary emphasis is on drawing the sword and responding to the sudden attack as quickly as possible (合).
Last character, 道 is generally translated into English as the way. The term 'iaido' approximately translates into English as "the way of mental presence and immediate reaction", and was popularized by Nakayama Hakudo.
The term emerged from the general trend to replace the suffix "-jutsu" (術) with "-dō" (道) in Japanese martial arts in order to emphasize a philosophical or spiritual aspects of practice.
Purpose of iaido.
Iaido encompasses hundreds of styles of swordsmanship, all of which subscribe to non-combative aims and purposes. Iaido is an intrinsic form of Japanese modern budo.
Iaido is a reflection of the morals of the classical warrior and to build a spiritually harmonious person possessed of high intellect, sensitivity, and resolute will.
Iaido is for the most part performed solo as an issue of kata, executing changed strategies against single or various fanciful rivals. Every kata starts and finishes with the sword sheathed. Notwithstanding sword method, it obliges creative ability and fixation to keep up the inclination of a genuine battle and to keep the kata new. Iaidoka are regularly prescribed to practice kendo to safeguard that battling feel; it is normal for high positioning kendoka to hold high rank in iaido and the other way around.
To appropriately perform the kata, iaidoka likewise learn carriage and development, hold and swing. At times iaidoka will practice accomplice kata like kendo or kenjutsu kata. Dissimilar to kendo, iaido is never honed in a free-competing way.
Moral and religious influence on iaido.
The metaphysical aspects in iaido have been influenced by several philosophical and religious directions. Iaido is a blend of the ethics of Confucianism, methods of Zen, the philosophical Taoism and aspects from bushido.
Seitei-gata techniques.
Because iaido is practiced with a weapon, it is almost entirely practiced using forms, or kata. Multiple person kata exist within some schools of iaido; consequently, iaidoka usually use bokken for such kata practice. Iaido does include competition in form of kata but does not use sparring of any kind. Because of this non-fighting aspect, and iaido's emphasis on precise, controlled, fluid motion, it is sometimes referred to as "moving Zen."
Iaido forms ("kata") are performed solitarily against one or more imaginary opponents. Some iaido schools, however, include "kata" performed in pairs. Most of the styles and schools do not practice "tameshigiri", cutting techniques.
A part of iaido is "nukitsuke". This is a quick draw of the sword, accomplished by simultaneously drawing the sword from the "saya" and also moving the "saya" back in "saya-biki".
History.
Iaido started in the mid-1500s. Hayashizaki Jinsuke Shigenobu (1542 - 1621) is generally acknowledged as the organizer of Iaido. There were a lot of people Koryu ( customary schools), however just a little extent remain today. Just about every one of them additionally concentrate on more seasoned school created amid 16-seventeenth century, in the same way as Muso-Shinden-ryu, Hoki-ryu, Muso-Jikiden-Eishin-ryu, Shinto-Munen-ryu, Tamiya-ryu, Yagyu-Shinkage-ryu, Mugai-ryu, Sekiguchi-ryu, et cetera.
After the collapse of the Japanese feudal system in 1868 the founders of the modern disciplines borrowed from the theory and the practice of classical disciplines as they had studied or practiced. The founding in 1895 of the Dai Nippon Butoku Kai (DNBK) 大日本武徳会 (lit. "Greater Japan Martial Virtue Society") in Kyoto, Japan. was also an important contribution to the development of modern Japanese swordsmanship. In 1932 DNBK officially approved and recognized the Japanese discipline, "iaido"; this year was the first time the term "iaido" appeared in Japan. After this initiative the modern forms of swordsmanship is organised in several iaido organisations. During the post-war occupation of Japan, the Dai Nippon Butoku Kai and its affiliates were disbanded by the Allies of World War II in the period 1945–1950. However, in 1950, the Dai Nippon Butoku Kai was reestablished and the practice of the Japanese martial disciplines began again.
In 1952, the Kokusai Budoin, International Martial Arts Federation (国際武道院・国際武道連盟, Kokusai Budoin Kokusai Budo Renmei) (IMAF) was founded in Tokyo, Japan. IMAF is a Japanese organization promoting international Budō, and has seven divisions representing the various Japanese martial arts, including iaido.
In 1952, the All Japan Kendo Federation (ZNKR) was founded, and the All Japan Iaido Federation (ZNIR) was founded in 1948.
Upon formation of various organizations overseeing martial arts, a problem of commonality appeared. Since members of the organization were drawn from various backgrounds, and had experience practicing different schools of iaido, there arose a need for a common set of kata, that would be known by all members of organization, and that could be used for fair grading of practitioner's skill. Two of the largest Japanese organizations, All Japan Kendo Federation (ZNKR) and All Japan Iaido Federation (ZNIR), each created their own representative set of kata for this purpose.
The role of Iaido in Kendo.
Despite the fact that the purposes of assault in current Kendo are entirely restricted, the strikes and assaults are performed with an opportunity of will that definitely prompts a component of rivalry.
In correlation with shinai Kendo, Iaido focus on preparing to create right developments. Therefore, regarding specialized immaculateness it involves a level much higher than that of shinai Kendo. In short, Iaido can serve to enhance and keep up specialized virtue in shinai Kendo. Iaido aides guarantee that body developments are legitimate and compelling in light of the fact that they are regular, precise, and spry.
Kata under the respective iaido organizations.
Tōhō Iaido.
The All Japan Iaido Federation (ZNIR, "Zen Nihon Iaido Renmei", founded 1948) has a set of five koryu iaido forms, called Tōhō, contributed from the five major schools that comprise the organization.
Seitei Iaido.
Seitei or Zen Nippon Kendo Renmei Iaido (制定) ("basis of the Iaido") are technical based on "seitei-gata", or standard form of sword-drawing techniques, created by the Zen Nihon Kendo Remmei (All Japan Kendo Federation) and the Zen Nihon Iaido Remmei (All Japan Iaido Federation). This standard set of iaido kata was created in 1968 by a committee formed by the All Japan Kendo Federation (AJKF, "Zen Nippon Kendo Renmei" or ZNKR).
The twelve Seitei iaido forms ("seitei-gata") are standardised for the tuition, promotion and propagation of iaido at the iaido clubs, that are members of the regional Kendo federations. All dojos, that are members of the regional Kendo federations teach this set. Since member federations of International Kendo Federation (FIK) uses seitei gata as a standard for their iaido exams and "shiai", "seitei iaido" has become the most widely practised form of iaido in Japan and the rest of the world.
Other organizations.
Single-style federations usually do not have a standardized "grading" set of kata, and use kata from their koryu curriculum for grading and demonstrations.
Iaido schools.
Many iaido organisations promote sword technique from the seiza (sitting position) and refer to their art as "iaido". One of the popular versions of these is the Musō Shinden-ryū 夢想神伝流, a iaido system created by Nakayama Hakudō (1872–1958) in the 1932. The Musō Shinden-ryū is an interpretation of one of the Jinsuke-Eishin lines, called Shimomura-ha.
The other line of Jinsuke-Eishin, called Tanimura-ha, was created by Gotō Magobei Masasuke] (died 1898) and Ōe Masamichi Shikei (1852–1927). It was Ōe Masamichi Shikei who began formally referring his iaido branch as the Musō Jikiden Eishin-ryū 無双直伝英信流 during the Taishō era (1912–1926).
Ranks in iaido.
The ranks in iaido is based on the modern kyu-dan system, created in 1883, that conform to the policies and criteria established by the iaido organisation concerned.
Kendo is unequivocally sorted out, with most kendo represented by a solitary league in every nation getting heading from the Global Kendo Alliance. There are exemptions to this, for example, the tragic part into two organizations in the United States. Iaido is generally associated with either the IKF/ZNKR or the Zen-Nippon Iaido Renmei (ZNIR).
Kendo and iaido have a nine dan arrangement of positioning. A commonplace advisory group for first dan would be six or more individuals positioned fifth dan or higher. Regularly, greater panels are utilized for higher positions, if enough qualified individuals are accessible. For IKF-partnered associations, obligation regarding meeting the positions rests with every part nation, yet every other nation is certain to perceive positions recompensed by part nations.
International Iaido Sport Competition.
Iaido, in its modern form, is practiced as a competitive sport, regulated by the All Japan Kendo Federation. The AJKF maintains the standardized iaido kata and etiquette, and organizes competitions.
An iaido competition consists of two iaidoka performing their kata next to each other and simultaneously. The competitors will be judged by a panel of judges according to the standardized regulations.
European Kendo Federation has arranged iaido championships since 2000, where this competition are held every year.
The 2010 European Iaido Championship finals can be seen at the following link to YouTube: 
Iaido organisations.
Many national and regional organisations manage and promote iaido activities. Organisations which on the international level include iaido are following organisations:
Dai Nippon Butoku Kai (DNBK) established in 1895 in Kyoto, approved and recognized the discipline iaido.
The International Martial Arts Federation (IMAF) was established in Kyoto in 1952 and is dedicated to the promotion and development of the martial arts worldwide, including iaido.
International Kendo Federation (FIK), established in 1970, an international organization for Kendo, Iaido and Jodo practitioners, which many national Kendo federations are a members of.
The World Musō Jikiden Eishin-ryū Iaido Federation, established in Tokyo in 2011, is dedicated to ensuring the orthodox transmission of MJER Iaido to future generations worldwide, as well as promoting and preserving the development of other schools.
External links.
Following organisations are national Iaido federations in Japan:
Iaido in the United States is under the auspices of the (AUSKF) and the various regional Kendo federations that are members of the AUSKF.

</doc>
<doc id="50740" url="http://en.wikipedia.org/wiki?curid=50740" title="Daishō">
Daishō

The daishō (大小, daishō) - literally "big-little" is a Japanese term for a matched pair of traditionally made Japanese swords worn by the samurai class in feudal Japan.
Description.
The etymology of the word "daishō" becomes apparent when the terms "daitō", meaning long sword, and "shōtō", meaning short sword, are used; daitō" + shōtō" = "daishō". A "daishō" is typically depicted as a katana and wakizashi mounted in matching but originally the "daishō" was the wearing of any long and short uchigatana together. The "katana/wakizashi" pairing is not the only "daishō" combination as generally any longer sword paired with a tantō is considered to be a "daishō". "Daishō" eventually came to mean two swords having a . A "daishō" could also have matching blades made by the same swordsmith, but this was in fact uncommon and not necessary for two swords to be considered to be a "daishō", as it would have been more expensive for a samurai.
History.
The concept of the "daisho" originated with the pairing of a short sword with whatever long sword was being worn during a particular time period. It has been noted that the "tachi" would be paired with a "tantō", and later the "uchigatana" would be paired with another shorter "uchigatana". With the advent of the katana, the "wakizashi" eventually was chosen by samurai as the short sword over the "tantō". Kanzan Satō in his book titled "The Japanese Sword" notes that there did not seem to be any particular need for the "wakizashi" and suggests that the "wakizashi" may have become more popular than the "tantō" due to the "wakizashi" being more suited for indoor fighting. He mentions the custom of leaving the "katana" at the door of a castle or palace when entering while continuing to wear the "wakizashi" inside.
According to most traditional "kenjutsu" schools, only one sword of the "daisho" would have been used in combat. However, in the first half of the 17th century, the famous swordsman Miyamoto Musashi promoted the use of a one-handed grip, which allowed both swords to be used simultaneously. This technique, called "nitōken", is a main element of the "Niten Ichi-ryū" style of swordsmanship that Musashi founded.
The wearing of "daishō" was limited to the samurai class after the sword hunt of Toyotomi Hideyoshi in 1588, and became a symbol or badge of their rank. "Daishō" may have become popular around the end of the Muromachi period (1336 to 1573) as several early examples date from the late sixteenth century. An edict in 1629 defining the duties of a samurai required the wearing of a "daishō" when on official duty. During the Meiji period an edict was passed in 1871 abolishing the requirement of the wearing of "daishō" by samurai, and in 1876 the wearing of swords in public by most of Japan's population was banned; this ended the use of the "daishō" as the symbol of the samurai, and the samurai class was abolished soon after the sword ban.

</doc>
<doc id="50744" url="http://en.wikipedia.org/wiki?curid=50744" title="Return of the Jedi">
Return of the Jedi

Return of the Jedi (also known as Star Wars Episode VI: Return of the Jedi) is a 1983 American epic space opera film directed by Richard Marquand. The screenplay by Lawrence Kasdan and George Lucas was from a story by Lucas, who was also the executive producer. It was the third film released in the "Star Wars" saga and the first film to use THX technology. The film is set one year after "The Empire Strikes Back" and was produced by Howard Kazanjian for Lucasfilm Ltd. The film stars Mark Hamill, Harrison Ford, Carrie Fisher, Billy Dee Williams, Anthony Daniels, David Prowse, Kenny Baker, Peter Mayhew and Frank Oz.
The evil Galactic Empire, under the direction of the ruthless Emperor Sheev Palpatine, is constructing a second Death Star in order to crush the Rebel Alliance once and for all. Since Palpatine plans to personally oversee the final stages of its construction, the Rebel Fleet launches a full-scale attack on the Death Star in order to prevent its completion and kill Palpatine, effectively bringing an end to the Empire's hold over the galaxy. Meanwhile, Luke Skywalker, the Rebel leader and Jedi apprentice, struggles to bring Darth Vader, who is his father Anakin and himself a fallen Jedi, back from the Dark Side of the Force.
David Lynch and David Cronenberg were considered to direct the project before Marquand signed on as director. The production team relied on Lucas' storyboards during pre-production. While writing the shooting script, Lucas, Kasdan, Marquand, and producer Howard Kazanjian spent two weeks in conference discussing ideas to construct it. Kazanjian's schedule pushed shooting to begin a few weeks early to allow Industrial Light & Magic more time to work on the film's effects in post-production. Filming took place in England, California, and Arizona from January to  1982 (1982-). Strict secrecy surrounded the production and the film used the working title "Blue Harvest" to prevent price gouging.
The film was released in theaters on May 25, 1983, receiving mostly positive reviews. The film grossed over $475 million worldwide. Several home video and theatrical releases and revisions to the film followed over the next 20 years. "Star Wars" continued with ' as part of the film series' prequel trilogy. A sequel, ', was announced on October 30, 2012, and it is set for release on December 18, 2015.
Plot.
Luke Skywalker initiates a plan to rescue Han Solo from the crime lord Jabba the Hutt with the help of Princess Leia, Lando Calrissian, Chewbacca, C-3PO, and R2-D2. Leia infiltrates Jabba's palace on Tatooine disguised as a bounty hunter with Chewbacca as her prisoner. Lando is already there disguised as a guard. Leia releases Han from his carbonite prison, but she is captured and enslaved. Luke arrives soon afterward but after a tense standoff, he is captured. After Luke survives his battle with Jabba's Rancor, Jabba sentences him and Han to death by feeding them to the sarlacc. Luke frees himself and battles Jabba's guards. During the chaos, Boba Fett, who has been working for Jabba since delivering Han, attempts to attack Luke but Han inadvertently knocks him into the Sarlaac pit. Meanwhile, Leia strangles Jabba to death, and Luke destroys Jabba's sail barge as the group escape. While Han and Leia rendezvous with the Rebel Alliance, Luke returns to Dagobah where he finds that Yoda is dying. As he dies, Yoda confirms that Darth Vader, once known as Anakin Skywalker, is Luke's father and there is "another Skywalker". The spirit of Obi-Wan Kenobi confirms that this other Skywalker is Luke's twin sister, Leia. Obi-Wan tells Luke that he must fight Vader again to defeat the Empire.
The Rebel Alliance learns that the Empire has been constructing a new Death Star under the supervision of Vader and his master Emperor Sheev Palpatine. In a plan to destroy the new weapon, Han leads a strike team to destroy the battle station's shield generator on the forest moon of Endor, allowing a squadron of starfighters to enter the incomplete infrastructure and destroy the station from within. The strike team, accompanied by Luke, travels to Endor in a stolen Imperial shuttle. On Endor, Luke and his companions encounter a tribe of Ewoks and, after an initial conflict, form a partnership with them. Later, Luke confesses to Leia that she is his sister, Vader is their father, and that he is leaving to confront him. Surrendering to Imperial troops, Luke is brought to Vader and unsuccessfully tries to convince him to turn from the dark side of the Force.
Vader takes Luke to the Death Star to meet Palpatine, intent on turning his son to the dark side. Palpatine reveals that the Death Star is fully operational and set to destroy the Rebellion. On Endor, Han's strike team is captured by Imperial forces, but a surprise counterattack by the Ewoks allows the Rebels to launch an attack. Meanwhile, Lando, piloting the "Millennium Falcon", leads the Rebel fleet to the Death Star, only to find that the station's shield is still active and the Imperial fleet is waiting for them. Palpatine tempts Luke to give in to his anger and join the dark side of the Force, and Luke engages Vader in a lightsaber duel. Vader discovers that Luke has a sister, and threatens to turn her to the dark side. Enraged, Luke attacks Vader and severs his father's right hand. Palpatine entreats Luke to kill Vader and take his place, but Luke refuses, declaring himself a Jedi. On Endor, the strike team defeats the Imperial forces and destroys the shield generator, allowing the Rebel fleet to launch their final assault on the Death Star. At the same time, a furious Palpatine tortures Luke with Force lightning. Unwilling to let his son die, Vader becomes Anakin Skywalker again and kills Palpatine, therefore fulfilling the prophecy that he would be the one to destroy the Sith and restore balance to the force but is mortally wounded in the process. At his father's request, Luke removes Anakin's mask to see his scarred face, allowing Anakin to look on his son with his own eyes before dying in Luke's arms.
Lando leads the remaining ships into the station's core and destroys the main reactor as the Imperial fleet is overwhelmed. Luke escapes on an Imperial shuttle with his father's body, while Lando escapes in the "Falcon" moments before the Death Star explodes. On Endor, Leia reveals to Han that Luke is her brother, and they share a kiss. Luke returns to Endor and cremates his father's armor on a funeral pyre. As the Rebels celebrate the end of the Empire, Luke sees the spirits of Obi-Wan, Yoda, and the redeemed Anakin watching over them.
Cast and characters.
Supporting cast.
To portray the numerous alien species featured in the film a multitude of puppeteers, voice actors, and stunt performers were employed. They included:
Additional voices were provided by; Annie Arbogast (Sy Snootles), Erik Bauersfeld (Ackbar/Bib Fortuna "- uncredited"), Ben Burtt (Droid/R2D2), Denny Delk (Ewoks "- uncredited"), Ernie Fosselius (Malakili/Giran "- uncredited"), Richard Marquand (EV-9D9), Kipsang Rotich (Nien Nunb "- uncredited") and Pat Welsh (Boushh "- uncredited")
Production.
Development.
As with the previous film, Lucas personally funded "Return of the Jedi". Lucas approached David Lynch, who had been nominated for the Academy Award for Best Director for "The Elephant Man" in 1980, to helm "Return of the Jedi", but Lynch declined in order to direct "Dune". David Cronenberg was also offered the chance to direct the film, but he declined the offer to make "Videodrome" and "The Dead Zone". Lucas eventually chose Richard Marquand. Lucas may have directed some of the second unit work personally as the shooting threatened to go over schedule; this is a function Lucas had willingly performed on previous occasions when he had only officially been producing a film (e.g. "More American Graffiti", "Raiders of the Lost Ark"). Lucas did operate a B camera on the set a few times. Lucas himself has admitted to being on the set frequently due to Marquand's relative inexperience with special effects. Lucas praised Marquand as a "very nice person who worked well with actors". Marquand did note that Lucas kept a conspicuous presence on set, joking, "It is rather like trying to direct "King Lear" – with Shakespeare in the next room!"
The screenplay was written by Lawrence Kasdan and Lucas (with uncredited contributions by David Peoples and Marquand), based on Lucas' story. Kasdan claims he told Lucas that "Return of the Jedi" was "a weak title", and Lucas later decided to name the film "Revenge of the Jedi". The screenplay itself was not finished until rather late in pre-production, well after a production schedule and budget had been created by Kazanjian and Marquand had been hired, which was unusual for a film. Instead, the production team relied on Lucas' story and rough draft in order to commence work with the art department. When it came time to formally write a shooting script, Lucas, Kasdan, Marquand and Kazanjian spent two weeks in conference discussing ideas; Kasdan used tape transcripts of these meetings to then construct the script. The issue of whether Harrison Ford would return for the final film arose during pre-production. Unlike the other stars of the first film, Ford had not contracted to do two sequels, and "Raiders of the Lost Ark" had made him an even bigger star. Ford suggested that Han Solo be killed through self-sacrifice. Kasdan concurred, saying it should happen near the beginning of the film to instill doubt as to whether the others would survive, but Lucas was vehemently against it and rejected the concept. Yoda was originally not meant to appear in the film, but Marquand strongly felt that returning to Dagobah was essential to resolve the dilemma raised by the previous film. The inclusion led Lucas to insert a scene in which Yoda confirms that Darth Vader is Luke's father because, after a discussion with a children's psychologist, he did not want younger moviegoers to dismiss Vader's claim as a lie. Many ideas from the original script were left out or changed. For instance, the Ewoks were going to be Wookiees, the "Millennium Falcon" would be used in the arrival at the forest moon of Endor, and Obi-Wan Kenobi would return to life from his spectral existence in the Force.
Gary Kurtz, who produced "Star Wars" and "The Empire Strikes Back" but was replaced as producer for "Return of the Jedi", claimed in 2010 that the ongoing success with "Star Wars" merchandise and toys led George Lucas to reconsider the idea of killing off Han Solo in the middle part of the film during a raid on an Imperial base. Luke Skywalker was also to have walked off alone and exhausted like the hero in a Spaghetti Western, but Lucas opted for a happier ending to encourage higher merchandise sales.
Filming.
Filming began on January 11, 1982 and lasted through May 20, 1982, a schedule six weeks shorter than "The Empire Strikes Back". Kazanjian's schedule pushed shooting as early as possible in order to give Industrial Light & Magic (ILM) as much time as possible to work on effects, and left some crew members dubious of their ability to be fully prepared for the shoot. Working on a budget of $32.5 million, Lucas was determined to avoid going over budget as had happened with "The Empire Strikes Back". Producer Howard Kazanjian estimated that using ILM (owned wholly by Lucasfilm) for special effects saved the production approximately $18 million. However, the fact that Lucasfilm was a non-union company made acquiring shooting locations more difficult and more expensive, even though "Star Wars" and "The Empire Strikes Back" had been big hits. The project was given the working title "Blue Harvest" with a tagline of "Horror Beyond Imagination." This disguised what the production crew was really filming from fans and the press, and also prevented price gouging by service providers.
The first stage of production started with 78 days at Elstree Studios in England, where the film occupied all nine stages. The shoot commenced with a scene later deleted from the finished film where the heroes get caught in a sandstorm as they leave Tatooine. (This was the only major sequence cut from the film during editing.) While attempting to film Luke Skywalker's battle with the rancor beast, Lucas insisted on trying to create the scene in the same style as Toho's "Godzilla" films by using a stunt performer inside a suit. The production team made several attempts, but were unable to create an adequate result. Lucas eventually relented and decided to film the rancor as a high-speed puppet. In April, the crew moved to the Yuma Desert in Arizona for two weeks of Tatooine exteriors. Production then moved to the redwood forests of northern California near Crescent City where two weeks were spent shooting the Endor forest exteriors, and then concluded at ILM in San Rafael, California for about ten days of bluescreen shots. One of two "skeletal" post-production units shooting background matte plates spent a day in Death Valley. The other was a special Steadicam unit shooting forest backgrounds from June 15–17, 1982 for the speeder chase near the middle of the film. Steadicam inventor Garrett Brown personally operated these shots as he walked through a disguised path inside the forest shooting at less than one frame per second. By walking at about 5 mi/h and projecting the footage at 24 frame/s, the motion seen in the film appeared as if it were moving at around 120 mi/h.
Music.
John Williams composed and conducted the film's musical score with performances by the London Symphony Orchestra. Orchestration credits also include Thomas Newman. The initial release of the film's soundtrack was on the RSO Records label in the United States. Sony Classical Records acquired the rights to the classic trilogy scores in 2004 after gaining the rights to release the second trilogy soundtracks ("The Phantom Menace" and "Attack of the Clones"). In the same year, Sony Classical re-pressed the 1997 RCA Victor release of "Return of the Jedi" along with the other two films in the trilogy. The set was released with the new artwork mirroring the first DVD release of the film. Despite the Sony digital re-mastering, which minimally improved the sound heard only on high-end stereos, this 2004 release is essentially the same as the 1997 RCA Victor release.
Post-production.
Meanwhile, special effects work at ILM quickly stretched the company to its operational limits. While the R&D work and experience gained from the previous two films in the trilogy allowed for increased efficiency, this was offset by the desire to have the closing film raise the bar set by each of these films. A compounding factor was the intention of several departments of ILM to either take on other film work or decrease staff during slow cycles. Instead, as soon as production began, the entire company found it necessary to remain running 20 hours a day on six-day weeks in order to meet their goals by April 1, 1983. Of about 900 special effects shots, all VistaVision optical effects remained in-house, since ILM was the only company capable of using the format, while about 400 4-perf opticals were subcontracted to outside effects houses. Progress on the opticals was severely retarded for a time due to ILM rejecting about 100000 ft of film when the film perforations failed image registration and steadiness tests.
Release.
"Return of the Jedi"‍ '​s theatrical release took place on May 25, 1983. It was originally slated to be May 27, but was subsequently changed to coincide with the date of the 1977 release of the original "Star Wars" film. With a massive worldwide marketing campaign, illustrator Tim Reamer created the image for the movie poster and other advertising. At the time of its release, the film was advertised on posters and merchandise as simply "Star Wars: Return of the Jedi", despite its on-screen "Episode VI" distinction. The original film was later re-released to theaters in 1985.
In 1997, for the 20th anniversary of the release of "Star Wars" (retitled "Episode IV: A New Hope"), Lucas released "The Star Wars Trilogy: Special Edition". Along with the two other films in the original trilogy, "Return of the Jedi" was re-released on March 14, 1997 with a number of changes and additions, which included the insertion of several alien band members in Jabba's throne room, the modification of the Sarlacc to include a beak, the replacement of music at the closing scene, and a montage of different alien worlds celebrating the fall of the Empire. According to Lucas, "Return of the Jedi" required fewer changes than the previous two films because it is more emotionally driven than the others. The changes have caused controversy among the fans as some believe that they detract from the films.
Title change.
The original teaser trailer for the film carried the name "Revenge of the Jedi". In December 1982, Lucas decided that "Revenge" was not appropriate (as Jedi should not seek revenge) and returned to his original title. By that time thousands of "Revenge" teaser posters (with artwork by Drew Struzan) had been printed and distributed. Note that the poster changes the correct color of the light sabers; Luke is seen wielding a red lightsaber while Vader wields a blue one, while in the movie Luke's saber is green and Vader's red. Lucasfilm stopped the shipping of the posters and sold the remaining stock of 6,800 posters to "Star Wars" fan club members for $9.50.
"", released in 2005 as part of the prequel trilogy, later alluded to the dismissed title of "Jedi", "Revenge of the Jedi".
Home media.
The original theatrical version of "Return of the Jedi" was released on VHS and Laserdisc several times between 1986 and 1995, followed by releases of the Special Edition in the same formats between 1997 and 2000. Some of these releases contained featurettes; some were individual releases of just this film, while others were boxed sets of all three original films.
On September 21, 2004, the Special Editions of all three original films were released in a boxed set on DVD (along with a bonus disc). It was digitally restored and remastered, with additional changes made by George Lucas. The DVD also featured English subtitles, Dolby Digital 5.1 EX surround sound, and commentaries by George Lucas, Ben Burtt, Dennis Muren, and Carrie Fisher. The bonus disc included documentaries including "Empire of Dreams: The Story of the Star Wars Trilogy" and several featurettes including "The Characters of Star Wars", "The Birth of the Lightsaber", and "The Legacy of Star Wars". Also included were teasers, trailers, TV spots, still galleries, and a demo for "".
With the release of "Revenge of the Sith", which depicts how and why Anakin Skywalker turned to the dark side of the Force, Lucas once again altered "Return of the Jedi" to bolster the relationship between the original trilogy and the prequel trilogy. The original and 1997 Special Edition versions of "Return of the Jedi" featured British theatre actor Sebastian Shaw playing both the dying Anakin Skywalker and his ghost. In the 2004 DVD and 2011 Blu-ray release, Shaw's portrayal of Anakin's ghost is replaced by Hayden Christensen, who portrayed Anakin in ' and '. The change drew further fan criticism directed toward Lucas.
All three films in the original unaltered "Star Wars" trilogy were later released, individually, on DVD on September 12, 2006. These versions were originally slated to only be available until December 31, 2006, although they remained in print until May 2011 and were packaged with the 2004 versions again in a new box set on November 4, 2008. Although the 2004 versions in these sets each feature an audio commentary, no other extra special features were included to commemorate the original cuts.
A Blu-ray Disc version of the "Star Wars" saga (Special Edition versions only) was announced for release in 2011 during Star Wars Celebration V. Several deleted scenes from "Return of the Jedi" were included for the Blu-ray version, including a sandstorm sequence following the Battle at the Sarlacc Pit, a scene featuring Moff Jerjerrod and Death Star officers during the Battle of Endor, and a scene where Darth Vader communicates with Luke via the Force as Skywalker is assembling his new lightsaber before he infiltrates Jabba's palace. On January 6, 2011, 20th Century Fox Home Entertainment announced the Blu-ray release for September 2011 in three different editions and the cover art was unveiled in May. Although selling well, the Blu-ray caused yet more criticism towards Lucas as the set featured further alterations and the original versions were not included.
Although unconfirmed, it was reported on August 16, 2014 that Disney/Lucasfilm plan to release the unaltered original trilogy on Blu-ray in 2015 prior to the release of "", which will be released to theaters on December 18, 2015.
Digital release.
On April 7, 2015, the Walt Disney Studios, 20th Century Fox, and Lucasfilm jointly announced the digital releases of the six released "Star Wars" films. As Lucasfilm had retained digital distribution rights to all Episodes sans IV, Walt Disney Studios Home Entertainment released "Return of the Jedi" for digital download on April 10, 2015.
Reception.
Although a critical and commercial hit, grossing more than $475 million worldwide, "Return of the Jedi" has, in the decades that followed, been considered by many critics and fans to be a slightly lesser achievement than its predecessors. At Rotten Tomatoes, "Return of the Jedi"'s 79% approval rating is surpassed by "The Empire Strikes Back" (96%), "A New Hope" (93%), and the final film of the prequel trilogy, "Revenge of the Sith" (80%).
On Metacritic, the film received a score of 52% based on 14 reviews from mainstream critics, and "The Empire Strikes Back" received a score of 78% based on 15 reviews.
Contemporary critics were largely complimentary. In 1983, movie critic Roger Ebert gave the film four stars out of four, and James Kendrick of Q Network Film Desk described "Return of the Jedi" as "a magnificent experience." The film was also featured on the May 23, 1983 "TIME magazine" cover issue (where it was labeled "Star Wars III"), with the reviewer Gerald Clarke saying that while it was not as exciting as the first "Star Wars" film, it was "better and more satisfying" than "The Empire Strikes Back", now considered by many as the best of the original trilogy. Vincent Canby, who enjoyed the first film and despised the second, felt that "Return of the Jedi" was the worst of all three. ReelViews.net's James Berardinelli noted for the 1997 special edition re-release that "Although it was great fun re-watching "Star Wars" and "The Empire Strikes Back" again on the big screen, "Return of the Jedi" doesn't generate the same sense of enjoyment. And, while Lucas worked diligently to re-invigorate each entry into the trilogy, Jedi needs more than the patches of improved sound, cleaned-up visuals, and a few new scenes. Still, despite the flaws, this is still "Star Wars", and, as such, represents a couple of lightly-entertaining hours spent with characters we have gotten to know and love over the years. "Return of the Jedi" is easily the weakest of the series, but its position as the conclusion makes it a must-see for anyone who has enjoyed its predecessor."
According to Rotten Tomatoes, Gene Siskel of the "Chicago Tribune" was somewhat critical of the film during the 1997 re-release, stating that it "Lack[s] the humanity and richly drawn characters that brighten "Star Wars"." Siskel later gave "Return of the Jedi" thumbs up on the television show "Siskel & Ebert" during the release of "The Star Wars Trilogy: Special Edition", saying: "This is my least favorite of the three episodes. That doesn't make it bad, the others are just a lot better." Siskel went on to praise the opening sequence at the Sarlacc pit and the chase sequence involving speeder bikes, but he states his dislike for the closing scenes involving the Ewoks. The "New York Post"'s Rex Reed gave the film a negative review, stating "Let's not pretend we're watching art!" Writing in "The New Yorker", Pauline Kael (who had praised "The Empire Strikes Back" after dismissing "Star Wars") called it "an impersonal and rather junky piece of filmmaking."
While the action set pieces – particularly the Sarlacc battle sequence, the speeder bike chase on the Endor moon, the space battle between Rebel and Imperial pilots, and Luke Skywalker's duel against Darth Vader – are well-regarded, the ground battle between the Ewoks and Imperial stormtroopers remains a bone of contention. Fans are also divided on the likelihood of Ewoks (being an extremely primitive race of small creatures armed with sticks and rocks) defeating an armed ground force comprising the Empire's "best troops". Lucas has defended the scenario, saying that the Ewoks' purpose was to distract the Imperial troops and that the Ewoks did not really win.
Accolades.
At the 56th Academy Awards in 1984, Richard Edlund, Dennis Muren, Ken Ralston, and Phil Tippett received the "Special Achievement Award for Visual Effects." Norman Reynolds, Fred Hole, James L. Schoppe, and Michael Ford were nominated for "Best Art Direction/Set Decoration". Ben Burtt received a nomination for "Best Sound Effects Editing". John Williams received the nomination for "Best Music, Original Score". Burtt, Gary Summers, Randy Thom and Tony Dawe all received the nominations for "Best Sound". At the 1984 BAFTA Awards, Edlund, Muren, Ralston, and Kit West won for "Best Special Visual Effects". Tippett and Stuart Freeborn were also nominated for "Best Makeup". Reynolds received a nomination for "Best Production Design/Art Direction". Burtt, Dawe, and Summers also received nominations for "Best Sound". Williams was also nominated "Best Album of Original Score Written for a Motion Picture or Television Special". The film also won for "Best Dramatic Presentation", the older award for science fiction and fantasy in film, at the 1984 Hugo Awards.
American Film Institute Lists
Marketing.
Novelization.
The novelization of "Return of the Jedi" was written by James Kahn and was released on May 12, 1983, thirteen days before the film's release. It contains many scenes that were deleted from the final cut as well as certain assertions which have since been superseded by the prequel trilogy. For example, Kahn writes that Owen Lars is the brother of Obi-Wan Kenobi, while in "Attack of the Clones" he is instead shown to be the stepbrother of Anakin Skywalker. When Leia is captured by Jabba, instead of him saying "I'm sure" to her warning of her powerful friends, he says, "I'm sure, but in the meantime, I shall thoroughly enjoy the pleasure of your company." Additionally, instead of simply licking his lips as seen in the movie, he is described as planting "a beastly kiss squarely on the Princess's lips." Later, the Force spirit of Obi-Wan reveals that he was able to hide Luke and Leia from Anakin because he did not know that his wife was pregnant when he "left," presumably when he became Vader. This is partly contradicted by "Revenge of the Sith", in which Anakin is unaware his wife was expecting twins and believes their child died with her. A facet of the story which was made more clear in the novel was the confusion which overtook the Imperial forces upon the death of Palpatine, who ceased to be the guiding will animating the Empire. It also further supports the events depicted in all post-"Return of the Jedi" fiction.
Radio drama.
A radio drama adaptation of the film was written by Brian Daley with additional material contributed by John Whitman and was produced for and broadcast on National Public Radio in 1996. It was based on characters and situations created by George Lucas and on the screenplay by Lawrence Kasdan and George Lucas. The first two "Star Wars" films were similarly adapted for National Public Radio in the early 1980s, but it was not until 1996 that a radio version of "Return of the Jedi" was heard. Anthony Daniels returned as C-3PO, but Mark Hamill and Billy Dee Williams did not reprise their roles as they had for the first two radio dramas. They were replaced by newcomer Joshua Fardon as Luke Skywalker and character actor Arye Gross as Lando Calrissian. John Lithgow voiced Yoda, whose voice actor in the films has always been Frank Oz. Bernard Behrens returned as Obi-Wan Kenobi and Brock Peters reprised his role as Darth Vader. Veteran character actor Ed Begley, Jr. played Boba Fett. Edward Asner also guest-starred speaking only in grunts as the voice of Jabba the Hutt. The radio drama had a running time of three hours.
Principal production of the show was completed on February 11, 1996. Only hours after celebrating its completion with the cast and crew of the show, Daley died of pancreatic cancer. The show is dedicated to his memory.
The cast and crew recorded a get-well message for Daley, but the author never got the chance to hear it. The message is included as part of the Star Wars Trilogy collector's edition box set.
Comic book adaptation.
Marvel Comics published a comic book adaptation of the film by writer Archie Goodwin and artists Al Williamson, Carlos Garzon, Tom Palmer, and Ron Frenz. The adaptation appeared in "Marvel Super Special" #27 and as a four-issue limited series. It was later reprinted in a mass market paperback.
Book-and-record set.
Lucasfilm adapted the story for a children's book-and-record set. Released in 1983, the 24-page "Star Wars: Return of the Jedi" read-along book was accompanied by a 33⅓ rpm 7-inch gramophone record. Each page of the book contained a cropped frame from the film with an abridged and condensed version of the story. The record was produced by Buena Vista Records.
Footnotes.
</dl>
Citations.
Arnold, Alan. "Once Upon a Galaxy: A Journal of Making the Empire Strikes Back". Sphere Books, London. 1980. ISBN 978-0-345-29075-5

</doc>
<doc id="50745" url="http://en.wikipedia.org/wiki?curid=50745" title="Paul Simon">
Paul Simon

Paul Frederic Simon (born October 13, 1941) is an American musician, actor and singer-songwriter. Simon's fame, influence, and commercial success began as part of the duo Simon & Garfunkel, formed in 1964 with musical partner Art Garfunkel. Simon wrote nearly all of the pair's songs, including three that reached No. 1 on the U.S. singles charts: "The Sound of Silence", "Mrs. Robinson", and "Bridge Over Troubled Water". The duo split up in 1970 at the height of their popularity, and Simon began a successful solo career as a guitarist and singer-songwriter, recording three highly acclaimed albums over the next five years. In 1986, he released "Graceland", an album inspired by South African township music. Simon also wrote and starred in the film "One-Trick Pony" (1980) and co-wrote the Broadway musical "The Capeman" (1998) with the poet Derek Walcott.
Simon has earned 12 Grammys for his solo and collaborative work, including the Lifetime Achievement Award. In 2001, he was inducted into the Rock and Roll Hall of Fame and in 2006 was selected as one of the "100 People Who Shaped the World" by "Time" magazine. In 2011, "Rolling Stone" magazine named Simon as one of the 100 Greatest Guitarists. Among many other honors, Simon was the first recipient of the Library of Congress's Gershwin Prize for Popular Song in 2007. In 1986, Simon was awarded an Honorary Doctor of Music degree from Berklee College of Music, where he currently serves on the Board of Trustees.
Biography.
Early years.
Simon was born on October 13, 1941, in Newark, New Jersey, to Hungarian Jewish parents. His father Louis (1916–1995) was a college professor, upright bass player, and dance bandleader who performed under the name "Lee Sims". His mother, Belle (1910–2007), was an elementary school teacher. In 1945, his family moved to the Kew Gardens Hills section of Flushing, Queens, in New York City. The musician Donald Fagen has described Simon's childhood as that of "a certain kind of New York Jew, almost a stereotype, really, to whom music and baseball are very important. I think it has to do with the parents. The parents are either immigrants or first-generation Americans who felt like outsiders, and assimilation was the key thought—they gravitated to black music and baseball looking for an alternative culture." Simon, upon hearing Fagen's description, said it "isn't far from the truth." Simon says about his childhood, "I was a ballplayer. I'd go on my bike, and I'd hustle kids in stickball." He adds that his father was a New York Yankees fan:
I used to listen to games with my father. He was a nice guy. Fun. Funny. Smart. He didn't play with me as much as I played with my kids. He was at work until late at night. ... Sometimes [until] two in the morning."
Simon's musical career began after meeting Art Garfunkel when they were both 11. They performed in a production of Alice in Wonderland for their sixth-grade graduation, and began singing together when they were 13, occasionally performing at school dances. Their idols were the Everly Brothers, whom they imitated in their use of close two-part harmony. Simon also developed an interest in jazz, folk and blues, especially in the music of Woody Guthrie and Lead Belly.
Simon's first song written for himself and Garfunkel, when Simon was 12 or 13, was called "The Girl for Me," and according to Simon became the "neighborhood hit." His father wrote the words and chords on paper for the boys to use. That paper became the first officially copyrighted Paul Simon and Art Garfunkel song, and is now in the Library of Congress. In 1957, in their mid-teens, they recorded the song "Hey, Schoolgirl" under the name "Tom & Jerry", given to them by their label Big Records. The single reached No. 49 on the pop charts.
After graduating from Forest Hills High School, Simon majored in English at Queens College, while Garfunkel studied mathematics at Columbia University in Manhattan. Simon was a brother in the Alpha Epsilon Pi fraternity, earned a degree in English literature, and briefly attended Brooklyn Law School after graduation, but his real passion was rock and roll.
Early career.
Between 1957 and 1964, Simon wrote, recorded, and released more than 30 songs, occasionally reuniting with Garfunkel as Tom & Jerry for some singles, including "Our Song" and "That's My Story". Most of the songs Simon recorded during that time were performed alone or with musicians other than Garfunkel. They were released on several minor record labels, such as Amy, Big, Hunt, King, Tribute, and Madison. He used several pseudonyms for these recordings, including Jerry Landis, Paul Kane and True Taylor. Simon enjoyed some moderate success in recording a few singles as part of a group called "Tico and the Triumphs", including a song called "Motorcycle" that reached No. 97 on the "Billboard" charts in 1962. Tico and the Triumphs released four 45s. Marty Cooper, known as Tico, sang lead on several of these releases. A childhood friend, Bobby Susser, children's songwriter, record producer, and performer, co-produced the Tico 45s with Simon. That year, Simon reached No. 99 on the pop charts as Jerry Landis with the novelty song "The Lone Teen Ranger." Both chart singles were released on Amy Records.
Simon & Garfunkel.
In early 1964, Simon and Garfunkel got an audition with Columbia Records, whose executive Clive Davis was impressed enough to sign the duo to a contract to produce an album. Columbia decided that the two would be called simply "Simon & Garfunkel," instead of the group's previous name "Tom and Jerry." Simon said in 2003 that this renaming as "Simon & Garfunkel" was the first time that artists' ethnic names had been used in pop music. Simon and Garfunkel's first LP, "Wednesday Morning, 3 A.M.", was released on October 19, 1964; it consisted of 12 songs in the folk vein, five written by Simon. The album initially flopped.
Simon moved to England to pursue a solo career, touring folk clubs and coffee houses. At the first club he played, the Railway Inn Folk Club in Brentwood, Essex, he met Kathy Chitty who became his girlfriend and inspiration for "Kathy's Song," "America," and others. He performed at Les Cousins in London and toured provincial folk clubs that exposed him to a wide range of musical influences. In 1965, he recorded a solo LP "The Paul Simon Songbook" in England.
While in the UK, Simon co-wrote several songs with Bruce Woodley of the Australian pop group the Seekers, including "I Wish You Could Be Here," "Cloudy," and "Red Rubber Ball." Woodley's co-author credit was omitted from "Cloudy" on the "Parsley, Sage, Rosemary and Thyme" album. The American group the Cyrkle recorded a cover of "Red Rubber Ball" that reached No. 2 in the U.S. Simon also contributed to the Seekers catalogue with "Someday One Day," which was released in March 1966, charting around the same time as Simon and Garfunkel's "Homeward Bound."
Back on the American East Coast, radio stations began receiving requests for one of the "Wednesday Morning" tracks, Simon's "The Sound of Silence." Their producer, Tom Wilson, overdubbed the track with electric guitar, bass guitar and drums, releasing it as a single that eventually went to No. 1 on the U.S. pop charts.
The song's success drew Simon back to the United States to reunite with Garfunkel. Together they recorded four more influential albums: "Sounds of Silence"; "Parsley, Sage, Rosemary and Thyme"; "Bookends"; and the hugely successful "Bridge over Troubled Water". Simon and Garfunkel also contributed extensively to the soundtrack of the Mike Nichols film "The Graduate" (1967), starring Dustin Hoffman and Anne Bancroft. While writing "Mrs. Robinson," Simon originally toyed with the title "Mrs. Roosevelt". When Garfunkel reported this indecision over the song's name to the director, Nichols replied, "Don't be ridiculous! We're making a movie here! It's Mrs. Robinson!"
Simon and Garfunkel returned to England in the fall of 1968 and did a church concert appearance at Kraft Hall, which was broadcast on the BBC, and also featured Paul's brother Ed on a performance of the instrumental "Anji."
Simon pursued solo projects after "Bridge over Troubled Water", reuniting occasionally with Garfunkel for various projects, such as their 1975 Top Ten single "My Little Town." Simon wrote it for Garfunkel, whose solo output Simon judged as lacking "bite." The song was included on their respective solo albums—Paul Simon's "Still Crazy After All These Years" and Garfunkel's "Breakaway". Contrary to popular belief, the song is not autobiographical of Simon's early life in New York City. In 1981, they reunited again for the famous concert in Central Park, followed by a world tour and an aborted reunion album, to have been entitled "Think Too Much", which was eventually released (without Garfunkel) as "Hearts and Bones". Together, they were inducted into the Rock and Roll Hall of Fame in 1990.
In 2003, Simon and Garfunkel reunited once again when they received a Grammy Lifetime Achievement Award. This reunion led to a US tour—the acclaimed "Old Friends" concert series—followed by a 2004 international encore that culminated in a free concert at the Colosseum in Rome that drew 600,000 people. In 2005, the pair sang "Mrs. Robinson" and "Homeward Bound," plus "Bridge Over Troubled Water" with Aaron Neville, in the benefit concert "From the Big Apple to The Big Easy – The Concert for New Orleans" (eventually released as a DVD) for Hurricane Katrina victims.
The pair reunited six years later in New Orleans at the New Orleans Jazz & Heritage Festival.
1971–1976.
After Simon and Garfunkel split in 1970, Simon began to write and record solo material. His album "Paul Simon" was released in January 1972, preceded by his first experiment with world music, the Jamaican-inspired "Mother and Child Reunion," considered one of the first examples of reggae by a white musician. The single was a hit, reaching both the American and British Top 5. The album received universal acclaim, with critics praising the variety of styles and the confessional lyrics, reaching No. 4 in the U.S. and No. 1 in the UK and Japan. It later spawned another Top 30 hit with "Me and Julio Down by the Schoolyard".
Simon's next project was the pop-folk album, "There Goes Rhymin' Simon", released in May 1973. It contained some of his most popular and polished recordings. The lead single, "Kodachrome," was a No. 2 hit in America, and the follow-up, the gospel-flavored "Loves Me Like a Rock" was even bigger, topping the Cashbox charts. Other songs like the weary "American Tune" or the melancholic "Something So Right" — a tribute to Simon's first wife, Peggy, which received an Grammy Award nomination for Best Song of the Year — became standards in the musician's catalog. Critical and commercial reception for this second album was even stronger than for his debut. At the time, reviewers noted how the songs were fresh and unworried on the surface, while still exploring socially and politically conscious themes on a deeper level. The album reached No. 1 on the Cashbox album charts. As a souvenir for the tour that came next, in 1974 it was released as a live album, "Live Rhymin"', which was moderately successful and displayed some changes in Simon's music style, adopting world and religious music.
Highly anticipated, "Still Crazy After All These Years" was his next album. Released in October 1975 and produced by Simon and Phil Ramone, it marked another departure. The mood of the album was darker, as he wrote and recorded it in the wake of his divorce. Preceded by the feel-good duet with Phoebe Snow, "Gone at Last" (a Top 25 hit) and the Simon & Garfunkel reunion track "My Little Town" (a No. 9 on Billboard), the album was his only No. 1 on the Billboard charts to date. The 18th Grammy Awards named it the Album of the Year and Simon's performance the year's Best Male Pop Vocal. With Simon in the forefront of popular music, the third single from the album, "50 Ways to Leave Your Lover" reached the top spot of the Billboard charts, his only single to reach No. 1 on this list. Also, on May 3, 1976, Simon put together a benefit show at Madison Square Garden to raise money for the New York Public Library. Phoebe Snow, Jimmy Cliff and the Brecker Brothers also performed. The concert produced over $30,000 for the Library.
1977–1985.
After three successful studio albums, Simon became less productive during the second half of the 1970s. He dabbled in various projects, including writing music for the film "Shampoo", which became the music for the song "Silent Eyes" on the "Still Crazy" album, and acting (he was cast as Tony Lacey in Woody Allen's film "Annie Hall"). He achieved another hit in this decade, with the lead single of his 1977 compilation, "Greatest Hits, Etc.", "Slip Slidin' Away," reaching No. 5 in the United States.
In 1980 he released "One-Trick Pony", his debut album with Warner Bros. Records and his first in almost five years. It was paired with the motion picture of the same name, which Simon wrote and starred in. Although it produced his last Top 10 hit with the upbeat "Late in the Evening" (also a No. 1 hit on the Radio & Records American charts), the album did not sell well, in a music market dominated by disco music. Simon recorded "Hearts and Bones", a polished and confessional album that was eventually viewed as one of his best works, but that marked a lull in his commercial popularity; both the album and the lead single, "Allergies," missed the American Top 40. "Hearts and Bones" included "The Late Great Johnny Ace," a song partly about Johnny Ace, an American R&B singer, and partly about slain Beatle John Lennon. A successful U.S. solo tour featured Simon and his guitar, with a recording of the rhythm track and horns for "Late In The Evening." In January 1985, Simon lent his talent to USA for Africa and performed on the relief fundraising single "We Are the World."
1986–1992.
As he commented years later, after the disappointing commercial performance of "Hearts and Bones," Simon felt he had lost his inspiration to a point of no return, and that his commercial fortunes were unlikely to change. While driving his car in late 1984 in this state of frustration, Simon listened to a cassette of the Boyoyo Boys' instrumental "Gumboots: Accordion Jive Volume II" which had been lent to him by Heidi Berg, a singer songwriter he was working with at the time. Lorne Michaels had introduced Paul to Heidi when Heidi was working as the bandleader for Lorne's "The New Show". Interested by the unusual sound, he wrote lyrics to the number, which he sang over a re-recording of the song. It was the first composition of a new musical project that became the celebrated album "Graceland", an eclectic mixture of musical styles including pop, a cappella, isicathamiya, rock, zydeco and mbaqanga. Simon felt that he had nothing to lose. He went to South Africa in an attempt to embrace the culture and find the most comfortable environment for recording the album. Sessions in Johannesburg took place in February 1985. Overdubbing and additional recording was done in April 1986, in New York. The sessions featured many South African musicians and groups, particularly Ladysmith Black Mambazo. Simon also collaborated with several American artists, singing a memorable duet with Linda Ronstadt in "Under African Skies," and playing with Los Lobos in "All Around the World or The Myth of the Fingerprints."
Warner Bros. Records had serious doubts about releasing such an eclectic album to the mainstream, but did so in August 1986. "Graceland" was praised by critics and the public, and became Simon's most successful solo album. Slowly climbing the worldwide charts, it reached #1 in many countries, including the UK, Canada, Australia, and New Zealand—and peaked at #3 in the U.S. It was the second-best-selling album of 1987 in the US, selling five million copies and eventually reaching 5x Platinum certification. Another seven million copies sold internationally, making it his best-selling album. Much of the success of the album was due to the lead single, the upbeat "You Can Call Me Al," whose lyrics describe a man experiencing an identity crisis. The track featured many memorable elements—a catchy synthesizer riff (played by Adrian Belew of King Crimson], an easy whistle solo, and an unusual bass run, in which the second half was a reversed recording of the first half. "You Can Call Me Al" was accompanied by a humorous video featuring actor Chevy Chase, which introduced Simon to a new audience through MTV. In the end, the track reached UK Top 5 and the U.S. Top 25. Further singles, including the title track, "The Boy in the Bubble" and "Diamonds on the Soles of Her Shoes," were not commercial hits but became radio standards and were highly praised.
At age 45, Simon found himself back at the forefront of popular music. He received the Grammy Award for Album of the Year in 1987 and also Grammy Award for Record of the Year for the title track one year later. He also embarked on the very successful "Graceland Tour", which was documented on music video. Simon found himself embracing new sounds, which some critics viewed negatively—however, Simon reportedly felt it was a natural artistic experiment, considering that "world music" was already present on much of his early work, including such Simon & Garfunkel hits as "El Condor Pasa" and his early solo recording "Mother and Child Reunion," which was recorded in Kingston, Jamaica. One way or another, Warner Bros. Records (who by this time controlled and reissued all his previous Columbia albums) re-established Simon as one of their most successful artists. In an attempt to capitalize on his renewed success, WB Records released the album "Negotiations and Love Songs" in November 1988, a mixture of popular hits and personal favorites that covered Simon's entire career and became an enduring seller in his catalog.
After "Graceland", Simon decided to extend his roots with the Brazilian music-flavored "The Rhythm of the Saints". Sessions for the album began in December 1989, and took place in Rio de Janeiro and New York, featuring guitarist JJ Cale and many Brazilian and African musicians. The tone of the album was more introspective and relatively low-key compared to the mostly upbeat numbers of "Graceland". Released on October 1990, the album received excellent critical reviews and achieved very respectable sales, peaking at #4 in the U.S. and No. 1 in the UK. The lead single, "The Obvious Child," featuring the Grupo Cultural Olodum, became his last Top 20 hit in the UK and appeared near the bottom of the "Billboard" Hot 100. Although not as successful as "Graceland", "The Rhythm of the Saints" was received as a competent successor and consistent complement on Simon's attempts to explore (and popularize) world music, and also received a Grammy nomination for Album of the Year.
Simon's ex-wife Carrie Fisher says in her autobiography "Wishful Drinking" that the song "She Moves On" is about her. It's one of several she claims, followed by the line, "If you can get Paul Simon to write a song about you, do it. Because he is so brilliant at it." 
The success of both albums allowed Simon to stage another New York concert. On August 15, 1991, almost a decade after his concert with Garfunkel, Simon staged a second concert in Central Park with African and South American bands. The success of the concert surpassed all expectations, and reportedly over 750,000 people attended—one of the largest concert audiences in history. He later remembered the concert as, "...the most memorable moment in my career." The success of the show led to both a live album and an Emmy-winning TV special. In the middle, Simon embarked on the successful "Born at the Right Time Tour", and promoted the album with further singles, including "Proof"—accompanied with a humorous video that again featured Chevy Chase, and added Steve Martin. On March 4, 1992, he appeared on his own MTV Unplugged, offering renditions of many of his most famous compositions. Broadcast in June, the show was a success, though it did not receive an album release.
1993–1998.
After "Unplugged", Simon's place in the forefront of popular music dropped notably. A Simon & Garfunkel reunion took place in September 1993, and in another attempt to capitalize on the occasion, Columbia released "Paul Simon 1964/1993" in September, a three-disc compilation that received a reduced version on the two-disc album "The Paul Simon Anthology" one month later. In 1995 he made news for appearing on "The Oprah Winfrey Show", where he performed the song "Ten Years," which he composed specially for the tenth anniversary of the show. Also that year, he was featured on the Annie Lennox version of his 1973 song "Something So Right," which appeared briefly on the UK Top 50 once it was released as a single in November.
Since the early stages of the nineties, Simon was fully involved on "The Capeman", a musical that finally opened on January 29, 1998. Simon worked enthusiastically on the project for many years and described it as "a New York Puerto Rican story based on events that happened in 1959—events that I remembered." The musical tells the story of real-life Puerto Rican youth Salvador Agron, who wore a cape while committing two murders in 1959 New York, and went on to become a writer in prison. Featuring Marc Anthony as the young Agron and Rubén Blades as the older Agron, the play received terrible reviews and very poor box office receipts from the very beginning, and closed on March 28 after just 68 performances—a failure that reportedly cost Simon 11 million dollars.
Simon recorded an album of songs from the show, which was released in November 1997. It was received with very mixed reviews, though many critics praised the combination of doo-wop, rockabilly and Caribbean music that the album reflected. In commercial terms, "Songs from The Capeman" was a failure—it found Simon missing the Top 40 of the Billboard charts for the first time in his career. The cast album was never released on CD but eventually became available online.
1999–2007.
After the disaster of "The Capeman", Simon's career was again in an unexpected crisis. However, entering the new millennium, he maintained a respectable reputation, offering critically acclaimed new material and receiving commercial attention. In 1999, Simon embarked on a North American tour with Bob Dylan, where each alternated as headline act with a "middle" section where they performed together, starting on the first of June and ending September 18. The collaboration was generally well-received, with just one critic, Seth Rogovoy, from the "Berkshire Eagle", questioning the collaboration.
In an attempt to return successfully to the music market, Simon wrote and recorded a new album very quickly, with "You're the One" arriving in October 2000. The album consisted mostly of folk-pop writing combined with foreign musical sounds, particularly grooves from North Africa. While not reaching the commercial heights of previous albums, it managed at least to reach both the British and American Top 20. It received favorable reviews and received a Grammy nomination for Album of the Year. He toured extensively for the album, and one performance in Paris was released to home video.
On September 21, 2001, Simon sang "Bridge Over Troubled Water" on "," a multinetwork broadcast to benefit the September 11 Telethon Fund. In 2002, he wrote and recorded "Father and Daughter," the theme song for the animated family film "The Wild Thornberrys Movie", The track was nominated for an Academy Award for Best Song. In 2003, he participated on another Simon & Garfunkel reunion. One year later, Simon's studio albums were re-released both individually and together in a limited-edition nine-CD boxed set, "Paul Simon: The Studio Recordings 1972–2000".
At the time, Simon was already working on a new album with Brian Eno called "Surprise", which was released in May 2006. Most of the album was inspired by the September 11 terrorist attacks, the Iraq invasion, and the war that followed. In personal terms, Simon was also inspired by the fact of having turned 60 in 2001, which he humorously referred to on "Old" from "You're the One".
Simon showed special care about the musical venture he traveled since 1986's "Graceland." As he put it, "Once you go away for a bit, you wonder who people think you are. If they don't know what you're up to, they just go by your history. I'm so often described as this person that went to other cultures, which is true, but I never thought of it that way. I suspect people are thinking, 'What culture did you go to?' But this record is straight-ahead American."
"Surprise" was a commercial hit, reaching #14 in the "Billboard" 200 and #4 in the UK. Most critics also praised the album, and many of them called it a "comeback". Stephen Thomas Erlewine from AllMusic wrote that "Simon doesn't achieve his comeback by reconnecting with the sound and spirit of his classic work; he has achieved it by being as restless and ambitious as he was at his popular and creative peak, which makes "Surprise" all the more remarkable." The album was supported with the successful "Surprise Tour".
On March 1, 2007, Simon made headlines again when the Library of Congress announced that he would be the first recipient of the recently created Gershwin Prize for Popular Song. Simon received the prize during a concert gala at the Warner Theatre in Washington, D.C., on the evening of May 23. The event featured his music, and was nationally broadcast on PBS on the evening of June 27, 2007. Performers at the concert included Shawn Colvin, Philip Glass, Alison Krauss, Jerry Douglas, Ladysmith Black Mambazo, Lyle Lovett, James Taylor, Stevie Wonder, and Simon's former partner Art Garfunkel. On June 26, Warner Bros. released the definitive Paul Simon greatest-hits collection. "The Essential Paul Simon" consisted of two discs that reviewed 36 songs from his ten studio albums, and was also released on a special edition featuring a DVD of music videos and memorable live performances. The album was a commercial hit, reaching #12 in the UK.
2008–2013.
After living in Montauk, New York, for many years, Simon relocated to New Canaan, Connecticut.
Simon is one of a small number of performers who are named as the copyright owner on their recordings (most records have the recording company as the named owner of the recording). This noteworthy development was spearheaded by the Bee Gees after their successful $200 million lawsuit against RSO Records, which remains the largest successful lawsuit against a record company by an artist or group. All of Simon's solo recordings, including those originally issued by Columbia Records, are currently distributed by Sony Records' Legacy Recordings unit. His albums were issued by Warner Music until mid-2010. In mid-2010, Simon moved his catalog of solo work from Warner Bros. Records to Sony/Columbia Records where Simon and Garfunkel's catalog is. Simon's back catalog of solo recordings would be marketed by Sony Music's Legacy Recordings unit.
In February 2009, Simon performed back-to-back shows in New York City at the Beacon Theatre, which had recently been renovated. Simon was reunited with Art Garfunkel at the first show as well as with the cast of "The Capeman"; also playing in the band was "Graceland" bassist Bakithi Kumalo. In May 2009, Simon toured with Garfunkel in Australia, New Zealand, and Japan. In October 2009, they appeared together at the 25th Anniversary of The Rock & Roll Hall of Fame concert at Madison Square Garden in New York City. The pair performed four of their most popular songs, "The Sound of Silence," "The Boxer," "Cecilia," and "Bridge Over Troubled Water."
Simon's album "So Beautiful or So What" was released on the Concord Music Group label on April 12, 2011. The album received high marks from the artist, "It's the best work I've done in 20 years." It was reported that Simon attempted to have Bob Dylan guest on the album.
On November 10, 2010, Simon released a new song called "Getting Ready for Christmas Day". It premiered on National Public Radio, and was included on the album "So Beautiful Or So What". The song samples a 1941 sermon by the Rev. J.M. Gates, also entitled "Getting Ready for Christmas Day". Simon performed the song live on "The Colbert Report" on December 16, 2010. The first video featured J.M. Gates' giving the sermon and his church in 2010 with its display board showing many of Simon's lyrics; the second video illustrates the song with cartoon images.
In the premiere show of the final season of "The Oprah Winfrey Show" on September 10, 2010, Simon surprised Oprah and the audience with a song dedicated to Oprah and her show lasting 25 years (an update of a song he did for her show's 10th anniversary).
Rounding off his 2011 World Tour, which included United States, England, the Netherlands, Switzerland and Germany, Simon appeared at Ramat Gan Stadium in Israel in July 2011, making his first concert appearance in Israel since 1983. On September 11, 2011, Paul Simon performed "The Sound of Silence" at the National September 11 Memorial & Museum, site of the World Trade Center, on the 10th anniversary of the September 11 attacks.
On February 26, 2012, Simon paid tribute to fellow musicians Chuck Berry and Leonard Cohen who were the recipients of the first annual PEN Awards for songwriting excellence at the JFK Presidential Library in Boston, Massachusetts. In 1986 Simon was awarded an Honorary Doctor of Music degree from Berklee College of Music where he currently serves on the Board of Trustees.
On June 5, 2012 Simon released a 25th anniversary box set of "Graceland", which included a remastered edition of the original album, the documentary film "Under African Skies", the original 1987 "African Concert" from Zimbabwe, an audio narrative "The Story of 'Graceland'" as told by Paul Simon, and other interviews and paraphernalia. He played a few concerts in Europe with the original musicians to commemorate the anniversary.
On December 19, 2012, Simon performed at the funeral of Victoria Leigh Soto, a teacher killed in the Sandy Hook Elementary School shooting.
On June 14, 2013, at Sting's Back to Bass Tour, Simon performed his song "The Boxer" and Sting's "Fields of Gold" with Sting.
In September 2013, Simon delivered the at Emory University.
2014–present.
In February 2014, Simon embarked on the On Stage Together Tour with English musician Sting, playing 21 concerts in North America. The tour will continue in early 2015, with ten shows in Australia and New Zealand and 23 concerts in Europe, ending on 18 April 2015.
Songwriting.
In an in-depth interview reprinted in "American Songwriter", Simon discusses the craft of songwriting with music journalist Tom Moon. In the interview, Simon explains the basic themes in his songwriting: love, family, social commentary, etc., as well as the overarching messages of religion, spirituality, and God in his lyrics. Simon goes on in the interview to explain the process of how he goes about writing songs, "The music always precedes the words. The words often come from the sound of the music and eventually evolve into coherent thoughts. Or incoherent thoughts. Rhythm plays a crucial part in the lyric-making as well. It's like a puzzle to find the right words to express what the music is saying."
Projects.
Music for Broadway.
In the late 1990s, Simon wrote and produced a Broadway musical called "The Capeman", which lost $11 million during its 1998 run. In April 2008, the Brooklyn Academy of Music celebrated Paul Simon's works, and dedicated a week to "Songs From the Capeman" with a good portion of the show's songs performed by a cast of singers and the Spanish Harlem Orchestra. Simon himself appeared during the BAM shows, performing "Trailways Bus" and "Late In the Evening". In August 2010, "The Capeman" was staged for three nights in the Delacorte Theatre in New York's Central Park. The production was directed by Diane Paulus and produced in conjunction with the Public Theater.
Film and television.
Simon has also dabbled in acting. He played music producer Tony Lacey, a supporting character, in the 1977 Woody Allen feature film "Annie Hall". He wrote and starred in 1980's "One Trick Pony" as Jonah Levin, a journeyman rock and roller. Simon also wrote all the songs in the film. Paul Simon also appeared on "The Muppet Show" (the only episode to use only the songs of one songwriter, Simon). In 1990, he played the character of—appropriately enough—Simple Simon on the Disney Channel TV movie, "Mother Goose Rock 'n' Rhyme".
In 1978, Simon made a cameo in the movie, "".
He has been the subject of two films by Jeremy Marre, the first on "Graceland", the second on "The Capeman".
On November 18, 2008, Simon was a guest on "The Colbert Report" promoting his book "Lyrics 1964–2008". After an interview with Stephen Colbert, Simon performed "American Tune".
Simon performed a Stevie Wonder song at the White House in 2009 at an event honoring Wonder's musical career and contributions.
In May 2009, "The Library of Congress: Paul Simon and Friends Live Concert" was released on DVD, via Shout! Factory. The PBS concert was recorded in 2007.
In April 2011 Simon was confirmed to appear at the Glastonbury music festival in England.
"Saturday Night Live".
Simon has appeared on "Saturday Night Live" ("SNL"), either as host or musical guest, 14 times. On one appearance in the late 1980s, he worked with his political namesake, Illinois Senator Paul Simon. Simon's most recent "SNL" appearance on a Saturday night was on the March 9, 2013 episode hosted by Justin Timberlake as a member of the Five-Timers Club. In one "SNL" skit from 1986 (when he was promoting "Graceland"), Simon plays himself, waiting in line with a friend to get into a movie. He amazes his friend by remembering intricate details about prior meetings with passers-by, but draws a complete blank when approached by Art Garfunkel, despite the latter's numerous memory prompts. Simon appeared alongside George Harrison as musical guest on the Thanksgiving Day episode of "SNL" (November 20, 1976). The two performed "Here Comes the Sun" and "Homeward Bound" together, while Simon performed "50 Ways to Leave Your Lover" solo earlier in the show. On that episode, Simon opened the show performing "Still Crazy After All These Years" in a turkey outfit, since Thanksgiving was the following week. About halfway through the song, Simon tells the band to stop playing because of his embarrassment. After giving a frustrating speech to the audience, he leaves the stage, backed by applause. Lorne Michaels positively greets him backstage, but Simon is still upset, yelling at him because of the humiliating turkey outfit. This is one of "SNL"‍ '​s most played sketches. Simon closed the 40th anniversary SNL show on February 15, 2015, with a performance of "Still Crazy After All These Years," sans turkey outfit. Simon also played a snippet of "I've Just Seen a Face" with Sir Paul McCartney during the special's introductory sequence.
On September 29, 2001, Simon made a special appearance on the first "SNL" to air after the September 11, 2001 attacks. On that show, he performed "The Boxer" to the audience and the NYC firefighters and police officers. He is also a friend of former "SNL" star Chevy Chase, who appeared in his video for "You Can Call Me Al" lip synching the song while Simon looks disgruntled and mimes backing vocals and the playing of various instruments beside him. Chase would also appear in Simon's 1991 video for the song "Proof" alongside Steve Martin. He is a close friend of "SNL" producer Lorne Michaels, who produced the 1977 TV show "The Paul Simon Special", as well as the Simon and Garfunkel concert in Central Park four years later. Simon and Lorne Michaels were the subjects of a 2006 episode of the Sundance channel documentary series, "Iconoclasts".
Awards and honors.
Simon has won 12 Grammy Awards (one of them a Lifetime Achievement Award) and five Album of the Year Grammy nominations, the most recent for "You're the One" in 2001. In 1998 he received a Grammy Hall of Fame Award for the Simon & Garfunkel album "Bridge Over Troubled Water". He received an Oscar nomination for the song "Father and Daughter" in 2002. He is also a two-time inductee into the Rock and Roll Hall of Fame; as a solo artist in 2001, and in 1990 as half of Simon & Garfunkel.
In 2001, Simon was honored as MusiCares Person Of The Year. The following year, he was one of the five recipients of the annual Kennedy Center Honors, the nation's highest tribute to performing and cultural artists.
In 2005, Simon was saluted as a BMI Icon at the 53rd Annual BMI Pop Awards. Simon's songwriting catalog has earned 39 BMI Awards including multiple citations for "Bridge Over Troubled Water," "Mrs. Robinson," "Scarborough Fair" and "The Sound of Silence". As of 2005, he has amassed nearly 75 million broadcast airplays, according to BMI surveys.
In 2006, Simon was selected by "Time Magazine" as one of the "100 People Who Shaped the World."
In 2007, Simon received the first annual Library of Congress Gershwin Prize for Popular Song. (Stevie Wonder and Paul McCartney followed in 2009 and 2010.) Named in honor of George and Ira Gershwin, this new award recognizes the profound and positive effect of popular music on the world's culture. On being notified of the honor, Simon said, "I am grateful to be the recipient of the Gershwin Prize and doubly honored to be the first. I look forward to spending an evening in the company of artists I admire at the award ceremony in May. I can think of a few who have expressed my words and music far better than I. I'm excited at the prospect of that happening again. It's a songwriter's dream come true." Among the performers who paid tribute to Simon were Stevie Wonder, Alison Krauss, Jerry Douglas, Lyle Lovett, James Taylor, Dianne Reeves, Marc Anthony, Yolanda Adams, and Ladysmith Black Mambazo. The event was professionally filmed and broadcast and is now available as "Paul Simon and Friends".
In 2010, Simon received an honorary degree from Brandeis University, where he performed "The Boxer" at the main commencement ceremony.
In October 2011, Simon was inducted into the American Academy of Arts and Science. At the induction ceremony, he performed "American Tune."
In 2012, Simon was awarded the Polar Music Prize.
Personal life.
When Simon moved to England in 1964, he met Kathleen Mary "Kathy" Chitty (born 1947) on April 12, 1964, at the first English folk club he played, the Hermit Club in Brentwood, Essex, where Chitty worked part-time selling tickets. She was 17, he was 22, and they fell in love. Later that year they visited the U.S. together, touring around mainly by bus. Kathy returned to England on her own with Simon returning to her some weeks later. When Simon returned to the U.S. with the growing success of "The Sound of Silence", Kathy, who was quite shy wanted no part of the success and fame that awaited Simon and they split. She is mentioned by name in at least two of his songs: "Kathy's Song" and "America," and is referred to in "Homeward Bound" and "The Late Great Johnny Ace." There is a photo of Simon and Kathy on the cover of "The Paul Simon Songbook".
Simon has been married three times, first to Peggy Harper in late autumn 1969. They had son Harper Simon in 1972 and divorced in 1975. The song "Train in the Distance," from Simon's 1983 album "Hearts and Bones", is about this relationship. Simon's 1972 song "Run That Body Down," from his second solo album, casually mentions both himself and his then-wife ("Peg") by name.
His second marriage, from 1983 to 1984, was to actress and author Carrie Fisher to whom he proposed after a New York Yankees game. The song "Hearts and Bones" was written about this relationship. The song "Graceland" is also thought to be about seeking solace from the end of this relationship by taking a road trip. A year after divorcing, Simon and Fisher resumed their relationship for several years.
His third wife is folk singer Edie Brickell whom he married on May 30, 1992. They have three children: Adrian, Lulu, and Gabriel.
Paul Simon and his younger brother, Eddie Simon, founded the Guitar Study Center in New York City. The Guitar Study Center later became part of The New School in New York City.
Philanthropy.
Simon is a proponent of music education for children.
In 1970, after recording his "Bridge Over Troubled Water", at the invitation of the NYU's Tisch School of the Arts, Simon held auditions for a young songwriter's workshop. Advertised in the Village Voice, the auditions brought hundreds of hopefuls to perform for Simon. Among the six teenage songwriters Simon selected for tutelage were Melissa Manchester, Tommy Mandel and rock/beat poet Joe Linus, with Maggie and Terre Roche (the Roche Sisters), who later sang back-up for Simon, joining the workshop in progress through an impromptu appearance.
Simon invited the six teens to experience recording at Columbia studios with engineer Roy Halee at the board. During these sessions, Bob Dylan was downstairs recording the album "Self-Portrait", which included a version of Simon's "The Boxer". Violinist Isaac Stern also visited the group with a CBS film crew, speaking to the young musicians about lyrics and music after Joe Linus performed his song "Circus Lion" for Stern.
Manchester later paid homage to Simon, on her recorded song, "Ode to Paul." Other younger musicians Simon has mentored include Nick Laird-Clowes, who later co-founded the band the Dream Academy. Laird-Clowes has credited Simon with helping to shape the band's biggest hit, "Life in a Northern Town".
In 2003, Simon signed on as an official supporter of Little Kids Rock, a nonprofit organization that provides free musical instruments and free lessons to children in public schools throughout the U.S. He sits on the organization's board of directors as an honorary member.
Simon is also a major benefactor and one of the co-founders, with Dr. Irwin Redlener, of the Children's Health Project and which started by creating specially equipped "buses" to take medical care to children in medically underserved areas, urban and rural. Their first bus was in the impoverished South Bronx of New York City, but they now operate in 12 states, including on the Gulf Coast. It has expanded greatly, partnering with major hospitals, local public schools and medical schools and advocating policy for children's health and medical care.
In May 2012, Paul Simon performed at a benefit dinner for the Turkana Basin Institute in New York City, raising more than $2 million for Richard Leakey's research institute in Africa.
Discography.
Studio albums

</doc>
<doc id="50746" url="http://en.wikipedia.org/wiki?curid=50746" title="Paul Simon (disambiguation)">
Paul Simon (disambiguation)

Paul Simon (born 1941) is an American musician and songwriter.
Paul Simon may also refer to:

</doc>
<doc id="50748" url="http://en.wikipedia.org/wiki?curid=50748" title="Paris Métro">
Paris Métro

The Paris Métro or Métropolitain (French: "Métro de Paris") is a rapid transit system in the Paris Metropolitan Area. A symbol of the city, it is noted for its density within the city limits and its uniform architecture, influenced by Art Nouveau. It is mostly underground and 214 km long. It has 303 stations, of which 62 have transfers between lines. There are 16 lines, numbered 1 to 14 with two lines, 3bis and 7bis, which are named because they started out as branches of lines 3 and 7; later they officially became separate lines; the Metro is still numbered as if these lines were absent. Lines are identified on maps by number and colour, and direction of travel is indicated by the terminus.
It is the second-busiest subway system in Europe, after Moscow. It carried 1.541 billion passengers in 2012, (up from 1.524 billion in 2011), 4.210 million passengers a day. It is one of the densest metro systems in the world, with 245 stations within the 86.9 km2 of the city of Paris. Châtelet – Les Halles, with 5 Métro lines and three RER commuter rail lines, is the world's largest metro (subway) station.
The first line opened without ceremony on 19 July 1900, during the World's Fair (Exposition Universelle). The system expanded quickly until the First World War and the core was complete by the 1920s. Extensions into suburbs and Line 11 were built in the 1930s. The network reached saturation after World War II, with new trains to allow higher traffic, but further improvements have been limited by the design of the network and in particular the short distances between stations. Besides the Métro, downtown Paris and its urban area are served by the RER developed from the 1960s, several tramway lines, Transilien suburban trains and two VAL lines, serving Charles De Gaulle and Orly airports. In the late 1990s, the automated line 14 was built to relieve RER line A.
Naming.
Métro is the abbreviated name of the company that originally operated most of the network: "La Compagnie du chemin de fer métropolitain de Paris" ("The Paris Metropolitan Railway Company"), shortened to "Le Métropolitain". That was quickly abbreviated to "métro", which became a common word to designate all subway networks (or any rapid transit system) in France or in many cities elsewhere "(a genericized trademark)".
The Métro is operated by the "Régie autonome des transports parisiens" (RATP), a public transport authority that also operates part of the RER network, bus services, light rail lines and many bus routes. The name "métro" proved very popular and was adopted in many languages, making it the most used word for a (generally underground) urban transit system. It is possible that "Compagnie du chemin de fer métropolitain" was copied from the name of London's pioneering underground railway company, the Metropolitan Railway, which had been in business for almost 40 years prior to the inauguration of Paris's first line.
History.
Paris and the railway companies were already thinking by 1845 about an urban railway system to link inner districts of the city. The railway companies and the French government wanted to extend main-line railroads into a new underground network, whereas the Parisians favoured a new and independent network and feared national takeover of any system it built. The disagreement lasted from 1856 to 1890. Meanwhile, the population became more dense and traffic congestion grew massively. The deadlock put pressure on the authorities and gave the city the chance to enforce its vision.
Prior to 1845, the urban transport network consisted primarily of a large number of omnibus lines, consolidated by the French government into a regulated system with fixed and unconflicting routes and schedules. The first concrete proposal for an urban rail system in Paris was put forward by civil engineer Florence de Kérizouet. This plan called for a surface cable car system. In 1855, civil engineers Edouard Brame and Eugène Flachat proposed an underground freight urban railroad, due to the high rate of accidents on surface rail lines. On November 19, 1871, the General Council of the Seine commissioned a team of 40 engineers to plan an urban rail network. This team proposed a network with a pattern of routes "resembling a cross enclosed in a circle" with axial routes following large boulevards. On May 11, 1872, the Council endorsed the plan, but the French government turned down the plan. After this point, a serious debate occurred over whether the new system should consist of elevated lines or of mostly underground lines; this debate involved numerous parties in France, including Victor Hugo, Guy de Maupassant, and the Eiffel Society of Gustave Eiffel, and continued until 1892. Eventually the underground option emerged as the preferred solution because of the high cost of buying land for rights-of-way in central Paris required for elevated lines, estimated at 70,000 francs per meter of line for a 20-meter-wide railroad.
The last remaining hurdle was the city's concern about national interference in its urban rail system. The city commissioned renowned engineer Jean-Baptiste Berlier, who designed Paris' postal network of pneumatic tubes, to design and plan its rail system in the early 1890s. Berlier recommended a special track gauge of 1300mm (versus the standard gauge of 1435mm) to protect the system from national takeover, which inflamed the issue substantially. The issue was finally settled when the Minister of Public Works begrudgedly recognized the city's right to build a local system in November 22, 1895, and by the city's secret designing of the trains and tunnels to be too narrow for main-line trains, while adopting standard gauge as a compromise with the state.
Fulgence Bienvenüe project.
On 20 April 1896, Paris adopted the Fulgence Bienvenüe project, which was to serve only the city proper of Paris. Many Parisians worried that extending lines to industrial suburbs would reduce the safety of the city. Paris forbade lines to the inner suburbs and, as a guarantee, Métro trains were to run on the right, as opposed to existing suburban lines, which ran on the left.
Unlike many other subway systems (such as that of London), this system was designed from the outset as a system of (initially) nine lines. Such a large project required a private-public arrangement right from the outset - the city would build most of the permanent way, while a private concessionaire company would supply the trains and power stations, and lease the system (each line separately, for initially 39-year leases). In July 1897, six bidders competed, and The Compagnie Generale de Traction won the contract; this company was then immediately reorganized as the Compagnie de Chemin de Fer Metropolitan.
Construction began on November 1898. The first line, Porte Maillot–Porte de Vincennes, was inaugurated on 19 July 1900 during the Paris World's Fair. Entrances to stations were designed in "art nouveau" style by Hector Guimard. Eighty-six of his entrances are still in existence.
Bienvenüe's project consisted of 10 lines, which correspond to today's lines 1 to 9. Construction was so intense that by 1920, despite a few changes from schedule, most lines had been completed. The shield method of construction was rejected in favor of the cut-and-cover method in order to speed up work. Bienvenüe, a highly regarded engineer, designed a special procedure of building the tunnels to allow the swift repaving of roads, and is credited with a largely swift and relatively uneventful construction through the difficult and heterogenous soils and rocks.
Lines 1 and 4 were conceived as central east-west and north-south lines. Two lines, "ligne 2 Nord" (line 2 North) and "ligne 2 Sud" (line 2 South), were also planned but line 2 South was merged with line 5 in 1906.
Line 3 was an additional east-west line to the north of line 1 and line 5 an additional north-south line to the east of line 4. Line 6 would run from Nation to Place d'Italie. Lines 7, 8 and 9 would connect commercial and office districts around the Opéra to residential areas in the north-east and the south-west.
Bienvenüe also planned a circular line, the "ligne circulaire intérieure", to connect the six main-line stations. A section opened in 1923 between Invalides and the Boulevard Saint-Germain before the plan was abandoned.
Nord-Sud: the competing network.
On 31 January 1904, a second concession was granted to the "Société du chemin de fer électrique souterrain Nord-Sud de Paris" (Paris North-South underground electrical railway company), abbreviated to the "Nord-Sud" (North-South) company. It was responsible for building three proposed lines:
Line A was inaugurated on 4 November 1910, after being postponed because of floods in January of that year. Line B was inaugurated on 26 February 1911. Because of the high construction costs, the construction of line C was postponed. Nord-Sud and CMP used compatible trains that could be used on both networks, but CMP trains used 600 volts third rail, and NS −600 volts overhead wire and +600 volts third rail. This was necessary because of steep gradients on NS lines. NS distinguished itself from its competitor with the high-quality decoration of its stations, the trains' extreme comfort and pretty lighting.
Nord-Sud did not become profitable and bankruptcy became unavoidable. By the end of 1930, the CMP bought Nord-Sud. Line A became line 12 and line B line 13. Line C was built and renamed line 14, that line was reorganized in 1937 with lines 8 and 10. This partial line is now the south part of line 13.
The last Nord-Sud train set was decommissioned on 15 May 1972.
1930–1950: The first inner suburbs are reached.
Bienvenüe's project was nearly completed during the 1920s. Paris planned three new lines and extensions of most lines to the inner suburbs, despite the reluctance of Parisians. Bienvenüe's inner circular line having been abandoned, the already-built portion between Duroc and Odéon for the creation of a new east-west line that became line 10, extended west to Porte de Saint-Cloud and the inner suburbs of Boulogne.
The line C planned by Nord-Sud between Montparnasse station and Porte de Vanves was built as line 14 (different from present line 14). It extended north in encompassing the already-built portion between Invalides and Duroc, initially planned as part of the inner circular.
The over-busy Belleville funicular tramway would be replaced by a new line, line 11, extended to Châtelet. Lines 10, 11 and 14 were thus the three new lines envisaged under this plan.
Most lines would be extended to the inner suburbs. The first to leave the city proper was line 9, extended in 1934 to Boulogne-Billancourt; more followed in the 1930s. World War II forced authorities to abandon projects such as the extension of lines 4 or 12 to the northern suburbs. By 1949, eight lines had been extended: line 1 to Neuilly and Vincennes, line 3 to Levallois-Perret, line 5 to Pantin, line 7 to Ivry, line 8 to Charenton, line 9 to Boulogne-Billancourt, line 11 to Les Lilas and line 12 to Issy-les-Moulineaux.
World War II had a massive impact on the Métro. Services were limited and many stations closed. The risk of bombing meant the service between Place d'Italie and Étoile was transferred from line 5 to line 6, so that most of the elevated portions of the Métro would be on line 6. As a result, lines 2 and 6 now form a circle. Most stations were too shallow to be used as bomb shelters. The French Resistance used the tunnels to conduct swift assaults throughout Paris.
It took a long time to recover after liberation in 1944. Many stations had not reopened by the 1960s and some closed for good. On 23 March 1948, the CMP (the underground) and the STCRP (bus and tramways) merged to form the RATP, which still operates the Métro.
1960–1990: the development of the RER.
The network grew saturated during the 1950s. Outdated technology limited the number of trains, which led the RATP to stop extending lines and concentrate on modernisation. The MP 51 prototype was built, testing both rubber-tyred metro and basic automatic driving on the "voie navette". The first replacements of the older Sprague trains began with experimental articulated trains and then with mainstream rubber-tyred metro MP 55 and MP 59, some of the latter still in service (line 4 and 11). Thanks to newer trains and better signalling, trains ran more frequently.
The population boomed from 1950 to 1980. Cars became more popular and suburbs grew further from the city. The main railway stations, termini of the suburban rail lines, were overcrowded during rush hour. The short distance between metro stations slowed the network and made it unprofitable to build extensions.
The solution in the 1960s was to revive a project abandoned at the end of the 19th century: joining suburban lines to new underground portions in the city centre as the "réseau express régional" (regional express network) (RER).
The RER plan initially included one east-west line and two north-south lines. RATP bought two unprofitable SNCF lines—the Ligne de Saint-Germain (westbound) and the Ligne de Vincennes (eastbound) with the intention of joining them and to serve multiple districts of central Paris with new underground stations. The new line created by this merger became line A. The Ligne de Sceaux, which served the southern suburbs and was bought by the CMP in the 1930s, would be extended north to merge with a line of the SNCF and reach the new Paris-Charles de Gaulle Airport in Roissy. This became line B. These new lines were inaugurated in 1977 and their wild success outperformed all the most optimistic forecasts to the extent that line A is the most used urban rail line in the world with nearly 300 million journeys a year.
Because of the enormous cost of these two lines, the third planned line was abandoned and the authorities decided that later developments of the RER network would be more cheaply developed by SNCF, alongside its continued management of other suburban lines. However, the RER developed by SNCF would never match the success of the RATP's two RER lines. In 1979, SNCF developed line C by joining the suburban lines of Gare d'Austerlitz and Gare d'Orsay, the latter being converted into a museum dedicated to impressionist paintings. During the 1980s, it developed line D, which was the second line planned by the initial RER schedule, but serving Châtelet instead of République to reduce costs. A huge Métro-RER hub was created at Châtelet-Les Halles, the world's largest underground station.
The same project of the 1960s also decided to merge lines 13 and 14 to create a quick connection between Saint-Lazare and Montparnasse as a new north-south line. Distances between stations on the lengthened line 13 differ from that on other lines in order to make it more "express" and hence to extend it farther in the suburbs. The new Line 13 was inaugurated on 9 November 1976.
1990–2010: Eole and Météor.
In October 1998, Line 14 was inaugurated. It was the first fully new Métro line in 63 years. Known during its conception as "Météor" (Métro Est-Ouest Rapide), it is one of the two fully automatic lines within the network along with Line 1. It was the first with platform screen doors to prevent suicides and accidents.
It was conceived with extensions to the suburbs in mind, similar to the extensions of the line 13 built during the 1970s. As a result, most of the stations are at least a kilometre apart. Like the RER lines designed by the RATP, nearly all stations offer connections with multiple Métro lines. The line runs between Saint-Lazare and Olympiades.
Lines 7 and 13 are the only two on the network to be split in branches. The RATP would like to get rid of those saturated branches in order to improve the network's efficiency. A project exists to attribute to line 14 one branch of each line, and to extend them further into the suburbs. This project has not yet been approved.
In 1999, the RER line E was inaugurated. Known during its conception as Eole (Est-Ouest Liaison Express), it is the fifth RER line. It terminates at Haussmann – Saint-Lazare, but a new project, financed by EPAD, the public authority managing the La Défense business district, should extend it west to La Défense – Grande Arche and the suburbs beyond.
Network.
Since the Métro was built to comprehensively serve the city inside its walls, the stations are very close: 548 metres apart on average, from 424 m on line 4 to one kilometre on the newer line 14, meaning Paris is densely networked with stations. The surrounding suburbs are served by later line extensions, thus traffic from one suburb to another must pass through the city. The slow average speed effectively prohibits service to the greater Paris area.
The Métro is mostly underground (197 km of 214 km). Above-ground sections consist of viaducts within Paris (on lines 1, 2, 5 & 6) and the suburban ends of lines 1, 5, 8, and 13. The tunnels are relatively close to the surface due to the variable nature of the terrain, which complicates deep digging; exceptions include parts of line 12 under the hill of Montmartre and line 2 under Ménilmontant. The tunnels follow the twisting lie of the streets. During construction in 1900 a minimum radius of curvature of 75 metres was imposed, though this low standard was not adhered to at Bastille and Notre-Dame-de-Lorette.
Like the New York City Subway and in contrast with the London Underground the Paris Métro mostly uses two-way tunnels. As in most French métro and tramway systems, trains drive on the right (the SNCF drives on the left). The tracks are standard gauge (1.435 metres). Electric power is supplied by a third rail which carries 750 volts DC.
The width of the carriages, 2.4 metres, is narrower than that of newer French systems (such as the 2.9 m carriages in Lyon, one of the widest in Europe) and trains on lines 1, 4 and 14 have capacities of 600-700 passengers; 2,600 on the Altéo MI 2N trains of RER A. The size of the Metro tunnels was deliberately chosen by the City of Paris to prevent the running of main-line trains; the city of Paris and the nation of France had historically poor relations. In contrast to many other historical metro systems (such as New York, Madrid, London, and Boston), all lines have tunnels and operate trains with the same dimensions. Five lines (1, 4, 6, 11 and 14) run on a rubber tire system developed by the RATP in the 1950s, exported to the Montreal, Santiago and Mexico City metros.
The number of cars in each train varies line by line from three to six; most have five, and eight is possible on line 14. Two lines, 7 and 13, have branches at the end, and trains serve every station on each line except when they are closed for renovations.
Opening hours.
The first train leaves each terminus at 05:30: on some lines additional trains start from an intermediate station. The last train, often called the "balai" (broom) because it sweeps up remaining passengers, arrives at the terminus at 01:15, except on Fridays (since 7 December 2007), Saturdays and on nights before a holiday, when the service ends at 02:15.
On New Year's Eve, "Fête de la Musique", "Nuit Blanche" and other events, some stations on lines 1, 4, 6, 9 and 14 remain open all night.
Tickets.
 
Fares are sold at kiosks and at automated machines in the station foyer. Entrance to platforms is by automated gate, opened by smart cards and simple tickets. Gates return tickets for passengers to retain for the duration of the journey. There is normally no system to collect or check tickets at the end of the journey, and tickets can be inspected at any point. The exit from all stations is clearly marked as to the point beyond which possession of a ticket is no longer required.
The standard ticket is ticket "t+". It is valid for a multi-transfer journey within one and a half hours from the first validation. It can be used on the Métro, buses and trams, and in zone 1 of the RER. It allows unlimited transfers between the same mode of transport (i.e. Métro to Métro, bus to bus and tram to tram), between bus and tram, and between metro and RER zone 1. When transferring between the Metro and the RER, it is necessary to retain the ticket. The RER requires a valid ticket for entry and exit, even for a transfer. It costs €1.80 or ten (a "carnet") for €14.10.
Other fares use the Navigo pass, an RFID-based contactless smart card. Fares include:
Facilities.
On June 26, 2012, it was announced that the Métro would get Wi-Fi in most stations. Access provided is free, with a premium paid alternative offer proposed for a faster internet connection.
Technical specifications.
The Métro has 214 km of track and 303 stations, 62 connecting between lines. These figures do not include the RER network. The average distance between stations is 562 m. Trains stop at all stations. Lines do not share tracks, even at interchange (transfer) stations.
Trains average 20 km/h with a maximum of 70 km/h on all but the automated driverless trains of line 14, which average 40 km/h and reach 80 km/h. An average interstation trip takes 58 seconds. Trains travel on the right. The track is standard gauge but the loading gauge is smaller than the mainline SNCF network. Power is from a lateral third rail, 750 V DC, except on the rubber-tyred lines where the current is from guide bars.
The loading gauge is small compared to those of newer metro systems (but comparable to that of early European metros), with capacities of between about 560 and 720 passengers per train on Lines 1–14. Many other metro systems (such as those of New York and London) adopted expanded tunnel dimensions for their newer lines (or used tunnels of multiple sizes almost from the outset, in the case of Boston), at the cost of operating incompatible fleets of rolling stock. Paris built all lines to the same dimensions as its original lines. Before the introduction of rubber-tire lines in the 1950s, this common shared size theoretically allowed any Metro rolling stock to operate on any line, but in practice each line was assigned a regular roster of trains.
A feature is the use of rubber-tired trains on five lines: this technique was developed by RATP and entered service in 1951. The technology was exported to many networks around the world (including Montreal, Mexico City, and Santiago). Lines 1, 4, 6, 11 and 14 have special adaptations to accommodate rubber-tyred trains. Trains are composed of 3 to 6 cars depending on the line, the most common being 5 cars (line 14 may have 8 cars in the future), but all trains on the same line have the same number of cars.
The Metro is designed to provide local, point-to-point service in Paris proper and service into the city from some close suburbs. Stations within Paris are very close together to form a grid structure, ensuring that every point in the city is close to a metro station (less than 500 m), but this makes the service slow 20 km/h, except on Line 14 where the stations are farther apart and the trains travel faster. The low speed virtually precludes feasible service to farther suburbs, which are serviced by the RER.
The metro is mostly underground; surface sections include sections on viaduct in Paris (lines 1, 2, 5 and 6) and at the surface in the suburbs (lines 1, 5, 8 and 13). In most cases both tracks are laid in a single tunnel. Almost all lines follow roads, having been built by the cut-and-cover method near the surface (the earliest by hand). Line 1 follows the straight course of the Champs-Elysées and on other lines some stations (for example, Commerce) have platforms that do not align: the street above is too narrow to fit both platforms opposite each other. Many lines have very sharp curves. The specifications established in 1900 required a very low minimum curve radius by railway standards, but even this was often not fully respected, for example near Bastille and Notre Dame de Lorette. Parts of the network are built at depth, in particular a section of line 12 under Montmartre, the sections under the Seine, and all of line 14.
Lines 7 and 13 have two terminal branches.
Rolling stock.
The rolling stock has steel-wheel ("MF" for "matériel fer") and rubber-tyred trains ("MP", "matériel pneu"). The different versions of each kind are specified by year of design.
Stations.
The typical station comprises two central tracks flanked by two 4‑m-wide platforms. About 50 stations, generally current or former termini, are exceptions; most have three tracks and two platforms (Porte d'Orléans), or two tracks and a central platform (Porte Dauphine). Some stations are single-track, either due to difficult terrain (Saint-Georges), a narrow street above (Liège) or track loops (Église d'Auteuil).
Station length was originally 75 m. This was extended to 90 m on high-traffic lines (1 and 4), with some stations at 105 m
(the difference as yet unused).
In general, stations were built near the surface by the cut-and-cover method, and are vaulted. Stations of the former "Nord-Sud" network (lines 12 and 13) have higher ceilings, due to the former presence of a ceiling catenary. There are exceptions to the rule of near-surface vaulting:
Several ghost stations are no longer served by trains. One of the three platforms at Porte des Lilas station is on a currently unused section of track, and is often used as a backdrop in films.
Interior decoration.
Concourses are decorated in Art Nouveau style defined at the Métro's opening in 1900. The spirit of this aesthetic has generally been respected in renovations.
Standard vaulted stations are lined by small white earthenware tiles, chosen because of the poor efficiency of early twentieth century electric lighting. From the outset walls have been used for advertising; posters in early stations are framed by coloured tiles with the name of the original operator (CMP or "Nord Sud"). Stations of the former "Nord Sud" (most of line 12 and parts of line 13) generally have more meticulous decoration. Station names are usually inscribed on metallic plaques in white letters on a blue background or in white tiles on a background of blue tiles.
The first renovations took place after the Second World War, when the installation of fluorescent lighting revealed the poor state of the original tiling. Three main styles of redecoration followed in succession.
A number of stations have original decorations to reflect the cultural significance of their locations. The first to receive this treatment was Louvre – Rivoli on line 1, which contains copies of the masterpieces on display at the museum. Other notable examples include Bastille (line 1), Saint-Germain-des-Prés (line 4), Cluny – La Sorbonne (line 10) and Arts et Métiers (line 11).
Exterior decoration.
The original "Art Nouveau" entrances are iconic symbols of Paris, and 83 survive. Designed by Hector Guimard in a style that caused some surprise and controversy in 1900, there are two main variants:
Later stations and redecorations have brought increasingly simple styles to entrances.
A handful of entrances have original architecture (Saint-Lazare), and a number are integrated into residential or standalone buildings (Pelleport).
Future.
Under construction.
There are three extensions being built.
Line 4 is being extended southward 3.2 km from Porte d'Orléans to Bagneux with two intermediate stations. Mairie de Montrouge opened in March 2013 and the further extension to Bagneux is due to open in 2019.
Line 12 had a new station at Front Populaire opened in 2012. It will be followed by a further extension to Mairie d’Aubervilliers, which is due to open in 2019.
Line 14 is being extended towards Mairie de Saint-Ouen with a projected opening in 2019.
Plans.
There are a few proposals that are being considered:
There have also been proposals for:
Cultural significance.
The Métro has a cultural significance that goes well beyond the city of Paris. The name Métropolitan (or Métro) has become a generic name for subways and urban underground railroads.
The station entrance kiosks, designed by Hector Guimard, fostered the Art Nouveau building style (once widely known as "le style Métro"), though, some French commentators criticized the Guimard station kiosks, including their green color and sign lettering as difficult to read.
The success of rubber-tired lines led to their export to metro systems around the world, starting with the Montreal Metro. The success of Montreal "did much to accelerate the international subway boom" of the 1960s/1970s and "assure the preeminence of the French in the process. Rubber-tired systems were adopted in Mexico City, Santiago, Lausanne, Turin, Singapore and other cities. The Japanese adopted rubber-tired metros (with their own technology and manufacturing firms) to systems in Kobe, Sapporo, and parts of Tokyo.
References.
Bibliography.
</dl>

</doc>
<doc id="50758" url="http://en.wikipedia.org/wiki?curid=50758" title="Carmen Miranda">
Carmen Miranda

Carmen Miranda, GCIH (], 9 February 1909 – 5 August 1955) was a Portuguese Brazilian samba singer, dancer, Broadway actress, and film star who was popular from the 1930s to the 1950s.
By the 1930s, Miranda was a local star, singing and dancing in musicals and five Brazilian feature films. Lee Shubert, a Broadway businessman, offered Carmen Miranda an eight-week contract to perform in "The Streets of Paris" on Broadway after seeing her perform at Cassino da Urca in Rio de Janeiro in 1939.
In 1940, she made her first Hollywood film, "Down Argentine Way", with Don Ameche and Betty Grable, her exotic clothing and Latin accent became her trademark. In the same year, she was voted the third most popular personality in the United States, and was invited to sing and dance for President Franklin Roosevelt, along with her group, Bando da Lua. Nicknamed "The Brazilian Bombshell", Carmen Miranda is noted for her signature fruit hat outfit she wore in her American films, particularly in 1943's "The Gang's All Here". By 1945, she was the highest paid woman in the United States.
Miranda made a total of fourteen Hollywood films between 1940 and 1953. Though hailed as a talented performer, her popularity waned by the end of World War II. She later grew to resent the stereotypical "Brazilian Bombshell" image she cultivated and attempted to break free of it, with limited success. Undaunted, Miranda focused increasingly on her nightclub appearances, also becoming a fixture on television variety shows—indeed, for all the stereotyping she faced throughout her career, her performances made huge strides in popularizing Brazilian music, while at the same time paving the way for the increasing awareness of all Latin culture.
Carmen Miranda was the first Latin American star to be invited to imprint her hands and feet in the courtyard of Grauman's Chinese Theatre, in 1941. She became the first South American to be honored with a star on the Hollywood Walk of Fame. She is considered the precursor of Brazil's Tropicalismo cultural movement of the 1960s.
A museum was later constructed in Rio de Janeiro in her honor, and in 1995 she was the subject of the acclaimed documentary "".
Early life.
Carmen Miranda was born Maria do Carmo Miranda da Cunha in , a village in the northern Portuguese municipality of Marco de Canaveses. She was the second daughter of José Maria Pinto da Cunha (17 February 1887 – 21 June 1938) and Maria Emília Miranda (10 March 1886 – Rio de Janeiro, 9 November 1971). In 1909 when she was ten months old, her father emigrated alone to Brazil and settled in Rio de Janeiro, where he opened a barber shop. Her mother followed in 1910 with their daughters Olinda (1907–1931) and Maria do Carmo. Maria do Carmo, later "Carmen", never returned to Portugal, but retained her Portuguese nationality. In Brazil, her parents had four more children: Amaro (1911), Cecília (1913–2011), Aurora (1915–2005) and Óscar (1916).
She was christened Carmen by her father because of his love for the opera comique, and also after Bizet's masterpiece "Carmen". This passion for opera influenced his children, and Miranda's love for singing and dancing at an early age. She went to school at the Convent of Saint Therese of Lisieux. Her father did not approve of her plans to enter show business. However, her mother supported her and was beaten when her husband discovered Miranda had auditioned for a radio show. She had previously sung at parties and festivals in Rio. Her older sister Olinda contracted tuberculosis and was sent to Portugal for treatment. Miranda went to work in a tie shop at age 14 to help pay her sister's medical bills. She next worked in a boutique, where she learned to make hats and opened her own hat business which became profitable.
Career.
Brazilian career.
Miranda was discovered when she was first introduced to composer Josué de Barros, who went on to promote and record her first album with Brunswick, a German recording company in 1929. The following year, she recorded "(Taí, Eu fiz Tudo) Prá Você Gostar de Mim" (also known as Taí) written by and became the most popular singing star in Brazil, a position she would maintain throughout the 1930s.
The increasing commercialization of popular music helped make Carmen Miranda the first truly national pop icon in Brazil's history. In November 1930, Miranda negotiated a recording contract with RCA Victor, the Brazilian subsidiary of the American music conglomerate. In 1933 went on to sign a two-year contract with Rádio Mayrink Veiga, the most popular station in the 1930s, becoming the first contract singer in the radio industry history of Brazil (though for a year – 1937 – she moved over to Radio Tupi). Later she signed a contract with record label RCA Records. In 1934, she was invited to perform as a guest artist on Radio Belgrano in Buenos Aires. In 1935, Odeon finally got her to sign a contract. This resulted in a number of hits, many of which are now classics of Brazilian music.
Miranda's rise to Brazilian stardom was intricately linked to the growing popularity of a distinctly Brazilian style of music: the samba. The expansion of the samba, and of Miranda's popularity, was greatly supportive of the refiguring of Brazilian nationalism during the regime of President Getúlio Vargas. Such was her gracefulness and vitality, as apparent in her recordings as in her live performances, that she was immediately dubbed "Cantora do It;" later she became "Ditadora Risonha do Samba," and then, in 1933, the radio announcer Cesar Ladeira gave her a lasting moniker: "A Pequena Notável". During the 1930s, Miranda recorded nearly three hundred songs, many written exclusively for her by Brazil's most renowned composers, such as Ary Barroso, Synval Silva and Dorival Caymmi. While recording or performing on radio and stage, she counted on Brazil's top musicians.
From 1933 to 1939, Brazil's burgeoning film industry, capitalizing on her widespread appeal, featured her in five films, invariably with parts that allowed her to showcase her vocal talent. As with other popular singers of the era, Miranda made her screen debut in the Brazilian documentary "A Voz Do Carnaval" (1933). Two years later, she appeared in her first feature film entitled "Alô, Alô Brasil". But it was the 1935 film "Estudantes" that seemed to solidify her in the minds of the movie-going public. In the 1936 movie "Hello, Hello, Carnival!", she performed the famous song "Cantoras do Rádio" with her sister Aurora, for the first time.
During her later career, Miranda would become primarily identified with her colorful fruit-hat costume and image, though she only adopted that costume in 1939. In that year she appeared in the film "Banana-da-Terra", where she wore a glamorized version of the traditional costume of a poor black girl of Bahia: flowing dress and fruit-hat turban. Singing the song "O que é que a Baiana Tem?"("What does a Baiana have?"), the intent was to empower a social class which was usually looked down upon.
In 1939 the Broadway impresario Lee Shubert visited Rio de Janeiro and witnessed the Brazilian sensation in action after seeing Miranda's extravagant stage show at the "Cassino da Urca". Shubert immediately offered her a contract to perform in his summer musical, "The Streets of Paris". Although she was intrigued by the possibility of performing in New York, Miranda refused to accept the deal unless Shubert agreed to also hire her band, the "Bando da Lua". The impresario refused, saying that there were plenty of great musicians in New York who could back her. But Miranda remained steadfast. She felt that North American musicians would not be able to authentically create the sounds of Brazil. As a compromise, Shubert agreed to hire the six band members, but he would not pay for their transport to New York. At this point, President Vargas, realizing the propaganda value of Miranda's tour, stepped in and announced that the Brazilian government would sponsor the band by providing free tickets on the Moore-McCormack Lines between Rio and New York.
He believed that Carmen Miranda would foster greater ties between northern and southern hemispheres and serve as an Ambassadress of Brazil in the United States. This could benefit Brazil economically by increasing its share of the American coffee market. Miranda took very seriously the official sanction of her trip and her duty of representing Brazil to the outside world. Before boarding the New York-bound SS Uruguay on 4 May. 1939, she held a press conference and told her fans:
American stage and films.
After seeing one of her performances in Rio, theatre owner Lee Shubert signed Miranda and her band, the Bando da Lua, to a contract. In 1939, Miranda sailed from Brazil aboard the ocean liner SS "Uruguay", arriving in New York on 18 May. She and the band made their first Broadway performance on 19 June 1939, in "The Streets of Paris". Although her part was small (she only spoke four words), Miranda received good reviews and became a media sensation. At the same time, she participated in the The Rudy Vallee Show (1929–43), or The Royal Gelatin Hour—one of the most popular American radio shows, broadcast weekly between 8:00 P.M. and 9:00 P.M. from New York's Radio City Music Hall.
The world's fair was attracting throngs to the Sunken Meadow fairgrounds just outside New York City in the summer of 1939, but Carmen Miranda still managed to make Shubert's show, "The Streets of Paris", a commercial success. "Life" magazine's reviewer noted:
"Time Magazine" dubbed her the "oomph that stops the show." New York audiences were enchanted by her exotic costume and accessories. One critic summed up her surprising appeal: "she is the biggest theatrical sensation of the year." By the end of the summer of 1939, the press lauded Miranda as "the girl who saved Broadway from the World's Fair." Her fame grew quickly, she having been formally presented to President Franklin D. Roosevelt at a White House banquet shortly after her arrival.
Her American film debut was in "Down Argentine Way" (1940), a musical produced by 20th Century Fox. Although the film's production and cast were based in Los Angeles, Miranda's scenes were filmed in New York City due to her obligation to perform for a club there. Fox combined the footage from both cities although Miranda has no on-screen dialogue with other cast members. The film was a great success and grossed $2 million that year in the American market.
The Shuberts brought Carmen back to Broadway, teaming her with Olsen and Johnson, Ella Logan, the Blackburn Twins, and others in the musical revue "Sons o'Fun" in 1 December 1941. The show was a hodgepodge of slapstick, songs, and skits. Richard Watts Jr. (New York Herald Tribune) concluded, "In her eccentric and highly personalized fashion, Miss Miranda is by way of being an artist and her numbers give the show its one touch of distinction." Her rousing showstopper was "Thank You, North American". On 1 June 1942, she left the production; her Shubert contract had expired. Meanwhile she made recordings for Decca Records, including "Chica, Chica Boom Chic," "O Tic-Tac do Meu Coração," and "Chattanooga Choo Choo."
Miranda was encouraged by the United States government as part of President Roosevelt's Good Neighbor policy, designed to strengthen links with Latin America and Europe. It was believed that in delivering content like hers, the policy would be better received by the American public. Miranda's contract with 20th Century Fox lasted from 1941 to 1946; this period coincides with the time of World War II (1939–1945) and the creation in 1940 of the Office of the Coordinator of Inter-American Affairs (OCIAA), based in Rio de Janeiro, whose goal was to obtain support from governments and Latin American societies for the cause of the United States.
The interference was linked to the Good Neighbor policy and Roosevelt sought to forge better diplomatic relations with Brazil and other South American nations, and pledged to refrain from further military intervention, which has sometimes been done to protect U.S. business interests in industries such as mining or agriculture. Hollywood was asked to help out with the Good Neighbor Policy, and both Walt Disney Studios and 20th Century Fox participated. Miranda was considered the goodwill ambassador and promoter of intercontinental culture.
Criticism.
While Miranda's popularity in the United States continued to rise, she began to lose favor with some Brazilians. On 10 July 1940, she returned to Brazil where she was welcomed by cheering fans. Soon after her arrival, however, the Brazilian press began criticizing Miranda for giving in to American commercialism and projecting a negative image of Brazil. Members of the upper class felt her image was "too black" and she was criticized in one Brazilian newspaper for "singing bad-tasting black sambas". Other Brazilians criticized her for playing up the stereotype of a "Latina bimbo" after her first interview upon arriving in the United States. In an interview with the "New York World-Telegram", Miranda discussed her then limited knowledge of the English language stating, "I say money, money, money. I say twenty words in English. I say money, money, money and I say hot dog!"
On 15 July, she appeared at a charity concert organized by Brazilian First Lady Darci Vargas. The concert was attended by members of Brazil's high society. She greeted the audience in English but was met with silence. When Miranda began singing a song from one of her club acts, "The South American Way", the audience began to boo her. She attempted to finish her act but gave up and left the stage after the audience continued to boo. The incident deeply hurt Miranda and she later cried in her dressing room. The following day, the Brazilian press criticized her for being "too Americanized".
Weeks later, Miranda responded to the criticism with the Portuguese language song "Disseram que Voltei Americanizada" (or "They Say I've Come Back Americanized"). Another song, "Bananas Is My Business" was based on a line in one of her movies and directly addressed her image. She was greatly upset by the criticism and did not return to Brazil again for fourteen years.
Miranda's films came under harsh scrutiny by Latin American audiences for characterizing Central and South America in a culturally homogenous way. When her films hit theatres in Central and South America, it was strongly felt that the films depicted Latin American cultures through the lens of American preconceptions, and not as they actually were. Many Latin Americans felt their cultures were being misrepresented, and felt that someone from their own region, Carmen Miranda, was misrepresenting them. Her film, "Down Argentine Way" (1940), was met with heavy criticism, with pundits in Argentina claiming that it failed to depict Argentinean culture. It was alleged that lyrics throughout the movie were filled with non-Argentine themes, and that the sets were not strictly Argentinean, but rather, a fusion of cultures from Mexico, Cuba, and Brazil. The film was subsequently banned in Argentina, for "wrongfully portraying life in Buenos Aires." Similar sentiments arose in Cuba after her the debut of Miranda's film, "Weekend in Havana" (1941). Cuban audiences were offended by Miranda's portrayal of a Cuban female. Reviewers of the film asserted that an import from Rio could not possibly portray a woman from Havana. Further, they claimed that throughout the film Miranda does not "dance anything Cuban." Miranda's performances, it was argued, were merely hybridizations of Brazilian culture and other Latin cultures. Critics contend that other of her films likewise misrepresented Latin locales, by assuming that Brazilian culture could suffice as a direct representation of Latin America.
Peak years.
Upon returning to the United States, Miranda kept up her film career in Hollywood while also appearing on Broadway and performing in clubs and restaurants.
The war years saw Carmen Miranda starring in eight of her fourteen films and, although the studios labelled her the "Brazilian Bombshell," the films tended to blur her Brazilian identity in favor of a generalized Latin American image, she began appearing in its films as a featured performer.
In 1941, she shared the screen with Alice Faye and Don Ameche in "That Night in Rio". Later that same year, she teamed up with Alice Faye again in "Week-End in Havana". Miranda was now earning $5,000 a week. On 24 March 1941, she became one of the first Latinas to leave her hand and footprints in the sidewalk of Grauman's Chinese Theater.
In 1943, she appeared in an extravaganza from noted director Busby Berkeley called "The Gang's All Here". Berkeley's musicals were known for their lavish production, and Miranda's role as Dorita featured her number "The Lady in the Tutti-Frutti Hat." An optical trick from the set behind her made the fruit-bedecked hat she was wearing appear even larger than humanly possible. By then, Miranda seemed to be locked into such roles as the exotic songstress, and her studio contract even forced her to appear at events in her trademark film costumes, which grew even more outlandish. One song she recorded, "Bananas Is My Business" seemed to pay somewhat ironic tribute to her typecasting. The following year, Miranda made a cameo appearance in "Four Jills in a Jeep". By 1945, she had become Hollywood's highest-paid entertainer and top female taxpayer in the United States, earning more than $200,000 that year ($2.2 million in 2010 adjusted for inflation).
Decline.
After World War II ended in 1945, the American public's tastes began to change and musicals began to fall out of favor. Hollywood studio heads and producers also felt that the novelty of Miranda's "Brazilian bombshell" image had worn thin.
As a result, Miranda's career declined. She made one last film for Fox, "Doll Face" (1945), before her contract was terminated in January 1946.
She later signed a contract with Universal but at the time, Universal was undergoing a merger with another studio. Due to a change in management, no films for Miranda were planned. Eager to break away from her well established image, Miranda attempted to branch out with different roles. In 1946, she portrayed an Irish American character in "If I'm Lucky". The following year, she played dual roles opposite Groucho Marx in "Copacabana" for United Artists. While the films were modest hits, film critics and the American public did not accept Miranda's new image.
Though her film career was faltering, Miranda music career remained solid and she was still a popular attraction at nightclubs. From 1948 to 1950, Miranda teamed with The Andrews Sisters to produce and record three Decca singles. Their first collaboration was on radio in 1945 when Miranda guested on ABC's "The Andrews Sisters Show". The first single, "Cuanto Le Gusta", was the most popular (a best-selling record and a number-twelve "Billboard" hit). "The Wedding Samba" (#23) followed in 1950.
In 1948, she co-starred opposite Wallace Beery and Jane Powell in "A Date with Judy", and "Nancy Goes to Rio" in 1950 for MGM. She made her final film appearance in the 1953 film "Scared Stiff" with Martin and Lewis for Paramount.
Following the release of "Scared Stiff" in April 1953, she embarked on a four-month European tour. While performing in Cincinnati in October 1953, Carmen Miranda collapsed from exhaustion. She began suffering from acute depression, and underwent electroshock therapy, and when that failed to cure her, her physician suggested a return visit to Brazil. Accompanied by her sister Aurora, she arrived in Rio de Janeiro on 3 December 1954, her first visit home in fourteen years. When she arrived, she was pleased to be greeted by her fans and commented, "My people, I'm happy! I can't say anything else. How good it is to be home." Carmen stayed four months in Brazil. Recovered, she returned to the United States on 4 April 1955.
Personal life.
In 1947, to achieve more creative freedom in a film she was making, Carmen decided to produce her own film. It was called "Copacabana" and she played opposite Groucho Marx. The budget was divided into around ten sponsors' quotas. A Texan investor, who held one of the quotas, sent his brother David Sebastian (23 November 1907 – 2 August 1990) to keep an eye on Carmen and look after his interests on the film set. His position allowed him to get close to Carmen and they started to date. On 17 March 1947, Miranda married Sebastian. In 1948 she became pregnant, but suffered a miscarriage after a show. The marriage lasted only a few months, but Carmen, who was Catholic, would not accept getting a divorce. Her sister Aurora Miranda later would state in the documentary "" that "he was very rude, many times even hit her. The marriage was a burden in her life; he only married her for her money. He did not like our family". In September 1949, the couple announced their separation, but they later reconciled.
Before leaving for the United States and before meeting her husband, Carmen had a relationship with the young Mario Cunha and bon vivant Carlos da Rocha Faria, son of a traditional family of Rio de Janeiro, and also the musician Aloysio de Oliveira, one of the "Bando da Lua" members. In the US, she maintained relationships with the Mexican Arturo de Córdova, Dana Andrews, Harold Young and John Wayne, and the Brazilian Carlos Niemeyer.
In her later years, in addition to her already heavy smoking and alcohol consumption, Miranda began taking amphetamines and barbiturates, all of which took a toll on her health.
Death.
In April 1955, Carmen performed at the New Frontier Hotel in Las Vegas, and in July, in Cuba. Thereafter, she returned to Los Angeles to recuperate from a recurring bronchial ailment.
On 4 August 1955, Miranda was shooting a segment for the filmed NBC variety series "The Jimmy Durante Show". According to Durante, Miranda had complained of feeling unwell before filming. Durante offered to get Miranda a replacement but she declined. After completing a song and dance number, "Jackson, Miranda, and Gomez", with Durante, she fell to one knee. Durante later said of the incident, "I thought she had slipped. She got up and said she was outa [sic] breath. I tells her I'll take her lines. But she goes ahead with 'em. We finished work about 11 o'clock and she seemed happy." At around 4 a.m. the following day, Miranda suffered a fatal heart attack at her home in Beverly Hills.
The "Jimmy Durante Show" episode in which Miranda appeared was aired two months after her death, on 15 October 1955. A clip of the episode was also included in the A&E Network's "Biography" episode about Miranda.
Funeral and burial.
In accordance with her wishes, Miranda's body was flown back to Rio de Janeiro where the Brazilian government declared a period of national mourning.
A crowd of about 60,000 people attended her mourning ceremony at the Rio town hall, and more than half a million Brazilians escorted the funeral cortège to her resting place.
She is buried in São João Batista Cemetery in Rio de Janeiro. In 1956, all her belongings were donated by her husband and family for the creation of Carmen Miranda Museum, which opened its doors in Rio on 5 August 1976.
For her contributions to the television industry, Carmen Miranda has a star on the Hollywood Walk of Fame at the south side of the 6262 block of Hollywood Boulevard.
Image.
Miranda's Hollywood image was one of a generic Latinness that blurred the distinctions between Brazil, Portugal, Argentina, and Mexico as well as between samba, tango and habanera. It was carefully stylized and outlandishly flamboyant. She was often shown wearing platform sandals and towering headdresses made of fruit, becoming famous as "the lady in the tutti-frutti hat." Miranda's enormous, fruit-laden hats are iconic visuals recognized around the world. These costumes led to Saks Fifth Avenue developing a line of turbans and jewelry inspired by Carmen Miranda in 1939. Many costume jewelry designers made fruit jewelry also inspired by Carmen Miranda which is still highly valued and collectible by vintage and antique costume jewelry collectors. Fruit jewelry is still popular in jewelry design today. Much of the fruit jewelry seen today is often still called "Carmen Miranda jewelry" because of this.
The striking figure of Carmen Miranda is today continually revisited. "Prada" (Alexandre Herchcovitch), "Salinas" and "Rosa Chá" are some of those who were inspired by the artist to create their collections. Miranda was even theme of São Paulo Fashion Week in January 2009 and a short film called "Tutti Frutti " from the German photographer Ellen von Unwerth, she is today a source of endless inspiration.
Her image was much satirized and taken up as camp, and today, the "Carmen Miranda" persona is popular among drag performers.
Image Rights.
The "Carmen Miranda Administração e Licenciamentos" is the an company in charge of protecting and managing Carmen Miranda’s artistic estate. Her comprehensive image, name, voice, and performances are protected by copyright under Brazilian Law nº 9,610, of 19 February 1998, in force since 21 June of the same year. Carmen Miranda is an trademark registered at the Instituto Nacional da Propriedade Industrial (INPI) and its use is prohibited without prior authorization.
Legacy.
The Brazilian musician Caetano Veloso, who is largely responsible for resurrecting Carmen Miranda as a Brazilian icon during the 1960s, has this to say about her legacy in an article for the "New York Times": "For generations of musicians who were adolescents in the second half of the 1950s and became adults at the height of the Brazilian military dictatorship and the international wave of counterculture-my generation-Carmen Miranda was first a cause of both pride and shame, and later, a symbol that inspired the merciless gaze we began to cast upon ourselves, Carmen conquered 'white' America as no other South American has done or ever would, in an era when it was enough to be 'recognizable Latin and Negroid' in style and aesthetics to attract attention." For Veloso and other musicians contemplating a career abroad, Miranda's pioneering experiences continue to loom as a point of reference. Miranda helped establish and transform the relationship between Brazilian musicians and American producers that now has created several remarkable transnational collaborations. In Veloso's words: "To think of her is to think about the complexity of this relationship".
When Carmen Miranda died in 1955, her popularity abroad was greater than in Brazil. Nonetheless, her contributions to the music and culture of Brazil should not be overlooked. Although she was accused of peddling Brazilian music and dance in a highly commercialized format, Carmen Miranda can be credited with bringing Brazil's national music, the samba, to a worldwide audience. In addition, she introduced the image of the "baiana" with wide skirts and turbaned headdress as the "showgirl" of Brazil at home and abroad. The baiana costume was adopted as the central feature of Carnival for women and, especially, for men, who famously dress up in elaborate Carmen Miranda style and parade through the streets of Brazil's cities during Carnival.
Even after her death, Carmen Miranda is remembered for being perhaps the most important Brazilian artistic personality of all time and one of the most influential in Hollywood. She is listed by the American Film Institute as one of the "500 great legends of Cinema".
On 25 September 1998, a city square in Hollywood was named Carmen Miranda Square in a ceremony headed by longtime honorary mayor of Hollywood, Johnny Grant, who was also one of the singer's friends dating back to World War II. Brazil's Consul General Jorió Gama was on hand for opening remarks, as were members of Bando da Lua, Carmen Miranda's original band. Carmen Miranda Square is only one of about a dozen Los Angeles city intersections named for historic performers. The square is located at the intersection of Hollywood Boulevard and Orange Drive across from Grauman's Chinese Theater. The location is especially noteworthy not only since Carmen Miranda's footprints are preserved in concrete at Grauman's Chinese Theatre, but in remembrance of an impromptu performance at a nearby Hollywood Boulevard intersection on V-J Day.
A museum dedicated to Carmen Miranda is located in Rio de Janeiro in the Flamengo neighborhood on Avenida Rui Barbosa. The museum includes several original costumes, and shows clips from her filmography. There is also a museum dedicated to her in Marco de Canaveses, Portugal called "Museu Municipal Carmen Miranda", with various photos and one of the famous hats. Outside the museum there is a statue of Carmen Miranda.
In honor of the 50th anniversary of the great star's death, many events were held in Brazil, including an exhibition, "Carmen Miranda Forever", that was initially mounted at the Museum of Modern Art in Rio de Janeiro in November 2005 and traveled to a number of Brazilian cities in 2006. and Ruy Castro, one of the city's best-known writers, has just published a 600-page biography of "the most famous Brazilian woman of the 20th century." Brazilians "tend to forget," Castro told Margolis in Newsweek International, that "no Brazilian woman has ever been as popular as Carmen Miranda – in Brazil or anywhere."
In 2009, the recording of "O que é que a baiana tem?" by Dorival Caymmi, sung by Miranda in 1939, was selected for preservation in the Library of Congress. The recording helped to introduce both the samba rhythm and Carmen Miranda to American audiences. It was also the first recording of a song by Caymmi, who went on to become a major composer and performer.
In 2011, along with Selena, Celia Cruz, Carlos Gardel and Tito Puente, Carmen Miranda was immortalized by the U.S. Postal Service in the series of Postage stamp: Latin Music Legends (Forever). The stamps were painted by artist Rafael Lopez. "From this day forward, these colorful, vibrant images of our Latin music legends will travel on letters and packages to every single household in America. In this small way, we have created a lasting tribute to five extraordinary performers, and we are proud and honored to share their legacy with Americans everywhere through these beautiful stamps", said Marie Therese Dominguez, vice president of Government Relations and Public Policy for the U.S. Postal Service.
In 2014, "Down Argentine Way" and "The Gang's All Here" were deemed "culturally, historically, or aesthetically significant" by the Library of Congress and selected for preservation in the National Film Registry.
In popular culture.
Covers of Carmen Miranda songs.
Miranda's songs "Disseram que Voltei Americanizada", "I, Yi, Yi, Yi, Yi (I Like You Very Much)", "South American Way" and "Tico-Tico no Fubá" have each been covered many times, often in tribute to her; see those songs' articles for information on other recordings.

</doc>
<doc id="50759" url="http://en.wikipedia.org/wiki?curid=50759" title="Elbe">
Elbe

The Elbe (Czech:   ; German: "Elbe"; Low German: "Elv") is one of the major rivers of Central Europe. It rises in the Krkonoše Mountains of the northern Czech Republic before traversing much of Bohemia (Czech Republic), then Germany and flowing into the North Sea at Cuxhaven, 110 km northwest of Hamburg. Its total length is 1094 km.
The Elbe's major tributaries include the Vltava, Saale, Havel, Mulde, Schwarze Elster, and Ohře rivers.
The Elbe River basin, comprising the Elbe and its tributaries, has a catchment area of 148268 km2, the fourth largest in Europe. The basin spans four countries, with its largest parts in Germany (65.5%) and the Czech Republic (33.7%). Much smaller parts lie in Austria (0.6%) and Poland (0.2%). The basin is inhabited by 24.5 million people.
Course.
In the Czech Republic.
The Elbe rises at an elevation of about 1400 m in the Krkonoše (also known as "Giant Mountains" or in German as "Riesengebirge") on the northwest borders of the Czech Republic near Labská bouda. Of the numerous small streams whose waters compose the infant river, the most important is the Bílé Labe, or White Elbe. After plunging down the 60 m of the Labský vodopád, or Elbe Falls, the latter stream unites with the steeply torrential Malé Labe, and thereafter the united stream of the Elbe pursues a southerly course, emerging from the mountain glens at and continuing on to Pardubice, where it turns sharply to the west. At Kolín some 43 km further on, it bends gradually towards the north-west. 
At the village of Káraný, a little above Brandýs nad Labem, it picks up the Jizera.
At Mělník its stream is more than doubled in volume by the Vltava, or Moldau, a river which winds northwards through Bohemia. Upstream from the confluence the Vltava is in fact longer (434 km against 294 km), and has a greater discharge and a larger drainage basin. Nonetheless, for historical reasons the river retains the name Elbe, also because at the confluence point it is the Elbe that flows through the main, wider valley while the Vltava flows into the valley to meet the Elbe at almost a right angle, and thus appears to be the tributary river.
Some distance lower down, at Litoměřice, the waters of the Elbe are tinted by the reddish Ohře (Eger). Thus augmented, and swollen into a stream 140 m wide, the Elbe carves a path through the basaltic mass of the České Středohoří, churning its way through a deep, narrow rocky gorge.
In Germany.
Shortly after crossing the Czech-German frontier, and passing through the sandstone defiles of the Elbe Sandstone Mountains, the stream assumes a north-westerly direction, which on the whole it preserves right to the North Sea.
The river rolls through Dresden and finally, beyond Meißen, enters on its long journey across the North German Plain passing along the former border of East Germany, touching Torgau, Wittenberg, Dessau, Magdeburg, Wittenberge, and Hamburg on the way, and taking on the waters of the Mulde and Saale from the west, and those of the Schwarze Elster, Havel and Elde from the east. In its northern section both banks of the Elbe are characterised by flat, very fertile marshlands (Elbe Marshes), former flood plains of the Elbe now diked. 
At Magdeburg there is a viaduct, the Magdeburg Water Bridge, that carries a canal and its shipping traffic over the Elbe and its banks, allowing shipping traffic to pass under it unhindered.
The middle Elbe in the North German Plain near the village of Gorleben. In this section, the river had been part of the Iron Curtain between West and East Germany during the Cold War. For that reason, the riverbanks even today look relatively natural and undeveloped.
From the sluice of Geesthacht (at kilometre 586) on downstream the Elbe is subject to the tides, the tidal Elbe section is called the Low Elbe (Unterelbe). Soon the Elbe reaches Hamburg. Within the city-state the Unterelbe has a number of branch streams, such as Dove Elbe, Gose Elbe, Köhlbrand, Northern Elbe (Norderelbe), Reiherstieg, Southern Elbe (Süderelbe). Some of which have been disconnected for vessels from the main stream by dikes. In 1390 the Gose Elbe (literally in English: shallow Elbe) was separated from the main stream by a dike connecting the two then-islands of Kirchwerder and Neuengamme. The Dove Elbe (literally in English: deaf Elbe) was diked off in 1437/38 at Gammer Ort. These hydraulic engineering works were carried out to protect marshlands from inundation, and to improve the water supply of the Port of Hamburg. After the heavy inundation by the North Sea flood of 1962 the western section of the Southern Elbe was separated, becoming the Old Southern Elbe, while the waters of the eastern Southern Elbe now merge into the Köhlbrand, which is bridged by the Köhlbrandbrücke, the last bridge over the Elbe before the North Sea. 
The Northern Elbe passes the Elbe Philharmonic Hall and is then crossed under by the old Elbe Tunnel (Alter Elbtunnel), both in Hamburg's city centre. A bit more downstream the Low Elbe's two main anabranches Northern Elbe and the Köhlbrand reunite south of Altona-Altstadt, a locality of Hamburg. Right after both anabranches reunited the Low Elbe is passed under by the New Elbe Tunnel (Neuer Elbtunnel), the last structural road link crossing the river before the North Sea. At the bay Mühlenberger Loch in Hamburg at kilometre 634, the Northern Elbe and the Southern Elbe (here now the cut-off meander Old Southern Elbe) used to reunite, which is why the bay is seen as the starting point of the Lower Elbe (Niederelbe). Leaving the city-state the Lower Elbe then passes between Holstein and the Elbe-Weser Triangle with Stade until it flows into the North Sea at Cuxhaven. Near its mouth it passes the entrance to the Kiel Canal at Brunsbüttel before it debouches into the North Sea.
View of the River Elbe in Saxon Switzerland, an area in Germany.
Navigation.
The Elbe has been navigable by commercial vessels since 1842, and provides important trade links as far inland as Prague. The river is linked by canals (Elbe-Seitenkanal, Elbe-Havel Canal, Mittellandkanal) to the industrial areas of Germany and to Berlin. The Elbe-Lübeck Canal links the Elbe to the Baltic Sea, as does the Kiel Canal, whose western entrance is near the mouth of the Elbe. The Elbe-Weser Shipping Channel connects the Elbe with the Weser. 
By the Treaty of Versailles the navigation on the Elbe became subject to the International Commission of the Elbe, seated in Dresden. The statute of the Commission was signed in Dresden on February 22, 1922. Following articles 363 and 364 of the Treaty of Versailles, Czechoslovakia was entitled to lease its own harbour bassin, Moldauhafen in Hamburg. The contract of lease with Germany, and supervised by the United Kingdom, was signed on February 14, 1929 and will end in 2028. Since 1993 the Czech Republic holds the former Czechoslovak legal position. 
Before Germany was reunited, waterway transport in Western Germany was hindered by the fact that inland navigation to Hamburg had to pass through the German Democratic Republic. The Elbe-Seitenkanal (Elbe Lateral Canal) was built between the West German section of the Mittellandkanal and the Lower Elbe to restore this connection. When the two nations were reunited, works were begun to improve and restore the original links: the Magdeburg Water Bridge now allows large barges to cross the Elbe without having to enter the river. The often low water levels of the Elbe do not hinder navigation to Berlin any longer.
Ferries.
The Elbe is crossed by many ferries, both passenger and car carrying. In downstream order, these include:
Many of these ferries are traditional reaction ferries, a type of cable ferry that uses the current flow of the river to provide propulsion.
Etymology.
First attested in Latin as "Albis", the name "Elbe" means "river" or "river-bed" and is nothing more than the High German version of a word ("albiz") found elsewhere in Germanic; cf. Old Norse river name "Elfr", Swedish "älv" "river", Norwegian "elv" "river", Old English river name "Ielf", and Middle Low German "elve" "river-bed".
History.
The Elbe was recorded by Ptolemy as "Albis" (Germanic for "river") in Germania Magna with its source in the "Asciburgis" mountains (Krkonoše, Riesengebirge or Giant Mountains), where the Germanic "Vandalii" lived.
The Elbe has long been an important delineator of European geography. The Romans knew the river as the "Albis"; however, they only attempted once to move the Eastern border of their empire forward from the Rhine to the Elbe, and this attempt failed in the Battle of the Teutoburg Forest in 9 AD, after which they never seriously tried again. In the Middle Ages it formed the eastern limit of the Empire of Charlemagne. The river's navigable sections were also essential to the success of the Hanseatic League and much trade was carried on its waters.
The Elbe delineated the western parts of Germany from the eastern so-called East Elbia, where soccage and serfdom were more strict and prevailed longer, than westwards of the river, and where feudal lords held bigger estates than in the west. Thus incumbents of huge land-holdings became characterised as East Elbian Junkers. The Northern German area north of the Lower Elbe used to be called North Albingia in the Middle Ages. When the four Lutheran church bodies there united in 1977 they chose the name North Elbian Evangelical Lutheran Church. Other, administrative units were named after the river Elbe, such as the Westphalian "Elbe département" (1807–1813) and the "Lower Elbe département" (1810), and the French département Bouches-de-l'Elbe (1811–1814). 
In 1945, as World War II was drawing to a close, Nazi Germany was caught between the armies of the western Allies advancing from the west and the Soviet Union advancing from the east. On 25 April 1945, these two forces linked up near Torgau, on the Elbe. The event was marked as Elbe Day. After the war, the Elbe formed part of the border between East Germany and West Germany.
During the 1970s, the Soviet Union stated that Adolf Hitler's ashes had been scattered in the Elbe following disinterment from their original burial site.
External links.
 Geographic data related to at OpenStreetMap

</doc>
<doc id="50766" url="http://en.wikipedia.org/wiki?curid=50766" title="Brethren">
Brethren

Brethren is a name adopted by a wide range of mainly Christian religious groups throughout history which do not necessarily share historical roots, including some of the earliest primitive churches, the Paulician Brethren, the Bogomil Brethren, the Brethren of the Free Spirit, the Schwarzenau Brethren and some Anabaptist groups, the Moravian Brethren, and the Plymouth Brethren, among many others.
Anabaptist groups.
These groups grew out of the Anabaptist movement at the time of the Protestant Reformation (16th century).
Schwarzenau Brethren.
The Schwarzenau Brethren originated in 1708 in Schwarzenau, Bad Berleburg, Germany, with Alexander Mack. Their roots are in the Radical Pietism movement but they were strongly influenced by Anabaptist theology. They have also been called "Dunkers" or "German Baptist Brethren". The group split into three wings in 1881–1883:
Plymouth Brethren.
The various Plymouth Brethren bodies originated in the 1820s work of John Nelson Darby and others in Ireland and the United Kingdom as well as India:
Methodist groups.
Some groups named "Brethren" have contributed to United Methodism:
River Brethren.
The River Brethren owe their origins to the combined labors of Reformed pastor Philip William Otterbein and Mennonite Martin Boehm, beginning in Lancaster County, Pennsylvania in the latter half of the 18th century. They were also influenced by the Schwarzenau Brethren and include (amongst others):

</doc>
<doc id="50767" url="http://en.wikipedia.org/wiki?curid=50767" title="Privateer">
Privateer

A privateer or "corsair" was a private person or ship authorized by a government by letters of marque to attack foreign vessels during wartime. Privateering was a way of mobilizing armed ships and sailors without having vessels be commissioned into regular service as warships. The crew of a privateer might be treated as prisoners of war by the enemy country if captured.
Historically, the distinction between a "privateer" and a "pirate" has been subjective, often depending on the source as to which label was correct in a particular circumstance. The actual work of a "pirate" and a "privateer" is generally the same (raiding and plundering ships); it is, therefore, the authorization and perceived legality of the actions that form the distinction. At various times, governments indiscriminately granted authorization for privateering to a variety of ships, so much so that would-be pirates could easily operate under a veil of legitimacy.
Legal framework.
Being privately owned and run, privateers did not take orders from the Naval command. The letter of marque of a privateer would typically limit activity to a specific area and to the ships of specific nations. Typically, the owners or captain would be required to post a performance bond against breaching these conditions, or they might be liable to pay damages to an injured party. In the United Kingdom, letters of marque were revoked for various offences.
Conditions on board privateers varied widely. Some crews were treated as harshly as naval crews of the time, while others followed the comparatively relaxed rules of merchant ships. Some crews were made up of professional merchant seamen, others of pirates, debtors, and convicts. Some privateers ended up becoming pirates, not just in the eyes of their enemies but also of their own nations. William Kidd, for instance, began as a legitimate British privateer but was later hanged for piracy.
The Paris Declaration Respecting Maritime Law of 16 April 1856 was issued to abolish privateering. It regulated the relationship between neutral and belligerent and shipping on the high seas introducing new prize rules.
Ships.
Entrepreneurs converted many different types of vessels into privateers, including obsolete warships and refitted merchant ships. The investors would arm the vessels and recruit large crews, much larger than a merchantman or a naval vessel would carry, in order to crew the prizes they captured. Privateers generally cruised independently, but it was not unknown for them to form squadrons, or to co-operate with the regular navy. A number of privateers were part of the English fleet that opposed the Spanish Armada in 1588. Privateers generally avoided encounters with warships, as such encounters would be at best unprofitable. Still, such encounters did occur. For instance, in 1815 "Chasseur" encountered HMS "St Lawrence", herself a former American privateer, mistaking her for a merchantman until too late; in this instance, the privateer prevailed.
The United States used mixed squadrons of frigates and privateers in the American Revolutionary War. Following the French Revolution, French privateers became a menace to British and American shipping in the western Atlantic and the Caribbean, resulting in the Quasi-War, a brief conflict between France and the United States, fought largely at sea, and to the Royal Navy's procuring Bermuda sloops to combat the French privateers.
Overall history.
England, and later the United Kingdom, used privateers to great effect and suffered much from other nations' privateering. During the 15th century, "piracy became an increasing problem and merchant communities such as Bristol began to resort to self-help, arming and equipping ships at their own expense to protect commerce." These privately owned merchant ships, licensed by the crown, could legitimately take vessels that were deemed pirates. This constituted a "revolution in naval strategy" and helped fill the need for protection that the current administration was unable to provide as it "lacked an institutional structure and coordinated finance."
The increase in competition for crews on armed merchant vessels and privateers was due, in a large part, because of the chance for a considerable payoff. "Whereas a seaman who shipped on a naval vessel was paid a wage and provided with victuals, the mariner on a merchantman or privateer was paid with an agreed share of the takings." This proved to be a far more attractive prospect and privateering flourished as a result.
During Queen Elizabeth's reign, she "encouraged the development of this supplementary navy." Over the course of her rule, she had "allowed Anglo-Spanish relations to deteriorate" to the point where one could argue that a war with the Spanish was inevitable. By using privateers, if the Spanish were to take offense at the plundering of their ships, Queen Elizabeth could always deny she had anything to do with the actions of such independents. Some of the most famous privateers that later fought in the Anglo-Spanish War (1585–1604) included the Sea Dogs.
In the late 16th century, English ships cruised in the Caribbean and off the coast of Spain, trying to intercept treasure fleets from the Spanish Main. At this early stage the idea of a regular navy (the Royal Navy, as distinct from the Merchant Navy) was not present, so there is little to distinguish the activity of English privateers from regular naval warfare. Attacking Spanish ships, even during peacetime, was part of a policy of military and economic competition with Spain - which had been monopolizing the maritime trade routes along with the Portuguese helping to provoke the first Anglo-Spanish War. Capturing a Spanish treasure ship would enrich the Crown as well as strike a practical blow against Spanish domination of America. Piet Pieterszoon Hein was a brilliantly successful Dutch privateer who captured a Spanish treasure fleet. Magnus Heinason was another privateer who served the Dutch against the Spanish. While their and others' attacks brought home a great deal of money, they hardly dented the flow of gold and silver from Mexico to Spain.
Elizabeth was succeeded by the first Stuart monarchs, James I and Charles I, who did not permit privateering. There were a number of unilateral and bilateral declarations limiting privateering between 1785 and 1823. However, the breakthrough came in 1856 when the Declaration of Paris, signed by all major European powers, stated that "Privateering is and remains abolished". The USA did not sign because a stronger amendment, protecting all private property from capture at sea, was not accepted. In the 19th century many nations passed laws forbidding their nationals from accepting commissions as privateers for other nations. The last major power to flirt with privateering was Prussia in the 1870 Franco-Prussian War, when Prussia announced the creation of a 'volunteer navy' of ships privately owned and manned and eligible for prize money. The only difference between this and privateering was that these volunteer ships were under the discipline of the regular navy.
17th, 18th and 19th centuries.
Privateers were a large part of the total military force at sea during the 17th and 18th centuries. In the first Anglo-Dutch War, English privateers attacked the trade on which the United Provinces entirely depended, capturing over 1,000 Dutch merchant ships. During the subsequent war with Spain, Spanish and Flemish privateers in the service of the Spanish Crown, including the notorious Dunkirkers, captured 1,500 English merchant ships, helping to restore Dutch international trade. British trade, whether coastal, Atlantic, or Mediterranean, was also attacked by Dutch privateers and others in the Second and Third Anglo-Dutch wars.
During King George's War, approximately 36,000 Americans served aboard privateers at one time or another. During the Nine Years War, the French adopted a policy of strongly encouraging privateers, including the famous Jean Bart, to attack English and Dutch shipping. England lost roughly 4,000 merchant ships during the war. In the following War of Spanish Succession, privateer attacks continued, Britain losing 3,250 merchant ships. Parliament passed an updated Cruisers and Convoys Act in 1708 allocating regular warships to the defence of trade.
In the subsequent conflict, the War of Austrian Succession, the Royal Navy was able to concentrate more on defending British ships. Britain lost 3,238 merchantmen, a smaller fraction of her merchant marine than the enemy losses of 3,434. While French losses were proportionally severe, the smaller but better protected Spanish trade suffered the least and it was Spanish privateers who enjoyed much of the best allied plunder of British trade, particularly in the West Indies.
During the American Civil War privateering took on several forms, including blockade running while privateering in general occurred in the interests of both the North and the South. Letters of marque would often be issued to private shipping companies and other private owners of ships, authorizing them to engage vessels deemed to be unfriendly to the issuing government. Crews of ships were awarded the cargo and other prizes aboard any captured vessel as an incentive to search far and wide for ships attempting to supply the Confederacy, or aid the Union, as the case may be.
Britain.
England and Scotland practiced privateering both separately and together after they united to create the Kingdom of Great Britain in 1707. It was a way to gain for themselves some of the wealth the Spanish and Portuguese were taking from the New World before beginning their own trans-Atlantic settlement, and a way to assert naval power before a strong Royal Navy emerged.
Sir Andrew Barton, Lord High Admiral of Scotland, followed the example of his father, who had been issued with letters of marque by James III of Scotland to prey upon English and Portuguese shipping in 1485; the letters in due course were reissued to the son. Barton was killed following an encounter with the English in 1511.
Sir Francis Drake, who had close contact with the sovereign, was responsible for some damage to Spanish shipping, as well as attacks on Spanish settlements in the Americas in the 16th century. He participated in the successful English defence against the Spanish Armada in 1588, though he was also partly responsible for the failure of the English Armada against Spain in 1589.
Sir George Clifford, 3rd Earl of Cumberland was a successful privateer against Spanish shipping in the Caribbean. He is also famous for his short-lived 1598 capture of "Fort San Felipe del Morro", the citadel protecting San Juan, Puerto Rico. He arrived in Puerto Rico on June 15, 1598, but by November of that year Clifford and his men had fled the island due to fierce civilian resistance. He gained sufficient prestige from his naval exploits to be named the official Champion of Queen Elizabeth I. Clifford became extremely wealthy through his buccaneering, but lost most of his money gambling on horse races.
Captain Christopher Newport led more attacks on Spanish shipping and settlements than any other English privateer. As a young man, Newport sailed with Sir Francis Drake in the attack on the Spanish fleet at Cadiz and participated in England’s defeat of the Spanish Armada. During the war with Spain, Newport seized fortunes of Spanish and Portuguese treasure in fierce sea battles in the West Indies as a privateer for Queen Elizabeth I. In 1592, Newport captured the Portuguese carrack "Madre de Deus" (Mother of God), valued at £500,000.
Sir Henry Morgan was a successful privateer. Operating out of Jamaica, he carried on a war against Spanish interests in the region, often using cunning tactics. His operation was prone to cruelty against those he captured, including torture to gain information about booty, and in one case using priests as human shields. Despite reproaches for some of his excesses, he was generally protected by Sir Thomas Modyford, the governor of Jamaica. He took an enormous amount of booty, as well as landing his privateers ashore and attacking land fortifications, including the sack of the city of Panama with only 1,400 crew.
Other British privateers of note include Fortunatus Wright, Edward Collier, Sir John Hawkins, his son Sir Richard Hawkins, Michael Geare, and Sir Christopher Myngs. Notable British colonial privateers in Nova Scotia include Alexander Godfrey of the brig "Rover" and Joseph Barss of the schooner "Liverpool Packet". The latter schooner captured over 50 American vessels during the War of 1812.
Spain and its colonies.
When Spain issued a decree blocking foreign countries from trading, selling or buying merchandise in its Caribbean colonies, the entire region became engulfed in a power struggle among the naval superpowers. The newly independent United States later became involved in this scenario, complicating the conflict. As a consequence, Spain increased the issuing of privateering contracts. These contracts allowed an income option to the inhabitants of these colonies that were not related to the Spanish conquistadores. The most notable example is Miguel Enríquez, a Puerto Rican mulatto who abandoned his work as a shoemaker to work as a privateer. Such was the success of Enríquez, that he became one of the wealthiest men in the New World. His fleet was composed of approx. 300 different ships during a career that expanded 35 years, becoming a military asset and reportedly outperforming the efficiency of the Armada de Barlovento. Enríquez was knighted and received the title of Don from Philip V, something unheard of due to his ethnic and social background. One of the most famous privateers from mainland Spain was Amaro Pargo.
Bermudians.
The English colony of Bermuda (or the "Somers Isles"), settled accidentally in 1609, was used as a base for English privateers from the time it officially became part of the territory of the Virginia Company in 1612. Many Bermudians were employed as crew aboard privateers throughout the century, although the colony was primarily devoted to farming cash crops 'til turning from its failed agricultural economy to the sea after the 1684 dissolution of the Somers Isles Company (a spin-off of the Virginia Company which had overseen the colony since 1615). With a total area of 21 sqmi and lacking any natural resources other than the Bermuda cedar, the colonists applied themselves fully to the maritime trades, developing the speedy Bermuda sloop, which was well suited both to commerce and to commerce raiding. Bermudian merchant vessels turned to privateering at every opportunity in the 18th century, preying on the shipping of Spain, France, and other nations during a series of wars, including: the 1688 to 1697 Nine Years' War (King William's War); the 1702 to 1713 Queen Anne's War; the 1739 to 1748 War of Jenkins' Ear; the 1740 to 1748 War of the Austrian Succession (King George's War); the 1754 to 1763 Seven Years' War (known in North America as the French and Indian War, this conflict was devastating for the colony's merchant fleet. Fifteen privateers operated from Bermuda during the war, but losses exceeded captures); the 1775 to 1783 American War of Independence; and the 1796 to 1808 Anglo-Spanish War. By the middle of the Eighteenth Century, Bermuda was sending twice as many privateers to sea as any of the continental colonies. They typically left Bermuda with very large crews. This advantage in manpower was vital in overpowering the crews of larger vessels, which themselves often lacked sufficient crewmembers to put up a strong defence. The extra crewmen were also useful as prize crews for returning captured vessels.
The Bahamas, which had been depopulated of its indigenous inhabitants by the Spanish, had been settled by England, beginning with the Eleutheran Adventurers, dissident Puritans driven out of Bermuda during the English Civil War. Spanish and French attacks destroyed New Providence in 1703, creating a stronghold for pirates, and it became a thorn in the side of British merchant trade through the area. In 1718, Britain appointed Woodes Rogers as Governor of the Bahamas, and sent him at the head of a force to reclaim the settlement. Before his arrival, however, the pirates had been forced to surrender by a force of Bermudian privateers who had been issued letters of marque by the Governor of Bermuda.
Bermuda was in de facto control of the Turks Islands, with their lucrative salt industry, from the late 17th century to the early 19th. The Bahamas made perpetual attempts to claim the Turks for itself. On several occasions, this involved seizing the vessels of Bermudian salt traders. A virtual state of war was said to exist between Bermudian and Bahamian vessels for much of the 18th Century. When the Bermudian sloop "Seaflower" was seized by the Bahamians in 1701, the response of the Governor of Bermuda Governor, Captain Benjamin Bennett, was to issue letters of marque to Bermudian vessels. In 1706, Spanish and French forces ousted the Bermudians, but were driven out themselves three years later by the Bermudian privateer Captain Lewis Middleton. His ship, the "Rose", attacked a Spanish and a French privateer holding a captive English vessel. Defeating the two enemy vessels, the "Rose" then cleared out the thirty-man garrison left by the Spanish and French.
Despite strong sentiments in support of the rebels, especially in the early stages, Bermudian privateers turned as aggressively on American shipping during the American War of Independence. The importance of privateering to the Bermudian economy had been increased not only by the loss of most of Bermuda's continental trade, but also by the Palliser Act, which forbade Bermudian vessels from fishing the Grand Banks. Bermudian trade with the rebellious American colonies actually carried on throughout the war. Some historians credit the large number of Bermuda sloops (reckoned at over a thousand) built in Bermuda as privateers and sold illegally to the Americans as enabling the rebellious colonies to win their independence. Also, the Americans were dependent on Turks salt, and one hundred barrels of gunpowder were stolen from a Bermudian magazine and supplied to the rebels at the request of George Washington, in exchange for which the Continental Congress authorised the sale of supplies to Bermuda, which was dependent on American produce. The realities of this interdependence did nothing to dampen the enthusiasm with which Bermudian privateers turned on their erstwhile countrymen.
An American naval captain, ordered to take his ship out of Boston Harbor to eliminate a pair of Bermudian privateering vessels that had been picking off vessels missed by the Royal Navy, returned frustrated, saying, "the Bermudians sailed their ships two feet for every one of ours". Around 10,000 Bermudians emigrated in the years prior to American independence, mostly to the American colonies. Many Bermudians occupied prominent positions in American seaports, from where they continued their maritime trades (Bermudian merchants controlled much of the trade through ports like Charleston, South Carolina, and Bermudian shipbuilders influenced the development of American vessels, like the Chesapeake Bay schooner), and in the Revolution they used their knowledge of Bermudians and of Bermuda, as well as their vessels, for the rebels' cause. In the 1777 Battle of Wreck Hill, brothers Charles and Francis Morgan, members of a large Bermudian enclave that had dominated Charleston, South Carolina and its environs since settlement, captaining two sloops (the "Fair American" and the "Expirement", respectively), carried out the only attack on Bermuda during the war. The target was a fort that guarded a little used passage through the encompassing reefline. After the soldiers manning the fort were forced to abandon it, they spiked its guns and fled themselves before reinforcements could arrive.
When the Americans captured the Bermudian privateer "Regulator", they discovered that virtually all of her crew were black slaves. Authorities in Boston offered these men their freedom, but all 70 elected to be treated as prisoners of war. Sent as such to New York on the sloop "Duxbury", they seized the vessel and sailed it back to Bermuda.
The War of 1812 saw an encore of Bermudian privateering, which had died out after the 1790s. The decline of Bermudian privateering was due partly to the buildup of the naval base in Bermuda, which reduced the Admiralty's reliance on privateers in the western Atlantic, and partly to successful American legal suits and claims for damages pressed against British privateers, a large portion of which were aimed squarely at the Bermudians. During the course of the War of 1812, Bermudian privateers captured 298 ships, some 19% of the 1,593 vessels captured by British naval and privateering vessels between the Great Lakes and the West Indies.
Amongst the better known (native-born and immigrant) Bermudian privateers were Hezekiah Frith, Bridger Goodrich, Henry Jennings, Thomas Hewetson, and Thomas Tew.
Providence Island colony.
Bermudians were also involved in privateering from the short-lived English colony on Isla de Providencia, off the coast of Nicaragua. This colony was initially settled largely via Bermuda, with about eighty Bermudians moved to Providence in 1631. Although it was intended that the colony be used to grow cash crops, its location in the heart of the Spanish controlled territory ensured that it quickly became a base for privateeering. 
Bermuda-based privateer Daniel Elfrith, while on a privateering expedition with Captain Sussex Camock of the bark "Somer Ilands" in 1625, discovered two islands off the coast of Nicaragua, 50 miles apart from each other. Camock stayed with 30 of his men to explore one of the islands, San Andrés, while Elfrith took the Warwicke back to Bermuda bringing news of Providence Island. Bermuda Governor Bell wrote on behalf of Elfrith to Sir Nathaniel Rich, a businessman and cousin of the Earl of Warwick, who presented a proposal for colonizing the island noting its strategic location "lying in the heart of the Indies & the mouth of the Spaniards". Elfrith was appointed admiral of the colony's military forces in 1631, remaining the overall military commander for over seven years. During this time, Elfrith served as a guide to other privateers and sea captains arriving in the Caribbean. Elfrith invited the well-known privateer Diego el Mulato to the island. Samuel Axe, one of the military leaders, also accepted letters of marque from the Dutch authorizing privateering.
The Spanish did not hear of the Providence Island colony until 1635, when they captured some Englishmen in Portobelo, on the Isthmus of Panama. Francisco de Murga, Governor and Captain General of Cartagena, dispatched Captain Gregorio de Castellar y Mantilla and engineer Juan de Somovilla Texada to destroy the colony. The Spanish were repelled and forced to retreat "in haste and disorder". After the attack, King Charles I of England issued letters of marque to the Providence Island Company on 21 December 1635 authorizing raids on the Spanish in retaliation for a raid that had destroyed the English colony on Tortuga earlier in 1635 (Tortuga had come under the protection of the Providence Island Company. In 1635 a Spanish fleet raided Tortuga. 195 colonists were hung and 39 prisoners and 30 slaves were captured). The company could in turn issue letters of marque to subcontracting privateers who used the island as a base, for a fee. This soon became an important source of profit. Thus the Company made an agreement with the merchant Maurice Thompson under which Thompson could use the island as a base in return for 20% of the booty.
In March 1636 the Company dispatched Captain Robert Hunt on the "Blessing" to assume the governorship of what was now viewed as a base for privateering. Depredations continued, leading to growing tension between England and Spain, which were still technically at peace.
On 11 July 1640 the Spanish Ambassador in London complained again, saying he
understands that there is lately brought in at the Isle of Wight by one, Captain James Reskinner [James Reiskimmer], a ship very richly laden with silver, gold, diamonds, pearls, jewels, and many other precious commodities taken by him in virtue of a commission of the said Earl [of Warwick] from the subjects of his Catholic Majesty ... to the infinite wrong and dishonour of his Catholic Majesty, to find himself thus injured and violated, and his subjects thus spoiled, robbed, impoverished and murdered in the highest time of peace, league and amity with your Majesty.
Nathaniel Butler, formerly Governor of Bermuda, was the last full governor of Providence Island, replacing Robert Hunt in 1638. Butler returned to England in 1640, satisfied that the fortifications were adequate, deputizing the governorship to Captain Andrew Carter.
In 1640, don Melchor de Aguilera, Governor and Captain-General of Cartagena, resolved to remove the intolerable infestation of pirates on the island. Taking advantage of having infantry from Castile and Portugal wintering in his port, he dispatched six hundred armed Spaniards from the fleet and the presidio, and two hundred black and mulatto militiamen under the leadership of don Antonio Maldonado y Tejada, his Sergeant Major, in six small frigates and a galleon. The troops were landed on the island, and a fierce fight ensued. The Spanish were forced to withdraw when a gale blew up and threatened their ships. Carter had the Spanish prisoners executed. When the Puritan leaders protested against this brutality, Carter sent four of them home in chains.
The Spanish acted decisively to avenge their defeat. General Francisco Díaz Pimienta was given orders by King Philip IV of Spain, and sailed from Cartagena to Providence with seven large ships, four pinnaces, 1,400 soldiers and 600 seamen, arriving on 19 May 1641. At first Pimienta planned to attack the poorly defended east side, and the English rushed there to improvise defenses. With the winds against him, Pimienta changed plans and made for the main New Westminster harbor and launched his attack on 24 May. He held back his large ships to avoid damage, and used the pinnaces to attack the forts. The Spanish troops quickly gained control, and once the forts saw the Spanish flag flying over the governor's house, they began negotiations for surrender. 
On 25 May 1641, Pimienta formally took possession and celebrated mass in the church.
The Spanish took sixty guns, and captured the 350 settlers who remained on the island – others had escaped to the Mosquito coast. They took the prisoners to Cartagena.
The women and children were given a passage back to England.
The Spanish found gold, indigo, cochineal and six hundred black slaves on the island, worth a total of 500,000 ducats, some of the accumulated booty from the English raids.
Rather than destroy the defenses, as instructed, Pimienta left a small garrison of 150 men to hold the island and prevent occupation by the Dutch. 
Later that year, Captain John Humphrey, who had been chosen to succeed Captain Butler as governor, arrived with a large group of dissatisfied settlers from New England. He found the Spanish in occupation.
Pimienta's decision to occupy the island was approved in 1643 and he was made a knight of the Order of Santiago.
France.
"Corsairs" (French: corsaire) were privateers, authorized to conduct raids on shipping of a nation at war with France, on behalf of the French Crown. Seized vessels and cargo were sold at auction, with the corsair captain entitled to a portion of the proceeds. Although not French Navy personnel, corsairs were considered legitimate combatants in France (and allied nations), provided the commanding officer of the vessel was in possession of a valid Letter of Marque (fr. "Lettre de Marque" or "Lettre de Course"), and the officers and crew conducted themselves according to contemporary admiralty law. By acting on behalf of the French Crown, if captured by the enemy, they could claim treatment as prisoners of war, instead of being considered pirates. Because corsairs gained a swashbuckling reputation, the word "corsair" is also used generically as a more romantic or flamboyant way of referring to privateers, or even to pirates. The Barbary pirates of North Africa as well as Ottomans were sometimes called "Turkish corsairs".
United States.
During the American Revolutionary War, the Continental Congress, and some state governments (on their own initiative), issued privateering licenses, authorizing "legal piracy", to merchant captains in an effort to take prizes from the British Navy and Tory (Loyalist) privateers. This was done due to the relatively small number of commissioned American naval vessels and the pressing need for prisoner exchange.
About 55,000 American seamen served aboard the privateers. They quickly sold their prizes, dividing their profits with the financier (persons or company) and the state (colony). Long Island Sound became a hornets' nest of privateering activity during the American Revolution (1775–1783), as most transports to and from New York went through the Sound. New London, Connecticut was a chief privateering port for the American colonies, leading to the British Navy blockading it in 1778-1779. Chief financiers of privateering included Thomas & Nathaniel Shaw of New London and John McCurdy of Lyme. In the months before the British raid on New London and Groton, a New London privateer took "Hannah" in what is regarded as the largest prize taken by any American privateer during the war. Retribution was likely part of Gov. Clinton's (NY) motivation for Arnold's Raid, as the "Hannah" had carried many of his most cherished items.
American privateers are thought to have seized up to 300 British ships during the war. One of the more successful of these ships was the Prince de Neufchatel, which once captured nine British prizes in swift succession in the English Channel. The British ship "Jack" was captured and turned into an American privateer, only to be captured again by the British in the naval battle off Halifax, Nova Scotia. American privateers not only fought naval battles but also raided numerous communities in British colonies, such as the Raid on Lunenburg, Nova Scotia (1782).
The United States Constitution authorized the U.S. Congress to grant letters of marque and reprisal. Between the end of the Revolutionary War and 1812, less than 30 years, Britain, France, Naples, the Barbary States, Spain, and the Netherlands seized approximately 2,500 American ships. Payments in ransom and tribute to the Barbary states amounted to 20% of United States government annual revenues in 1800 and would lead the United States to fight the Barbary states in the First Barbary War and Second Barbary Wars.
During the War of 1812, the British attacked Essex, Connecticut, and burned the ships in the harbor, due to the construction there of a number of privateers. This was the greatest financial loss of the entire War of 1812 suffered by the Americans. However, the private fleet of James De Wolf, which sailed under the flag of the American government in 1812, was most likely a key factor in the naval campaign of the war. De Wolf's ship, the "Yankee", was possibly the most financially successful ship of the war. Privateers proved to be far more successful than their US Navy counterparts, claiming three quarters of the 1600 British merchant ships taken during the war (although a third of these were recaptured prior to making landfall). Whilst apparently successful the privateer campaign was ultimately a failure. British convoy systems honed during the Napoleonic Wars limited losses to singleton ships, and the effective blockade of American and continental ports prevented captured ships being taken in for sale. This ultimately led to orders forbidding US privateers from attempting to bring their prizes in to port, with captured ships instead having to be burnt. Over 200 American privateer ships were captured by the Royal Navy, many of which were turned on their former owners and used by the British blockading forces.
Jean Lafitte and his privateers aided US General Andrew Jackson in the defeat of the British in the Battle of New Orleans in order to receive full pardons for their previous crimes. Jackson formally requested clemency for Lafitte and the men who had served under him, and the US government granted them all a full pardon on February 6, 1815.
The US was not one of the initial signatories of the 1856 Declaration of Paris which outlawed privateering, and the Confederate Constitution authorized use of privateers. However, the USA did offer to adopt the terms of the Declaration during the American Civil War, when the Confederates sent several privateers to sea before putting their main effort in the more effective commissioned raiders.
During the Civil War Confederate President Jefferson Davis issued letters of marque to anyone who would employ their ship to either attack Union shipping or bring badly needed supplies through the Union blockade into southern ports. Many of the supplies brought into the Confederacy were carried aboard privately owned vessels. When word came about that the Confederacy was willing to pay almost any price for military supplies various interested parties designed and built specially designed light weight seagoing steamers, blockade runners specifically designed and built to outrun Union ships on blockade patrol.
No letter of marque has been legitimately issued by the United States since the nineteenth century. The status of submarine hunting Goodyear airships in the early days of the second world war has created significant confusion. According to one story, the United States Navy issued a Letter of Marque to the Airship "Resolute" on the West Coast of the United States at the beginning of World War II, making it the first time the US Navy commissioned a privateer since the War of 1812. However, this story, along with various other accounts referring to the airships "Resolute" and "Volunteer" as operating under a "privateer status", is highly dubious. Since neither the Congress nor the President appears to have authorized a privateer during the war, the Navy would not have had the authority to do so by itself.
Latin America.
Warships were recruited by the insurgent governments during Spanish American wars of independence to destroy Spanish trade, and capture Spanish Merchant vessels. The private armed vessels came largely from the United States. Seamen from Britain, United States and France often manned these ships.

</doc>
<doc id="50768" url="http://en.wikipedia.org/wiki?curid=50768" title="People mover">
People mover

A people mover or automated people mover (APM) is a type of grade-separated mass transit system. The term is generally used only to describe systems serving relatively small areas such as airports, downtown districts or theme parks, but is sometimes applied to considerably more complex automated systems.
The term was originally applied to three different systems, developed roughly at the same time. One was Skybus, an automated mass transit system prototyped by the Westinghouse Electric Corporation beginning in 1964. The second, alternately called the People Mover and Minirail, opened in Montreal at Expo 67. Finally the last, called PeopleMover or Goodyear PeopleMover, was an attraction sponsored by the Goodyear Tire and Rubber Company which opened at Disneyland in 1967. Now, however, the term "people mover" is generic, and may use technologies such as monorail, duorail, automated guideway transit or maglev. Propulsion may involve conventional on-board electric motors, linear motors or cable traction.
Generally speaking, larger APMs are referred to by other names. The most generic is "automated guideway transit", which encompasses any automated system regardless of size. Some complex APMs deploy fleets of small vehicles over a track network with off-line stations, and supply near non-stop service to passengers. These taxi-like systems are more usually referred to as personal rapid transit (PRT). Larger systems, with vehicles with 20 to 40 passengers, are sometimes referred to as "group rapid transit" (GRT), although this term is not particularly common. Other complex APMs have similar characteristics to mass transit systems, and there is no clear cut distinction between a complex APM of this type and an automated mass transit system. Another term "Light Metro" is also applied to describe the system worldwide.
History.
Never-Stop Railway.
One of the first automated systems for human transportation was the screw-driven 'Never-Stop-Railway', constructed for the British Empire Exhibition at Wembley, London in 1924. This railway consisted of 88 unmanned carriages, on a continuous double track along the northern and eastern sides of the exhibition, with reversing loops at either end.
The carriages ran on two parallel concrete beams and were guided by pulleys running on the inner side of these concrete beams, and were propelled by gripping a revolving screw thread running between the tracks in a pit; by adjusting the pitch of this thread at different points, the carriages could be sped up, or slowed down to a walking pace at stations, to allow passengers to join and leave. The railway ran reliably for the two years of the exhibition, and was then dismantled.
Small sections of this track bed, and a nearby heavy rail track bed, have been proposed for reuse.
Goodyear and Stephens-Adamson.
In late 1949, Mike Kendall, chief engineer and Chairman of the Board of Stephens-Adamson Manufacturing Company, an Illinois-based manufacturer of conveyor belts and systems, asked Al Neilson, an engineer in the Industrial Products Division of Goodyear Tire and Rubber Co., if Goodyear had ever considered working on People Movers. He felt that with Goodyear's ability to move materials in large quantities on conveyor belts they should consider moving batches of people.
Four years of engineering design, development and testing led to a joint patent being issued for three types of people movers, named Speedwalk, Speedramp, and Carveyor.
Goodyear would sell the concept and Stephens-Adamson would manufacture and install the components.
A Speedwalk consisted of a flat conveyor belt riding on a series of rollers, or a flat slippery surface, moving at 1.5 mi/h (approximately half the speed of walking). The passengers would walk onto the belt and could stand or walk to the exit point. They were supported by a moving handrail. Customers were expected to include airport terminals, ballparks, train stations, etc. Today, several manufacturers produce similar units called moving walkways.
A Speedramp was very similar to a Speedwalk but it was used to change elevations; up or down a floor level. This could have been accomplished by an escalator, but the Speedramp would allow wheeled luggage, small handcarts etc. to ride the belt at an operating cost predicted to be much lower than escalators or elevators. The first successful installation of a Speedramp was in the spring of 1954 at the Hudson and Manhattan Railroad Station in Jersey City, New Jersey to connect the Erie Railroad to the Hudson and Manhattan Tubes. This unit was 227 ft long with a rise of 22 ft on a 15 degree grade, and only cost $75,000.
A Carveyor consisted of many small cubicles or cars carrying ten people riding on a flat conveyor belt from point A to point B. The belt rode on a series of motorized rollers. The purpose of the motorized rollers was to facilitate the gradual acceleration and deceleration speeds on the conveyor belt and overcome the tendency of all belts to stretch at start up and during shutdown. At point "A" passengers would enter a Speedwalk running parallel to the belts and cars of the Carveyor. The cars would be moving at the same speed as the Speedwalk; the passengers would enter the cars and be seated, while the motorized rollers would increase the speed of the cars up to the traveling speed (which would be preset depending on the distance to be covered). At point B Passengers could disembark and by means of a series of flat slower belts (Speedwalks) go to other Carveyors to other destinations or out to the street. The cars at point B would continue on rollers around a semicircle and then reverse the process carrying passengers back to point A. The initial installation was to be the 42nd Street Shuttle in New York City between Times Square and Grand Central station.
The first mention of the Carveyor in a hardback book was in "There's Adventure in Civil Engineering" by Neil P. Ruzic (1958), one of a series of books published by "Popular Mechanics" in the 1950s in their "Career" series. In the book the Carveyor was already installed and operational in downtown Los Angeles.
Colonel Sydney H. Bingham, Chairman of the New York City Board of Transportation, had several meetings with a group of architects who were trying to revamp the whole New York City Subway system in the heart of town to connect Pennsylvania Station, Madison Square Garden, Times Square, Grand Central and several new office complexes together. Several of these architects were involved in other programs, and in later years many variations of the Carveyor people movers were developed.
In November 1954 the New York City Transit Authority issued an order to Goodyear and Stephens-Adamson to build a complete Carveyor system between Times Square and Grand Central. A brief summary and confirmation can be found in "Time" magazine on November 15, 1954. under the heading "Subway of the Future". The cost was to be under $4 million, but the order was never fulfilled due to political difficulties.
Chocolate World in Hershey, Pennsylvania, Disneyland in California, and Disney World in Florida are among many locations that have used variations of the Carveyor concept.
Other developments.
The term 'people mover' was used by Walt Disney, when he and his Imagineers were working on the new 1967 Tomorrowland at Disneyland. The name was used as a working title for a new attraction, the PeopleMover. According to Imagineer Bob Gurr, "the name got stuck," and it was no longer a working title.
Starting in the late 1960s and into the 1970s, people movers were the topic of intense development around the world. Worried about the growing congestion and pollution in downtown areas due to the spread of cars, many countries started studying mass transit systems that would lower capital costs to the point where any city could afford to deploy them. Most of these systems used elevated guideways, which were much less expensive to deploy than underground tunnels. However, elevating the track causes problems with noise, so traditional steel-wheel-on-rail solutions were rare as they squealed when rounding bends in the rails. Rubber tired solutions were common, but some systems used hovercraft techniques or various magnetic levitation systems.
Two major government funded APM projects are notable. In Germany, Mannesmann Demag and Messerschmitt-Bölkow-Blohm developed a system known as Cabinentaxi during the 1970s. Cabinetaxi featured small cars with from four to eight seats that were called to pick up passengers on-demand and drove directly to their destination. The stations were "offline", allowing the cabs to stop by moving off the main lines while other cars continued to their destinations. The system was designed so the cars could be adapted to run on top or bottom of the track (but not easily converted from one to the other), allowing dual-track movements from a single elevated guideway only slightly wider than the cars. A test track was completed in 1975 and ran until development was completed in 1979, but no deployments followed and the companies abandoned the system shortly thereafter.
In the U.S., a 1966 federal bill provided funding that led to the development of APM systems under the Downtown People Mover Program. Four systems were developed, Rohr's ROMAG, LTV's AirTrans, Ford's APT and Otis Elevator's hovercraft design. A major presentation of the systems was organized as TRANSPO'72 at Dulles Airport where the various systems were presented to delegations from numerous cities in the US. Prototype systems and test tracks were built during the 1970s. One notable example was Pittsburgh's Skybus, which was proposed by the Port Authority of Allegheny County to replace its streetcar system, which, having large stretches of private right of way, was not suited for bus conversion. A short demonstration line was set up in South Park and large tracts of land were secured for its facilities. However, opposition arose to the notion that it would replace the streetcar system. This, combined with the immaturity of the technology and other factors, led the Port Authority to abandon the project and pursue alternatives. By the start of the 1980s most politicians had lost interest in the concept and the project was repeatedly de-funded in the early 1980s. Only two APMs were developed as a part of the People Mover Program in the US, the Metromover in Miami, and the Detroit People Mover.
From development to implementation.
Although many government-funded systems were generally considered failures, several APM systems developed by other groups have been much more successful. The first driverless public transport rail networks are the Port Island Line deployed in Kobe, Japan (1981) and the French Véhicule Automatique Léger or VAL, first deployed in Lille in 1983. The Vancouver SkyTrain system is the largest fully automated metro system in the world, with over 60 km of track. These systems are on the "heavy" end of the APM scale. Lighter systems with shorter tracks are widely deployed at airports; the world's first airport people mover was installed in 1971 at Tampa International Airport in the United States. APMs have now become common at large airports and hospitals in the United States.
Driver-less metros have become common in Europe and parts of Asia. The economics of automated trains tend to reduce the scale so tied to "mass" transit (the largest operating expense is the driver's salary, which is only affordable if very large numbers of passengers are paying fares), so that small-scale installations are feasible. Thus cities normally thought of as too small to build a metro (e.g. Rennes, Lausanne, Brescia, etc.) are now doing so.
On September 30, 2006, the Peachliner in Komaki, Aichi Prefecture, Japan became that nation's first people mover to cease operations.
Examples.
Urban transit.
Republic of China (Taiwan).
Taipei: Wenhu Line
Airport.
Many large international airports around the world feature people mover systems to transport passengers between terminals or within a terminal itself. Some people mover systems at airports connect with other public transportation systems to allow passengers to travel into the airport's city.
Rural and suburban transit.
The Pillar-track Regional People Mover is a concept of a people mover system, that is optimized for rural and suburban areas with lower population density. The aim of the system is to provide short intervals and appropriate riding and waiting times (including changing between lines) outside of urban areas.

</doc>
<doc id="50771" url="http://en.wikipedia.org/wiki?curid=50771" title="Ferry">
Ferry

A ferry (or ferryboat) is a boat or ship (a merchant vessel) used to carry (or "ferry") primarily passengers, and sometimes vehicles and cargo as well, across a body of water. Most ferries operate on regular, frequent, return services. A passenger ferry with many stops, such as in Venice, Italy, is sometimes called a water bus or water taxi.
Ferries form a part of the public transport systems of many waterside cities and islands, allowing direct transit between points at a capital cost much lower than bridges or tunnels. However, ship connections of much larger distances (such as over long distances in water bodies like the Mediterranean Sea) may also be called ferry services, especially if they carry vehicles.
History.
In ancient times.
The profession of the ferryman is embodied in Greek mythology in Charon, the boatman who transported souls across the River Styx to the Underworld.
Speculation that a pair of oxen propelled a ship having a water wheel can be found in 4th century Roman literature ""Anonymus De Rebus Bellicis". Though impractical, there is no reason why it could not work and such a ferry, modified by using horses, was used in Lake Champlain in 19th-century America. See "When Horses Walked on Water: Horse-Powered Ferries in Nineteenth-Century America"" (Smithsonian Institution Press; Kevin Crisman, co-authored with Arthur Cohn, Executive Director of the Lake Champlain Maritime Museum). "See" Experiment (horse powered boat).
Notable services.
Africa.
The Marine Services Company of Tanzania offers passenger and cargo services in three of the African Great Lakes viz. Lake Victoria, Lake Tanganyika and Lake Nyasa. It also operates one of the oldest ferries in the region, the MV "Liemba" which was built in 1913 during the German colonial rule.
Europe.
The busiest seaway in the world, the English Channel, connects Great Britain and mainland Europe, sailing mainly to French ports, such as Calais, Boulogne, Dunkirk, Dover, Dieppe, Roscoff, Cherbourg-Octeville, Caen, St Malo and Le Havre. Ferries from Great Britain also sail to Belgium, Denmark, the Netherlands, Norway, Spain and Ireland. Some ferries carry mainly tourist traffic, but most also carry freight, and some are exclusively for the use of freight lorries. In Britain, car-carrying ferries are sometimes referred to as RORO (roll-on, roll-off) for the ease by which vehicles can board and leave.
The busiest single ferry route (at least in terms of the number of departures) is across the northern part of Øresund, between Helsingborg, Scania, Sweden and Elsinore, Denmark. Before the Øresund bridge was opened in July 2000, car and "car & train" ferries departed up to seven times every hour. In 2013, this has been reduced, but a car ferry still departs from each harbor every 15 minutes during daytime. The route is around 2.2 nmi and the crossing takes 22 minutes. Today, all ferries on this route are constructed so that they do not need to turn around in the harbors. This also means that the ferries lack "natural" stems and sterns, since the vessels sail in both directions (rather than "sail backwards"). Due to the same circumstances, starboard and port-side are "dynamic" and depending of in what direction the ferry sails. Despite the short crossing, the ferries are equipped with restaurants (on 3 out of 4 ferries), cafeteria, kiosks and WC toilets. (Passengers without cars often make a "double or triple return" journey in the restaurants; for this, a common return ticket is sufficient. Passenger and bicycle passenger tickets are inexpensive compared with longer routes.)
Large cruiseferries sail in the Baltic Sea between Finland, Åland, Sweden, Estonia, Latvia and Saint Petersburg, Russia and from Italy to Sardinia, Corsica, Spain and Greece. In many ways, these ferries are like cruise ships, but they can also carry hundreds of cars on car decks. Besides providing passenger and car transport across the sea, Baltic Sea cruiseferries are a popular tourist destination unto themselves, with multiple restaurants, nightclubs, bars, shops and entertainment on board.
The south-west and southern parts of the Baltic Sea has several routes mainly for heavy traffic and cars. The ferry routes of Trelleborg-Rostock SWE-GER, Trelleborg-Travemünde SWE-GER, Trelleborg-Świnoujście SWE-POL, Gedser-Rostock DEN-GER, Gdynia-Karlskrona POL-SWE, and Ystad-Świnoujście SWE-POL are all typical "transports" ferries. On the longer of these routes, simple cabins are available. The Rødby-Puttgarden DEN-GER route also carries day passenger trains between Copenhagen and Hamburg, and on the Trelleborg-Sassnitz SWE-GER route, it also has capacities for the daily night trains between Berlin and Malmö.
In Istanbul, ferries connect the European and Asian shores of Bosphorus, as well as Princes Islands and nearby coastal towns.
North America.
Due to the numbers of large freshwater lakes and length of shoreline in Canada, many provinces and territories have ferry services. BC Ferries carries travellers between Vancouver Island and the British Columbia mainland on the country's west coast. This ferry service operates to other islands including the Gulf Islands and the Queen Charlotte Islands. Canada's east coast has been home to numerous inter- and intra-provincial ferry and coastal services, including a large network operated by the federal government under CN Marine and later Marine Atlantic. Private and publicly owned ferry operations in eastern Canada include Marine Atlantic, serving the island of Newfoundland, as well as Bay, NFL, CTMA, Coastal Transport, and STQ to name but a few. Canadian waters in the Great Lakes once hosted numerous ferry services, however these have been reduced to those offered by Owen Sound Transportation and several smaller operations. There are also several commuter passenger ferry services operated in major cities, such as Metro Transit in Halifax, Toronto Island Ferry in Toronto and SeaBus in Vancouver.
Washington State Ferries operates the most extensive ferry system in the United States, with ten routes on Puget Sound and the Strait of Juan de Fuca serving terminals in Washington and Vancouver Island. In 2012, Washington State Ferries carried 10 million vehicles and 22 million passengers.
The Staten Island Ferry in New York City, sailing between the boroughs of Manhattan and Staten Island, is the nation's single busiest ferry route by passenger volume. New York City also has a network of smaller ferries, or "water taxis", that shuttle commuters along the Hudson River from locations in New Jersey and Northern Manhattan down to the midtown, downtown and Wall Street business centers. Several ferry companies also offer service linking midtown and lower Manhattan with locations in the boroughs of Queens and Brooklyn, crossing the city's East River.
The New Orleans area also has many ferries in operation that carry both vehicles and pedestrians. Most notable is the Algiers Ferry. This service has been in continuous operation since 1827 and is one of the oldest operating ferries in North America.
Vehicle-carrying ferry services between mainland Cape Cod and the islands of Martha's Vineyard and Nantucket are operated by The Woods Hole, Martha's Vineyard and Nantucket Steamship Authority, which sails year-round between Woods Hole and Vineyard Haven as well as Hyannis and Nantucket. Seasonal service is also operated from Woods Hole to Oak Bluffs from Memorial Day to Labor Day. As there are no bridges or tunnels connecting the islands to the mainland, The Steamship Authority ferries in addition to being the only method for transporting private cars to or from the islands, also serves as the only link by which heavy freight and supplies such as food and gasoline can be trucked to the islands. Additionally, Hy-Line Cruises operates high speed catamaran service from Hyannis to both islands, as well as traditional ferries, and several smaller operations run seasonal passenger only service primarily geared towards tourist day-trippers from other mainland ports, including New Bedford, (New Bedford Fast Ferry) Falmouth, (Island Queen ferry and Falmouth Ferry) and Harwich (Freedom Cruise Line).
The San Francisco Bay Area has several ferry services, such as the Blue & Gold Fleet, connecting with cities as far as Vallejo. The majority of ferry passengers are daily commuters and tourists. A ferry serves Angel Island (which also accepts private craft). The "only" way to get to Alcatraz is by ferry.
Until the completion of the Mackinac Bridge in the 1950s, ferries were used for vehicle transportation between the Lower Peninsula of Michigan and the Upper Peninsula of Michigan, across the Straits of Mackinac in the United States. Ferry service for bicycles and passengers continues across the straits for transport to Mackinac Island, where motorized vehicles are almost completely prohibited. This crossing is made possible by three ferry lines, Arnold Transit Company, Shepler's Ferry, and Star Line Ferry.
Oceania.
In Australia, two Spirit of Tasmania ferries carry passengers and vehicles 300 kilometres across Bass Strait, the body of water that separates Tasmania from the Australian mainland, often under famously turbulent sea conditions. These run overnight but also include day crossings in peak time. Both ferries are based in the northern Tasmanian port city of Devonport and sail to Melbourne.
In New Zealand, ferries connect Wellington in the North Island with Picton in the South Island, linking New Zealand's two main islands. The 92 km route takes three hours, and is run by two companies – government-owned Interislander, and independent Bluebridge.
Asia.
Hong Kong has the Star Ferry carry passengers across Victoria Harbour and various carriers carrying travellers between Hong Kong Island to outlying islands like Cheung Chau, Lantau Island and Lamma Island.
Water transport in Mumbai consists of ferries, hovercrafts, and catamarans, operated by various government agencies as well as private entities. The Kerala State Water Transport Department (SWTD), operating under the Ministry of Transport, Government of Kerala, India regulates the inland navigation systems in the Indian state of Kerala and provides inland water transport facilities. It stands for catering to the passenger and cargo traffic needs of the inhabitants of the waterlogged areas of the Districts of Alappuzha, Kottayam, Kollam, Ernakulam, Kannur and Kasargode. SWTD ferry service is also one of the most affordable modes to enjoy the beauty of the scenic Kerala backwaters.
The Penang Ferry Service is the oldest ferry service in Malaysia. The famous ferry service connects Sultan Abdul Halim ferry terminal in Butterworth on Peninsular Malaysia to Raja Tun Uda ferry terminal at Weld Quay in George Town on Penang Island. It has also become a famous tourist attraction among foreigners. Along the way, ferry commuters will get to see the Penang Bridge and also the skyline of George Town and Butterworth.
Types.
Ferry designs depend on the length of the route, the passenger or vehicle capacity required, speed requirements and the water conditions the craft must deal with.
Double-ended.
Double-ended ferries have interchangeable bows and sterns, allowing them to shuttle back and forth between two terminals without having to turn around. Well-known double-ended ferry systems include the Staten Island Ferry, Washington State Ferries, Star Ferry, several boats on the North Carolina Ferry System, and the Lake Champlain Transportation Company. Most Norwegian fjord and coastal ferries are double-ended vessels. Some ferries in Sydney, Australia and British Columbia are also double-ended. In 2008, BC Ferries launched three of the largest double-ended ferries in the world.
Hydrofoil.
Hydrofoils have the advantage of higher cruising speeds, succeeding hovercraft on some English Channel routes where the ferries now compete against the Eurotunnel and Eurostar trains that use the Channel Tunnel. Passenger-only hydrofoils also proved a practical, fast and relatively economical solution in the Canary Islands but were recently replaced by faster catamaran "high speed" ferries that can carry cars. Their replacement by the larger craft is seen by critics as a retrograde step given that the new vessels use much more fuel and foster the inappropriate use of cars in islands already suffering from the impact of mass tourism.
Hovercraft.
Hovercraft were developed in the 1960s and 1970s to carry cars. The largest was the massive SR.N4 which carried cars in its centre section with ramps at the bow and stern between England and France. The hovercraft was superseded by catamarans which are nearly as fast and are less affected by sea and weather conditions. Only one service now remains, a foot passenger service between Portsmouth and the Isle of Wight run by Hovertravel.
Catamaran.
Catamarans are normally associated with high-speed ferry services. Stena Line operates the largest catamarans in the world, the Stena HSS class, between the United Kingdom and Ireland. These waterjet-powered vessels, displacing 19,638 tonnes, are larger than most catamarans and can accommodate 375 passenger cars and 1,500 passengers. Other examples of these super-sizer catamarans are found in the Brittany Ferries fleet with the Normandie Express and the Normandie Vitesse.
Roll-on/roll-off.
Roll-on/roll-off ferries (RORO) are large conventional ferries named for the ease by which vehicles can board and leave.
Cruiseferry.
A cruiseferry is a ship that combines the features of a cruise ship with a roll-on/roll-off ferry.
Fast RoPax ferry.
Fast RoPax ferries are conventional ferries with a large garage intake and a relatively large passenger capacity, with conventional diesel propulsion and propellers that sail over 25 kn. Pioneering this class of ferries was Attica Group, when it introduced Superfast I between Greece and Italy in 1995 through its subsidiary company Superfast Ferries. Cabins, if existent, are much smaller than those on cruise ships.
Turntable ferry.
This type of ferry allows vehicles to load from the "side". The vehicle platform can be turned. When loading, the platform is turned sideways to allow sideways loading of vehicles. Then the platform is turned back, in line with the vessel, and the journey across water is made.
Pontoon ferry.
Pontoon ferries carry vehicles across rivers and lakes and are widely used in less-developed countries with large rivers where the cost of bridge construction is prohibitive. One or more vehicles are carried on a pontoon with ramps at either end for vehicles to drive on and off. Cable ferries (next section) are usually pontoon ferries, but pontoon ferries on larger rivers are motorised and able to be steered independently like a boat.
Train ferry.
A Train ferry is a ship designed to carry railway vehicles. Typically, one level of the ship is fitted with railway tracks, and the vessel has a door at the front and/or rear to give access to the wharves.
Foot ferry.
Foot ferries are small craft used to ferry foot passengers, and often also cyclists, over rivers. These are either self-propelled craft or cable ferries. Such ferries are for example to be found on the lower River Scheldt in Belgium and in particular the Netherlands. Regular foot ferry service also exists in the capital of the Czech Republic, Prague, and across the Yarra River in Melbourne, Australia at Newport. Restored, expanded ferry service in the Port of New York and New Jersey uses boats for pedestrians only.
Cable ferry.
Very short distances may be crossed by a cable or chain ferry, which is usually a pontoon ferry (see above), where the ferry is propelled along and steered by cables connected to each shore. Sometimes the cable ferry is human powered by someone on the boat. Reaction ferries are cable ferries that use the perpendicular force of the current as a source of power. Examples of a current propelled ferry are the four Rhine ferries in Basel, Switzerland. Cable ferries may be used in fast-flowing rivers across short distances.
Free ferries operate in some parts of the world, such as at Woolwich in London, England (across the River Thames); in Amsterdam, Netherlands (across the IJ waterway); in New York Harbor, connecting Manhattan to Staten Island; along the Murray River in South Australia, and across many lakes in British Columbia. Many cable ferries operate on lakes and rivers in Canada, among them a cable ferry that charges a toll operates on the Rivière des Prairies between Laval-sur-le-Lac and Île Bizard in Quebec, Canada.
Air ferries.
In the 1950s and 1960s, travel on an "air ferry" was possible—aeroplanes, often ex-military, specially equipped to take a small number of cars in addition to "foot" passengers. These operated various routes including between the United Kingdom and Continental Europe. Companies operating such services included Channel Air Bridge, Silver City Airways, and Corsair.
The term is also applied to any "ferrying" by air, and is commonly used when referring to airborne military operations.
Docking.
Ferry boats often dock at specialized facilities designed to position the boat for loading and unloading, called a ferry slip. If the ferry transports road vehicles or railway carriages there will usually be an adjustable ramp called an apron that is part of the slip. In other cases, the apron ramp will be a part of the ferry itself, acting as a wave guard when elevated and lowered to meet a fixed ramp at the terminus — a road segment that extends partially underwater.
First, shortest, largest.
The world's largest car ferry is the MS "Ulysses", operated by Irish Ferries between Ireland and Wales.
On 11 October 1811, inventor John Stevens' ship the "Juliana", began operation as the first steam-powered ferry (service was between New York City, and Hoboken, New Jersey). 
The Elwell Ferry, a cable ferry in North Carolina, travels a distance of 110 yd, shore to shore, with a travel time of five minutes.
A contender as oldest ferry in continuous operation is the Mersey Ferry from Liverpool to Birkenhead, England. In 1150, the Benedictine Priory at Birkenhead was established. The monks used to charge a small fare to row passengers across the estuary. In 1330, Edward III granted a charter to the Priory and its successors for ever: "the right of ferry there… for men, horses and goods, with leave to charge reasonable tolls". However there may have been a short break following the Dissolution of the monasteries.
Another claimant as the oldest ferry service in continuous operation is the Rocky Hill - Glastonbury Ferry, running between the towns of Rocky Hill and Glastonbury, Connecticut. Established in 1655, the ferry has run continuously since, only ceasing operation every winter when the river freezes over. A long running salt water ferry service is the Halifax/Dartmouth ferry, running between the cities of Halifax and Dartmouth, Nova Scotia, which has run year-round since 1752, and is currently run by the region's transit authority, Metro Transit. However the Mersey Ferry predates it as the oldest salt water ferry.
Two of the world's largest ferry systems are located in the Strait of Georgia, in the Canadian province of British Columbia, and Puget Sound, in the U.S. state of Washington. BC Ferries in British Columbia operates 36 vessels, visiting 47 ports of call, while Washington State Ferries owns 28 vessels, travelling to 20 ports of call around Puget Sound. On the west coast of Scotland, Caledonian MacBrayne operate a network calling at 50 ports using a fleet of 29 vessels, 9 of which are 80m or longer. This includes a high proportion of lifeline services to island communities and as such most of the routes are heavily subsidised by the government.
Sydney Ferries in Sydney, Australia operates 31 passenger ferries in Port Jackson (Sydney Harbour), carrying 18 million passengers annually. It operates catamarans and other types of ferries on these routes, with the most famous likely being the Circular Quay-Manly route. Between 1938 and 1974 this route operated the South Steyne, billed at the time as the largest and fastest ferry of its type. Sydney Ferries became an independent corporation owned by the government in 2004.
Some of world's busiest ferry routes include the Star Ferry in Hong Kong and the Staten Island Ferry in New York City.
Metrolink Queensland operates 21 passenger ferries on behalf of Brisbane City Council, 12 being single-hulled ferries and 9 CityCats (catamarans), along the Brisbane River from the University of Queensland through the city to Brett's Wharf.
The gas-powered "Luciano Federico L" operated by Montevideo-based Buquebus, holds the Guinness World Record for the fastest car ferry boat in the world, in service between Montevideo, Uruguay and Buenos Aires, Argentina: its maximum speed, achieved in sea trials, was 60.2 kn. It can carry 450 passengers and 52 cars along the 110 nmi route.
Sustainability.
The contributions of ferry travel to climate change have received less scrutiny than land and air transport, and vary considerably according to factors like speed and the number of passengers carried. Average carbon dioxide emissions by ferries per passenger-kilometre seem to be 0.12 kg. However, 18-knot ferries between Finland and Sweden produce 0.221 kg of CO2, with total emissions equalling a CO2 equivalent of 0.223 kg, while 24–27-knot ferries between Finland and Estonia produce 0.396 kg of CO2 with total emissions equalling a CO2 equivalent of 0.4 kg.
With the price of oil at high levels, and with increasing pressure from consumers for measures to tackle global warming, a number of innovations for energy and the environment were put forward at the Interferry conference in Stockholm. According to the company Solar Sailor, hybrid marine power and solar wing technology are suitable for use with ferries, private yachts and even tankers.
Alternative fuels.
Alternative fuels are becoming more widespread on ferries. The fastest passenger ferry in the world Buquebus, runs on LNG, while Sweden's Stena plans to operate its 1500 passenger ferry on methanol in 2015. Both fuels reduce emissions considerably and displace costly diesel fuel.
Accidents.
The following notable maritime disasters involved ferries.

</doc>
<doc id="50778" url="http://en.wikipedia.org/wiki?curid=50778" title="Jar Jar Binks">
Jar Jar Binks

Jar Jar Binks is a fictional character from the Star Wars saga created by George Lucas. A major character in ', he also appears but with smaller roles in ', ', and the television series '. The first lead computer generated character of the franchise, he was portrayed by Ahmed Best in most of his appearances.
Jar Jar's primary role in "Episode I" was to provide comic relief for the audience, and was generally met with extremely negative comments from both critics and viewers. He is often acknowledged as one of the worst and most hated characters of the "Star Wars" franchise and the entire film industry in general.
Appearances.
Films.
Jar Jar Binks first appears in " as a bumbling, foolish Gungan from the planet Naboo. Banished by his tribe through Boss Rugor Nass for his clumsiness, he is nearly killed by a Federation transport, only to be saved at the last minute by Jedi Knight Qui-Gon Jinn (Liam Neeson). Qui-Gon and his padawan, Obi-Wan Kenobi (Ewan McGregor), persuade Jar Jar's tribe to release him to their custody as a guide. He later goes with the Jedi and Queen Padmé Amidala (Natalie Portman) to the planet Tatooine, where he meets and befriends Anakin Skywalker (Jake Lloyd).
Jar Jar later appears in the film's climactic battle scene, where he leads his fellow Gungans, as a general in the Gungan army, in defeating the Trade Federation. After the battle he appears at the funeral of Qui-Gon Jinn and in the ending parade with his fellow Gungans. 
Jar Jar's role in " is much smaller, but his actions are significant. Ten years after helping to save his planet, he is a delegate to the Galactic Senate and as such, plays a role in bringing his old friends, Obi-Wan and Anakin (Hayden Christensen) back to Coruscant, where he greets them with enthusiasm. Later, on the behalf of the Naboo, he gives a speech to the assembled Senate in favor of granting Chancellor Palpatine (Ian McDiarmid) vast emergency powers. These are granted, giving Palpatine the necessary power he needs to subsequently overthrow the senate and bring the galaxy into the dictatorial control of the Sith's Galactic Empire.
Jar Jar appears in only a few scenes in "", and has no dialogue (besides a brief "'scuse me" at one point). He was originally given some dialogue in the beginning, but this was cut. He is most prominently featured in Padmé Amidala's funeral procession at the end of the film.
In a recent interview, director J.J. Abrams suggested that Jar Jar's death may be referenced in . 
"Clone Wars".
Jar Jar Binks is a supporting character in the animated series "The Clone Wars", once again voiced by Best, although BJ Hughes voiced the character in a handful of season one episodes. In this series, he is a Senate representative who sometimes accompanies the main characters—Anakin, Ahsoka, Obi-Wan, and Padmé—on their adventures. He and master Mace Windu are the two main characters of the episodes "The Disappeared" part 1 and 2. In which they had to search for missing elders and rescue a princess, who was Jar Jar's past love.
Reception.
Even before the release of "The Phantom Menace", Jar Jar Binks became the subject of a great deal of media and popular attention, though not in the way his creators intended. Binks became symbolic of what many reviewers such as Brent Staples ("The New York Times"), David Edelstein ("Slate"), and Eric Harrison ("Los Angeles Times") considered to be creative flaws of the film. The character was widely rejected and often ridiculed by people who felt that Jar Jar was included in the film solely to appeal to children. One fan, Mike J. Nichols, created and distributed, free of charge, a modified version of the film, entitled "The Phantom Edit", which cut out several scenes featuring what Nichols dubbed 'Jar Jar antics.' The character was also lampooned on an episode of the television show "South Park" entitled "Jakovasaurs", in "The Fairly OddParents" (Episode: "Abra-Catastrophe!"), "The Simpsons" (Episode: "Co-Dependent's Day"), as well as the parody "" of "Robot Chicken", in which Best reprised the role in voice-over form.
Along with film critics, many have also accused the film's creators of excessive commercialization directed at young children (a criticism first leveled with the introduction of Ewoks in "Star Wars Episode VI: Return of the Jedi"). "Star Wars" creator George Lucas stated that he feels there is a section of the fanbase who get upset with aspects of "Star Wars" because "[t]he movies are for children but they don't want to admit that... There is a small group of fans that do not like comic sidekicks. They want the films to be tough like "The Terminator", and they get very upset and opinionated about anything that has anything to do with being childlike." Rob Coleman, who was the lead on the Industrial Light & Magic animation team, warned Lucas that the team thought Jar Jar's character came across poorly. Lucas told him that he specifically put Jar Jar in the film to appeal to small children twelve or under.
Allegations of racial caricature.
Joe Morgenstern of "The Wall Street Journal" described the character as a "Rastafarian Stepin Fetchit on platform hoofs, crossed annoyingly with Butterfly McQueen." Patricia J. Williams suggested that many aspects of Jar Jar's character are highly reminiscent of the archetypes portrayed in blackface minstrelsy, while others have suggested the character is a "laid-back clown character" representing a black Caribbean stereotype.
George Lucas has denied any racist implications. Ahmed Best also rejected the allegations, saying that "Jar Jar has nothing to do with the Caribbean".
Video game appearance.
Jar Jar appears as a LEGO mini-figure in the "Lego Star Wars" video games. He also appears as an Angry Bird with a hook move in "Angry Birds Star Wars II".

</doc>
<doc id="50779" url="http://en.wikipedia.org/wiki?curid=50779" title="Endor">
Endor

Endor or Ein Dor may refer to:

</doc>
<doc id="50781" url="http://en.wikipedia.org/wiki?curid=50781" title="Wedge Antilles">
Wedge Antilles

Wedge Antilles is a fictional character in the "Star Wars" universe. He is a supporting character portrayed by Denis Lawson in the original "Star Wars" trilogy. Antilles was a starfighter pilot for the Rebel Alliance, and founded Rogue Squadron with his friend Luke Skywalker. Wedge is notable for being the only Rebel pilot to have survived both attack runs on the Death Stars at the battles of Yavin and Endor. He also appears in the Star Wars Expanded Universe, most notably as the lead character in most of the "" novels.
Depiction.
Antilles first appears in "Star Wars" during the Rebels' Death Star attack briefing. In this scene Wedge is portrayed by Colin Higgins and voiced by David Ankrum, who dubs the character throughout the film. Denis Lawson plays the character for the remaining scenes filmed in the X-wing cockpit. Lawson also portrays Antilles in "The Empire Strikes Back" and "Return of the Jedi"; the former featured Lawson's voice dubbed over by an unidentified actor, while Lawson used his own voice in the latter (with a false American accent). Coincidentally, Lawson is the maternal uncle of Ewan McGregor, who played Obi-Wan Kenobi in the prequel trilogy.
Expanded Universe literature explains that Wedge's parents were killed when his parents' starship-refueling depot exploded. He piloted a freighter before joining the Rebel Alliance as a starfighter pilot. Michael A. Stackpole's and Aaron Allston's "X-wing" books and several Dark Horse Comics series focus on Antilles and the rest of Rogue Squadron after the events depicted in the films. Antilles also appears in other Expanded Universe works, including the "New Jedi Order" and "Legacy of the Force" novels. Antilles is the player's character in the ' and ' video games (reprised by Denis Lawson in the latter) and is a playable character in '. He was also a minor character in ', helping the player during one mission. He was a classic character in "".
Appearances.
"Star Wars" films.
Wedge appears towards the end of "Star Wars" as "Red Two," an X-Wing pilot and a member of Red Squadron along with Luke Skywalker. His considerable dogfighting prowess is shown in the Battle of Yavin when he saves Luke by shooting down a TIE Fighter that Luke had been unable to shake off. Along with Luke, he is one of only three X-Wing pilots to survive until the final attack run along the Death Star's trench. However, his X-Wing is damaged, and he is forced to retreat before the Death Star is destroyed. He and Luke are the only two X-Wing pilots to survive the battle, along with a Y-Wing and the "Millennium Falcon".
Wedge appears in "The Empire Strikes Back" as a member of the newly formed Rogue Squadron. He flies a Snowspeeder against the Empire's AT-AT ground assault with Wes Janson as his harpoon gunner. Their Snowspeeder inflicts the first casualty against the AT-AT attack group by firing a harpoon trailing a tow cable into one of the walker's legs and circling the walker several times, causing it to trip and fall and allowing Rebel forces to destroy the walker. After the battle, Wedge is seen and heard wishing Luke a safe trip.
Wedge appears towards the end of "Return of the Jedi" as the leader of Red Squadron in the Battle of Endor. Along with Lando Calrissian, who pilots the "Millennium Falcon" with Nien Nunb, he leads the fighter attack on the second Death Star. His dogfighting prowess is shown again in this battle, as Wedge personally shoots down a number of Imperial TIE Fighters, and appears to easily navigate the narrow and treacherous flight spaces inside the Death Star that lead to its core. When he and Lando reach the Death Star's core, Wedge destroys the power regulator on the core's northwest tower, while Lando destroys the core itself. He later appears at the victory celebration at the Ewok village on Endor.
Denis Lawson confirmed in May 2014 that he was asked to reprise the role of Wedge in the upcoming ""; however, he turned it down, saying that it would have "bored him".
In popular culture.
Wedge is a popular and important character of the original "Star Wars" trilogy despite his brief screen time. IGN named Wedge the 24th greatest "Star Wars" character of all time, claiming "Wedge Antilles may be the most important ancillary character in the "Star Wars" universe".
Wedge was portrayed by three actors in the "Star Wars" radio series from National Public Radio: Meshach Taylor in "A New Hope", Don Scardino in "The Empire Strikes Back", and Jon Matthews in "Return of the Jedi".
Two characters named "Biggs and Wedge" (where "Biggs" is a reference to Red Squadron member Biggs Darklighter) appear frequently in RPGs made by "Square".
In the CW television series "Supernatural", the two protagonists use aliases when checking in to hotel rooms and when impersonating law enforcement. In the season four episode "Lazarus Rising," Sam Winchester disappears. His brother Dean calls Sam's cell phone company, claiming that his phone was lost the night before and he needs the phone's GPS activated. He gives the phone company the alias "Wedge Antilles," knowing Sam would have used that name.

</doc>
<doc id="50784" url="http://en.wikipedia.org/wiki?curid=50784" title="Princess Leia">
Princess Leia

Princess Leia Organa of Alderaan, later Leia Organa Solo, and born Leia Amidala Skywalker, is a fictional character in the "Star Wars" universe, portrayed by the actress Carrie Fisher.
The character is one of the main protagonists of the original "Star Wars" trilogy, the romantic interest of Han Solo and revealed as the twin sister of Luke Skywalker as well as the daughter of Anakin Skywalker (Darth Vader) and Padmé Amidala.
Appearances.
"Star Wars Episode IV: A New Hope".
In her first appearance in "", Princess Leia is introduced as the Princess of Alderaan and a member of the Imperial Senate. 
Darth Vader captures her on board the ship "Tantive IV", where she is acting as a spy for the Rebel Alliance. He accuses her of being a traitor and demands to know the location of the secret technical plans of the Death Star, the Galactic Empire's most powerful weapon. Unknown to Vader, the young Senator has hidden the plans inside the Astromech droid R2-D2 and has sent it to find Jedi Master Obi-Wan Kenobi on the nearby planet of Tatooine.
Later, Vader has her tortured but she resists telling him anything. Still believing she could be useful, Death Star commander Grand Moff Tarkin threatens to destroy Alderaan with the super weapon unless she reveals the location of the hidden Rebel base. She provides the location of an old, abandoned base but Tarkin orders Alderaan to be destroyed anyway. She is finally rescued by Luke Skywalker, Han Solo, Obi-Wan, the Wookiee Chewbacca, and the two droids R2-D2 and C-3PO. 
When they finally escape at the expense of Obi-Wan's life sacrificed in a duel with Vader, they take part in a major battle in which Luke destroys the Death Star in his X-wing fighter. In the Massassi Temple at the hidden Rebel base on Yavin 4, the radiant Princess presents the Alderaanian Medal of Freedom to her rescuers and the heroes of the battle.
"Star Wars Episode V: The Empire Strikes Back".
Three years later in "", Princess Leia is at the Rebel base on Hoth. 
She later helps with the evacuation of Hoth during an Imperial attack, remaining at her station until Han Solo forces her to leave together with him. She flees with Han, C-3PO, and Chewbacca on their "Millennium Falcon" ship. Although they are pursued by Imperial TIE fighters, they dodge their fire by flying into an asteroid field when the hyperdrive for the "Falcon" breaks down. Romance blossoms between Leia and Han during their flight from the Empire; while hiding in the stomach of a space slug, she finally shares a kiss with the Corellian smuggler. Later, when they stop at Bespin for repairs, Lando Calrissian turns them over to the Empire to which Darth Vader plans to use them as bait for Luke Skywalker. Han is also used as a test subject for the carbon freezing chamber meant for Luke, and it is there that Leia finally confesses her love for Han. Vader then gives the frozen Han to bounty hunter Boba Fett. 
Later, Lando helps Leia, Chewbacca and the two droids escape. Chewbacca chokes Lando for giving Han to Vader and Fett, and Leia angrily tells Lando they do not need his help. While escaping, she senses that Luke is in trouble and makes them go back for Luke. They save Luke after falling into Vader's trap and is badly hurt in a near-fatal duel against Vader.
"Star Wars Episode VI: Return of the Jedi".
One year later in "", Princess Leia along with Chewbacca, Lando Calrissian, Luke Skywalker and the droids C-3PO and R2-D2 go to Tatooine to try to rescue Han Solo from Jabba the Hutt.
Leia's part of the plan consists of posing as the Ubese bounty hunter Boushh turning Chewbacca over to Jabba. Her ruthless bargaining, negotiating the price for Chewbacca while holding an armed thermal detonator, impresses Jabba enough to allow her quarters in the palace for the night. The Huttese gangster eventually discovers her real identity, captures her, makes her his new slave girl and forces her to wear her iconic metal bikini after she frees Han from the carbonite. After Luke kills the Rancor, Jabba sentences Luke and Han to be fed to the Sarlacc. Just as all seems lost, Lando (disguised as a guard) helps Luke and Han overpower their captors. Leia seizes the moment to kill Jabba by strangling him with the very chain that bound her. R2-D2 cuts her loose, and after Luke boards the sailbarge, the Jedi Knight rescues Leia. With Leia's help, Luke uses a deck cannon to blow up his barge as they swing to safety.
While preparing for a last battle with the Empire on Endor, Luke reveals to Leia that she is, in fact, his twin sister and that Darth Vader is their father. Initially reluctant to believe him, Leia realizes that Luke speaks the truth. Leia joins Han in leading the Rebels in a final battle as the Rebel Fleet battle the second Death Star. Leia is slightly injured in the battle but nevertheless helps the Rebels, allied with the Ewoks, to defeat the Empire.
"Star Wars Holiday Special".
Leia briefly appears in the 1978 TV movie "Star Wars Holiday Special". In the show, Leia is a leader and administrator of the new Rebel Alliance base. She is accompanied by C-3PO when contacting Mallatobuck (Chewbacca's wife) for assistance in finding Chewbacca and Han. She also appears in the cartoon segment at a different Rebel Base, located in an asteroid field, and at the Life Day ceremony at the end of the film. Fisher also appeared in and hosted the November 18, 1978 episode of "Saturday Night Live" that aired one day after the holiday special.
"Star Wars Episode III: Revenge of the Sith".
In the prequel "", Padmé Amidala is pregnant with Anakin Skywalker's progeny near the end of the Clone Wars. During the film's epilogue, Padmé gives birth to, and names, Luke and Leia on Polis Massa. After her mother dies from childbirth, Leia is sent to Alderaan to be raised by her adoptive parents (Bail Organa and Breha Organa) and be safe from Emperor Palpatine's newly declared Galactic Empire. The expanded universe indicates that her birth name was thus "Leia Amidala Skywalker", although this isn't explicitly confirmed on-screen.
"Star Wars Episode VII: The Force Awakens".
In March 2013, Carrie Fisher confirmed that she would reprise her character in "".
"Star Wars" literature.
In "The Truce at Bakura", set one day after the ending of "Return of the Jedi", Leia establishes New Alderaan, a sanctuary for the destroyed planet's surviving inhabitants. The Royal House of Alderaan, as represented by Leia Organa Solo and her children, officially holds sovereignty over both New Alderaan and the old Alderaan system. Her position is largely ceremonial, however, as the government on New Alderaan actually administers the Alderaan system and New Alderaan in her name.
That same day, she encounters the spirit of her father, now redeemed and resuming the form of his original self, Anakin Skywalker. He pleads for her forgiveness, but she angrily banishes him from her life because of his past crimes. Knowing this, Anakin says that he will be there for her should she need him, and disappears.
In Expanded Universe materials set after "Return of the Jedi", Leia is portrayed as a founding member of the New Republic. Although most of her life is devoted to such matters of state, she engages in limited study of the Jedi arts, with Luke as her teacher. Notably, she wields a blue lightsaber that she built herself.
In "Queen of the Empire" by Paul Davids and Hollace Davids, she is kidnapped by the "Prophets of the Dark Side", who try to brainwash her into pledging her loyalty to the Empire and marry their leader, Trioculus. Leia tricks her captors by having a look-alike droid take her place; the droid eventually kills Trioculus.
In "Heir to the Empire" by Timothy Zahn, Luke builds her a green lightsaber which she uses to help free the Noghri from their debt to the Empire. Luke then gives her a red lightsaber to complement the weapon she had constructed earlier.
As described in "The Courtship of Princess Leia", Leia marries Han after a near-disastrous courtship in which Prince Isolder vies for her affections. Han kidnaps Leia and takes her to the planet Dathomir, which he had won in a game of sabacc. There they encounter the Nightsisters, whose attempt to escape eventually leads to the demise of the Warlord Zsinj and his empire, equal rival at this time to both the Imperial Remnant and the New Republic.
At first, Leia does not want to have children, fearing they would succumb to the dark side as her father had done. In the novel "Tatooine Ghost", however, she begins to understand what happened to her father to bring him to the dark side. When she and Han go on a mission to Tatooine to retrieve the Alderaanian moss-painting "Killik Twilight" and the Rebel code hidden within it, Leia discovers her grandmother Shmi Skywalker's diary and, with the help of her father's childhood friends Kitster and Wald, discovers her father was not always the monster she thought he was. Touched, Leia finally forgives her the spirit of her father.
In the Thrawn trilogy, Grand Admiral Thrawn, who had formed an alliance with Joruus C'Baoth, orders Noghri commandos to kidnap Leia, who is pregnant. C'Baoth intends to warp Leia and Luke to the Dark Side, and plans to corrupt also the two unborn twins. To avoid capture, she hides on the planet Kashyyyk, but her would-be kidnappers track her down. She later learns that Vader once landed on the Noghri home planet Honoghr and tricked the Noghri into serving the Empire by promising to help their planet recover from the ecological disaster that it suffered during the Clone Wars. Because of this, they are fiercely loyal to Vader. Leia is able to leverage her biological relationship to Vader to persuade a Noghri assassin to travel with her to Honoghr and help convince the Noghri of the Empire's deception. Leia shows an assembly of Noghri matriarchs that the droids which the Empire demanded are actively poisoning the land and slowing down the reconstruction. They leave the service of the Empire after one of their assassins (and Thrawn's personal bodyguard), Rukh, kills Thrawn during the Battle of Bilbringi and becomes allies of the New Republic. For her efforts, Leia is known as "Lady Vader" among the Noghri, and she and her family become revered figures in their society.
In "The Last Command", Leia gives birth to the twins Jaina and Jacen on Coruscant during Thrawn's siege.
During the events of "Dark Empire", the New Republic suffers severe setbacks, losing most of its worlds, as well as Luke Skywalker to the dark side. After her brother's capture on Coruscant, subsequent transport to Byss, and temptation by the cloned Palpatine, a pregnant Leia along with her husband, Han Solo, reach the Emperor's new stronghold of Byss, where she confronts the reincarnated Sith Lord. At first Leia is unsuccessful in turning Luke away from the dark side, but does manage to take a Jedi Holocron away from Palpatine's chambers. Leia boards Palpatine's Super Star Destroyer, "Eclipse I", and appeals to the good in Luke, ultimately redeeming him. Brother and sister then fight Palpatine with the light side of the Force, cutting him off from the dark side and control of the titanic Force-generated storm he had created, intending to obliterate the Rebel Alliance fleet. The storm grows out of control, destroying both "Eclipse I" and Palpatine; Luke and Leia escape just in time.
In "Empire's End", Leia gives birth to a second son, whom she names Anakin in honor of her father's redemption (See Solo family). Along with a Jedi named Jen, she defeats Palpatine's second-in-command, a Dark Jedi assassin. Palpatine is soon reborn in his last remaining clone body, which is quickly deteriorating, due to sabotage by traitorous minions. Leia is forced to flee to Onderon to hide Anakin from Palpatine, who intends to transfer his spirit into the infant. Palpatine does eventually find her, but Han accidentally shoots him in the back just as he is about to possess the baby. A sacrifice by a dying Jedi named Empatojayos Brand saves them both from Palpatine's wrath, and destroys Palpatine's malignant spirit forever.
In the "New Jedi Order" series, Leia resigns as Chief of State, and is replaced by Borsk Fey'lya. After the Yuuzhan Vong attack on Sernpidal, Leia goes before the Senate to bring attention to the threat posed by the approaching Yuuzhan Vong. Her pleas go unheeded and the Vong legions swarm into the galaxy, destroying system after system and defeating the Jedi and the New Republic army in countless battles. Leia contributes to the war effort by joining SELCORE, a movement that helps refugees.
In "Vector Prime", Chewbacca's death sends Han into a deep depression, causing a large rift between him and Leia, culminating in his walking out of the marriage after an argument. They patch things up after Leia is gravely wounded by Tsavong Lah at the Battle of Duro. Their troubles are not yet over, however; when the Vong unleash the deadly voxyn, Leia is targeted by a Voxyn master slayer who has already killed many Jedi. With the help of her Noghri bodyguards, she eliminates the assassin with her lightsaber.
In "Star By Star", Leia and Han lose their youngest son, Anakin, during the Myrkr mission and the fall of Coruscant. Leia and Han go to Hapes for Anakin's funeral, then on several missions to restore HoloNet communications to the Unknown Regions, including foiling a second attempt of the Ssi-ruuk to invade Bakura in the process.
Near the end of the Yuuzhan Vong war, she and Han rescue Thorsh, a prisoner from the internment camps of planet Selvaris. Later, they enter with Jedi Master Kyp Durron, a Bothan secret agent named Wraw, and a few more allies on the planet Callulla. During the battle with the Yuuzhan Vong warriors, she destroys a few Slayers and a Commander before being captured. The commando is eventually rescued by Lando Calrissian, Talon Karrde and Tendra, Lando's wife.
When Zonama Sekot makes its existence known near Coruscant in "The Final Prophecy", Leia and Han travel there to be reunited with the rest of their family. While there, they meet Harrar, a Yuuzhan Vong priest. Leia, Han, and a few companions work with Harrar and a group of heretics to get inside the Well of the World Brain on Coruscant.
After the destruction of Shimrra and Supreme Overlord Onimi, Nom Anor travels with the Solo family across the labyrinth to escape the mighty war vessel of the Master Shaper. However, the executor turns on them and shoots his venom towards Han, but Jacen catches the poison, saving his father from certain death. Leia engages the Prefect before he can eliminate her husband and her elder son. She proves victorious and cuts off the rogue Nom Anor's arm. Leia and Han then leave Anor to die.
Leia then gives up politics and becomes Han's copilot, a position she holds for the next five years.
In "The Joiner King", Leia and Han follow various Jedi who had disappeared into the Unknown Regions, and discover Raynar Thul is alive and had been taken in by a nest of Killiks. To avoid a war with the Chiss, Leia suggests to "UnuThul" (as Raynar was now known) that the Killik nest be moved to a new planet, but makes him think it is his idea. At this time, Leia comes to terms with her heritage and asks Saba Sebatyne to train her as a Jedi Knight, as per a promise Luke had made to her during the Thrawn crisis.
In "The Unseen Queen", R2-D2 suffers some severe malfunctions and shows Luke a holoclip of his father and a pregnant woman, whom Luke learns is his and Leia's real mother, Padmé Amidala. In the holoclips, Anakin and Padmé are discussing a dream of Anakin's in which Padmé dies in childbirth. Before Luke can get more info out of R2, the droid has a meltdown, claiming he is protecting information. Frustrated, Luke contacts master slicer Ghent, who manages to recover one other holoclip from R2, this time featuring a scene in which Padmé is talking to Obi-Wan Kenobi about Anakin, which is displayed to both Luke and Leia. In "The Swarm War", Luke and Leia finally see their mother's death. (All of these scenes were originally portrayed in "".) Also, Sebatyne tells Leia to construct a new lightsaber to show she is a true Jedi Knight.
Later on in the novel, she defeats the evil Joiner — and fallen Jedi — Alema Rar. She defeats her and assumes that she is dead, although Alema was in fact still alive, but severely injured.
Leia and Han unknowingly become grandparents to Allana, Jacen's daughter, but they finally find out the truth as of "Fury".
During the "Legacy of the Force" series, set some 35 years after "Return of the Jedi", Leia supports Han, who feels allegiance to his native Corellia, even though she remains a Jedi Knight. However, they soon break from Corellia following that planet's plot to assassinate Queen Mother Tenel Ka, Allana's mother and a former Jedi.
In this series, her own son, Jacen, gradually falls to the dark side, and terrorizes the galaxy as the ruthless Sith Lord Darth Caedus. Leia tries to reason with Jacen at first, but ultimately disowns him after he commits a series of atrocities, and tacitly agrees to Han and Jaina's plan to hunt down and kill him. Jaina kills Jacen in "Invincible", the final novel in the series, leaving the Solo family stricken with grief, even as they acknowledge that his death was "necessary". To cope with the loss, Leia and Han adopt Allana and pledge to raise her as their own.
Character development.
In the rough draft of "A New Hope", Leia was roughly 13 - 15-year-old princess (the same age as Padmé Amidala in '), the spoiled daughter of King Kayos and Queen Breha of Aquilae. In that draft, she had two brothers, Biggs and Windy, whose identities were substantially revised into their current form by the fourth draft (though they did not appear in intervening versions). The later story synopsis established her as Leia Antilles, the daughter of Bail Antilles from the peaceful world of Organa Major. In the fourth draft the names were turned around so that Leia Organa"' came instead from Alderaan.
Characteristics.
Princess Leia Organa is characterized as a driven, dedicated woman with a forceful personality. The "petite, fair-skinned human female" is known perhaps as the most beautiful and remembered woman in the "Star Wars" universe. Leia was loved by Luke Skywalker (before their sibling relationship was discovered), Han Solo, Prince Isolder (before he met Teneniel Djo) and others. In the "" novels, Leia is seduced by the crime lord Prince Xizor, who bewitches her with pheromones. Chewbacca manages to break off the seduction, allowing Leia to defeat the gangster.
A woman warrior, Leia frequently takes part in combat operations. She is an excellent sharpshooter, missing rarely, if ever, with a blaster. She kills, among other villainous characters, Jabba the Hutt, choking him with the very chain that bound her to him; Leia is also responsible for Grand Admiral Thrawn's death, since she turns the Noghri against him. By redeeming her brother, she also helps bring about the destruction of the Clone Emperor. She even kills a Dark Jedi named Kueller with a blaster rifle, right before he is about to deal a fatal blow to Luke.
Though initially hesitant about her Jedi training, Leia later learns various Force techniques, and becomes a very proficient Jedi after finishing her training under Luke's guidance. She eventually becomes a full-fledged member of the New Jedi Order, developing her lightsaber skills and even further under the training of Master Saba Sebatyne. Leia displays her powers of the lightsaber by slaying a fully trained Dark Jedi, even before training under Master Sebatyne.
In popular culture.
The "cinnamon bun hairstyle".
Leia's well-known hairdo in "A New Hope" has been affectionately dubbed the "doughnut hairstyle", or "cinnamon buns", by many science fiction fans. Carrie Fisher, in an appearance on the UK TV show "Bring Back Star Wars", said she hated her character's hairstyle; she felt it made her face look rounder, and it took two hours every day to style. Miss Piggy of "Jim Henson's Muppet Babies" copied the hairdo with doughnuts in a "Star Wars"-themed episode of the series. Also, in one scene from Mel Brooks' "Spaceballs", Princess Vespa also appears to have the hairstyle, but reveals that she is actually wearing a large pair of headphones. In the parody film "Thumb Wars", the role of Leia was filled by a character named Princess Bunhead, who, as the name implies, had two cinnamon rolls for hair.
The hairdo has also been compared to the Iberian sculpture Lady of Elche.
Young marriageable Hopi Indian women wear a very elaborate "Squash Blossom" hairdo that superficially resembles Princess Leia's. George Lucas, however, has denied a connection, saying: "In the 1977 film, I was working very hard to create something different that wasn't fashion, so I went with a kind of Southwestern Pancho Villa woman revolutionary look, which is what that is. The buns are basically from turn-of-the-century Mexico."
"Princess Leia's Theme".
"Princess Leia's Theme" is represented by the musical leitmotif in the "Star Wars" saga. The piece was composed by John Williams.
It first appeared in "A New Hope", heard when Princess Leia is captured by Darth Vader. Later, it plays as R2-D2 plays her holographic message to Obi-Wan Kenobi. The theme plays when Obi-Wan Kenobi is killed by Vader. Finally, it plays in the end credits.
In "The Empire Strikes Back", the theme is developed into "Han Solo and the Princess". "Princess Leia's Theme" plays as Han Solo tells Leia that he must leave to settle his debt with Jabba the Hutt. The only other time it plays is at the end of the movie, when Leia and Lando Calrissian rescue Luke Skywalker from Cloud City and Leia and Luke are reunited.
In "Return of the Jedi", the theme is heard as Leia shoots a stormtrooper on Endor. Earlier it plays as Luke mentions her name on Dagobah.
The theme was revived for "Revenge of the Sith", when Bail Organa has a conversation with Obi-Wan Kenobi about adopting her. It is then heard during the final montage at the end of the film as her adoptive parents hold the newborn Leia. It plays one last time during the end credits.
Metal bikini.
The term "Metal Bikini" refers to the iconic slave girl costume worn by Princess Leia when she was held captive in Jabba the Hutt's palace, at the beginning of "Return of the Jedi". The skimpy costume consisted of a copper brassiere fastened over the neck and behind the back with string, copper plates at the groin in front and back, a red silk loincloth, and leather boots. There were other various adornments, including a hair fastener, a snake arm-wrap and two bracelets. Last, there was the chain and collar that bound her to Jabba.
After her appearance wearing the "golden metal bikini", Carrie Fisher (and thus Leia herself) almost immediately became a cult sex symbol. Leia's metallic bikini scenes were voted by "Empire" magazine as among the most memorable in movie history. 
One "Wired" magazine editor stated the only reason for the outfit's fame is "no doubt that the sight of Carrie Fisher in the gold sci-fi swimsuit was burned into the sweaty subconscious of a generation of fanboys hitting puberty in the spring of 1983."
Reception.
In 2008, Princess Leia was selected by "Empire" magazine as the 89th of the greatest film characters of all time, with IGN listing her as the 8th top "Star Wars" hero. UGO Networks listed Leia as one of their best heroes of all time, stating that "even though she ended up hanging out with goofy teddy bears and making out with a doughy Harrison Ford, she always had that special something".

</doc>
<doc id="50785" url="http://en.wikipedia.org/wiki?curid=50785" title="Lando Calrissian">
Lando Calrissian

Lando Calrissian is a fictional character in the "Star Wars" universe. He is portrayed by Billy Dee Williams in ' and '. He also appears frequently in the "Star Wars" "Expanded Universe" of novels, comic books and video games, including a series of novels in which he is the primary protagonist.
Appearances.
Film.
"Star Wars Episode V: The Empire Strikes Back".
Baron Lando Calrissian first appears in "The Empire Strikes Back" as the baron administrator of Cloud City, concerned primarily with keeping the Galactic Civil War and the Empire out of his affairs. The bounty hunter Boba Fett, working for Darth Vader, tracks Han Solo, Princess Leia, Chewbacca, and C-3PO, traveling in the hyperdrive-damaged "Millennium Falcon", to Bespin. Shortly before Han and crew make it to Bespin, Darth Vader and a contingent of Imperial forces arrive at Bespin and threaten to take over the city. Lando is strongarmed by Vader into betraying his old friend Han and turning him over to Fett. Unwilling to leave the city in the hands of the Empire, Lando reluctantly does so, but his conscience gets the better of him when Vader goes back on his word and takes Leia and Chewbacca as prisoners. When Lando sets Leia, C-3PO and Chewbacca free, Chewbacca chokes him for giving Han to Vader and Fett, and Leia angrily tells Lando that they do not need his help before they chase after Fett, who escapes with Solo. In the ensuing evacuation of Cloud City, he helps Leia, Chewbacca, C-3PO, and R2-D2 escape. He then assists Leia in rescuing the maimed Luke Skywalker, who had fallen into Vader's trap and lost his right hand in the duel, from the underside of Cloud City. Afterwards, he joins the Rebel Alliance and promises Leia he will find Han.
"Star Wars Episode VI: Return of the Jedi".
In "", Lando goes undercover to help Luke rescue Han from crime lord Jabba the Hutt. During a heated battle with Jabba's henchmen, Han saves Lando from being devoured by the Sarlacc; Lando then helps Han and the others destroy Jabba's barge. For his heroics, he is made a general in the Rebel Alliance. Lando then takes the pilot chair in his old ship, the "Millennium Falcon", and leads the attack on the second Death Star. He helps the Rebels to victory by destroying the gigantic Imperial battle station.
Television.
"Star Wars Rebels".
Billy Dee Williams returned to the role in the "Star Wars Rebels" episode "Idiot's Array".
Expanded Universe.
"Star Wars" Legends.
Pre-"Empire Strikes Back".
Lando's life prior to "The Empire Strikes Back" is chronicled in "The Adventures of Lando Calrissian" series of novels. Early in his career, Lando is a prodigious gambler and wins the "Millennium Falcon" in a game of sabacc. He also wins a strange star-shaped droid named Vuffi Raa, who would be his friend and ally on many occasions. After acquiring the "Falcon", and under the tutelage of his friend Han Solo, Lando begins to develop his skills as a pilot.
In "", Lando's father's name is revealed as "Lindo Calrissan".
In "Lando Calrissian and the Mindharp of Sharu", he is conned by the Sorcerer of Tund, Rokur Gepta, into hunting down the titular object in the Rafa system. When he arrives, he finds it totally covered in sand and plastic pyramids, and inhabited by a dull and slow-witted society, the Toka. Lando eventually finds the mindharp, but the human governor of Rafa IV activates it. Majestically, the pyramids crumble and the Toka are revealed as the Sharu, an ancient civilization that had drained their intelligence away for safety. Once the Sharu are resurrected, they drive out all the humans. Lando is forced to return to Rokur empty-handed.
In "Lando Calrissian and the Starcave of ThonBoka", Lando tries to save a space-borne species called the Oswaft. During the mission, strange spherical droids appear and take Vuffi Raa away, for he is actually a scout for this strange culture. Then, he enters the sabacc championships, and loses to Han Solo. Han makes off with the "Millennium Falcon".
In "Rebel Dawn", Lando helps Han Solo, Chewbacca, and Solo's old flame Bria Tharen (then a Commander in the Rebel Alliance) in a raid against the Hutt-controlled slave world of Ylesia. During the raid which promises generous rewards to Han and his compatriots, Tharen's Red Hand Squadron double crosses Lando and the rest of Han's friends. In the ensuing chaos, Han is branded an accomplice and a traitor. Back on Nar Shaddaa (then Solo's home) word soon spreads, and Lando punches him out.
Later, he wins the mining facility of Cloud City from its ruler, Baron Administrator Dominic Raynor, in a game of sabacc. He becomes a responsible leader, keeping his operations out of the eyes of the Galactic Empire. These events are chronicled in the comic book story "Lady Luck," written by Rich Handley and Darko Macan, and its sequel, "Lando Calrissian: Idiot's Array," by Rich Handley.
Pre-"Return of the Jedi".
The "Star Wars" comic book series released by Marvel Comics featured Lando as a prominent character following "The Empire Strikes Back". In the comic series, he has a crime lord nemesis named Drebble, and Lando will frequently make use of his foil's name as a cover identity so that any animosity he generates while using the alias will be brought against the real Drebble, not Lando himself. Drebble features in the comic story "Lady Luck," in which he receives a first name, Barpotomous. Lando's use of Drebble's name backfires when the alliance hears how much trouble "Drebble" has been causing the Empire, and orders Lando to find and decorate him.
Post-"Return of the Jedi".
Following the events of "Return of the Jedi", he builds a mining complex on the superhot planet Nkllon called Nomad City. The city consists of an old Dreadnaught carried across the landscape on the night side of the planet.
Nomad City is destroyed during Grand Admiral Thrawn's campaign against the New Republic, portrayed in "The Thrawn Trilogy". When Emperor Palpatine reappears in "Dark Empire" and begins his own campaign against the New Republic, Calrissian rejoins the Republic's military at his previous rank. After Palpatine is finally destroyed in "Empire's End", Calrissian leaves the military. When Kessel is abandoned following Admiral Daala's attack on the planet in "Darksaber", Calrissian takes over the planet, converts its prison into a mining complex, and mines spice from the planet for a number of years.
During "The Corellian Trilogy", Calrissian goes on a galaxy-wide hunt for a rich wife. The businessman, reasoning that marriage is partly a financial relationship, meets Tendra Risant, whom he marries. With his inlaws' money and his entrepreneurial abilities, he opens a mining facility on the outer rim planet of Dubrillion. On the side, he unofficially runs an asteroid training facility for smuggler pilots.
In "Vector Prime", Calrissian asks Solo and Chewie to help him with a business transaction on the planet of Sernpidal. During this trip, Chewbacca meets his demise, saving Solo's son Anakin from the moon the Yuuzhan Vong sends crashing onto the planet. Solo, Anakin, and the thousands of people they saved head to Dubrillion, where the first major clash between the biological Vong and the mechanical New Republic occurs. Calrissian's ground defenses, as well as his newly created shielding technologies, help save Dubrillion from the initial assault. When the Vong retreat, the Skywalker family launch a desperate attack with the Star Destroyer "Rejuvenator" and its task force. Calrissian serves as gunner for Solo on the "Falcon", but the "Rejuvenator" and most of her complement of ships are destroyed.
In "", Calrissian helps the Solos organize an escape route for Skywalker's Jedi Academy. He poses as a citizen of the besieged world Talfaglia in order to deliver 17 Jedi "hostages" to the Yuuzhan Vong; this is actually a strike team, which attempts to destroy the cloning facilities on the worldship orbiting Myrkr. He also creates Yuuzhan Vong Hunter Droids, two of which prove crucial in the assault on Myrkr to destroy the voxyn queen, and dozens more are used in assaults on Vong strongholds throughout the war.
In "Star By Star", Calrissian's efforts are crucial in delaying the Vong victory in a battle on Coruscant. Instead of using mines to attack the Vong piecemeal, he lets them into the minefield, then turns them on the approaching enemy. Over one thousand of the Vong are destroyed or damaged. He also rescues the kidnapped Ben Skywalker from the clutches of the treacherous Senator Viqi Shesh.
In "", Calrissian leads the ground troops to a quick victory, then volunteers to lead a small task force to destroy Vong ambushes of convoys and find out how they tracked said convoys. These efforts are wholly successful, with large Vong capital ships destroyed with minimal effort, time, fuel, and munitions.
Finally, when the newly located Galactic Senate on Mon Calamari sets out to vote for a new Chief of State, Calrissian and Talon Karrde provide "incentives" such as silence, money, and blackmail to convince a group of corrupt senators to vote for Cal Omas, who supports the Jedi. When Omas is elected, Calrissian, Karrde, and Star Destroyer owner Booster Terrik lead the Smuggler's Alliance, with Han Solo commanding, to victory over a massive Yuuzhan Vong force in the rout at Ebaq 9.
Calrissian eventually retires to private life after proving crucial in the Battle of Yuuzhan'tar and creating a new Holonet to replace the one the Vong destroyed.
In the seventh novel of the "Legacy of the Force" series, "Fury", Calrissian announces to Han Solo and Leia Organa Solo that he and Tendra are having a child.
Lando is revealed to have a son Lando Jr. nicknamed "Chance".
"Star Wars" Canon.
Lando Calrissian appears in the episode "Idiot's Array"of "Star Wars Rebels". He wins Chopper, the repair droid of the crew of the "Ghost", in a game of sabacc, forcing the crew to assist him with a dangerous smuggling run to get their droid back. The smuggling run involves trading a female member to a crime lord in exchange for a puffer pig, leaving the traded crew member to escape in an escape pod. Humiliated, Azmorigan and his henchmen intercept the rebels on Lothal to take revenge. The crew successfully drive the gangsters away and part ways with Lando, though not before Chopper steals the fuel to Lando's ship, which the smuggler concedes as payment for their help.
Reception.
Lando Calrissian was chosen as the 11th top "Star Wars" character by IGN and the character was chosen as the 12th top "Star Wars" hero by Jesse Schedeen. Schedeen also said that Calrissian was one of the characters he would most like seeing in "".
Cultural references.
Lando appears briefly in the animated film "The Lego Movie", voiced by Billy Dee Williams.
Lando is featured in Chapter 2 of by M. K. Asante 

</doc>
<doc id="50786" url="http://en.wikipedia.org/wiki?curid=50786" title="Boba Fett">
Boba Fett

Boba Fett is a fictional character in "Star Wars". A recurring antagonist in ' and ', he is a bounty hunter hired by Darth Vader. He also made a cameo appearance in the digitally remastered Special Edition of ' working for Jabba the Hutt. ' establishes his origin as an unaltered clone of the bounty hunter Jango Fett raised as his son. His aura of danger and mystery have created a cult following for the character.
Appearances.
Original trilogy.
Boba Fett first appeared at the September 20, 1978, San Anselmo Country Fair parade. The character appeared on television several weeks later, animated by Nelvana Studios for "Star Wars Holiday Special" as a mysterious figure who betrays Luke Skywalker after saving him, Chewbacca, C-3PO and R2-D2 from a giant monster, only to be revealed as a bounty hunter working for Darth Vader. After his image and identity were revealed in "Star Wars Holiday Special", costumed Fett characters appeared in shopping malls and special events, putting up "Wanted" posters of the character to distinguish him from the franchise's Imperial characters. He also appears in Marvel Comics' "Star Wars" newspaper strip.
"" features Boba Fett as the "next major villain" after Darth Vader. Fett tracks the "Millennium Falcon" to Cloud City, where Vader captures its passengers and tortures its captain, Han Solo. Wanting to collect a bounty on Solo, Fett confronts Vader about whether Solo will survive carbon freeze. Vader promises that the Empire will compensate Fett if Solo dies; after Solo is determined to be alive, Vader turns him over to Fett.
" features Boba Fett at Jabba the Hutt's palace where Solo's rescuers are captured, and he travels on Jabba's sail barge to the sarlacc pit, where the prisoners are to be executed. When the prisoners mount an escape he attempts to intervene, and ends up in a tussle with Luke Skywalker, but Solo accidentally ignites Fett's rocket pack, sending the bounty hunter falling into the Sarlacc's mouth.
In the digitally remastered Special Edition version of ", Boba Fett briefly appears outside the "Millennium Falcon" with Jabba, as well as several added and altered scenes.
Prequel trilogy.
"Star Wars Episode II: Attack of the Clones" revealed that Boba Fett is an unaltered child clone whom Jango Fett raises as his son. Boba helps Jango escape from Obi-Wan Kenobi, but later witnesses Jango's decapitation by Mace Windu.
Boba Fett also appears in the CGI animated series "". Boba is featured as a supporting antagonist during the second season finale, and later the fourth season.
Possible film.
On February 6, 2013, "Entertainment Weekly" reported that The Walt Disney Company, the new parent company of Lucasfilm and "Star Wars", is developing a stand-alone film featuring Boba Fett which would take place either between "A New Hope" and "The Empire Strikes Back" or between "The Empire Strikes Back" and "Return of the Jedi".
Expanded Universe and television.
Boba appears extensively in the "Star Wars" Legends non-canonical Expanded Universe of novels, comic books, and video games. The young adult Fett book series released after "Clones" depicts Fett taking his father's ship and armor to begin his own bounty-hunting career. Some Expanded Universe stories released before "Attack of the Clones" depict other accounts of Fett's origins. These stories include him being a stormtrooper who killed his commanding officer; a leader of the fabled Mandalorian warriors; and Jaster Mereel, a "Journeyman Protector" convicted of treason. Karen Traviss' novel "Bloodlines" (2006), published four years after "Clones", states that Fett seeded some of these "false" backstories himself.
Various video games and books depict Fett's work as a bounty hunter, for which he charges "famously expensive" fees and that he undertakes only when the mission meets "his harsh sense of justice." K. W. Jeter's "Bounty Hunter Wars" trilogy (1998–1999) depicts Fett as more communicative than in the films because the books' plots require Fett to show "an ability to convince people as well as kill them." Works such as Dark Horse Comics' "Dark Empire" series (1991–1992) describe Fett escaping from the Sarlacc. In another trilogy, after it describes him nearly killing the Sarlacc, it goes into detail about how Dengar, one of the bounty hunters hired to find Solo in Star Wars V, and featured in the series, Star Wars: The Clone Wars, finds him and restores him back to health. He would have occasional run-ins with Han Solo and his family over the years, before fighting against the Yuuzhan Vong and becoming the leader of the Mandalorians. In the "Legacy of the Force" series (2006–2008), Jaina Solo asks Fett to train her to help her defeat her corrupted brother and ends up helping fight against Jacen's troops. Such media also reveal that Fett became a family man at one point, though he was forcibly separated from his wife after killing his commanding officer for assaulting her. His wife subsequently disappeared and was presumed dead, and their daughter blamed Boba for her fate, and began hunting him. Her own daughter, Mirta Gev, later sought Boba out and connected with her grandfather, while her mother was killed by Jacen Solo after he became Darth Caedus. Mirta later married a Mandalorian warrior, and Boba's wife was discovered to be alive, having been frozen in carbonite some decades previously.
Concept and development.
Boba Fett stems from initial design concepts for Darth Vader, who was originally conceived as a rogue bounty hunter. While Vader became less a mercenary and more of a dark knight, the bounty hunter concept remained, and Fett became "an equally villainous" but "less conspicuous" character. Concept artist Ralph McQuarrie influenced Fett's design, which was finalized by and is credited to Joe Johnston. Fett's armor was originally designed for "super troopers", and was adapted for Fett as the script developed. Screen-tested in all-white, Fett's armor eventually garnered a subdued color scheme intended to visually place him between white-armored "rank-and-file" Imperial stormtroopers and Vader, who wears black. This color scheme had the added bonus of conveying the "gray morality" of his character. The character's armor was designed to appear to have been scavenged from multiple sources, and it is adorned with trophies. A description of the character's armor in the summer 1979 "Bantha Tracks" newsletter catalyzed "rampant speculation" about the character's mysterious origins.
Despite two years of widespread publicity about Fett's appearance in "The Empire Strikes Back", script rewrites significantly reduced the character's presence in the film. Fett's "distinctive" theme, composed by John Williams, is "not music, exactly" ... but "more of a gurgly, viola-and-bassoon thing aurally cross-pollinated with some obscure static sounds." Sound editor Ben Burtt added the sound of jangling spurs, created and performed by the Foley artist team of Robert Rutledge and Edward Steidele, to Fett's appearance in Cloud City, intending to make the character menacing and the scene reminiscent of similar gunfighter appearances in Western films.
Daniel Keys Moran, who wrote several novels featuring Boba Fett, cited Westerns as an influence on his development of the character. Moran said
The difficult thing with Fett was finding a worldview for him that permitted him to proclaim a Code — given the stark Evil that permeated the Empire, Fett pretty much had to be either 1) Evil, or 2) an incredibly unforgiving, harsh, "greater good" sort of guy. The second approach worked and has resonated with some readers.
"Star Wars" creator George Lucas considered adding a shot of Fett escaping the Sarlacc in "Return of the Jedi", but decided against it because it would have detracted from the story's focus, instead leaving the task of "reviving" Fett to Expanded Universe canon. Lucas also said that, had he known Fett would be so popular, he would have made the character's death "more exciting." Lucas at one point considered depicting Vader and Fett as brothers in the prequel films, but discounted it as too "hokey." In continuing to develop the character in the prequel films, Lucas closed some avenues for expanding the character's story while opening others.
The cancelled video game "Star Wars 1313" would have told the story of the character's career as a young bounty hunter.
Film casting and production.
Boba Fett is primarily played by Jeremy Bulloch in "The Empire Strikes Back" and "Return of the Jedi". Bulloch's half-brother alerted him to the role. He was cast as Fett because the costume happened to fit "as if a Savile Row tailor had come out and made it"; he did not have to do a reading or a screen test, and Bulloch never worked from a script for either film.
Filming the role for "Empire" lasted three weeks. The actor was pleased with the costume and used it to convey the character's menace. Bulloch based his performance on Clint Eastwood's portrayal of the Man with No Name in "A Fistful of Dollars"; similar to the Western character, Bulloch cradled the gun prop, made the character seem ready to shoot, slightly tilted his head, and stood a particular way. Bulloch did not try to construct a backstory for the character, and said later that "the less you do with Boba Fett, the stronger he becomes". Playing Fett in "Empire" was both the smallest and most physically uncomfortable role Bulloch has played; Bulloch said donning the heavy jetpack was the worst aspect of the role.
Bulloch spent four weeks working on "Jedi". He was unaware of Fett's demise before filming began and was "very upset" by the development; he would like to have done more with Fett. Still, Bulloch believed killing Fett made the character stronger, and that his "weak" death makes fans want the character to return. Bulloch thinks a scene created for "Jedi" Special Edition in which Fett flirts with one of Jabba's dancers is not in keeping with the character's nature.
A younger version of the character is played by Daniel Logan in "Attack of the Clones". Logan had not seen the "Star Wars" films prior to being cast as Fett, but he watched the original trilogy at Lucas' request. The actor had to rely on his imagination for the bluescreen filming.
Other portrayals.
Boba Fett was voiced by Don Francks in "Star Wars Holiday Special" and in the "" episode "A Race to the Finish". Although Bulloch wore Fett's costume in "Empire" and "Jedi", John Morton filled in during one scene for "Empire", while Jason Wingreen voiced Fett for "Empire". His brief appearance in "Hope" was performed by Industrial Light & Magic creature animator Mark Austin; The character's appearance in the Special Edition footage of "Jedi" was performed by Don Bies and Nelson Hall. For the 2004 re-releases, Temuera Morrison replaced the character's original voice.
The character's voice in National Public Radio's "Star Wars" radio dramas was provided by Alan Rosenberg in "The Empire Strikes Back" and Ed Begley, Jr. in "Return of the Jedi", Tim Glovatsky in the audio adaptation of "Dark Forces: Rebel Agent", Joe Hacker in audio adaptation of the "Dark Empire" comics, Temuera Morrison for ', "Star Wars Battlefront II" and ', Dee Bradley Baker in ', ' and ', Chris Cox in ', Tom Kane in ', ' and ', and Daniel Logan for ' and "".
Boba Fett (as an evil pig) is featured in the "Angry Birds Star Wars" episode entitled "Boba Fett Missions". In the "Family Guy" parody, "Something, Something, Dark Side," Boba Fett is played by the Giant Chicken.
Popularity.
Boba Fett is a "cult figure" and one of the most popular "Star Wars" characters. In 2008, Boba Fett was selected by "Empire" magazine as the 79th greatest movie character of all time, and he is included on "Fandomania"‍ '​s list of "The 100 Greatest Fictional Characters". He personifies "danger and mystery", and Susan Mayse calls Fett "the unknowable "Star Wars" character" who "delivers mythic presence." Although Tom Bissell asserts that no one knows why Boba Fett has become so popular, nor cares why, both Lucas and Bulloch cite Fett's mysterious nature as reasons for his popularity. Bulloch, who has never fully understood the character's popularity, attributes it to the costume and the respect Fett garners from Darth Vader and Jabba the Hutt. The initial Boba Fett toy, more than Fett's actual film appearance, might be responsible for the character's popularity; Henry Jenkins suggests children's play helped the character "take on a life of its own". Moran said Vader's admonition specifically to Fett in "The Empire Strikes Back" — "No disintegrations" — gives Fett credibility; he was interested in Fett because the character is "strong, silent, [and] brutal". Jeter says that even when Fett appears passive, he conveys "capability and ruthlessness". Bissell credits Bulloch for giving Fett "effortless authority" in his first scene in "The Empire Strikes Back", using such nuances as "cradling" his blaster and slightly cocking his head. Fett's small role in "The Empire Strikes Back" may actually have made the character seem more intriguing. Logan, who was a Young Artist Award nominee for his portrayal of Fett, compares Fett to "that boy in school who never talks" and who attracts others' curiosity.
Bissell adds that Boba Fett, along with other minor characters like Darth Maul and Kyle Katarn, appeals to adolescent boys' "images of themselves: essentially bad-ass but ... honorable about it." This tension and the absence of a clear "evil nature" (distinct from evil actions) offer Fett dramatic appeal. Furthermore, Fett "is cool because he was designed to be cool", presenting a "wicked ambiguity" akin to John Milton's portrayal of Satan in "Paradise Lost" and Iago in William Shakespeare's "Othello". Bissell compares Fett to Beowulf, Ahab, and Huckleberry Finn: characters "too "big"" for their original presentation, and apt for continued development in other stories. Moran finds Fett reminiscent of the Man with No Name.
"The San Francisco Chronicle" describes Boba Fett fans as "among the most passionate", and the character is important to "Star Wars" fan culture. Boba Fett's popular following before the character even appeared in "The Empire Strikes Back" influenced Damon Lindelof's interest in developing "Lost" across multiple media. Will Brooker calls "superb" a fan's campaign to have Boba Fett unmasked as a woman. Fan parodies include Boba Phat, a cosplay "intergalactic booty hunter" created by David James. IGN ranked Boba Fett as the eighth top Star Wars character, due to his status as a fan-favourite and cult following.
Merchandising.
Fett is one of the top five best-selling "Star Wars" action figures, and Boba Fett-related products are "among the most expensive" "Star Wars" merchandise. Fett was the first new mail-away action figure created for "The Empire Strikes Back"; although advertised as having a rocket-firing backpack, safety concerns led Kenner to sell his rocket attached. Gray called the early toy "a rare and precious commodity", and one of the rocket-firing prototypes sold at auction for $16,000 in 2003. In August 2009, Hasbro released a Fett action figure based on McQuarrie's white-armored concept, and Boba Fett as both a child and bounty hunter have been made into Lego minifigures. Wizards of the Coast's Star Wars Trading Card Game includes several Boba Fett cards. Hallmark Cards created a Boba Fett Christmas tree ornament.

</doc>
<doc id="50788" url="http://en.wikipedia.org/wiki?curid=50788" title="Māori language">
Māori language

Maori or Māori (; ]) is an Eastern Polynesian language spoken by the Māori people, the indigenous population of New Zealand. Since 1987, it has been one of New Zealand's official languages. It is closely related to Cook Islands Māori, Tuamotuan, and Tahitian.
According to a 2001 survey on the health of the Māori language, the number of very fluent adult speakers was about 9% of the Māori population, or adults. A national census undertaken in 2006 says that about 4% of the New Zealand population, or 23.7% of the Maori population could hold a conversation in Maori about everyday things.
Name.
The English word comes from the Maori language, where it is spelled "Māori". In New Zealand the Maori language is commonly referred to as "Te Reo" ] "the language", short for "te reo Māori".
The spelling "Maori" (without macron) is standard in English outside New Zealand in both general and linguistic usage. The Maori-language spelling "Māori" (with macron) has become common in New Zealand English in recent years, particularly in Maori-specific cultural contexts, although the traditional English spelling is still prevalent in general media and government use.
Preferred and alternate pronunciations in English vary by dictionary, with being most frequent today, and , , and also given. Spelling pronunciations as are also encountered in popular speech in the United States particularly, but are considered incorrect.
Official status.
New Zealand has three official languages – English, Māori and New Zealand Sign Language. Māori gained this status with the passing of the Māori Language Act in 1987. Most government departments and agencies have bilingual names; for example, the Department of Internal Affairs "Te Tari Taiwhenua", and places such as local government offices and public libraries display bilingual signs and use bilingual stationery. New Zealand Post recognises Māori place-names in postal addresses. Dealings with government agencies may be conducted in Māori, but in practice, this almost always requires interpreters, restricting its everyday use to the limited geographical areas of high Māori fluency, and to more formal occasions, such as during public consultation.
An interpreter is on hand at sessions of Parliament, in case a Member wishes to speak in Māori. In 2009, Opposition parties held a filibuster against a local government bill, and those who could recorded their voice votes in Māori, all faithfully interpreted.
A 1994 ruling by the Privy Council in the United Kingdom held the New Zealand Government responsible under the Treaty of Waitangi (1840) for the preservation of the language. Accordingly, since March 2004, the state has funded Māori Television, broadcast partly in Māori. On 28 March 2008, Māori Television launched its second channel, Te Reo, broadcast entirely in the Māori language, with no advertising or subtitles.
In 2008, Land Information New Zealand published the first list of official place names with macrons, which indicate long vowels. Previous place name lists were derived from systems (usually mapping and GIS systems) that could not handle macrons.
History.
According to legend, Māori came to New Zealand from the mythical Hawaiki. Current anthropological thinking places their origin in tropical eastern Polynesia, mostly likely from the Southern Cook or Society Islands region, and that they arrived by deliberate voyages in seagoing canoes – possibly double-hulled and probably sail-rigged. These settlers probably arrived by about AD 1280 (see Māori origins). Their language and its dialects developed in isolation until the 19th century.
Since about 1800, the Māori language has had a tumultuous history. It started this period as the predominant language of New Zealand. In the 1860s, it became a minority language in the shadow of the English spoken by many settlers, missionaries, gold seekers, and traders. In the late 19th century, the colonial governments of New Zealand and its provinces introduced an English-style school system for all New Zealanders. From the 1880s, on the insistence of Maori MPs, the government forbade the use of the Māori language in schools. Increasing numbers of Māori people learned English. 
Until the Second World War (1939–1945), most Māori people spoke Māori as their first language. Worship took place in Māori; it functioned as the language of Māori homes; Māori politicians conducted political meetings in Māori; and some literature and many newspapers appeared in Māori.
Before 1880, some Māori parliamentarians suffered disadvantages because Parliament's proceedings took place in English. However, by 1900, all Maori members of parliament, such as Ngata, were university graduates who spoke fluent English. From this period, the number of speakers of Māori began to decline rapidly. By the 1980s, fewer than 20% of the Maori spoke the language well enough to be classed as native speakers. Even many of those people no longer spoke Māori in the home. As a result, many Māori children failed to learn their ancestral language, and generations of non-Māori-speaking Māori emerged.
By the 1980s, Māori leaders began to recognise the dangers of the loss of their language, and initiated Māori-language recovery-programs such as the Kōhanga Reo movement, which from 1982 immersed infants in Māori from infancy to school age. There followed in 1985 the founding of the first Kura Kaupapa Māori (Years 1 to 8 Māori-medium education programme) and later the first Wharekura (Years 9 to 13 Māori-medium education programme). Although "there was a true revival of te reo in the 1980s and early to-mid-1990s ... spurred on by the realisation of how few speakers were left, and by the relative abundance of older fluent speakers in both urban neighbourhoods and rural communities", the language has been in a "renewed decline" since (p. 439). The decline is believed "to have several underlying causes". These include:
Based on the principles of partnership, Māori-speaking government, general revitalisation and dialectal protective policy, and adequate resourcing, the Waitangi Tribunal has recommended "four fundamental changes":
Linguistic classification.
Comparative linguists classify Māori as a Polynesian language; specifically as an Eastern Polynesian language belonging to the Tahitic subgroup, which includes Rarotongan, spoken in the southern Cook Islands, and Tahitian, spoken in Tahiti and the Society Islands. Other major Eastern Polynesian languages include Hawaiian, Marquesan (languages in the Marquesic subgroup), and the Rapa Nui language of Easter Island.
While the preceding are all distinct languages, they remain similar enough that Tupaia, a Tahitian travelling with Captain James Cook in 1769–1770, communicated effectively with Māori. Speakers of modern Māori generally report that they find the languages of the Cook Islands, including Rarotongan, the easiest other Polynesian languages to understand and converse in. See also Austronesian languages.
Geographic distribution.
Nearly all speakers are ethnic Māori resident in New Zealand. Estimates of the number of speakers vary: the 1996 census reported 160,000, while other estimates have reported as few as 10,000 fluent adult speakers in 1995 according to the Maori Language Commission.
According to the 2006 census, 131,613 Māori (23.7%) "could [at least] hold a conversation about everyday things in te reo Māori". In the same census, Māori speakers were 4.2% of the New Zealand population.
The level of competence of self-professed Māori speakers varies from minimal to total. Statistics have not been gathered for the prevalence of different levels of competence. Only a minority of self-professed speakers use Māori as their main language in the home. The rest use only a few words or phrases (passive bilingualism).
Māori still[ [update]] is a community language in some predominantly-Māori settlements in the Northland, Urewera and East Cape areas. "Kohanga reo" Māori-immersion kindergartens throughout New Zealand use Māori exclusively. Increasing numbers of Māori raise their children bilingually.
Urbanisation after the Second World War led to widespread language shift from Māori predominance (with Māori the primary language of the rural "whānau") to English predominance (English serving as the primary language in the Pākehā cities). Therefore Māori-speakers almost always communicate bilingually, with New Zealand English as either their first or second language.
The percentage prevalence of the Māori language in the Māori diaspora is far lower than in New Zealand. Census data from Australia show it as the home language of 5,504 people in 2001, or 7.5% of the Māori community in Australia. This represents an increase of 32.5% since 1996.
Orthography.
The modern Māori alphabet has 20 letters, two of which are digraphs: A Ā E Ē H I Ī K M N O Ō P R T U Ū W NG and WH.
Attempts to write Māori words using the Latin script began with Captain James Cook and other early explorers, with varying degrees of success. Consonants seem to have caused the most difficulty, but medial and final vowels are often missing in early sources. Anne Salmond records "aghee" for aki (In the year 1773, from the North Island East Coast, p. 98), "Toogee" and "E tanga roak" for Tuki and Tangaroa (1793, Northland, p216), "Kokramea", "Kakramea" for Kakaramea (1801, Hauraki, p261), "toges" for toki(s), "Wannugu" for Uenuku and "gumera" for kumara (1801, Hauraki, p261, p266, p269), "Weygate" for Waikato (1801, Hauraki, p277), "Bunga Bunga" for pungapunga, "tubua" for tupua and "gure" for kurī (1801, Hauraki, p279), as well as "Tabooha" for Te Puhi (1823, Northern Northland, p385).
From 1814, missionaries tried to define the sounds of the language. Thomas Kendall published a book in 1815 entitled "He Korao no New Zealand", which in modern orthography and usage would be "He Kōrero nō Aotearoa". Professor Samuel Lee, working with chief Hongi Hika and Hongi's junior relative Waikato at Cambridge University, established a definitive orthography based on Northern usage in 1820. Professor Lee's orthography continues in use, with only two major changes: the addition of "wh" to distinguish the voiceless bilabial fricative phoneme from the labio-velar phoneme /w/; and the consistent marking of long vowels. The macron has become the generally accepted device for marking long vowels ("hāngi"), but double vowel letters have also been used ("haangi").
The Māori embraced literacy enthusiastically, and missionaries reported in the 1820s that Māori all over the country taught each other to read and write, using sometimes quite innovative materials in the absence of paper, such as leaves and charcoal, carved wood, and hides.
Long vowels.
The alphabet devised at Cambridge University was deficient in that it did not mark vowel length. The following examples show that vowel length is phonemic in Māori:
Māori devised ways to mark vowel-length, sporadically at first. Occasional and inconsistent vowel-length markings occur in 19th-century manuscripts and newspapers written by Māori, including macron-like diacritics and the doubling of letters. Māori writer Hare Hongi (Henry Stowell) uses macrons in his "Maori-English Tutor and Vade Mecum" of 1911, as does Sir Apirana Ngata, inconsistently, in his "Maori Grammar and Conversation" (7th printing 1953). Once the Māori language started to be taught in universities in the 1960s, vowel-length marking was made systematic. At Auckland University, Professor Bruce Biggs (of Ngāti Maniapoto descent) promoted the use of double vowels (thus "Maaori"), and this became the standard at Auckland until Biggs died in 2000. The Māori Language Commission, set up by the Māori Language Act 1987 to act as the authority for Māori spelling and orthography, favours the use of macrons, which are now the established means of indicating long vowels. Occasionally, diaeresis are seen instead of macrons (e.g. "Mäori") due to technical limitations producing letters with macrons on typewriters and older computers.
Phonology.
Māori has five phonemically distinct vowel articulations and ten consonant phonemes.
Vowels.
Although it is commonly claimed that vowel realisations (pronunciations) in Māori show little variation, linguistic research has shown this not to be the case.
Vowel length is phonemic; but four of the five long vowels occur in only a handful of word roots, the exception being /ā/. As noted above, it has recently become standard in Māori spelling to indicate a long vowel by a macron. For older speakers, long vowels tend to be more peripheral and short vowels more centralised, especially with the low vowel, which is long [aː] but short [ɐ]. For younger speakers, they are both [a]. For older speakers, /u/ is only fronted after /t/; elsewhere it is [u]. For younger speakers, it is fronted [ʉ] everywhere, as with the corresponding phoneme in New Zealand English.
As in many other Polynesian languages, diphthongs in Māori vary only slightly from sequences of adjacent vowels, except that they belong to the same syllable, and all or nearly all sequences of nonidentical vowels are possible. All sequences of nonidentical short vowels occur and are phonemically distinct. With younger speakers, /ai, au/ start with a higher vowel than the [a] of /ae, ao/.
The following table shows the five vowel phonemes and the allophones for some of them according to Bauer 1997. Some of these phonemes occupy large spaces in the anatomical vowel triangle (actually a trapezoid) of tongue positions. For example, /u/ is sometimes realised (pronounced) as IPA [ʉ].
Diphthongs are /a/ or /o/ followed by a mid or high vowel: /ae, ai, ao, au, oi, oe, ou/.
Consonants.
The consonant phonemes of Māori are listed in the following table. Seven of the ten Māori consonant letters have the same pronunciation as they do in the International Phonetic Alphabet (IPA). For those that do not, the IPA phonetic transcription is included, enclosed in square brackets per IPA convention. Māori stops /p, t, k/ are nonaspirated, unlike in English. Māori /ɾ/ is a tap, similar to the "r" in "very" in many dialects of England (and slightly less similar to the t in the American English pronunciation of "city" or "letter").
The pronunciation of /wh/ is extremely variable, but its most common pronunciation (its canonical allophone) is the labiodental fricative, IPA [f] found in English. Another allophone is the bilabial fricative, IPA [ɸ], which is usually supposed to be the sole pre-European pronunciation, although in fact linguists are not sure of the truth of this supposition.
Because English stops /p, t, k/ primarily have aspiration, speakers of English often hear the Māori nonaspirated stops as English /b, d, g/. However, younger Māori speakers tend to aspirate /p, t, k/ as in English. English speakers also tend to hear Māori /r/ as English /l/. These ways of hearing have given rise to place-name spellings which are incorrect in Māori, like Tolaga Bay in the North Island and Otago and Waihola in the South Island.
/ng/ can come at the beginning of a word, like "sing-along" without the "si", which is difficult for English speakers outside of New Zealand to manage.
/h/ is pronounced as a glottal stop, [ʔ], and /wh/ as [ʔw], in some western areas of North Island.
/r/ is typically a flap, especially before /a/. However, elsewhere it is sometimes trilled.
Syllables.
Syllables in Māori have one of the following forms: V, VV, CV, CVV. This set of four can be summarised by the notation, (C)V(V), in which the segments in parentheses may or may not be present. A syllable cannot begin with two consonant sounds (the digraphs "ng" and "wh" represent single consonant sounds), and cannot end in a consonant, although some speakers may occasionally devoice a final vowel. All possible CV combinations are grammatical, though "wo", "who", "wu", and "whu" occur only in a few loanwords from English such as "wuru", "wool" and "whutuporo", "football".
As in many other Polynesian languages, e.g., Hawaiian, the rendering of loanwords from English includes representing every English consonant of the loanword (using the native consonant inventory; English has 24 consonants to 10 for Māori) and breaking up consonant clusters. For example, "Presbyterian" has been borrowed as "Perehipeteriana"; no consonant position in the loanword has been deleted, but /s/ and /b/ have been replaced with /h/ and /p/, respectively.
Stress is typically within the last four vowels of a word, with long vowels and diphthongs counting double. That is, on the last four moras. However, stressed moras are longer than unstressed moras, so the word does not have the precision in Māori that it does in some other languages. It falls preferentially on the first long vowel, on the first diphthong if there is no long vowel (though for some speakers never a final diphthong), and on the first syllable otherwise. Compound words (such as names) may have a stressed syllable in each component word. In long sentences, the final syllable before a pause may have a stress in preference to the normal stressed syllable.
Dialects.
Biggs proposed that historically there were two major dialect groups, North Island and South Island, and that South Island Māori is extinct. Biggs has analysed North Island Māori as comprising a western group and an eastern group with the boundary between them running pretty much along the island's north–south axis.
Within these broad divisions regional variations occur, and individual regions show tribal variations. The major differences occur in the pronunciation of words, variation of vocabulary, and idiom. A fluent speaker of Māori has no problem understanding other dialects.
There is no significant variation in grammar between dialects. "Most of the tribal variation in grammar is a matter of preferences: speakers of one area might prefer one grammatical form to another, but are likely on occasion to use the non-preferred form, and at least to recognise and understand it." Vocabulary and pronunciation vary to a greater extent, but this does not pose barriers to communication.
North Island dialects.
In the southwest of the island, in the Whanganui and Taranaki regions, the phoneme /h/ is a glottal stop and the phoneme /wh/ is [ʔw]. This difference has been the subject of considerable debate during the 1990s and 2000s over the then-proposed change of the name of the city Wanganui to Whanganui.
In Tūhoe and the Eastern Bay of Plenty (northeastern North Island) "ng" has merged with "n". In parts of the Far North, "wh" has merged with "w".
South Island dialects.
In the extinct South Island dialects, "ng" merged with "k" in many regions. Thus "Kāi Tahu" and "Ngāi Tahu" are variations in the name of the same iwi (the latter form is the one used in acts of Parliament). Since 2000, the government has altered the official names of several southern place names to the southern dialect forms by replacing "ng" with "k". New Zealand's highest mountain, known for centuries as "Aoraki" in southern Māori dialects that merge "ng" with "k", and as "Aorangi" by other Māori, was later named "Mount Cook", in honour of Captain Cook. Now its sole official name is "Aoraki/Mount Cook", which favours the local dialect form. Similarly, the Māori name for Stewart Island, "Rakiura", is cognate with the name of the Canterbury town of Rangiora. Likewise, Dunedin's main research library, the Hocken Library, has the name "Te Uare Taoka o Hākena" rather than the northern (standard) "Te Whare Taonga o Hākena". Goodall & Griffiths say there is also a voicing of "k" to "g" – this is why the region of Otago (southern dialect) and the settlement it is named after – Otakou (standard Māori) – vary in spelling (the pronunciation of the latter having changed over time to accommodate the northern spelling).
The standard Māori "r" is also found occasionally changed to an "l" in these southern dialects and the "wh" to "w". These changes are most commonly found in place names, such as Lake Waihola and the nearby coastal settlement of Wangaloa (which would, in standard Māori, be rendered "Whangaroa"), and Little Akaloa, on Banks Peninsula. M. Goodall & Griffiths claim that final vowels are given a centralised pronunciation as schwa or that they are elided (pronounced indistinctly or not at all), resulting in such seemingly-bastardised place names as The Kilmog, which in standard Māori would have been rendered "Kirimoko", but which in southern dialect would have been pronounced very much as the current name suggests. This same elision is found in numerous other southern placenames, such as the two small settlements called The Kaik (from the term for a fishing village, "kainga" in standard Māori), near Palmerston and Akaroa, and the early spelling of Lake Wakatipu as "Wagadib". In standard Māori, Wakatipu would have been rendered "Whakatipua", showing further the elision of a final vowel.
Despite being officially regarded as extinct, many government and educational agencies in Otago and Southland encourage the use of the dialect in signage and official documentation.
Grammar and syntax.
Bases.
Biggs (1998) developed an analysis that the basic unit of Māori speech is the phrase, rather than the word. The lexical word forms the "base" of the phrase. "Nouns" include those bases that can take a definite article, but cannot occur as the nucleus of a verbal phrase; for example: "ika" (fish) or "rākau" (tree). Plurality is marked by various means, including the definite article (singular "te", plural "ngā"), deictic particles "tērā rākau" (that tree), "ērā rākau" (those trees), possessives "taku whare" (my house), "aku whare" (my houses). Some nouns lengthen a vowel in the plural, such as "wahine" (woman); "wāhine" (women).
Statives serve as bases usable as verbs but not available for passive use, such as "ora", alive, "tika", correct. Grammars generally refer to them as "stative verbs". When used in sentences, statives require different syntax than other verb-like bases.
Locative bases can follow the locative particle "ki" (to, towards) directly, such as "runga", above, "waho", outside, and placenames ("ki Tamaki", to Auckland).
Personal bases take the personal article "a" after "ki", such as names of people ("ki a Hohepa", to Joseph), personified houses, personal pronouns, "wai?" who? and "Mea", so-and-so.
Particles.
Like all Polynesian languages, Māori has a rich array of particles. These include verbal particles, pronouns, locative particles, definitives and possessives.
Verbal particles indicate aspectual properties of the verb they relate to. They include "ka" (inceptive), "i" (past), "kua" (perfect), "kia" (desiderative), "me" (prescriptive), "e" (non-past), "kei" (warning, "lest"), "ina" or "ana" (punctative-conditional, "if and when"), and "e … ana" (imperfect).
Pronouns have singular, dual and plural number. Different first-person forms in the dual and in the plural express groups either inclusive or exclusive of the listener.
Locative particles refer to position in time and/or space, and include "ki" (towards), "kei" (at), "i" (past position), and "hei" (future position).
Possessives fall into one of two classes marked by "a" and "o", depending on the dominant versus subordinate relationship between possessor and possessed, so "ngā tamariki a te matua", the children of the parent, but "te matua o ngā tamariki", the parent of the children.
Definitives include the articles "te" (singular) and "ngā" (plural) and the possessives "tā" and "tō". These also combine with the pronouns. Demonstratives have a deictic function, and include "tēnei", this (near me), "tēnā", that (near you), "tērā", that (far from us both), and "taua", the aforementioned. Other definitives include "tēhea?" (which?), and "tētahi", (a certain). Definitives that begin with "t" form the plural by dropping the "t": "tēnei" (this), "ēnei" (these).
Bases as qualifiers.
In general, bases used as qualifiers follow the base they qualify, e.g. "matua wahine" (mother, female elder) from "matua" (parent, elder) "wahine" (woman).
Personal pronouns.
Like other Polynesian languages, Māori has three numbers for pronouns and possessives: singular, dual and plural. For example: "ia" (he/she), "rāua" (they two), "rātou" (they, three or more). The dual and plural suffixes are modern reflexes of historical words "rua" and "toru". Māori pronouns and possessives further distinguish exclusive "we" from inclusive "we", second and third. It has the plural pronouns: "mātou" (we, exc), "tātou" (we, inc), "koutou" (you), "rātou" (they). The language features the dual pronouns: "māua" (we two, exc), "tāua" (we two, inc), "kōrua" (you two), "rāua" (they two). The difference between exclusive and inclusive lies in the treatment of the person addressed. "Mātou" refers to the speaker and others but not the person or persons spoken to ("i.e.", "I and some others, but not you"), while "tātou" refers to the speaker, the person or persons spoken to, and everyone else ("i.e.", "you and I and others"). Examples:
Calendar.
From missionary times, Māori used transliterations of English names for days of the week and for months of the year. Since about 1990 the Māori Language Commission / Te Taura Whiri o te Reo Māori has promoted new ("traditional") sets. Its days of the week have no pre-European equivalent but reflect the pagan origins of the English names (for example, Hina = moon). The commission based the months of the year on one of the traditional tribal lunar calendars.

</doc>
<doc id="50790" url="http://en.wikipedia.org/wiki?curid=50790" title="Star Destroyer">
Star Destroyer

Star Destroyers and Star Dreadnoughts are capital ships in the fictional "Star Wars" universe. The Imperial Star Destroyer, which first appears in the first seconds of ' (1977), is "the signature vessel of the Imperial fleet". The term "Star Destroyer" also refers to other vessels in the Star Wars universe. Star Destroyers appear in the original three "Star Wars" movies and in ' (2005). In addition to their movie appearances, Star Destroyers appear throughout the Star Wars Expanded Universe in books, comics, and games. Numerous Star Destroyer models and toys have been released.
"Imperial" class.
The Star Destroyer first appears in the original "Star Wars" trilogy. In the 1977 film "Star Wars", Han Solo refers to them as "Imperial cruisers". The term "Star Destroyer" is not used in dialogue until "" (1980).
In the original draft scripts of the movie that would become "Star Wars", the term "Stardestroyer" refers to two-man fighters flown by what would become the Galactic Empire in later drafts. Industrial Light & Magic (ILM) built a 91 cm shooting model of the ship for "A New Hope". ILM built another model measuring 259 cm, equipped with internal lighting to provide a better sense of scale, for "The Empire Strikes Back".
Depiction.
An Imperial Star Destroyer first appears in the opening scene of "A New Hope", chasing a Rebel ship (the "Tantive IV"). This appearance shows the Imperial ship's massive size in comparison to the "Tantive IV".
"Imperial"-class Star Destroyers are constructed by Kuat Drive Yards and hold a distinguished place in the Imperial Navy, symbolizing the Empire's military might with a peak number of more than 25,000 vessels, though thousands were lost/destroyed to warlordism following Palpatine's death. The Imperial-class Star Destroyer is also informally referred to as an Imperial cruiser. Like the "Victory"- and "Venator"-class ships that precede it, the Imperial Star Destroyer is most notable for its massive size and overwhelming firepower; a single "Imperial"-class ship is capable of overwhelming most starships or devastating a hostile planet, and its mere presence is often enough to deter rebellion. At 1600 m long, "Imperial"-class Star Destroyers are armed with turbolasers, ion cannons and tractor beam projectors. They carry 72 TIE fighters, numerous ground forces (including stormtroopers, 20 AT-ATs and 30 AT-STs), a prefabricated base for rapid deployment to planetary surfaces and a variety of support and landing craft.
In the Expanded Universe, after the climactic Battle of Endor, the Rebel Alliance captured several "Imperial"-class ships and added them to their own fleet. As the Rebel Alliance transitioned into the New Republic and gradually took large portions of the galaxy from the remnants of the Empire, it was able to procure more of the powerful ships as well as the vital shipyards and facilities with which to operate them. Although the New Republic eventually upgrades its starfleet with newer ship types, the "Imperial"-class Star Destroyer remains in service well into the "New Jedi Order" era and fights during the Yuuzhan Vong war.
Super Star Destroyer.
Super Star Destroyers ("SSD" or ""Super"-class Star Destroyers") are gargantuan vessels that dwarf even the standard "Imperial"-class Star Destroyers. The shooting model for the Super Star Destroyer "Executor" in "The Empire Strikes Back" and "Return of the Jedi" was 282 cm long. In addition to appearing in these two films, Super Star Destroyers appear in the Star Wars Expanded Universe's books and games. Some Expanded Universe material refer to these ships as "Star Dreadnoughts".
Depiction.
Kevin J. Anderson's novel "Darksaber" describes a Super Star Destroyer as being "worth twenty Imperial Star Destroyers". The "Executor" is Darth Vader's flagship in "The Empire Strikes Back" and is the Imperial command ship at the Battle of Endor in "Return of the Jedi". Admiral Ozzel commands the "Executor" at the beginning of "The Empire Strikes Back", with command shifting to Admiral Piett later in the movie and through "Return of the Jedi". During the Battle of Endor, a crippled A-wing piloted by Arvel Crynyd crashes into the ship's bridge, causing the "Executor" to crash into the second Death Star and explode.
Roleplaying material identifies the "Executor" as the first of the "Executor" class, while starwars.com identifies it as the first of the "Super" class. Wizards of the Coast explains that the ""Super" class" label was used to hide the ship's "true nature from the Imperial Senate"; this bureaucratic fiction became the source of the type's nickname. Originally described in "A Guide to the Star Wars Universe" in 1984 as being 8 km long, "Executor"-class ships were later described as being 19 km long.
Another Super-class Star Destroyer, the "Lusankya", is described in Michael Stackpole's "X-Wing" novels as Ysanne Isard's private prison and, later, the primary Imperial support vessel during Isard's time on Thyferra. The ship is captured and used by the New Republic, eventually being destroyed when it rams a Yuuzhan Vong worldship in Aaron Allston's "Enemy Lines" novels. The novel "Darksaber" features the "Night Hammer", a Super Star Destroyer "plated with stealth armor", making it "virtually invisible to enemy forces".
Other types.
"Venator"-class Star Destroyers, also known as "Republic attack cruisers", appear in "Revenge of the Sith" and in the Expanded Universe. The ship's design is meant to bridge the appearance of the "Acclamator"-class transports in "" and the "Imperial" class in the original trilogy. While the ships first appear with a red and grey Republic color scheme, the "Venator" at the movie's end is gray and white; this lack of color signifies the Empire's rise to power.
"Victory"-class Star Destroyers appear as early as Brian Daley's 1979 book "Han Solo's Revenge", and since then have appeared in Expanded Universe books and games. Designed by Rendili StarDrive in the "Star Wars" universe, the "Victory"-class is 900 m long and features fewer weapons and less cargo space than an Imperial Star Destroyer.
The Super Star Destroyer "Eclipse" is the reborn Emperor Palpatine's flagship in the "Dark Empire" comic series, and it also appears in the ' expansion for '. According to sourcebooks, the "Eclipse"‍ '​s armor and shields allow it to ram Mon Calamari cruisers, and its thousands of armaments include a scaled down version of the Death Stars' superlaser cannon, capable of cracking a planet's crust. The ship is destroyed when Luke Skywalker and Leia Organa Solo combine Force abilities to cause Palpatine's Force Storms to go out of control and destroy the vessel. The "Eclipse II" serves as Palpatine's flagship until it is destroyed in "Empire's End". A scaled-down version of the "Eclipse", the "Sovereign" class, is featured in Expanded Universe material from the same time period.
"Star Wars" creator George Lucas calls the cigar-shaped Separatist cruisers visible at the beginning of "Revenge of the Sith" "Star Destroyers" in the movie's DVD commentary track. The Dark Nest trilogy by Troy Denning includes two types of Star Destroyers used by the Chiss Ascendancy and the Star Destroyer "Admiral Ackbar" used by the Galactic Alliance. The existence of the "Tector"-class is established in the "Revenge of the Sith Incredible Cross-Sections" book. The introduced "Interdictor"-class Star Destroyers that can pull ships out of, or prevent their entry into, hyperspace. The ' comics introduce the "Pellaeon"-class Star Destroyers.
Merchandise.
In 2002, Lego released a 3,104-piece "Imperial"-class Star Destroyer building kit for their Ultimate Collectors' Series line. The model is almost one meter long and includes a scale model of the Rebel Blockade Runner. In 2004, they released a Star Destroyer set for their Minis series. This one is just a few inches long and has no minifigures. In 2006, a smaller set was released, just over half a meter long and containing various minifigures. In 2010 a Midi-Scale model was also released. This includes no minifigures and is less than one foot long. Lego also released a 50 in, 3,152-piece model of the "Executor" in September 2011.
The Super Star Destroyer has also been merchandised. Kenner wanted to use a less ominous name than "Executor" for the toy playset of Darth Vader's meditation chamber. An advertisement agency's list of 153 alternatives included Starbase Malevolent, Black Coven, Haphaestus VII, and Cosmocurse; ultimately, the toy was labeled "Darth Vader's Star Destroyer". In 2006, Wizards of the Coast created an "Executor" miniature as part of its Star Wars Miniatures Starship Battles game. An electronic Super Star Destroyer toy released by Hasbro "is the rarest among Hasbro's Collector Fleet".

</doc>
<doc id="50795" url="http://en.wikipedia.org/wiki?curid=50795" title="TIE fighter">
TIE fighter

TIE fighters are fictional starfighters in the "Star Wars" universe. Propelled by Twin Ion Engines (hence the TIE acronym), TIE fighters are depicted as fast, fragile starfighters produced by Sienar Fleet Systems for the Galactic Empire. TIE fighters and other TIE craft appear in the original "Star Wars" trilogy—' (1977), ' (1980), and "" (1983)—and throughout the Star Wars Expanded Universe. Several TIE fighter replicas and toys, as well as a TIE flight simulator, have been released by merchandise companies.
Origin and design.
Industrial Light & Magic's (ILM) Colin Cantwell created the concept model that established the TIE fighter's ball-cockpit and hexagonal panels design for "A New Hope". Initially given a blue color scheme, the TIE fighter models for the first "Star Wars" movie were grey to better film against a bluescreen; TIE fighters in the next two movies shifted back to being a muted blue. Sound designer Ben Burtt created the distinctive TIE fighter sound effect by combining an elephant call with a car driving on wet pavement.
Combat scenes between TIE fighters and the "Millennium Falcon" and Rebel X-wings in "A New Hope" were meant to be reminiscent of World War II dogfight footage; editors used World War II air combat clips as placeholders while Industrial Light and Magic completed the movie's special effects. The Jedi starfighter, created for ', was designed to bridge the appearance of the Jedi starfighter in ' and the TIE fighter design from the original trilogy. The V-Wing Starfighter, seen at the end of "Revenge of the Sith," also makes the distinctive TIE fighter sound when flying by a Star Destroyer. Dark Horse Comics' Sean Cooke designed the TIE predator in "", set 130 years after the events of "A New Hope", to appear both reminiscent of and more advanced than the original TIE fighter.
Depiction.
"Star Wars" literature states that Sienar Fleet Systems manufactures TIE fighters and most TIE variants. TIE fighters' solar panels power a twin ion engine (TIE) system that accelerates gases at a high speed along almost any vector, affording the ships tremendous speed and maneuverability. Described as lacking a hyperdrive, life support, or shield generators, the fragile TIE fighters are deployed in large numbers from bases or larger ships; a Star Destroyer carries a wing of 72 various TIE craft. Although Expanded Universe material often describes the TIE fighter as lacking an ejection seat, the player can eject from TIE craft in LucasArts' "" flight simulator. Expanded Universe material holds that TIE fighter pilots, who undergo intense physical and psychological testing, are trained to be intensely loyal to Emperor Palpatine and the Empire, willing to sacrifice themselves and their wingmates to accomplish their mission. TIE pilots were seen as expendable assets, as it was far cheaper to manufacture a great deal of standardized spacecraft in overwhelming numbers than it was to properly equip said craft. 
Other TIE craft.
In addition to the TIE fighter, a variety of other TIE craft appear throughout the films.
Darth Vader flies a TIE Advanced in "A New Hope". ILM gave it a design different from the other TIE fighters to make it instantly recognizable. The next movie, "The Empire Strikes Back", introduces a TIE shuttle and TIE bombers, which ferry Captain Needa (Michael Culver) to Darth Vader's Super Star Destroyer and bomb asteroids in the hunt for the "Millennium Falcon", respectively. Both TIE craft have a design that stems from an unused "TIE boarding craft" concept developed for "A New Hope". The TIE bomber's double-hull design led ILM's modelmakers to dub the ship a "double chili dog" fighter. TIE interceptors—faster TIE fighters with dagger-shaped wings and four laser cannons—appear at various points in "Return of the Jedi". Two scales of TIE interceptor models were used during filming.
Additionally, LucasArts "Star Wars" video games introduce several TIE variants, such as the TIE Hunter starfighter in ' and the TIE Mauler surface vehicle in '. The TIE Advanced (nicknamed 'Avenger' in-game) and TIE Defender — heavily upgraded derivatives of previous craft seen in the "Star Wars" universe — first appear in "TIE Fighter" as player-pilotable craft. The plot of ' revolves around destroying the Empire's ability to manufacture the cloaking TIE Phantom starfighter, and a campaign in ' centers on destroying experimental remote-controlled TIE fighters.
"Star Wars" literature also introduces TIE varieties. Corran Horn flies a TIE clutch in "I, Jedi" and TIE raptors attack Rogue Squadron in "Solo Command". TYE wings—TIE fighter and Y-wing hybrids—appear both in "I, Jedi" and "". Dark Horse's "Dark Empire" introduces both the droid-piloted TIE/D and the TIE crawler "century tank". West End Games' roleplaying sourcebooks introduce varieties that include the TIE/fc fire-control support ship, the TIE/gt ground-attack fighter, the TIE/rc reconnaissance vessel, and the TIE scout.
Cultural impact.
A TIE fighter model used in filming the climax of "A New Hope" sold at auction for $350,000. Fans built a 16-foot-by-20-foot, 1,000-pound TIE fighter float to commemorate "Star Wars"' thirtieth anniversary as part of the 2007 Crystal Lake Gala Parade. A "Wired" editor's creation of a TIE fighter model out of Starbucks cups and stirrers prompted the magazine to create a contest for its readers to submit their own art out of similar Starbucks material.
Kenner released TIE fighter and TIE interceptor toys during the original "Star Wars" trilogy's theatrical release, and Kenner's die-cast TIE bomber is a rare collector's item. Hasbro also released TIE fighter, TIE bomber, and TIE interceptor toys. Both Kenner and Hasbro also manufactured TIE fighter pilot action figures. Lego manufactured TIE fighter, TIE bomber, TIE interceptor, TIE defender, and TIE advanced models. One of eight Lego mini-kit vehicles released in 2002 is a TIE advanced, and the pieces to all eight can be combined to create a TIE bomber. Lucasfilm members had access to a limited-edition mini-kit TIE fighter. Decipher and Wizards of the Coast published various TIE starfighter and TIE-related cards for the Star Wars Customizable Card Game and Star Wars Trading Card Game, respectively.
In 1994, LucasArts released the ' flight simulator, which casts the player as an Imperial pilot flying a variety of TIE starfighters. TIE starfighter variants are also playable in ' and appear in other LucasArts "Star Wars" titles. In 2012, Fantasy Flight Games released "", a miniatures game with pre-painted and to scale miniature X-wings and TIE fighters.

</doc>
<doc id="50798" url="http://en.wikipedia.org/wiki?curid=50798" title="Insomnia">
Insomnia

Insomnia, or sleeplessness, is a sleep disorder in which there is an inability to fall asleep or to stay asleep as long as desired. While the term is sometimes used to describe a disorder demonstrated by polysomnographic or actigraphic evidence of disturbed sleep, this sleep disorder is often practically defined as a positive response to either of two questions: "Do you experience difficulty sleeping?" or "Do you have difficulty falling or staying asleep?"
Insomnia is most often thought of as both a medical sign and a symptom that can accompany several sleep, medical, and psychiatric disorders characterized by a persistent difficulty falling asleep and/or staying asleep or sleep of poor quality. Insomnia is typically followed by functional impairment while awake. Insomnia can occur at any age, but it is particularly common in the elderly. Insomnia can be short term (up to three weeks) or long term (above 3–4 weeks); it can lead to memory problems, depression, irritability and an increased risk of heart disease and automobile related accidents.
Those who are having trouble sleeping sometimes turn to sleeping pills, which may help, but also may lead to substance dependency or addiction if used regularly for an extended period.
Insomnia can be grouped into primary and secondary, or comorbid, insomnia. Primary insomnia is a sleep disorder not attributable to a medical, psychiatric, or environmental cause. It is described as a complaint of prolonged sleep onset latency, disturbance of sleep maintenance, or the experience of non-refreshing sleep. A complete diagnosis will differentiate between free-standing primary insomnia, insomnia as secondary to another condition, and primary insomnia co-morbid with one or more conditions.
Classification.
DSM-5 criteria for insomnia.
The DSM-5 criteria for insomnia include the following:
Predominant complaint of dissatisfaction with sleep quantity or quality, associated with one (or more) of the following symptoms:
In addition,
note: The DSM-5 criteria for insomnia is intended for use by general mental health and medical clinicians (those caring for adult, geriatric, and pediatric patients).
Types of insomnia.
Insomnia can be classified as transient, acute, or chronic.
Patterns of insomnia.
Symptoms of insomnia:
Sleep-onset insomnia is difficulty falling asleep at the beginning of the night, often a symptom of anxiety disorders. Delayed sleep phase disorder can be misdiagnosed as insomnia, as sleep onset is delayed to much later than normal while awakening spills over into daylight hours.
It is common for patients who have difficulty falling asleep to also have nocturnal awakenings with difficulty returning to sleep. Two thirds of these patients wake up in middle of the night, with more than half having trouble falling back to sleep after a middle of the night awakening.
Early morning awakening is an awakening occurring earlier (more than 30 minutes) than desired with an inability to go back to sleep, and before total sleep time reaches 6.5 hours. Early morning awakening is often a characteristic of depression.
Poor sleep quality.
Poor sleep quality can occur as a result of, for example, restless legs, sleep apnea or major depression. Poor sleep quality is caused by the individual not reaching stage 3 or delta sleep which has restorative properties.
Major depression leads to alterations in the function of the hypothalamic-pituitary-adrenal axis, causing excessive release of cortisol which can lead to poor sleep quality.
Nocturnal polyuria, excessive nighttime urination, can be very disturbing to sleep.
Subjective insomnia.
Some cases of insomnia are not really insomnia in the traditional sense. People experiencing sleep state misperception often sleep for normal durations, yet severely overestimate the time taken to fall asleep. They may believe they slept for only four hours while they, in fact, slept a full eight hours.
Causes and comorbidity.
Symptoms of insomnia can be caused by or be co-morbid with:
Sleep studies using polysomnography have suggested that people who have sleep disruption have elevated nighttime levels of circulating cortisol and adrenocorticotropic hormone They also have an elevated metabolic rate, which does not occur in people who do not have insomnia but whose sleep is intentionally disrupted during a sleep study. Studies of brain metabolism using positron emission tomography (PET) scans indicate that people with insomnia have higher metabolic rates by night and by day. The question remains whether these changes are the causes or consequences of long-term insomnia.
Steroid hormones and insomnia.
Studies have been conducted with steroid hormones and insomnia. Changes in levels of cortisol, progesterone in the female cycle, or estrogen during menopause are correlated with increased occurrences of insomnia. Those with differing levels of cortisol often have long-term insomnia, where estrogen is onset insomnia catalyzed by menopause, and progesterone is temporary insomnia within the monthly female cycle.
Cortisol.
Cortisol is typically thought of as the stress hormone in humans, but it is also the awakening hormone. Analyzing saliva samples taken in the morning has shown that patients with insomnia wake up with significantly lower cortisol levels when compared to a control group with regular sleeping patterns. Further studies have revealed that those with lower levels of cortisol upon awakening also have poorer memory consolidation in comparison to those with normal levels of cortisol. Studies support that larger amounts of cortisol released in the evening occurs in primary insomnia. In this case, drugs related to calming mood disorders or anxiety, such as antidepressants, would regulate the cortisol levels and help prevent insomnia.
Estrogen.
Many postmenopausal women have reported changes in sleep patterns since entering menopause that reflect symptoms of insomnia. This could occur because of the lower levels of estrogen. Lower estrogen levels can cause hot flashes, change in stress reactions, or overall change in the sleep cycle, which all could contribute to insomnia. Estrogen treatment as well as estrogen-progesterone combination supplements as a hormone replacement therapy can help regulate menopausal women’s sleep cycle again.
Progesterone.
Low levels of progesterone throughout the female menstruation cycle, but primarily near the end of the luteal phase, have also been known to correlate with insomnia as well as aggressive behavior, irritability, and depressed mood in women. Around 67% of women have problems with insomnia right before or during their menstrual cycle. Lower levels of progesterone can, like estrogen, correlate with insomnia in menopausal women.
A common misperception is that the amount of sleep required decreases as a person ages. The ability to sleep for long periods, rather than the need for sleep, appears to be lost as people get older. Some elderly insomniacs toss and turn in bed and occasionally fall off the bed at night, diminishing the amount of sleep they receive.
Risk factors.
Insomnia affects people of all age groups but people in the following groups have a higher chance of acquiring insomnia.
Diagnosis.
In medicine, insomnia is widely measured using the Athens insomnia scale. It is measured using eight different parameters related to sleep, finally it is represented as an overall scale which assess an individual's sleep pattern
A qualified sleep specialist should be consulted in the diagnosis of any sleep disorder so the appropriate measures can be taken. Past medical history and a physical examination need to be done to eliminate other conditions that could be the cause of the insomnia. After all other conditions are ruled out a comprehensive sleep history should be taken. The sleep history should include sleep habits, medications (prescription and non-prescription), alcohol consumption, nicotine and caffeine intake, co-morbid illnesses, and sleep environment. A sleep diary can be used to keep track of the individual's sleep patterns. The diary should include time to bed, total sleep time, time to sleep onset, number of awakenings, use of medications, time of awakening and subjective feelings in the morning.
Workers who complain of insomnia should not routinely have polysomnography to screen for sleep disorders. This test may be indicated for patients with symptoms in addition to insomnia, including sleep apnea, obesity, a risky neck diameter, or risky fullness of the flesh in the oropharynx. Usually, the test is not needed to make a diagnosis, and insomnia especially for working people can often be treated by changing a job schedule to make time for sufficient sleep and by improving sleep hygiene.
Some patients may need to do a sleep study to determine if insomnia is present. The sleep study will involve the assessment tools of a polysomnogram and the multiple sleep latency test and will be conducted in a sleep center or a designated hotel. 
Specialists in sleep medicine are qualified to diagnose the many different sleep disorders. Patients with various disorders, including delayed sleep phase syndrome, are often mis-diagnosed with primary insomnia. When a person has trouble getting to sleep, but has a normal sleep pattern once asleep, a delayed circadian rhythm is the likely cause.
In many cases, insomnia is co-morbid with another disease, side-effects from medications, or a psychological problem. Approximately half of all diagnosed insomnia is related to psychiatric disorders. In depression in many cases "insomnia should be regarded as a co-morbid condition, rather than as a secondary one;" insomnia typically predates psychiatric symptoms. "In fact, it is possible that insomnia represents a significant risk for the development of a subsequent psychiatric disorder."
Knowledge of causation is not necessary for a diagnosis.
Treatment.
It is important to identify or rule out medical and psychological causes before deciding on the treatment for insomnia. Cognitive behavioral therapy (CBT) "has been found to be as effective as prescription medications are for short-term treatment of chronic insomnia. Moreover, there are indications that the beneficial effects of CBT, in contrast to those produced by medications, may last well beyond the termination of active treatment." Pharmacological treatments have been used mainly to reduce symptoms in acute insomnia; their role in the management of chronic insomnia remains unclear. Several different types of medications are also effective for treating insomnia. However, many doctors do not recommend relying on prescription sleeping pills for long-term use. It is also important to identify and treat other medical conditions that may be contributing to insomnia, such as depression, breathing problems, and chronic pain.
Non-pharmacological.
Non-pharmacological strategies have comparable efficacy to hypnotic medication for insomnia and they may have longer lasting effects. Hypnotic medication is only recommended for short-term use because dependence with rebound withdrawal effects upon discontinuation or tolerance can develop.
Non pharmacological strategies provide long lasting improvements to insomnia and are recommended as a first line and long term strategy of management. The strategies include attention to sleep hygiene, stimulus control, behavioral interventions, sleep-restriction therapy, paradoxical intention, patient education and relaxation therapy. Reducing the temperature of blood flowing to the brain slows the brain's metabolic rate thereby reducing insomnia. Some examples are keeping a journal, restricting the time spending awake in bed, practicing relaxation techniques, and maintaining a regular sleep schedule and a wake-up time. Behavioral therapy can assist a patient in developing new sleep behaviors to improve sleep quality and consolidation. Behavioral therapy may include, learning healthy sleep habits to promote sleep relaxation, undergoing light therapy to help with worry-reduction strategies and regulating the circadian clock.
EEG biofeedback has demonstrated effectiveness in the treatment of insomnia with improvements in duration as well as quality of sleep.
Sleep hygiene is a common term for all of the behaviors which relate to the promotion of good sleep. These behaviors are used as the basis of sleep interventions and are the primary focus of sleep education programs. Behaviors include the use of caffeine, nicotine and alcohol consumption, maximizing the regularity and efficiency of sleep episodes, minimizing medication usage and daytime napping, the promotion of regular exercise, and the facilitation of a positive sleep environment. Exercise can be helpful when establishing a routine for sleep but should not be done close to the time that you are planning on going to sleep. The creation of a positive sleep environment may also be helpful in reducing the symptoms of insomnia. In order to create a positive sleep environment one should remove objects that can cause worry or distressful thoughts from view.
Stimulus control therapy is a treatment for patients who have conditioned themselves to associate the bed, or sleep in general, with a negative response. As stimulus control therapy involves taking steps to control the sleep environment, it is sometimes referred interchangeably with the concept of sleep hygiene. Examples of such environmental modifications include using the bed for sleep or sex only, not for activities such as reading or watching television; waking up at the same time every morning, including on weekends; going to bed only when sleepy and when there is a high likelihood that sleep will occur; leaving the bed and beginning an activity in another location if sleep does not result in a reasonably brief period of time after getting into bed (commonly ~20 min); reducing the subjective effort and energy expended trying to fall asleep; avoiding exposure to bright light during nighttime hours, and eliminating daytime naps.
A component of stimulus control therapy is sleep restriction, a technique that aims to match the time spent in bed with actual time spent asleep. This technique involves maintaining a strict sleep-wake schedule, sleeping only at certain times of the day and for specific amounts of time to induce mild sleep deprivation. Complete treatment usually lasts up to 3 weeks and involves making oneself sleep for only a minimum amount of time that they are actually capable of on average, and then, if capable (i.e. when sleep efficiency improves), slowly increasing this amount (~15 min) by going to bed earlier as the body attempts to reset its internal sleep clock. Bright light therapy, which is often used to help early morning wakers reset their natural sleep cycle, can also be used with sleep restriction therapy to reinforce a new wake schedule. Although applying this technique with consistency is difficult, it can have a positive effect on insomnia in motivated patients.
Paradoxical intention is a cognitive reframing technique where the insomniac, instead of attempting to fall asleep at night, makes every effort to stay awake (i.e. essentially stops trying to fall asleep). One theory that may explain the effectiveness of this method is that by not voluntarily making oneself go to sleep, it relieves the performance anxiety that arises from the need or requirement to fall asleep, which is meant to be a passive act. This technique has been shown to reduce sleep effort and performance anxiety and also lower subjective assessment of sleep-onset latency and overestimation of the sleep deficit (a quality found in many insomniacs).
Meditation has been recommended for the treatment of insomnia. The meditation teacher Siddhārtha Gautama, 'The Buddha', is recorded as having recommended the practice of 'loving-kindness' meditation, or mettā bhāvanā as a way to produce relaxation and thereby, sound sleep – putting it first in a list of the benefits of that meditation. More recently, studies have concluded that: a mindfulness practice reduced mental and bodily restlessness before sleep and the subjective symptoms of insomnia; and that mindfulness-based cognitive behavioural therapy reduced restlessness, sleep effort and dysfunctional sleep-related thoughts including worry.
Cognitive behavioral therapy.
There is some evidence that cognitive behavioural therapy for insomnia is superior in the long-term to benzodiazepines and the nonbenzodiazepines in the treatment and management of insomnia. In this therapy, patients are taught improved sleep habits and relieved of counter-productive assumptions about sleep. Common misconceptions and expectations that can be modified include: (1) unrealistic sleep expectations (e.g., I need to have 8 hours of sleep each night), (2) misconceptions about insomnia causes (e.g., I have a chemical imbalance causing my insomnia), (3) amplifying the consequences of insomnia (e.g., I cannot do anything after a bad night's sleep), and (4) performance anxiety after trying for so long to have a good night's sleep by controlling the sleep process. Numerous studies have reported positive outcomes of combining cognitive behavioral therapy for insomnia treatment with treatments such as stimulus control and the relaxation therapies. Hypnotic medications are equally effective in the short-term treatment of insomnia but their effects wear off over time due to tolerance. The effects of CBT-I have sustained and lasting effects on treating insomnia long after therapy has been discontinued. The addition of hypnotic medications with CBT-I adds no benefit in insomnia. The long lasting benefits of a course of CBT-I shows superiority over pharmacological hypnotic drugs. Even in the short term when compared to short-term hypnotic medication such as zolpidem (Ambien), CBT-I still shows significant superiority. Thus CBT-I is recommended as a first line treatment for insomnia. Metacognition is also a recent trend in approach to behaviour therapy of insomnia.
Prevention.
Insomnia can be short-term or long-term. Prevention of the sleep disorder may include maintaining a consistent sleeping schedule, such as waking up and sleeping at the same times every day. Also, one should avoid caffeinated drinks during the 8 hours before sleeping time. While exercise is essential and can aid the process of sleeping, it is important to not exercise right before bedtime, therefore creating a calm environment. Lastly, one's bed should only be for sleep and sex. These are some of the points included in sleep hygiene. Going to sleep and waking up at the same time every day can create a steady pattern, which may help against insomnia.
Internet interventions.
Despite the therapeutic effectiveness and proven success of CBT, treatment availability is significantly limited by a lack of trained clinicians, poor geographical distribution of knowledgeable professionals, and expense. One way to potentially overcome these barriers is to use the Internet to deliver treatment, making this effective intervention more accessible and less costly. The Internet has already become a critical source of health-care and medical information. Although the vast majority of health websites provide general information, there is growing research literature on the development and evaluation of Internet interventions.
These online programs are typically behaviorally-based treatments that have been operationalized and transformed for delivery via the Internet. They are usually highly structured; automated or human supported; based on effective face-to-face treatment; personalized to the user; interactive; enhanced by graphics, animations, audio, and possibly video; and tailored to provide follow-up and feedback.
A number of Internet interventions for insomnia have been developed and a few of them have been evaluated as part of scientific research trials.
A paper published in 2012 reviewed the related literature and found good evidence for the use of Internet interventions for insomnia.
Medications.
Many insomniacs rely on sleeping tablets and other sedatives to get rest. In some places medications are prescribed to over 95% of insomniac cases. The percentage of adults using a prescription sleep
aid increases with age. During 2005–2010, about 4% of U.S. adults aged 20 and over reported that they took prescription sleep aids in the past 30 days. Prevalence of use was lowest among the youngest age group (those aged 20–39) at about 2%, increased to 6% among those aged 50–59, and reached 7% among those aged 80 and over. More adult women (5.0%) reported using prescription sleep aids than adult men (3.1%). Non-Hispanic white adults reported higher use of sleep aids (4.7%) than non-Hispanic black (2.5%) and Mexican-American (2.0%) adults. No difference was shown between non-Hispanic black adults and Mexican-American adults in use of prescription sleep aids. As an alternative to taking prescription drugs, some evidence shows that an average person seeking short-term help may find relief from taking over-the-counter antihistamines such as diphenhydramine or doxylamine. Certain classes of sedatives such as benzodiazepines and newer nonbenzodiazepine drugs can also cause physical dependence, which manifests in withdrawal symptoms if the drug is not carefully tapered down. The benzodiazepine and nonbenzodiazepine hypnotic medications also have a number of side-effects such as day time fatigue, motor vehicle crashes, cognitive impairments and falls and fractures. Elderly people are more sensitive to these side-effects. The non-benzodiazepines zolpidem and zaleplon have not adequately demonstrated effectiveness in sleep maintenance. Some benzodiazepines have demonstrated effectiveness in sleep maintenance in the short term but in the longer term are associated with tolerance and dependence. Drugs that may prove more effective and safer than existing drugs for insomnia is an area of active research.
Benzodiazepines and nonbenzodiazepines have similar efficacy that is not significantly more than for antidepressants. Benzodiazepines did not have a significant tendency for more adverse drug reactions. Chronic users of hypnotic medications for insomnia do not have better sleep than chronic insomniacs not taking medications. In fact, chronic users of hypnotic medications have more regular nighttime awakenings than insomniacs not taking hypnotic medications. A further review of the literature regarding benzodiazepine hypnotic as well as the nonbenzodiazepines concluded that these drugs cause an unjustifiable risk to the individual and to public health and lack evidence of long-term effectiveness. The risks include dependence, accidents, and other adverse effects. Gradual discontinuation of hypnotics in long-term users leads to improved health without worsening of sleep. It is preferred that hypnotics be prescribed for only a few days at the lowest effective dose and avoided altogether wherever possible in the elderly.
Antihistamines.
The antihistamine diphenhydramine is widely used in nonprescription sleep aids such as Benadryl. The antihistamine doxylamine is used in nonprescription sleep aids such as Unisom (USA) and Unisom 2 (Canada). In some countries, including Australia, it is marketed under the names Restavit and Dozile. It is the most effective over-the-counter sedative currently available in the United States, and is more sedating than some prescription hypnotics.
While the two drugs mentioned above are available over the counter in most countries, the effectiveness of these agents may decrease over time, and the incidence of next-day sedation is higher than for most of the newer prescription drugs. Anticholinergic side-effects may also be a draw back of these particular drugs. While addiction does not seem to be an issue with this class of drugs, they can induce dependence and rebound effects upon abrupt cessation of use.
Benzodiazepines.
The most commonly used class of hypnotics prescribed for insomnia are the benzodiazepines. Benzodiazepines all bind unselectively to the GABAA receptor. But certain benzodiazepines (hypnotic benzodiazepines) have significantly higher activity at the α1 subunit of the GABAA receptor compared to other benzodiazepines (for example, triazolam and temazepam have significantly higher activity at the α1 subunit compared to alprazolam and diazepam, making them superior sedative-hypnotics – alprazolam and diazepam in turn have higher activity at the α2 subunit compared to triazolam and temazepam, making them superior anxiolytic agents). Modulation of the α1 subunit is associated with sedation, motor-impairment, respiratory depression, amnesia, ataxia, and reinforcing behavior (drug-seeking behavior). Modulation of the α2 subunit is associated with anxiolytic activity and disinhibition. For this reason, certain benzodiazepines are better suited to treat insomnia than others. Hypnotic benzodiazepines include drugs such as temazepam, clonazepam, lorazepam, oxazepam, diazepam, flunitrazepam, triazolam, flurazepam, midazolam, nitrazepam, and quazepam. These drugs can lead to tolerance, physical dependence, and the benzodiazepine withdrawal syndrome upon discontinuation, especially after consistent usage over long periods of time. Benzodiazepines, while inducing unconsciousness, actually worsen sleep as they promote light sleep while decreasing time spent in deep sleep. A further problem is, with regular use of short-acting sleep aids for insomnia, daytime rebound anxiety can emerge. Benzodiazepines can help to initiate sleep and increase sleep time, but they also decrease deep sleep and increase light sleep. Although there is little evidence for benefit of benzodiazepines in insomnia and evidence of major harm, prescriptions have continued to increase. There is a general awareness that long-term use of benzodiazepines for insomnia in most people is inappropriate and that a gradual withdrawal is usually beneficial due to the adverse effects associated with the long-term use of benzodiazepines and is recommended whenever possible.
Non-benzodiazepines.
Nonbenzodiazepine sedative-hypnotic drugs, such as zolpidem (Ambien), zaleplon, zopiclone (Imovane), and eszopiclone (Lunesta), are a class hypnotic medications indicated for mild to moderate insomnia. Their effectiveness at improving time to sleeping is slight.
Orexin antagonists.
Suvorexant is a recently FDA approved treatment for insomnia, characterized by difficulties with sleep onset and/or sleep maintenance. It exerts its therapeutic effect in insomnia through antagonism of orexin receptors. The orexin neuropeptide signaling system is a central promoter of wakefulness. Blocking the binding of wake-promoting neuropeptides orexin A and orexin B to receptors orexin receptor type 1 and orexin receptor type 2 is thought to suppress wake drive. Two other dual orexin antagonists currently in clinical trials are Filorexant and SB-649,868.
Antidepressants.
Some antidepressants such as amitriptyline, doxepin, mirtazapine, and trazodone can have a sedative effect, and are prescribed to treat insomnia. Amitriptyline and doxepin both have antihistaminergic, anticholinergic, and antiadrenergic properties, which contribute to their side effect profile, while mirtazapine's side effects are primarily antihistaminergic, and trazadone's side-effects are primarily antiadrenergic. Some also alter sleep architecture. As with benzodiazepines, the use of antidepressants in the treatment of insomnia can lead to withdrawal effects; withdrawal may induce rebound insomnia.
Mirtazapine is known to decrease sleep latency, promoting sleep efficiency and increasing the total amount of sleeping time in people with both depression and insomnia.
Agomelatine a novel melatonergic antidepressant with sleep-improving qualities that does not cause daytime drowsiness, is licensed for marketing in the European Union and TGA Australia. After trials in the United States its development for use there was discontinued in October 2011 by Novartis, who had bought the rights to market it there from the European pharmaceutical company Servier.
Melatonin.
Melatonin is a hormone synthesized by the pineal gland, secreted through the bloodstream in the dark or commonly at nighttime, in order to control the sleep cycle. It is available in some countries labeled "dietary supplement".
Evidence for ramelteon, a melatonin receptor agonist approved by the Food and Drug Administration, looks promising. It and tasimelteon increase sleep time due to a melatonin rhythm shift with no apparent negative effects on the next day. Most melatonin drugs have not been tested for longitudinal side effects.
Studies have also shown that children who are on the Autism spectrum or have learning disabilities, Attention-Deficit Hyperactivity Disorder (ADHD) or related neurological diseases can benefit from the use of melatonin. This is because they often have trouble sleeping due to their disorders. For example, children with ADHD tend to have trouble falling asleep because of their hyperactivity and, as a result, tend to be tired during most of the day. Children who have ADHD then, as well as the other disorders mentioned, may be given melatonin before bedtime in order to help them sleep.
Alcohol.
Alcohol is often used as a form of self-treatment of insomnia to induce sleep. However, alcohol use to induce sleep can be a cause of insomnia. Long-term use of alcohol is associated with a decrease in NREM stage 3 and 4 sleep as well as suppression of REM sleep and REM sleep fragmentation. Frequent moving between sleep stages occurs, with awakenings due to headaches, the need to urinate, dehydration, and excessive sweating. Glutamine rebound also plays a role as when someone is drinking; alcohol inhibits glutamine, one of the body's natural stimulants. When the person stops drinking, the body tries to make up for lost time by producing more glutamine than it needs.
The increase in glutamine levels stimulates the brain while the drinker is trying to sleep, keeping him/her from reaching the deepest levels of sleep. Stopping chronic alcohol use can also lead to severe insomnia with vivid dreams. During withdrawal REM sleep is typically exaggerated as part of a rebound effect.
Opioids.
Opioid medications such as hydrocodone, oxycodone, and morphine are used for insomnia that is associated with pain due to their analgesic properties and hypnotic effects. Opioids can fragment sleep and decrease REM and stage 2 sleep. By producing analgesia and sedation, opioids may be appropriate in carefully selected patients with pain-associated insomnia. However, dependence on opioids can lead to suffering from long time disturbance in sleep.
Antipsychotics.
The use of antipsychotics for insomnia, while common, is not recommended as the evidence does not demonstrate a benefit and the risk of adverse effects is significant. Concerns regarding side effects is greater in the elderly.
Alternative medicine.
Some insomniacs use herbs such as valerian, chamomile, lavender, cannabis, hops, "Withania somnifera", and passion-flower. -Arginine -aspartate, "S"-adenosyl--homocysteine, and delta sleep-inducing peptide (DSIP) may be also helpful in alleviating insomnia.
Epidemiology.
A survey of 1.1 million residents in the United States found that those that reported sleeping about 7 hours per night had the lowest rates of mortality, whereas those that slept for fewer than 6 hours or more than 8 hours had higher mortality rates. Getting 8.5 or more hours of sleep per night increased the mortality rate by 15%. Severe insomnia – sleeping less than 3.5 hours in women and 4.5 hours in men – also led to a 15% increase in mortality. However, most of the increase in mortality from severe insomnia was discounted after controlling for co-morbid disorders. After controlling for sleep duration and insomnia, use of sleeping pills was also found to be associated with an increased mortality rate.
The lowest mortality was seen in individuals who slept between six and a half and seven and a half hours per night. Even sleeping only 4.5 hours per night is associated with very little increase in mortality. Thus, mild to moderate insomnia for most people is associated with increased longevity and severe insomnia is associated only with a very small effect on mortality.
As long as a patient refrains from using sleeping pills, there is little to no increase in mortality associated with insomnia, but there does appear to be an increase in longevity. This is reassuring for patients with insomnia in that, despite the sometimes-unpleasantness of insomnia, insomnia itself appears to be associated with increased longevity. It is unclear why sleeping longer than 7.5 hours is associated with excess mortality.
Insomnia is 40% more common in women than in men.
Prevalence.
The National Sleep Foundation's 2002 "Sleep in America" poll showed that 58% of adults in the U.S. experienced symptoms of insomnia a few nights a week or more. Although insomnia was the most common sleep problem among about one half of older adults (48%), they were less likely to experience frequent symptoms of insomnia than their younger counterparts (45% vs. 62%), and their symptoms were more likely to be associated with medical conditions, according to the poll of adults between the ages of 55 and 84.
As explained by Thomas Roth, estimates of the prevalence of insomnia depend on the criteria used as well as the population studied. About 30% of adults report at least one of the symptoms of insomnia. When daytime impairment is added as a criterion, the prevalence is about 10%. Primary insomnia persisting for at least one month yields estimates of 6%.
Society.
The topic of insomnia is discussed in many cultural contexts.
Bibliography.
</dl>

</doc>
<doc id="50799" url="http://en.wikipedia.org/wiki?curid=50799" title="Y-wing">
Y-wing

Y-wings are fictional Rebel Alliance and New Republic starfighters in the "Star Wars" universe. They appear in ', ', and "", as well as the and the Expanded Universe's books, comics, and games.
Origin and design.
Industrial Light & Magic's Colin Cantwell, who also designed the saga's TIE fighters, initially designed the Y-wing with a large bubble turret for a gunner. However, the dome did not appear properly when filmed against bluescreen, and subsequent designs excised the turret.
Depiction.
Y-wings are described in the Expanded Universe as durable but slow tactical strike spacecraft, although notes and diagrams by the special effects crew for "Return of the Jedi" (shown in "The Art of Return of the Jedi") show the Y-wing as possessing the same speed and maneuverability as the X-wing and standard TIE fighter. The destruction of Y-wings tasked with destroying the Death Star at the Battle of Yavin in "A New Hope" leads to Luke Skywalker (Mark Hamill) firing the proton torpedoes that destroy the Empire's battle station. Only one Y-wing survives the battle.
Y-wings also participate at the Battle of Endor in "Return of the Jedi". The Expanded Universe describes several Y-wing varieties, such as a one-seater version, a two-seater (one pilot, one gunner) variety, and the "Longprobe" reconnaissance ship.
Y-Wings were later seen in "The Clone Wars" TV series as a new, prototype fighter-bomber used by the Republic as a weapon against the Separatists. General Anakin Skywalker leads a squadron of Y-Wings on one of their first missions to take out a Separatist cruiser, and are noted for their powerful shields and secondary gunner. 

</doc>
<doc id="50801" url="http://en.wikipedia.org/wiki?curid=50801" title="X-wing">
X-wing

X-wings are fictional starfighters from the original "Star Wars" trilogy and the expanded universe. They are depicted as the primary interceptor and dogfighter of the Rebel Alliance and the New Republic. The craft has been merchandised as a variety of toys and models and licensed for use in games, novels and comics.
Origin and design.
Industrial Light & Magic's (ILM) Joe Johnston sketched and Colin Cantwell built models that eventually became the final X-wing fighter in "". The X-wings were designed to appear more "traditional" than the Empire's TIE fighters. ILM built miniatures in various scales, with wing markings indicating which prop represented which pilot. When ILM fell behind on generating X-wing footage, "Star Wars" producer George Lucas and his editors temporarily used World War II dogfight footage for initial editing cuts. Each X-wing model was built around a hollow core made from surgical tubing, which allowed lighting, cooling, and electrical connectors for the wing motors to be installed and maintained. The cockpit windows were made from faceted glass so that accurate reflections could be filmed. Although the movie's initial script and novelization describe the X-wings as belonging to "Blue squadron", limitations in bluescreen photography led to the markings on the filming models, as well as the fictional squadron affiliation being changed to red.
In addition to miniatures, the production crew made a single, full-size X-wing for scenes in the Rebels' Yavin IV base hangar; combined with cardboard cutouts and careful editing, the Rebels appear to have dozens of fighters. The production crew also made a full-size X-wing cockpit that was used for all actors; the astromech droid visible behind each actor was changed for each starfighter. Background noise pitch and tone also varied between X-wings to further differentiate the characters piloting them.
The "lake" in which Luke Skywalker (Mark Hamill) crashes his X-wing in "" was only 3.5 ft deep, requiring the creation of a rig resembling the starfighter sitting in the lake at an angle. The rig was built in hinged sections so it could be manipulated by frogmen to sink or rise, a key feature for the scene when Luke fails to levitate his ship from the water.
In 1993, ILM visual effects specialist John Knoll created a proof of concept test of dogfighting X-wings and TIE fighters to demonstrate the feasibility of using commercially-available desktop computer software for simple animation work. This resulted in numerous parts of space battle scenes being "re-shot" as digital animations for original trilogy's Special Edition releases. The ARC-170 starfighter seen in "" is deliberately reminiscent of the X-wing's design.
Depiction.
The Expanded Universe states that Incom Corporation designers defected to the Rebel Alliance and handed over the X-wing's design. One design of the Z-95 Headhunter is based on an initial Johnston X-wing sketch; the Z-95 has since been described as an X-wing precursor.
According to roleplaying and other material, X-wings depicted in the movies and Expanded Universe material that takes place around the same era are equipped with four laser cannons and a pair of proton torpedo launchers. The fighter has two flight modes: one in which the wings are flat and another when they are expanded into "attack position", affording the wingtip lasers a larger fire area. "S-foil" servomotors control the transition between the two, hence the expression "to lock s-foils in attack position". Lacking an on-board navigation computer, they rely on an astromech droid to calculate hyperspace jumps. The presence of a hyperdrive and deflector shields differentiate the X-wing from the Empire's TIE fighters, emphasizing the importance the Rebels place on pilots surviving their missions. Novels and roleplaying material state that the X-wing continues to be refined and upgraded in the years beyond ""; the "XJ"-series X-wings depicted in the war against the Yuuzhan Vong have a third proton torpedo launcher, stronger lasers, and improved engines.
Merchandise and licensing.
Kenner Toys produced an X-wing toy as a complement to its action figure line in 1977, complete with pilot figure; this model was made from formed plastic and had a battery-operated light and buzzer in the forward fuselage. The "s-foils" were activated by depressing the molded astromech droid. Kenner also produced a die-cast 1:72 miniature X-Wing in 1978 and a smaller scaled version with "battle damage crash feature" for the short lived Micro Collection line in 1982.
The X-wing appeared in four Micro Machines three-packs, including the first "Star Wars" pack released, a bronzed version, and a pack of three "battle damaged" X-wings with different colored markings. The Micro Machines X-wing has also been released in two single-packs, as a promotional souvenir with German video releases, in a nine-pack of "Original Trilogy" vehicles, and once in clear plastic. The X-wing appears eight times in the Micro Machines Action Fleet toy line: Luke's starfighter on its own, with "targeter" stand, with Dagobah swamp damage, and in a double pack with a TIE Fighter, Wedge's starfighter on its own, and as a component of the Yavin Rebel Base playset, a toy based on the prototype packaged with Biggs Darklighter's starfighter, and Jek Porkins' starfighter. Lego also released several X-wing models, including a 76-piece miniature X-wing/TIE advanced kit, a 263-piece X-wing (1999/2002), a 563-piece X-wing kit with Yoda's Hut (2004), a 437-piece X-Wing (2006), and a 560-piece X-wing (2012). A 1,304-piece "Ultimate Collector's" model was released in 2000. A new "Ultimate Collector's" model with 1,559 pieces was released in 2013.
X-wings also appear in numerous "Star Wars" games and Expanded Universe stories. The player pilots an X-wing in the Atari "Star Wars" game. It is also a playable ship in numerous LucasArts games, and is the eponymous vessel in the first of several . Both the ' and ' rail shooters include X-wing levels, and X-wing squadrons are controllable units in the ' and ' strategy games. Decipher and Wizards of the Coast published X-wing and X-wing-related cards for the Star Wars Customizable Card Game and Star Wars Trading Card Game, respectively. Michael A. Stackpole and Aaron Allston wrote the "" novel series that focuses on the X-wing pilots of Rogue Squadron and Wraith Squadron, the former expanding the story of pilots like Wedge Antilles who appear in the films. Dark Horse Comics has also published an "X-Wing Rogue Squadron" series.
Cultural impact.
A model of Luke Skywalker's X-wing was among 250 "Star Wars"-related items on display at the National Air and Space Museum celebrating the franchise's twentieth anniversary. In 2007, the San Diego Tripoli Rocket Association built and launched a 23-foot-long X-wing model propelled by four rockets, which exploded seconds after launch. A life-size X-wing is suspended from the ceiling at the Star Trader gift shop in Disneyland in California. A life-sized X-wing made from Lego blocks appeared in Times Square A 3/4 size replica is on display at the Wings Over the Rockies Air & Space Museum in Denver Co 

</doc>
<doc id="50802" url="http://en.wikipedia.org/wiki?curid=50802" title="A-wing">
A-wing

A-wings are fictional Rebel Alliance and New Republic starfighters in the "Star Wars" universe. They first appear in "" and subsequently in the Star Wars Expanded Universe's books, comics, and games.
Origin and design.
The A-wing was one of two new Rebel Alliance starfighters created for "Return of the Jedi". Dubbed the "A fighter" as it was the first of the two designs created, the starfighter was intended to resemble the snowspeeders seen in The Empire Strikes Back. The model was built by Wesley Seeds and Lorne Peterson of Industrial Light & Magic, and included a pilot figurine based on a World War I German airman. A battle-damaged engine "wing" was also created, and could be snapped into place to represent a damaged fighter.
Ralph McQuarrie's A-wing concept art showed them to be blue in colouration; because of bluescreen technology limitations, this was changed to red.
Depiction.
A-wings are described as the fastest starfighters in the war between the Rebel Alliance and the Galactic Empire; however, "" states that they are poorly shielded against attack. A-wings from Green Squadron participate in the Battle of Endor depicted in "Return of the Jedi". At Endor, an A-wing piloted by Arvel Crynyd (Hilton McRae) crashes into the bridge of the Super Star Destroyer "Executor", resulting in the "Executor" crashing out of control into the second Death Star.
The A-wing's backstory shows its creation by Jan Dodonna (Alex McCrindle), based on his analysis of the role of speed in the Battle of Yavin. However, the A-wing appears in several Expanded Universe works set before or during the events of "A New Hope", including the "" flight simulator and the cartoon series. To resolve this inconsistency, Daniel Wallace created the R-22 Spearhead when writing "The New Essential Guide to Characters" in 2002, naming it the basis of the A-wing design.
The A-wing's speed is used in the "A-wing Slash" maneuver depicted in Timothy Zahn's novel "Dark Force Rising". The A-wing is also flyable in ' and the "Rogue Squadron" series, and it appears in other LucasArts "Star Wars" titles such as '.

</doc>
<doc id="50803" url="http://en.wikipedia.org/wiki?curid=50803" title="Victimless crime">
Victimless crime

A victimless crime is a term used to refer to actions that have been made illegal but which do not directly violate or threaten the rights of any other individual. It often involves consensual acts, or solitary acts in which no other person is involved. For example, in the United States, current victimless crimes include prostitution, gambling, and illicit drug use. Edwin Schur and Hugo Bedau state in their book "Victimless Crimes: Two Sides of a Controversy" that "some of these laws produce secondary crime, and all create new 'criminals' many of whom are otherwise law-abiding citizens and people in authority." 
For example, in politics, a lobbyist or an activist might use this phrase with the implication that the law in question should be abolished.
Victimless crimes are, in the harm principle of John Stuart Mill, "victimless" from a position that considers the individual as the sole sovereign, to the exclusion of more abstract bodies such as a community or a state against which criminal offenses may be directed.
Many victimless crimes begin because of a desire to obtain illegal products or services that are in high demand. Criminal penalties thus tends to limit the supply more than the demand, driving up the black-market price and creating monopoly profits for those criminals who remain in business. This "crime tariff" encourages the growth of sophisticated and well-organized criminal groups. Organized crime in turn tends to diversify into other areas of crime. Large profits provide ample funds for bribery of public officials, as well as capital for diversification.
The War on Drugs is a commonly cited example of prosecution of victimless crime. The reasoning behind this is that drug use does not directly harm other people. It is argued that the criminalization of drugs lead to highly inflated prices for drugs. For example, Bedau and Schur found in 1974 that "In England the pharmacy cost of heroin [was] 0.06 cents per grain. In the United States street price [was] $30-90 per grain." This inflation in price is believed to drive addicts to commit crimes such as theft and robbery, which are thought to be inherently damaging to society, in order to be able to purchase the drugs on which they are dependent.
In addition to the creation of a black market for drugs, the War on Drugs is argued by proponents of legalization to reduce the workforce by damaging the ability of those convicted to find work. It is reasoned that this reduction of the workforce is ultimately harmful to an economy reliant on labor. The number of drug arrests increases every year. In a poll taken by the Bureau of Justice Statistics between 1980 and 2009, "[over a] 30-year period...[arrest] rates for drug possession or use doubled for whites and tripled for blacks."
Legalization of victimless acts.
Many activities that were once considered crimes are no longer illegal in some countries, at least in part because of their status as victimless crimes. Following the 1917 Bolshevik Revolution led by Vladimir Lenin and Leon Trotsky, Russia became the first nation to legalize homosexuality. The new Bolshevik legal code contained within it the concept that, if there was no victim, there was no crime. When Joseph Stalin came to power, these changes were reversed bit by bit, until homosexuality was effectively made illegal again by the bureaucratic regime. In the UK in the 1950s, the Wolfenden report recommended the legalization of homosexuality for these reasons. Almost fifty years later, "Lawrence v. Texas" struck down Texas sodomy laws. 
Marijuana is legalized in the Netherlands. Australia only tickets for possession over 50 grams; Portugal also has this policy. Prohibition of alcohol was repealed in the United States, and there are efforts to legalize cannabis and other "illegal drugs" in many countries, including the United States. 
Prostitution is legal in many countries, including Argentina, Australia, Brazil, Canada, Colombia, Costa Rica, France, Germany, Greece, Mexico, Italy, Israel, and others, in one form or another.

</doc>
<doc id="50804" url="http://en.wikipedia.org/wiki?curid=50804" title="Consensual crime">
Consensual crime

A consensual crime is a public-order crime that involves more than one participant, all of whom give their consent as willing participants in an activity that is unlawful. Legislative bodies and interest groups sometimes rationalize the criminalization of consensual activity because they feel it offends cultural norms, or because one of the parties to the activity is considered a "victim" despite their informed consent.
Consensual crimes are sometimes described as crimes in which the victim is the state, the judicial system, or society at large and so affect the general (sometimes ideological or cultural) interests of the system, such as common sexual morality. Victimless crimes, while similar, typically involve acts that do not involve multiple persons. Drug use is typically considered a victimless crime whereas the sale of drugs between two or more persons would be a consensual crime. The fact that no persons come forward to claim injury has essentially made the two terms interchangeable in common use.
Giving consent.
When discussing consensual crime, one issue is whether all the participants are capable of giving genuine legal consent. This may not be the case if one or more of the participants are:
Examples.
The generally accepted definition of a consensual crime is a criminal act committed by two or more people, who consent to involvement, and does not involve any nonconsenting individuals. The following is a list of criminal acts in various societies at various times and in different societies, where the issue of liability hinges on consent or the lack of it:
The issue in each of these situations is the same. Society has created a formal framework of laws to prohibit types of conduct thought to be against the public interest. Laws proscribing homicide, assaults and rape are common to most cultures. Thus, when the supposed victim freely consents to be the victim in one of these crimes, the question is whether the state should make an exception from the law for this one situation. Take euthanasia as an example. If one person intentionally takes the life of another, this is usually murder. If the motive for this is to collect the inheritance, society has no difficulty in ignoring the motive and convicting the killer. But if the motive is to relieve the suffering of the victim by providing a clean death that would otherwise be denied, can society so quickly reject the motive? It is a case of balancing the harms. On the one hand, society could impose pain and suffering on the victim by forcing him or her to endure a long decline into death. Or society could permit a system for terminating life under controlled circumstances so that the victim's wishes could be respected without exposing others to the criminal system for assisting in realising those wishes.
The other situations move down the hierarchy of non-fatal and sexual assaults with society deciding whether, and in what circumstances, to offer an excuse or exculpation to those who freely participate.

</doc>
<doc id="50805" url="http://en.wikipedia.org/wiki?curid=50805" title="905">
905

Year 905 (CMV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="50806" url="http://en.wikipedia.org/wiki?curid=50806" title="White Australia policy">
White Australia policy

The term White Australia Policy comprises various historical policies that intentionally favoured immigration to Australia from certain European countries, and especially from Britain. It came to fruition in 1901 soon after the Federation of Australia, and the policies were progressively dismantled between 1949 and 1973. Australia's official First World War historian Charles Bean defined the early intentions of the policy as "a vehement effort to maintain a high Western standard of economy, society and culture (necessitating at that stage, however it might be camouflaged, the rigid exclusion of Oriental peoples)."
Competition in the goldfields between British and Chinese miners, and labour union opposition to the importation of Pacific Islanders into the sugar plantations of Queensland, reinforced the demand to eliminate or minimize low wage immigration from Asia and the Pacific Islands. Soon after Australia became a federation it passed the "Immigration Restriction Act" of 1901. The passage of this bill is considered the commencement of the White Australia Policy as Australian government policy. Subsequent acts further strengthened the policy up to the start of the Second World War. These policies effectively allowed for British migrants to be preferred over all others through the first four decades of the 20th century. During the Second World War, Prime Minister John Curtin reinforced the policy, saying "This country shall remain forever the home of the descendants of those people who came here in peace in order to establish in the South Seas an outpost of the British race."
The policy was dismantled in stages by successive governments after the conclusion of the Second World War, with the encouragement of first non-British, non-white immigration, allowing for a large multi-ethnic post-war program of immigration. The Menzies and Holt Governments effectively dismantled the policies between 1949 and 1966 and the Whitlam Government passed laws to ensure that race would be totally disregarded as a component for immigration to Australia in 1973. In 1975 the Whitlam Government passed the Racial Discrimination Act, which made racially-based selection criteria unlawful. In the decades since, Australia has maintained largescale multi-ethnic immigration. Australia's current Migration Program allows people from any country to apply to migrate to Australia, regardless of their nationality, ethnicity, culture, religion, or language, provided that they meet the criteria set out in law.
Immigration policy prior to Federation.
Gold Rush Era.
The discovery of gold in Australia in 1851 led to an influx of immigrants from all around the world. The colony of New South Wales had a population of just 200,000 in 1851, but the huge influx of settlers spurred by the goldrushes transformed the Australian colonies economically, politically and demographically. Over the next 20 years, 40,000 Chinese men and over 9,000 women (mostly Cantonese) immigrated to the goldfields seeking prosperity.
Gold brought great wealth but also new social tensions. Multiethnic migrants came to New South Wales in large numbers for the first time. Competition on the goldfields, particularly resentment among white miners towards the successes of Chinese miners, led to tensions between groups and eventually a series of significant protests and riots, including the Buckland Riot in 1857 and the Lambing Flat Riots between 1860 and 1861. Governor Hotham, on 16 November 1854, appointed a Royal Commission on Victorian goldfields problems and grievances. This led to restrictions being placed on Chinese immigration and residency taxes levied from Chinese residents in Victoria from 1855 with New South Wales following suit in 1861. These restrictions remained in force until the early 1870s.Reference does not support the argument of this paragraph
Support from the Australian Labour Movement.
The growth of the sugar industry in Queensland in the 1870s led to searching for labourers prepared to work in a tropical environment. During this time, thousands of "Kanakas" (Pacific Islanders) were brought into Australia as indentured workers. This and related practices of bringing in non-white labour to be cheaply employed was commonly termed "blackbirding" and refers to the recruitment of people through trickery and kidnappings to work on plantations, particularly the sugar cane plantations of Queensland (Australia) and Fiji. In the 1870s and 1880s, the trade union movement began a series of protests against foreign labour. Their arguments were that Asians and Chinese took jobs away from white men, worked for "substandard" wages, lowered working conditions and refused unionisation.
Objections to these arguments came largely from wealthy land owners in rural areas. It was argued that without Asiatics to work in the tropical areas of the Northern Territory and Queensland, the area would have to be abandoned. Despite these objections to restricting immigration, between 1875–1888 all Australian colonies enacted legislation which excluded all further Chinese immigration. Asian immigrants already residing in the Australian colonies were not expelled and retained the same rights as their Anglo and Southern compatriots.
Agreements were made to further increase these restrictions in 1895 following an Inter-colonial Premier's Conference where all colonies agreed to extend entry restrictions to all non-white races. However, in attempting to enact this legislation, the Governors of New South Wales, South Australia and Tasmania reserved the bills, due to a treaty with Japan, and they did not become law. Instead, the Natal Act of 1897 was introduced, restricting "undesirable persons" rather than any specific race.
The British government in London was not pleased with legislation that discriminated against certain subjects of its Empire, but decided not to disallow the laws that were passed. Colonial Secretary Joseph Chamberlain explained in 1897:
From Federation to World War II.
In writing about the preoccupations of the Australian population in early Federation Australia before the First World War, the official historian of the war, Charles Bean, considered the White Australia Policy and defined it as follows:
 "White Australia Policy"- a vehement effort to maintain a high Western standard of economy, society and culture (necessitating at that stage, however it might be camouflaged, the rigid exclusion of Oriental peoples).
 — Extract from "ANZAC to Amiens" by C. E. W. Bean.
Federation Convention and Australia's first government.
Immigration was a prominent topic in the lead up to Australian Federation. At the third Session of the Australasian Federation Convention of 1898, Western Australian premier and future federal cabinet member John Forrest summarised the prevailing feeling:
 It is of no use to shut our eyes to the fact that there is a great feeling all over Australia against the introduction of coloured persons. It goes without saying that we do not like to talk about it, but it is so.
 — 1898 Australasian Federation Convention 3rd Session Debates
The Barton Government which came to power following the first elections to the Commonwealth parliament in 1901 was formed by the Protectionist Party with the support of the Australian Labor Party. The support of the Labor Party was contingent upon restricting non-white immigration, reflecting the attitudes of the Australian Workers Union and other labour organisations at the time, upon whose support the Labor Party was founded.
The first Parliament of Australia quickly moved to restrict immigration to maintain Australia's British character, and the Pacific Island Labourers Bill and the Immigration Restriction Bill were passed shortly before parliament rose for its first Christmas recess. The Colonial Secretary in Britain had however made it clear that a race based immigration policy would run "contrary to the general conceptions of equality which have ever been the guiding principle of British rule throughout the Empire". The Barton Government therefore conceived of the "language dictation test", which would allow the government, at the discretion of the minister, to block unwanted migrants by forcing them to sit a test in "any European language". Race had already been established as a premise for exclusion among the colonial parliaments, so the main question for debate was who exactly the new Commonwealth ought to exclude, with the Labor Party rejecting Britain's calls to placate the populations of its non-white colonies and allow "aboriginal natives of Asia, Africa, or the islands thereof". There was opposition from Queensland and its sugar industry to the proposals of the Pacific Islanders Bill to exclude "Kanaka" laborers, however Barton argued that the practice was "veiled slavery" that could lead to a "negro problem" similar to that in the United States, and the Bill was passed.
Immigration Restriction Act 1901.
The new Federal Parliament, as one of its first pieces of legislation, passed the Immigration Restriction Act 1901 to "place certain restrictions on immigration and... for the removal... of prohibited immigrants". The Act drew on similar legislation in South Africa. Edmund Barton, the prime minister, argued in support of the Bill with the following statement: "The doctrine of the equality of man was never intended to apply to the equality of the Englishman and the Chinaman."
The Attorney General tasked with drafting the legislation was Alfred Deakin. Deakin supported Barton's position over that of the Labor Party in drafting the Bill (the ALP wanted more direct methods of exclusion than the dictation test) and redacted the more vicious racism proposed for the text in his Second Reading of the Bill. In seeking to justify the policy, Deakin said he believed that the Japanese and Chinese might be a threat to the newly formed federation and it was this belief that led to legislation to ensure they would be kept out:
Early drafts of the Act explicitly banned non-Europeans from migrating to Australia but objections from the British government, which feared that such a measure would offend British subjects in India and Britain's allies in Japan, caused the Barton government to remove this wording. Instead, a "dictation test" was introduced as a device for excluding unwanted immigrants. Immigration officials were given the power to exclude any person who failed to pass a 50-word dictation test. At first this was to be in any European language, but was later changed to include "any" language. The tests were written in such a way to make them nearly impossible to pass. The first of these tests was written by Federal MP Stewart Parnaby as an example for officers to follow when setting future tests. The "Stewart" test was unofficially standardised as the English version of the test, due to its extremely high rates of failure resulting from a very sophisticated use of language. While specifically asked by Barton to carry out this task, Parnaby allegedly shared similar views to Donald Cameron despite never publically admitting so .
The legislation found strong support in the new Australian Parliament, with arguments ranging from economic protection to outright racism. The Labor Party wanted to protect "white" jobs and pushed for more explicit restrictions. A few politicians spoke of the need to avoid hysterical treatment of the question. Member of Parliament Bruce Smith said he had "no desire to see low-class Indians, Chinamen or Japanese...swarming into this country... But there is obligation...not (to) unnecessarily offend the educated classes of those nations" Donald Cameron, a member from Tasmania, expressed a rare note of dissention:
 [N]o race on... this earth has been treated in a more shameful manner than have the Chinese... They were forced at the point of a bayonet to admit Englishmen... into China. Now if we compel them to admit our people... why in the name of justice should we refuse to admit them here?
 — Donald Cameron (Free Trade Party)
Outside parliament, Australia's first Catholic cardinal, Patrick Francis Moran was politically active and denounced anti-Chinese legislation as "unchristian". The popular press mocked the cardinal's position and the small European population of Australia generally supported the legislation and remained fearful of being overwhelmed by an influx of non-British migrants from the vastly different cultures of the highly populated empires to Australia's north.
Pacific Island Labourers Act 1901.
In 1901 the Australian parliament passed the Pacific Island Labourers Act 1901. The result of this legislation was that 7,500 Pacific Islanders (called "Kanakas") working mostly on plantations in Queensland were deported and entry into Australia by Pacific Islanders after 1904 was prohibited.
The Paris Peace Conference.
At the 1919 Paris Peace Conference following World War 1, Japan sought to include a racial equality clause in the Covenant of the League of Nations. Japanese policy reflected their desire to remove or to ease the immigration restrictions against Japanese (especially in the United States and Canada), which Japan regarded as a humiliation and affront to its prestige.
Australian Prime Minister Billy Hughes was already concerned by the prospect of Japanese expansion in the Pacific. Australia, Japan and New Zealand had seized the German colonial empire's territories in the Pacific in the early stages of the War and Hughes was concerned to retain German New Guinea as vital to the defence of Australia. The Treaty ultimately granted Australia a League of Nations Mandate over German New Guinea and Japan to the South Pacific Mandate immediately to its north - thus bringing Australian and Japanese territory to a shared border - a situation altered only by Japan's World War II invasion of New Guinea.
Hughes vehemently opposed Japan's racial equality proposition. Hughes recognised that such a clause would be a threat to White Australia and made it clear to British Prime Minister David Lloyd George that he would leave the conference if the clause was adopted. When the proposal failed, Hughes reported in the Australian parliament:
"The White Australia is yours. You may do with it what you please, but at any rate, the soldiers have achieved the victory and my colleagues and I have brought that great principle back to you from the conference, as safe as it was on the day when it was first adopted."
Stanley Bruce.
Australian Prime Minister Stanley Bruce was a supporter of the White Australia Policy, and made it an issue in his campaign for the 1925 Australian Federal election.
Abolition of the policy.
World War II.
Australian anxiety at the prospect of Japanese expansionism and war in the Pacific continued through the 1930s. Billy Hughes, by then a minister in the United Australia Party's Lyons Government, made a notable contribution to Australia's attitude towards immigration in a 1935 speech in which he argued that "Australia must... populate or perish". However Hughes was forced to resign in 1935 after his book "Australia and the War Today" exposed a lack of preparation in Australia for what Hughes correctly supposed to be a coming war.
Between the Great Depression starting in 1929 and the end of World War II in 1945, global conditions kept immigration to very low levels. At the start of the war, Prime Minister John Curtin (ALP) reinforced the message of the White Australia Policy by saying: "This country shall remain forever the home of the descendants of those people who came here in peace in order to establish in the South Seas an outpost of the British race."
Following the 1942 Fall of Singapore, Australians feared invasion by Imperial Japan. Australian cities were bombed by the Japanese Airforce and Navy and Axis Naval Forces menaced Australian shipping, while the Royal Navy remained pre-occupied with the battles of the Atlantic and Mediterranean in the face of Nazi aggression in Europe. A Japanese invasion fleet headed for the Australian Territory of New Guinea was only halted by the intervention of the United States Navy in the Battle of the Coral Sea. Australia received thousands of refugees from territories falling to advancing Japanese forces - notably thousands of Dutch who fled the Dutch East Indies (now Indonesia). Australian Aborigines, Torres Strait Islanders, Papua New Guineans and Timorese served in the frontline of the defence of Australia, bringing Australia's racially discriminatory immigration and political rights policies into focus and wartime service gave many Indigenous Australians confidence in demanding their rights upon return to civilian life.
Post-war immigration.
Following the trauma of World War II, Australia's vulnerability during the Pacific War and its small population led to policies summarised by the slogan, "populate or perish". According to author Lachlan Strahan, this was an ethnocentric slogan that in effect was an admonition to fill Australia with Europeans or else risk having it overrun by Asians. Calwell stated in 1947 to critics of the government's mass immigration program: "We have 25 years at most to populate this country before the yellow races are down on us." 
During the war, many non-white refugees, including Malays, Indonesians, and Filipinos, arrived in Australia, but Immigration Minister Arthur Calwell controversially sought to have them all deported. In 1948, Iranian Bahá'ís seeking to immigrate to Australia were classified as "Asiatic" by the policy and were denied entry. In 1949, Calwell's successor Harold Holt allowed the remaining 800 non-white refugees to apply for residency, and also allowed Japanese "war brides" to settle in Australia. In the meantime, encouraging immigration from Europe, Australia admitted large numbers of immigrants from mostly Italy, Greece, and Yugoslavia, as well as its traditional source of the British Isles. Ambitious post-war development projects like the Snowy Mountains Scheme (1949–1972) required a large labour force that could only be sourced by diversifying Australia's migrant intake.
Relaxation of restrictions.
Australian policy began to shift towards significantly increasing immigration. Legislative changes over the next few decades continuously opened up immigration in Australia.
Labor Party Chifley Government:
Liberal-Country Party Menzies Government (1949-1966):
In 1963, a paper "Immigration: Control or Colour Bar?" was published by a group of students and academics at Melbourne University. It proposed eliminating the White Australia policy, and was influential towards this end.
End of the White Australia Policy.
In 1966, the Holt Liberal Government introduced the "Migration Act, 1966", a watershed moment in immigration reform, it effectively dismantled the White Australia Policy and increased access to non-European migrants, including refugees fleeing the Vietnam War. After a review of the European policy in March 1966, Immigration Minister Hubert Opperman announced applications for migration would be accepted from well-qualified people on the basis of their suitability as settlers, their ability to integrate readily and their possession of qualifications positively useful to Australia. At the same time, Harold Holt's government decided a number of "temporary resident" non-Europeans, who were not required to leave Australia, could become permanent residents and citizens after five years (the same as for Europeans).
As a result, annual non-European settler arrivals rose from 746 in 1966 to 2,696 in 1971, while annual part-European settler arrivals rose from 1,498 to 6,054.
The legal end of the White Australia policy is usually placed in the year 1973, when the Whitlam Labor government implemented a series of amendments preventing the enforcement of racial aspects of the immigration law. These amendments:
The 1975 Racial Discrimination Act made the use of racial criteria for any official purpose illegal.
It was not until the Fraser Liberal government's review of immigration law in 1978 that all selection of prospective migrants based on country of origin was entirely removed from official policy.
In 1981 the Minister for Immigration announced a Special Humanitarian Assistance Program (SHP) for Iranians to seek refuge in Australia and by 1988 some 2500 Bahá'ís and many more others had arrived in Australia through either SHP or Refugee Programs. The last selective immigration policy, offering relocation assistance to British nationals, was finally removed in 1982.
Aftermath.
Australia's contemporary immigration program has two components: a program for skilled and family migrants and a humanitarian program for refugees and asylum seekers. By 2010, the post-war immigration program had received more than 6.5 million migrants from every continent. The population tripled in the six decades to around 21 million in 2010, comprising people originating from 200 countries.
Legacy.
In addition to the obvious demographic effect of creating a population of European, and largely Anglo-Celtic, descent, by effectively limiting the immigration of practitioners of non-Christian faiths, the White Australia policy ensured that Christianity remained the religion of the overwhelming majority of Australians. While non-European and non-Christian immigration has increased substantially since the dismantling of the White Australia policy, Australian society inevitably remains rooted in the demographic legacy of the 72 years of White Australia, during which time the country underwent its most substantial population growth.
Contemporary demographics.
The 2001 Australian census results indicate that many Australians claim some European heritage: English 37%, Irish 11%, Italian 5%, German 4.3%, Scottish 3%, Greek 2%, Former Yugoslav 1.8%, Dutch 1.5%, Polish 0.9%. Australians of some non-European origin form a significant but still relatively small part of the population: Chinese 3.2%, Indian 0.9%, Lebanese 0.9%, Vietnamese 0.9%. About 2.2% identified themselves as Indigenous Australians. 39% of the population gave their ancestry as "Australian". The Australian census does not classify people according to race, only ethnic ancestry. (Note that subjects were permitted to select more than one answer for this census question.)
15% of the population now speaks a language other than English at home. The most commonly spoken languages are Italian, Greek, Cantonese Chinese and Arabic.
Political and social legacy.
The story of Australia since the Second World War - and particularly since the final relegation of the white Australia Policy - has been one of ever-increasing ethnic and cultural diversity. Successive governments have sustained a large programmes of multiethnic immigration from all continents.
Discrimination on the basis of race or ethnicity was legally sanctioned until 1975. Australia's new official policy on racial diversity is: "to build on our success as a culturally diverse, accepting and open society, united through a shared future". The White Australia Policy continues to be mentioned in modern contexts, although it is generally only mentioned by politicians when denouncing their opposition. As Leader of the Opposition, John Howard, argued for restricting Asian immigration in 1988, as part of his One Australia policy, later admitting that his comments cost him his job at the time:
Howard later retracted and apologised for the remarks, and was returned to the leadership of the Liberal Party in 1995. The Howard Government (1996–2007) in turn ran a large programme of non-discriminatory immigration and, according to the Australian Bureau of Statistics, Asian countries became an increasingly important source of immigration over the decade from 1996 to 2006, with the proportion of migrants from Southern and Central Asian countries doubling from 7% to 14%. The proportion of immigrants from Sub-Saharan Africa also increased. By 2005–06, China and India were the third and fourth largest sources of all migration (after New Zealand and the United Kingdom). In 2005–06, there were 180,000 permanent additions of migrants to Australia (72% more than the number in 1996–97). This figure included around 17,000 through the humanitarian programme, of whom Iraqis and Sudanese accounted for the largest portions. China became Australia's biggest source of migrants, for the first time in 2009, surpassing New Zealand and Britain.
Despite the overall success and generally bipartisan support for Australia's multi-ethnic immigration programme, there remain voices of opposition to immigration within the Australian electorate. At its peak, Pauline Hanson's One Nation Party received 9% of the national vote at the 1998 Federal Election.
Hanson was widely accused of trying to take Australia back to the days of the White Australia Policy, particularly through reference to Arthur Calwell, one of the policy's strongest supporters. In her maiden address to the Australian Parliament following the 1996 Election, Hanson said:
Hanson's remarks generated wide interest in the media both nationally and internationally, but she herself did not retain her seat in Parliament at the 1998 Election or subsequent 2001 and 2004 Federal Elections. Hanson also failed to win election in the 2003 and 2011 New South Wales State Elections. In May 2007, Pauline Hanson, with her new Pauline's United Australia Party, continued her call for a freeze on immigration, arguing that African migrants carried disease into Australia.
Topics related to racism and immigration in Australia are still regularly connected by the media to the White Australia Policy. Some examples of issues and events where this connection has been made include:
reconciliation with Aborigines; mandatory detention and the "Pacific Solution"; the 2005 Cronulla riots, and the 2009 attacks on Indians in Australia. Former opposition Labor party leader Mark Latham, in his book "The Latham Diaries", described the ANZUS alliance as a legacy of the White Australia policy.
In 2007, the Howard Government proposed an Australian Citizenship Test intended "to get that balance between diversity and integration correct in future, particularly as we now draw people from so many different countries and so many different cultures". The draft proposal contained a pamphlet introducing Australian history, Culture and Democracy. Migrants were to be required to correctly answer at least 12 out of 20 questions on such topics in a citizenship quiz. Migrants would also be required to demonstrate an adequate level of understanding of the English language. The Rudd Government reviewed and then implemented the proposal in 2009.
Australian government policy from earlier years has been claimed to be the original impetus for the apartheid system in South Africa.

</doc>
<doc id="50807" url="http://en.wikipedia.org/wiki?curid=50807" title="River Severn">
River Severn

The River Severn (Welsh: "Afon Hafren", Latin: "Sabrina") is the longest river in the United Kingdom, at about 220 mi, and the second longest in the British Isles, behind the River Shannon in Ireland. It rises at an altitude of 2001 ft on Plynlimon, Ceredigion near Llanidloes, Powys, in the Cambrian Mountains of mid Wales. It then flows through Shropshire, Worcestershire and Gloucestershire, with the county towns of Shrewsbury, Worcester and Gloucester on its banks. With an average discharge of 107 m³/s at Apperley, Gloucestershire, the Severn is the greatest river in terms of water flow in England and Wales.
The river is usually considered to become the Severn Estuary after the Second Severn Crossing between Severn Beach, South Gloucestershire and Sudbrook, Monmouthshire. The river then discharges into the Bristol Channel which in turn discharges into the Celtic Sea and the wider Atlantic Ocean. The Severn's drainage basin area is 4409 sqmi, excluding the River Wye and Bristol Avon which flow into the Severn Estuary. The major tributaries to the Severn are the Vyrnwy, Clywedog, Teme, Warwickshire Avon and Stour.
Etymology and mythology.
The name Severn is thought to derive from a Celtic original name "*sabrinn-â", of uncertain meaning. That name then developed in different languages to become "Sabrina" to the Romans, "Hafren" in Welsh, and Severn in English. A folk etymology later developed, deriving the name from a mythical story of a nymph, Sabrina, who drowned in the river. Sabrina is also the goddess of the River Severn in Brythonic mythology. The story of Sabrina is featured in Milton's "Comus". There is a statue of 'Sabrina' in the Dingle Gardens at the Quarry, Shrewsbury, as well as a metal sculpture erected in 2013 also in the town. As the Severn becomes tidal the associated deity changed to Nodens, who was represented mounted on a seahorse, riding on the crest of the Severn bore.
Tributary rivers.
The River Stour rises in the north of Worcestershire in the Clent Hills, near St Kenelm's Church at Romsley. It flows north into the adjacent West Midlands at Halesowen. It then flows westwards through Cradley Heath and Stourbridge where it leaves the Black Country. It is joined by the Smestow Brook at Prestwood before it winds around southwards to Kinver, and then flows back into Worcestershire. It then passes through Wolverley, Kidderminster and Wilden to its confluence with the Severn at Stourport-on-Severn.
The River Vyrnwy, which begins at Lake Vyrnwy, flows eastwards through Powys before forming part of the border between England and Wales, joining the Severn near Melverley, Shropshire. The Rea Brook flows north from its source in the Stiperstones and joins the Severn at Shrewsbury. The River Tern, after flowing south from Market Drayton and being joined by the River Meese and the River Roden, meets the Severn at Attingham Park.
The River Worfe joins the Severn, just above Bridgnorth. The River Stour rising on the Clent Hills and flowing through Halesowen, Stourbridge, and Kidderminster, joins the Severn at Stourport. On the opposite bank, the tributaries are only brooks, Borle Brook, Dowles Brook draining the Wyre Forest, Dick Brook and Shrawley Brook.
The River Teme flows eastwards from its source in Mid Wales, straddling the border between Shropshire and Herefordshire, it is joined by the River Onny, River Corve and River Rea before it finally joins the Severn slightly downstream of Worcester. Shit Brook near Much Wenlock was culverted to flow into the Severn.
One of the several rivers named Avon, in this case the Warwickshire Avon, flows west through Rugby, Warwick and Stratford-upon-Avon. It is then joined by its tributary the River Arrow, before finally joining the Severn at Tewkesbury, Gloucestershire.
The port of Bristol is located on the Severn Estuary, where another River Avon flows into it through the Avon Gorge.
The River Wye, from its source in Plynlimon, Wales (2 mi from the source of the Severn), flows generally south east through the Welsh towns of Rhayader and Builth Wells. It enters Herefordshire, flows through Hereford, and is shortly afterwards joined by the River Lugg, before flowing through Ross-on-Wye and Monmouth, and then southwards where it forms part of the boundary between England (Forest of Dean) and Wales. It flows into the Severn near the town of Chepstow, slightly upstream of the Bristol Avon on the opposite bank.
The River Usk flows into the Severn Estuary just south of Newport.
The Rad Brook is a small river in Shropshire, England. It flows through Shrewsbury and enters the River Severn there.
Major settlements.
Below is a list of major towns and cities that the Severn flows through (in order running downstream):
Through Powys:
Through Shropshire:
Through Worcestershire:
Through Gloucestershire:
Transport.
Bridges.
The Severn is bridged at many places, and many of these bridges are notable in their own right, with several designed and built by the engineer Thomas Telford. There also is the famous Iron Bridge at Ironbridge, which was the world's first iron arch bridge.
The two road bridges of the Severn crossing link Wales with the southern counties of England.
Prior to the construction of the first bridge in 1966, the channel was crossed by the Aust Ferry.
Other notable bridges include:
Rail.
The Severn Tunnel, completed in 1886 by John Hawkshaw on behalf of the Great Western Railway, lies near the Second Severn Crossing road bridge, and carries the South Wales Main Line section of the Great Western Main Line under the channel. The original line built before the Severn Tunnel was the South Wales Railway from Gloucester, that followed the estuary alongside present day stations of Lydney, Chepstow, Caldicot and Severn Tunnel Junction to Newport.
Cars could also be transported through the Severn Tunnel. In the 1950s three trains a day made round trips between Severn Tunnel Junction and Pilning. The vehicles were loaded onto open flat bed carriages and pulled by a small pannier tank locomotive, although sometimes they were joined to a scheduled passenger train. The prudent owner paid to cover the vehicle with a sheet, as sparks often flew when the steam locomotive tackled the slope leading to the tunnel exit. A railway coach was provided for passengers and drivers. Reservations could be made and the fee for the car was about thirty shillings (£1.50) in the early 1950s.
Disasters.
There have been many disasters on the Severn and it has claimed many lives (figures vary depending on how it is recorded, circa 300 people), especially during the 20th century. The Severn Railway Bridge was badly damaged by the collision of two river barges in 1960, which led to its demolition in 1970. Five crew members of both the Arkendale H and Wastdale H died in the accident. More recently the river flooded during the 2007 United Kingdom floods.
Navigation.
There is a public right of navigation between Pool Quay, near Welshpool, and Stourport. However this stretch of the river has little traffic, other than small pleasure boats, canoes and some tour boats in Shrewsbury. Below Stourport, where the river is more navigable for larger craft, users must obtain permits from the Canal & River Trust, who are the navigation authority. During spring freshet the river can be closed to navigation.
At Upper Parting above Gloucester, the river divides into two, and flows either side of Alney Island to Lower Parting. The West Channel is no longer navigable. The East Channel is navigable as far as Gloucester Docks, from where the Gloucester and Sharpness Canal provides a navigable channel south. Between the docks and Lower Parting Llanthony Weir marks the Normal Tidal Limit (NTL) of the East Channel of the river.
In the tidal section of the river below Gloucester, the Gloucester Harbour Trustees are the competent harbour authority. The Trustees maintain navigation lights at various points along the river (including on Chapel Rock and Lyde Rock, and leading lights at Slime Road, Sheperdine and Berkeley Pill).
Locks.
There are locks on the lower Severn to enable seagoing boats to reach as far as Stourport. The most northerly lock is at Lincombe, about a mile downstream from Stourport.
Associated canals.
The Staffordshire and Worcestershire Canal, the Worcester and Birmingham Canal, (both narrow beam) and the Herefordshire and Gloucestershire Canal join the Severn at Stourport, Worcester and Gloucester respectively. The Droitwich Barge Canal, a broad beam canal, joins the Severn at Hawford, near to the River Salwarpe, and connects to the Droitwich Canal (narrow beam) in the name town, which then forms a link to the Worcester and Birmingham Canal. The two Droitwich canals re-opened in 2010 after major restoration.
The Gloucester and Sharpness Canal connects the Severn at Gloucester to the Severn at Sharpness, avoiding a stretch of the tidal river which is dangerous to navigate. The Stroudwater Navigation used to join the tidal Severn at Framilode, but since the 1920s has connected to the Severn only via the Gloucester and Sharpness Canal.
The Lydney Canal is a short canal which connects Lydney to the river.
The section of the river between Tewkesbury and Worcester forms part of the Avon Ring, a 109 mi circular cruising route which includes 129 locks and covers parts of three other waterways.
Passenger transport.
The tidal river.
Paddle steamers were operated in the Severn Estuary from the mid 19th century to the late 1970s by P and A Campbell of Bristol. Since 1986 Waverley Excursions has operated occasional sailings to Sharpness and Lydney by the "MV Balmoral".
A number of ferries were also operated on the tidal river, for example at New Passage, Purton and Arlingham. The last ferry was the Aust Ferry, which closed in 1966 when the Severn Bridge opened. One of the Aust ferries, "Severn Princess", is still in Chepstow although largely derelict.
The upper river.
In Worcester, the Worcester River Cruises have boat trips up and down the river between Tewkesbury and Stourport, operating the boats "The Pride of the Midlands" and "The Earl Grosvenor".
The Cathedral Ferry, a foot passenger ferry, also operates on summer weekends from the steps of Worcester Cathedral.
In Shropshire the Hampton Loade Ferry operates across the river.
In Shrewsbury, boat trips around the loop of the town centre are at present provided by the "Sabrina" and depart from Victoria Quay near the Welsh Bridge during the summer.
Severn Estuary.
The river becomes tidal at Maisemore, on the West Channel just north of Gloucester, and at Llanthony Weir on the East Channel. The tidal river downstream from Gloucester is sometimes referred to as the Severn Estuary, but the river is usually considered to become the Severn Estuary after the Second Severn Crossing near Severn Beach, South Gloucestershire (the point to which the jurisdiction of the Gloucester Harbour Trustees extends), or at Aust, the site of the Severn Bridge.
The Severn Estuary extends to a line from Lavernock Point (south of Cardiff) to Sand Point near Weston-super-Mare. West of this line is the Bristol Channel. In the Severn Estuary (or the Bristol Channel in the last two cases, depending where the boundary is drawn) are the rocky islands called Denny Island, Steep Holm and Flat Holm.
The estuary is about 2 mi wide at Aust, and about 9 mi wide between Cardiff and Weston-super-Mare.
Severn Sea.
Until Tudor times the Bristol Channel was known as the Severn Sea, and it is still known as this in both Welsh and Cornish (Môr Hafren and Mor Havren respectively, with Môr meaning Sea).
During the 18th century, the Devon, Somerset and Exmoor coastline was full of smugglers. Coastguards patrolled the shores all night in all weathers, one man for every ¼ mile of the path. Almost every officer and man in the Royal Navy must have taken part either in smuggling or in its prevention. The resulting skill in foul weather seamanship and coastal raiding certainly contributed to the Navy's success against Napoleon Bonaparte.
The views of the Severn Sea also inspired the poet Samuel Taylor Coleridge while he was living at Nether Stowey on the edge of the Quantock hills. Three times he completed the 45-mile trek from the Quantocks to Lynton in a single day.
Severn bore.
A curious phenomenon associated with the lower reaches of the Severn is the tidal bore, which forms somewhat upstream of the port of Sharpness.
It is frequently asserted that the river's estuary, which empties into the Bristol Channel, has the second largest tidal range in the world—48 ft, exceeded only by the Bay of Fundy. However a tidal range greater than that of the Severn is recorded from the lesser known Ungava Bay in Canada. During the highest tides, the rising water is funnelled up the Severn estuary into a wave that travels rapidly upstream against the river current. The largest bores occur in spring, but smaller ones can be seen throughout the year. The bore is accompanied by a rapid rise in water level which continues for about one and a half hours after the bore has passed.
Industry.
A 3 mi stretch of the River Severn in Shropshire, is known as Ironbridge Gorge. It was designated a World Heritage Site by UNESCO in 1986. Its historic importance is due to its role as the centre of the iron industry in the early stages of the Industrial Revolution. The gorge and the village of Ironbridge get their name from the Iron Bridge across the Severn, built in 1779, which was the first cast-iron arch bridge ever constructed.
Wildlife.
The sides of the estuary are also important feeding grounds for waders, notably at the Bridgwater Bay National Nature Reserve and the Slimbridge Wildfowl Trust. River shingle habitat can also be found on the lower estuary, notable for its population of the endangered 5-spot Ladybird.
Literary and musical allusions.
The River Severn is named several times in A.E. Housman's "A Shropshire Lad" (1896): "It dawns in Asia, tombstones show/And Shropshire names are read;/And the Nile spills his overflow/Beside the Severn's dead" (“1887"); "Severn stream" (“The Welsh Marches"); and "Severn shore" (“Westward from the high-hilled plain...”).
In William Shakespeare's "Henry IV, Part 1," Henry Hotspur Percy recalls the valor of Edmund Mortimer, 5th Earl of March in a long battle against Welshman Owain Glyndŵr upon the Severn's banks, claiming the flooding Severn "affrighted with (the warriors') bloody looks ran fearfully among the trembling reeds and hid his crisp head in the hollow bank, bloodstained with these valiant combatants."
The Severn was the inspiration for a number of works by Gloucestershire composer Ivor Gurney, including the songs "Western Sailors" (1925) and "Severn Meadows" (1917).
Gloucestershire writer and poet Brian Waters published "Severn Tide" with J. M. Dent in 1947 and followed it with"Severn Stream" in 1949. With anecdotal stories about his travels, both books tell of the lives of the people who lived and worked on and along the river, describing the landscape with a poet's eye. Waters links Nodens with the Seven Bore and the association of the Celtic deity with the river is explored at length by Rogers.
The English composer Gerald Finzi (1901-1956) wrote "A Severn Rhapsody," his Opus 3, in 1923; taking the Severn River and its surrounding countryside as his inspiration.
The Severn is often mentioned in Ellis Peters' Cadfael novels, set in or around Shrewsbury Abbey, beside the river.

</doc>
<doc id="50808" url="http://en.wikipedia.org/wiki?curid=50808" title="Andaman Sea">
Andaman Sea

The Andaman Sea (Bengali: আন্দামান সাগর; Hindi: अंडमान सागर) is a body of water to the southeast of the Bay of Bengal, south of Burma (Myanmar), west of Thailand, north-west of Malay Peninsula, north of Sumatra and east of the Andaman Islands, India; it is part of the Indian Ocean.
The sea has been traditionally used for fishery and transportation of goods between the coastal countries and its coral reefs and islands are popular tourist destinations. The fishery and tourist infrastructure was severely damaged by the 2004 Indian Ocean earthquake and tsunami.
Extent.
At its southeastern reaches, the Andaman Sea narrows to form the Straits of Malacca, which separate the Malay Peninsula from the island of Sumatra.
The International Hydrographic Organization defines the limits of the "Andaman Sea" as follows::p.21
"On the Southwest." A line running from Oedjong Raja () in Sumatra to Poeloe Bras (Breuëh) and on through the Western Islands of the Nicobar Group to Sandy Point in Little Andaman Island, in such a way that all the narrow waters appertain to the Andaman Sea.
"On the Northwest." The Eastern limit of the Bay of Bengal [A line running from Cape Negrais (16°03'N) in Burma through the larger islands of the Andaman group, in such a way that all the narrow waters between the islands lie to the Eastward of the line and are excluded from the Bay of Bengal, as far as a point in Little Andaman Island].
"On the Southeast." A line joining Lem Voalan (7°47'N) in Siam [Thailand], and Pedropunt (5°40'N) in Sumatra.
Geology.
The average depth of the sea is about 1,000 m. The northern and eastern parts are shallower than 180 m due to the silt deposited by the Irrawaddy River. This major river flows into the sea from the north through Burma. The western and central areas are 900–3,000 meters deep (3,000–10,000 ft). Less than 5% of the sea is deeper than 3,000 m, and in a system of submarine valleys east of the Andaman-Nicobar Ridge, the depth exceeds 4,000 m. The sea floor is covered with pebbles, gravel and sand.
Ocean floor tectonics.
Running in a rough north–south line on the seabed of the Andaman Sea is the boundary between two tectonic plates, the Burma plate and the Sunda Plate. These plates (or microplates) are believed to have formerly been part of the larger Eurasian Plate, but were formed when transform fault activity intensified as the Indian Plate began its substantive collision with the Eurasian continent. As a result, a back-arc basin center was created, which began to form the marginal basin which would become the Andaman Sea, the current stages of which commenced approximately 3–4 million years ago (Ma).
The boundary between two major tectonic plates results in high seismic activity in the region (see ). Numerous earthquakes have been recorded, and at least six, in 1797, 1833, 1861, 2004, 2005 and 2007, had the magnitude of 8.4 or higher. On December 26, 2004, a large portion of the boundary between the Burma Plate and the Indo-Australian Plate slipped, causing the 2004 Indian Ocean earthquake. This megathrust earthquake had a magnitude of 9.3. Between 1300 and 1600 kilometers of the boundary underwent thrust faulting and shifted by about 20 meters, with the sea floor being uplifted several meters. This rise in the sea floor generated a massive tsunami with an estimated height of 28 m that killed approximately 280,000 people along the coast of the Indian Ocean. The initial quake was followed by a series of aftershocks along the arc of the Andaman and Nicobar Islands. The entire event severely damaged the tourist and fishing infrastructure.
Volcanic activity.
Within the sea, to the east of the main Great Andaman island group, lies Barren Island, the only presently active volcano associated with the Indian subcontinent. This island-volcano is 3 km in diameter and rises 354 meters above the sea level. Its recent activity resumed in 1991 after a quiet period of almost 200 years. It is caused by the ongoing subduction of the India Plate beneath the Andaman island arc, which forces magma to rise in this location of the Burma Plate. The last eruption has started on 13 May 2008 and still continues. The volcanic island of Narcondam which lies further to the north was also formed by this process; however, no records exist of its activity.
Hydrology and climate.
The climate and water salinity of the Andaman Sea are mostly determined by the monsoons of southeast Asia. Air temperature is stable over the year at 26 °C in February and 27 °C in August. Precipitation is as high as 3,000 mm/year and mostly occurs in summer. Sea currents are south-easterly and easterly in winter and south-westerly and westerly in summer. The average surface water temperature is 26–28 °C in February and 29 °C in May. The water temperature is constant at 4.8 °C at the depths of 1,600 m and below. Salinity is 31.5–32.5‰ (parts per thousand) in summer and 30.0–33.0‰ in winter in the southern part. In the northern part, it decreases to 20–25‰ due to the inflow of fresh water from the Irrawaddy River. Tides are semidiurnal (i.e. rising twice a day) with the amplitude of up to 7.2 meters.
Flora.
The coastal areas of the Andaman Sea are characterized by mangrove forests and seagrass meadows. Mangroves cover between more than 600 km² of the Thai shores of Malay Peninsula whereas seagrass meadows occupy an area of 79 km². Mangroves are largely responsible for the high productivity of the coastal waters – their roots trap soil and sediment and provide shelter from predators and nursery for fish and small aquatic organisms. Their body protects the shore from the wind and waves, and their detritus are a part of the aquatic food chain. A significant part of the Thai mangrove forests in the Andaman Sea was removed during the extensive brackish water shrimp. Mangroves were also significantly damaged by the 2004 tsunami. They were partly replanted after that, but their area is still gradually decreasing due to human activities.
Other important sources of nutrients in the Andaman Sea are seagrass and the mud bottoms of lagoons and coastal areas. They also create a habitat or temporal shelter for many burrowing and benthic organisms. Many aquatic species migrate from and to seagrass either daily or at certain stages of their life cycle. The human activities which damage seagrass beds include waste water discharge from coastal industry, shrimp farms and other forms of coastal development, as well as trawling and the use of push nets and dragnets. The 2004 tsunami affected 3.5% of seagrass areas along the Andaman Sea via siltation and sand sedimentation and 1.5% suffered total habitat loss.
Fauna.
The sea waters along the Malay Peninsula favor molluscan growth, and there are about 280 edible fish species belonging to 75 families. Of those, 232 species (69 families) are found in mangroves and 149 species (51 families) reside in seagrass; so 101 species are common to both habitats. The sea also hosts many vulnerable fauna species, including dugong ("Dugong dugon"), several dolphin species, such as Irrawaddy Dolphin ("Orcaella brevirostris") and four species of sea turtles: critically endangered leatherback turtle ("Dermochelys coriacea") and hawksbill turtle (Eletmochelys imbricata) and threatened green turtle ("Chelonia mydas") and Olive Ridley turtle ("Lepidochelys olivacea"). There are only about 150 dugongs in the Andaman Sea, scattered between Ranong and Satun Provinces. These species are sensitive to the degradation of seagrass meadows.
Human activities.
The sea has long been used for fishing and transportation of goods between the coastal countries. Thailand alone harvested about 943,000 tonnes of fish in 2005 and about 710,000 tonnes in 2000. Of those 710,000 tonnes, 490,000 are accounted for by trawling (1,017 vessels), 184,000 by purse seine (415 vessels), and about 30,000 by gillnets. The production numbers are significantly smaller for Malaysia and are comparable, or higher, for Burma. Competition for fish resulted in numerous conflicts between Burma and Thailand. In 1998 and 1999, they resulted in fatalities on both sides and nearly escalated into a military conflict. In both cases, the Thai navy intervened when Burmese vessels tried to intercept Thai fishing boats in the contested sea areas, and Thai fighter aircraft were thought to be deployed by the National Security Council. Thai fishing boats were also frequently confronted by the Malaysian navy to the extent that the Thai government had to caution its own fishers against fishing without license in foreign waters.
The 2004 marine production in Thailand was composed of: pelagic fish 33%, demersal fish 18%, cephalopod 7.5%, crustaceans 4.5%, trash fish 30% and others 7%. Trash fish refers to non-edible species, edible species of low commercial value and juveniles, which are released to the sea. Pelagic fishes were distributed between anchovies ("Stolephorus" spp., 19%), Indo-Pacific mackerel ("Rastrelliger brachysoma", 18%), sardinellas ("Sardinellars" spp., 14%), scad (11%), longtail tuna ("Thunnus tonggol", 9%), eastern little tuna ("Euthynnus affinis", 6%), trevallies (6%), bigeye scad (5%), Indian mackerel ("Rastrelliger kanagurta", 4%), king mackerel ("Scomberomorus cavalla", 3%), torpedo scad ("Megalaspis cordyla ", 2%), wolf herrings (1%) and others (2%). Demersal fish production was dominated by purple-spotted bigeye ("Priacanthus tayenus"), threadfin bream ("Nemipterus hexodon"), brushtooth lizardfish ("Saurida undosquamis"), slender lizardfish ("Saurida elongata") and Jinga shrimp ("Metapenaeus affinis"). Most species are overfished since the 1970s–1990s, except for Spanish mackerel ("Scomberomorus commersoni"), carangidae and torpedo scad ("Meggalaspis" spp.). The overall overfishing rate was 333% for pelagic and 245% for demersal species in 1991. Cephalopods are divided into squid, cuttlefish and molluscs, where squid and cuttlefish in Thai waters consists of 10 families, 17 genera and over 30 species. The main mollusk species captured in the Andaman Sea are scallop, blood cockle ("Anadara granosa") and short-necked clam. Their collection requires bottom dredge gears, which damage the sea floor and the gears themselves and are becoming unpopular. So, the mollusk production has decreased from 27,374 tonnes in 1999 to 318 tonnes in 2004. While crustaceans composed only 4.5% of the total marine products in 2004 by volume, they accounted for 21% of the total value. They were dominated by banana prawn, tiger prawn, king prawn, school prawn, bay lobster ("Thenus orientalis"), mantis shrimp, swimming crabs and mud crabs. The total catch in 2004 was 51,607 tonnes for squid and cuttlefish and 36,071 tonnes for crustaceans.
The sea’s mineral resources include tin deposits off the coasts of Malaysia and Thailand. Major ports are Dawei, Mawlamyine, Mergui, Pathein and Yangon in Myanmar; George Town and Penang in Malaysia; and Belawan in Indonesia.
The Andaman Sea, particularly the western coast of the Malay Peninsula, is rich in coral reefs and offshore islands with spectacular topography, such as Phuket, Phi Phi Islands, Ko Tapu and islands of Krabi Province. Despite having been devastated by the 2004 Sumatra earthquake and tsunami, they remain popular tourist destinations. The nearby coast also has numerous marine national parks – 16 only in Thailand, and four of them are candidates for inclusion into UNESCO World Heritage Sites.

</doc>
<doc id="50810" url="http://en.wikipedia.org/wiki?curid=50810" title="Geography of Tunisia">
Geography of Tunisia

Tunisia is a country located in Northern Africa, bordering the Mediterranean Sea, between Algeria and Libya. Its geographic coordinates are . Tunisia occupies an area of 163,610 square kilometres, of which 8,250 are water. Tunisia borders Algeria for 965 km and Libya for 459 km.
Physical geography.
Tunisia is situated on the Mediterranean coast of North Africa, midway between the Atlantic Ocean and the Nile Delta. It is bordered by Algeria on the west and Libya on the south east. It lies between latitudes 30° and 38°N, and longitudes 7° and 12°E. An abrupt southward turn of the Mediterranean coast in northern Tunisia gives the country two distinctive Mediterranean coasts, west-east in the north, and north-south in the east
Tunisia is about the size of the American state of Wisconsin. Despite its relatively small size, Tunisia has great environmental diversity due to its north-south extent. Its east-west extent is limited. Differences in Tunisia, like the rest of the Maghreb, are largely north-south environmental differences defined by sharply decreasing rainfall southward from any point. The Dorsal, the eastern extension of the Atlas Mountains, runs across Tunisia in a northeasterly direction from the Algerian border in the west to the Cape Bon peninsula in the east. North of the Dorsal is the Tell, a region characterized by low, rolling hills and plains, again an extension of mountains to the west in Algeria. In the Khroumerie, the northwestern corner of the Tunisian Tell, elevations reach 1,050 m and snow occurs in winter.
The Sahel, a broadening coastal plain along Tunisia's eastern Mediterranean coast, is among the world's premier areas of olive cultivation. Inland from the Sahel, between the Dorsal and a range of hills south of Gafsa, are the steppes. Much of the southern region is desert.
Tunisia has a coastline 1148 km long. In maritime terms, the country claims a contiguous zone of 24 nmi, and a territorial sea of 12 nmi.
Climate.
Tunisia's climate is hot-summer Mediterranean climate (Köppen climate classification "Csa") in the north, where winters are mild with moderate rainfall and summers are hot and dry. Temperatures in July and August can exceed 40 °C (104 °F) when the tropical continental air mass of the desert reaches the whole Tunisia. Winters are mild with temperatures rarely exceeding 20 °C (68 °F) (exception is the south-west of the country). The south of the country is desert. The terrain in the north is mountainous, which, moving south, gives way to a hot, dry central plain. As we go to the south, the climate naturally becomes hotter, drier and sunnier. The southern part has therefore a hot desert climate (Köppen climate classification "BWh") with extremely hot summers, warm winters and very low annual rainfall amount. Daytime temperatures consistently turn around 45 °C (113 °F) during summers. However, the warmth of winters is only during daytime because nights can be cold in the desert. A series of salt lakes, known as "chottzz" or "shatts", lie in an east-west line at the northern edge of the Sahara Desert, extending from the Gulf of Gabes into Algeria. The lowest point is Chott el Djerid, at - 17 m, and the highest is Jebel ech Chambi, at 1544 m. Annual average rainfall amount is lower than 500 mm (19.68 in) nearly everywhere in Tunisia. Tunisia is therefore a dry, semi-arid country. Areas with a pre-Saharan climate receive below 250 mm (9.84 in) and areas with a typical Saharan climate receive below 100 mm (3.94 in) of annual average precipitation. The southernmost part receives rainfall as low as 50 mm (1,97 in) in areas around El Borma, along the Algerian border.
Largest cities.
source: 
Natural resources.
Tunisia possesses petroleum, phosphates, iron ore, lead, zinc, salt and arable land. 3,850 km² of land is irrigated in Tunisia. The use of land in the country is demonstrated in the following table.
Environment.
Current environmental issues for Tunisia include:
Tunisia is a party to the following international agreements: Biodiversity, Climate Change, Climate Change-Kyoto Protocol, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Ozone Layer Protection, Ship Pollution (MARPOL 73/78) and Wetlands. Tunisia has signed, but not ratified the Marine Life Conservation agreement.
Tunisia, like other North African countries, has lost much of its prehistoric biodiversity due to the ongoing expanding human population; for example, until historic times there was a population of the endangered primate Barbary Macaque, "Macaca sylvanus". The Monk Seal is now extirpated from Tunisia.
Extreme points.
This is a list of the extreme points of Tunisia, the points that are farther north, south, east or west than any other

</doc>
<doc id="50811" url="http://en.wikipedia.org/wiki?curid=50811" title="History of Tunisia">
History of Tunisia

The History of Tunisia is subdivided into the following articles:
Names.
Tunisia, al-Jumhuriyyah at-Tunisiyyah, is a sovereign republic. Yet the country's proper name has changed radically more than once over the course of millennia. Hence, such a term as "ancient Tunisia" is frankly anachronistic. Nonetheless, "Tunisia" is used throughout this history for continuity. 
Undoubtedly, the most ancient Berbers had various names for their land and settlements here, one early Punic-era Berber name being Massyli. After the Phoenicians arrived, their city of Carthage evolved to assume a dominant position over much of the western Mediterranean; this city-state gave its name to the region. Following the Punic Wars, the Romans established here their Province of Africa, taking the then not-widely-known name of "Africa" from a Berber word for 'the people'. The Roman capital was the rebuilt city of Carthage. After the Arab and Muslim conquest, this name continued in use, as the region was called in Arabic "Ifriqiya". Its capital was relocated to the newly built city of Kairouan.
In the twelfth century the Berber Almohads [Almohad or Al Muwahhidun] conquered the country and began to rule it from Tounes [Tunis], an ancient but until-then unimportant city, which thus rose to become the capital. The whole country then came to be called "Tounes" after this city (near the ruins of ancient Carthage). Tunis continued as the capital under Turkish rule, and remains so today. Only in the last years of the nineteenth century, under the French protectorate, did the current name "Tunisie" [in French] from "Tounes" [in Arabic], (Tunisia [in English]), come into wide use. 
During these millennia of history the different civilizations and regimes flourished, and the country has been called by various names. These include: Massyli, Carthage, Africa, Ifriqiya, Tounes, Tunisia.
History Outline.
Its long history may be very briefly outlined or summarized. Here a reverse chronological order is employed. 
Climate change.
Earlier in an era of prehistory the Sahara region to the south was not an arid desert, but rather in places grasslands grew with seasonal lakes, and corresponding flora and fauna. Prior to 6000 years ago, evidently the vast Sahara region to the south was better watered, more a savanna which could support herds; yet then a desiccation process set in, leaving a more parched desert as it is today. 
The wettest time of the Sahara appears the more archaic, which may correspond to a certain glacial period in Europe (the Würm, which ended c. 9000 BC). Subsequently, during neolithic pre-history in North Africa (c. 6000 to 2500) and continuing to modern times, "damp "pluvial"" phases appear to alternate with "dry "inter-pluvial"" phases. The general 'post-Würm' climate (the repeated succession of long periods of more rainy weather followed by arid) remains problematic with respect to fixing dates in history, e.g., apparent contradictions may arise simply due to divergent micro-climates. 
During its recorded history the physical features and environment of the land now called Tunisia have remained fairly constant; however, there were differences, e.g., A study by Prof. Shaw has criticized a century of scholarly literature which theorizes a long decline in agricultural conditions since ancient times due to destructive invasions and significant climate change; Shaw writes that such theories may be based on "false assumptions about the past, dubious literary evidence, and misunderstood archaeological data." Deforestation since ancient times probably dates to the late 19th and 20th centuries, with its general increase in the "intensity of agricultural exploitation of the countryside." The lack of a modern grain surplus for export may be chiefly due to an increase in local population.
Geography.
Weather in the far north is temperate, enjoying a Mediterranean climate, with mild rainy winters and hot dry summers. The natural terrain is fertile, the fields often broken by woodlands, e.g., with cork, oak, and pine. Bizerta on the north coast has a large, developed harbor. Nearby lies the large lake of Ichkeul, a favored stop used by hundreds of thousands of migrating birds. The fertile river valley of the Medjerda ("Wadi Majardah") (anciently called the "Bagradas") flows eastward and empties into the sea north of Tunis. The Medjerda and vicinity have been very productive throughout history and today remain valuable farmland. Grain is grown in the upper Medjerda, while on the lower Medjerda and in spots surrounding Tunis, vinyards and vegetables. 
Along the eastern sea coast the sahel enjoys a moderate climate, less rainfall but with heavy dew; these coastlands currently support orchards (predominately olive, also various fruit trees), and livestock grazing. The port cities of Hammamet, Sousse, Monastir, Mahdia are here; further south are Sfax [Safaqis], Gabès [Qabis], and also the island of Djerba. In and around Djerba lie lands continuing the Sahel. Mineral wealth is extracted from various sites, e.g., phosphates (near Gafsa) and hydrocarbons (in the desert south). Near the mountainous Algerian border in the west rises Tunisia's highest point, "Jebel ech Chambi" at 1544 meters. From this area the high tell descends northeastward to the coast, continuing through Cape Bon, east of Tunis. Called the "Dorsale", Tunisia's mountain range is interrupted by several passes, including the Kasserine. 
Between the coastal "sahel" and the high mountains lies the "bled", seasonally parched plains that are more sparsely populated, but where the sacred city of Kairouan is situated. In the near south, cutting east-west across the low-lying country, are the Tunisian salt lakes (called "chotts" or "shatts"), which continue westward far into Algeria. This region forms the Djerid; quality dates are cultivated here in substantial quantities, due to use of subsurface aquifers. Further south lies the Sahara desert; here Tunisia touches the north-eastern edge of vast sand dunes comprising the "Grand Erg Oriental".
Until the arrival of the Ottomans, Tunisia included additional lands to the west, and to the east. The region surrounding Constantine, Algeria (anciently, western Numidia) was formerly ruled primarily from Tunis. The coastlands by Tripoli, Libya [also called Tarabulus] also had been, before the Turks, in long political association with Tunis.
Today Tunisia has 163,610 square kilometers (63,170 square miles). It fronts the Mediterranean Sea to the north and east, Libya extends to the southeast, and Algeria is west. The capital Tunis is located near the coast, roughly between the mouth of the Medjerda river to the north and Cap Bon (Watan el-Kibli). With a population now of about 800,000, Tunis has been the principal city in the region for over eight centuries. The second largest city is Sfax which is noted for industry, with about 350,000 people.
Population.
The present day Republic of Tunisia includes about ten million inhabitants, chiefly of Arab-Berber descent. Yet also a broad ethnic mix constitutes a substantial minority, coming from throughout the Mediterranean region, both east and west, many dating to the Phoenician, Roman, or Ottoman eras. Included are Sicilians and Greeks, Corsicans and French, Spanish and Germans, Egyptians and Jews, Circassians, Iranians, and Turks; also in this mix are Tunisians whose ancestry traces southward across the deserts to Black Africa. Arabic became the primary language following the 7th-century Muslim conquest, with French also widely spoken, following the French protectorate. 99% of the Tunisian population is said to be Sunni Muslim and the rest Jewish or Christians, although there were never official polls, questionnaires or studies on that matter.

</doc>
<doc id="50812" url="http://en.wikipedia.org/wiki?curid=50812" title="Politics of Tunisia">
Politics of Tunisia

The politics of Tunisia function within a framework of a democratic constitutional republic, with a President serving as head of state, Prime Minister as head of government, a unicameral legislature and a court system influenced by French civil law. Between 1956 and 2011, Tunisia operated as a de facto single party state, with politics dominated by the secular Constitutional Democratic Rally (RCD) under former Presidents Habib Bourgiba and then Zine el Abidine Ben Ali. However, in 2011 a national uprising led to the ousting of the President and the dismantling of the RCD, paving the way for a multi-party democracy.
Tunisia is a member of the Arab League, the African Union and the Organisation of Islamic Cooperation. It maintains close relations with France and the European Union, with which it entered an Association Agreement in 1995. Tunisia’s favorable relations with the European Union was earned following years of successful economic cooperation in the private sector and infrastructure modernization.
Structure of government.
Tunisia is a constitutional republic characterized by an executive president, a legislature and judiciary. The military is neutral and does not play any role in national politics.
Executive branch.
In Tunisia, the President is elected to five-year terms. He appoints a Prime Minister and cabinet, who play a strong role in the execution of policy. Regional governors and local administrators also are appointed by the central government. Mayors and municipal councils are elected.
Legislative branch.
Tunisia's legislative branch consists of the Assembly of the Representatives of the People, which consists of 217 seats. The first elections for the Assembly of the Representative of the People occurred on 26 October 2014. 
Before 2011 revolution the parliament was bicameral. The lower house of the bicameral Parliament was the Chamber of Deputies of Tunisia ("Majlis al-Nuwaab"), which had 214 seats. Members were elected by popular vote to serve five-year terms. At least 25% of the seats in the House of Deputies were reserved for the opposition. More than 27% of the members of the Chamber of Deputies were women. The Lower House played a growing role as an arena for debate on national policy especially that it hosted representatives from six opposition parties. Opposition members often voted against bills or abstain. Because of the comfortable majority enjoyed by the governing party, bills usually passed with only minor changes.
The upper house was the Chamber of Advisors, which included 112 members including representatives of governorates (provinces), professional organizations and national figures. 41 members were appointed by the Head of state while the remainder were elected by their peers. About 15% of the members of the Chamber of advisors were women.
Judicial branch.
The Tunisian legal system is based on French civil law system and Islamic law; some judicial review of legislative acts in the Supreme Court in joint session. The judiciary is independent, although the judicial council is chaired by the head of state.
Political parties and elections.
Since 1987 Tunisia has reformed its political system several times, abolishing life-term presidencies and opening up the parliament to opposition parties. The number of new political parties and associations has notably increased since the beginning of Ben Ali's presidency in 1987. Currently there are eight recognized national parties, six of which hold national legislative seats.
Since his accession to the Presidency, the President's party, known as the Constitutional Democratic Rally (RCD), rallied majorities in local, regional, and national elections. Although the party was renamed (in President Bourguiba’s days it was the Socialist Destourian Party), its policies were still considered to be largely secular and conservative. However, the Tunisian Revolution in 2011 saw its removal from power.
2009 national elections.
The Tunisian national elections of 2009, overseen by the Interior Ministry and held on October 25, 2009, elected candidates for president and legislative offices. During the campaign, speeches by candidates were aired on Tunisian radio and television stations. Participation was 89% of resident citizens and 90% of citizens living abroad. In the presidential vote, Ben Ali soundly defeated his challengers, Mohamed Bouchiha (PUP), Ahmed Inoubli (UDU) and Ahmed Ibrahim (Ettajdid Movement) for a fifth term in office. His 89% of the vote was slightly lower than in the 2004 election. In the parliamentary elections, the RCD received 84% of the vote for 161 constituency seats. The MDS won 16 seats under the proportional representation system, followed by the PUP with 12 seats. 59 women were elected to legislative seats.
The election was criticized by opposition parties and some international observers for limitations placed on non-incumbents. In one instance, the Ettajdid party's weekly publication, Ettarik al-Jadid, was seized by authorities for violating campaign communications laws. Meanwhile, a delegation from the African Union Commission praised the election for taking place with "calm and serenity" Prior to the 2009 election, Tunisia amended its constitution to allow more candidates to run for president, allowing the top official from each political party to compete for the presidency regardless of whether they held seats in parliament.
2011 Constituent Assembly election.
Following the 2010–2011 protests and the vacation of the Presidency by President Ben Ali, elections for a Constituent Assembly were held on 23 October 2011. Results were announced on 25 October 2011 with the center-right and moderately Islamist Ennahda winning a plurality with 37% of the vote.
2014 parliamentary elections.
Parliamentary elections were held in Tunisia on 26 October 2014. Results were announced on 27 October 2014 with secularist Nidaa Tounes winning a plurality with 38% of the vote.
Politics and society.
Women's equality.
Women hold 23% of the seats in the Chamber of Deputies, outpacing the percentage of women serving in the U.S. Congress, which stands at 17% in the 111th Congress. More than one-fifth of the seats in both chambers of parliament are held by women, an exceptionally high level in the Arab world.
Tunisia is the only country in the Arab world where polygamy is forbidden by law. This is part of a provision in the country’s Code of Personal Status which was introduced by President Bourguiba in 1956.
Civil unrest.
The government's success in suppressing violent Islamist extremists, along with its pro-western foreign policy, has moderated Western criticism of what some have characterized as Tunisia’s slow pace in improving democratic practices. Groups such as Amnesty International have documented some restriction of basic human rights and obstruction of human rights organizations. The Economist's 2008 Democracy Index ranks Tunisia 141 out of 167 studied countries and 143 out of 173 regarding freedom of the press.
Though the government received criticism in 2008 for its handling of social unrest in the town of Gafsa, it has been broadly praised for its efforts to respond constructively to the events. Trade unionists initially arrested for protesting working conditions were released on the order of President Ben Ali and officially pardoned in October 2009 in a move that was welcomed by Amnesty International.
Levels of democracy and freedom of expression in the country are criticised by Amnesty International and various other organizations.
2010–2011 revolution.
Protests in 2010–2011 led to President Ben Ali fleeing Tunisia, his presidency being declared vacant by the Constitutional Council, and Fouad Mebazaa becoming acting President for up to 60 days.
Media.
Freedom of the press is officially guaranteed and condoned, however, independent press remains restricted, as does a substantial amount of web content. Journalists are often obstructed from reporting on controversial events. Prior to the Jasmine Revolution, Tunisia practiced internet censorship against popular websites such as YouTube. Reporters Without Borders includes Tunisia in the country list of “Enemies of the Internet". However, Tunisia has recently shown interest in improving its information policy, hosting the second half of the United Nations-sponsored World Summit on the Information Society in 2005, which endorsed the freedom of the internet as a platform for political participation and human rights protection. Furthermore, Tunisians have grown online, as witnessed by the more than 3.5 million regular internet users, 1.6 million Facebook users and hundreds of internet cafes, known as ‘publinet.’
Five private radio stations have been established, including Mosaique FM, Express FM, Shems FM and private television stations such as Hannibal TV and Nessma TV.
Administrative divisions.
Tunisia is divided into 24 governorates:
International organization participation.
Tunisia is a participant in the following international organizations:

</doc>
<doc id="50813" url="http://en.wikipedia.org/wiki?curid=50813" title="Economy of Tunisia">
Economy of Tunisia

Tunisia is in the process of economic reform and liberalization after decades of heavy state direction and participation in the economy. Prudent economic and fiscal planning have resulted in moderate but sustained growth for over a decade. Tunisia's economic growth historically has depended on oil, phosphates, agri-food products, car parts manufacturing, and tourism. In the World Economic Forum 2008/2009 Global Competitiveness Report, the country ranks first in Africa and 36th globally for economic competitiveness, well ahead of Portugal (43), Italy (49) and Greece (67). With a GDP (PPP) per capita of $9795 Tunisia is among the wealthiest countries in Africa. Based on HDI, Tunisia ranks 5th in Africa.
Historical trend.
Current GDP per capita soared by more than 380% in the Seventies (1970–1980: USD 280–1,369). But this proved unsustainable and it collapsed to a cumulative 10% growth in the turbulent Eighties (1980–1990: USD 1,369–1,507), rising again to almost 50% cumulative growth in the Nineties (1990–2000: USD 1,507–2,245), signifying the impact of successful diversification.
Growing foreign debt and the foreign exchange crisis in the mid-1980s. In 1986, the government launched a structural adjustment program to liberalize prices, reduce tariffs, and reorient Tunisia toward a market economy.
Tunisia's economic reform program has been lauded as a model by international financial institutions. The government has liberalized prices, reduced tariffs, lowered debt-service-to-exports and debt-to-GDP ratios, and extended the average maturity of its $10 billion foreign debt. Structural adjustment brought additional lending from the World Bank and other Western creditors. In 1990, Tunisia acceded to the General Agreement on Tariffs and Trade (GATT) and is a member of the World Trade Organization (WTO).
In 1996 Tunisia entered into an "Association Agreement" with the European Union (EU) which removes tariff and other trade barriers on most goods by 2008. In conjunction with the Association Agreement, the EU is assisting the Tunisian government's "Mise A Niveau" (upgrading) program to enhance the productivity of Tunisian businesses and prepare for competition in the global marketplace.
The government has totally or partially privatized around 160 state-owned enterprises since the privatization program was launched in 1987. Although the program is supported by the GATT, the government has had to move carefully to avoid mass firings. Unemployment continues to plague Tunisia's economy and is aggravated by a rapidly growing work force. An estimated 55% of the population is under the age of 25. Officially, 15.2% of the Tunisian work force is unemployed.
In 2011, after the Arab Spring, the economy slumped but then recovered with a 2.81% GDP growth in 2014. However, unemployment is still one of the major issues with 15.2% of the labor force unemployed as of the first quarter of 2014. Tunisia’s political transition gained new momentum in early 2014, with the resolution of a political deadlock, the adoption of a new Constitution and the appointment of a new government. The national dialogue platform, brokered by key civil society organizations, played a crucial role in gathering all major political parties.This consensus will allow for further reform in the economy and public sector.
External trade and investment.
In 1992, Tunisia re-entered the private international capital market for the first time in 6 years, securing a $10-million line of credit for balance-of-payments support. In January 2003 Standard & Poor's affirmed its investment grade credit ratings for Tunisia. The World Economic Forum 2002-03 ranked Tunisia 34th in the Global Competitiveness Index Ratings (two places behind South Africa, the continent's leader). In April 2002, Tunisia's first US dollar-denominated sovereign bond issue since 1997 raised $458 million, with maturity in 2012.
The Bourse de Tunis is under the control of the state-run Financial Market Council and lists over 50 companies. The government offers substantial tax incentives to encourage companies to join the exchange, and expansion is occurring.
The Tunisian government adopted a unified investment code in 1993 to attract foreign capital. More than 1,600 export-oriented joint venture firms operate in Tunisia to take advantage of relatively low labor costs and preferential access to nearby European markets. Economic links are closest with European countries, which dominate Tunisia's trade. Tunisia's currency, the dinar, is not traded outside Tunisia. However, partial convertibility exists for bonafide commercial and investment transaction. Certain restrictions still limit operations carried out by Tunisian residents.
The stock market capitalisation of listed companies in Tunisia was valued at $5.3 Billion in 2007, 15% of 2007 GDP, by the World Bank.
For 2007, foreign direct investment totaled TN Dinar 2 billion in 2007, or 5.18% of the total volume of investment in the country. This figure is up 35.7% from 2006 and includes 271 new foreign enterprises and the expansion of 222 others already based in the country.
The economic growth rate seen for 2007, at 6.3% is the highest achieved in a decade.
Loan Guarantee.
On April 20, 2012, U.S. Treasury Secretary and Tunisian Finance Minister Houcine Dimassi signed a declaration of intent to move forward on a U.S. loan guarantee for Tunisia. The U.S. Government would provide this loan guarantee to enable the Tunisian government to access significant market financing at affordable rates and favorable maturities with the backing of a U.S. guarantee of principal and interest (up to 100 percent).
The support would consist of the U.S. guarantee of Tunisian government-issued debt (or of bank loans made to the Government of Tunisia). This guarantee will significantly reduce the Tunisian government's borrowing costs at a time when market access has become more expensive for many emerging market countries. In the weeks ahead, both governments intend to make progress on a loan guarantee agreement that would allow Tunisia to move forward with a debt issuance.
The ceremony took place at the World Bank immediately following the meeting of Finance Ministers of the Deauville Partnership with Arab Countries in Transition.
Agriculture.
Agriculture - products: olives, grain, tomatoes, citrus fruit, sugar beets, dates, almonds,

</doc>
<doc id="50814" url="http://en.wikipedia.org/wiki?curid=50814" title="Transport in Tunisia">
Transport in Tunisia

Tunisia has a number of international airports to service its sizable tourist trade. Tunis is the center of the transport system as the largest city having the largest port and a light transit system.
Railways.
Tunisia inherited much of its rail transport system from the French and the Tunisian Government has developed infrastructure further. The railways are operated by the Société Nationale de Chemins de Fer Tunisiens (SNCFT), the Tunisian national railway. A modernisation program is currently underway. It has a total of 2,152 km consisting of 468 km of railways and 1,674 kilometres of . Tunis has a light rail system. In the south of Tunisia, there is a narrow gauge railway called the Sfax-Gafsa Railway which delivers phosphates and iron ore to the harbour at Sfax. Tunisia has rail links with the neighbouring country of Algeria via the Ghardimaou-Souk Ahras line, and another connection to Tébessa, however, the latter link is currently not used.
There are no railways yet in neighbouring Libya though some are under construction in 2008; some gauge conversion would be required for efficient connections.
Highways.
As of 2004, there were 18,997 km of highway including 12,310 of paved road and 6,387 of unpaved road. The major cities are all linked by road through the interior. In 2002, Tunisia borrowed €300 million from the European Investment Bank in 2002 to be used to improve roads in the country including €120 million towards building a motorway between Tunis and Sfax. (MEED Middle East Economic Digest, Feb 15, 2002 v46 i7(1))
International highways.
Route 1 in the Trans-African Highway network passes through Tunisia, linking it to North African nations including Algeria, Morocco, Libya and Egypt, and to West African nations via Mauritania. In addition a feeder road links Tunisia to the Trans-Sahara Highway from Algeria to West Africa.
Pipelines.
Tunisia has an extensive pipeline network including 3,059 km of gas pipelines, 1,203 kilometres of oil pipeline and 345 km of refined products. Petrochemicals are Tunisia's third most important export despite the small size of its oil and gas fields as compared to Libya and Algeria. It also gets a royalty rate of 5 per cent on the Algerian gas that runs through Tunis to Sicily through the Trans-Mediterranean gas pipeline. (IPR Strategic Business Information Database, Dec 18, 2003) Libya's National Oil Corporation formed a joint venture with Societe Tunisienne de l'Electricite et du Gaz to construct a national gas pipeline between the two countries. (Petroleum Economist, Dec 2003 v70 i12 p43(1))
Ports and harbours.
Tunis is the most significant port in Tunisia with other significant ports on the Mediterranean Sea including Bizerte, Gabès, La Goulette, Sfax, Sousse and Zarzis. Tunisia's merchant marine consisted of 14 ships as at 2002.
Aviation.
As of 2002, Tunisia had 30 airports including several international airports. The most important one is the Tunis-Carthage International Airport but other significant airports serve Sfax, Djerba-Zarzis, Enfidha, Monastir, Tozeur and Tabarka. Tunisair is the national airline.

</doc>
<doc id="50815" url="http://en.wikipedia.org/wiki?curid=50815" title="Demographics of Tunisia">
Demographics of Tunisia

The majority (99% ) of modern Tunisians are Arabized Berber or Arab-Berber, and are speakers of Tunisian Arabic. However, there is also a small (1 percent at most) of pure native Berbers located mainly in the Jabal Dahar mountains in the South East and on the island of Jerba. The Berbers primarily speak Berber languages, often called Shilha or Tashlihit, or have shifted to Tunisian Arabic. 
Nearly all Tunisians (99 percent of the population) are Muslim. There is a Jewish population on the southern island of Djerba and Tunis. There is also a small indigenous Christian population.
Population.
Source: National Institute of Statistics
Vital statistics.
Source: National Institute of Statistics
Genetic.
While the vast majority of modern Tunisians identify themselves as Arabs, they are predominantly descended from Berber groups, with some Arab input. Tunisians are also descended, to a lesser extent, from other African, Middle Eastern and European peoples, specifically the Phoenicians/Punics, Romans, Vandals, French and Haratin. In sum, a little less than 20 percent of their genetic material (Y-chromosome analysis) comes from present day Arabian Peninsula, Europe or Sub-Saharan Africa.
"In fact, the Tunisian genetic distances to European samples are smaller than those to North African groups. (...) This could be explained by the history of the Tunisian population, reflecting the influence of the ancient Punic settlers of Carthage followed, among others, by Roman, Byzantine, Arab and French occupations, according to historical records. Notwithstanding, other explanations cannot be discarded, such as the relative heterogeneity within current Tunisian populations, and/or the limited sub-Saharan genetic influence in this region as compared with other North African areas, without excluding the possibility of the genetic drift, whose effect might be particularly amplified on the X chromosome.", This suggests a fairly significant European input to Tunisian genetics compared to other neighbouring populations.
Y-Chromosome.
Listed here are the human Y-chromosome DNA haplogroups in Tunisia.
CIA World Factbook demographic statistics.
The following demographic statistics are from the "CIA World Factbook", unless otherwise indicated.
Nationality
Vital Statistics.
Age structure
Net migration rate
Urbaniziation
Sex ratio
Infant mortality rate
Life expectancy at birth
Literacy.
"definition:"
age 15 and over can read and write
<br>"total population:"
74.3%
<br>"male:"
83.4%
<br>"female:"
65.3% (2004 est.)
The literacy rate among the Tunisian population increased greatly after its independence from France. According to the 1996 census data, the literacy rate of the last generation of Tunisian men educated under the French rule (those born 1945-49) was less than 65%. For the first generation educated after independence (born 1950-1954), literacy in Arabic among males had increased to nearly 80%. (Sixty-two percent were also literate in French and 15 percent literate in English). Among the youngest generation included in the census (those born 1980-1984), 96.6% were literate in Arabic.
Among Tunisian women, the increase in literacy was even greater. The literacy rate among the last generation of women educated under the French was less than 30%. In the first generation educated after independence, this increased to just over 40%. For the youngest generation of women cited (born 1980-1984), literacy in Arabic had increased to slightly over 90%; over 70% of women were also literate in French.

</doc>
<doc id="50816" url="http://en.wikipedia.org/wiki?curid=50816" title="Tunisian Armed Forces">
Tunisian Armed Forces

The Tunisian Armed Forces (Arabic: القوات المسلحة التونسية‎) consist of the Tunisian Army, Navy, and Air Force.
As of 2012, Tunisia had an army of 40,500 personnel equipped with 84 main battle tanks and 48 light tanks. The navy numbered 4,800 operating 25 patrol boats and 6 other crafts. The Air Force had 4,000 personnel, 27 combat aircraft and 43 helicopters. Paramilitary forces consisted of a 12,000-member national guard. Tunisia participates in United Nations peacekeeping efforts in the DROC (MONUSCO) and Côte d'Ivoire. Previous United Nations peacekeeping deployments for the Tunisian armed forces have included Cambodia (UNTAC), Namibia (UNTAG), Somalia, Rwanda, Burundi, Ethiopia/Eritrea (UNMEE), and the 1960s mission in the Congo, ONUC.
The former minister of defence was .
History.
The modern Tunisian Army had its origins in the time of the French Protectorate (1881–1956). During this period, Tunisians were recruited in significant numbers into the French Army, serving as tirailleurs (infantry) and spahis (cavalry). These units saw active service in Europe during both World Wars, as well as in Indo-China prior to 1954. The only exclusively Tunisian military force permitted under French rule was the Beylical Guard.
Following independence.
On June 30, 1956, the Tunisian army was officially founded by decree, in the form of a combined-arms regiment. The necessary equipment was made available to the young state from French stocks. The new army initially comprised 25 Tunisian officers, 250 NCOs and 1,250 men transferred from French Army service, plus 850 former members of the Beylical Guard. Approximately 4,000 Tunisian soldiers continued in French Army service until 1958, when the majority transferred to the Tunisian Army, which reached a strength of over 6,000 that year.
Intakes of conscripts for military service, made mandatory in January 1957, plus the recall of reservists allowed the army to grow to twelve battalions numbering 20,000 men by 1961. Sixty per cent of those troops were deployed for border monitoring and defense duties.
Tunisian units first saw action in 1958 after French intrusions into the south in pursuit of National Liberation Army (Algeria) fighters. In 1960 Tunisian troops served with the United National Peacekeeping Force in the Congo. In 1961 clashes occurred with French forces based at Bizerte. More than 600 men fell in battle against the French forces. The French evacuated the base after subsequent negotiations with the Tunisian Government.
The Tunisian Navy, founded in 1958, received its first ship in the fall of 1959. The Air Force acquired its first combat aircraft in 1960 . While the Tunisian armed forces obtain equipment from several sources, the United States remains the largest single supplier. Officer and specialist training for Tunisian personnel was formerly undertaken in French and American military academies. Tunisian trainees are now gradually being assigned to newly established military schools within the country.
The January 10, 1957, a law prohibits any military officer to be a member of a group or a political party. However after 7 November 1987 when the former Prime Minister, General Zine el-Abidine Ben Ali took power senior officers such as Abdelhamid Escheikh and Mustapha Bouaziz took up ministerial appointments.
On April 30, 2002, at around 18.15, the direction of the Army - Brigadier General Abdelaziz Skik who led the Tunisian contingent to Cambodia, two colonels - majors, three colonels, four majors, two lieutenants and a sergeant-major - disappeared in a helicopter crash near the town of Medjez el-Bab.
Tunisia has contributed military forces to United Nations peacekeeping missions, including an army company to the United Nations Assistance Mission for Rwanda (UNAMIR) during the Rwandan Genocide. In his book "Shake Hands with the Devil", Canadian force commander Roméo Dallaire gave the Tunisian soldiers high credit for their work and effort in the conflict and referred to them as his "ace in the hole".
During the 2011 Libyan civil war, Tunisian forces, mostly border guards, saw some limited action when fighting between Libyan rebels and loyalist soldiers spilled over the border.
The military and politics.
The Library of Congress Country Study says:
His exclusive power to promote military officers has been among the strongest components of Bourguiba's control over the armed forces. From independence, high-ranking officers—general staff and senior commanders in particular—have been carefully selected for their party loyalty more than for their professional experience and competence. This began in the late 1950s when the president dismissed those officers who had trained in the Middle East and who might therefore have been expected to sympathize with the militant Pan-Arab policies of Egypt's Nasser. The hand-picked senior officers, in turn, carefully screened all officers who were considered for positions of authority in line units to ensure that antiregime elements did not pose potential threats at any level of the military establishment.
As a result of these promotion policies, the Tunisian officer corps took on a very homogeneous character that only began to break down in the 1970s. Senior officers have been generally representative of Tunisia's economically and politically dominant families from the north, the coastal areas, and the major cities. Although military men have been kept from operating major business ventures or holding political office while in uniform, it has been common for family members to be prominent in business or in the Destourian political movement. Generally Western and Francophile in outlook, tied by kinship to the country's upper socioeconomic stratum, and personally familiar with leading figures in the PSD, high-ranking Tunisian officers must be classed as part of the national elite.
General Staff.
In accordance with Article 44 of the constitution, the supreme commander of the armed forces is the President of the Republic of Tunisia.
In December 2010, the staff is composed as follows: Chief of Staff of the Army corps is the General Rashid Ammar, one of the Air Force is Brigadier General Taieb Lajimi and that the navy is Rear Admiral Mohamed Khamassi. In April 2011, Ammar became chief of staff inter-armed.
The Inspector General of the armed forces is Rear Admiral Tarek Faouzi Larbi, the Director of Military Engineering is Brigadier General Mohammed Hedi Abdelkafi and the director of military security Brigadier General Ahmed Chabir.
Tunisian Army.
The Tunisian Army is 27,000 strong and is composed essentially of:
Army Ranks.
 with editing the source
Air Force equipment.
The Tunisian Air Force is equipped with 10 Northrop F-5E Tiger II and two Northrop F-5F Tiger II. These form 15 Squadron at Bizerte-Sidi Ahmed Air Base. It also includes 12 Aero L-59T, as well as three Aermacchi MB-326K (combat capable) as well as 4 MB-326B, and 3 MB326L. Previously up to 8 Aermacchi MB-326B, 7-16 Aermacchi MB-326KT, and 4 Aermacchi MB-326LT were in service.
The IISS Military Balance 2013 lists six Lockheed C-130B Hercules, one Lockheed C-130H Hercules, five G-222s, three Let L-410UVP Turbolet (all assigned to one transport squadron) plus a liaison unit with two S-208A. Other reported transport aircraft include one Boeing 737-700/BBJ, two Dassault Falcon 20, and two Lockheed C-130J-30 Super Hercules.
Reported attack helicopters include four Hughes MD-500 Defenders, and 7-8 SNIAS SA-342 Gazelle.
Reported training/COIN and liaison aircraft include 12 SIAI Marchetti SF.260WC Warriors and 9 SIAI-Marchetti SF-260C, as well as 4 SIAI-Marchetti S.208A/M and one Reims F406.
Apart from Bizerte Sidi-Ahmed, there are military airfields reported at Bizerte (La-Kharouba), Gabes, Gafsa, and Sfax.
Navy equipment.
Established in 1959, the Marine nationale tunisienne (Tunisian National Navy) initially received French assistance, including advisory personnel and several small patrol vessels. On 22 October 1973, the U.S. Edsall-class destroyer escort was decommissioned in ceremonies at the Quai d'Honneur, Bizerte. Moments later, the ship was commissioned by the Tunisian Navy as the "President Bourgiba". In the mid-1980s the force included "President Bourguiba", two United States-built coastal minesweepers, and a variety of fast-attack and patrol craft. The most important additions to the fleet in the 1980s were three La Combattante III class fast attack craft armed with Exocet surface-to-surface missiles. Apart from these vessels, however, most of the fleet's units were old and capable of little more than coastal patrol duties.
During the 1960s and 1970s the navy was primarily involved in combating the smuggling of contraband, the illegal entry of un-
desirable aliens, and unauthorized emigration as well as other coastal security activities. In these matters the overall effort was shared with agencies of the Ministry of Interior, especially the customs agents and immigration personnel of the Surete Nationale.
"President Bourgiba" suffered a major fire on 16 April 1992 and later left operational service.
Today the Tunisian Navy reportedly has bases at Bizerte, Kelibia, La Goulette, and Sfax. Formerly reported in service were six Kondor-II class minsweepers of 635 tons, equipped with 3x2x25mm Guns. However none were listed in service by the IISS Military Balance 2013. Also formerly in use were MBDA MM-40 Exocet and Nord SS-12M surface-to-surface missiles.
Fast attack craft and gunboats include:
Patrol boats.
Landing craft and auxiliary vessels include one LCT-3 class LCT, one Robert Conard class 63.7m Survey vessel (NHO Salammbo), one Wilkes class (T-AGS-33) survey ship (NRF Khaireddine), two El Jem class training ships (ex A 5378 Aragosta and A 5381 Polipo delivered by Italian Navy on 17 July 2002), one Simeto class Tanker ( Ain Zaghouin - ex A 5375 delivered by Italian Navy on 10.7.2003) and one White Sumac 40.5m class.
Weapons of mass destruction.
No known nuclear activity. Signatory to the Nuclear Non-Proliferation Treaty (NPT).
No known chemical weapons activity. Party to the Chemical Weapons Convention (CWC).
No known biological weapons activity. Party to the Biological Weapons Convention (BWC).

</doc>
<doc id="50817" url="http://en.wikipedia.org/wiki?curid=50817" title="Telecommunications in Tunisia">
Telecommunications in Tunisia

Telecommunications in Tunisia includes telephones (fixed and mobile), radio, television, and the Internet. The Ministry of Communication Technologies, a cabinet-level governmental agency, is in charge of organizing the sector.
Radio and television.
The government of former President Ben Ali tightly controlled the press and broadcasting. But since the 2011 popular revolt, many journalists have enjoyed new-found freedoms. The number of radio and TV channels and print publications has increased, as has their freedom to report and debate political and social issues. State TV, which had toed the government line, has changed tack, giving airtime to the former opposition. 
Prior to the Tunisian revolution there were four private radio stations operating in Tunisia. In June 2011, following the Tunisian revolution, a recommendation to license twelve new private radio stations was forwarded to the interim Prime Minister. In August 2011 none of the recommendations had been acted upon. However, several stations began broadcasting under time-limited provisional licenses. The stations operate without specific operating rules because a new regulatory framework is not yet in place. In part due to the lack of a regulatory framework the government’s National Office of Broadcasting (ONT) requires broadcasters to pay a licensing fee of 120,000 dinars (approximately $75,000), and while that license is not necessary to broadcast, it confers a certain amount of legitimacy that broadcasters need to draw advertisers. The large fee is difficult for new start-up stations and the new stations feel that the fees provide an unfair advantage for the older more established private groups organized under the previous regime.
Information and communications technology.
The Tunisian government considers information and communications technology (ICT) an important tool to boost the country’s economy and to adapt the education system to the opportunities available from using Information Technology (IT) as a tool. E-commerce, e-learning, and e-medicine are all areas of strong interest where the Government is seeking international partnership and investments. During the last 15 years, several important efforts were made to invest in ICT and the Internet. Physical infrastructures were modernised. In July 2004 the World Bank approved a $13 million loan to the Tunisian government to support the government effort in accelerating its ICT reforms. Though, beyond the high priority the government is giving to ICT, development of telecommunications in Tunisia has been slower than expected compared to other developing countries in Middle East and North Africa.
2005 World Summit on the Information Society.
The first World Summit on the Information Society (WSIS) was held in Geneva in 2003. Tunisia hosted the second World Summit in November 2005. The Tunisian government took the initiative to host the summit in 1998. It was organised by the International Telecommunication Union (ITU) under the auspices of UNESCO. A declaration of Principles and Plan of Action were approved in order to bridge the digital gap between developing and developed countries within the World Information Society.

</doc>
<doc id="50818" url="http://en.wikipedia.org/wiki?curid=50818" title="Grand Canal">
Grand Canal

Grand Canal can refer to multiple waterways:

</doc>
<doc id="50819" url="http://en.wikipedia.org/wiki?curid=50819" title="Yellow River">
Yellow River

The Yellow River or Huang He is the third-longest river in Asia, following the Yangtze River and Yenisei River, and the sixth-longest in the world at the estimated length of 5464 km. Originating in the Bayan Har Mountains in Qinghai province of western China, it flows through nine provinces, and it empties into the Bohai Sea near the city of Dongying in Shandong province. The Yellow River basin has an east–west extent of about 1900 km and a north–south extent of about 1100 km. Its total basin area is about 742443 km².
The Yellow River is called "the cradle of Chinese civilization", because its basin was the birthplace of ancient Chinese civilization, and it was the most prosperous region in early Chinese history. However, frequent devastating floods and course changes produced by the continual elevation of the river bed (due in part to manmade erosion upstream), sometimes above the level of its surrounding farm fields, has also earned it the unenviable names China's Sorrow and Scourge of the Sons of Han.
Name.
Early Chinese literature including the "Yu Gong" or "Tribute of Yu" dating to the Warring States period (475 – 221 BC) refers to the Yellow River as simply 河 (Old Chinese: "*C.gˤaj"), a character that has come to mean "river" in modern usage. The first appearance of the name 黃河 (Old Chinese: "*N-kʷˤaŋ C.gˤaj"; Middle Chinese: "Hwang Ha") is in the Book of Han written during the Western Han dynasty (206 BC – AD 9). The adjective "yellow" describes the perennial color of the muddy water in the lower course of the river, which arises from soil (loess) being carried downstream.
One of its older Mongolian names was the "Black River", because the river runs clear before it enters the Loess Plateau, but the current name of the river among Inner Mongolians is "Ȟatan Gol" (Хатан гол, "Queen River"). In Mongolia itself, it is simply called the "Šar Mörön" (Шар мөрөн, "Yellow River").
In Qinghai, the river's Tibetan name is "River of the Peacock" (Tibetan: རྨ་ཆུ།, "Ma Chu"; Chinese: 玛曲, 瑪曲, "Mǎ Qū").
The name Hwang Ho in English is the Postal Map romanization of the river's Mandarin name.
History.
Dynamics.
The Yellow River is one of several rivers that are essential for China's very existence. At the same time, however, it has been responsible for several deadly floods, including the only natural disasters in recorded history that have killed more than a million people. The deadliest was a 1332–33 flood that killed 7 million people. Close behind is the 1887 flood, which killed anywhere from 900,000 to 2 million people, and a 1931 flood (part of a massive number of floods that year) that killed 1–4 million people.
The cause of the floods is the large amount of fine-grained loess carried by the river from the Loess Plateau, which is continuously deposited along the bottom of its channel. The sedimentation causes natural dams to slowly accumulate. These subaqueous dams were unpredictable and generally undetectable. Eventually, the enormous amount of water has to find a new way to the sea, forcing it to take the path of least resistance. When this happens, it bursts out across the flat North China Plain, sometimes taking a new channel and inundating any farmland, cities or towns in its path. The traditional Chinese response of building higher and higher levees along the banks sometimes also contributed to the severity of the floods: When flood water did break through the levees, it could no longer drain back into the river bed as it would after a normal flood as the river bed was sometimes now higher than the surrounding countryside. These changes could cause the river's mouth to shift as much as 480 km, sometimes reaching the ocean to the north of Shandong Peninsula and sometimes to the south.
Another historical source of devastating floods is the collapse of upstream ice dams in Inner Mongolia with an accompanying sudden release of vast quantities of impounded water. There have been 11 such major floods in the past century, each causing tremendous loss of life and property. Nowadays, explosives dropped from aircraft are used to break the ice dams before they become dangerous.
Prior to the advent of modern dams in China, the Yellow River was extremely prone to flooding. In the 2,540 years prior to A.D. 1946, the Yellow River has been reckoned to have flooded 1,593 times, shifting its course 26 times noticeably and nine times severely. These floods include some of the deadliest natural disasters ever recorded. Before modern disaster management, when floods occurred, some of the population might initially die from drowning but then many more would suffer from the ensuing famine and spread of diseases.
Ancient times.
Historical documents from the Spring and Autumn period and Qin Dynasty indicate that the Yellow River at that time flowed considerably north of its present course. These accounts show that after the river passed Luoyang, it flowed along the border between Shanxi and Henan Provinces, then continued along the border between Hebei and Shandong before emptying into Bohai Bay near present-day Tianjin. Another outlet followed essentially the present course.
The river left these paths in 602 BC and shifted completely south of the Shandong Peninsula. Sabotage of dikes, canals, and reservoirs and deliberate flooding of rival states became a standard military tactic during the Warring States period. Major flooding in AD 11 is credited with the downfall of the short-lived Xin dynasty, and another flood in AD 70 returned the river north of Shandong on essentially its present course.
Medieval times.
In 923 a desperate Later Liang general Duan Ning again broke the dikes, flooding 1000 sqmi in a failed attempt to protect the Later Liang capital from the Later Tang. A similar proposal from the Song engineer Li Chun concerning flooding the lower reaches of the river to protect the central plains from the Khitai was overruled in 1020: the Treaty of Shanyuan between the two states had expressly forbidden the Song from establishing new moats or changing river courses.
Breaches occurred regardless: one at Henglong in 1034 divided the course in three and repeatedly flooded the northern regions of Dezhou and Bozhou. The Song worked for five years futilely attempting to restore the previous course – using over 35,000 employees, 100,000 conscripts, and 220,000 tons of wood and bamboo in a single year – before abandoning the project in 1041. The more sluggish river then occasioned a breach at Shanghu that sent the main outlet north towards Tianjin in 1048 and by 1194 blocked the mouth of the Huai River. The buildup of silt deposits was such that even after the Yellow River later shifted its course, the Huai was no longer able to flow along its historic one. Instead, its water pools into Hongze Lake and then runs southward toward the Yangtze River.
A flood in 1344 returned the Yellow River south of Shandong. The Yuan dynasty was waning, and the emperor forced enormous teams to build new embankments for the river. The terrible conditions helped fuel rebellions that led to the founding of the Ming dynasty. The course changed again in 1391 when the river flooded from Kaifeng to Fengyang in Anhui. It was finally stabilized by the eunuch Li Xing during the public works projects following the 1494 flood.
The river flooded many times in the 16th century, including in 1526, 1534, 1558, and 1587. Each flood affected the river's lower course.
The 1642 flood was man-made, caused by the attempt of the Ming governor of Kaifeng to use the river to destroy the peasant rebels under Li Zicheng who had been besieging the city for the past six months. He directed his men to break the dikes in an attempt to flood the rebels, but destroyed his own city instead: the flood and the ensuing famine and plague are estimated to have killed 300,000 of the city's previous population of 378,000. The once-prosperous city was nearly abandoned until its rebuilding under the Kangxi Emperor in the Qing Dynasty.
Recent times.
Between 1851 and 1855, it returned to the north amid the floods that provoked the Nien and Taiping Rebellions. The 1887 flood has been estimated to have killed between 900,000 and 2 million people, and is the second-worst natural disaster in history (excluding famines and epidemics). The Yellow River more or less adopted its present course during the 1897 flood.
The 1931 flood killed an estimated 1,000,000 to 4,000,000, and is the worst natural disaster recorded (excluding famines and epidemics).
On 9 June 1938, during the Second Sino-Japanese War, Nationalist troops under Chiang Kai-Shek broke the levees holding back the river near the village of Huayuankou in Henan, causing what has been called a "war-induced natural disaster". The goal of the operation was to stop the advancing Japanese troops by following a strategy of "using water as a substitute for soldiers" ("yishui daibing"). The 1938 flood of an area covering 54000 km² took some 500,000 to 900,000 Chinese lives, along with an unknown number of Japanese soldiers. The flood prevented the Japanese Army from taking Zhengzhou, on the southern bank of the Yellow River, but did not stop them from reaching their goal of capturing Wuhan, which was the temporary seat of the Chinese government and straddles the Yangtze River.
Geography.
According to the China Exploration and Research Society, the source of the Yellow River is at 34° 29' 31.1" N, 96° 20' 24.6" E in the Bayan Har Mountains near the eastern edge of the Yushu Tibetan Autonomous Prefecture. The source tributuaries drain into Gyaring Lake and Ngoring Lake on the western edge of Golog Prefecture high in the Bayan Har Mountains of Qinghai. In the Zoige Basin along the boundary with Gansu, the Yellow River loops northwest and then northeast before turning south, creating the "Ordos Loop", and then flows generally eastward across the North China Plain to the Gulf of Bohai, draining a basin of 752443 km² which nourishes 140 million people with drinking water and irrigation.
The Yellow River passes through seven present-day provinces and two autonomous regions, namely (from west to east) Qinghai, Gansu, Ningxia, Inner Mongolia, Shaanxi, Shanxi, Henan, and Shandong. Major cities along the present course of the Yellow River include (from west to east) Lanzhou, Yinchuan, Wuhai, Baotou, Luoyang, Zhengzhou, Kaifeng, and Jinan. The current mouth of the Yellow River is located at Kenli County, Shandong.
The river is commonly divided into three stages. These are roughly the northeast of the Tibetan Plateau, the Ordos Loop and the North China Plain. However, different scholars have different opinions on how the three stages are divided. This article adopts the division used by the Yellow River Conservancy Commission.
Upper reaches.
The upper reaches of the Yellow River constitute a segment starting from its source in the Bayan Har Mountains and ending at Hekou Town (Togtoh County), Inner Mongolia just before it turns sharply to the south. This segment has a total length of 3472 km and total basin area of 386000 km², 51.4% of the total basin area. Along this length, the elevation of the Yellow River drops 3496 m, with an average grade of 0.10%.
The source section flows mainly through pastures, swamps, and knolls between the Bayan Har Mountains, and the Anemaqen (Amne Machin) Mountains. The river water is clear and flows steadily. Crystal clear lakes are characteristic of this section. The two main lakes along this section are Lake Zhaling (扎陵湖) and Lake Eling (鄂陵湖), with capacities of 4.7 billion and 10.8 billion m³ (166 and 381 billion ft3), respectively. At elevations over 4290 m) above sea level they are the two largest plateau freshwater lakes nationwide. A significant amount of land in the Yellow River's source area has been designated as the Sanjiangyuan ("'Three Rivers' Sources") National Nature Reserve, to protect the source region of the Yellow River, the Yangtze, and the Mekong.
The valley section stretches from Longyang Gorge in Qinghai to Qingtong Gorge in Gansu. Steep cliffs line both sides of the river. The water bed is narrow and the average drop is large, so the flow in this section is extremely turbulent and fast. There are 20 gorges in this section, the most famous of these being the Longyang, Jishi, Liujia, Bapan, and Qingtong gorges. The flow conditions in this section makes it the best location for hydroelectric plants.
After emerging from the Qingtong Gorge, the river comes into a section of vast alluvial plains, the Yinchuan Plain and Hetao Plain. In this section, the regions along the river are mostly deserts and grasslands, with very few tributaries. The flow is slow. The Hetao Plain has a length of 900 km and width of 30 to. It is historically the most important irrigation plain along the Yellow River.
Middle reaches.
The part of the Yellow River (see Ordos Loop) between Hekou Town (Togtoh County), in Inner Mongolia and Zhengzhou, Henan constitutes the middle reaches of the river. The middle reaches are 1206 km long, with a basin area of 344000 km², 45.7% of the total, with a total elevation drop of 890 m, an average drop of 0.074%. There are 30 large tributaries along the middle reaches, and the water flow is increased by 43.5% on this stage. The middle reaches contribute 92% of the river's silts.
The middle stream of the Yellow River passes through the Loess Plateau, where substantial erosion takes place. The large amount of mud and sand discharged into the river makes the Yellow River the most sediment-laden river in the world. The highest recorded annual level of silts discharged into the Yellow River is 3.91 billion tons in 1933. The highest silt concentration level was recorded in 1977 at 920 kg/m³ (57.4 lb/ft3). These sediments later deposit in the slower lower reaches of the river, elevating the river bed and creating the famous "river above ground".
From Hekou to Yumenkou, the river passes through the longest series of continuous valleys on its main course, collectively called the Jinshan Valley. The abundant hydrodynamic resources stored in this section make it the second most suitable area to build hydroelectric power plants. The famous Hukou Waterfall is in the lower part of this valley, on the border of Shanxi and Shaanxi.
Lower reaches.
In the lower reaches, from Zhengzhou, Henan to its mouth, a distance of 786 km, the river is confined to a levee-lined course as it flows to the northeast across the North China Plain before emptying into the Bohai Sea. The basin area in this stage is only 23000 km², a mere 3% of the total, because few tributaries add to the flow in this stage; nearly all rivers to the south drain into the Huai River, whereas those to the north drain into the Hai River. The total drop in elevation of the lower reaches is 93.6 m, with an average grade of 0.012%.
The silts received from the middle reaches form sediments here, elevating the river bed. During 2,000 years of levee construction, excessive sediment deposits have raised the riverbed several meters above the surrounding ground.
At Kaifeng, Henan, the Yellow River is 10 m above the ground level.
Tributaries.
Tributaries of the Yellow River include (upstream to downstream (?))
The Wei River is the largest of these tributaries.
Characteristics.
The Yellow River is notable for the large amount of silt it carries—1.6 billion tons annually at the point where it descends from the Loess Plateau. If it is running to the sea with sufficient volume, 1.4 billion tons are carried to the sea annually. One estimate gives 34 kilograms of silt per cubic meter as opposed to 10 for the Colorado and 1 for the Nile.
Its average discharge is said to be 2,110 cubic meters per second (32,000 for the Yangtze), with a maximum of 25,000 and minimum of 245. However, since 1972, it often runs dry before it reaches the sea. The low volume is due to increased agricultural irrigation, increased by a factor of five since 1950. Water diverted from the river as of 1999 served 140 million people and irrigated 74,000 km² (48,572 mi²) of land. The Yellow River delta totals 8,000 square kilometers (3,090 mi²). However, with the decrease in silt reaching the sea, it has been reported to be shrinking slightly each year since 1996 through erosion.
The highest volume occurs during the rainy season from July to October, when 60% of the annual volume of the river flows. Maximum demand for irrigation is needed between March and June. In order to capture excess water for use when needed and for flood control and electricity generation, several dams have been built, but their expected life is limited due to the high silt load. A proposed South–North Water Transfer Project involves several schemes to divert water from the Yangtze River: one in the western headwaters of the rivers where they are closest to one another, another from the upper reaches of the Han River, and a third using the route of the old Grand Canal.
Due to its heavy load of silt the Yellow River is a depositing stream – that is, it deposits part of its carried burden of soil in its bed in stretches where it is flowing slowly. These deposits elevate the riverbed which flows between natural levees in its lower reaches. Should a flood occur, the river may break out of the levees into the surrounding lower flood plain and take
a new channel. Historically this has occurred about once every hundred years. In modern times, considerable effort has been made to strengthen levees and control floods.
Hydroelectric power dams.
Below is the list of hydroelectric power stations built on the Yellow River, arranged according to the first year of operation (in brackets):
As reported in 2000, the 7 largest hydro power plants (Longyangxia, Lijiaxia, Liujiaxia, Yanguoxia, Bapanxia, Daxia and Qinglongxia) had the total installed capacity of 5,618 MW.
Crossings.
The main bridges and ferries by the province names in the order of downstream to upstream are:
Shandong
Shandong–Henan
Henan
Shanxi–Henan
Shaanxi–Henan
Ningxia
Inner Mongolia
Gansu
Qinghai
Aquaculture.
Although Yellow River is generally less suitable for aquaculture than the rivers of central and southern China, such as the Yangtze or the Pearl River, aquaculture is practiced in some areas along the Yellow River as well. An important aquaculture area is the riverside plain in Xingyang City, upstream from Zhengzhou. Since the development of fish ponds started in Xingyang's riverside Wangcun Town in 1986, the pond systems in Wangcun have grown to the total size of 15,000 "mu" (10 km2), making the town the largest aquaculture center in North China.
A variety of the Chinese softshell turtle popular with China's gourmets is called the Yellow River Turtle (黄河鳖). Now-a-days most of Yellow River Turtles eaten in China's restaurants comes from turtle farms, which may or may not be located near the actual Yellow River. 
In 2007, construction started in Wangcun on a large farm for raising this turtle variety. With the capacity for raising 5 million turtles a year, the facility was expected to become Henan's largest farm of this kind.
Pollution.
On 25 November 2008, Tania Branigan of "The Guardian" filed a report "China's Mother River: the Yellow River", claiming that severe pollution has made one-third of China's Yellow River unusable even for agricultural or industrial use, due to factory discharges and sewage from fast-expanding cities. The Yellow River Conservancy Commission had surveyed more than 8384 mi of the river in 2007 and said 33.8% of the river system registered worse than "level five" according the criteria used by the UN Environment Program. Level five is unfit for drinking, aquaculture, industrial use, or even agriculture. The report said waste and sewage discharged into the system last year totaled 4.29b tons. Industry and manufacturing made up 70% of the discharge into the river with households accounting for 23% and just over 6% coming from other sources.
Yellow River in culture.
In ancient times, it was believed that the Yellow River flowed from Heaven as a continuation of the Milky Way. In a Chinese legend, Zhang Qian is said to have been commissioned to find the source of the Yellow River. After sailing up-river for many days, he saw a girl spinning and a cow herd. Upon asking the girl where he was, she presented him with her shuttle with instructions to show it to the astrologer Yen Chün-p'ing. When he returned, the astrologer recognized it as the shuttle of the Weaving Girl (Vega), and, moreover, said that at the time Zhang received the shuttle, he had seen a wandering star interpose itself between the Weaving Girl and the cow herd (Altair).
The provinces of Hebei and Henan derive their names from the Yellow River. Their names mean, respectively, "North of the River" and "South of the River".
Traditionally, it is believed that the Chinese civilization originated in the Yellow River basin. The Chinese refer to the river as "the Mother River" and "the cradle of the Chinese civilization".
During the long history of China, the Yellow River has been considered a blessing as well as a curse and has been nicknamed both "China's Pride" () and "China's Sorrow" ().
Sometimes the Yellow River is poetically called the "Muddy Flow" (). The Chinese idiom "when the Yellow River flows clear" is used to refer to an event that will never happen and is similar to the English expression "when pigs fly".
"The Yellow River running clear" was reported as a good omen during the reign of the Yongle Emperor, along with the appearance of such auspicious legendary beasts as "qilin" (an African giraffe brought to China by a Bengal embassy aboard Zheng He's ships in 1414) and "zouyu" (not positively identified) and other strange natural phenomena.
See also.
</dl>

</doc>
