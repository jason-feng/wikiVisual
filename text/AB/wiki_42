<doc id="18816" url="http://en.wikipedia.org/wiki?curid=18816" title="Mural">
Mural

A mural is any piece of artwork painted or applied directly on a wall, ceiling or other large permanent surface. A distinguishing characteristic of mural painting is that the architectural elements of the given space are harmoniously incorporated into the picture.
Some wall paintings are painted on large canvases, which are then attached to the wall (e.g., with marouflage). Whether these works can be accurately called "murals" is a subject of some controversy in the art world, but the technique has been in common use since the late 19th century.
History.
Murals of sorts date to Upper Paleolithic times such as the paintings in the Chauvet Cave in Ardèche department of southern France (around 30,000 BC). Many ancient murals have survived in Egyptian tombs (around 3150 BC), the Minoan palaces (Middle period III of the Neopalatial period, 1700-1600 BC) and in Pompeii (around 100 BC - AD 79).
During the Middle Ages murals were usually executed on dry plaster (secco). In Italy, circa 1300, the technique of painting of frescos on wet plaster was reintroduced and led to a significant increase in the quality of mural painting. 
In modern times, the term became more well-known with the Mexican "muralista" art movement (Diego Rivera, David Siqueiros, or José Orozco). There are many different styles and techniques. The best-known is probably "fresco", which uses water-soluble paints with a damp lime wash, a rapid use of the resulting mixture over a large surface, and often in parts (but with a sense of the whole). The colors lighten as they dry. The "marouflage" method has also been used for millennia.
Murals today are painted in a variety of ways, using oil or water-based media. The styles can vary from abstract to "trompe-l'œil" (a French term for "fool" or "trick the eye"). Initiated by the works of mural artists like Graham Rust or Rainer Maria Latzke in the 1980s, trompe-l'oeil painting has experienced a renaissance in private and public buildings in Europe.
Today, the beauty of a wall mural has become much more widely available with a technique whereby a painting or photographic image is transferred to poster paper or canvas which is then pasted to a wall surface "(see wallpaper, Frescography)" to give the effect of either a hand-painted mural or realistic scene.
Technique.
Historical mural techniques.
In the history of mural several methods have been used:
"A fresco" painting, from the Italian word "affresco" which derives from the adjective "fresco" ("fresh"), describes a method in which the paint is applied on plaster on walls or ceilings. The "buon fresco" technique consists of painting in pigment mixed with water on a thin layer of wet, fresh, lime mortar or plaster. The pigment is then absorbed by the wet plaster; after a number of hours, the plaster dries and reacts with the air: it is this chemical reaction which fixes the pigment particles in the plaster. After this the painting stays for a long time up to centuries in fresh and brilliant colors.
"Fresco-secco" painting is done on dry plaster ("secco" is "dry" in Italian). The pigments thus require a binding medium, such as egg (tempera), glue or oil to attach the pigment to the wall.
"Mezzo-fresco" is painted on nearly-dry plaster, and was defined by the sixteenth-century author Ignazio Pozzo as "firm enough not to take a thumb-print" so that the pigment only penetrates slightly into the plaster. By the end of the sixteenth century this had largely displaced the "buon fresco" method, and was used by painters such as Gianbattista Tiepolo or Michelangelo. This technique had, in reduced form, the advantages of "a secco" work.
Material.
In Greco-Roman times, mostly encaustic colors applied in a cold state were used.
Tempera painting is one of the oldest known methods in mural painting. In tempera, the pigments are bound in an albuminous medium such as egg yolk or egg white diluted in water.
In 16th-century Europe, oil painting on canvas arose as an easier method for mural painting. The advantage was that the artwork could be completed in the artist’s studio and later transported to its destination and there attached to the wall or ceiling. Oil paint can be said to be the least satisfactory medium for murals because of its lack of brilliance in colour. Also the pigments are yellowed by the binder or are more easily affected by atmospheric conditions. The canvas itself is more subject to rapid deterioration than a plaster ground.
Different muralists tend to become experts in their preferred medium and application, whether that be oil paints, emulsion or acrylic paints applied by brush, roller or airbrush/aerosols. Clients will often ask for a particular style and the artist may adjust to the appropriate technique.
A consultation usually leads to a detailed design and layout of the proposed mural with a price quote that the client approves before the muralist starts on the work. The area to be painted can be gridded to match the design allowing the image to be scaled accurately step by step. In some cases the design is projected straight onto the wall and traced with pencil before painting begins. Some muralists will paint directly without any prior sketching, preferring the spontaneous technique.
Once completed the mural can be given coats of varnish or protective acrylic glaze to protect the work from UV rays and surface damage.
As an alternative to a hand-painted or airbrushed mural, digitally printed murals can also be applied to surfaces. Already existing murals can be photographed and then be reproduced in near-to-original quality.
The disadvantages of pre-fabricated murals and decals are that they are often mass-produced and lack the allure and exclusivity of an original artwork. They are often not fitted to the individual wall sizes of the client and their personal ideas or wishes can not be added to the mural as it progresses. The Frescography technique, a digital manufacturing method (CAM) invented by Rainer Maria Latzke addresses some of the personalisation and size restrictions.
Digital techniques are commonly used in advertisements. A "wallscape" is a large advertisement on or attached to the outside wall of a building. Wallscapes can be painted directly on the wall as a mural, or printed on vinyl and securely attached to the wall in the manner of a billboard. Although not strictly classed as murals, large scale printed media are often referred to as such. Advertising murals were traditionally painted onto buildings and shops by sign-writers, later as large scale poster billboards.
Significance of murals.
Murals are important in that they bring art into the public sphere. Due to the size, cost, and work involved in creating a mural, muralists must often be commissioned by a sponsor. Often it is the local government or a business, but many murals have been paid for with grants of patronage. For artists, their work gets a wide audience who otherwise might not set foot in an art gallery. A city benefits by the beauty of a work of art.
Murals can be a relatively effective tool of social emancipation or achieving a political goal. Murals have sometimes been created against the law, or have been commissioned by local bars and coffeeshops. Often, the visual effects are an enticement to attract public attention to social issues.
State-sponsored public art expressions, particularly murals, are often used by totalitarian regimes as a tool of mass-control and propaganda. However, despite the propagandist character of that works, some of them still have an artistic value.
Murals can have a dramatic impact whether consciously or subconsciously on the attitudes of passers by, when they are added to areas where people live and work. It can also be argued that the presence of large, public murals can add aesthetic improvement to the daily lives of residents or that of employees at a corporate venue.
Other world-famous murals can be found in Mexico, New York, Philadelphia, Belfast, Derry, Los Angeles, Nicaragua, Cuba and in India. They have functioned as an important means of communication for members of socially, ethnically and racially divided communities in times of conflict. They also proved to be an effective tool in establishing a dialogue and hence solving the cleavage in the long run.
The Indian state Kerala has exclusive murals. These Kerala mural painting are on walls of Hindu temples. They can be dated from 9th century AD.
The San Bartolo murals of the Maya civilization in Guatemala, are the oldest example of this art in Mesoamerica and are dated at 300 BC.
Many rural towns have begun using murals to create tourist attractions in order to boost economic income. Colquitt, Georgia is one such town. Colquitt was chosen to host the 2010 Global Mural Conference. The town has more than twelve murals completed, and will host the Conference along with Dothan, Alabama, and Blakely, Georgia. In the summer of 2010, Colquitt will begin work on their Icon Mural.
Murals and politics.
The Mexican mural movement in the 1930s brought a new prominence to murals as a social and political tool. Diego Rivera, José Orozco and David Siqueiros were the most famous artists of the movement. Between 1932 and 1940, Rivera also painted murals in San Francisco, Detroit, and New York City. In 1933 he completed a famous series of twenty-seven fresco panels entitled "Detroit Industry" on the walls of an inner court at the Detroit Institute of Arts. During the McCarthyism of the 1950s, a was placed in the courtyard defending the artistic merit of the murals while attacking his politics as "detestable."
In 1948 the Colombian Government hosted the IX Pan-American Conference to establish the Marshall plan for the Americas. The director of the OEA and the Colombian government commissioned Master Santiago Martinez Delgado, to paint a mural in the Colombian congress building to commemorate the event. Martinez decided to make it about the Cúcuta Congress, and painted Bolívar in front of Santander, making liberals upset; so, due to the murder of Jorge Elieser Gaitan the mobs of el bogotazo tried to burn the capitol, but the Colombian Army stopped them. Years later, in the 1980s, with liberals in charge of the congress, they passed a resolution to turn the whole chamber in the Elliptic Room 90 degrees to put the main mural on the side and commissioned Alejandro Obregon to paint a non-partisan mural in the surrealist style.
Northern Ireland contains some of the most famous political murals in the world. Almost 2,000 murals have been documented in Northern Ireland since the 1970s. In recent times, many murals are non-sectarian, concerning political and social issues such as racism and environmentalism, and many are completely a-political, depicting children at play and scenes from everyday life. (See Northern Irish murals.)
A not political, but social related mural covers a wall in an old building, once a prison, at the top of a cliff in Bardiyah, in Libya. It was painted and signed by the artist on April 1942, weeks before his death on the first day of the First Battle of El Alamein. Known as the Bardia Mural, it was created by English artist, Private John Frederick Brill.
In 1961 East Germany began to erect a wall between East and West Berlin, which became famous as the Berlin Wall. While on the East Berlin side painting was not allowed, artists painted on the Western side of the Wall from the 80s until the fall of the Wall in 1989.
Many unknown and known artists such as Thierry Noir and Keith Haring painted on the Wall, the “World's longest canvas”. The sometimes detailed artwork were often painted over within hours or days. On the Western side the Wall was not protected, so everybody could paint on the Wall. After the fall of the Berlin Wall in 1989 the Eastern side of the Wall became also a popular “canvas” for many mural and graffiti artists.
Orgosolo, in Sardinia, is a most important center of murals politics.
It is also common for mural graffiti be used as a memoir. In the book "Somebody Told Me," Rick Bragg writes about a series of communities, mainly located in New York, that have walls dedicated to the innocent lives lost. These memorials, both written word and mural style, provide the deceased to be present in the communities in which they lived. Bragg states that the "murals have woven themselves in the fabric of the neighborhoods, and the city." These memorials act as a constant reminder to the living community of innocent lives lost due to inner city violence.
Murals in contemporary interior design.
Traditional interior murals.
Many people like to express their individuality by commissioning an artist to paint a mural in their home, this is not an activity exclusively for owners of large houses. A mural artist is only limited by the fee and therefore the time spent on the painting; dictating the level of detail; a simple mural can be added to the smallest of walls.
Private commissions can be for dining rooms, bathrooms, living rooms or, as is often the case- children's bedrooms. A child's room can be transformed into the 'fantasy world' of a forest or racing track, encouraging imaginative play and an awareness of art.
The current trend for feature walls has increased commissions for muralists in the UK. A large hand-painted mural can be designed on a specific theme, incorporate personal images and elements and may be altered during the course of painting it. The personal interaction between client and muralist is often a unique experience for an individual not usually involved in the arts.
Public commissions of murals in schools, hospitals and retirement homes can achieve
a pleasing and welcoming atmosphere in these caring institutions.
In the 1980s, illusionary wall painting experienced a renaissance in private homes. The reason for this revival in interior design could, in some cases be attributed to the reduction in living space for the individual. Faux architectural features as well as natural scenery and views can have the effect of 'opening out' the walls. Densely built up areas of housing may also contribute to people's feelings of being cut off from nature in its free form. A mural commission of this sort may be an attempt by some people to re-establish a balance with nature.
Graffiti-style interior murals.
Recently, graffiti and street art have played a key role in contemporary wall painting. Such graffiti/street artists as Keith Haring, Shepard Fairey, ABOVE, Mint&Serf, Futura 2000, Os Gemeos, and Faile among others have successfully transcended their street art aesthetic beyond the walls of urban landscape and onto walls of private and corporate clients.
As graffiti/street art became more main stream in the late 1990s, youth oriented brands such as Nike, Red Bull and Wieden Kennedy have turned to graffiti/street artists to decorate walls of their respective offices. This trend continued through 2000's with graffiti/street art gaining more recognition from art institutions worldwide.
Tile mural.
Tile murals are murals made out of stone, ceramic, porcelain, glass and or metal tiles that are installed within, or added onto the surface of an existing wall. They are also inlaid into floors. Mural tiles are painted, glazed, sublimation printed (as described below) or more traditionally cut or broken into pieces. Unlike the traditional painted murals described above, tile murals are always made with the use of tiles.
Mosaic murals are made by combining small 1/4" to 2" size pieces of colorful stone, ceramic,or glass tiles which are then laid out to create a picture. Modern day technology has allowed commercial mosaic mural makers to use computer programs to separate photographs into colors that are automatically cut and glued onto sheets of mesh creating precise murals fast and in large quantities.
The azulejo (], ]) refers to a typical form of Portuguese or Spanish painted, tin-glazed, ceramic tilework. They have become a typical aspect of Portuguese culture, manifesting without interruption during five centuries, the consecutive trends in art.
Azulejos can be found inside and outside churches, palaces, ordinary houses and even railway stations or subway stations.
They were not only used as an ornamental art form, but also had a specific functional capacity like temperature control in homes. Many azulejos chronicle major historical and cultural aspects of Portuguese history.
Custom-printed tile murals can be produced using digital images for kitchen splashbacks, wall displays, and flooring. Digital photos and artwork can be resized and printed to accommodate the desired size for the area to be decorated. Custom tile printing uses a variety of techniques including dye sublimation and ceramic-type laser toners. The latter technique can yield fade-resistant custom tiles which are suitable for long term exterior exposure.

</doc>
<doc id="18819" url="http://en.wikipedia.org/wiki?curid=18819" title="Microeconomics">
Microeconomics

Microeconomics (from Greek prefix "mikro-" meaning "small" and economics) is a branch of economics that studies the behavior of individuals and small impacting organizations in making decisions on the allocation of limited resources (see scarcity). Typically, it applies to markets where goods or services are bought and sold. Microeconomics examines how these decisions and behaviors affect the supply and demand for goods and services, which determines prices, and how prices, in turn, determine the quantity supplied and quantity demanded of goods and services.
This is in contrast to macroeconomics, which involves the "sum total of economic activity, dealing with the issues of growth, inflation, and unemployment." Microeconomics also deals with the effects of national economic policies (such as changing taxation levels) on the aforementioned aspects of the economy. Particularly in the wake of the Lucas critique, much of modern macroeconomic theory has been built upon 'microfoundations'—i.e. based upon basic assumptions about micro-level behavior.
One of the goals of microeconomics is to analyze market mechanisms that establish relative prices amongst goods and services and allocation of limited resources amongst many alternative uses. Microeconomics also analyzes market failure, where markets fail to produce efficient results, and describes the theoretical conditions needed for perfect competition. Significant fields of study in microeconomics include general equilibrium, markets under asymmetric information, choice under uncertainty and economic applications of game theory. Also considered is the elasticity of products within the market system.
Assumptions and definitions.
Microeconomic theory typically begins with the study of a single rational and utility maximizing individual. To economists, rationality means an individual possesses stable preferences that are both complete and transitive. The technical assumption that preference relations are continuous is needed to ensure the existence of a utility function. Although microeconomic theory can continue without this assumption, it would make comparative statics impossible since there is no guarantee that the resulting utility function would be differentiable. 
Microeconomic theory progresses by defining a competitive budget set which is a subset of the consumption set. It as at this point that economists make the technical assumption that preferences are locally non-satiated. Without the assumption of LNS (local non-satiation) there is no guarantee that a rational individual would maximize utility. With the necessary tools and assumptions in place the utility maximization problem (UMP) is developed.
The utility maximization problem is the heart of consumer theory. The utility maximization problem attempts to explain the action axiom by imposing rationality axioms on consumer preferences and then mathematically modeling and analyzing the consequences. The utility maximization problem serves not only as the mathematical foundation of consumer theory but as a metaphysical explanation of it as well. That is, the utility maximization problem is used by economists to not only explain "what" or "how" individuals make choices by "why" individuals make choices as well.
The utility maximization problem is a constrained optimization problem in which an individual seeks to maximize utility subject to a budget constraint. Economists use the extreme value theorem to guarantee that a solution to the utility maximization problem exists. That is, since the budget constraint is both bounded and closed a solution to the utility maximization problem exists. Economists call the solution to the utility maximization problem a Walrasian demand function or correspondence.
The utility maximization problem has so far been developed by taking consumer tastes (i.e. consumer utility) as the primitive. However, an alternative way to develop microeconomic theory is by taking consumer choice as the primitive. This model of microeconomic theory is referred to as Revealed_preference theory. To reconcile these two models, restrictions are placed on the Walrasian demand function. Specifically, the solution to a utility maximization problem must satisfy the Weak Axiom of Revealed Preferences (WARP). In order for a Walrasian demand function to satisfy WARP the Substitution (Slutsky) Matrix must be negative semi-definite. Put more simply, in order for a Walrasian demand function to be consistent with WARP its substitution effect must always be non-positive. 
The theory of supply and demand usually assumes that markets are perfectly competitive. This implies that there are many buyers and sellers in the market and none of them have the capacity to significantly influence prices of goods and services. In many real-life transactions, the assumption fails because some individual buyers or sellers have the ability to influence prices. Quite often, a sophisticated analysis is required to understand the demand-supply equation of a good model. However, the theory works well in situations meeting these assumptions.
Mainstream economics does not assume "a priori" that markets are preferable to other forms of social organization. In fact, much analysis is devoted to cases where market failures lead to resource allocation that is suboptimal and creates deadweight loss. A classic example of suboptimal resource allocation is that of a public good. In such cases, economists may attempt to find policies that avoid waste, either directly by government control, indirectly by regulation that induces market participants to act in a manner consistent with optimal welfare, or by creating "missing markets" to enable efficient trading where none had previously existed.
This is studied in the field of collective action and public choice theory. "Optimal welfare" usually takes on a Paretian norm, which is a mathematical application of the Kaldor–Hicks method. This can diverge from the Utilitarian goal of maximizing utility because it does not consider the distribution of goods between people. Market failure in positive economics (microeconomics) is limited in implications without mixing the belief of the economist and their theory.
The demand for various commodities by individuals is generally thought of as the outcome of a utility-maximizing process, with each individual trying to maximize their own utility under a budget constraint and a given consumption set.
Microeconomic topics.
The study of microeconomics involves several "key" areas:
Demand, supply, and equilibrium.
Supply and demand is an economic model of price determination in a perfectly competitive market. It concludes that in a perfectly competitive market with no externalities, per unit taxes, or price controls, the unit price for a particular good is the price at which the quantity demanded by consumers equals the quantity supplied by producers. This price results in a stable economic equilibrium.
Measurement of elasticities.
Elasticity is the measurement of how responsive an economic variable is to a change in another variable. Elasticity can be quantified as the ratio of the percentage change in one variable to the percentage change in another variable, when the later variable has a causal influence on the former. It is a tool for measuring the responsiveness of a variable, or of the function that determines it, to changes in causative variables in unitless ways. Frequently used elasticities include price elasticity of demand, price elasticity of supply, income elasticity of demand, elasticity of substitution between factors of production and elasticity of intertemporal substitution.
Consumer demand theory.
Consumer demand theory relates preferences for the consumption of both goods and services to the consumption expenditures; ultimately, this relationship between preferences and consumption expenditures is used to relate preferences to consumer demand curves. The link between personal preferences, consumption and the demand curve is one of the most closely studied relations in economics. It is a way of analyzing how consumers may achieve equilibrium between preferences and expenditures by maximizing utility subject to consumer budget constraints.
Theory of production.
Production theory is the study of production, or the economic process of converting inputs into outputs. Production uses resources to create a good or service that is suitable for use, gift-giving in a gift economy, or exchange in a market economy. This can include manufacturing, storing, shipping, and packaging. Some economists define production broadly as all economic activity other than consumption. They see every commercial activity other than the final purchase as some form of production.
Costs of production.
The cost-of-production theory of value is the price of an object or condition is determined by the sum of the cost of the resources that went into making it. The cost can comprise any of the factors of production: labour, capital, land. Technology can be viewed either as a form of fixed capital (ex:plant) or circulating capital (ex:intermediate goods).
Perfect competition.
Perfect competition describes markets such that no participants are large enough to have the market power to set the price of a homogeneous product. An example is Ebay.
Monopoly.
A monopoly (from Greek "monos" μόνος (alone or single) + "polein" πωλεῖν (to sell)) exists when a single company is the only supplier of a particular commodity.
Oligopoly.
An oligopoly is a market form in which a market or industry is dominated by a small number of sellers (oligopolists). Oligopolies can create the incentive for firms to engage in collusion and form cartels that reduce competition leading to higher prices for consumers and less overall market output.
Market structure.
The market structure can have several types of interacting market systems. 
Different forms of markets is a feature of capitalism and advocates of socialism often criticize markets and aim to substitute markets with economic planning to varying degrees. Competition is the regulatory mechanism of the market system. 
Examples of markets include but are not limited to: commodity markets, insurance markets, bond markets, energy markets, flea markets, debt markets, stock markets, online auctions, media exchange markets, real estate market.
Game theory.
Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design,and voting systems, and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.
Labour economics.
Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms.
Welfare economics.
Welfare economics is a branch of economics that uses microeconomic techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium. It analyzes "social welfare", however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no "social welfare" apart from the "welfare" associated with its individual units.
Economics of information.
Information economics or the economics of information is a branch of microeconomic theory that studies how information and information systems affect an economy and economic decisions. Information has special characteristics. It is easy to create but hard to trust. It is easy to spread but hard to control. It influences many decisions. These special characteristics (as compared with other types of goods) complicate many standard economic theories.
Opportunity cost.
Opportunity cost of an activity (or goods) is equal to the best next alternative uses/foregone.
Although "opportunity cost" can be hard to quantify, the effect of opportunity cost is universal and very real on the individual level. In fact, this principle applies to all decisions, not just economic ones.
Opportunity cost is one way to measure the cost of something. Rather than merely identifying and adding the costs of a project, one may also identify the next best alternative way to spend the same amount of money. The forgone profit of this "next best alternative" is the opportunity cost of the original choice. A common example is a farmer that chooses to farm their land rather than rent it to neighbors, wherein the opportunity cost is the forgone profit from renting. In this case, the farmer may expect to generate more profit alone. This kind of reasoning is a very important part of the calculation of discount rates in discounted cash flow investment valuation methodologies. Similarly, the opportunity cost of attending university is the lost wages a student could have earned in the workforce, rather than the cost of tuition, books, and other requisite items (whose sum makes up the total cost of attendance).
Note that opportunity cost is not the "sum" of the available alternatives, but rather the benefit of the single, best alternative. Possible opportunity costs of a city's decision to build a hospital on its vacant land are the loss of the land for a sporting center, "or" the inability to use the land for a parking lot, "or" the money that could have been made from selling the land, "or" the loss of any of the various other possible uses — but not all of these in aggregate. The true opportunity cost would be the forgone profit of the most lucrative of those listed.
One question that arises here is how to determine a money value for each alternative to facilitate comparison and assess opportunity cost, which may be more or less difficult depending on the things we are trying to compare. For example, many decisions involve environmental impacts whose monetary value is difficult to assess because of scientific uncertainty. Valuing a human life or the economic impact of an Arctic oil spill involves making subjective choices with ethical implications.
It is imperative to understand that no decision on allocating time is free. No matter what one chooses to do, they are always giving something up in return. An example of opportunity cost is deciding between going to a concert and doing homework. If one decides to go the concert, then they are giving up valuable time to study, but if they choose to do homework then the cost is giving up the concert. Any decision in allocating capital is likewise: there is an opportunity cost of capital, or a hurdle rate, defined as the expected rate one could get by investing in similar projects on the open market. Opportunity cost is vital in understanding microeconomics and decisions that are made.
Applied microeconomics.
Applied microeconomics includes a range of specialized areas of study, many of which draw on methods from other fields. Industrial organization examines topics such as the entry and exit of firms, innovation, and the role of trademarks. Labor economics examines wages, employment, and labor market dynamics. Financial economics examines topics such as the structure of optimal portfolios, the rate of return to capital, econometric analysis of security returns, and corporate financial behavior. Public economics examines the design of government tax and expenditure policies and economic effects of these policies (e.g., social insurance programs). Political economy examines the role of political institutions in determining policy outcomes. Health economics examines the organization of health care systems, including the role of the health care workforce and health insurance programs. Urban economics, which examines the challenges faced by cities, such as sprawl, air and water pollution, traffic congestion, and poverty, draws on the fields of urban geography and sociology. Law and economics applies microeconomic principles to the selection and enforcement of competing legal regimes and their relative efficiencies. Economic history examines the evolution of the economy and economic institutions, using methods and techniques from the fields of economics, history, geography, sociology, psychology, and political science.
Further reading.
</dl>

</doc>
<doc id="18820" url="http://en.wikipedia.org/wiki?curid=18820" title="Macroeconomics">
Macroeconomics

Macroeconomics (from the Greek prefix "makro-" meaning "large" and economics) is a branch of economics dealing with the performance, structure, behavior, and decision-making of an economy as a whole, rather than individual markets. This includes national, regional, and global economies. With microeconomics, macroeconomics is one of the two most general fields in economics.
Macroeconomists study aggregated indicators such as GDP, unemployment rates, and price indexes to understand how the whole economy functions. Macroeconomists develop models that explain the relationship between such factors as national income, output, consumption, unemployment, inflation, savings, investment, international trade and international finance. In contrast, microeconomics is primarily focused on the actions of individual agents, such as firms and consumers, and how their behavior determines prices and quantities in specific markets
While macroeconomics is a broad field of study, there are two areas of research that are emblematic of the discipline: the attempt to understand the causes and consequences of short-run fluctuations in national income (the business cycle), and the attempt to understand the determinants of long-run economic growth (increases in national income). Macroeconomic models and their forecasts are used by governments to assist in the development and evaluation of economic policy.
Basic macroeconomic concepts.
Macroeconomics encompasses a variety of concepts and variables, but there are three central topics for macroeconomic research. Macroeconomic theories usually relate the phenomena of output, unemployment, and inflation. Outside of macroeconomic theory, these topics are also important to all economic agents including workers, consumers, and producers.
Output and income.
National output is the lowest amount of everything a country produces in a given time period. Everything that is produced and sold generates income. Therefore, output and income are usually considered equivalent and the two terms are often used interchangeably. Output can be measured as total income, or, it can be viewed from the production side and measured as the total value of final goods and services or the sum of all value added in the economy.
Macroeconomic output is usually measured by Gross Domestic Product (GDP) or one of the other national accounts. Economists interested in long-run increases in output study economic growth. Advances in technology, accumulation of machinery and other capital, and better education and human capital all lead to increased economic output over time. However, output does not always increase consistently. Business cycles can cause short-term drops in output called recessions. Economists look for macroeconomic policies that prevent economies from slipping into recessions and that lead to faster long-term growth.
Unemployment.
The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labor force. The labor force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labor force.
Unemployment can be generally broken down into several types that are related to different causes.
Inflation and deflation.
A general price increase across the entire economy is called inflation. When prices decrease, there is deflation. Economists measure these changes in prices with price indexes. Inflation can occur when an economy becomes overheated and grows too quickly. Similarly, a declining economy can lead to deflation.
Central bankers, who control a country's money supply, try to avoid changes in price level by using monetary policy. Raising interest rates or reducing the supply of money in an economy will reduce inflation. Inflation can lead to increased uncertainty and other negative consequences. Deflation can lower economic output. Central bankers try to stabilize prices to protect economies from the negative consequences of price changes.
Changes in price level may be result of several factors. The quantity theory of money holds that changes in price level are directly related to changes in the money supply. Most economists believe that this relationship explains long-run changes in the price level. Short-run fluctuations may also be related to monetary factors, but changes in aggregate demand and aggregate supply can also influence price level. For example, a decrease in demand because of a recession can lead to lower price levels and deflation. A negative supply shock, like an oil crisis, lowers aggregate supply and can cause inflation.
Macroeconomic models.
Aggregate demand–aggregate supply.
The AD-AS model has become the standard textbook model for explaining the macroeconomy. This model shows the price level and level of real output given the equilibrium in aggregate demand and aggregate supply. The aggregate demand curve's downward slope means that more output is demanded at lower price levels. 
The downward slope is the result of three effects: the Pigou or real balance effect, which states that as real prices fall, real wealth increases, so consumers demand more goods; the Keynes or interest rate effect, which states that as prices fall the demand for money declines causing interest rates to decline and borrowing for investment and consumption to increase; and the net export effect, which states that as prices rise, domestic goods become comparatively more expensive to foreign consumers and thus exports decline.
In the conventional Keynesian use of the AS-AD model, the aggregate supply curve is horizontal at low levels of output and becomes inelastic near the point of potential output, which corresponds with full employment. Since the economy cannot produce beyond more than potential output, any AD expansion will lead to higher price levels instead of higher output.
The AD–AS diagram can model a variety of macroeconomic phenomena including inflation. When demand for goods exceeds supply there is an inflationary gap where demand-pull inflation occurs and the AD curve shifts upward to a higher price level. When the economy faces higher costs, cost-push inflation occurs and the AS curve shifts upward to higher price levels. The AS–AD diagram is also widely used as a pedagogical tool to model the effects of various macroeconomic policies.
IS–LM.
The IS–LM model represents the equilibrium in interest rates and output given by the equilibrium in the goods and money markets. The goods market is represented by the equilibrium in investment and saving (IS), and the money market is represented by the equilibrium between the money supply and liquidity preference. The IS curve consists of the points where investment, given the interest rate, is equal to savings, given output.
The IS curve is downward sloping because output and the interest rate have an inverse relationship in the goods market: As output increases more money is saved, which means interest rates must be lower to spur enough investment to match savings. The LM curve is upward sloping because interest rates and output have a positive relationship in the money market. As output increases, the demand for money increases, and interest rates increase.
The IS/LM model is often used to demonstrate the effects of monetary and fiscal policy. Textbooks frequently use the IS/LM model, but it does not feature the complexities of most modern macroeconomic models. Nevertheless, these models still feature similar relationships to those in IS/LM.
Growth models.
The neoclassical growth model of Robert Solow has become a common textbook model for explaining economic growth in the long-run. The model begins with a production function where national output is the product of two inputs: capital and labor. The Solow model assumes that labor and capital are used at constant rates without the fluctuations in unemployment and capital utilization commonly seen in business cycles.
An increase in output, economic growth, can only occur because of an increase in the capital stock, a larger population, or technological advancements that lead to higher productivity (total factor productivity). An increase in the savings rate leads to a temporary increase as the economy creates more capital, which adds to output. However, eventually the depreciation rate will limit the expansion of capital: Savings will be used up replacing depreciated capital, and no savings will remain to pay for an additional expansion in capital. Solow's model suggests that economic growth in terms of output per capita depends solely on technological advances that enhance productivity.
In the 1980s and 1990s endogenous growth theory arose to challenge neoclassical growth theory. This group of models explains economic growth through other factors, like increasing returns to scale for capital and learning-by-doing, that are endogenously determined instead of the exogenous technological improvement used to explain growth in Solow's model.
Macroeconomic policy.
Macroeconomic policy is usually implemented through two sets of tools: fiscal and monetary policy. Both forms of policy are used to stabilize the economy, which usually means boosting the economy to the level of GDP consistent with full employment.
Monetary policy.
Central banks implement monetary policy by controlling the money supply through several mechanisms. Typically, central banks take action by issuing money to buy bonds (or other assets), which boosts the supply of money and lowers interest rates, or, in the case of contractionary monetary policy, banks sell bonds and take money out of circulation. Usually policy is not implemented by directly targeting the supply of money.
Banks continuously shift the money supply to maintain a fixed interest rate target. Some banks allow the interest rate to fluctuate and focus on targeting inflation rates instead. Central banks generally try to achieve high output without letting loose monetary policy create large amounts of inflation.
Conventional monetary policy can be ineffective in situations such as a liquidity trap. When interest rates and inflation are near zero, the central bank cannot loosen monetary policy through conventional means. Central banks can use unconventional monetary policy such as quantitative easing to help increase output. Instead of buying government bonds, central banks implement quantitative easing by buying other assets such as corporate bonds, stocks, and other securities.
This allows lower interest rates for a broader class of assets beyond government bonds. In another example of unconventional monetary policy, the United States Federal Reserve recently made an attempt at such a policy with Operation Twist. Unable to lower current interest rates, the Federal Reserve lowered long-term interest rates by buying long-term bonds and selling short-term bonds to create a flat yield curve.
Fiscal policy.
Fiscal policy is the use of government's revenue and expenditure as instruments to influence the economy. Examples of such tools are expenditure, taxes, debt.
For example, if the economy is producing less than potential output, government spending can be used to employ idle resources and boost output. Government spending does not have to make up for the entire output gap. There is a multiplier effect that boosts the impact of government spending. For example, when the government pays for a bridge, the project not only adds the value of the bridge to output, it also allows the bridge workers to increase their consumption and investment, which also help close the output gap.
The effects of fiscal policy can be limited by crowding out. When government takes on spending projects, it limits the amount of resources available for the private sector to use. Crowding out occurs when government spending simply replaces private sector output instead of adding additional output to the economy. Crowding out also occurs when government spending raises interest rates which limits investment. Defenders of fiscal stimulus argue that crowding out is not a concern when the economy is depressed, plenty of resources are left idle, and interest rates are low.
Fiscal policy can be implemented through automatic stabilizers. Automatic stabilizers do not suffer from the policy lags of discretionary fiscal policy. Automatic stabilizers use conventional fiscal mechanisms but take effect as soon as the economy takes a downturn: spending on unemployment benefits automatically increases when unemployment rises and, in a progressive income tax system, the effective tax rate automatically falls when incomes decline.
Comparison.
Economists usually favor monetary over fiscal policy because it has two major advantages. First, monetary policy is generally implemented by independent central banks instead of the political institutions that control fiscal policy. Independent central banks are less likely to make decisions based on political motives. Second, monetary policy suffers shorter inside lags and outside lags than fiscal policy. Central banks can quickly make and implement decisions while discretionary fiscal policy may take time to pass and even longer to carry out.
Development.
Origins.
Macroeconomics descended from the once divided fields of business cycle theory and monetary theory. The quantity theory of money was particularly influential prior to World War II. It took many forms including the version based on the work of Irving Fisher:
In the typical view of the quantity theory, money velocity (V) and the quantity of goods produced (Q) would be constant, so any increase in money supply (M) would lead to a direct increase in price level (P). The quantity theory of money was a central part of the classical theory of the economy that prevailed in the early twentieth century.
Austrian School.
Ludwig Von Mises work "Theory of Money and Credit" published in 1912 was one of the first books from the Austrian School to deal with macroeconomic topics.
Keynes and his followers.
Macroeconomics, at least in its modern form, began with the publication of John Maynard Keynes's "General Theory of Employment, Interest and Money". When the Great Depression struck, classical economists had difficulty explaining how goods could go unsold and workers could be left unemployed. In classical theory, prices and wages would drop until the market cleared, and all goods and labor were sold. Keynes offered a new theory of economics that explained why markets might not clear, which would evolve (later in the 20th century) into a group of macroeconomic schools of thought known as Keynesian economics – also called Keynesianism or Keynesian theory.
In Keynes's theory, the quantity theory broke down because people and businesses tend to hold on to their cash in tough economic times, a phenomenon he described in terms of liquidity preferences. Keynes also explained how the multiplier effect would magnify a small decrease in consumption or investment and cause declines throughout the economy. Keynes also noted the role uncertainty and animal spirits can play in the economy.
The generation following Keynes combined the macroeconomics of the "General Theory" with neoclassical microeconomics to create the neoclassical synthesis. By the 1950s, most economists had accepted the synthesis view of the macro economy. Economists like Paul Samuelson, Franco Modigliani, James Tobin, and Robert Solow developed formal Keynesian models, and contributed formal theories of consumption, investment, and money demand that fleshed out the Keynesian framework.
Monetarism.
Milton Friedman updated the quantity theory of money to include a role for money demand. He argued that the role of money in the economy was sufficient to explain the Great Depression and aggregate demand oriented explanations were not necessary. Friedman argued that monetary policy was more effective than fiscal policy; however, Friedman doubted the government has ability to "fine-tune" the economy with monetary policy. He generally favored a policy of steady growth in money supply instead of frequent intervention.
Friedman also challenged the Phillips curve relationship between inflation and unemployment. Friedman and Edmund Phelps (who was not a monetarist) proposed an "augmented" version of the Phillips curve that excluded the possibility of a stable, long-run tradeoff between inflation and unemployment. When the oil shocks of the 1970s created a high unemployment and high inflation, Friedman and Phelps were vindicated. Monetarism was particularly influential in the early 1980s. Monetarism fell out of favor when central banks found it difficult to target money supply instead of interest rates as monetarists recommended. Monetarism also became politically unpopular when the central banks created recessions in order to slow inflation.
New classicals.
New classical macroeconomics further challenged the Keynesian school. A central development in new classical thought came when Robert Lucas introduced rational expectations to macroeconomics. Prior to Lucas, economists had generally used adaptive expectations where agents were assumed to look at the recent past to make expectations about the future. Under rational expectations, agents are assumed to be more sophisticated. A consumer will not simply assume a 2% inflation rate because that has been the average the past few years; she will look at current monetary policy and economic conditions to make an informed forecast. When new classical economists introduced rational expectations into their models, they showed that monetary policy could only have a limited impact.
Lucas also made an influential critique of Keynesian empirical models. He argued that forecasting models based on empirical relationships would keep producing the same predictions even as the underlying model generating the data changed. He advocated models based on fundamental economic theory that would, in principle, be structurally accurate as economies changed. Following Lucas's critique, new classical economists, led by Edward C. Prescott and Finn E. Kydland created real business cycle (RBC) models of the macroeconomy.
RBC models were created by combining fundamental equations from neo-classical microeconomics. In order to generate macroeconomic fluctuations, RBC models explained recessions and unemployment with changes in technology instead changes in the markets for goods or money. Critics of RBC models argue that money clearly plays an important role in the economy, and the idea that technological regress can explain recent recessions is also implausible. However, technological shocks are only the more prominent of a myriad of possible shocks to the system that can be modeled. Despite questions about the theory behind RBC models, they have clearly been influential in economic methodology.
New Keynesian response.
New Keynesian economists responded to the new classical school by adopting rational expectations and focusing on developing micro-founded models that are immune to the Lucas critique. Stanley Fischer and John B. Taylor produced early work in this area by showing that monetary policy could be effective even in models with rational expectations when contracts locked-in wages for workers. Other new Keynesian economists expanded on this work and demonstrated other cases where inflexible prices and wages led to monetary and fiscal policy having real effects.
Like classical models, new classical models had assumed that prices would be able to adjust perfectly and monetary policy would only lead to price changes. New Keynesian models investigated sources of sticky prices and wages due to imperfect competition, which would not adjust, allowing monetary policy to impact quantities instead of prices.
By the late 1990s economists had reached a rough consensus. The rigidities of new Keynesian theory were combined with rational expectations and the RBC methodology to produce dynamic stochastic general equilibrium (DSGE) models. The fusion of elements from different schools of thought has been dubbed the new neoclassical synthesis. These models are now used by many central banks and are a core part of contemporary macroeconomics.
New Keynesian economics: which developed partly in response to new classical economics, strives to provide microeconomic foundations to Keynesian economics by showing how imperfect markets can justify demand management.

</doc>
<doc id="18823" url="http://en.wikipedia.org/wiki?curid=18823" title="Mary Pickford">
Mary Pickford

Mary Pickford (April 8, 1892 – May 29, 1979) was a Canadian-American motion picture actress, co-founder of the film studio United Artists and one of the original 36 founders of the Academy of Motion Picture Arts and Sciences. Known as "America's Sweetheart", "Little Mary" and the "girl with the curls", she was one of the Canadian pioneers in early Hollywood and a significant figure in the development of film acting.
In consideration of her contributions to American cinema, the American Film Institute ranked Pickford as 24th in its 1999 list of greatest female stars of all time.
Early life.
Mary Pickford was born Gladys Louise Smith in 1892 at 211 University Avenue, Toronto, Ontario, Canada. Her father, John Charles Smith, was the son of English Methodist immigrants, and worked a variety of odd jobs. Her mother, Charlotte Hennessey, was of Irish Catholic descent and worked for a time as a seamstress. She had two younger siblings, Charlotte, called "Lottie" (born 1893), and John Charles, called "Jack" (born 1896), who also became actors. To please her husband's relatives, Pickford's mother baptized her children as Methodists, the faith of their father. John Charles Smith was an alcoholic; he abandoned the family and died on February 11, 1898 from a fatal blood clot caused by a workplace accident when he was a purser with Niagara Steamship.
When Gladys was age four, her household was under infectious quarantine, a public health measure. Her mother asked a visiting priest to baptize the children as Catholic. Their devoutly Catholic maternal grandmother (Catherine Faeley Hennessey) approved the baptism.
Charlotte Hennessey Smith began taking in boarders after being widowed. One of these was a theatrical stage manager. At his suggestion, Gladys (age 7) was given two small roles, one as a boy and the other as a girl, in a stock company production of "The Silver King" at Toronto's Princess Theatre. She subsequently acted in many melodramas with Toronto's Valentine Company, finally playing the major child role in their version of "The Silver King". She capped her short career in Toronto with the starring role of Little Eva in their production of "Uncle Tom's Cabin," adapted from the 1852 novel by American writer and abolitionist Harriet Beecher Stowe.
Career.
Early years.
By the early 1900s, theatre had become a family enterprise. Gladys, her mother and two younger siblings toured the United States by rail, performing in third-rate companies and plays. After six impoverished years, Pickford allowed one more summer to land a leading role on Broadway, planning to quit acting if she failed. In 1906 Gladys, Lottie and Jack Smith supported singer Chauncey Olcott on Broadway in "Edmund Burke". Gladys finally landed a supporting role in a 1907 Broadway play, "The Warrens of Virginia". The play was written by William C. deMille, whose brother, Cecil, appeared in the cast. David Belasco, the producer of the play, insisted that Gladys Smith assume the stage name Mary Pickford. After completing the Broadway run and touring the play, however, Pickford was again out of work.
On April 19, 1909, the Biograph Company director D. W. Griffith screen-tested her at the company's New York studio for a role in the nickelodeon film, "Pippa Passes". The role went to someone else but Griffith was immediately taken with Pickford. She quickly grasped that movie acting was simpler than the stylized stage acting of the day. Most Biograph actors earned $5 a day but, after Pickford's single day in the studio, Griffith agreed to pay her $10 a day against a guarantee of $40 a week.
Pickford, like all actors at Biograph, played both bit parts and leading roles, including mothers, ingenues, charwomen, spitfires, slaves, native Americans, spurned women, and a prostitute. As Pickford said of her success at Biograph:"I played scrubwomen and secretaries and women of all nationalities ... I decided that if I could get into as many pictures as possible, I'd become known, and there would be a demand for my work." She appeared in 51 films in 1909 – almost one a week. She also introduced her friend Florence La Badie to D. W. Griffith, who launched La Badie's career.
In January 1910, Pickford traveled with a Biograph crew to Los Angeles. Many other film companies wintered on the West Coast, escaping the weak light and short days that hampered winter shooting in the East. Pickford added to her 1909 Biographs ("Sweet and Twenty", "They Would Elope," and "To Save Her Soul", to name a few) with films made in California. Actors were not listed in the credits in Griffith's company. Audiences noticed and identified Pickford within weeks of her first film appearance. Exhibitors in turn capitalized on her popularity by advertising on sandwich boards that a film featuring "The Girl with the Golden Curls", "Blondilocks", or "The Biograph Girl" was inside.
Pickford left Biograph in December 1910. The following year, she starred in films at Carl Laemmle's Independent Moving Pictures Company (IMP). IMP was absorbed into Universal Pictures in 1912, along with Majestic. Unhappy with their creative standards, Pickford returned to work with Griffith in 1912. Some of her best performances were in his films, such as "Friends", "The Mender of Nets", "Just Like a Woman", and "The Female of the Species". That year Pickford also introduced Dorothy and Lillian Gish (both friends from her days in touring melodrama) to Griffith. Both became major silent stars, in comedy and tragedy, respectively. Pickford made her last Biograph picture, "The New York Hat," in late 1912.
She returned to Broadway in the David Belasco production of "A Good Little Devil" (1912). This was a major turning point in her career. Pickford, who had always hoped to conquer the Broadway stage, discovered how deeply she missed film acting. In 1913, she decided to work exclusively in film. The previous year, Adolph Zukor had formed Famous Players in Famous Plays. It was later known as Famous Players-Lasky and then Paramount Pictures, one of the first American feature film companies.
Pickford left the stage to join Zukor's roster of stars. Zukor believed film's potential lay in recording theatrical players in replicas of their most famous stage roles and productions. Zukor first filmed Pickford in a silent version of "A Good Little Devil". The film, produced in 1913, showed the play's Broadway actors reciting every line of dialogue, resulting in a stiff film that Pickford later called "one of the worst [features] I ever made ... it was deadly". Zukor agreed; he held the film back from distribution for a year.
Pickford's work in material written for the camera by that time had attracted a strong following. Comedy-dramas, such as "In the Bishop's Carriage" (1913), "Caprice" (1913), and especially "Hearts Adrift" (1914), made her irresistible to moviegoers. "Hearts Adrift" was so popular that Pickford asked for the first of her many publicized pay raises based on the profits and reviews. The film marked the first time Pickford’s name was featured above the title on movie marquees. "Tess of the Storm Country" was released five weeks later. Biographer Kevin Brownlow observed that the film "sent her career into orbit and made her the most popular actress in America, if not the world".
Her appeal was summed up two years later by the February 1916 issue of "Photoplay" as "luminous tenderness in a steel band of gutter ferocity". Only Charlie Chaplin, who reportedly slightly surpassed Pickford's popularity in 1916, had a similarly spellbinding pull with critics and the audience. Each enjoyed a level of fame far exceeding that of other actors. Throughout the 1910s and 1920s, Pickford was believed to be the most famous woman in the world, or, as a silent-film journalist described her, "the best known woman who has ever lived, the woman who was known to more people and loved by more people than any other woman that has been in all history."
Stardom.
 Pickford starred in 52 features throughout her career. In 1916, Pickford signed a new contract with Zukor that granted her full authority over production of the films in which she starred, and a record-breaking salary of $10,000 a week. Occasionally, she played a child, in films such as "The Poor Little Rich Girl" (1917), "Rebecca of Sunnybrook Farm" (1917), "Daddy-Long-Legs" (1919) and "Pollyanna" (1920). Pickford's fans were devoted to these "Little Girl" roles, but they were not typical of her career.
In August 1918, Pickford's contract expired and, when refusing Zukor's terms for a renewal, she was offered $250,000 to leave the motion picture business. She declined, and went to First National Pictures, which agreed to her terms. In 1919, Pickford, along with D.W. Griffith, Charlie Chaplin, and Douglas Fairbanks, formed the independent film production company United Artists. Through United Artists, Pickford continued to produce and perform in her own movies; she could also distribute them as she chose. In 1920, Pickford's film "Pollyanna" grossed around $1,100,000. The following year, Pickford's film "Little Lord Fauntleroy" was also a success, and in 1923, "Rosita" grossed over $1,000,000 as well. During this period, she also made "Sparrows" (1926), which blended the Dickensian with newly minted German expressionist style, and the romantic comedy "My Best Girl" (1927).
The arrival of sound was her undoing. Pickford underestimated the value of adding sound to movies, claiming that "adding sound to movies would be like putting lipstick on the Venus de Milo".
She played a reckless socialite in "Coquette" (1929), a role for which her famous ringlets were cut into a 1920s bob. Pickford had already cut her hair in the wake of her mother's death in 1928. Fans were shocked at the transformation. Pickford's hair had become a symbol of female virtue, and when she cut it, the act made front-page news in "The New York Times" and other papers. "Coquette" was a success and won her an Academy Award for Best Actress, but the public failed to respond to her in the more sophisticated roles. Like most movie stars of the silent era, Pickford found her career fading as talkies became more popular among audiences.
Her next film, "The Taming of The Shrew", made with husband Douglas Fairbanks, was not well received at the box office. Established Hollywood actors were panicked by the impending arrival of the talkies. On March 29, 1928, "The Dodge Brothers Hour" was broadcast from Pickford's bungalow, featuring Douglas Fairbanks, Charlie Chaplin, Norma Talmadge, Gloria Swanson, John Barrymore, D.W. Griffith and Dolores del Rio, among others. They spoke on the radio show to prove that they could meet the challenge of talking movies.
But the transition came as Pickford was in her late 30s, no longer able to play the children, teenage spitfires, and feisty young women so adored by her fans. She was not suited for the sleekly elegant heroines of early sound. In 1933, Pickford underwent a Technicolor screen test for an animated/live action film version of "Alice in Wonderland", but Walt Disney discarded the project when Paramount released its own version of the book. Only one Technicolor still of her screen test still exists. She retired from acting in 1933; her last acting film was released in 1934. She continued to produce for others, however, including "Sleep, My Love" (1948; with Claudette Colbert) and "Love Happy" (1949;, with the Marx Brothers).
The film industry.
Pickford used her stature in the movie industry to promote a variety of causes. During World War I, she promoted the sale of Liberty Bonds, making an intensive series of fund-raising speeches that kicked off in Washington, D.C., where she sold bonds alongside Charles Chaplin, Douglas Fairbanks, Theda Bara, and Marie Dressler. Five days later she spoke on Wall Street to an estimated 50,000 people. Though Canadian-born, she was a powerful symbol of Americana, kissing the American flag for cameras and auctioning one of her world-famous curls for $15,000. In a single speech in Chicago she sold an estimated five million dollars' worth of bonds. She was christened the U.S. Navy's official "Little Sister"; the Army named two cannons after her and made her an honorary colonel.
At the end of World War I, Pickford conceived of the Motion Picture Relief Fund, an organization to help financially needy actors. Leftover funds from her work selling Liberty Bonds were put toward its creation, and in 1921, the Motion Picture Relief Fund (MPRF) was officially incorporated, with Joseph Schenck voted its first president and Pickford its vice president. In 1932, Pickford spearheaded the "Payroll Pledge Program", a payroll-deduction plan for studio workers who gave one half of one percent of their earnings to the MPRF. As a result, in 1940 the Fund was able to purchase land and build the Motion Picture Country House and Hospital, in Woodland Hills, California.
An astute businesswoman, Pickford became her own producer within three years of her start in features. According to her Foundation, "she oversaw every aspect of the making of her films, from hiring talent and crew to overseeing the script, the shooting, the editing, to the final release and promotion of each project." She demanded (and received) these powers in 1916, when she was under contract to Zukor's Famous Players In Famous Plays (later Paramount). Zukor also acquiesced to her refusal to participate in block-booking, the widespread practice of forcing an exhibitor to show a bad film of the studio's choosing in order to also show a Pickford film. In 1916, Pickford's films were distributed, singly, through a special distribution unit called Artcraft. The Mary Pickford Corporation was briefly Pickford's motion-picture production company.
In 1919, she increased her power by co-founding United Artists (UA) with Charlie Chaplin, D. W. Griffith, and her soon-to-be husband, Douglas Fairbanks. Before UA's creation, Hollywood studios were vertically integrated, not only producing films but forming chains of theaters. Distributors (also part of the studios) arranged for company productions to be shown in the company's movie venues. Filmmakers relied on the studios for bookings; in return they put up with what many considered creative interference. 
United Artists broke from this tradition. It was solely a distribution company, offering independent film producers access to its own screens as well as the rental of temporarily unbooked cinemas owned by other companies. Pickford and Fairbanks produced and shot their films after 1920 at the jointly owned Pickford-Fairbanks studio on Santa Monica Boulevard. The producers who signed with UA were true independents, producing, creating and controlling their work to an unprecedented degree. As a co-founder, as well as the producer and star of her own films, Pickford became the most powerful woman who has ever worked in Hollywood. By 1930, Pickford's acting career had largely faded. After retiring three years later, however, she continued to produce films for United Artists. She and Chaplin remained partners in the company for decades. Chaplin left the company in 1955, and Pickford followed suit in 1956, selling her remaining shares for three million dollars.
Personal life.
Pickford was married three times. She married Owen Moore, an Irish-born silent film actor, on January 7, 1911. It is believed she became pregnant by Moore in the early 1910s and had a miscarriage or an abortion. Some accounts suggest this resulted in her later inability to have children. The couple had numerous marital problems, notably Moore's alcoholism, insecurity about living in the shadow of Pickford's fame, and bouts of domestic violence. The couple lived apart for several years.
Pickford became secretly involved in a relationship with Douglas Fairbanks. They toured the US together in 1918 to promote Liberty Bond sales for the World War I effort. Around this time, Pickford also suffered from the flu during the 1918 flu pandemic, but survived. Pickford divorced Moore on March 2, 1920, and married Fairbanks on March 28 of the same year. They went to Europe for their honeymoon; fans in London and in Paris caused riots trying to get to the famous couple. The couple's triumphant return to Hollywood was witnessed by vast crowds who turned out to hail them at railway stations across the United States.
"The Mark of Zorro" (1920) and a series of other swashbucklers gave the popular Fairbanks a more romantic, heroic image. Pickford continued to epitomize the virtuous but fiery girl next door. Even at private parties, people instinctively stood up when Pickford entered a room; she and her husband were often referred to as "Hollywood royalty". Their international reputations were broad. Foreign heads of state and dignitaries who visited the White House often asked if they could also visit Pickfair, the couple's mansion in Beverly Hills.
Dinners at Pickfair included a number of notable guests. Charlie Chaplin, Fairbanks' best friend, was often present. Other guests included George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, H. G. Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, Baron Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder, and Meher Baba, among others. The public nature of Pickford's second marriage strained it to the breaking point. Both she and Fairbanks had little time off from producing and acting in their films. They were also constantly on display as America's unofficial ambassadors to the world, leading parades, cutting ribbons, and making speeches. When their film careers both began to founder at the end of the silent era, Fairbanks' restless nature prompted him to overseas travel (something which Pickford did not enjoy). When Fairbanks' romance with Sylvia, Lady Ashley became public in the early 1930s, he and Pickford separated. They divorced January 10, 1936. Fairbanks' son by his first wife, Douglas Fairbanks Jr., claimed his father and Pickford long regretted their inability to reconcile.
On June 24, 1937, Pickford married her third and last husband, actor and band leader Buddy Rogers. They adopted two children: Roxanne (born 1944, adopted 1944) and Ronald Charles (born 1937, adopted 1943, a.k.a. Ronnie Pickford Rogers). As a PBS "American Experience" documentary noted, Pickford's relationship with her children was tense. She criticized their physical imperfections, including Ronnie's small stature and Roxanne's crooked teeth. Both children later said their mother was too self-absorbed to provide real maternal love. In 2003, Ronnie recalled that "Things didn't work out that much, you know. But I'll never forget her. I think that she was a good woman."
Later years.
After retiring from the screen, Pickford became an alcoholic, as her father had been. Her mother Charlotte died of breast cancer in March 1928. Her siblings, Lottie and Jack, both died of alcohol-related causes. These deaths, her divorce from Fairbanks, and the end of silent films left Pickford deeply depressed. Her relationship with her children, Roxanne and Ronald, was turbulent at best. Pickford withdrew and gradually became a recluse, remaining almost entirely at Pickfair and allowing visits only from Lillian Gish, her stepson Douglas Fairbanks, Jr., and few other people. She appeared in court in 1959, in a matter pertaining to her co-ownership of North Carolina TV station WSJS-TV. The court date coincided with the date of her 67th birthday; under oath, when asked to give her age, Pickford replied, "I'm 21, going on 20".
In the mid-1960s, Pickford often received visitors only by telephone, speaking to them from her bedroom. Buddy Rogers often gave guests tours of Pickfair, including views of a genuine western bar Pickford had bought for Douglas Fairbanks, and a portrait of Pickford in the drawing room. A print of this image now hangs in the Library of Congress. In addition to her Oscar as best actress for "Coquette" (1929), Mary Pickford received an Academy Honorary Award in 1976 for a lifetime of achievements. The Academy sent a TV crew to her house to record her short statement of thanks – offering the public a very rare glimpse into Pickfair Manor.
Pickford had become an American citizen upon her marriage to Fairbanks in 1920. Toward the end of her life, Pickford made arrangements with the Department of Citizenship to regain her Canadian citizenship because she wished to "die as a Canadian". Her request was approved and she became a dual Canadian-American citizen.
Death.
On May 29, 1979, Pickford died at a Santa Monica hospital of complications from a cerebral hemorrhage she had suffered the week before. She was interred in the Garden of Memory of the Forest Lawn Memorial Park cemetery in Glendale, California. Buried alongside her in the Pickford private family plot are her mother Charlotte, her siblings Lottie and Jack Pickford, and the family of Elizabeth Watson, Charlotte's sister, who had helped raise Pickford in Toronto.

</doc>
<doc id="18824" url="http://en.wikipedia.org/wiki?curid=18824" title="Mack Sennett">
Mack Sennett

Mack Sennett (January 17, 1880 – November 5, 1960) was a Canadian-born American director and actor and was known as the innovator of slapstick comedy in film. During his lifetime he was known at times as the "King of Comedy". His short "Wrestling Swordfish" was awarded the Academy Award for Best Live Action Short Film in 1932 and he earned an Academy Honorary Award in 1937.
Early life.
Born as Mikall (or Michael) Sinnott in Danville, Quebec, Canada, the son of Irish Catholic immigrant farmers, his father was a blacksmith in the small Eastern Townships village. When he was 17 years old his family moved to Connecticut.
The family lived for a time in Northampton, Massachusetts, where, according to his autobiography, Sennett first got the idea to become an opera singer after seeing a vaudeville show. He claimed that the most respected lawyer in town, Northampton mayor (and future President of the United States) Calvin Coolidge, as well as Sennett's own mother, tried to talk him out of his musical ambitions.
In New York City, Sennett became an actor, singer, dancer, clown, set designer, and director for Biograph. A major distinction in his acting career, often overlooked, is the fact that Sennett played "Sherlock Holmes" 11 times, albeit as a parody, between 1911 and 1913. 
Keystone Studios.
With financial backing from Adam Kessel and Charles O. Bauman of the New York Motion Picture Company, in 1912 Sennett founded Keystone Studios in Edendale, California, (which is now a part of Echo Park). The original main building, the first totally enclosed film stage and studio in history, is still there. Many important actors cemented their film careers with Sennett, including Marie Dressler, Mabel Normand, Charles Chaplin, Harry Langdon, Roscoe Arbuckle, Harold Lloyd, Raymond Griffith, Gloria Swanson, Ford Sterling, Andy Clyde, Chester Conklin, Polly Moran, Louise Fazenda, The Keystone Cops, Bing Crosby, and W. C. Fields.
Sennett's slapstick comedies were noted for their wild car chases and custard pie warfare. His first comedienne was Mabel Normand, who became a major star (and with whom he embarked on a tumultuous personal relationship). Sennett developed the "Kid Comedies", a forerunner of the "Our Gang" films, and in a short time his name became synonymous with screen comedy. In 1915 Keystone Studios became an autonomous production unit of the ambitious Triangle Film Corporation, as Sennett joined forces with movie bigwigs D. W. Griffith and Thomas Ince. 
Sennett Bathing Beauties.
Also beginning in 1915, Sennett assembled a bevy of girls known as the "Sennett Bathing Beauties" to appear in provocative bathing costumes in comedy short subjects, in promotional material, and in promotional events like Venice Beach beauty contests.
Two of those often named as Bathing Beauties do not qualify. Mabel Normand was a featured player, and her 1912 8-minute film "The Water Nymph" may have been the direct inspiration for the Bathing Beauties. Although Gloria Swanson worked for Sennett in 1916 and was photographed in a bathing suit, she was also a star and "vehemently denied" being one of the bathing beauties.
Not individually featured or named, many of these young women ascended to significant careers of their own. They included Juanita Hansen, Claire Anderson, Marie Prevost, Phyllis Haver, and Carole Lombard. In the 1920s Sennett's Bathing Beauties remained popular enough to provoke imitators like the Christie Studios' Bathing Beauties (counting Raquel Torres and Laura La Plante as alumnae) and Fox Film Corporation's "Sunshine Girls" (counting Janet Gaynor as alumna).
The Sennett Bathing Beauties would continue to appear through 1928.
Independent production.
In 1917 Sennett gave up the Keystone trademark and organized his own company, Mack Sennett Comedies Corporation. (Sennett's corporate bosses retained the Keystone trademark and produced a cheap series of comedy shorts that were "Keystones" in name only: they were unsuccessful, and Sennett had no connection with them.) Sennett went on to produce more ambitious comedy short films and a few feature-length films. 
During the 1920s his short subjects were in much demand, with stars like Billy Bevan, Andy Clyde, Harry Gribbon, Vernon Dent, Alice Day, Ralph Graves, Charlie Murray, and Harry Langdon. He produced several features with his brightest stars, such as Ben Turpin and Mabel Normand.
Many of Sennett's films of the early 1920s were inherited by Warner Brothers when Warners merged with the original distributor, First National. Warner added music and commentary to several of these shorts, but eventually destroyed the original elements for storage space. As a result, many Sennett films, especially those from his most productive and creative period, no longer exist. 
Move to Pathé Exchange.
In the mid-1920s Sennett moved over to Pathé Exchange distribution. Pathé had a huge market share, but made bad corporate decisions, such as attempting to sell too many comedies at once (including those of Sennett's main competitor, Hal Roach). In 1927 Paramount and MGM, Hollywood's two top studios, noting the profits being made by companies like Pathé Exchange and Educational, both re-entered the production and distribution of short subjects after several years. Roach signed with MGM, but Sennett found himself and Pathé Exchange in hard times, because the hundreds of exhibitors who had previously rented their shorts had switched to the new MGM or Paramount products. 
Experiments, awards, and bankruptcy.
Sennett made a reasonably smooth transition to sound films, releasing them through Earle Hammons's Educational Pictures. Sennett occasionally experimented with color and was the first to get a talkie short subject on the market, in 1928. In 1932 he was nominated for the Academy Award for Live Action Short Film in the comedy division for producing "The Loud Mouth" (with Matt McHugh, in the sports-heckler role later taken in Columbia Pictures remakes by Charley Chase and Shemp Howard), and he won in the novelty division for his film "Wrestling Swordfish".
Sennett often clung to outmoded techniques, making his early-1930s films seem dated and quaint. This doomed his attempt to re-enter the feature film market with "Hypnotized" (starring blackface comedians Moran and Mack, "The Two Black Crows"). However, Sennett enjoyed great success with short comedies starring Bing Crosby; these films were probably instrumental in Sennett's product being picked up by a major studio, Paramount Pictures. W. C. Fields conceived and starred in four famous Sennett-Paramount comedies. Fields himself recalled that he "made seven comedies for the Irishman" (his original deal called for one film and an option for six more), but ultimately only four were made.
Sennett's studio did not survive the Great Depression; the Sennett-Paramount partnership lasted only one year, and Sennett was forced into bankruptcy in November 1933. 
On January 12, 1934 he was in an automobile accident that killed Charles E. Mack in Mesa, Arizona.
His last work, in 1935, was as a producer-director for Educational Pictures; he directed Buster Keaton in "The Timid Young Man" and Joan Davis in "Way Up Thar". (The 1935 Vitaphone short subject "Keystone Hotel" is not a Sennett production; it featured several alumni from the Sennett studio, but Sennett himself had no connection with the film.)
Mack Sennett went into semi-retirement at the age of 55, having produced more than 1,000 silent films and several dozen talkies during a 25-year career. His studio property was purchased by Mascot Pictures (later part of Republic Pictures), and many of his former staffers found work at Columbia Pictures.
In March 1938, Sennett was presented with an honorary Academy Award: "for his lasting contribution to the comedy technique of the screen, the basic principles of which are as important today as when they were first put into practice, the Academy presents a Special Award to that master of fun, discoverer of stars, sympathetic, kindly, understanding comedy genius - Mack Sennett."
Later projects.
Rumors abounded that Sennett would be returning to film production (a 1938 publicity release indicated that he would be working with Stan Laurel of Laurel and Hardy), but apart from Sennett reissuing a couple of his Bing Crosby two-reelers to theaters, nothing happened. Sennett did appear in front of the camera, however, in "Hollywood Cavalcade" (1939), itself a thinly disguised version of the Mack Sennett-Mabel Normand romance. In 1949 he provided film footage for, and appeared in, the first full-length comedy compilation, "Down Memory Lane" (1949), which was written and narrated by Steve Allen. Sennett was profiled in the television series "This is Your Life" in 1954, and made a cameo appearance (for $1,000) in "Abbott and Costello Meet the Keystone Kops" (1955). He contributed to the radio program "Biography in Sound", broadcast February 28, 1956.
Death.
He died on November 5, 1960 in Woodland Hills, California, aged 80, and was interred in the Holy Cross Cemetery in Culver City, California.
Tributes.
For his contribution to the motion picture industry Sennett was given a star on the Hollywood Walk of Fame at 6712 Hollywood Blvd. Also in 2004, he was inducted into .
The Keystone legacy.
Some historians credit Sennett's films with having been responsible for municipal police forces across North America altering their uniforms to include military style officers' caps since by the 1920s tall, British-style hats had become so indelibly associated with slapstick comedy. 
A line in a Henry Kuttner science fiction short story, Piggy Bank, reads "Within seconds the scene resembled a Mack Sennett pie-throwing comedy." 
Henry Mancini's score for the 1963 film, "The Pink Panther", the original entry in the series, contains a segment called "Shades of Sennett". It is played on a silent film era style "honky tonk" piano, and accompanies a climactic scene in which the incompetent police detective Inspector Clouseau is involved in a multi-vehicle chase with the antagonists.
In 1974, Michael Stewart and Jerry Herman wrote the musical "Mack & Mabel", chronicling the romance between Sennett and Mabel Normand. Sennett also was a leading character in "The Biograph Girl", a 1980 musical about the silent film era.
Peter Lovesey's 1983 novel "Keystone" is a whodunnit set in the Keystone Studios and involving (among others), Mack Sennett, Mabel Normand, Roscoe Arbuckle, and the Keystone Cops.
Dan Aykroyd portrays Mack Sennett in the 1992 movie "Chaplin". Marisa Tomei plays Mabel Normand and Robert Downey, Jr. stars as Charlie Chaplin.
Joseph Beattie and Andrea Deck portray Mack Sennett and Mabel Normand, respectively, in episode 8 of series two of ITV's "Mr. Selfridge".

</doc>
<doc id="18825" url="http://en.wikipedia.org/wiki?curid=18825" title="Motion Picture Patents Company">
Motion Picture Patents Company

The Motion Picture Patents Company (MPPC, also known as the Edison Trust), founded in December 1908, was a trust of all the major American film companies (Edison, Biograph, Vitagraph, Essanay, Selig Polyscope, Lubin Manufacturing, Kalem Company, Star Film Paris, American Pathé), the leading film distributor (George Kleine) and the biggest supplier of raw film stock, Eastman Kodak. The MPPC ended the domination of foreign films on American screens, standardized the manner in which films were distributed and exhibited in America, and improved the quality of American motion pictures by internal competition. But it also discouraged its members' entry into feature film production, and the use of outside financing, both to its members' eventual detriment.
Creation.
The MPPC was preceded by the Edison licensing system, in effect in 1907–1908, on which the MPPC was modeled. Since the 1890s, Thomas Edison owned most of the major American patents relating to motion picture cameras. The Edison Manufacturing Company's patent lawsuits against each of its domestic competitors crippled the American film industry, reducing American production mainly to two companies: Edison and Biograph, which used a different camera design. This left Edison's other rivals with little recourse but to import French and British films. 
Since 1902, Edison had also been notifying distributors and exhibitors that if they did not use Edison machines and films exclusively, they would be subject to litigation for supporting filmmaking that infringed Edison's patents. Exhausted by the lawsuits, Edison's competitors — Essanay, Kalem, Pathé Frères, Selig, and Vitagraph — approached him in 1907 to negotiate a licensing agreement, which Lubin was also invited to join. The one notable filmmaker excluded from the licensing agreement was Biograph, which Edison hoped to squeeze out of the market. No further applicants could become licensees. The purpose of the licensing agreement, according to an Edison lawyer, was to "preserve the business of present manufacturers and not to throw the field open to all competitors."
The addition of Biograph.
Biograph retaliated for being frozen out of the Trust agreement by purchasing the patent to the Latham film loop, a key feature of virtually all motion picture cameras then in use. Edison sued to gain control of the patent; however, after a federal court upheld the validity of the patent in 1907, Edison began negotiation with Biograph in May 1908 to reorganize the Edison licensing system. The resulting trust pooled 16 motion picture patents. Ten were considered of minor importance; the remaining key six pertained one each to films, cameras, and the Latham loop, and three to projectors.
Policies.
The MPPC eliminated the outright sale of films to distributors and exhibitors, replacing it with rentals, which allowed quality control over prints that had formerly been exhibited long past their prime. The Patents Company also established a uniform rental rate for all licensed films, thereby removing price as a factor for the exhibitor in film selection, in favor of selection made on quality, which in turn encouraged the upgrading of production values. 
However, the MPPC also established a monopoly on all aspects of filmmaking. Eastman Kodak, which owned the patent on raw film stock, was a member of the Trust and thus agreed to only sell stock to other members. Likewise, the Trust's control of patents on motion picture cameras ensured that only MPPC studios were able to film, and the projector patents allowed the Trust to make licensing agreements with distributors and theaters – and thus determine who screened their films and where.
The patents owned by the MPPC allowed them to use federal law enforcement officials to enforce their licensing agreements and to prevent unauthorized use of their cameras, films, projectors, and other equipment. In some cases, however, the MPPC made use of hired thugs and mob connections to violently disrupt productions that were not licensed by the Trust.
Content.
The MPPC also strictly regulated the production content of their films, primarily as a means of cost control. Films were initially limited to one reel in length (13–17 minutes), although competition by independent and foreign producers by 1912 led to the introduction of two-reelers, and by 1913, three- and four-reelers.
Backlash and decline.
Many independent filmmakers, who controlled from one-quarter to one-third of the domestic marketplace, responded to the creation of the MPPC by moving their operations to Hollywood, whose distance from Edison's home base of New Jersey made it more difficult for the MPPC to enforce its patents. The Ninth Circuit Court of Appeals, which is headquartered in San Francisco, California, and covers the area, was averse to enforcing patent claims. Southern California was also chosen because of its beautiful year-round weather and varied countryside; its topography, semi-arid climate and widespread irrigation gave its landscapes the ability to offer motion picture shooting scenes set in deserts, jungles and great mountains. 
The reasons for The MPPC's decline are manifold. The first blow came in 1911, when Eastman Kodak modified its exclusive contract with the MPPC, to allow Kodak to sell its raw film stock, which led the industry in quality and price, to unlicensed independents. The number of theaters exhibiting independent films grew by 33 percent within twelve months, to half of all houses.
Another reason was the MPPC's overestimation of the efficiency of controlling the motion picture industry through patent litigation and the exclusion of independents from licensing. The slow process of using detectives to investigate patent infringements, and of obtaining injunctions against the infringers, was outpaced by the dynamic rise of new companies in diverse locations.
Despite the rise in popularity of feature films in 1912–1913 from independent producers and foreign imports, the MPPC was very reluctant to make the changes necessary to distribute such longer films. Edison, Biograph, Essanay, and Vitagraph did not release their first features until 1914, after dozens, if not hundreds, of feature films had been released by independents.
Patent royalties to the MPPC ended in September 1913 with the expiration of the last of the patents filed in the mid-1890s at the dawn of commercial film production and exhibition. Thus the MPPC lost the ability to control the American film industry through patent licensing, and had to rely instead on its subsidiary, the General Film Company, formed in 1910, which monopolized film distribution in America.
The outbreak of World War I in 1914 cut off most of the European market, which played a much more significant part of the revenue and profit for MPPC members than for the independents, who concentrated on Westerns produced for a primarily American market.
The end came with a federal court decision in "United States v. Motion Picture Patents Co." on October 1, 1915, which ruled that the MPPC's acts went "far beyond what was necessary to protect the use of patents or the monopoly which went with them" and was therefore an illegal restraint of trade under the Sherman Antitrust Act. An appellate court dismissed the Patent Company's appeal, and officially terminated the MPPC in 1918.

</doc>
<doc id="18826" url="http://en.wikipedia.org/wiki?curid=18826" title="MD5">
MD5

The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety of cryptographic applications, and is also commonly used to verify data integrity.
MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function, MD4. The source code in RFC 1321 contains a "by attribution" RSA license.
In 1996 a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1—which has since been found to be vulnerable as well.
In 2004 it was shown that MD5 is not collision resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity, and CMU Software Engineering Institute now says that MD5 "should be considered cryptographically broken and unsuitable for further use", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.
History and cryptanalysis.
MD5 is one in a series of message digest algorithms designed by Professor Ronald Rivest of MIT (Rivest, 1992). When analytic work indicated that MD5's predecessor MD4 was likely to be insecure, MD5 was designed in 1991 to be a secure replacement. (Weaknesses were indeed later found in MD4 by Hans Dobbertin.)
In 1993, Den Boer and Bosselaers gave an early, although limited, result of finding a "pseudo-collision" of the MD5 compression function; that is, two different initialization vectors which produce an identical digest.
In 1996, Dobbertin announced a collision of the compression function of MD5 (Dobbertin, 1996). While this was not an attack on the full MD5 hash function, it was close enough for cryptographers to recommend switching to a replacement, such as SHA-1 or RIPEMD-160.
The size of the hash value (128 bits) is small enough to contemplate a birthday attack. MD5CRK was a distributed project started in March 2004 with the aim of demonstrating that MD5 is practically insecure by finding a collision using a birthday attack.
MD5CRK ended shortly after 17 August 2004, when collisions for the full MD5 were announced by Xiaoyun Wang, Dengguo Feng, Xuejia Lai, and Hongbo Yu. Their analytical attack was reported to take only one hour on an IBM p690 cluster.
On 1 March 2005, Arjen Lenstra, Xiaoyun Wang, and Benne de Weger demonstrated construction of two X.509 certificates with different public keys and the same MD5 hash value, a demonstrably practical collision. The construction included private keys for both public keys. A few days later, Vlastimil Klima described an improved algorithm, able to construct MD5 collisions in a few hours on a single notebook computer. On 18 March 2006, Klima published an algorithm that can find a collision within one minute on a single notebook computer, using a method he calls tunneling.
Various MD5-related RFC errata have been published. 
In 2009, the United States Cyber Command used an MD5 hash value of their mission statement as a part of their official emblem.
On 24 December 2010, Tao Xie and Dengguo Feng announced the first published single-block (512 bit) MD5 collision. Previous collision discoveries relied on multi-block attacks. For "security reasons", Xie and Feng did not disclose the new attack method. They have issued a challenge to the cryptographic community, offering a US$10,000 reward to the first finder of a different 64-byte collision before 1 January 2013. Marc Stevens responded to the challenge and published colliding single-block messages as well as the construction algorithm and sources.
In 2011, an informational RFC 6151 was approved to update the security considerations in MD5 and HMAC-MD5.
Security.
The security of the MD5 hash function is severely compromised. A collision attack exists that can find collisions within seconds on a computer with a 2.6 GHz Pentium 4 processor (complexity of 224.1). Further, there is also a chosen-prefix collision attack that can produce a collision for two inputs with specified prefixes within hours, using off-the-shelf computing hardware (complexity 239).
The ability to find collisions has been greatly aided by the use of off-the-shelf GPUs. On an NVIDIA GeForce 8400GS graphics processor, 16–18 million hashes per second can be computed. An NVIDIA GeForce 8800 Ultra can calculate more than 200 million hashes per second.
These hash and collision attacks have been demonstrated in the public in various situations, including colliding document files and digital certificates.
Collision vulnerabilities.
In 1996, collisions were found in the compression function of MD5, and Hans Dobbertin wrote in the RSA Laboratories technical newsletter, "The presented attack does not yet threaten practical applications of MD5, but it comes rather close ... in the future MD5 should no longer be implemented...where a collision-resistant hash function is required."
In 2005, researchers were able to create pairs of PostScript documents and X.509 certificates with the same hash. Later that year, MD5's designer Ron Rivest wrote, "md5 and sha1 are both clearly broken (in terms of collision-resistance)."
On 30 December 2008, a group of researchers announced at the 25th Chaos Communication Congress how they had used MD5 collisions to create an intermediate certificate authority certificate which appeared to be legitimate when checked via its MD5 hash. The researchers used a cluster of Sony PlayStation 3 units at the EPFL in Lausanne, Switzerland to change a normal SSL certificate issued by RapidSSL into a working CA certificate for that issuer, which could then be used to create other certificates that would appear to be legitimate and issued by RapidSSL. VeriSign, the issuers of RapidSSL certificates, said they stopped issuing new certificates using MD5 as their checksum algorithm for RapidSSL once the vulnerability was announced. Although Verisign declined to revoke existing certificates signed using MD5, their response was considered adequate by the authors of the exploit (Alexander Sotirov, Marc Stevens, Jacob Appelbaum, Arjen Lenstra, David Molnar, Dag Arne Osvik, and Benne de Weger). Bruce Schneier wrote of the attack that "we already knew that MD5 is a broken hash function" and that "no one should be using MD5 anymore". The SSL researchers wrote, "Our desired impact is that Certification Authorities will stop using MD5 in issuing new certificates. We also hope that use of MD5 in other applications will be reconsidered as well."
In 2012, according to Microsoft, the authors of the Flame malware used an MD5 collision to forge a Windows code-signing certificate.
MD5 uses the Merkle–Damgård construction, so if two prefixes with the same hash can be constructed, a common suffix can be added to both to make the collision more likely to be accepted as valid data by the application using it. Furthermore, current collision-finding techniques allow to specify an arbitrary "prefix": an attacker can create two colliding files that both begin with the same content. All the attacker needs to generate two colliding files is a template file with a 128-byte block of data, aligned on a 64-byte boundary that can be changed freely by the collision-finding algorithm. An example MD5 collision, with the two messages differing in 6 bits, is:
 d131dd02c5e6eec4 693d9a0698aff95c 2fcab58712467eab 4004583eb8fb7f89
 55ad340609f4b302 83e488832571415a 085125e8f7cdc99f d91dbdf280373c5b
 d8823e3156348f5b ae6dacd436c919c6 dd53e2b487da03fd 02396306d248cda0
 e99f33420f577ee8 ce54b67080a80d1e c69821bcb6a88393 96f9652b6ff72a70
 d131dd02c5e6eec4 693d9a0698aff95c 2fcab50712467eab 4004583eb8fb7f89
 55ad340609f4b302 83e4888325f1415a 085125e8f7cdc99f d91dbd7280373c5b
 d8823e3156348f5b ae6dacd436c919c6 dd53e23487da03fd 02396306d248cda0
 e99f33420f577ee8 ce54b67080280d1e c69821bcb6a88393 96f965ab6ff72a70
Both produce the MD5 hash 79054025255fb1a26e4bc422aef54eb4.
The difference between the two samples is the leading bit in each nibble has been flipped. For example, the 20th byte (offset 0x13) in the top sample, 0x87, is 10000111 in binary. The leading bit in the byte (also the leading bit in the first nibble) is flipped to make 00000111, which is 0x07 as shown in the lower sample.
Later it was also found to be possible to construct collisions between two files with separately chosen prefixes. This technique was used in the creation of the rogue CA certificate in 2008. A new variant of parallelized collision searching using MPI was proposed by Anton Kuznetsov in 2014 which allowed to find a collision in 11 hours on a computing cluster.
Preimage vulnerability.
In April 2009, a preimage attack against MD5 was published that breaks MD5's preimage resistance. This attack is only theoretical, with a computational complexity of 2123.4 for full preimage.
Other vulnerabilities.
A number of projects have published MD5 rainbow tables online, which can be used to reverse many MD5 hashes into strings that collide with the original input, usually for the purposes of password cracking.
The use of MD5 in some websites' URLs means that search engines such as Google can also sometimes function as a limited tool for reverse lookup of MD5 hashes.
Both these techniques are rendered ineffective by the use of a sufficiently long salt.
Applications.
MD5 digests have been widely used in the software world to provide some assurance that a transferred file has arrived intact. For example, file servers often provide a pre-computed MD5 (known as Md5sum) checksum for the files, so that a user can compare the checksum of the downloaded file to it. Most unix-based operating systems include MD5 sum utilities in their distribution packages; Windows users may install a Microsoft utility, or use third-party applications. Android ROMs also utilize this type of checksum.
However, now that it is easy to generate MD5 collisions, it is possible for the person who created the file to create a second file with the same checksum, so this technique cannot protect against some forms of malicious tampering. Also, in some cases, the checksum cannot be trusted (for example, if it was obtained over the same channel as the downloaded file), in which case MD5 can only provide error-checking functionality: it will recognize a corrupt or incomplete download, which becomes more likely when downloading larger files.
MD5 can be used to store a one-way hash of a password, often with key stretching. Along with other hash functions, it is also used in the field of electronic discovery, in order to provide a unique identifier for each document that is exchanged during the legal discovery process. This method can be used to replace the Bates stamp numbering system that has been used for decades during the exchange of paper documents.
Algorithm.
MD5 processes a variable-length message into a fixed-length output of 128 bits. The input message is broken up into chunks of 512-bit blocks (sixteen 32-bit words); the message is padded so that its length is divisible by 512. The padding works as follows: first a single bit, 1, is appended to the end of the message. This is followed by as many zeros as are required to bring the length of the message up to 64 bits fewer than a multiple of 512. The remaining bits are filled up with 64 bits representing the length of the original message, modulo 264.
The main MD5 algorithm operates on a 128-bit state, divided into four 32-bit words, denoted "A", "B", "C", and "D". These are initialized to certain fixed constants. The main algorithm then uses each 512-bit message block in turn to modify the state. The processing of a message block consists of four similar stages, termed "rounds"; each round is composed of 16 similar operations based on a non-linear function "F", modular addition, and left rotation. Figure 1 illustrates one operation within a round. There are four possible functions "F"; a different one is used in each round:
formula_5 denote the XOR, AND, OR and NOT operations respectively.
Pseudocode.
The MD5 hash is calculated according to this algorithm. All values are in little-endian.
 //"Note: All variables are unsigned 32 bit and wrap modulo 2^32 when calculating"
 var "int"[64] s, K
 //"s specifies the per-round shift amounts"
 //"Use binary integer part of the sines of integers (Radians) as constants:"
 for i from 0 to 63
 K[i] := floor(abs(sin(i + 1)) × (2 pow 32))
 end for
 //"(Or just use the following table):"
 //"Initialize variables:"
 var "int" a0 := 0x67452301 //A
 var "int" b0 := 0xefcdab89 //B
 var "int" c0 := 0x98badcfe //C
 var "int" d0 := 0x10325476 //D
 //"Pre-processing: adding a single 1 bit"
 append "1" bit to message 
 /* Notice: the input bytes are considered as bits strings,
 where the first bit is the most significant bit of the byte.
 
 //"Pre-processing: padding with zeros"
 append "0" bit until message length in bits ≡ 448 (mod 512)
 append original length in bits mod (2 pow 64) to message
 
 //"Process the message in successive 512-bit chunks:"
 for each "512-bit" chunk of message
 break chunk into sixteen 32-bit words M[j], 0 ≤ j ≤ 15
 //"Initialize hash value for this chunk:"
 var "int" A := a0
 var "int" B := b0
 var "int" C := c0
 var "int" D := d0
 //"Main loop:"
 for i from 0 to 63
 if 0 ≤ i ≤ 15 then
 F := (B and C) or ((not B) and D)
 g := i
 else if 16 ≤ i ≤ 31
 F := (D and B) or ((not D) and C)
 g := (5×i + 1) mod 16
 else if 32 ≤ i ≤ 47
 F := B xor C xor D
 g := (3×i + 5) mod 16
 else if 48 ≤ i ≤ 63
 F := C xor (B or (not D))
 g := (7×i) mod 16
 dTemp := D
 D := C
 C := B
 B := B + leftrotate((A + F + K[i] + M[g]), s[i])
 A := dTemp
 end for
 //"Add this chunk's hash to result so far:"
 a0 := a0 + A
 b0 := b0 + B
 c0 := c0 + C
 d0 := d0 + D
 end for
 var "char" digest[16] := a0 append b0 append c0 append d0 //"(Output is in little-endian)"
 //"leftrotate function definition"
 leftrotate (x, c)
 return (x « c) binary or (x » (32-c));
"Note: Instead of the formulation from the original RFC 1321 shown, the following may be used for improved efficiency (useful if assembly language is being used – otherwise, the compiler will generally optimize the above code. Since each computation is dependent on another in these formulations, this is often slower than the above method where the nand/and can be parallelised):"
 ( 0 ≤ i ≤ 15): F := D xor (B and (C xor D))
 (16 ≤ i ≤ 31): F := C xor (D and (B xor C))
MD5 hashes.
The 128-bit (16-byte) MD5 hashes (also termed "message digests") are typically represented as a sequence of 32 hexadecimal digits. The following demonstrates a 43-byte ASCII input and the corresponding MD5 hash:
 MD5("The quick brown fox jumps over the lazy dog") =
 9e107d9d372bb6826bd81d3542a419d6
Even a small change in the message will (with overwhelming probability) result in a mostly different hash, due to the avalanche effect. For example, adding a period to the end of the sentence:
 MD5("The quick brown fox jumps over the lazy dog.") = 
 e4d909c290d0fb1ca068ffaddf22cbd0
The hash of the zero-length string is:
 MD5("") = 
 d41d8cd98f00b204e9800998ecf8427e
The MD5 algorithm is specified for messages consisting of any number of bits; it is not limited to multiples of eight bit (octets, bytes) as shown in the examples above. Some MD5 implementations such as md5sum might be limited to octets, or they might not support "streaming" for messages of an initially undetermined length
External links.
 

</doc>
<doc id="18831" url="http://en.wikipedia.org/wiki?curid=18831" title="Mathematics">
Mathematics

Mathematics (from Greek μάθημα "máthēma", “knowledge, study, learning”) is the study of topics such as quantity (numbers), structure, space, and change. There is a range of views among mathematicians and philosophers as to the exact scope and definition of mathematics. 
Mathematicians seek out patterns and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity for as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.
Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's "Elements". Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.
Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth." Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences". Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions". David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise." Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality." French mathematician Claire Voisin states "There is creative drive in mathematics, it's all about movement trying to express itself." 
Mathematics is used throughout the world as an essential tool in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics, the branch of mathematics concerned with application of mathematical knowledge to other fields, inspires and makes use of new mathematical discoveries, which has led to the development of entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.
History.
Evolution.
The evolution of mathematics might be seen as an ever-increasing series of abstractions, or alternatively an expansion of subject matter. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.
Evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.
More complex mathematics did not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time.
In Babylonian mathematics elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus.
Between 600 and 300 BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics.
Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the "Bulletin of the American Mathematical Society", "The number of papers and books included in the "Mathematical Reviews" database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."
Etymology.
The word "mathematics" comes from the Greek μάθημα ("máthēma"), which, in the ancient Greek language, means "that which is learnt", "what one gets to know", hence also "study" and "science", and in modern Greek just "lesson". The word "máthēma" is derived from μανθάνω ("manthano"), while the modern Greek equivalent is μαθαίνω ("mathaino"), both of which mean "to learn". In Greece, the word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times. Its adjective is μαθηματικός ("mathēmatikós"), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, μαθηματικὴ τέχνη ("mathēmatikḗ tékhnē"), Latin: "ars mathematica", meant "the mathematical art".
In Latin, and in English until around 1700, the term "mathematics" more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations: a particularly notorious one is Saint Augustine's warning that Christians should beware of "mathematici" meaning astrologers, which is sometimes mistranslated as a condemnation of mathematicians.
The apparent plural form in English, like the French plural form "les mathématiques" (and the less commonly used singular derivative "la mathématique"), goes back to the Latin neuter plural "mathematica" (Cicero), based on the Greek plural τα μαθηματικά ("ta mathēmatiká"), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective "mathematic(al)" and formed the noun "mathematics" anew, after the pattern of physics and metaphysics, which were inherited from the Greek. In English, the noun "mathematics" takes singular verb forms. It is often shortened to "maths" or, in English-speaking North America, "math".
Definitions of mathematics.
Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century. Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals. There is not even consensus on whether mathematics is an art or a science. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. Some just say, "Mathematics is what mathematicians do."
Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe problems, none has widespread acceptance, and no reconciliation seems possible.
An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870). In the "Principia Mathematica", Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proven entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).
Intuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other." A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proven to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.
Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems". A formal system is a set of symbols, or "tokens", and some "rules" telling how the tokens may be combined into "formulas". In formal systems, the word "axiom" has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.
Inspiration, pure and applied mathematics, and aesthetics.
Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics. 
Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.
For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the "elegance" of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in "A Mathematician's Apology" expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erdős. The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.
Notation, language, and rigor.
Most of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, a painstaking process that limited mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. It is extremely compressed: a few symbols contain a great deal of information. Like musical notation, modern mathematical notation has a strict syntax (which to a limited extent varies from author to author and from discipline to discipline) and encodes information that would be difficult to write in any other way.
Mathematical language can be difficult to understand for beginners. Words such as "or" and "only" have more precise meanings than in everyday speech. Moreover, words such as "open" and "field" have been given specialized mathematical meanings. Technical terms such as "homeomorphism" and "integrable" have precise meanings in mathematics. Additionally, shorthand phrases such as "iff" for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".
Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.
Axioms in traditional thought were "self-evident truths", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.
Fields of mathematics.
Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty.
Foundations and philosophy.
In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.
Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if "sound" (meaning that all theorems that can be proven are true), is necessarily "incomplete" (meaning that there are true theorems which cannot be proved "in that system"). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory.
Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.
Pure mathematics.
Quantity.
The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.
As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". Another area of study is size, which leads to the cardinal numbers and then to another conception of infinity: the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.
Structure.
Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra. 
By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.
Space.
The study of space originates with geometry – in particular, Euclidean geometry. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions; it combines space and numbers, and encompasses the well-known Pythagorean theorem. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.
Change.
Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.
Applied mathematics.
Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, "applied mathematics" focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.
In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.
Statistics and other decision sciences.
Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.
Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.
Computational mathematics.
Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.
Mathematical awards.
Arguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and now awarded every four years. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize. 
The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was introduced in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.
A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. A solution to each of these problems carries a $1 million reward, and only one (the Riemann hypothesis) is duplicated in Hilbert's problems.
Mathematics as science.
Gauss referred to mathematics as "the Queen of the Sciences". In the original Latin "Regina Scientiarum", as well as in German "Königin der Wissenschaften", the word corresponding to "science" means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to "natural science" follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as psychology, biology, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality." More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery".
Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper. However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently." Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.
An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. The theoretical physicist J.M. Ziman proposed that science is "public knowledge", and thus includes mathematics. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.
The opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is "created" (as in art) or "discovered" (as in science). It is common to see universities divided into sections that include a division of "Science and Mathematics", indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.
References.
</dl>
Further reading.
</dl>
External links.
 
 At Wikiversity, you can learnmore and teach others about Mathematics at the .
</dl>

</doc>
<doc id="18835" url="http://en.wikipedia.org/wiki?curid=18835" title="Manhattan (disambiguation)">
Manhattan (disambiguation)

Manhattan is a borough of New York City, primarily on Manhattan Island.
Manhattan may also refer to:
Places.
In the United States:
The arts.
In music:
In film and television:
In literature:

</doc>
<doc id="18836" url="http://en.wikipedia.org/wiki?curid=18836" title="Middle Ages">
Middle Ages

In European history, the Middle Ages, or Medieval period, lasted from the 5th to the 15th century. It began with the collapse of the Western Roman Empire and merged into the Renaissance and the Age of Discovery. The Middle Ages is the middle period of the three traditional divisions of Western history: Antiquity, Medieval period, and Modern period. The Medieval period is itself subdivided into the Early, the High, and the Late Middle Ages.
Depopulation, deurbanisation, invasion, and movement of peoples, which had begun in Late Antiquity, continued in the Early Middle Ages. The barbarian invaders, including various Germanic peoples, formed new kingdoms in what remained of the Western Roman Empire. In the 7th century, North Africa and the Middle East, once part of the Eastern Roman Empire came under the rule of the Caliphate, an Islamic empire, after conquest by Muhammad's successors. Although there were substantial changes in society and political structures, the break with Antiquity was not complete. The still-sizeable Byzantine Empire survived in the east and remained a major power. The empire's law code, the Code of Justinian, was rediscovered in Northern Italy in 1070 and became widely admired later in the Middle Ages. In the West, most kingdoms incorporated the few extant Roman institutions. Monasteries were founded as campaigns to Christianise pagan Europe continued. The Franks, under the Carolingian dynasty, briefly established an empire covering much of Western Europe; the Carolingian Empire during the later 8th and early 9th century, but it later succumbed to the pressures of internal civil wars combined with external invasions—Vikings from the north, Magyars from the east, and Saracens from the south.
During the High Middle Ages, which began after AD 1000, the population of Europe increased greatly as technological and agricultural innovations allowed trade to flourish and the Medieval Warm Period climate change allowed crop yields to increase. Manorialism, the organisation of peasants into villages that owed rent and labour services to the nobles, and feudalism, the political structure whereby knights and lower-status nobles owed military service to their overlords in return for the right to rent from lands and manors, were two of the ways society was organised in the High Middle Ages. The Crusades, first preached in 1095, were military attempts by Western European Christians to regain control of the Middle Eastern Holy Land from the Muslims. Kings became the heads of centralised nation states, reducing crime and violence but making the ideal of a unified Christendom more distant. Intellectual life was marked by scholasticism, a philosophy that emphasised joining faith to reason, and by the founding of universities. The theology of Thomas Aquinas, the paintings of Giotto, the poetry of Dante and Chaucer, the travels of Marco Polo, and the architecture of Gothic cathedrals such as Chartres are among the outstanding achievements of this period.
The Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which much diminished the population of Western Europe; between 1347 and 1350, the Black Death killed about a third of Europeans. Controversy, heresy, and schism within the Church paralleled the interstate conflict, civil strife, and peasant revolts that occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning the early modern period.
Etymology and periodisation.
The Middle Ages is one of the three major periods in the most enduring scheme for analysing European history: classical civilisation, or Antiquity; the Middle Ages; and the Modern Period.
Medieval writers divided history into periods such as the "Six Ages" or the "Four Empires", and considered their time to be the last before the end of the world. When referring to their own times, they spoke of them as being "modern". In the 1330s, the humanist and poet Petrarch referred to pre-Christian times as "antiqua" (or "ancient") and to the Christian period as "nova" (or "new"). Leonardo Bruni was the first historian to use tripartite periodisation in his "History of the Florentine People" (1442). Bruni and later historians argued that Italy had recovered since Petrarch's time, and therefore added a third period to Petrarch's two. The "Middle Ages" first appears in Latin in 1469 as "media tempestas" or "middle season". In early usage, there were many variants, including "medium aevum", or "middle age", first recorded in 1604, and "media saecula", or "middle ages", first recorded in 1625. The alternative term "medieval" (or occasionally "mediaeval") derives from "medium aevum". Tripartite periodisation became standard after the German historian Christoph Cellarius (1638–1707) divided history into three periods: Ancient, Medieval, and Modern.
The most commonly given starting point for the Middle Ages is 476, first used by Bruni. For Europe as a whole, 1500 is often considered to be the end of the Middle Ages, but there is no universally agreed upon end date. Depending on the context, events such as Christopher Columbus's first voyage to the Americas in 1492, the conquest of Constantinople by the Turks in 1453, or the Protestant Reformation in 1517 are sometimes used. English historians often use the Battle of Bosworth Field in 1485 to mark the end of the period. For Spain, dates commonly used are the death of King Ferdinand II in 1516, the death of Queen Isabella I of Castile in 1504, or the conquest of Granada in 1492. Historians from Romance-speaking countries tend to divide the Middle Ages into two parts: an earlier "High" and later "Low" period. English-speaking historians, following their German counterparts, generally subdivide the Middle Ages into three intervals: "Early", "High", and "Late". In the 19th century, the entire Middle Ages were often referred to as the "Dark Ages", but with the adoption of these subdivisions, use of this term was restricted to the Early Middle Ages, at least among historians.
Later Roman Empire.
The Roman Empire reached its greatest territorial extent during the 2nd century AD; the following two centuries witnessed the slow decline of Roman control over its outlying territories. Economic issues, including inflation, and external pressure on the frontiers combined to make the 3rd century politically unstable, with emperors coming to the throne only to be rapidly replaced by new usurpers. Military expenses increased steadily during the 3rd century, mainly in response to the war with Sassanid Persia, which revived in the middle of the 3rd century. The army doubled in size, and cavalry and smaller units replaced the legion as the main tactical unit. The need for revenue led to increased taxes and a decline in numbers of the curial, or landowning, class, and decreasing numbers of them willing to shoulder the burdens of holding office in their native towns. More bureaucrats were needed in the central administration to deal with the needs of the army, which led to complaints from civilians that there were more tax-collectors in the empire than tax-payers.
The Emperor Diocletian (r. 284–305) split the empire into separately administered eastern and western halves in 286; the empire was not considered divided by its inhabitants or rulers, as legal and administrative promulgations in one division were considered valid in the other. In 330, after a period of civil war, Constantine the Great (r. 306–337) refounded the city of Byzantium as the newly renamed eastern capital, Constantinople. Diocletian's reforms strengthened the governmental bureaucracy, reformed taxation, and strengthened the army, which bought the empire time but did not resolve the problems it was facing: excessive taxation, a declining birthrate, and pressures on its frontiers, among others. Civil war between rival emperors became common in the middle of the 4th century, diverting soldiers from the empire's frontier forces and allowing invaders to encroach. For much of the 4th century, Roman society stabilised in a new form that differed from the earlier classical period, with a widening gulf between the rich and poor, and a decline in the vitality of the smaller towns. Another change was the Christianisation, or conversion of the empire to Christianity, a gradual process that lasted from the 2nd to the 5th centuries.
In 376, the Ostrogoths, fleeing from the Huns, received permission from Emperor Valens (r. 364–378) to settle in the Roman province of Thracia in the Balkans. The settlement did not go smoothly, and when Roman officials mishandled the situation, the Ostrogoths began to raid and plunder. Valens, attempting to put down the disorder, was killed fighting the Ostrogoths at the Battle of Adrianople on 9 August 378. As well as the threat from such tribal confederacies from the north, internal divisions within the empire, especially within the Christian Church, caused problems. In 400, the Visigoths invaded the Western Roman Empire and, although briefly forced back from Italy, in 410 sacked the city of Rome. In 406 the Alans, Vandals, and Suevi crossed into Gaul; over the next three years they spread across Gaul and in 409 crossed the Pyrenees Mountains into modern-day Spain. The Migration Period began, where various people, initially largely Germanic peoples, moved across Europe. The Franks, Alemanni, and the Burgundians all ended up in northern Gaul while the Angles, Saxons, and Jutes settled in Britain. In the 430s the Huns began invading the empire; their king Attila (r. 434–453) led invasions into the Balkans in 442 and 447, Gaul in 451, and Italy in 452. The Hunnic threat remained until Attila's death in 453, when the Hunnic confederation he led fell apart. These invasions by the tribes completely changed the political and demographic nature of what had been the Western Roman Empire.
By the end of the 5th century the western section of the empire was divided into smaller political units, ruled by the tribes that had invaded in the early part of the century. The deposition of the last emperor of the west, Romulus Augustus, in 476 has traditionally marked the end of the Western Roman Empire. The Eastern Roman Empire, often referred to as the Byzantine Empire after the fall of its western counterpart, had little ability to assert control over the lost western territories. The Byzantine emperors maintained a claim over the territory, but none of the new kings in the west dared to elevate himself to the position of emperor of the west, Byzantine control of most of the Western Empire could not be sustained; the reconquest of the Italian peninsula and Mediterranean periphery by Justinian (r. 527–565) was the sole, and temporary, exception.
Early Middle Ages.
New societies.
The political structure of Western Europe changed with the end of the united Roman Empire. Although the movements of peoples during this period are usually described as "invasions", they were not just military expeditions but migrations of entire peoples into the empire. Such movements were aided by the refusal of the western Roman elites to support the army or pay the taxes that would have allowed the military to suppress the migration. The emperors of the 5th century were often controlled by military strongmen such as Stilicho (d. 408), Aspar (d. 471), Ricimer (d. 472), or Gundobad (d. 516), who were partly or fully of non-Roman background. When the line of western emperors ceased, many of the kings who replaced them were from the same background. Intermarriage between the new kings and the Roman elites was common. This led to a fusion of Roman culture with the customs of the invading tribes, including the popular assemblies that allowed free male tribal members more say in political matters than was common in the Roman state. Material artefacts left by the Romans and the invaders are often similar, and tribal items were often modelled on Roman objects. Much of the scholarly and written culture of the new kingdoms was also based on Roman intellectual traditions. An important difference was the gradual loss of tax revenue by the new polities. Many of the new political entities no longer supported their armies through taxes, instead relying on granting them land or rents. This meant there was less need for large tax revenues and so the taxation systems decayed. Warfare was common between and within the kingdoms. Slavery declined as the supply weakened, and society became more rural.
Between the 5th and 8th centuries, new peoples and powerful individuals filled the political void left by Roman centralised government. The Ostrogoths settled in Italy in the late 5th century under Theodoric (d. 526) and set up a kingdom marked by its co-operation between the Italians and the Ostrogoths, at least until the last years of Theodoric's reign. The Burgundians settled in Gaul, and after an earlier realm was destroyed by the Huns in 436 formed a new kingdom in the 440s. Between today's Geneva and Lyon, it grew to become the powerful realm of Burgundy in the late 5th and early 6th centuries. In northern Gaul, the Franks and Britons set up small polities. The Frankish Kingdom was centred in north-eastern Gaul, and the first king of whom much is known is Childeric (d. 481). Under Childeric's son Clovis (r. 509–511), the Frankish kingdom expanded and converted to Christianity. Britons, related to the natives of Britannia—modern-day Great Britain—settled in what is now Brittany. Other monarchies were established by the Visigoths in Iberia, the Suevi in north-western Iberia, and the Vandals in North Africa. In the 6th century, the Lombards settled in northern Italy, replacing the Ostrogothic kingdom with a grouping of duchies that occasionally selected a king to rule over them all. By the late 6th century this arrangement had been replaced by a permanent monarchy.
The invasions brought new ethnic groups to Europe, although some regions received a larger influx of new peoples than others. In Gaul for instance, the invaders settled much more extensively in the north-east than in the south-west. Slavic peoples settled in Central and Eastern Europe and the Balkan Peninsula. The settlement of peoples was accompanied by changes in languages. The Latin of the Western Roman Empire was gradually replaced by languages based on, but distinct from, Latin, collectively known as Romance languages. These changes from Latin to the new languages took many centuries. Greek remained the language of the Byzantine Empire, but the migrations of the Slavs added Slavonic languages to Eastern Europe.
Byzantine survival.
As Western Europe witnessed the formation of new kingdoms, the Eastern Roman Empire remained intact and experienced an economic revival that lasted into the early 7th century. There were fewer invasions of the eastern section of the empire; most occurred in the Balkans. Peace with Persia, the traditional enemy of Rome, lasted throughout most of the 5th century. The Eastern Empire was marked by closer relations between the political state and Christian Church, with doctrinal matters assuming an importance in eastern politics that they did not have in Western Europe. Legal developments included the codification of Roman law; the first effort—the "Theodosian Code"—was completed in 438. Under Emperor Justinian (r. 527–565), another compilation took place—the "Corpus Juris Civilis". Justinian also oversaw the construction of the Hagia Sophia in Constantinople and the reconquest of North Africa from the Vandals and Italy from the Ostrogoths, under Belisarius (d. 565). The conquest of Italy was not complete, as a deadly outbreak of plague in 542 led to the rest of Justinian's reign concentrating on defensive measures rather than further conquests. At the emperor's death, the Byzantines had control of most of Italy, North Africa, and a small foothold in southern Spain. Justinian's reconquests have been criticised by historians for overextending his realm and setting the stage for the Muslim conquests, but many of the difficulties faced by Justinian's successors were due not just to over-taxation to pay for his wars but to the essentially civilian nature of the empire, which made raising troops difficult.
In the Eastern Empire the slow infiltration of the Balkans by the Slavs added a further difficulty for Justinian's successors. It began gradually, but by the late 540s Slavic tribes were in Thrace and Illyrium, and had defeated an imperial army near Adrianople in 551. In the 560s the Avars began to expand from their base on the north bank of the Danube; by the end of the 6th century they were the dominant power in Central Europe and routinely able to force the eastern emperors to pay tribute. They remained a strong power until 796. An additional problem to face the empire came as a result of the involvement of Emperor Maurice (r. 582–602) in Persian politics when he intervened in a succession dispute. This led to a period of peace, but when Maurice was overthrown, the Persians invaded and during the reign of Emperor Heraclius (r. 610–641) controlled large chunks of the empire, including Egypt, Syria, and Asia Minor, until Heraclius' successful counterattack. In 628 the empire secured a peace treaty and recovered all of its lost territories.
Western society.
In Western Europe, some of the older Roman elite families died out while others became more involved with Church than secular affairs. Values attached to Latin scholarship and education mostly disappeared, and while literacy remained important, it became a practical skill rather than a sign of elite status. In the 4th century, Jerome (d. 420) dreamed that God rebuked him for spending more time reading Cicero than the Bible. By the 6th century, Gregory of Tours (d. 594) had a similar dream, but instead of being chastised for reading Cicero, he was chastised for learning shorthand. By the late 6th century, the principal means of religious instruction in the Church had become music and art rather than the book. Most intellectual efforts went towards imitating classical scholarship, but some original works were created, along with now-lost oral compositions. The writings of Sidonius Apollinaris (d. 489), Cassiodorus (d. c. 585), and Boethius (d. c. 525) were typical of the age.
Changes also took place among laymen, as aristocratic culture focused on great feasts held in halls rather than on literary pursuits. Clothing for the elites was richly embellished with jewels and gold. Lords and kings supported entourages of fighters who formed the backbone of the military forces. Family ties within the elites were important, as were the virtues of loyalty, courage, and honour. These ties led to the prevalence of the feud in aristocratic society, examples of which included those related by Gregory of Tours that took place in Merovingian Gaul. Most feuds seem to have ended quickly with the payment of some sort of compensation. Women took part in aristocratic society mainly in their roles as wives and mothers of men, with the role of mother of a ruler being especially prominent in Merovingian Gaul. In Anglo-Saxon society the lack of many child rulers meant a lesser role for women as queen mothers, but this was compensated for by the increased role played by abbesses of monasteries. Only in Italy does it appear that women were always considered under the protection and control of a male relative.
Peasant society is much less documented than the nobility. Most of the surviving information available to historians comes from archaeology; few detailed written records documenting peasant life remain from before the 9th century. Most the descriptions of the lower classes come from either law codes or writers from the upper classes. Landholding patterns in the West were not uniform; some areas had greatly fragmented landholding patterns, but in other areas large contiguous blocks of land were the norm. These differences allowed for a wide variety of peasant societies, some dominated by aristocratic landholders and others having a great deal of autonomy. Land settlement also varied greatly. Some peasants lived in large settlements that numbered as many as 700 inhabitants. Others lived in small groups of a few families and still others lived on isolated farms spread over the countryside. There were also areas where the pattern was a mix of two or more of those systems. Unlike in the late Roman period, there was no sharp break between the legal status of the free peasant and the aristocrat, and it was possible for a free peasant's family to rise into the aristocracy over several generations through military service to a powerful lord.
Roman city life and culture changed greatly in the early Middle Ages. Although Italian cities remained inhabited, they contracted significantly in size. Rome, for instance, shrank from a population of hundreds of thousands to around 30,000 by the end of the 6th century. Roman temples were converted into Christian churches and city walls remained in use. In Northern Europe, cities also shrank, while civic monuments and other public buildings were raided for building materials. The establishment of new kingdoms often meant some growth for the towns chosen as capitals. Although there had been Jewish communities in many Roman cities, the Jews suffered periods of persecution after the conversion of the empire to Christianity. Officially they were tolerated, if subject to conversion efforts, and at times were even encouraged to settle in new areas.
Rise of Islam.
Religious beliefs in the Eastern Empire and Persia were in flux during the late 6th and early 7th centuries. Judaism was an active proselytising faith, and at least one Arab political leader converted to it. Christianity had active missions competing with the Persians' Zoroastrianism in seeking converts, especially among residents of the Arabian Peninsula. All these strands came together with the emergence of Islam in Arabia during the lifetime of Muhammad (d. 632). After his death, Islamic forces conquered much of the Eastern Empire and Persia, starting with Syria in 634–635 and reaching Egypt in 640–641, Persia between 637 and 642, North Africa in the later 7th century, and the Iberian Peninsula in 711. By 714, Islamic forces controlled much of the peninsula, a region they called Al-Andalus.
The Islamic conquests reached their peak in the mid-8th century. The defeat of Muslim forces at the Battle of Poitiers in 732 led to the reconquest of southern France by the Franks, but the main reason for the halt of Islamic growth in Europe was the overthrow of the Umayyad dynasty and its replacement by the Abbasid dynasty. The Abbasids moved their capital to Baghdad and were more concerned with the Middle East than Europe, losing control of sections of the Muslim lands. Umayyad descendants took over the Iberian Peninsula, the Aghlabids controlled North Africa, and the Tulunids became rulers of Egypt. By the middle of the 8th century, new trading patterns were emerging in the Mediterranean; trade between the Franks and the Arabs replaced the old Roman patterns of trade. Franks traded timber, furs, swords and slaves in return for silks and other fabrics, spices, and precious metals from the Arabs.
Trade and economy.
The migrations and invasions of the 4th and 5th centuries disrupted trade networks around the Mediterranean. African goods stopped being imported into Europe, first disappearing from the interior and by the 7th century found only in a few cities such as Rome or Naples. By the end of the 7th century, under the impact of the Muslim conquests, African products were no longer found in Western Europe. The replacement of goods from long-range trade with local products was a trend throughout the old Roman lands that happened in the Early Middle Ages. This was especially marked in the lands that did not lie on the Mediterranean, such as northern Gaul or Britain. Non-local goods appearing in the archaeological record are usually luxury goods. In the northern parts of Europe, not only were the trade networks local, but the goods carried were simple, with little pottery or other complex products. Around the Mediterranean, pottery remained prevalent and appears to have been traded over medium-range networks, not just produced locally.
The various Germanic states in the west all had coinages that imitated existing Roman and Byzantine forms. Gold continued to be minted until the end of the 7th century, when it was replaced by silver coins. The basic Frankish silver coin was the denarius or denier, while the Anglo-Saxon version was called a penny. From these areas, the denier or penny spread throughout Europe during the centuries from 700 to 1000. Copper or bronze coins were not struck, nor were gold except in Southern Europe. No silver coins denominated in multiple units were minted.
Church and monasticism.
Christianity was a major unifying factor between Eastern and Western Europe before the Arab conquests, but the conquest of North Africa sundered maritime connections between those areas. Increasingly the Byzantine Church differed in language, practices, and liturgy from the western Church. The eastern church used Greek instead of the western Latin. Theological and political differences emerged, and by the early and middle 8th century issues such as iconoclasm, clerical marriage, and state control of the church had widened to the extent that the cultural and religious differences were greater than the similarities. The formal break came in 1054, when the papacy and the patriarchy of Constantinople clashed over papal supremacy and excommunicated each other, which led to the division of Christianity into two churches—the western branch became the Roman Catholic Church and the eastern branch the Orthodox Church.
The ecclesiastical structure of the Roman Empire survived the movements and invasions in the west mostly intact, but the papacy was little regarded, and few of the western bishops looked to the bishop of Rome for religious or political leadership. Many of the popes prior to 750 were more concerned with Byzantine affairs and eastern theological controversies. The register, or archived copies of the letters, of Pope Gregory the Great (pope 590–604) survives, and of those more than 850 letters, the vast majority were concerned with affairs in Italy or Constantinople. The only part of Western Europe where the papacy had influence was Britain, where Gregory had sent the Gregorian mission in 597 to convert the Anglo-Saxons to Christianity. Irish missionaries were most active in Western Europe between the 5th and the 7th centuries, going first to England and Scotland and then on to the continent. Under such monks as Columba (d. 597) and Columbanus (d. 615), they founded monasteries, taught in Latin and Greek, and authored secular and religious works.
The Early Middle Ages witnessed the rise of monasticism in the West. The shape of European monasticism was determined by traditions and ideas that originated with the Desert Fathers of Egypt and Syria. Most European monasteries were of the type that focuses on community experience of the spiritual life, called cenobitism, which was pioneered by Pachomius (d. 348) in the 4th century. Monastic ideals spread from Egypt to Western Europe in the 5th and 6th centuries through hagiographical literature such as the "Life of Anthony". Benedict of Nursia (d. 547) wrote the Benedictine Rule for Western monasticism during the 6th century, detailing the administrative and spiritual responsibilities of a community of monks led by an abbot. Monks and monasteries had a deep effect on the religious and political life of the Early Middle Ages, in various cases acting as land trusts for powerful families, centres of propaganda and royal support in newly conquered regions, and bases for missions and proselytisation. They were the main and sometimes only outposts of education and literacy in a region. Many of the surviving manuscripts of the Latin classics were copied in monasteries in the Early Middle Ages. Monks were also the authors of new works, including history, theology, and other subjects, written by authors such as Bede (d. 735), a native of northern England who wrote in the late 7th and early 8th centuries.
Carolingian Europe.
The Frankish kingdom in northern Gaul split into kingdoms called Austrasia, Neustria, and Burgundy during the 6th and 7th centuries, all of them ruled by the Merovingian dynasty, who were descended from Clovis. The 7th century was a tumultuous period of wars between Austrasia and Neustria. Such warfare was exploited by Pippin (d. 640), the Mayor of the Palace for Austrasia who became the power behind the Austrasian throne. Later members of his family inherited the office, acting as advisers and regents. One of his descendants, Charles Martel (d. 741), won the Battle of Poitiers in 732, halting the advance of Muslim armies across the Pyrenees. Great Britain was divided into small states dominated by the kingdoms of Northumbria, Mercia, Wessex, and East Anglia, which were descended from the Anglo-Saxon invaders. Smaller kingdoms in present-day Wales and Scotland were still under the control of the native Britons and Picts. Ireland was divided into even smaller political units, usually known as tribal kingdoms, under the control of kings. There were perhaps as many as 150 local kings in Ireland, of varying importance.
The Carolingian dynasty, as the successors to Charles Martel are known, officially took control of the kingdoms of Austrasia and Neustria in a coup of 753 led by Pippin III (r. 752–768). A contemporary chronicle claims that Pippin sought, and gained, authority for this coup from Pope Stephen II (pope 752–757). Pippin's takeover was reinforced with propaganda that portrayed the Merovingians as inept or cruel rulers, exalted the accomplishments of Charles Martel, and circulated stories of the family's great piety. At the time of his death in 768, Pippin left his kingdom in the hands of his two sons, Charles (r. 768–814) and Carloman (r. 768–771). When Carloman died of natural causes, Charles blocked the succession of Carloman's young son and installed himself as the king of the united Austrasia and Neustria. Charles, more often known as Charles the Great or Charlemagne, embarked upon a programme of systematic expansion in 774 that unified a large portion of Europe, eventually controlling modern-day France, northern Italy, and Saxony. In the wars that lasted beyond 800, he rewarded allies with war booty and command over parcels of land. In 774, Charlemagne conquered the Lombards, which freed the papacy from the fear of Lombard conquest and marked the beginnings of the Papal States.
The coronation of Charlemagne as emperor on Christmas Day 800 is regarded as a turning point in medieval history, marking a return of the Western Roman Empire, since the new emperor ruled over much of the area previously controlled by the western emperors. It also marks a change in Charlemagne's relationship with the Byzantine Empire, as the assumption of the imperial title by the Carolingians asserted their equivalence to the Byzantine state. There were several differences between the newly established Carolingian Empire and both the older Western Roman Empire and the concurrent Byzantine Empire. The Frankish lands were rural in character, with only a few small cities. Most of the people were peasants settled on small farms. Little trade existed and much of that was with the British Isles and Scandinavia, in contrast to the older Roman Empire with its trading networks centred on the Mediterranean. The empire was administered by an itinerant court that travelled with the emperor, as well as approximately 300 imperial officials called counts, who administered the counties the empire had been divided into. Clergy and local bishops served as officials, as well as the imperial officials called "missi dominici", who served as roving inspectors and troubleshooters.
Carolingian Renaissance.
Charlemagne's court in Aachen was the centre of the cultural revival sometimes referred to as the "Carolingian Renaissance". The period saw an increase in literacy, developments in the arts, architecture and jurisprudence, as well as liturgical and scriptural studies. The English monk Alcuin (d. 804) was invited to Aachen and brought the education available in the monasteries of Northumbria. Charlemagne's chancery—or writing office—made use of a new script today known as Carolingian minuscule, allowing a common writing style that advanced communication across much of Europe. Charlemagne sponsored changes in church liturgy, imposing the Roman form of church service on his domains, as well as the Gregorian chant in liturgical music for the churches. An important activity for scholars during this period was the copying, correcting, and dissemination of basic works on religious and secular topics, with the aim of encouraging learning. New works on religious topics and schoolbooks were also produced. Grammarians of the period modified the Latin language, changing it from the Classical Latin of the Roman Empire into a more flexible form to fit the needs of the church and government. By the reign of Charlemagne, the language had so diverged from the classical that it was later called Medieval Latin.
Breakup of the Carolingian Empire.
Charlemagne planned to continue the Frankish tradition of dividing his kingdom between all his heirs, but was unable to do so as only one son, Louis the Pious (r. 814–840), was still alive by 813. Just before Charlemagne died in 814, he crowned Louis as his successor. Louis's reign of 26 years was marked by numerous divisions of the empire among his sons and, after 829, civil wars between various alliances of father and sons over the control of various parts of the empire. Eventually, Louis recognised his eldest son Lothair I (d. 855) as emperor and gave him Italy. Louis divided the rest of the empire between Lothair and Charles the Bald (d. 877), his youngest son. Lothair took East Francia, comprising both banks of the Rhine and eastwards, leaving Charles West Francia with the empire to the west of the Rhineland and the Alps. Louis the German (d. 876), the middle child, who had been rebellious to the last, was allowed to keep Bavaria under the suzerainty of his elder brother. The division was disputed. Pepin II of Aquitaine (d. after 864), the emperor's grandson, rebelled in a contest for Aquitaine, while Louis the German tried to annex all of East Francia. Louis the Pious died in 840, with the empire still in chaos.
A three-year civil war followed his death. By the Treaty of Verdun (843), a kingdom between the Rhine and Rhone rivers was created for Lothair to go with his lands in Italy, and his imperial title was recognised. Louis the German was in control of Bavaria and the eastern lands in modern-day Germany. Charles the Bald received the western Frankish lands, comprising most of modern-day France. Charlemagne's grandsons and great-grandsons divided their kingdoms between their descendants, eventually causing all internal cohesion to be lost. In 987 the Carolingian dynasty was replaced in the western lands, with the crowning of Hugh Capet (r. 987–996) as king. In the eastern lands the dynasty had died out earlier, in 911, with the death of Louis the Child, and the selection of the unrelated Conrad I (r. 911–918) as king.
The breakup of the Carolingian Empire was accompanied by invasions, migrations, and raids by external foes. The Atlantic and northern shores were harassed by the Vikings, who also raided the British Isles and settled there as well as in Iceland. In 911, the Viking chieftain Rollo (d. c. 931) received permission from the Frankish King Charles the Simple (r. 898–922) to settle in what became Normandy. The eastern parts of the Frankish kingdoms, especially Germany and Italy, were under continual Magyar assault until the invader's defeat at the Battle of Lechfeld in 955. The breakup of the Abbasid dynasty meant that the Islamic world fragmented into smaller political states, some of which began expanding into Italy and Sicily, as well as over the Pyrenees into the southern parts of the Frankish kingdoms.
New kingdoms and a revived Byzantium.
Efforts by local kings to fight the invaders led to the formation of new political entities. In Anglo-Saxon England, King Alfred the Great (r. 871–899) came to an agreement with the Viking invaders in the late 9th century, resulting in Danish settlements in Northumbria, Mercia, and parts of East Anglia. By the middle of the 10th century, Alfred's successors had conquered Northumbria, and restored English control over most of the southern part of Great Britain. In northern Britain, Kenneth MacAlpin (d. c. 860) united the Picts and the Scots into the Kingdom of Alba. In the early 10th century, the Ottonian dynasty had established itself in Germany, and was engaged in driving back the Magyars. Its efforts culminated in the coronation in 962 of Otto I (r. 936–973) as Holy Roman Emperor. In 972, he secured recognition of his title by the Byzantine Empire, which he sealed with the marriage of his son Otto II (r. 967–983) to Theophanu (d. 991), daughter of an earlier Byzantine Emperor Romanos II (r. 959–963). By the late 10th century Italy had been drawn into the Ottonian sphere after a period of instability; Otto III (r. 996–1002) spent much of his later reign in the kingdom. The western Frankish kingdom was more fragmented, and although kings remained nominally in charge, much of the political power devolved to the local lords.
Missionary efforts to Scandinavia during the 9th and 10th centuries helped strengthen the growth of kingdoms such as Sweden, Denmark, and Norway, which gained power and territory. Some kings converted to Christianity, although not all by 1000. Scandinavians also expanded and colonised throughout Europe. Besides the settlements in Ireland, England, and Normandy, further settlement took place in what became Russia and in Iceland. Swedish traders and raiders ranged down the rivers of the Russian steppe, and even attempted to seize Constantinople in 860 and 907. Christian Spain, initially driven into a small section of the peninsula in the north, expanded slowly south during the 9th and 10th centuries, establishing the kingdoms of Asturias and León.
In Eastern Europe, Byzantium revived its fortunes under Emperor Basil I (r. 867–886) and his successors Leo VI (r. 886–912) and Constantine VII (r. 913–959), members of the Macedonian dynasty. Commerce revived and the emperors oversaw the extension of a uniform administration to all the provinces. The military was reorganised, which allowed the emperors John I (r. 969–976) and Basil II (r. 976–1025) to expand the frontiers of the empire on all fronts. The imperial court was the centre of a revival of classical learning, a process known as the Macedonian Renaissance. Writers such as John Geometres (fl. early 10th century) composed new hymns, poems, and other works. Missionary efforts by both eastern and western clergy resulted in the conversion of the Moravians, Bulgars, Bohemians, Poles, Magyars, and Slavic inhabitants of the Kievan Rus'. These conversions contributed to the founding of political states in the lands of those peoples—the states of Moravia, Bulgaria, Bohemia, Poland, Hungary, and the Kievan Rus'. Bulgaria, which was founded around 680, at its height reached from Budapest to the Black Sea and from the Dnieper River in modern Ukraine to the Adriatic Sea. By 1018, the last Bulgarian nobles had surrendered to the Byzantine Empire.
Art and architecture.
Few large stone buildings were constructed between the Constantinian basilicas of the 4th century and the 8th century, although many smaller ones were built during the 6th and 7th centuries. By the beginning of the 8th century, the Carolingian Empire revived the basilica form of architecture. One feature of the basilica is the use of a transept, or the "arms" of a cross-shaped building that are perpendicular to the long nave. Other new features of religious architecture include the crossing tower and a monumental entrance to the church, usually at the west end of the building.
Carolingian art was produced for a small group of figures around the court, and the monasteries and churches they supported. It was dominated by efforts to regain the dignity and classicism of imperial Roman and Byzantine art, but was also influenced by the Insular art of the British Isles. Insular art integrated the energy of Irish Celtic and Anglo-Saxon Germanic styles of ornament with Mediterranean forms such as the book, and established many characteristics of art for the rest of the medieval period. Surviving religious works from the Early Middle Ages are mostly illuminated manuscripts and carved ivories, originally made for metalwork that has since been melted down. Objects in precious metals were the most prestigious form of art, but almost all are lost except for a few crosses such as the Cross of Lothair, several reliquaries, and finds such as the Anglo-Saxon burial at Sutton Hoo and the hoards of Gourdon from Merovingian France, Guarrazar from Visigothic Spain and Nagyszentmiklós near Byzantine territory. There are survivals from the large brooches in fibula or penannular form that were a key piece of personal adornment for elites, including the Irish Tara Brooch. Highly decorated books were mostly Gospel Books and these have survived in larger numbers, including the Insular Book of Kells, the Book of Lindisfarne, and the imperial Codex Aureus of St. Emmeram, which is one of the few to retain its "treasure binding" of gold encrusted with jewels. Charlemagne's court seems to have been responsible for the acceptance of figurative monumental sculpture in Christian art, and by the end of the period near life-sized figures such as the Gero Cross were common in important churches.
Military and technological developments.
During the later Roman Empire, the principal military developments were attempts to create an effective cavalry force as well as the continued development of highly specialised types of troops. The creation of heavily armoured cataphract-type soldiers as cavalry was an important feature of the 5th-century Roman military. The various invading tribes had differing emphasis on types of soldiers—ranging from the primarily infantry Anglo-Saxon invaders of Britain to the Vandals and Visigoths, who had a high proportion of cavalry in their armies. During the early invasion period, the stirrup had not been introduced into warfare, which limited the usefulness of cavalry as shock troops because it was not possible to put the full force of the horse and rider behind blows struck by the rider. The greatest change in military affairs during the invasion period was the adoption of the Hunnic composite bow in place of the earlier, and weaker, Scythian composite bow. Another development was the increasing use of longswords and the progressive replacement of scale armour by mail armour and lamellar armour.
The importance of infantry and light cavalry began to decline during the early Carolingian period, with a growing dominance of elite heavy cavalry. The use of militia-type levies of the free population declined over the Carolingian period. Although much of the Carolingian armies were mounted, a large proportion during the early period appear to have been mounted infantry, rather than true cavalry. One exception was Anglo-Saxon England, where the armies were still composed of regional levies, known as the "fyrd", which were led by the local elites. In military technology, one of the main changes was the return of the crossbow, which had been known in Roman times and reappeared as a military weapon during the last part of the Early Middle Ages. Another change was the introduction of the stirrup, which increased the effectiveness of cavalry as shock troops. A technological advance that had implications beyond the military was the horseshoe, which allowed horses to be used in rocky terrain.
High Middle Ages.
Society and economic life.
The High Middle Ages saw an expansion of population. The estimated population of Europe grew from 35 to 80 million between 1000 and 1347, although the exact causes remain unclear: improved agricultural techniques, the decline of slaveholding, a more clement climate and the lack of invasion have all been suggested. As much as 90 per cent of the European population remained rural peasants. Many were no longer settled in isolated farms but had gathered into small communities, usually known as manors or villages. These peasants were often subject to noble overlords and owed them rents and other services, in a system known as manorialism. There remained a few free peasants throughout this period and beyond, with more of them in the regions of Southern Europe than in the north. The practice of assarting, or bringing new lands into production by offering incentives to the peasants who settled them, also contributed to the expansion of population.
Other sections of society included the nobility, clergy, and townsmen. Nobles, both the titled nobility and simple knights, exploited the manors and the peasants, although they did not own lands outright but were granted rights to the income from a manor or other lands by an overlord through the system of feudalism. During the 11th and 12th centuries, these lands, or fiefs, came to be considered hereditary, and in most areas they were no longer divisible between all the heirs as had been the case in the early medieval period. Instead, most fiefs and lands went to the eldest son. The dominance of the nobility was built upon its control of the land, its military service as heavy cavalry, control of castles, and various immunities from taxes or other impositions. Castles, initially in wood but later in stone, began to be constructed in the 9th and 10th centuries in response to the disorder of the time, and provided protection from invaders as well as allowing lords defence from rivals. Control of castles allowed the nobles to defy kings or other overlords. Nobles were stratified; kings and the highest-ranking nobility controlled large numbers of commoners and large tracts of land, as well as other nobles. Beneath them, lesser nobles had authority over smaller areas of land and fewer people. Knights were the lowest level of nobility; they controlled but did not own land, and had to serve other nobles.
The clergy was divided into two types: the secular clergy, who lived out in the world, and the regular clergy, who lived under a religious rule and were usually monks. Throughout the period monks remained a very small proportion of the population, usually less than one per cent. Most of the regular clergy were drawn from the nobility, the same social class that served as the recruiting ground for the upper levels of the secular clergy. The local parish priests were often drawn from the peasant class. Townsmen were in a somewhat unusual position, as they did not fit into the traditional three-fold division of society into nobles, clergy, and peasants. During the 12th and 13th centuries, the ranks of the townsmen expanded greatly as existing towns grew and new population centres were founded. But throughout the Middle Ages the population of the towns probably never exceeded 10 per cent of the total population.
Jews also spread across Europe during the period. Communities were established in Germany and England in the 11th and 12th centuries, but Spanish Jews, long settled in Spain under the Muslims, came under Christian rule and increasing pressure to convert to Christianity. Most Jews were confined to the cities, as they were not allowed to own land or be peasants. Besides the Jews, there were other non-Christians on the edges of Europe—pagan Slavs in Eastern Europe and Muslims in Southern Europe.
Women in the Middle Ages were officially required to be subordinate to some male, whether their father, husband, or other kinsman. Widows, who were often allowed much control over their own lives, were still restricted legally. Women's work generally consisted of household or other domestically inclined tasks. Peasant women were usually responsible for taking care of the household, child-care, as well as gardening and animal husbandry near the house. They could supplement the household income by spinning or brewing at home. At harvest-time, they were also expected to help with field-work. Townswomen, like peasant women, were responsible for the household, and could also engage in trade. What trades were open to women varied by country and period. Noblewomen were responsible for running a household, and could occasionally be expected to handle estates in the absence of male relatives, but they were usually restricted from participation in military or government affairs. The only role open to women in the Church was that of nuns, as they were unable to become priests.
In central and northern Italy and in Flanders, the rise of towns that were to a degree self-governing stimulated economic growth and created an environment for new types of trade associations. Commercial cities on the shores of the Baltic entered into agreements known as the Hanseatic League, and the Italian Maritime republics such as Venice, Genoa, and Pisa expanded their trade throughout the Mediterranean. Great trading fairs were established and flourished in northern France during the period, allowing Italian and German merchants to trade with each other as well as local merchants. In the late 13th century new land and sea routes to the Far East were pioneered, famously described in "The Travels of Marco Polo" written by one of the traders, Marco Polo (d. 1324). Besides new trading opportunities, agricultural and technological improvements enabled an increase in crop yields, which in turn allowed the trade networks to expand. Rising trade brought new methods of dealing with money, and gold coinage was again minted in Europe, first in Italy and later in France and other countries. New forms of commercial contracts emerged, allowing risk to be shared among merchants. Accounting methods improved, partly through the use of double-entry bookkeeping; letters of credit also appeared, allowing easy transmission of money.
Rise of state power.
The High Middle Ages was the formative period in the history of the modern Western state. Kings in France, England, and Spain consolidated their power, and set up lasting governing institutions. New kingdoms such as Hungary and Poland, after their conversion to Christianity, became Central European powers. The Magyars settled Hungary around 900 under King Árpád (d. c. 907) after a series of invasions in the 9th century. The papacy, long attached to an ideology of independence from secular kings, first asserted its claim to temporal authority over the entire Christian world; the Papal Monarchy reached its apogee in the early 13th century under the pontificate of Innocent III (pope 1198–1216). Northern Crusades and the advance of Christian kingdoms and military orders into previously pagan regions in the Baltic and Finnic north-east brought the forced assimilation of numerous native peoples into European culture.
During the early High Middle Ages, Germany was ruled by the Ottonian dynasty, which struggled to control the powerful dukes ruling over territorial duchies tracing back to the Migration period. In 1024, they were replaced by the Salian dynasty, who famously clashed with the papacy under Emperor Henry IV (r. 1084–1105) over church appointments as part of the Investiture Controversy. His successors continued to struggle against the papacy as well as the German nobility. A period of instability followed the death of Emperor Henry V (r. 1111–25), who died without heirs, until Frederick I Barbarossa (r. 1155–90) took the imperial throne. Although he ruled effectively, the basic problems remained, and his successors continued to struggle into the 13th century. Barbarossa's grandson Frederick II (r. 1220–1250), who was also heir to the throne of Sicily through his mother, clashed repeatedly with the papacy. His court was famous for its scholars and he was often accused of heresy. He and his successors faced many difficulties, including the invasion of the Mongols into Europe in the mid-13th century. Mongols first shattered the Kievan Rus' principalities and then invaded Eastern Europe in 1241, 1259, and 1287.
Under the Capetian dynasty France slowly began to expand its authority over the nobility, growing out of the Île-de-France to exert control over more of the country in the 11th and 12th centuries. They faced a powerful rival in the Dukes of Normandy, who in 1066 under William the Conqueror (duke 1035–1087), conquered England (r. 1066–87) and created a cross-channel empire that lasted, in various forms, throughout the rest of the Middle Ages. Normans also settled in Sicily and southern Italy, when Robert Guiscard (d. 1085) landed there in 1059 and established a duchy that later became the Kingdom of Sicily. Under the Angevin dynasty of Henry II (r. 1154–89) and his son Richard I (r. 1189–99), the kings of England ruled over England and large areas of France, brought to the family by Henry II's marriage to Eleanor of Aquitaine (d. 1204), heiress to much of southern France. Richard's younger brother John (r. 1199–1216) lost Normandy and the rest of the northern French possessions in 1204 to the French King Philip II Augustus (r. 1180–1223). This led to dissension among the English nobility, while John's financial exactions to pay for his unsuccessful attempts to regain Normandy led in 1215 to "Magna Carta", a charter that confirmed the rights and privileges of free men in England. Under Henry III (r. 1216–72), John's son, further concessions were made to the nobility, and royal power was diminished. The French monarchy continued to make gains against the nobility during the late 12th and 13th centuries, bringing more territories within the kingdom under their personal rule and centralising the royal administration. Under Louis IX (r. 1226–70), royal prestige rose to new heights as Louis served as a mediator for most of Europe.
In Iberia, the Christian states, which had been confined to the north-western part of the peninsula, began to push back against the Islamic states in the south, a period known as the "Reconquista". By about 1150, the Christian north had coalesced into the five major kingdoms of León, Castile, Aragon, Navarre, and Portugal. Southern Iberia remained under control of Islamic states, initially under the Caliphate of Córdoba, which broke up in 1031 into a shifting number of petty states known as "taifas", who fought with the Christians until the Almohad Caliphate re-established centralised rule over Southern Iberia in the 1170s. Christian forces advanced again in the early 1200s, culminating in the capture of Seville in 1248.
Crusades.
In the 11th century, the Seljuk Turks took over much of the Middle East, occupying Persia during the 1040s, Armenia in the 1060s, and Jerusalem in 1070. In 1071, the Turkish army defeated the Byzantine army at the Battle of Manzikert and captured the Byzantine Emperor Romanus IV (r. 1068–71). The Turks were then free to invade Asia Minor, which dealt a dangerous blow to the Byzantine Empire by seizing a large part of its population and its economic heartland. Although the Byzantines regrouped and recovered somewhat, they never fully regained Asia Minor and were often on the defensive. The Turks also had difficulties, losing control of Jerusalem to the Fatimids of Egypt and suffering from a series of internal civil wars. The Byzantines also faced a revived Bulgaria, which in the late 12th and 13th centuries spread throughout the Balkans.
The crusades were intended to seize Jerusalem from Muslim control. The First Crusade was proclaimed by Pope Urban II (pope 1088–99) at the Council of Clermont in 1095 in response to a request from the Byzantine Emperor Alexios I Komnenos (r. 1081–1118) for aid against further Muslim advances. Urban promised indulgence to anyone who took part. Tens of thousands of people from all levels of society mobilised across Europe and captured Jerusalem in 1099. One feature of the crusades was the pogroms against local Jews that often took place as the crusaders left their countries for the East. These were especially brutal during the First Crusade, when the Jewish communities in Cologne, Mainz, and Worms were destroyed, and other communities in cities between the rivers Seine and Rhine suffered destruction. Another outgrowth of the crusades was the foundation of a new type of monastic order, the military orders of the Templars and Hospitallers, which fused monastic life with military service.
The crusaders consolidated their conquests into crusader states. During the 12th and 13th centuries, there were a series of conflicts between those states and the surrounding Islamic states. Appeals from those states to the papacy led to further crusades, such as the Third Crusade, called to try to regain Jerusalem, which had been captured by Saladin (d. 1193) in 1187. In 1203, the Fourth Crusade was diverted from the Holy Land to Constantinople, and captured the city in 1204, setting up a Latin Empire of Constantinople and greatly weakening the Byzantine Empire. The Byzantines recaptured the city in 1261, but never regained their former strength. By 1291 all the crusader states had been captured or forced from the mainland, although a titular Kingdom of Jerusalem survived on the island of Cyprus for several years afterwards.
Popes called for crusades to take place elsewhere besides the Holy Land: in Spain, southern France, and along the Baltic. The Spanish crusades became fused with the "Reconquista" of Spain from the Muslims. Although the Templars and Hospitallers took part in the Spanish crusades, similar Spanish military religious orders were founded, most of which had become part of the two main orders of Calatrava and Santiago by the beginning of the 12th century. Northern Europe also remained outside Christian influence until the 11th century or later, and became a crusading venue as part of the Northern Crusades of the 12th to 14th centuries. These crusades also spawned a military order, the Order of the Sword Brothers. Another order, the Teutonic Knights, although originally founded in the crusader states, focused much of its activity in the Baltic after 1225, and in 1309 moved its headquarters to Marienburg in Prussia.
Intellectual life.
During the 11th century, developments in philosophy and theology led to increased intellectual activity. There was debate between the realists and the nominalists over the concept of "universals". Philosophical discourse was stimulated by the rediscovery of Aristotle and his emphasis on empiricism and rationalism. Scholars such as Peter Abelard (d. 1142) and Peter Lombard (d. 1164) introduced Aristotelian logic into theology. The late 11th and early 12th centuries also saw the rise of cathedral schools throughout Western Europe, signalling the shift of learning from monasteries to cathedrals and towns. Cathedral schools were in turn replaced by the universities established in major European cities. Philosophy and theology fused in scholasticism, an attempt by 12th- and 13th-century scholars to reconcile authoritative texts, most notably Aristotle and the Bible. This movement tried to employ a systemic approach to truth and reason and culminated in the thought of Thomas Aquinas (d. 1274), who wrote the "Summa Theologica", or "Summary of Theology".
Royal and noble courts saw the development of chivalry and the ethos of courtly love. This culture was expressed in the vernacular languages rather than Latin, and comprised poems, stories, legends, and popular songs spread by troubadours, or wandering minstrels. Often the stories were written down in the "chansons de geste", or "songs of great deeds", such as "The Song of Roland" or "The Song of Hildebrand". Secular and religious histories were also produced. Geoffrey of Monmouth (d. c. 1155) composed his "Historia Regum Britanniae", a collection of stories and legends about Arthur. Other works were more clearly history, such as Otto von Freising's (d. 1158) "Gesta Friderici Imperatoris" detailing the deeds of Emperor Frederick Barbarossa, or William of Malmesbury's (d. c. 1143) "Gesta Regum" on the kings of England.
Legal studies advanced during the 12th century. Both secular law and canon law, or ecclesiastical law, were studied in the High Middle Ages. Secular law, or Roman law, was advanced greatly by the discovery of the "Corpus Juris Civilis" in the 11th century, and by 1100 Roman law was being taught at Bologna. This led to the recording and standardisation of legal codes throughout Western Europe. Canon law was also studied, and around 1140 a monk named Gratian (fl. 12th century), a teacher at Bologna, wrote what became the standard text of canon law—the "Decretum".
Among the results of the Greek and Islamic influence on this period in European history was the replacement of Roman numerals with the decimal positional number system and the invention of algebra, which allowed more advanced mathematics. Astronomy advanced following the translation of Ptolemy's "Almagest" from Greek into Latin in the late 12th century. Medicine was also studied, especially in southern Italy, where Islamic medicine influenced the school at Salerno.
Technology and military.
In the 12th and 13th centuries, Europe saw economic growth and innovations in methods of production. Major technological advances included the invention of the windmill, the first mechanical clocks, the manufacture of distilled spirits, and the use of the astrolabe. Concave spectacles were invented around 1286 by an unknown Italian artisan, probably working in or near Pisa.
The development of a three-field rotation system for planting crops increased the usage of land from one half in use each year under the old two-field system to two-thirds under the new system, with a consequent increase in production. The development of the heavy plough allowed heavier soils to be farmed more efficiently, aided by the spread of the horse collar, which led to the use of draught horses in place of oxen. Horses are faster than oxen and require less pasture, factors that aided the implementation of the three-field system.
The construction of cathedrals and castles advanced building technology, leading to the development of large stone buildings. Ancillary structures included new town halls, houses, bridges, and tithe barns. Shipbuilding improved with the use of the rib and plank method rather than the old Roman system of mortise and tenon. Other improvements to ships included the use of lateen sails and the stern-post rudder, both of which increased the speed at which ships could be sailed.
Military affairs saw an increase in the use of infantry with specialised roles. Along with the still-dominant heavy cavalry, armies often included mounted and infantry crossbowmen, as well as sappers and engineers. Crossbows, which had been known in Late Antiquity, increased in use partly because of the increase in siege warfare in the 10th and 11th centuries. The increasing use of crossbows during the 12th and 13th centuries led to the use of closed-face helmets, heavy body armour, as well as horse armour. Gunpowder was known in Europe by the mid-13th century with a recorded use in European warfare by the English against the Scots in 1304, although it was merely used as an explosive and not as a weapon. Cannon were being used for sieges in the 1320s, and hand-held guns were in use by the 1360s.
Architecture, art, and music.
In the 10th century the establishment of churches and monasteries led to the development of stone architecture that elaborated vernacular Roman forms, from which the term "Romanesque" is derived. Where available, Roman brick and stone buildings were recycled for their materials. From the tentative beginnings known as the First Romanesque, the style flourished and spread across Europe in a remarkably homogeneous form. Just before 1000 there was a great wave of building stone churches all over Europe. Romanesque buildings have massive stone walls, openings topped by semi-circular arches, small windows, and, particularly in France, arched stone vaults. The large portal with coloured sculpture in high relief became a central feature of façades, especially in France, and the capitals of columns were often carved with narrative scenes of imaginative monsters and animals. According to art historian C. R. Dodwell, "virtually all the churches in the West were decorated with wall-paintings", of which few survive. Simultaneous with the development in church architecture, the distinctive European form of the castle was developed, and became crucial to politics and warfare.
Romanesque art, especially metalwork, was at its most sophisticated in Mosan art, in which distinct artistic personalities including Nicholas of Verdun (d. 1205) become apparent, and an almost classical style is seen in works such as a font at Liège, contrasting with the writhing animals of the exactly contemporary Gloucester Candlestick. Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a "Last Judgement" on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
From the early 12th century, French builders developed the Gothic style, marked by the use of rib vaults, pointed arches, flying buttresses, and large stained glass windows. It was used mainly in churches and cathedrals, and continued in use until the 16th century in much of Europe. Classic examples of Gothic architecture include Chartres Cathedral and Reims Cathedral in France as well as Salisbury Cathedral in England. Stained glass became a crucial element in the design of churches, which continued to use extensive wall-paintings, now almost all lost.
During this period the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according to Janetta Benton "by 1300 most monks bought their books in shops", and the book of hours developed as a form of devotional book for lay-people. Metalwork continued to be the most prestigious form of art, with Limoges enamel a popular and relatively affordable option for objects such as reliquaries and crosses. In Italy the innovations of Cimabue and Duccio, followed by the Trecento master Giotto (d. 1337), greatly increased the sophistication and status of panel painting and fresco. Increasing prosperity during the 12th century resulted in greater production of secular art; many carved ivory objects such as gaming-pieces, combs, and small religious figures have survived.
Church life.
Monastic reform became an important issue during the 11th century, as elites began to worry that monks were not adhering to the rules binding them to a strictly religious life. Cluny Abbey, founded in the Mâcon region of France in 909, was established as part of the Cluniac Reforms, a larger movement of monastic reform in response to this fear. Cluny quickly established a reputation for austerity and rigour. It sought to maintain a high quality of spiritual life by placing itself under the protection of the papacy and by electing its own abbot without interference from laymen, thus maintaining economic and political independence from local lords.
Monastic reform inspired change in the secular church. The ideals that it was based upon were brought to the papacy by Pope Leo IX (pope 1049–1054), and provided the ideology of the clerical independence that led to the Investiture Controversy in the late 11th century. This involved Pope Gregory VII (pope 1073–85) and Emperor Henry IV, who initially clashed over episcopal appointments, a dispute that turned into a battle over the ideas of investiture, clerical marriage, and simony. The emperor saw the protection of the Church as one of his responsibilities as well as wanting to preserve the right to appoint his own choices as bishops within his lands, but the papacy insisted on the Church's independence from secular lords. These issues remained unresolved after the compromise of 1122 known as the Concordat of Worms. The dispute represents a significant stage in the creation of a papal monarchy separate from and equal to lay authorities. It also had the permanent consequence of empowering German princes at the expense of the German emperors.
The High Middle Ages was a period of great religious movements. Besides the Crusades and monastic reforms, people sought to participate in new forms of religious life. New monastic orders were founded, including the Carthusians and the Cistercians. The latter especially expanded rapidly in their early years under the guidance of Bernard of Clairvaux (d. 1153). These new orders were formed in response to the feeling of the laity that Benedictine monasticism no longer met the needs of the laymen, who along with those wishing to enter the religious life wanted a return to the simpler hermetical monasticism of early Christianity, or to live an Apostolic life. Religious pilgrimages were also encouraged. Old pilgrimage sites such as Rome, Jerusalem, and Compostela received increasing numbers of visitors, and new sites such as Monte Gargano and Bari rose to prominence.
In the 13th century mendicant orders—the Franciscans and the Dominicans—who swore vows of poverty and earned their living by begging, were approved by the papacy. Religious groups such as the Waldensians and the Humiliati also attempted to return to the life of early Christianity in the middle 12th and early 13th centuries, but they were condemned as heretical by the papacy. Others joined the Cathars, another heretical movement condemned by the papacy. In 1209, a crusade was preached against the Cathars, the Albigensian Crusade, which in combination with the medieval Inquisition, eliminated them.
Late Middle Ages.
War, famine and plague.
The first years of the 14th century were marked by famines, culminating in the Great Famine of 1315–17. The causes of the Great Famine included the slow transition from the Medieval Warm Period to the Little Ice Age, which left the population vulnerable when bad weather caused crop failures. The years 1313–14 and 1317–21 were excessively rainy throughout Europe, resulting in widespread crop failures. The climate change—which resulted in a declining average annual temperature for Europe during the 14th century—was accompanied by an economic downturn.
These troubles were followed in 1347 by the Black Death, a disease that spread throughout Europe during the following three years. The death toll was probably about 35 million people in Europe, about one-third of the population. Towns were especially hard-hit because of their crowded conditions. Large areas of land were left sparsely inhabited, and in some places fields were left unworked. Wages rose as landlords sought to entice the reduced number of available workers to their fields. Further problems were the lower rents and lower demands for food, both of which cut into agricultural income. Urban workers also felt that they had a right to greater earnings, and popular uprisings broke out across Europe. Among the uprisings were the "jacquerie" in France, the Peasants' Revolt in England, and revolts in the cities of Florence in Italy and Ghent and Bruges in Flanders. The trauma of the plague led to an increased piety throughout Europe, which manifested itself in the foundation of new charities, the self-mortification of the flagellants, and the scapegoating of the Jews. Conditions were further unsettled by the return of the plague throughout the rest of the 14th century; it continued to strike Europe periodically during the rest of the Middle Ages.
Society and economy.
Society throughout Europe was disturbed by the dislocations caused by the Black Death. Lands that had been marginally productive were abandoned, as the survivors were able to acquire more fertile areas. Although serfdom declined in Western Europe it became more common in Eastern Europe, as landlords imposed it on those of their tenants who had previously been free. Most peasants in Western Europe managed to change the work they had previously owed to their landlords into cash rents. The percentage of serfs amongst the peasantry declined from a high of 90 to closer to 50 per cent by the end of the period. Landlords also became more conscious of common interests with other landholders, and joined together to extort privileges from their governments. Partly at the urging of landlords, governments attempted to legislate a return to the economic conditions that existed before the Black Death. Non-clergy became increasingly literate, and urban populations began to imitate the nobility's interest in chivalry.
Jewish communities were expelled from England in 1290, and from France in 1306. Although some were allowed back into France, most were not, and many Jews emigrated eastwards, and Hungary. The Jews were expelled from Spain in 1492, and dispersed to Turkey, France, Italy, and Holland. The rise of banking in Italy during the 13th century continued throughout the 14th century, fuelled partly by the increasing warfare of the period and the needs of the papacy to move money between kingdoms. Many of the banking firms loaned money to royalty, at great risk, as some were bankrupted when kings defaulted on their loans.
State resurgence.
The Late Middle Ages witnessed the rise of strong, royalty-based nation states throughout Europe, particularly in England, France, and the Christian kingdoms of the Iberian Peninsula: Aragon, Castile, and Portugal. The long conflicts of the later Middle Ages strengthened royal control over their kingdoms, and were extremely hard on the peasantry. Kings profited from warfare which extended royal legislation throughout their kingdoms and increased the lands they directly controlled. Paying for the wars required that methods of taxation become more effective and efficient, and the rate of taxation often increased. The requirement to obtain the consent of those being taxed meant that representative bodies such as the English Parliament or the French Estates General gained power and authority.
Throughout the 14th century, French kings sought to expand their influence throughout the kingdom at the expense of the territorial holdings of the nobility. They ran into difficulties when attempting to confiscate the holdings of the English kings in southern France, leading to the Hundred Years' War, which lasted until 1453. Early in the war the English under Edward III (r. 1327–77) and his son Edward, the Black Prince (d. 1376), won the battles of Crécy and Poitiers, captured the city of Calais, and won control of much of France. The resulting stresses almost caused the disintegration of the French kingdom during the early years of the war. In the early 15th century, France once more came close to dissolving, but in the late 1420s the military successes of Joan of Arc (d. 1431) led to the victory of the French kings over the English and the capture of the last of the English possessions in southern France in 1453. The price was high, as the population of France at the end of the Wars was likely half what it had been at the start of the conflict. Conversely, the Wars had a positive effect on English national identity, doing much to fuse the various local identities into a national English ideal. The conflict with the French also helped create a national culture in England that was separate from French culture, which had been the dominant cultural influence in England before the outbreak of the Hundred Years' War. The early Hundred Years' War also saw the dominance of the English longbow, and the appearance of cannon on the battlefield at Crécy in 1346.
In modern-day Germany, the Empire continued, but the elective nature of the imperial crown meant that there was no enduring dynasty around which a strong state could form. Further east, the kingdoms of Poland, Hungary, and Bohemia grew powerful. The Iberian Peninsula kingdoms continued to gain land from the Muslim kingdoms of the peninsula; Portugal concentrated on expanding overseas during the 15th century, while the other kingdoms were riven by difficulties over the royal succession and other concerns. England, after losing the Hundred Years' War, went on to suffer a long civil war known as the Wars of the Roses, which lasted into the 1490s, and only ended when Henry Tudor (r. 1485–1509 as Henry VII) became king and consolidated his hold on England after his victory over Richard III (r. 1483–85) at Bosworth in 1485. Scandinavia went through a period of union under the Union of Kalmar in the late 14th and early 15th centuries, but dissolved once more after the death of Margaret I of Denmark (r. in Denmark 1387–1412), who had united Norway, Denmark, and Sweden. The major power around the Baltic Sea was the Hanseatic League, a commercial confederation of city states that traded from Western Europe to Russia. Scotland emerged from English domination under Robert the Bruce (r. 1306–29), who secured papal recognition of his kingship in 1328.
Collapse of Byzantium.
Although the Palaeologi emperors recaptured Constantinople from the Western Europeans in 1261, they were never able to regain control of much of the former imperial lands. They usually controlled only a small section of the Balkan Peninsula near Constantinople, the city itself, and some coastal lands on the Black Sea and around the Aegean Sea. The former Byzantine lands in the Balkans were divided between the new Kingdom of Serbia, the Second Bulgarian Empire and the city-state of Venice. The power of the Byzantine emperors was threatened by a new Turkish tribe, the Ottomans, who established themselves in Anatolia in the 13th century and steadily expanded throughout the 14th century. The Ottomans expanded into Europe, reducing Bulgaria to a vassal state by 1366 and taking over Serbia after its defeat at the Battle of Kosovo in 1389. Western Europeans rallied to the plight of the Christians in the Balkans and declared a new crusade in 1396; a great army was sent to the Balkans, where it was defeated at the Battle of Nicopolis. Constantinople was finally captured by the Ottomans in 1453.
Controversy within the Church.
The troubled 14th century saw the Avignon Papacy of 1305–78, also called the "Babylonian Captivity of the Papacy" (a reference to the Babylonian captivity of the Jews), and then the Great Schism that lasted from 1378 to 1418, when there were two, then later three, rival popes, each supported by several states. In the early years of the 15th century, after a century of turmoil, ecclesiastical officials convened in Constance in 1414, and the following year the council deposed one of the rival popes, leaving only two claimants. Further depositions followed, and in November 1417 the council elected Martin V (pope 1417–31) as pope.
Besides the schism, the western church was riven by theological controversies, some of which turned into heresies. John Wycliffe (d. 1384), an English theologian, was condemned as a heretic in 1415 for teaching that the laity should have access to the text of the Bible as well as holding views on the Eucharist that were contrary to church doctrine. Wycliffe's teachings influenced two of the major heretical movements of the later Middle Ages—Lollardy in England and Hussitism in Bohemia. The Bohemians were also influenced by the teaching of Jan Hus, who was burned at the stake in 1415 after being condemned as a heretic by the Council of Constance. The Hussite church, although the target of a crusade, survived beyond the Middle Ages. Other heresies were manufactured, such as the accusations against the Knights Templar that resulted in their suppression in 1312, and the division of their great wealth between the French King Philip IV (r. 1285–1314) and the Hospitallers.
The papacy refined the concept of transubstantiation further in the Late Middle Ages, stating that the clergy alone was allowed to partake of the wine in the Eucharist. This further distanced the secular laity from the clergy. The laity continued the practices of pilgrimages, veneration of relics, and belief in the power of the Devil. Mystics such as Meister Eckhart (d. 1327) or Thomas à Kempis (d. 1471) wrote works that taught the laity to focus on their inner spiritual life, which laid the groundwork for the Protestant Reformation. Besides mysticism, belief in witches and witchcraft became widespread, and by the late 15th century the Church had begun to lend credence to populist fears of witchcraft by its condemnation of witches in 1484 and the publication in 1486 of the "Malleus Maleficarum", the most popular handbook for witch-hunters.
Scholars, intellectuals, and exploration.
The Later Middle Ages saw a reaction against scholasticism led by John Duns Scotus (d. 1308) and William of Ockham (d. c. 1348), both of whom objected to the application of reason to faith. Their efforts, along with others, led to an undermining of the prevailing Platonic idea of "universals". Ockham's insistence that reason operates independently of faith allowed science to be separated from theology and philosophy. Legal studies were marked by the steady advance of Roman law into areas of jurisprudence previously governed by customary law. The one exception to this trend was England, where the common law remained pre-eminent. Countries also codified their laws; legal codes were promulgated in countries as far apart as Castile, Poland, and Lithuania.
Education remained mostly focused on the training of future clergy. The basic learning of the letters and numbers remained the province of the family or a village priest, but the secondary subjects of the trivium—grammar, rhetoric, logic—were studied in cathedral schools or in schools provided by cities. Commercial secondary schools spread, and some Italian towns had more than one such enterprise. Universities also spread throughout Europe in the 14th and 15th centuries. The rise of vernacular literature increased, with Dante (d. 1321), Petrarch (d. 1374) and Giovanni Boccaccio (d. 1375) in 14th-century Italy, Geoffrey Chaucer (d. 1400) and William Langland (d. c. 1386) in England, and François Villon (d. 1464) and Christine de Pizan (d. c. 1430) in France. Much literature remained religious in character, and although a great deal of it continued to be written in Latin, a new demand developed for saints' lives and other devotional tracts in the vernacular languages. This was fed by the growth of the "Devotio Moderna" movement, most prominently in the formation of the Brethren of the Common Life, but also in the works of German mystics such as Meister Eckhart and Johannes Tauler (d. 1361). Theatre also developed in the guise of miracle plays put on by the Church. At the end of the period, the development of the printing press in about 1450 led to the establishment of publishing houses throughout Europe by 1500. Lay literacy rates rose, but were still low; one estimate gave a literacy rate of ten per cent of males and one per cent of females in 1500.
Beginning in the early 15th century, the countries of the Iberian peninsula began to sponsor exploration beyond the boundaries of Europe. Prince Henry the Navigator of Portugal (d. 1460) sent expeditions that discovered the Canary Islands, the Azores, and Cape Verde during his lifetime. After his death, exploration continued; Bartolomeu Dias (d. 1500) went around the Cape of Good Hope in 1486 and Vasco da Gama (d. 1524) sailed around Africa to India in 1498. The combined Spanish monarchies of Castile and Aragon sponsored Christopher Columbus' (d. 1506) voyage of exploration in 1492 that discovered the Americas. The English crown under Henry VII sponsored the voyage of John Cabot (d. 1498) in 1497, which landed on Cape Breton Island.
Technological and military developments.
One of the major developments in the military sphere during the Late Middle Ages was the increasing use of infantry and light cavalry. The English also employed longbowmen, but other countries were unable to create similar forces that enjoyed the same military success. Armour continued to advance, spurred on by the increasing power of crossbows, and plate armour was developed to help protect against the threat from crossbows as well as the hand-held guns that were developed. Pole arms reached new prominence with the development of the Flemish and Swiss infantry armed with pikes and other long spears.
In agriculture, one major advance was the increasing use of sheep with long-fibred wool, which allowed a stronger thread to be spun. Also important was the replacement of the traditional distaff for spinning wool with the spinning wheel, which tripled production over hand spinning. A less technological refinement that still greatly affected daily life was the use of buttons as closures for garments, which allowed for better fitting without having to lace clothing on the wearer. Windmills were refined with the creation of the tower mill, which allowed the upper part of the windmill to be spun around to face whichever direction the wind was blowing. The blast furnace appeared around 1350 in Sweden, increasing the quantity of iron produced and improving its quality. The first patent law in 1447 in Venice protected the rights of inventors to their inventions.
Late medieval art and architecture.
The Late Middle Ages in Europe as a whole correspond to the Trecento and Early Renaissance cultural periods in Italy, although Northern Europe and Spain continued to use Gothic styles, increasingly elaborate in the 15th century, until almost the end of the period. International Gothic was a courtly style that reached much of Europe in the decades around 1400, producing masterpieces such as the Très Riches Heures du Duc de Berry. All over Europe secular art continued to increase in quantity and quality, and in the 15th century the mercantile classes of Italy and Flanders became important patrons, commissioning small portraits of themselves in oils as well as a growing range of luxury items such as jewellery, ivory caskets, cassone chests, and maiolica pottery. These objects also included the Hispano-Moresque ware produced by mostly Mudéjar potters in Spain. Although royalty owned huge collections of plate, little survives except for the Royal Gold Cup. Italian silk manufacture developed, so that western churches and elites no longer needed to rely on imports from Byzantium or the Islamic world. In France and Flanders tapestry weaving of sets like "The Lady and the Unicorn" became a major luxury industry.
The large external sculptural schemes of Early Gothic churches gave way to more sculpture inside the building, as tombs became more elaborate and other features such as pulpits were sometimes lavishly carved, as in the Pulpit by Giovanni Pisano in Sant'Andrea. Painted or carved wooden relief altarpieces became common, especially as churches created many side-chapels. Early Netherlandish painting by artists such as Jan van Eyck (d. 1441) and Rogier van der Weyden (d. 1464) rivalled that of Italy, as did northern illuminated manuscripts, which in the 15th century began to be collected on a large scale by secular elites, who also commissioned secular books, especially histories. From about 1450 printed books rapidly became popular, though still expensive. There were around 30,000 different editions of incunabula, or works printed before 1500, by which time illuminated manuscripts were commissioned only by royalty and a few others. Very small woodcuts, nearly all religious, were affordable even by peasants in parts of Northern Europe from the middle of the 15th century. More expensive engravings supplied a wealthier market with a variety of images.
Modern perceptions.
The medieval period is frequently caricatured as a "time of ignorance and superstition" that placed "the word of religious authorities over personal experience and rational activity." This is a legacy from both the Renaissance and Enlightenment, when scholars contrasted their intellectual cultures with those of the medieval period, to the detriment of the Middle Ages. Renaissance scholars saw the Middle Ages as a period of decline from the high culture and civilisation of the Classical world; Enlightenment scholars saw reason as superior to faith, and thus viewed the Middle Ages as a time of ignorance and superstition.
Others argue that reason was generally held in high regard during the Middle Ages. Science historian Edward Grant writes, "If revolutionary rational thoughts were expressed [in the 18th century], they were only made possible because of the long medieval tradition that established the use of reason as one of the most important of human activities". Also, contrary to common belief, David Lindberg writes, "the late medieval scholar rarely experienced the coercive power of the church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led".
The caricature of the period is also reflected in some more specific notions. One misconception, first propagated in the 19th century and still very common, is that all people in the Middle Ages believed that the Earth was flat. This is untrue, as lecturers in the medieval universities commonly argued that evidence showed the Earth was a sphere. Lindberg and Ronald Numbers, another scholar of the period, state that there "was scarcely a Christian scholar of the Middle Ages who did not acknowledge [Earth's] sphericity and even know its approximate circumference". Other misconceptions such as "the Church prohibited autopsies and dissections during the Middle Ages", "the rise of Christianity killed off ancient science", or "the medieval Christian church suppressed the growth of natural philosophy", are all cited by Numbers as examples of widely popular myths that still pass as historical truth, although they are not supported by current historical research.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="18837" url="http://en.wikipedia.org/wiki?curid=18837" title="Median">
Median

In statistics and probability theory, the median is the number separating the higher half of a data sample, a population, or a probability distribution, from the lower half. The "median" of a finite list of numbers can be found by arranging all the observations from lowest value to highest value and picking the middle one (e.g., the median of {3, 3, 5, 9, 11} is 5). If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values
(the median of {3, 5, 7, 9} is (5 + 7) / 2 = 6), which corresponds to interpreting the median as the fully trimmed mid-range. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data is contaminated, the median will not give an arbitrarily large result.
A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions.
In a sample of data, or a finite population, there may be no member of the sample whose value is identical to the median (in the case of an even sample size); if there is such a member, there may be more than one so that the median may not uniquely identify a sample member. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.
At most, half the population have values strictly less than the "median", and, at most, half have values strictly greater than the median. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if "a" < "b" < "c", then the median of the list {"a", "b", "c"} is "b", and, if "a" < "b" < "c" < "d", then the median of the list {"a", "b", "c", "d"} is the mean of "b" and "c"; i.e., it is ("b" + "c")/2.
The median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors.
In terms of notation, some authors represent the median of a variable "x" either as formula_1 or as formula_2 sometimes also "M". There is no widely accepted standard notation for the median, so the use of these or other symbols for the median needs to be explicitly defined when they are introduced.
The median is the 2nd quartile, 5th decile, and 50th percentile.
Measures of location and dispersion.
The median is one of a number of ways of summarising the typical values associated with members of a statistical population; thus, it is a possible location parameter. Since the median is the same as the "second quartile", its calculation is illustrated in the article on quartiles.
When the median is used as a location parameter in descriptive statistics, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.
For practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that for a wide range of distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see Efficiency (statistics)#Asymptotic efficiency and references therein.
Probability distributions.
For any probability distribution on the real line R with cumulative distribution function "F", regardless of whether it is any kind of continuous probability distribution, in particular an absolutely continuous distribution (which has a probability density function), or a discrete probability distribution, a median is by definition any real number "m" that satisfies the inequalities
or, equivalently, the inequalities
in which a Lebesgue–Stieltjes integral is used. For an absolutely continuous probability distribution with probability density function "ƒ", the median satisfies
Any probability distribution on R has at least one median, but there may be more than one median. Where exactly one median exists, statisticians speak of "the median" correctly; even when the median is not unique, some statisticians speak of "the median" informally.
Medians of particular distributions.
The medians of certain types of distributions can be easily calculated from their parameters:
Descriptive statistics.
The median is used primarily for skewed distributions, which it summarizes differently from the arithmetic mean. Consider the multiset { 1, 2, 2, 2, 3, 14 }. The median is 2 in this case, (as is the mode), and it might be seen as a better indication of central tendency (less susceptible to the exceptionally large value in data) than the arithmetic mean of 4.
Calculation of medians is a popular technique in summary statistics and summarizing statistical data, since it is simple to understand and easy to calculate, while also giving a measure that is more robust in the presence of outlier values than is the mean.
Populations.
Optimality property.
The "mean absolute error" of a real variable "c" with respect to the random variable "X" is
Provided that the probability distribution of "X" is such that the above expectation exists, then "m" is a median of "X" if and only if "m" is a minimizer of the mean absolute error with respect to "X". In particular, "m" is a sample median if and only if "m" minimizes the arithmetic mean of the absolute deviations.
See also "k"-medians clustering.
Unimodal distributions.
It can be shown for a unimodal distribution that the median formula_7 and the mean formula_8 lie within (3/5)1/2 ≈ 0.7746 standard deviations of each other. In symbols,
where |.| is the absolute value.
A similar relation holds between the median and the mode: they lie within 31/2 ≈ 1.732 standard deviations of each other:
Inequality relating means and medians.
If the distribution has finite variance, then the distance between the median and the mean is bounded by one standard deviation.
This bound was proved by Mallows, who used Jensen's inequality twice, as follows. We have
The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex. The second inequality comes from the fact that a median minimizes the absolute deviation function
This proof can easily be generalized to obtain a multivariate version of the inequality, as follows:
where "m" is a spatial median, that is, a minimizer of the function
formula_14 The spatial median is unique when the data-set's dimension is two or more. An alternative proof uses the one-sided Chebyshev inequality; it appears in .
Jensen's inequality for medians.
Jensen's inequality states that for any random variable "x" with a ﬁnite expectation "E"("x") and for any convex function "f"
It has been shown that if "x" is a real variable with a unique median "m" and "f" is a C function then
A C function is a real valued function, defined on the set of real numbers "R", with the property that for any real "t"
is a closed interval, a singleton or an empty set.
Medians for samples.
The sample median.
Efficient computation of the sample median.
Even though comparison-sorting "n" items requires Ω("n" log "n") operations, selection algorithms can compute the "k"th-smallest of "n" items with only Θ("n") operations. This includes the median, which is the (n/2)th order statistic (or for an even number of samples, the average of the two middle order statistics).
Easy explanation of the sample median.
In individual series (if number of observation is very low) first one must arrange all the observations in order. Then count("n") is the total number of observation in given data.
If "n" is odd then Median ("M") = value of (("n" + 1)/2)th item term.
If "n" is even then Median ("M") = value of [(("n")/2)th item term + (("n")/2 + 1)th item term ]/2
As an example, we will calculate the sample median for the following set of observations: 1, 5, 2, 8, 7.
Start by sorting the values: 1, 2, 5, 7, 8.
In this case, the median is 5 since it is the middle observation in the ordered list.
The median is the (("n" + 1)/2)th item, where "n" is the number of values. For example, for the list {1, 2, 5, 7, 8}, we have "n" = 5, so the median is the ((5 + 1)/2)th item.
As an example, we will calculate the sample median for the following set of observations: 1, 6, 2, 8, 7, 2.
Start by sorting the values: 1, 2, 2, 6, 7, 8.
In this case, the arithmetic mean of the two middlemost terms is (2 + 6)/2 = 4. Therefore, the median is 4 since it is the arithmetic mean of the middle observations in the ordered list.
We also use this formula MEDIAN = {("n" + 1 )/2}th item . "n" = number of values
As above example 1, 2, 2, 6, 7, 8
"n" = 6 Median = {(6 + 1)/2}th item = 3.5th item. In this case, the median is average of the 3rd number and the next one (the fourth number). The median is (2 + 6)/2 which is 4.
Variance.
The distribution of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function formula_18 is asymptotically normal with mean formula_19 and variance
where formula_19 is the median value of distribution and formula_22 is the sample size. In practice this may be difficult to estimate as the density function is usually unknown.
These results have also been extended. It is now known for the formula_23-th quantile that the distribution of the sample formula_23-th quantile is asymptotically normal around the formula_23-th quantile with variance equal to
where formula_27 is the value of the distribution density at the formula_23-th quantile.
The value of formula_29—the asymptotic value of formula_30 where formula_31 is the population median—has been studied by several authors. The standard 'delete one' jackknife method produces inconsistent results. An alternative—the 'delete k' method—where formula_32 grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of formula_33). Other methods have been proposed but their behavior may differ between large and small samples.
The efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size formula_34 from the normal distribution, the ratio is
For large samples (as formula_22 tends to infinity) this ratio tends to formula_37
Other estimators.
For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median.
If data are represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.
Coefficient of dispersion.
The coefficient of dispersion (CD) is defined as the ratio of the average absolute deviation from the median to the median of the data. It is a statistical measure used by the states of Iowa, New York and South Dakota in estimating dues taxes. In symbols
where "n" is the sample size, "m" is the sample median and "x" is a variate. The sum is taken over the whole sample.
Confidence intervals for a two sample test where the sample sizes are large have been derived by Bonett and Seier This test assumes that both samples have the same median but differ in the dispersion around it. The confidence interval (CI) is bounded inferiorly by
where "t"j is the mean absolute deviation of the "j"th sample, "var"() is the variance and "zα" is the value from the normal distribution for the chosen value of "α": for "α" = 0.05, "zα" = 1.96. The following formulae are used in the derivation of these confidence intervals
where "r" is the Pearson correlation coefficient between the squared deviation scores
"a" and "b" here are constants equal to 1 and 2, "x" is a variate and "s" is the standard deviation of the sample.
Multivariate median.
Previously, this article discussed the concept of a univariate median for a one-dimensional object (population, sample). When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one. In higher dimensions, however, there are several multivariate medians.
Marginal median.
The marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.
Spatial median (L1 median).
In a normed vector space of dimension two or greater, the "spatial median" minimizes the expected distance
where "X" and "a" are vectors, if this expectation has a finite minimum; another definition is better suited for general probability-distributions. The spatial median is unique when the data-set's dimension is two or more. It is a robust and highly efficient estimator of a central tendency of a population.
The Geometric median is the corresponding estimator based on the sample statistics of a finite set of points, rather than the population statistics. It is the point minimizing the arithmetic average of Euclidean distances to the given sample points, instead of the expectation. Note that the arithmetic average and sum are interchangeable since they differ by a fixed constant which does not alter the location of the minimum.
Other multivariate medians.
An alternative generalization of the spatial median in higher dimensions that does not relate to a particular metric is the centerpoint.
Other median-related concepts.
Pseudo-median.
For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population "pseudo-median", which is the median of a symmetrized distribution and which is close to the population median. The Hodges–Lehmann estimator has been generalized to multivariate distributions.
Variants of regression.
The Theil–Sen estimator is a method for robust linear regression based on finding medians of slopes.
Median filter.
In the context of image processing of monochrome raster images there is a type of noise, known as the salt and pepper noise, when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively reduce noise in this case.
Cluster analysis.
In cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.
Median-Median Line.
This is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter formula_45: a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent formula_46 and independent formula_45 variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.
Nair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.
Median-unbiased estimators.
Any "mean"-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A "median"-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.
The theory of median-unbiased estimators was revived by in 1947:
An estimate of a one-dimensional parameter θ will be said to be median-unbiased if, for fixed θ, the median of the distribution of the estimate is at the value θ; i.e., the estimate underestimates just as often as it overestimates. This requirement seems for most purposes to accomplish as much as the mean-unbiased requirement and has the additional property that it is invariant under one-to-one transformation.—page 584
Further properties of median-unbiased estimators have been reported. In particular, median-unbiased estimators exist in cases where mean-unbiased and maximum-likelihood estimators do not exist. Median-unbiased estimators are invariant under one-to-one transformations.
History.
The idea of the median originated in Edward Wright's book on navigation ("Certaine Errors in Navigation") in 1599 in a section concerning the determination of location with a compass. Wright felt that this value was the most likely to be the correct value in a series of observations.
In 1757, Roger Joseph Boscovich developed a regression method based on the L1 norm and therefore implicitly on the median.
In 1774, Laplace suggested the median be used as the standard estimator of the value of a posterior pdf. The specific criteria was to minimize the expected magnitude of the error; |"α" - "α*"| where "α*" is the estimate and "α" is the true value. Laplaces's criterion was generally rejected for 150 years in favor of the least squares method of Gauss and Legendgre which minimizes < ("α" - "α*")2 > to obtain the mean. The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s.
Antoine Augustin Cournot in 1843 was the first to use the term "median" ("valeur médiane") for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median ("Centralwerth") in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace.
Francis Galton used the English term "median" in 1881, having earlier used the terms "middle-most value" in 1869 and the "medium" in 1880.
External links.
"This article incorporates material from on PlanetMath, which is licensed under the ."

</doc>
<doc id="18838" url="http://en.wikipedia.org/wiki?curid=18838" title="Mammal">
Mammal

Mammals (class Mammalia ) are a clade of endothermic amniotes distinguished from reptiles and birds by the possession of hair, three middle ear bones, mammary glands, and a neocortex (a region of the brain). The mammalian brain regulates body temperature and the circulatory system, including the four-chambered heart. The mammals include the largest animals on the planet, the rorquals and some other whales, as well as some of the most intelligent, such as elephants, some primates and some cetaceans. The basic body type is a four-legged land-borne animal, but some mammals are adapted for life at sea, in the air, in the trees, or on two legs. The largest group of mammals, the placentals, have a placenta which feeds the offspring during pregnancy. Mammals range in size from the 30 – bumblebee bat to the 33 m blue whale.
The word "mammal" is modern, from the scientific name "Mammalia" coined by Carl Linnaeus in 1758, derived from the Latin "mamma" ("teat, pap"). All female mammals nurse their young with milk, which is secreted from special glands, the mammary glands. According to "Mammal Species of the World", 5,416 species were known in 2006. These were grouped in 1,229 genera, 153 families and 29 orders. In 2008 the IUCN completed a five-year, 1,700-scientist Global Mammal Assessment for its IUCN Red List, which counted 5,488 accepted species at the end of that period. In some classifications, the mammals are divided into two subclasses (not counting fossils): the Prototheria (order of Monotremata) and the Theria, the latter composed of the infraclasses Metatheria and Eutheria. The marsupials constitute the crown group of the Metatheria and therefore include all living metatherians as well as many extinct ones; the placentals likewise constitute the crown group of the Eutheria.
Except for the five species of monotremes (egg-laying mammals), all modern mammals give birth to live young. Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders, in descending order, are Rodentia (mice, rats, porcupines, beavers, capybaras, and other gnawing mammals), Chiroptera (bats), and Soricomorpha (shrews, moles and solenodons). The next three largest orders, depending on the classification scheme used, are the primates (to which the human species belongs), the Cetartiodactyla (including the even-toed hoofed mammals and the whales), and the Carnivora (cats, dogs, weasels, bears, seals, and their relatives). While the classification of mammals at the family level has been relatively stable, different treatments at higher levels—subclass, infraclass, and order—appear in contemporaneous literature, especially for the marsupials. Much recent change has reflected the results of cladistic analysis and molecular genetics. Results from molecular genetics, for example, have led to the adoption of new groups such as the Afrotheria and the abandonment of traditional groups such as the Insectivora.
The early synapsid mammalian ancestors were sphenacodont pelycosaurs, a group that also included "Dimetrodon". At the end of the Carboniferous period, this group diverged from the sauropsid line that led to today's reptiles and birds. Preceded by many diverse groups of non-mammalian synapsids (sometimes referred to as mammal-like reptiles), the first mammals appeared in the early Mesozoic era. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of the non-avian dinosaurs 66 million years ago.
Varying definitions, varying dates.
In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and therian mammals (marsupials and placentals) and all descendants of that ancestor. Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century.
T. S. Kemp has provided a more traditional definition: "synapsids that possess a dentary–squamosal jaw articulation and occlusion between upper and lower molars with a transverse component to the movement" or, equivalently in Kemp's view, the clade originating with the last common ancestor of "Sinoconodon" and living mammals.
If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. "Ambondro" is more closely related to monotremes than to therian mammals while "Amphilestes" and "Amphitherium" are more closely related to the therians; as fossils of all three genera are dated about million years ago in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group. The earliest known synapsid satisfying Kemp's definitions is "Tikitherium", dated Ma, so the appearance of mammals in this broader sense can be given this Late Triassic date. In any case, the temporal range of the group extends to the present day.
Distinguishing features.
Living mammal species can be identified by the presence of sweat glands, including those that are specialized to produce milk to nourish their young. In classifying fossils, however, other features must be used, since soft tissue glands and many other features are not visible in fossils.
Many traits shared by all living mammals appeared among the earliest members of the group:
For the most part, these characteristics were not present in the Triassic ancestors of the mammals.
For palaeontologists who define Mammalia phylogenetically, no limit can be set on the features used to distinguish the group. Any feature may be relevant to a fossil's phylogenetic position. Palaeontologists defining Mammalia in terms of traits, on the other hand, need only consider those features that appear in the definition. The dentary-squamosal jaw joint is generally included.
Classification.
George Gaylord Simpson's "Principles of Classification and a Classification of Mammals" (AMNH "Bulletin" v. 85, 1945) was the original source for the taxonomy listed here. Simpson laid out a systematics of mammal origins and relationships that was universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though field work gradually made Simpson's classification outdated, it remained the closest thing to an official classification of mammals.
McKenna/Bell classification.
In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, "Classification of Mammals above the Species Level", is the most comprehensive work to date on the systematics, relationships, and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though recent molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History, New York. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia.
The McKenna/Bell hierarchical listing of many terms used for mammal groups above the species includes extinct mammals, as well as modern groups, and introduces some fine distinctions such as legions and sublegions (ranks which fall between classes and orders) that are likely to be glossed over by the nonprofessionals.
Extinct groups are represented by a dagger (†).
Class Mammalia
Molecular classification of placentals.
Molecular studies based on DNA analysis have suggested new relationships among mammal families over the last few years. Most of these findings have been independently validated by retrotransposon presence/absence data. Classification systems based on molecular studies reveal three major groups or lineages of placental mammals- Afrotheria, Xenarthra, and Boreoeutheria- which diverged from early common ancestors in the Cretaceous. The relationships between these three lineages is contentious, and three different hypotheses have been proposed with respect to which group is basal with respect to other placentals. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra), and Exafroplacentalia (basal Afrotheria). Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
Estimates for the divergence times between these three placental groups range from 105 to 120 million years ago, depending on type of DNA (e.g. nuclear or mitochondrial) and varying interpretations of paleogeographic data.
Group I: Afrotheria
Group II: Xenarthra
Group III: Boreoeutheria
Evolutionary history.
Synapsida, the group which contains mammals and their extinct relatives, originated during the Pennsylvanian subperiod, when they split from the lineage that led to reptiles and birds. Crown group mammals evolved from earlier mammaliaforms during the Early Jurassic.
Cladogram following, which takes Mammalia to be the crown group.
A cladogram compiled by Mikko Haaramo and based on individual cladograms of After Rowe 1988; Luo, Crompton & Sun 2001; Luo, Cifelli & Kielan-Jaworowska 2001, Luo, Kielan-Jaworowska & Cifelli 2002, Kielan-Jaworowska, Cifelli & Luo 2004, and Luo & Wible 2005.
Evolution from amniotes in the Paleozoic.
The first fully terrestrial vertebrates were amniotes. Like their amphibian predecessors, they have lungs and limbs. Amniotes' eggs, however, have internal membranes which allow the developing embryo to breathe but keep water in. Hence, amniotes can lay eggs on dry land, while amphibians generally need to lay their eggs in water.
The first amniotes apparently arose in the Late Carboniferous. They descended from earlier reptiliomorph amphibians, which lived on land that was already inhabited by insects and other invertebrates as well as by ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which include mammals; and the sauropsids, which include turtles, lizards, snakes, crocodilians, dinosaurs and birds. Synapsids have a single hole (temporal fenestra) low on each side of the skull.
One synapsid group, the pelycosaurs, included the largest and fiercest animals of the early Permian.
Therapsids descended from pelycosaurs in the Middle Permian, about 265 million years ago, and took over their position as the dominant land vertebrates. They differ from pelycosaurs in several features of the skull and jaws, including: larger temporal fenestrae and incisors which are equal in size. The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very like their pelycosaur ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:
Nonmammalian synapsids are sometimes called "mammal-like reptiles".
The mammals appear.
The Permian–Triassic extinction event, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of the carnivores among the therapsids. In the early Triassic, all the medium to large land carnivore niches were taken over by archosaurs which, over an extended period of time (35 million years), came to include the crocodylomorphs, the pterosaurs, and the dinosaurs. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.
The first mammals (in Kemp's sense) appeared in the Late Triassic epoch (about 225 million years ago), 40 million years after the first therapsids. They expanded out of their nocturnal insectivore niche from the mid-Jurassic onwards; "Castorocauda", for example, had adaptations for swimming, digging and catching fish. Most, if not all, are thought to have remained nocturnal (the Nocturnal bottleneck), accounting for much of the typical mammalian traits.
The majority of the mammal species that existed in the Mesozoic Era were multituberculates, eutriconodonts and spalacotheriids.
The earliest known monotreme is "Teinolophos", which lived about 123 million years ago in Australia. Monotremes have some features which may be inherited from the original amniotes:
Unlike other mammals, female monotremes do not have nipples and feed their young by "sweating" milk from patches on their bellies.
The earliest known metatherian is "Sinodelphys", found in 125 million-year-old Early Cretaceous shale in China's northeastern Liaoning Province. The fossil is nearly complete and includes tufts of fur and imprints of soft tissues.
The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike "Juramaia sinensis", or "Jurassic mother from China," dated to 160 million years ago in the Late Jurassic. A later eutherian, "Eomaia", dated to 125 million years ago in the Early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. In particular:
Rise to dominance in the Cenozoic.
Mammals took over the medium- to large-sized ecological niches in the Cenozoic, after the Cretaceous–Paleogene extinction event emptied ecological space once filled by non-avian dinosaurs and groups of reptiles that were now absent. Then mammals diversified very quickly; both birds and mammals show an exponential rise in diversity. For example, the earliest known bat dates from about 50 million years ago, only 16 million years after the extinction of the dinosaurs.
Recent molecular phylogenetic studies suggest that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene. But paleontologists object that no placental fossils have been found from before the end of the Cretaceous. The earliest undisputed fossils of placentals come from the early Paleocene, after the extinction of the dinosaurs. In particular, scientists have recently identified an early Paleocene animal named "Protungulatum donnae" as one of the first placental mammals. The earliest known ancestor of primates is "Archicebus achilles" from around 55 million years ago. This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.
During the Cenozoic, several groups of mammals appeared which were much larger than their nearest modern equivalents, but none was even close to the size of the largest dinosaurs with similar feeding habits.
Earliest appearances of features.
"Hadrocodium", whose fossils date from approximately 195 million years ago, in the Early Jurassic, provides the first clear evidence of a jaw joint formed solely by the squamosal and dentary bones; there is no space in the jaw for the articular, a bone involved in the jaws of all early synapsids.
It has been suggested that the original function of lactation (milk production) was to keep eggs moist. Much of the argument is based on monotremes, the egg-laying mammals.
The earliest clear evidence of hair or fur is in fossils of "Castorocauda", from 164 million years ago in the Middle Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur; it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard "Tupinambis" has foramina which are almost identical to those found in the nonmammalian cynodont "Thrinaxodon". Popular sources, nevertheless, continue to attribute whiskers to "Thrinaxodon".
The evolution of erect limbs in mammals is incomplete — living and fossil monotremes have sprawling limbs. The parasagittal (nonsprawling) limb posture appeared sometime in the Early Cretaceous or latest Jurassic; it is found in the eutherian "Eomaia" and the metatherian "Sinodelphys", both dated 125 million years ago.
When endothermy first appeared in the evolution of mammals is uncertain. Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. Some of the evidence found so far suggests that Triassic cynodonts had fairly high metabolic rates, but it is not conclusive. For small animals, an insulative covering like fur is necessary for the maintenance of a high and stable body temperature.
Anatomy and morphology.
Skeletal system.
The majority of mammals have seven cervical vertebrae (bones in the neck), including bats, giraffes, whales, and humans. The exceptions are the manatee and the two-toed sloth, which have only six cervical vertebrae, and the three-toed sloth with nine cervical vertebrae.
Respiratory system.
The lungs of mammals have a spongy texture and are honeycombed with epithelium having a much larger surface area in total than the outer surface area of the lung itself. The lungs of humans are typical of this type of lung.
Breathing is largely driven by the muscular diaphragm, which divides the thorax from the abdominal cavity, forming a dome with its convexity towards the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the cavity in which the lung is enclosed. Air enters through the oral and nasal cavities; it flows through the larynx, trachea and bronchi and expands the alveoli. Relaxation of the diaphragm has the opposite effect, passively recoiling during normal breathing. During exercise, the abdominal wall contracts, increasing visceral pressure on the diaphragm, thus forcing the air out more quickly and forcefully. The rib cage itself also is able to expand and contract the thoracic cavity to some degree, through the action of other respiratory and accessory respiratory muscles. As a result, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. This type of lung is known as a bellows lung as it resembles a blacksmith's bellows. Mammals take oxygen into their lungs, and discard carbon dioxide.
Nervous system.
All mammalian brains possess a neocortex, a brain region unique to mammals. Placental mammals have a corpus callosum, unlike monotremes and marsupials. The size and number of cortical areas (Brodmann's areas) is least in monotremes (about 8-10) and most in placentals (up to 50).
Integumentary system.
The integumentary system is made up of three layers: the outermost epidermis, the dermis, and the hypodermis.
The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue. Its job is to store lipids, and to provide cushioning and insulation. The thickness of this layer varies widely from species to species.
Although other animals have features such as whiskers, feathers, setae, or cilia that superficially resemble it, no animals other than mammals have hair. It is a definitive characteristic of the class. Though some mammals have very little, careful examination reveals the characteristic, often in obscure parts of their bodies.
Color Variation in Mammals This hair, also known as pelage, can vary in color between populations, organisms within a population, and even on the individual organism. Light-dark color variation is common in the mammalian taxa. Sometimes, this color variation is determined by age variation, however, in other cases, it is determined by other factors Selective pressures, such as ecological interactions with other populations or environmental conditions, often lead to the variation in mammalian coloration. These selective pressures favor certain colors in order to increase survival. Camouflage is thought to be a major selection pressure shaping coloration in mammals, although there is also evidence that sexual selection, communication, and physiological processes may influence the evolution of coloration as well.
Camouflage is the most predominant mechanism for color variation, as it aids in the concealment of the organisms from predators or from their prey. Coat color can also be for intraspecies communication such as warning members of their species about predators, indicating health for reproductive purposes, communicating between mother and young, and intimidating predators.
Studies have shown that in some cases, differences in female and male coat color could indicate information nutrition and hormone levels, which are important in the mate selection process. One final mechanism for coat color variation is physiological response purposes, such as temperature regulation in tropical or arctic environments. Although much has been observed about color variation, much of the genetic that link coat color to genes is still unknown. The genetic sites where pigmentation genes are found are known to affect phenotype by: 1) altering the spatial distribution of pigmentation of the hairs, and 2) altering the density and distribution of the hairs. Quantitative trait mapping is being used to better understand the distribution of loci responsible for pigmentation variation. However, although the genetic sites are known, there is still much to learn about how these genes are expressed.
Some primates and marsupials have shades of violet, green, or blue skin on parts of their bodies. The two-toed sloth and the polar bear sometimes appear to have green fur, but this color is caused by algal growths.
Reproductive system.
Most mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypuses and the echidnas, lay eggs. The monotremes have a sex determination system different from that of most other mammals. In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal.
The mammary glands of mammals are specialized to produce milk, a liquid used by newborns as their primary source of nutrition. The monotremes branched early from other mammals and do not have the nipples seen in most mammals, but they do have mammary glands. The young lick the milk from a mammary patch on the mother's belly.
Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. A marsupial has a short gestation period, typically shorter than its estrous cycle, and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. The placentals give birth to complete and fully developed young, usually after long gestation periods.
Physiology.
Endothermy.
Nearly all mammals are endothermic ("warm-blooded"). Most mammals also have hair to help keep them warm. Like birds, mammals can forage or hunt in weather and climates too cold for nonavian reptiles and large insects.
Endothermy requires plenty of food energy, so mammals eat more food per unit of body weight than most reptiles. Small insectivorous mammals eat prodigious amounts for their size.
A rare exception, the naked mole-rat, produces little metabolic heat, so it is considered an operational poikilotherm. Birds and tuna are also endothermic, so endothermy is not peculiar to mammals.
Intelligence.
In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.
Locomotion.
Mammals evolved from four-legged ancestors. They use their limbs to walk, climb, swim, or fly. Some land mammals have toes that produce claws for climbing or hooves for running. Aquatic mammals like whales and dolphins have flippers which evolved from legs.
Terrestrial'Arboreal'Aquatic
Whales and dolphins propel themselves through the water by moving their up and down, adjusting the angle of the flukes as needed. The more massive front of the body contributes stability.
Aerial
Feeding.
To maintain a high constant body temperature is energy expensive – mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals – this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants. A herbivorous diet includes subtypes such as fruit-eating and grass-eating. An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract, because the proteins, lipids, and minerals found in meat require little in the way of specialized digestion. Plants, on the other hand, contain complex carbohydrates, such as cellulose. The digestive tract of an herbivore is therefore host to bacteria that ferment these substances, and make them available for digestion. The bacteria are either housed in the multichambered stomach or in a large cecum. The size of an animal is also a factor in determining diet type. Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about 18 oz (500 g) are mostly insectivorous because they cannot tolerate the slow, complex digestive process of a herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (those that prey on larger vertebrates) or a slower digestive process (herbivores). Furthermore, mammals that weigh more than 18 oz (500 g) usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).
Specializations in herbivory include:
Granivory "seed eating", folivory "leaf eating", frugivory "fruit eating", nectivory "nectar eating", gummivory "gum eating", and mycophagy "fungus eating".
Hybrid mammals.
The deliberate or accidental hybridising of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown in recent times for economic purposes. The number of successful interspecific mammalian hybrids is relatively small, although it has come to be known that there is a significant number of naturally occurring hybrids between forms or regional varieties of a single species. These may form zones of gradation known as clines. Indeed the distinction between some hitherto distinct species can become clouded once it can be shown that they may not only breed but produce fertile offspring. Some hybrid animals exhibit greater strength and resilience than either parent. This is known as hybrid vigor. The existence of the mule (donkey sire; horse dam) being used widely as a hardy draught animal throughout ancient and modern history is testament to this. Other well known examples are the lion/tiger hybrid, the liger, which is by far the largest big cat and sometimes used in circuses; and cattle hybrids such as between European and Indian domestic cattle or between domestic cattle and American bison, which are used in the meat industry and marketed as Beefalo. There is some speculation that the donkey itself may be the result of an ancient hybridisation between two wild ass species or sub-species. Hybrid animals are normally infertile partly because their parents usually have slightly different numbers of chromosomes, resulting in unpaired chromosomes in their cells, which prevents division of sex cells and the gonads from operating correctly, particularly in males. There are exceptions to this rule, especially if the speciation process was relatively recent or incomplete as is the case with many cattle and dog species. Normally behavior traits, natural hostility, natural ranges and breeding cycle differences maintain the separateness of closely related species and prevent natural hybridisation. However the widespread disturbances to natural animal behaviours and range caused by human activity, cities, dumping grounds with food, agriculture, fencing, roads and so on do force animals together which would not normally breed. Clear examples exist between the various sub-species of grey wolf, coyote and domestic dog in North America. As many birds and mammals imprint on their mother and immediate family from infancy, a practice used by animal hybridizers is to foster a planned parent in a hybridization program with the same species as the one with which they are planned to mate.
External links.
<br>

</doc>
<doc id="18839" url="http://en.wikipedia.org/wiki?curid=18839" title="Music">
Music

Music is an art form whose medium is sound. Generally, a song is considered the smallest standalone work of music, especially when involving singing. The common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics, and the sonic qualities of timbre and texture. The word derives from Greek μουσική ("mousike"; "art of the Muses"). In its most general form the activities describing music as an art form include the production of works of music, the criticism of music, the study of the history of music, and the aesthetic dissemination of music.
The creation, performance, significance, and even the definition of music vary according to culture and social context. Music ranges from strictly organized compositions (and their recreation in performance), through improvisational music to aleatoric forms. Music can be divided into genres and subgenres, although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial. Within the arts, music may be classified as a performing art, a fine art, and auditory art. It may also be divided among art music and folk music. There is also a strong connection between music and mathematics. Music may be played and heard live, may be part of a dramatic work or film, or may be recorded.
To many people in many cultures, music is an important part of their way of life. Ancient Greek and Indian philosophers defined music as tones ordered horizontally as melodies and vertically as harmonies. Common sayings such as "the harmony of the spheres" and "it is music to my ears" point to the notion that music is often ordered and pleasant to listen to. However, 20th-century composer John Cage thought that any sound can be music, saying, for example, "There is no noise, only sound."
Etymology.
The word derives from Greek μουσική ("mousike"; "art of the Muses").
Music as form of art.
Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. As the poet and essayist Geoffrey O'Brien notes, mix tapes are an art form in themselves, a "self-portrait, a gesture of friendship, prescription for an ideal party... an environment consisting solely of what is most ardently loved.". Amateur musicians compose and perform music for their own pleasure, and they do not derive their income from music. Professional musicians are employed by a range of institutions and organisations, including armed forces, churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers, seeking contracts and engagements in a variety of settings.
There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles, such as concert bands, orchestras, and other ensembles. In some cases, amateur musicians attain a professional level of competence, and they are able to perform in professional performance settings. A distinction is often made between music performed for the benefit of a live audience and music that is performed for the purpose of being recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is recorded and distributed (or broadcast).
Composition.
"Composition" is often classed as the creation and recording of music via a medium by which others can interpret it (i.e., paper or sound). Many cultures use at least part of the concept of preconceiving musical material, or composition, as held in western classical music. Even when music is notated precisely, there are still many decisions that a performer has to make. The process of a performer deciding how to perform music that has been previously composed and notated is termed interpretation. Different performers' interpretations of the same music can vary widely. Composers and song writers who present their own music are interpreting, just as much as those who perform the music of others or folk music. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean either individual choices of a performer, or an aspect of music that is not clear, and therefore has a "standard" interpretation.
In some musical genres, such as jazz and blues, even more freedom is given to the performer to engage in improvisation on a basic melodic, harmonic, or rhythmic framework. The greatest latitude is given to the performer in a style of performing called free improvisation, which is material that is spontaneously "thought of" (imagined) while being performed, "not" preconceived. Improvised music usually follows stylistic or genre conventions and even "fully composed" includes some freely chosen material. Composition does not always mean the use of notation, or the known sole authorship of one individual. Music can also be determined by describing a "process" that creates musical sounds. Examples of this range from wind chimes, through computer programs that select sounds. Music from random elements is called Aleatoric music, and is associated with such composers as John Cage, Morton Feldman, and Witold Lutosławski.
Music can be composed for repeated performance or it can be improvised: composed on the spot. The music can be performed entirely from memory, from a written system of musical notation, or some combination of both. Study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include spontaneously improvised works like those of free jazz performers and African drummers such as the Ewe drummers.
Notation.
Notation is the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music is notated, along with instructions on how to perform the music. The study of how to read notation involves music theory, harmony, the study of performance practice, and in some cases an understanding of historical performance methods. Written notation varies with style and period of music. In Western Art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands."
In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as "tab"), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tabulature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument. Notated music is produced as sheet music. To perform music from notation requires an understanding of both the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or a genre. In improvisation, the performer often plays from music where only the chord changes are written, requiring a great understanding of the music's structure and chord progressions.
Improvisation.
Musical improvisation is the creation of spontaneous music. Improvisation is often considered an act of instantaneous composition by performers, where compositional techniques are employed with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos and melody lines. In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era; solo performers and singers improvised virtuoso cadenzas during concerts. However, in the 20th and early 21st century, as "common practice" western art music performance became institutionalized in symphony orchestras, opera houses and ballets, improvisation has played a smaller role at the same time that many composers increasingly returned to its inclusion in their creative work. In Indian classical music, spontaneous improvisation is a core component and an essential criteria of any performance.
Theory.
Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques and examining the language and notation of music. In a grand sense, music theory distills and analyzes the parameters or elements of music – rhythm, harmony (harmonic function), melody, structure, form, and texture. Broadly, music theory may include any statement, belief, or conception of or about music. People who study these properties are known as music theorists. Some have applied acoustics, human physiology, and psychology to the explanation of how and why music is perceived. Music has many different fundamentals or elements. These are, but are not limited to: pitch, beat or pulse, rhythm, melody, harmony, texture, allocation of voices, timbre or color, expressive qualities (dynamics and articulation), and form or structure.
Pitch is a subjective sensation, reflecting generally the lowness or highness of a sound. Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars. A melody is a series of notes sounding in succession. The notes of a melody are typically created with respect to pitch systems such as scales or modes. Harmony is the study of vertical sonorities in music. Vertical sonority refers to considering the relationships between pitches that occur together; usually this means at the same time, although harmony can also be implied by a melody that outlines a harmonic structure. Notes can be arranged into different scales and modes. Western music theory generally divides the octave into a series of 12 notes that might be included in a piece of music. In music written using the system of major-minor tonality, the key of a piece determines the scale used. Musical texture is the overall sound of a piece of music commonly described according to the number of and relationship between parts or lines of music: monophony, heterophony, polyphony, homophony, or monody.
Timbre, sometimes called "Color" or "Tone Color" is the quality or sound of a voice or instrument. Expressive Qualities are those elements in music that create change in music that are not related to pitch, rhythm or timbre. They include Dynamics and Articulation. Form is a facet of music theory that explores the concept of musical syntax, on a local and global level. Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo. Popular Music often makes use of strophic form often in conjunction with Twelve bar blues. Analysis is the effort to describe and explain music.
History.
Prehistoric eras.
Prehistoric music can only be theorized based on findings from paleolithic archaeology sites. Flutes are often discovered, carved from bones in which lateral holes have been pierced; these are thought to have been blown at one end like the Japanese shakuhachi. The Divje Babe flute, carved from a cave bear femur, is thought to be at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley Civilization archaeological sites. India has one of the oldest musical traditions in the world—references to Indian classical music ("marga") are found in the Vedas, ancient scriptures of the Hindu tradition. The earliest and largest collection of prehistoric musical instruments was found in China and dates back to between 7000 and 6600 BC. The Hurrian song, found on clay tablets that date back to approximately 1400 BC, is the oldest surviving notated work of music.
Ancient Egypt.
The ancient Egyptians credited one of their gods, Thoth, with the invention of music, with Osiris in turn used as part of his effort to civilize the world. The earliest material and representational evidence of Egyptian musical instruments dates to the Predynastic period, but the evidence is more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments, lyres and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today. Egyptian folk music, including the traditional Sufi "dhikr" rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments.
Asian cultures.
Indian classical music is one of the oldest musical traditions in the world. The Indus Valley civilization has sculptures that show dance and old musical instruments, like the seven holed flute. Various types of stringed instruments and drums have been recovered from Harrappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler. The Rigveda has elements of present Indian music, with a musical notation to denote the metre and the mode of chanting. Indian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas.Silappadhikaram" by Ilango Adigal gives so much information about how new scale can be formed by modal shift of tonic from existing scale. Hindustani music was influenced by the Persian performance practices of the Afghan Mughals. Carnatic music popular in the southern states, is largely devotional; the majority of the songs are addressed to the Hindu deities. There are a lot of songs emphasising love and other social issues.
Asian music covers the music cultures of Arabia, Central Asia, East Asia, South Asia, and Southeast Asia. Chinese classical music, the traditional art or court music of China, has a history stretching over around three thousand years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or musical genres. Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5 + 7 = 12) as does European-influenced music. Persian music is the music of Persia and Persian language countries: "musiqi", the science and art of music, and "muzik", the sound and performance of music (Sakata 1983).
References in the Bible.
Music and theatre scholars studying the history and anthropology of Semitic and early Judeo-Christian culture have discovered common links in theatrical and musical activity between the classical cultures of the Hebrews and those of later Greeks and Romans. The common area of performance is found in a "social phenomenon called litany," a form of prayer consisting of a series of invocations or supplications. "The Journal of Religion and Theatre" notes that among the earliest forms of litany, "Hebrew litany was accompanied by a rich musical tradition:"
Antiquity.
Western cultures have had a major influence on the development of music. The history of the music of the Western cultures can be traced back to Ancient Greece times.
Ancient Greece.
Music was an important part of social and cultural life in Ancient Greece. Musicians and singers played a prominent role in Greek theater. Mixed-gender choruses performed for entertainment, celebration, and spiritual ceremonies. Instruments included the double-reed "aulos" and a plucked string instrument, the "lyre", principally the special kind called a "kithara". Music was an important part of education, and boys were taught music starting at age six. Greek musical literacy created a flowering of music development. Greek music theory included the Greek musical modes, that eventually became the basis for Western religious and classical music. Later, influences from the Roman Empire, Eastern Europe, and the Byzantine Empire changed Greek music. The Seikilos epitaph is the oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world.
The first work written on the subject of music theory is "Harmonika Stoicheia".
The Middle Ages.
The medieval era (476 to 1400) started with the introduction of chanting into Roman Catholic Church services. Western Music then started becoming more of an art form with the advances in music notation. The only European Medieval repertory that survives from before about 800 is the monophonic liturgical plainsong of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song. Examples of composers from this period are Léonin, Pérotin and Guillaume de Machaut.
The Renaissance.
Renaissance music (c. 1400 to 1600) was more focused on secular themes. Around 1450, the printing press was invented, and that helped to disseminate musical styles more quickly and across a larger area. Thus, music could play an increasingly important role in daily life. Musicians worked for the church, courts and towns. Church choirs grew in size, and the church remained an important patron of music. By the middle of the 15th century, composers wrote richly polyphonic sacred music. Prominent composers from this era are Guillaume Dufay, Giovanni Pierluigi da Palestrina, Thomas Morley, and Orlande de Lassus. However, musical activity shifted to the courts. Kings and princes competed for the finest composers.
Many leading important composers came from the Netherlands, Belgium, and northern France and are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical lives include Germany, England, and Spain.
The Baroque.
The Baroque era of music took place from 1600 to 1750, as the Baroque artistic style flourished across Europe; and during this time, music expanded in its range and complexity. Baroque music began when the first operas were written and when contrapuntal music became prevalent. German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as choirs, pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and ornamental and rich in its melodies. Composers from the Baroque era include Johann Sebastian Bach, George Frideric Handel, and Georg Philipp Telemann.
Classicism.
The music of the Classical Period (1750 to 1830) looked to the art and philosophy of Ancient Greece and Rome, to the ideals of balance, proportion and disciplined expression. It has a lighter, clearer and considerably simpler texture, and tended to be almost voicelike and singable. New genres were discovered. The main style was the homophony, where prominent melody and accompaniment are clearly distinct.
Importance was given to instrumental music. It was dominated by further evolution of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Others main kinds were trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era were based on the dramatic structure of the sonata.
One of the most important evolutionary steps made in the Classical period was the development of public concerts. The aristocracy would still play a significant role in the sponsorship of musical life, but it was now possible for composers to survive without being its permanent employees. The increasing popularity led to a growth in both the number and range of the orchestras. The expansion of orchestral concerts necessitated large public spaces. As a result of all these processes, symphonic music (including opera, ballet and oratorio) became more extroverted.
The best known composers of Classicism are Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in evolution towards Romanticism.
Romanticism.
Romantic music (c. 1810 to 1900) turned the rigid styles and forms of the Classical era into more passionate and expressive pieces. It attempted to increase emotional expression and power to describe deeper truths or human feelings. The emotional and expressive qualities of music came to take precedence over technique and tradition. Romantic composers grew in idiosyncrasy, and went further in the syncretism of different art-forms (such as literature), history (historical figures), or nature itself with music. Romantic love was a prevalent theme in many works composed during this period. In some cases the formal structures from the classical period were preserved, but in many others existing genres, forms, and functions were improved. Also, new forms were created that were deemed better suited to the new subject matter. Opera and ballet continued to evolve.
In 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, motifs, developed organically, came to replace melody as the most significant compositional unit. Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more elaborated chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During Romantic period tonality was at its peak. The late 19th century saw a dramatic expansion in the size of the orchestra, and in the role of concerts as part of urban society. It also saw a new diversity in theatre music, including operetta, and musical comedy and other forms of musical theatre.
20th- and 21st-century music.
With 20th-century music, there was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music. The focus of art music was characterized by exploration of new rhythms, styles, and sounds. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th-century art music. The invention of sound recording and the ability to edit music gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition.
Jazz evolved and became an important genre of music over the course of the 20th century, and during the second half of that century, rock music did the same. Jazz is an American musical artform that originated in the beginning of the 20th century in African American communities in the Southern United States from a confluence of African and European music traditions. The style's West African pedigree is evident in its use of blue notes, improvisation, polyrhythms, syncopation, and the swung note. From its early development until the present, jazz has also incorporated music from 19th- and 20th-century American popular music. Jazz has, from its early-20th-century inception, spawned a variety of subgenres, ranging from New Orleans Dixieland (1910s) to 1970s and 1980s-era jazz-rock fusion.
Rock music is a genre of popular music that developed in the 1960s from 1950s rock and roll, rockabilly, blues, and country music. The sound of rock often revolves around the electric guitar or acoustic guitar, and it uses a strong back beat laid down by a rhythm section of electric bass guitar, drums, and keyboard instruments such as organ, piano, or, since the 1970s, analog synthesizers and digital ones and computers since the 1990s. Along with the guitar or keyboards, saxophone and blues-style harmonica are used as soloing instruments. In its "purest form," it "has three chords, a strong, insistent back beat, and a catchy melody." In the late 1960s and early 1970s, it branched out into different subgenres, ranging from blues rock and jazz-rock fusion to heavy metal and punk rock, as well as the more classical influenced genre of progressive rock and several types of experimental rock genres.
Performance.
Performance is the physical expression of music. Often, a musical work is performed once its structure and instrumentation are satisfactory to its creators; however, as it gets performed, it can evolve and change. A performance can either be rehearsed or improvised. Improvisation is a musical idea created without premeditation, while rehearsal is vigorous repetition of an idea until it has achieved cohesion. Musicians will sometimes add improvisation to a well-rehearsed idea to create a unique performance.
Many cultures include strong traditions of solo and performance, such as in Indian classical music, and in the Western art-music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing for one's enjoyment to highly planned and organised performance rituals such as the modern classical concert, religious processions, music festivals or music competitions. Chamber music, which is music for a small ensemble with only a few of each type of instrument, is often seen as more intimate than symphonic works.
Aural tradition.
Many types of music, such as traditional blues and folk music were originally preserved in the memory of performers, and the songs were handed down orally, or aurally (by ear). When the composer of music is no longer known, this music is often classified as "traditional." Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those that demand improvisation or modification to the music. A culture's history may also be passed by ear through song.
Ornamentation.
The detail included explicitly in the music notation varies between genres and historical periods. In general, art music notation from the 17th through the 19th century required performers to have a great deal of contextual knowledge about performing styles. For example, in the 17th and 18th century, music notated for solo performers typically indicated a simple, unadorned melody. However, performers were expected to know how to add stylistically appropriate ornaments, such as trills and turns. In the 19th century, art music for solo performers may give a general instruction such as to perform the music expressively, without describing in detail how the performer should do this. The performer was expected to know how to use tempo changes, accentuation, and pauses (among other devices) to obtain this "expressive" performance style. In the 20th century, art music notation often became more explicit and used a range of markings and annotations to indicate to performers how they should play or sing the piece.
In popular music and jazz, music notation almost always indicates only the basic framework of the melody, harmony, or performance approach; musicians and singers are expected to know the performance conventions and styles associated with specific genres and pieces. For example, the "lead sheet" for a jazz tune may only indicate the melody and the chord changes. The performers in the jazz ensemble are expected to know how to "flesh out" this basic structure by adding ornaments, improvised music, and chordal accompaniment.
Philosophy and aesthetics.
Philosophy of music is the study of fundamental questions regarding music. The philosophical study of music has many connections with philosophical questions in metaphysics and aesthetics. 
Some basic questions in the philosophy of music are:
Traditionally, the aesthetics of music explored the mathematical and cosmological dimensions of rhythmic and harmonic organization. In the eighteenth century, focus shifted to the experience of hearing music, and thus to questions about its beauty and human enjoyment ("plaisir" and "jouissance") of music. The origin of this philosophic shift is sometimes attributed to Baumgarten in the 18th century, followed by Kant. Through their writing, the ancient term 'aesthetics', meaning sensory perception, received its present day connotation. In recent decades philosophers have tended to emphasize issues besides beauty and enjoyment. For example, music's capacity to express emotion has been a central issue.
In the 20th century, important contributions were made by Peter Kivy, Jerrold Levinson, Roger Scruton, and Stephen Davies. However, many musicians, music critics, and other non-philosophers have contributed to the aesthetics of music. In the 19th century, a significant debate arose between Eduard Hanslick, a music critic and musicologist, and composer Richard Wagner. Harry Partch and some other musicologists, such as Kyle Gann, have studied and tried to popularize microtonal music and the usage of alternate musical scales. Also many modern composers like Lamonte Young, Rhys Chatham and Glenn Branca paid much attention to a scale called just intonation.
It is often thought that music has the ability to affect our emotions, intellect, and psychology; it can assuage our loneliness or incite our passions. The philosopher Plato suggests in "the Republic" that music has a direct effect on the soul. Therefore, he proposes that in the ideal regime music would be closely regulated by the state. (Book VII)
There has been a strong tendency in the aesthetics of music to emphasize the paramount importance of compositional structure; however, other issues concerning the aesthetics of music include lyricism, harmony, hypnotism, emotiveness, temporal dynamics, resonance, playfulness, and color (see also musical development).
Psychology.
Modern music psychology aims to explain and understand musical behavior and experience. Research in this field and its subfields are primarily empirical; their knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. In addition to its focus on fundamental perceptions and cognitive processes, music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.
Cognitive neuroscience of music.
Cognitive neuroscience of music is the scientific study of brain-based mechanisms involved in the cognitive processes underlying music. These behaviours include music listening, performing, composing, reading, writing, and ancillary activities. It also is increasingly concerned with the brain basis for musical aesthetics and musical emotion. The field is distinguished by its reliance on direct observations of the brain, using such techniques as functional magnetic resonance imaging (fMRI), transcranial magnetic stimulation (TMS), magnetoencephalography (MEG), electroencephalography (EEG), and positron emission tomography (PET).
Cognitive musicology.
Cognitive musicology is a branch of cognitive science concerned with computationally modeling musical knowledge with the goal of understanding both music and cognition. The use of computer models provides an exacting, interactive medium in which to formulate and test theories and has roots in artificial intelligence and cognitive science.
This interdisciplinary field investigates topics such as the parallels between language and music in the brain. Biologically inspired models of computation are often included in research, such as neural networks and evolutionary programs. This field seeks to model how musical knowledge is represented, stored, perceived, performed, and generated. By using a well-structured computer environment, the systematic structures of these cognitive phenomena can be investigated.
Psychoacoustics.
Psychoacoustics is the scientific study of sound perception. More specifically, it is the branch of science studying the psychological and physiological responses associated with sound (including speech and music). It can be further categorized as a branch of psychophysics.
Evolutionary musicology.
Evolutionary musicology concerns the "origins of music, the question of animal song, selection pressures underlying music evolution", and "music evolution and human evolution". It seeks to understand music perception and activity in the context of evolutionary theory. Charles Darwin speculated that music may have held an adaptive advantage and functioned as a protolanguage, a view which has spawned several competing theories of music evolution. An alternate view sees music as a by-product of linguistic evolution; a type of "auditory cheesecake" that pleases the senses without providing any adaptive function. This view has been directly countered by numerous music researchers.
Culture in music cognition.
An individual's culture or ethnicity plays a role in their music cognition, including their preferences, emotional reaction, and musical memory. Musical preferences are biased toward culturally familiar musical traditions beginning in infancy, and adults' classification of the emotion of a musical piece depends on both culturally specific and universal structural features. Additionally, individuals' musical memory abilities are greater for culturally familiar music than for culturally unfamiliar music.
Sociology.
Many ethnographic studies demonstrate that music is a participatory, community-based activity. Music is experienced by individuals in a range of social settings ranging from being alone to attending a large concert, forming a music community, which cannot be understood as a function of individual will or accident; it includes both commercial and non-commercial participants with a shared set of common values. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there is often a divide between what types of music are viewed as a "high culture" and "low culture." "High culture" types of music typically include Western art music such as Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly in seats.
Other types of music—including, but not limited to, jazz, blues, soul, and country—are often performed in bars, nightclubs, and theatres, where the audience may be able to drink, dance, and express themselves by cheering. Until the later 20th century, the division between "high" and "low" musical forms was widely accepted as a valid distinction that separated out better quality, more advanced "art music" from the popular styles of music heard in bars and dance halls.
However, in the 1980s and 1990s, musicologists studying this perceived divide between "high" and "low" musical genres argued that this distinction is not based on the musical value or quality of the different types of music. Rather, they argued that this distinction was based largely on the socioeconomics standing or social class of the performers or audience of the different types of music. For example, whereas the audience for Classical symphony concerts typically have above-average incomes, the audience for a rap concert in an inner-city area may have below-average incomes. Even though the performers, audience, or venue where non-"art" music is performed may have a lower socioeconomic status, the music that is performed, such as blues, rap, punk, funk, or ska may be very complex and sophisticated.
When composers introduce styles of music that break with convention, there can be a strong resistance from academic music experts and popular culture. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop-era jazz, hip hop, punk rock, and electronica have all been considered non-music by some critics when they were first introduced. Such themes are examined in the sociology of music. The sociological study of music, sometimes called sociomusicology, is often pursued in departments of sociology, media studies, or music, and is closely related to the field of ethnomusicology.
Media and technology.
The music that composers make can be heard through several media; the most traditional way is to hear it live, in the presence of the musicians (or as one of the musicians), in an outdoor or indoor space such as an amphitheatre, concert hall, cabaret room or theatre. Live music can also be broadcast over the radio, television or the Internet. Some musical styles focus on producing a sound for a performance, while others focus on producing a recording that mixes together sounds that were never played "live." Recording, even of essentially live styles, often uses the ability to edit and splice to produce recordings considered better than the actual performance.
As talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s live musical performances by orchestras, pianists, and theater organists were common at first-run theaters. With the coming of the talking motion pictures, those featured performances were largely eliminated. The American Federation of Musicians (AFM) took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the "Pittsburgh Press" features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever"
Since legislation introduced to help protect performers, composers, publishers and producers, including the Audio Home Recording Act of 1992 in the United States, and the 1979 revised Berne Convention for the Protection of Literary and Artistic Works in the United Kingdom, recordings and live performances have also become more accessible through computers, devices and Internet in a form that is commonly known as Music-On-Demand.
In many cultures, there is less distinction between performing and listening to music, since virtually everyone is involved in some sort of musical activity, often communal. In industrialized countries, listening to music through a recorded form, such as sound recording or watching a music video, became more common than experiencing live performance, roughly in the middle of the 20th century.
Sometimes, live performances incorporate prerecorded sounds. For example, a disc jockey uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Computers and many keyboards can be programmed to produce and play Musical Instrument Digital Interface (MIDI) music. Audiences can also "become" performers by participating in karaoke, an activity of Japanese origin centered on a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks.
Internet.
The advent of the Internet has transformed the experience of music, partly through the increased ease of access to music and the increased choice. Chris Anderson, in his book "The Long Tail: Why the Future of Business is Selling Less of More", suggests that while the economic model of supply and demand describes scarcity, the Internet retail model is based on abundance. Digital storage costs are low, so a company can afford to make its whole inventory available online, giving customers as much choice as possible. It has thus become economically viable to offer products that very few people are interested in. Consumers' growing awareness of their increased choice results in a closer association between listening tastes and social identity, and the creation of thousands of niche markets.
Another effect of the Internet arises with online communities like YouTube and Facebook, a social networking service. Such sites simplify connecting with other musicians, and greatly facilitate the distribution of music. Professional musicians also use YouTube as a free publisher of promotional material. YouTube users, for example, no longer only download and listen to MP3s, but also actively create their own. According to Don Tapscott and Anthony D. Williams, in their book "Wikinomics", there has been a shift from a traditional consumer role to what they call a "prosumer" role, a consumer who both creates and consumes. Manifestations of this in music include the production of mashes, remixes, and music videos by fans.
Business.
The music industry refers to the business industry connected with the creation and sale of music. It consists of record companies, labels and publishers that distribute recorded music products internationally and that often control the rights to those products. Some music labels are "independent," while others are subsidiaries of larger corporate entities or international media groups. In the 2000s, the increasing popularity of listening to music as digital music files on MP3 players, iPods, or computers, and of trading music on file sharing sites or buying it online in the form of digital files had a major impact on the traditional music business. Many smaller independent CD stores went out of business as music buyers decreased their purchases of CDs, and many labels had lower CD sales. Some companies did well with the change to a digital format, though, such as Apple's iTunes, an online store that sells digital files of songs over the Internet.
Education.
Non-professional.
The incorporation of music training from preschool to post secondary education is common in North America and Europe. Involvement in music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas. In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music. In secondary schools students may have the opportunity to perform some type of musical ensembles, such as choirs, marching bands, concert bands, jazz bands, or orchestras, and in some school systems, music classes may be available. Some students also take private music lessons with a teacher. Amateur musicians typically take lessons to learn musical rudiments and beginner- to intermediate-level musical techniques.
At the university level, students in most arts and humanities programs can receive credit for taking music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some type of musical ensembles that non-music students are able to participate in, such as choirs, marching bands, concert bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as the Indonesian Institute of the Arts in Yogyakarta, Indonesia, or the classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music).
Academia.
Musicology is the study of the subject of music. The earliest definitions defined three sub-disciplines: systematic musicology, historical musicology, and comparative musicology or ethnomusicology. In contemporary scholarship, one is more likely to encounter a division of the discipline into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-western cultures, and the cultural study of music, is called ethnomusicology. Students can pursue the undergraduate study of musicology, ethnomusicology, music history, and music theory through several different types of degrees, including a B.Mus, a B.A. with concentration in music, a B.A. with Honors in Music, or a B.A. in Music History and Literature. Graduates of undergraduate music programs can go on to further study in music graduate programs.
Graduate degrees include the Master of Music, the Master of Arts, the Doctor of Philosophy (Ph.D.) (e.g., in musicology or music theory), and more recently, the Doctor of Musical Arts, or DMA. The Master of Music degree, which takes one to two years to complete, is typically awarded to students studying the performance of an instrument, education, voice or composition. The Master of Arts degree, which takes one to two years to complete and often requires a thesis, is typically awarded to students studying musicology, music history, or music theory. Undergraduate university degrees in music, including the Bachelor of Music, the Bachelor of Music Education, and the Bachelor of Arts (with a major in music) typically take three to five years to complete. These degrees provide students with a grounding in music theory and music history, and many students also study an instrument or learn singing technique as part of their program.
The PhD, which is required for students who want to work as university professors in musicology, music history, or music theory, takes three to five years of study after the Master's degree, during which time the student will complete advanced courses and undertake research for a dissertation. The DMA is a relatively new degree that was created to provide a credential for professional performers or composers that want to work as university professors in musical performance or composition. The DMA takes three to five years after a Master's degree, and includes advanced courses, projects, and performances. In Medieval times, the study of music was one of the Quadrivium of the seven Liberal Arts and considered vital to higher learning. Within the quantitative Quadrivium, music, or more accurately harmonics, was the study of rational proportions.
Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" François-Bernard Mâche's "Musique, mythe, nature, ou les Dauphins d'Arion" (1983), a study of "ornitho-musicology" using a technique of Nicolas Ruwet's "Langage, musique, poésie" (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human."
Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even of music of the common practice period, may take many other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. "Speculative music theory", contrasted with "analytic music theory", is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition.
Ethnomusicology.
In the West, much of the history of music that is taught deals with the Western civilization's art music. The history of music in other cultures ("world music" or the field of "ethnomusicology") is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. Popular styles of music varied widely from culture to culture, and from period to period. Different cultures emphasised different instruments, or techniques, or uses for music. Music has been used not only for entertainment, for ceremonies, and for practical and artistic communication, but also for propaganda.
There is a host of music classifications, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music – including rock music, country music, and pop music). Some genres do not fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz music).
As world cultures have come into greater contact, their indigenous musical styles have often merged into new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic society. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like George Gershwin's "Rhapsody in Blue", are claimed by both jazz and classical music, while Gershwin's "Porgy and Bess" and Leonard Bernstein's "West Side Story" are claimed by both opera and the Broadway musical tradition. Many current music festivals celebrate a particular musical genre.
Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly three forms of classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only percussion music such as the talavadya performances famous in South India.
Music therapy.
Music therapy is an interpersonal process in which the therapist uses music and all of its facets—physical, emotional, mental, social, aesthetic, and spiritual—to help clients to improve or maintain their health. In some instances, the client's needs are addressed directly through music; in others they are addressed through the relationships that develop between the client and therapist. Music therapy is used with individuals of all ages and with a variety of conditions, including: psychiatric disorders, medical problems, physical handicaps, sensory impairments, developmental disabilities, substance abuse, communication disorders, interpersonal problems, and aging. It is also used to: improve learning, build self-esteem, reduce stress, support physical exercise, and facilitate a host of other health-related activities.
One of the earliest mentions of music therapy was in Al-Farabi's (c. 872 – 950) treatise "Meanings of the Intellect", which described the therapeutic effects of music on the soul. Music has long been used to help people deal with their emotions. In the 17th century, the scholar Robert Burton's "The Anatomy of Melancholy" argued that music and dance were critical in treating mental illness, especially melancholia. He noted that music has an "excellent power ...to expel many other diseases" and he called it "a sovereign remedy against despair and melancholy." He pointed out that in Antiquity, Canus, a Rhodian fiddler, used music to "make a melancholy man merry, ...a lover more enamoured, a religious man more devout." In November 2006, Dr. Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients. In the Ottoman Empire, mental illnesses were treated with music.

</doc>
<doc id="18842" url="http://en.wikipedia.org/wiki?curid=18842" title="Mode">
Mode

Mode (Latin "modus", "manner, tune, measure, due measure, rhythm, melody") may refer to:

</doc>
<doc id="18845" url="http://en.wikipedia.org/wiki?curid=18845" title="Mouse">
Mouse

A mouse (plural: mice) is a small mammal belonging to the order of rodents, characteristically having a pointed snout, small rounded ears, a body-length scaly tail and a high breeding rate. The best known mouse species is the common house mouse ("Mus musculus"). It is also a popular pet. In some places, certain kinds of field mice are also common. They are known to invade homes for food and occasionally shelter.
The American white-footed mouse ("Peromyscus leucopus") and the "deer mouse" ("Peromyscus maniculatus"), as well as other common species of mouse-like rodents around the world, also sometimes live in houses. These, however, are in other genera.
Cats, wild dogs, foxes, birds of prey, snakes and even certain kinds of arthropods have been known to prey heavily upon mice. Nevertheless, because of its remarkable adaptability to almost any environment, the mouse is one of the most successful mammalian genera living on Earth today.
Mice can at times be vermin, damaging and eating crops, causing structural damage and spreading diseases through their parasites and feces. In North America, breathing dust that has come in contact with mouse excrement has been linked to hantavirus, which may lead to Hantavirus Pulmonary Syndrome (HPS).
Primarily nocturnal animals, mice compensate for their poor eyesight with a keen sense of hearing, and rely especially on their sense of smell to locate food and avoid predators.
Mice build intricate burrows in the wild. These burrows typically have long entrances and are equipped with escape tunnels/routes. In at least one species, the architectural design of a burrow is a genetic trait.
Reproduction.
Breeding onset is at about 50 days of age in both females and males, although females may have their first estrus at 25–40 days. Mice are polyestrous and breed year round; ovulation is spontaneous. The duration of the estrous cycle is 4–5 days and estrus itself lasts about 12 hours, occurring in the evening. Vaginal smears are useful in timed matings to determine the stage of the estrous cycle. Mating is usually nocturnal and may be confirmed by the presence of a copulatory plug in the vagina up to 24 hours post-copulation. The presence of sperm on a vaginal smear is also a reliable indicator of mating.
Female mice housed together tend to go into anestrus and do not cycle. If exposed to a male mouse or the pheromones of a male mouse, most of the females will go into estrus in about 72 hours. This synchronization of the estrous cycle is known as the Whitten effect. The exposure of a recently bred mouse to the pheromones of a strange male mouse may prevent implantation (or pseudopregnancy), a phenomenon known as the Bruce effect.
The average gestation period is 20 days. A fertile postpartum estrus occurs 14–24 hours following parturition, and simultaneous lactation and gestation prolongs gestation 3–10 days owing to delayed implantation. The average litter size is 10–12 during optimum production, but is highly strain-dependent. As a general rule, inbred mice tend to have longer gestation periods and smaller litters than outbred and hybrid mice. The young are called pups and weigh 0.5 - at birth, are hairless, and have closed eyelids and ears. Cannibalism is uncommon, but females should not be disturbed during parturition and for at least 2 days postpartum. Pups are weaned at 3 weeks of age; weaning weight is 10 -. If the postpartum estrus is not utilized, the female resumes cycling 2–5 days post-weaning.
Newborn male mice are distinguished from newborn females by noting the greater anogenital distance and larger genital papilla in the male. This is best accomplished by lifting the tails of littermates and comparing perineums.
Laboratory mice.
Mice are common experimental animals in laboratory research of biology and psychology fields primarily because they are mammals, and also because they share a high degree of homology with humans. They are the most commonly used mammalian model organism, more common than rats. The mouse genome has been sequenced, and virtually all mouse genes have human homologs. They can also be manipulated in ways that are illegal with humans, although animal rights activitsts often object. A knockout mouse is a genetically engineered mouse that has had one or more of its genes made inoperable through a gene knockout.
Reasons for common selection of mice are small size, inexpensive, widely varied diet, easily maintained, and can reproduce quickly. Several generations of mice can be observed in a relatively short period of time. Mice are generally very docile if raised from birth and given sufficient human contact. However, certain strains have been known to be quite temperamental. Mice and rats have the same organs in the same places, with the difference of size.
Subgenera.
All members of the "Mus" genus are referred to as mice. However, the term "mouse" can also be applied to species outside of this genus. "Mouse" often refers to any small muroid rodent, while "rat" refers to larger muroid rodents. Therefore these terms are not taxonomically specific. For simplicity, only the rodent subgenera belonging to the "Mus" genus are listed here.
Genus Mus - Typical mice
As pets.
Many people buy mice as companion pets. They can be playful, loving and can grow used to being handled. Like pet rats, pet mice should not be left unsupervised outside as they have many natural predators, including (but not limited to) birds, snakes, lizards, cats, and dogs. Male mice tend to have a stronger odor than the females. However, mice are careful groomers and as pets they never need bathing. Well looked-after mice can make ideal pets. Some common mouse care products are:
Diet.
In nature, mice are largely herbivores, consuming any kind of fruit or grain from plants. However, mice adapt well to urban areas and are known for eating almost all types of food scraps. In captivity, mice are commonly fed commercial pelleted mouse diet. These diets are nutritionally complete, but they still need a large variety of vegetables. Food intake is approximately 15 g per 100 g of body weight per day; water intake is approximately 15 ml per 100 g of body weight per day.
As food.
Mice are a staple in the diet of many small carnivores. Humans have eaten mice since prehistoric times and still eat them as a delicacy throughout eastern Zambia and northern Malawi, where they are a seasonal source of protein. Mice are no longer routinely consumed by humans elsewhere.
In various countries mice are used as food for pets such as snakes, lizards, frogs, tarantulas and birds of prey, and many pet stores carry mice for this purpose. Some countries, such as Germany and the United Kingdom, have banned the practice of feeding live mice, citing ethical concerns regarding both predator and prey.
Common terms used to refer to different ages/sizes of mice when sold for pet food are "pinkies", "fuzzies", "crawlers", "hoppers", and "adults". Pinkies are newborn mice that have not yet grown fur; fuzzies have some fur but are not very mobile; hoppers have a full coat of hair and are fully mobile but are smaller than adult mice. Mice without fur are easier for the animal to consume; however, mice with fur may be more convincing as animal feed. These terms are also used to refer to the various growth stages of rats (see Fancy rat).
Use for sense of smell.
Israeli scientists have tested mice as a new form of airport security detector. It consists of three concealed cartridges, each containing eight specially trained mice. If they sense traces of explosives or drugs, they will trigger the alarm. According to the "New Scientist", the mice work four-hour shifts and are more accurate than using dogs or X-ray machines.

</doc>
<doc id="18847" url="http://en.wikipedia.org/wiki?curid=18847" title="Multics">
Multics

Multics ("Multiplexed Information and Computing Service") was an influential early time-sharing operating system. The project was started in 1964 in Cambridge, Massachusetts. The last known running Multics installation was shut down on October 30, 2000, at the Canadian Department of National Defence in Halifax, Nova Scotia, Canada.
Overview.
Initial planning and development for Multics started in 1964. Originally it was a cooperative project led by MIT (Project MAC with Fernando Corbató) along with General Electric and Bell Labs. Bell Labs pulled out in 1969, and in 1970 GE's computer business including Multics was taken over by Honeywell.
Multics was conceived as a commercial product for GE, and became one for Honeywell, but not a very successful one. Due to its many novel and valuable ideas, Multics had a significant impact in the computer field even though it was derided by some critics at the time.
Multics had numerous features intended to result in high availability so that it would support a computing utility similar to the telephone and electricity services. Modular hardware structure and software architecture were used to achieve this. The system could grow in size by simply adding more of the appropriate resource—computing power, main memory, disk storage, etc. Separate access control lists on every file provided flexible information sharing, but complete privacy when needed. Multics had a number of standard mechanisms to allow engineers to analyze the performance of the system, as well as a number of adaptive performance optimization mechanisms.
Novel ideas.
Multics implemented a single level store for data access, discarding the clear distinction between files (called "segments" in Multics) and "process memory". The memory of a process consisted solely of segments which were mapped into its address space. To read or write to them, the process simply used normal CPU instructions, and the operating system took care of making sure that all the modifications were saved to disk. In POSIX terminology, it was as if every file was codice_1ed; however, in Multics there was no concept of "process memory", separate from the memory used to hold mapped-in files, as Unix has. "All" memory in the system was part of "some" segment, which appeared in the file system; this included the temporary scratch memory of the process, its kernel stack, etc.
One disadvantage of this was that the size of segments was limited to 256 kilowords, giving just over 1 MiB. This was due to the particular hardware architecture of the machines on which Multics ran, having a 36-bit word size (not using the 8-bit byte). Extra code had to be used to work on files larger than this, called multisegment files. In the days when one megabyte of memory was prohibitively expensive, and before large databases and later huge bitmap graphics, this limit was rarely encountered.
Another major new idea of Multics was dynamic linking, in which a running process could request that other segments be added to its address space, segments which could contain code that it could then execute. This allowed applications to automatically use the latest version of any external routine they called, since those routines were kept in other segments, which were dynamically linked only when a process first tried to begin execution in them. Since different processes could use different search rules, different users could end up using different versions of external routines automatically. Equally importantly, with the appropriate settings on the Multics security facilities, the code in the other segment could then gain access to data structures maintained in a different process.
Thus, to interact with an application running in part as a daemon (in another process), a user's process simply performed a normal procedure-call instruction, to a code segment to which it had dynamically linked (a code segment which implemented some operation associated with the daemon). The code in that segment could then modify data maintained and used in the daemon. When the action necessary to commence the request was completed, a simple procedure return instruction returned control of the user's process to the user's code.
The single-level store and dynamic linking are still not available to their full power in other widely used operating systems, despite the rapid and enormous advance in the computer field since the 1960s. They are becoming more widely accepted and available in more limited forms, "e.g.", dynamic linking.
Multics also supported extremely aggressive on-line reconfiguration; central processing units, memory banks, disk drives, etc. could be added and removed while the system continued operating. At the MIT system, where most early software development was done, it was common practice to split the multiprocessor system into two separate systems during off-hours by incrementally removing enough components to form a second working system, leaving the rest still running the original logged-in users. System software development testing could be done on the second machine, then the components of the second system were added back onto the main user system, without ever having shut it down. Multics supported multiple CPUs: It was one of the earliest multiprocessor systems.
Multics was the first major operating system to be designed as a secure system from the outset. Despite this, early versions of Multics were broken into repeatedly. This led to further work that made the system much more secure and prefigured modern security engineering techniques. Break-ins became very rare once the second-generation hardware base was adopted; it had hardware support for ring-oriented security, a multilevel refinement of the concept of master mode.
Multics was the first operating system to provide a hierarchical file system, and file names could be of almost arbitrary length and syntax. A given file or directory could have multiple names (typically a long and short form), and symbolic links between directories were also supported. Multics was the first to use the now-standard concept of per-process stacks in the kernel, with a separate stack for each security ring. It was also the first to have a command processor implemented as ordinary user code - an idea later used in the Unix shell (although the details are different, since Multics possessed powerful mechanisms which Unix then lacked). It was also one of the first written in a high level language (Multics PL/1), just after the Burroughs MCP system written in ALGOL.
Project history.
Multics was developed initially for the GE-645 mainframe, a 36-bit system; later, it was supported on the Honeywell 6180 series machines. "With Multics they tried to have a much more versatile and flexible operating system, and it failed miserably," said Dr. Peter H. Salus, author of the definitive history of Unix's early years. This position, however, has been widely discredited in the computing community as many of the technical innovations of the Multics project have found their way into modern commercial computing systems.
Bell Labs pulled out of the project in 1969; some of the people who had worked on it there went on to create the Unix system. Multics development continued at MIT and General Electric.
In 1970, Honeywell bought General Electric's computer division, released a better hardware base, and continued system development until 1985. About 80 multimillion-dollar sites were installed, at universities, industry, and government sites. The French university system had quite a few installations in the early 1980s. After Honeywell stopped supporting Multics, users migrated to other systems, including Unix. The last Multics machine was shut down on October 30, 2000, at the Canadian Department of National Defence.
In 1985, Multics was issued certification as a B2 level secure operating system using the Trusted Computer System Evaluation Criteria from the National Computer Security Center (NCSC) a division of the NSA, the first operating system evaluated to this level.
Multics was distributed from 1975 to 2000 by Groupe Bull in Europe, and by Bull HN Information Systems Inc. in the United States. In 2006, Bull SAS open sourced Multics versions MR10.2, MR11.0, MR12.0, MR12.1, MR12.2, MR12.3, MR12.4 & MR12.5.
Retrospective observations.
The permanently resident kernel of this powerful multiprocessor mainframe "computing utility", much derided in its day as being too large and complex, was only 135 KiB of code. The first MIT GE-645 had 512 kilowords of memory (2 MiB), a truly enormous amount at the time, and the kernel only used a moderate portion of Multics main memory.
The entire system, including the operating system and the complex PL/1 compiler, user commands, and subroutine libraries, consisted of about 1500 source modules. These averaged roughly 200 lines of source code each, and compiled to produce a total of roughly 4.5 MiB of procedure code, which was fairly large by the standards of the day.
Multics compilers generally optimised more for code density than CPU performance, for example using small sub-routines called "operators" for short standard code-sequences, making direct comparison of object code size with more modern systems less useful. High code density was a good optimisation choice for a multi-user system with expensive main memory, such as Multics.
Influence on other projects.
UNIX.
The design and features of Multics greatly influenced the Unix operating system, which was originally written by two ex-programmers from the older project, Ken Thompson and Dennis Ritchie. Superficial influence of Multics on Unix is evident in many areas, including the naming of some commands. But the internal design philosophy was quite different, focusing on keeping the system small and simple, and so correcting some deficiencies of Multics because of its high resource demands on the limited computer hardware of the time. 
The name "Unix" (originally Unics) is itself a pun on "Multics". The "U" in Unix is rumored to stand for "uniplexed" as opposed to the "multiplexed" of Multics, further underscoring the designers' rejections of Multics' complexity in favor of a more straightforward and workable approach for smaller computers. (Garfinkel and Abelson cite an alternative origin: Peter Neumann at Bell Labs, watching a demonstration of the prototype, suggested the name/pun UNICS (pronounced "Eunuchs"), as a "castrated Multics", although Dennis Ritchie is claimed to have denied this.) 
Ken Thompson, in a transcribed 2007 interview with Peter Seibel refers to Multics as "...overdesigned and overbuilt and over everything. It was close to unusable. They (i.e., Massachusetts Institute of Technology) still claim it’s a monstrous success, but it just clearly wasn't." He admits, however, that "the things that I liked enough (about Multics) to actually take were the hierarchical file system and the shell—a separate process that you can replace with some other process."
Other operating systems.
The Prime Computer operating system, PRIMOS, was referred to as "Multics in a shoebox" by William Poduska, a founder of the company. Poduska later moved on to found Apollo Computer, whose AEGIS and later Domain/OS operating systems, sometimes called "Multics in a matchbox", extended the Multics design to a heavily-networked graphics workstation environment.
The Stratus VOS operating system of Stratus Computer (now Stratus Technologies) was very strongly influenced by Multics, and both its external user interface and internal structure bear many close resemblances to the older project. The high-reliability, availability, and security features of Multics were extended in Stratus VOS to support a new line of fault tolerant computer systems supporting secure, reliable transaction processing. Stratus VOS is the most directly-related descendant of Multics still in active development and production usage today.
The protection architecture of Multics, restricting the ability of code at one level of the system to access resources at another, was adopted as the basis for the security features of ICL's VME operating system.
See the "External Links" section of this article for more information about Multics influences on other software and hardware systems.
Further reading.
The literature contains a large number of papers about Multics, and various components of it; a fairly complete list is available here. The most important and/or informative ones are listed below.

</doc>
<doc id="18849" url="http://en.wikipedia.org/wiki?curid=18849" title="Marxist film theory">
Marxist film theory

Marxist film theory is one of the oldest forms of film theory. 
Sergei Eisenstein and many other Soviet filmmakers in the 1920s expressed ideas of Marxism through film. In fact, the Hegelian dialectic was considered best displayed in film editing through the Kuleshov Experiment and the development of montage.
While this structuralist approach to Marxism and filmmaking was used, the more vociferous complaint that the Russian filmmakers had was with the narrative structure of Hollywood filmmaking.
Eisenstein's solution was to shun narrative structure by eliminating the individual protagonist and tell stories where the action is moved by the group and the story is told through a clash of one image against the next (whether in composition, motion, or idea) so that the audience is never lulled into believing that they are watching something that has not been worked over.
Eisenstein himself, however, was accused by the Soviet authorities under Joseph Stalin of "formalist error," of highlighting form as a thing of beauty instead of portraying the worker nobly.
French Marxist film makers, such as Jean-Luc Godard, would employ radical editing and choice of subject matter, as well as subversive parody, to heighten class consciousness and promote Marxist ideas.
Situationist film maker Guy Debord, author of "The Society of the Spectacle", began his film "In girum imus nocte et consumimur igni" [Wandering around in the night we are consumed by fire] with a radical critique of the spectator who goes to the cinema to forget about his dispossessed daily life.
Situationist film makers produced a number of important films, where the only contribution by the situationist film cooperative was the sound-track. In "Can dialectics break bricks?" (1973) a Chinese Kung Fu film was transformed by redubbing into an epistle on state capitalism and Proletarian revolution. The intellectual technique of using capitalism's own structures against itself is known as détournement. 
Marxist film theory has developed from these precise and historical beginnings and is now sometimes viewed in a wider way to refer to any power relationships or structures within a moving image text. 

</doc>
<doc id="18851" url="http://en.wikipedia.org/wiki?curid=18851" title="Mars (disambiguation)">
Mars (disambiguation)

Mars is the fourth planet from the Sun. 
Mars or MARS may also refer to:

</doc>
<doc id="18852" url="http://en.wikipedia.org/wiki?curid=18852" title="Morpheme">
Morpheme

In linguistics, a morpheme is the smallest grammatical unit in a language. In other words, it is the smallest meaningful unit of a language. The field of study dedicated to morphemes is called morphology. A morpheme is not identical to a word, and the principal difference between the two is that a morpheme may or may not stand alone, whereas a word, by definition, is freestanding. When it stands by itself, it is considered a root because it has a meaning of its own (e.g. the morpheme "cat") and when it depends on another morpheme to express an idea, it is an affix because it has a grammatical function (e.g. the "–s" in "cats" to specify that it is plural). Every word comprises one or more morphemes. The more combinations a morpheme is found in, the more productive it is said to be.
Classification of morphemes.
Free and bound Morphemes.
Every morpheme can be classified as either free or bound. These categories are mutually exclusive, and as such, a given morpheme will belong to exactly one of them.
Bound morphemes can be further classified as derivational or inflectional.
Allomorphs.
Allomorphs are variants of a morpheme that differ in pronunciation but are semantically identical. For example, in English, the plural marker "-(e)s" of regular nouns can be pronounced /-z/, /-s/, or /-ɨz/, depending on the final sound of the noun's singular form.
Content vs. function.
Content morphemes express a concrete meaning or "content", while function morphemes have more of a grammatical role. For example, the morphemes "fast" and "sad" can be considered content morphemes. On the other hand, the suffix "–ed" belongs to the function morphemes given that it has the grammatical function of indicating past tense. Although these categories seem very clear and intuitive, the idea behind it can be harder to grasp given that they overlap with each other. Examples of an ambiguous situation are the preposition "over" and the determiner "your", which seem to have a concrete meaning, but are considered function morphemes because their role is to connect ideas grammatically. A general rule to follow to determine the category of a morpheme is:
Additional notes.
First, roots are composed of only one morpheme while stems can be composed of more than one morpheme. Also, any additional affixes are considered morphemes. An example of this is the word "quirkiness". The root is "quirk", but the stem is "quirky" which has two morphemes. Second, another thing to take in consideration is that there might be affixes that have the same phonological form, but have different meaning. For example, the suffix "–er" can be derivative (e.g. "wonder" ⇒ "wonderer") or inflectional (e.g. "small" ⇒ "smaller"). These types of morphemes are called homophonous.
A final factor to keep in consideration is to not be confused by monomorphemic words, which contain only one morpheme. For instance, some words might seem to be composed of multiple morphemes, but in fact they are not. This is why we have to consider form and meaning when searching for morphemes. For example, we might think that the word "relate" is composed of two morphemes, "re-" (prefix) and the word "late", but this is not correct. It has no relationship with the definitions relevant to the word like “feel sympathy”, “narrate”, or “being connected by blood or marriage”. Furthermore, the length of the words does not determine if it has multiple morphemes or not. To demonstrate, the word "Madagascar" is long and it might seem to have morphemes like "mad", "gas", and "car", but it does not. Conversely, small words can have multiple morphemes (e.g. "dogs").
Morphological analysis.
In natural language processing for Korean, Japanese, Chinese and other languages, morphological analysis is the process of segmenting a sentence into a row of morphemes. Morphological analysis is closely related to part-of-speech tagging, but word segmentation is required for these languages because word boundaries are not indicated by blank spaces.
The purpose of morphological analysis is to determine the minimal units of meaning in a language or morphemes by using comparisons of similar forms. For example, comparing forms such as “She is walking” and “They are walking” rather than comparing any of the previous sentences with something completely different like “You are reading”. Thus, we can effectively break down the forms in parts and distinguishing the different morphemes. Similarly, keep in mind that the meaning and the form are equally important during the identification of morphemes. For instance, agent and comparative morphemes illustrate this point. An agent morpheme is an affix like "-er" that transforms a verb into a noun (e.g. "teach" ⇒ "teacher"). On the other hand, "–er" can also be a comparative morpheme that changes an adjective into another degree of the same adjective (e.g. "small" ⇒ "smaller"). In this case, the form is the same, but the meaning of both morphemes is different. Also, the opposite can occur in which the meaning is the same but the form is different.
Changing definitions of morpheme.
In generative grammar, the definition of a morpheme depends heavily on whether syntactic trees have morphemes as leaves or features as leaves.
Given the definition of morpheme as "the smallest meaningful unit" Nanosyntax aims to account for idioms where it is often an entire syntactic tree which contributes "the smallest meaningful unit." An example idiom is "Don't let the cat out of the bag" where the idiom is composed of "let the cat out of the bag" and that might be considered a semantic morpheme, which is composed of many syntactic morphemes. Other cases where the "smallest meaningful unit" is larger than a word include some collocations such as "in view of" and "business intelligence" where the words together have a specific meaning.
The definition of morphemes also plays a significant role in the interfaces of generative grammar in the following theoretical constructs;
References.
</dl>

</doc>
<doc id="18856" url="http://en.wikipedia.org/wiki?curid=18856" title="MTV">
MTV

MTV (formerly an initialism of Music Television) is an American basic cable and satellite television channel owned by the Viacom Media Networks Music & Logo Group, a unit of the Viacom Media Networks division of Viacom. The channel itself is headquartered in New York City, and is a subsidiary of Viacom Inc. Launched on August 1, 1981, the original purpose of the channel was to play music videos guided by television personalities known as "video jockeys," or VJs. In its early years, MTV's main target demographic were young adults, but today, MTV's programming is primarily targeted at adolescents and teenagers.
MTV has spawned numerous sister channels in the U.S. and affiliated channels internationally, some of which have gone independent. MTV's influence on its audience, including issues related to censorship and social activism, has been a subject of debate for years.
As of February 2015, approximately 93,240,000 American households (80.1% of households with television) received MTV.
Launch.
Previous concepts (1964–77).
Several concepts for music video television programming had been around since the early 1960s.
The Beatles had used music videos to promote their records starting in the mid-1960s. The creative use of music videos within their 1964 debut film "A Hard Day's Night", particularly the performance of "Can't Buy Me Love", led MTV decades later to honor the film's director, Richard Lester, with an award for "basically inventing" the music video.
In his book "The Mason Williams FCC Rapport", author Mason Williams states that he pitched an idea to CBS for a television program that featured "video-radio," where disc jockeys would play avant-garde art pieces set to music on the air. CBS cancelled the idea, but Williams premiered his own musical composition, "Classical Gas", on the "Smothers Brothers Comedy Hour", where he was head writer.
In 1970, Philadelphia-based disk jockey Bob Whitney created "The Now Explosion", a television series filmed in Atlanta and broadcast in syndication to other local television stations throughout the United States. The series, which featured promotional clips from various popular artists, was canceled by its distributor in 1971.
Several music programs originating outside the US, including Australia's "Countdown" and the UK's "Top of the Pops", which had initially aired music videos in lieu of performances from artists who were not available to perform live, began to feature them regularly by the mid-1970s.
In 1974, Gary Van Haas, vice president of Televak Corporation, introduced a concept to distribute a music video channel to record stores across the United States, and promoted the channel, Music Video TV, to distributors and retailers in a May 1974 issue of "Billboard". The channel, which featured video disc jockeys, signed a deal with US Cable in 1978 to expand its audience from retail to cable television. The service was no longer active by the time MTV launched in 1981.
Pre-history (1977–81).
MTV's pre-history began in 1977, when Warner Cable (a division of Warner Communications from Warner Bros.), and an ancestor of Warner-Amex Satellite Entertainment (WASEC) launched the first two-way interactive cable television system, QUBE, in Columbus, Ohio. The QUBE system offered many specialized channels. One of these specialized channels was Sight on Sound, a music channel that featured concert footage and music-oriented television programs; with the interactive QUBE service, viewers could vote for their favorite songs and artists.
The original programming format of MTV was created by media executive Robert W. Pittman, who later became president and chief executive officer (CEO) of MTV Networks. Pittman had test-driven the music format by producing and hosting a 15-minute show, "Album Tracks", on New York City television station WNBC in the late 1970s.
Pittman's boss, WASEC executive vice president John Lack, had shepherded "PopClips", a television series created by former Monkee-turned solo artist Michael Nesmith, whose attention had turned to the music video format by the late 1970s. The inspiration for "PopClips" came from a similar program on New Zealand's TVNZ network, "Radio with Pictures", which premiered in 1976. The concept itself had been in the works since 1966, when major record companies began supplying the New Zealand Broadcasting Corporation with promotional music clips to play on the air at no charge (few artists made the long trip to New Zealand to appear live).
Music Television debuts.
On Saturday, August 1, 1981, at 12:01 a.m. Eastern Time, MTV launched with the words "Ladies and gentlemen, rock and roll," spoken by John Lack, and played over footage of the first Space Shuttle launch countdown of "Columbia", which took place earlier that year, and of the launch of Apollo 11. Those words were immediately followed by the original MTV theme song, a crunching rock tune composed by Jonathan Elias and John Petersen, playing over photos of the Apollo 11 moon landing, with the flag featuring MTV's logo changing various colors, textures, and designs. MTV producers Alan Goodman and Fred Seibert used this public domain footage as a conceit. Seibert said they had originally planned to use Neil Armstrong's "One small step" quote, but lawyers said Armstrong owns his name and likeness, and Armstrong had refused, so the quote was replaced with a beeping sound. The shuttle launch identification ran at the top of every hour in various forms from MTV's first day until it was pulled in early 1986, in the wake of the "Challenger" disaster.
The first music video shown on MTV was The Buggles' "Video Killed the Radio Star". This was followed by the video for Pat Benatar's "You Better Run". Sporadically, the screen would go black when an employee at MTV inserted a tape into a VCR. MTV's lower third graphics that appear near the beginning and end of music videos would eventually use the recognizable Kabel typeface for about 25 years, but these graphics differed on MTV's first day of broadcast; they were set in a different typeface and included record label information such as the year and label name.
As programming chief, Robert W. Pittman recruited and managed a team for the launch that included Tom Freston (who succeeded Pittman as CEO of MTV Networks), Fred Seibert, John Sykes, Carolyn Baker (original head of talent and acquisition), Marshall Cohen (original head of research), Gail Sparrow (of talent and acquisition), Sue Steinberg (executive producer), Julian Goldberg, Steve Lawrence, Geoff Bolton; studio producers and MTV News writers/associate producers Liz Nealon, Nancy LaPook and Robin Zorn; Steve Casey (creator of the name "MTV" and its first program director), Marcy Brahman, Ronald E. "Buzz" Brindle, and Robert Morton. Kenneth M. Miller is credited as being the first technical director to officially launch MTV from its New York City-based network operations facility.
MTV's effect was immediate in areas where the new music video channel was carried. Within two months, record stores in areas where MTV was available were selling music that local radio stations were not playing, such as Men at Work, Bow Wow Wow and the Human League. MTV sparked the Second British Invasion, with British acts, who had been accustomed to using music videos for half a decade, featuring heavily on the channel.
Subsequent concepts (1981–92).
HBO also had a 30-minute program of music videos, called "Video Jukebox", that first aired around the time of MTV's launch and would last until late 1986. Also around this time, HBO, as well as other premium channels such as Cinemax, Showtime and The Movie Channel, would occasionally play one or a few music videos between movies.
SuperStation WTBS launched "Night Tracks" on June 3, 1983, with up to 14 hours of music video airplay each late night weekend by 1985. Its most noticeable difference was that black artists received airplay that MTV initially ignored. The program ran until the end of May 1992.
A few markets also launched music-only channels including Las Vegas' KVMY (channel 21), which debuted in the summer of 1984 as KRLR-TV and branded as "Vusic 21". The first video played on that channel was "Video Killed the Radio Star", following in the footsteps of MTV.
Shortly after TBS began "Night Tracks", NBC launched a music video program called "Friday Night Videos", which was considered network television's answer to MTV. Later renamed simply "Friday Night", the program ran from 1983 to 2002. ABC's contribution to the music video program genre in 1984, "ABC Rocks", was far less successful, lasting only a year.
TBS founder Ted Turner started the Cable Music Channel in 1984, designed to play a broader mix of music videos than MTV's rock format allowed. But after one month as a money-losing venture, Turner sold it to MTV, who redeveloped the channel into VH1.
Shortly after its launch, The Disney Channel aired a program called "D-TV", a play on the MTV acronym. The program used music cuts, both from current and past artists. Instead of music videos, the program used clips of various vintage Disney cartoons and animated films to go with the songs. The program aired in multiple formats, sometimes between shows, sometimes as its own program, and other times as one-off specials. The specials tended to air both on The Disney Channel and NBC. The program aired at various times between 1984 and 1999. In 2009, Disney Channel revived the "D-TV" concept with a new series of short-form segments called "Re-Micks".
Music videos.
The original purpose of MTV was to be "music television", playing music videos 24 hours a day, seven days a week, guided by on-air personalities known as VJs, or video jockeys. The original taglines of the channel were "You'll never look at music the same way again," and "On cable. In stereo." Although the original MTV channel no longer plays music videos 24/7, several of its spin-off channels do, including MTV Hits and MTV Jams. In addition, viewers can play music videos on-demand at MTV.com. MTV continues to support a broad selection of music videos on its international channels as well.
Original VJs and format (1980–94).
MTV's earliest format was modeled after AOR (album-oriented rock) radio; MTV would transition to mimic a full top 40 station in 1984. Fresh-faced young men and women were hired to host the channel's programming and to introduce music videos that were being played. The term VJ (video jockey) was coined, which was a play on the initialism DJ (disc jockey). Many VJs eventually became celebrities in their own right. The original five MTV VJs in 1981 were Nina Blackwood, Mark Goodman, Alan Hunter, J.J. Jackson and Martha Quinn.
The VJs would record "intro" and "outro" segments to music videos, along with music news, interviews, concert dates and promotions. These segments would appear to air "live" and debut across the MTV program schedule 24 hours a day, seven days a week, though the segments themselves were pre-taped within a regular work week at MTV's studios.
The early music videos that made up the bulk of MTV's programming in the 1980s were promotional videos (or "promos", a term that originated in the United Kingdom) that record companies had commissioned for international use or concert clips from any available sources.
Rock bands and performers of the 1980s who had airplay on MTV ranged from new wave to hard rock or heavy metal bands such as Adam Ant, Bryan Adams, Blondie, Eurythmics, Culture Club, Mötley Crüe, Split Enz, Prince, Ultravox, Duran Duran, Van Halen, Bon Jovi, RATT, Def Leppard, The Police, and The Cars. The channel also rotated the music videos of "Weird Al" Yankovic, who made a career out of parodying other artists' videos. MTV also aired several specials by "Weird Al" in the 1980s and 1990s under the title "Al TV".
MTV also played classic rock acts from the 1980s and earlier decades, including David Bowie, Dire Straits (whose 1985 song and video "Money for Nothing" both referenced MTV and also included the slogan "I want my MTV" in its lyrics), Journey, Rush, Linda Ronstadt, Genesis, Billy Squier, Aerosmith, The Rolling Stones, The Moody Blues, John Mellencamp, Billy Joel, Robert Palmer, Rod Stewart, The Who, and ZZ Top; newly solo acts such as Peter Gabriel, Robert Plant, Phil Collins, Paul McCartney, David Lee Roth, and Pete Townshend; supergroup acts such as Asia, The Power Station, Yes, The Firm, and Traveling Wilburys, as well as forgotten acts such as Michael Stanley Band, Shoes, Blotto, and Taxxi. The hard rock band Kiss publicly appeared without their trademark makeup for the first time on MTV in 1983.
During the early days of the channel, MTV would occasionally let other stars take over the channel within an hour as "guest VJs"; these guests included musicians such as Adam Ant, Billy Idol, Phil Collins, Simon LeBon, and Nick Rhodes of Duran Duran, Tina Turner; and comedians such as Eddie Murphy, Martin Short, Dan Aykroyd, and Steven Wright; as they chose their favorite music videos.
The 1983 film "Flashdance" was the first film in which its promoters excerpted musical segments from it and supplied them to MTV as music videos, which the channel then aired in regular rotation.
In addition to bringing lesser-known artists into view, MTV was instrumental in adding to the booming eighties dance wave. Videos' budgets increased, and artists began to add fully choreographed dance sections. Michael Jackson's music became synonymous with dance. In addition to learning the lyrics, fans also learned his choreography so they could dance along. Madonna capitalized on dance in her videos, using classically trained jazz and break-dancers. Along with extensive costuming and make-up, Duran Duran used tribal elements, pulled from Dunham technique, in "Wild Boys", and Kate Bush used a modern dance duet in "Running Up That Hill". MTV brought more than music into public view, it added to the ever-growing resurgence of dance in the early 1980s that has carried through to today.
In 1984 more record companies and artists began making video clips for their music than in the past, realizing the popularity of MTV and the growing medium. In keeping with the influx of videos, MTV announced changes to its playlists in the November 3, 1984 issue of Billboard Magazine that would take effect the following week. The playlist categories would be expanded to 7, from 3 (light, medium, heavy); including New, Light, Breakout, Medium, Active, Heavy and Power. This would ensure artists with hit records on the charts would be get the exposure they deserved, with Medium being a home for the established hits still on the climb up to the top 10; and Heavy being a home for the big hits – without the bells and whistle – just the exposure they commanded.
Breaking the "color barrier" (1981–83).
During MTV's first few years on the air, very few black artists were included in rotation on the channel. The select few who were in MTV's rotation were Prince, Eddy Grant, Donna Summer, Musical Youth, and Herbie Hancock. The very first non-white act played on MTV was the British band The Specials, which featured an integrated line-up of white and black musicians and vocalists. The Specials' video "Rat Race" was played as the 58th video on the station's first day of broadcasting.
MTV rejected other black artists' videos, such as Rick James' "Super Freak", because they didn't fit the channel's carefully selected AOR format at the time. The exclusion enraged James; he publicly advocated the addition of more black artists' videos on the channel. Rock legend David Bowie also questioned MTV's 'lack of negro artists' during an on-air interview with VJ Mark Goodman in 1983. MTV's original head of talent and acquisition, Carolyn B. Baker, who was black, had questioned why the definition of music had to be so narrow, as had a few others outside the network. "The party line at MTV was that we weren't playing black music because of the "research"," said Baker years later. "But the research was based on ignorance... we were young, we were cutting edge. We didn't have to be on the cutting edge of racism." Nevertheless, it was Baker who had personally rejected Rick James' video for "Super Freak" "because there were half-naked women in it, and it was a piece of crap. As a black woman, I did not want that representing my people as the first black video on MTV."
The network's director of music programming Buzz Brindle told an interviewer in 2006, "MTV was originally designed to be a rock music channel. It was difficult for MTV to find African American artists whose music fit the channel's format that leaned toward rock at the outset." Writers Craig Marks and Rob Tannenbaum noted that the channel "aired videos by plenty of white artists who didn't play rock." Andrew Goodwin later wrote, "[MTV] denied racism, on the grounds that it merely followed the rules of the rock business (which were, nonetheless, the consequence of a long history of racism)." MTV senior executive vice president Les Garland complained decades later, "The worst thing was that "racism" bullshit... there were hardly any videos being made by black artists. Record companies weren't funding them. "They" never got charged with racism."
Before 1983, Michael Jackson also struggled to receive airtime on MTV. To resolve the struggle and finally "break the color barrier," the president of CBS Records at the time, Walter Yetnikoff, denounced MTV in a strong, profane statement, threatening to take away MTV's ability to play any of the record label's music videos. However, Les Garland, then acquisitions head, said he decided to air Jackson's "Billie Jean" video without pressure from CBS. This was contradicted by CBS head of Business Affairs David Benjamin in Vanity Fair.
According to "The Austin Chronicle", Jackson's video for the song "Billie Jean" was "the video that broke the color barrier, even though the channel itself was responsible for erecting that barrier in the first place." But change was not immediate. "Billie Jean" was not added to MTV's "medium rotation" playlist (two to three airings per day) until after it had already reached #1 on the "Billboard" Hot 100 chart. A month later, it was bumped up into "heavy rotation," one week before the MTV debut of Jackson's "Beat It" video. Both videos were played several times a day for the next two months; by early summer, the channel had ceased playing either song. But the impact was permanent as by that point the videos by other black artists such as "Little Red Corvette" and "1999" by Prince and "She Works Hard for the Money" by Donna Summer were in heavy rotation on the channel . When Jackson's elaborate video for "Thriller" was released late in the year, which raised the ambition bar for what a video could be, the network's support for it was total; subsequently more pop and R&B videos were played on MTV.
Eventually, videos from the emerging genre of rap and hip hop would also begin to enter rotation on MTV. A majority of the rap artists appearing on MTV in the mid-1980s, such as Run-DMC, The Fat Boys, Whodini, L.L. Cool J, and the Beastie Boys, were from the East Coast.
Video director Don Letts has a different view of the timeline, saying, "People often say "Billie Jean" was the first black music video on MTV. "Pass the Dutchie" was first. Because they were little and spoke in funny British accents, Musical Youth were deemed as non-threatening, and therefore non-black."
Regardless of the timeline, many black artists had their videos played in "heavy" rotation the following year (1984). Along with Prince, Michael Jackson and Donna Summer, other black artists such as Billy Ocean, Tina Turner, Stevie Wonder, Chaka Khan, Lionel Ritchie, Ray Parker Jr, Rockwell, The Pointer Sisters, Kool and the Gang, Diana Ross, Shannon, and Deniece Williams all had videos played in heavy rotation on MTV.
"Video Music Awards" (1984–present).
In 1984, the channel produced its first "MTV Video Music Awards" show, or VMAs. The first award show, in 1984, was punctuated by a live performance by Madonna of "Like A Virgin". The statuettes that are handed out at the "Video Music Awards" are of the MTV moonman, the channel's original image from its first broadcast in 1981. Presently, the "Video Music Awards" are MTV's most watched annual event.
Special, annual events (1986–present).
MTV began its annual "Spring Break" coverage in 1986, setting up temporary operations in Daytona Beach, Florida, for a week in March, broadcasting live eight hours per day. "Spring break is a youth culture event," MTV's vice president Doug Herzog said at the time. "We wanted to be part of it for that reason. It makes good sense for us to come down and go live from the center of it, because obviously the people there are the kinds of people who watch MTV." The channel's coverage featured numerous live performances from artists and bands on location. The annual tradition would continue into the 2000s, when it would become de-emphasized and handed off to mtvU, the spin-off channel of MTV targeted at college campuses.
The channel would later expand its beach-themed events to the summer, dedicating most of each summer season to broadcasting live from a beach house at various locations away from New York City, eventually leading to channel-wide branding throughout the summer in the 1990s and early 2000s such as "Motel California", "Summer Share", "Isle of MTV", "SoCal Summer", "Summer in the Keys", and "Shore Thing". MTV VJs would host blocks of music videos, interview artists and bands, and introduce live performances and other programs from the beach house location each summer. In the 2000s, as the channel reduced its airtime for music videos and eliminated much of its in-house programming, its annual summer-long events came to an end.
MTV would also hold week-long music events that would take over the presentation of the channel. Examples from the 1990s and 2000s include "All Access Week", a week in the summer dedicated to live concerts and festivals; "Spankin' New Music Week", a week in the fall dedicated to brand new music videos; and week-long specials that culminated in a particular live event, such as "Wanna be a VJ" and the "Video Music Awards".
At the end of each year, MTV takes advantage of its home location in New York City to broadcast live coverage on New Year's Eve in Times Square. Several live music performances are featured alongside interviews with artists and bands that were influential throughout the year. For many years from the 1980s to the 2000s, the channel upheld a tradition of having a band perform a cover song at midnight immediately following the beginning of the new year.
Live concert broadcasts (1985–2005).
Throughout its history, MTV has covered global benefit concert series live. For most of July 13, 1985, MTV showed the Live Aid concerts, held in London and Philadelphia and organized by Bob Geldof and Midge Ure to raise funds for famine relief in Ethiopia. While the ABC network showed only selected highlights during primetime, MTV broadcast 16 hours of coverage.
Along with VH1, MTV broadcast the Live 8 concerts, a series of concerts set in the G8 states and South Africa, on July 2, 2005. Live 8 preceded the 31st G8 summit and the 20th anniversary of Live Aid. MTV drew heavy criticism for its coverage of Live 8. The network cut to commercials, VJ commentary, or other performances during performances. Complaints surfaced on the internet over MTV interrupting the reunion of Pink Floyd. In response, MTV president Van Toeffler stated that he wanted to broadcast highlights from every venue of Live 8 on MTV and VH1, and clarified that network hosts talked over performances only in transition to commercials, informative segments or other musical performances. Toeffler acknowledged that "MTV should not have placed such a high priority on showing so many acts, at the expense of airing complete sets by key artists." He also blamed the Pink Floyd interruption on a mandatory cable affiliate break. MTV averaged 1.4 million viewers for its original July 2 broadcast of Live 8. Consequently, MTV and VH1 aired five hours of uninterrupted Live 8 coverage on July 9, with each channel airing different blocks of artists.
Formatted music series (1986–2008).
MTV introduced "120 Minutes" in 1986, a show that would feature low-rotation, alternative rock and other "underground" videos for the next 14 years on MTV and three additional years on sister channel MTV2. The program then became known as "Subterranean" on MTV2. Eight years later, on July 31, 2011, "120 Minutes" was resurrected with Matt Pinfield taking over hosting duties once again and airing monthly on MTV2.
Another late night music video show was added in 1987, "Headbangers Ball", which featured heavy metal music and news. Before its abrupt cancellation in 1995, it featured several hosts including Riki Rachtman and Adam Curry. A weekly block of music videos with the name "Headbangers Ball" aired from 2003 to 2011 on sister channel MTV2, before spending an additional two years as a web-only series on MTV2's website, until "Headbangers Ball" was discontinued once again in 2013.
In 1988, MTV debuted "Yo! MTV Raps", a hip hop/rap formatted program. The program continued until August 1995. It was renamed to simply "Yo!" and aired as a one-hour program from 1995 to 1999. The concept was reintroduced as "Direct Effect" in 2000, which became "Sucker Free" in 2006 and was cancelled in 2008, after briefly celebrating the 20th anniversary of "Yo! MTV Raps" throughout the months of April and May 2008. Despite its cancellation on MTV, a weekly countdown of hip hop videos known as "Sucker Free" still airs on MTV2 through the present day.
By the end of the 1980s, the channel had debuted "Dial MTV", a daily top ten music video countdown show for which viewers could call the toll-free telephone number 1-800-DIAL-MTV to request a music video. Although "Dial MTV" ended in the 1990s, the phone number remained in use for video requests until 2006.
In 1989, MTV began to premiere music-based specials such as "MTV Unplugged", an acoustic performance show, which has featured dozens of acts as its guests and has remained active in numerous iterations on various platforms for over 20 years.
Rise of the directors (1990–93).
By the early 1990s, MTV was playing a combination of pop-friendly hard rock acts, chart-topping metal and hard rock acts such as Metallica, Nirvana and Guns N' Roses, pop singers such as Michael Jackson, Madonna, 2 Unlimited, and New Kids on the Block, and R&B quartets such as New Edition, Bell Biv Devoe, Tony Toni Tone, and Boyz II Men, while introducing hit rappers Vanilla Ice and MC Hammer. MTV progressively increased its airing of hip hop acts, such as LL Cool J, Naughty By Nature, Onyx, and Sir-Mix-A-Lot, and by 1993, the channel added West Coast rappers previously associated with gangsta rap, with a less pop-friendly sound, such as Tupac Shakur, Ice Cube, Warren G, Dr. Dre, and Snoop Doggy Dogg.
To accompany the new sounds, a new form of music videos came about: more creative, funny, artistic, experimental, and technically accomplished than those in the 1980s. Several noted film directors got their start creating music videos. After pressure from the Music Video Production Association, MTV began listing the names of the videos' directors at the bottom of the credits by December 1992. As a result, MTV's viewers became familiar with the names of Spike Jonze, Michel Gondry, David Fincher, Mary Lambert, Samuel Bayer, Matt Mahurin, Mark Romanek, Jonathan Dayton and Valerie Faris, Anton Corbijn, Mark Pellington, Tarsem, Hype Williams, Jake Scott, Jonathan Glazer, Marcus Nispel, F. Gary Gray, Jim Yukich, Russell Mulcahy, Steve Barron, and Marty Callner, among others.
As the PBS series "Frontline" explored, MTV was a driving force that catapulted music videos to a mainstream audience, turning music videos into an art form as well as a marketing machine that became beneficial to artists.Danny Goldberg, chairman and CEO of Artemis Records, said the following about the art of music videos: "I know when I worked with Nirvana, Kurt Cobain cared as much about the videos as he did about the records. He wrote the scripts for them, he was in the editing room, and they were part of his art. And I think they stand up as part of his art, and I think that's true of the great artists today. Not every artist is a great artist and not every video is a good video, but in general having it available as a tool, to me, adds to the business. And I wish there had been music videos in the heyday of The Beatles, and The Rolling Stones. I think they would've added to their creative contribution, not subtracted from it."
Alternative is mainstream (1991–97).
Nirvana led a sweeping transition into the rise of alternative rock music on MTV in 1991 with their video for "Smells Like Teen Spirit". By late 1991 going into 1992, MTV began frequently airing videos from their heavily promoted "Buzz Bin", such as Nirvana, Pearl Jam, Alice in Chains, Soundgarden, Nine Inch Nails, Tori Amos, PM Dawn, Arrested Development, Björk, and Gin Blossoms. MTV increased rotation of its weekly alternative music program "120 Minutes" and added the daily "Alternative Nation" to play videos of these and other underground music acts. Subsequently, grunge and alternative rock had a rise in mainstream tastes, while 1980s-style glam bands and traditional rockers were phased out, with some exceptions such as Aerosmith and Tom Petty. Older acts such as R.E.M. and U2 remained relevant by making their music more experimental or unexpected.
In 1993, more hit alternative rock acts were on heavy rotation, such as Stone Temple Pilots, Soul Asylum, Rage Against the Machine, Tool, Beck, Therapy?, Radiohead, and The Smashing Pumpkins. Other hit acts such as Weezer, Collective Soul, Blind Melon, The Cranberries, Bush, and Silverchair would follow in the next couple of years. Alternative bands that appeared on "Beavis and Butt-head" included White Zombie.
By the next few years, 1994 through 1997, MTV began promoting new power pop acts, most successfully Green Day and The Offspring, and ska-rock acts such as No Doubt, The Mighty Mighty Bosstones and Sublime. Pop singers were added to the rotation with success as long as they were considered "alternative," such as Alanis Morissette, Jewel, Fiona Apple, and Sarah McLachlan.
Electronica and pop (1997–99).
By 1997, MTV focused heavily on introducing electronica acts into the mainstream, adding them to its musical rotation, including The Prodigy, The Chemical Brothers, Moby, Aphex Twin, Daft Punk, The Crystal Method, and Fatboy Slim. Some musicians who proceeded to experiment with electronica were still played on MTV including Madonna, U2, David Bowie, Radiohead, and Smashing Pumpkins. That year, MTV also attempted to introduce neo-swing bands, but they did not meet with much success.
However, in late 1997, MTV began shifting more progressively towards pop music, inspired by the success of the Spice Girls and the rise of boy bands in Europe. Between 1998 and 1999, MTV's musical content consisted heavily of videos of boy bands such as Backstreet Boys and 'N Sync as well as teen pop "princesses" such as Britney Spears, Christina Aguilera, Lynda Thomas, Mandy Moore, and Jessica Simpson. Airplay of rock, electronica, and alternative acts was reduced. Hip-hop music continued in heavy rotation, through the likes of Puff Daddy, Master P, DMX, Busta Rhymes, Jay-Z, Missy Elliott, Eminem, Ja Rule, Nas, and their associates. R&B was also heavily represented with acts such as Aaliyah ,Destiny's Child and Brandy.
"Return of the Rock" (1997–2004).
Beginning in late 1997, MTV progressively reduced its airing of rock music videos, leading to the slogan among skeptics, "Rock is dead." The facts that at the time rock music fans were less materialistic, and bought less music based on television suggestion, were cited as reasons that MTV abandoned its once staple music. MTV instead devoted its musical airtime mostly to pop and hip hop/R&B music. All rock-centric shows were eliminated and the rock-related categories of the "Video Music Awards" were pared down to one.
From this time until 2004, MTV made some periodic efforts to reintroduce pop rock music videos to the channel. By 1998 through 1999, the punk-rock band Blink-182 received regular airtime on MTV due in large part to their "All the Small Things" video that made fun of the boy bands that MTV was airing at the time. Meanwhile, some rock bands that were not receiving MTV support, such as Korn and Creed, continued to sell albums. Then, upon the release of Korn's rock/rap hybrid album "Follow the Leader", MTV began playing Korn's videos "Got the Life" and "Freak on a Leash".
A band sponsored by Korn, Limp Bizkit, received airtime for its cover of George Michael's "Faith", which became a hit. Subsequently, MTV began airing more rap/rock hybrid acts, such as Limp Bizkit and Kid Rock. Some rock acts with more comical videos, such as Rob Zombie, Red Hot Chili Peppers and Foo Fighters, also received airtime.
In the fall of 1999, MTV announced a special "Return of the Rock" weekend, in which new rock acts received airtime, after which a compilation album was released. System of a Down, Staind, Godsmack, Green Day, Incubus, Papa Roach, P.O.D., Sevendust, Powerman 5000, Slipknot, Kittie, Static X, and CKY were among the featured bands. These bands received some airtime on MTV and more so on MTV2, though both channels gave emphasis to the rock/rap acts.
By 2000, Sum 41, Linkin Park, Jimmy Eat World, Mudvayne, Cold, At the Drive-In, Alien Ant Farm, and other acts were added to the musical rotation. MTV also launched digital cable channel MTVX to play rock music videos exclusively, an experiment that lasted until 2002. A daily music video program on MTV that carried the name "Return of the Rock" ran through early 2001, replaced by a successor, "All Things Rock", from 2002 until 2004.
"Total Request Live" (1998–2008).
Also by 1997, MTV was criticized heavily for not playing as many music videos as it had in the past. In response, MTV created four shows that centered around music videos: "MTV Live", "Total Request", "Say What?", and "12 Angry Viewers". Also at this time, MTV introduced its new studios in Times Square.
A year later, in 1998, MTV merged "Total Request" and "MTV Live" into a live daily top ten countdown show, "Total Request Live", which would become known as "TRL" (the original host being Carson Daly) and secure its place as the channel's unofficial flagship program. In the fall of 1999, a live studio audience was added to the show. By spring 2000, the countdown reached its peak. The program enjoyed success playing the top ten pop, rock, R&B, and hip hop music videos, and featuring live interviews with artists and celebrities.
From 1998 to 2001, MTV also aired several other music video programs from its studios in Times Square and on location at various beach-themed locations each summer. These programs included "Say What? Karaoke", a game show hosted by Dave Holmes that evolved from "Say What?", MTV's earlier program that ran the lyrics of music videos across the screen. "TRL Wannabes" aired from 1999 to 2000 and featured a selection of music videos that just missed the "TRL" top ten. "VJ for a Day", hosted by Raymond Munns, continued this concept in early 2001. "VJ for a Day" was an extension of an annual event, "Wanna be a VJ", which aired each spring from 1998 to 2000 to select a new VJ to host programs on MTV.
MTV also aired "Hot Zone", hosted by Ananda Lewis, which featured pop music videos during the midday time period and was a casual alternative to "TRL"; it later became "MTV Hits". Other programs were "Direct Effect", "Return of the Rock", "MTV Jams", "BeatSuite", "MTV Soul", and blocks of music videos hosted by VJs simply called "Music Television" in the spirit of the channel's original purpose.
During the September 11, 2001 terror attacks on the World Trade Center and The Pentagon, MTV suspended all of its programming, along with its sister cable channel VH1, and it began simulcasting coverage from CBS News (the news division of CBS, which was acquired by MTV parent Viacom two years earlier) until about 11:00 pm. ET that night. The channels then played a looped set of music videos without commercial interruption until an MTV News special edition of "TRL" aired on September 14, 2001.
In 2002, Carson Daly left MTV and "TRL" to pursue a late night talk show on NBC. After his departure, the relevance and impact of "Total Request Live" slowly diminished. "TRL" ultimately remained a part of MTV's regular program schedule for ten years. The series came to an end with a special finale episode, "Total Finale Live", which aired November 16, 2008, and featured all the show's hosts from over the years, many special guests from the history of the show, and played its last music video, "...Baby One More Time" by Britney Spears.
Milestones and specials (1999–2011).
Around 1999 through 2001, as MTV aired fewer music videos throughout the day, it regularly aired compilation specials from its then 20-year history to look back on its roots. An all-encompassing special, "MTV Uncensored", premiered in 1999 and was later released as a book.
MTV celebrated its 20th anniversary on August 1, 2001, beginning with a 12-hour retrospective called "MTV20: Buggles to Bizkit", which featured over 100 classic videos played chronologically, hosted by various VJs in reproductions of MTV's old studios. The day of programming culminated in a three-hour celebratory live event called "MTV20: Live and Almost Legal", which was hosted by Carson Daly and featured numerous guests from MTV's history, including the original VJs from 1981. Various other related "MTV20" specials aired in the months surrounding the event.
Janet Jackson became the inaugural honoree of the "mtvICON" award, "an annual recognition of artists who have made significant contributions to music, music video and pop culture while tremendously impacting the MTV generation." Subsequent recipients included Aerosmith, Metallica and The Cure.
Five years later, on August 1, 2006, MTV celebrated its 25th anniversary. On their website, MTV.com, visitors could watch the very first hour of MTV, including airing the original promos and commercials from Mountain Dew, Atari, Chewels gum, and Jovan. Videos were also shown from The Buggles, Pat Benatar, Rod Stewart, and others. The introduction of the first five VJs was also shown. Additionally, MTV.com put together a "yearbook" consisting of the greatest videos of each year from 1981 to 2006. MTV itself only mentioned the anniversary once on "TRL".
Although MTV reached its 30th year of broadcasting in 2011, the channel itself passed over this milestone in favor of its current programming schedule. The channel instead aired its 30th anniversary celebrations on its sister networks MTV2 and VH1 Classic. Nathaniel Brown, senior vice president of communications for MTV, confirmed that there were no plans for an on-air MTV celebration similar to the channel's 20th anniversary. Brown explained, "MTV as a brand doesn't age with our viewers. We are really focused on our current viewers, and our feeling was that our anniversary wasn't something that would be meaningful to them, many of whom weren't even alive in 1981."
Fewer music videos (2000–08).
From 1995 to 2000, MTV played 36.5% fewer music videos. MTV president Van Toeffler explained: "Clearly, the novelty of just showing music videos has worn off. It's required us to reinvent ourselves to a contemporary audience." Despite targeted efforts to play certain types of music videos in limited rotation, MTV greatly reduced its overall rotation of music videos by the mid-2000s. While music videos were featured on MTV up to eight hours per day in 2000, the year 2008 saw an average of just three hours of music videos per day on MTV. The rise of social media and websites like YouTube as a convenient outlet for the promotion and viewing of music videos signaled this reduction.
As the decade progressed, MTV continued to play some music videos instead of relegating them exclusively to its sister channels, but around this time, the channel began to air music videos only in the early morning hours or in a condensed form on "Total Request Live". As a result of these programming changes, Justin Timberlake implored MTV to "play more damn videos!" while giving an acceptance speech at the "2007 Video Music Awards".
Despite the challenge from Timberlake, MTV continued to decrease its total rotation time for music videos in 2007, and the channel eliminated its long-running special tags for music videos such as "Buzzworthy" (for under-represented artists), "Breakthrough" (for visually stunning videos), and "Spankin' New" (for brand new videos). Additionally, the historic Kabel typeface, which MTV displayed at the beginning and end of all music videos since 1981, was phased out in favor of larger text and less information about the video's record label and director. The classic font can still be seen in "prechyroned" versions of old videos on sister network VH1 Classic, which had their title information recorded onto the same tape as the video itself.
"FNMTV" and "AMTV" (2008–present).
Prior to its finale in 2008, MTV's main source of music videos was "Total Request Live", airing four times per week, featuring short clips of music videos along with VJs and guests. MTV was experimenting at the time with new ideas for music programs to replace the purpose of "TRL" but with a new format.
In mid-2008, MTV premiered new music video programming blocks called "FNMTV" and a weekly special event called "FNMTV Premieres", hosted from Los Angeles by Pete Wentz of the band Fall Out Boy, which was designed to premiere new music videos and have viewers provide instantaneous feedback.
The "FNMTV Premieres" event ended before the "2008 Video Music Awards" in September. With the exception of a holiday themed episode in December 2008 and an unrelated "Spring Break" special in March 2009 with the same title, "FNMTV Premieres" never returned to the channel's regular program schedule, leaving MTV without any music video programs hosted by VJs for the first time in its history.
Music video programming returned to MTV in March 2009 as "AMTV", an early morning block of music videos that originally aired from 3 a.m. to 9 a.m. on most weekdays. It was renamed "Music Feed" in 2013 with a reduced schedule. Unlike the "FNMTV" block that preceded it, "Music Feed" features many full-length music videos, including some older videos that have been out of regular rotation for many years on MTV. It also features music news updates, interviews, and performances. "Music Feed" is the only current program on MTV's main channel that is dedicated to music videos.
During the rest of the day, MTV also plays excerpts from music videos, usually the hook, in split screen format during the closing credits of most programs, along with the address of a website to encourage the viewer to watch the full video online. MTV has positioned its website, MTV.com, as its primary destination for music videos.
Recent music programs (2009–present).
MTV again resurrected the long-running series "MTV Unplugged" in 2009 with performances from acts such as Adele and Paramore. However, unlike past "Unplugged" specials, these new recordings usually only aired in their entirety on MTV's website, MTV.com. Nevertheless, short clips of the specials are shown on MTV during the "AMTV" block of music videos in the early morning hours. On June 12, 2011, MTV aired a traditional television premiere of a new installment of "MTV Unplugged" instead of a web debut. The featured artist was rapper Lil Wayne and the show debuted both on MTV and MTV2. The channel followed up with a similar television premiere of "MTV Unplugged" with Florence and the Machine on April 8, 2012.
MTV launched "10 on Top" in May 2010, a weekly program airing on Saturdays and hosted by Lenay Dunn, that counts down the top 10 most trending and talked about topics of the week (generally focused on entertainment). Dunn also appeared in segments between MTV's shows throughout the day as a recognizable personality and face of the channel in the absence of traditional VJs aside from its MTV News correspondents.
The animated series "Beavis and Butt-head" returned to MTV in October 2011 with new episodes. As with the original version of the series that ran from 1993 to 1997, the modern-day "Beavis and Butt-head" features segments in which its main characters watch and criticize music videos.
Sometime in 2012, MTV debuted "Clubland", which previously existed as an hour of EDM videos during the "AMTV" video block. The show has no host, but most editorial content is pushed online by the show's Tumblr and other social media outlets like Facebook and Twitter. The show, which leads off the channel's music video programming during the early morning hours on Tuesdays through Thursdays and also airs on MTV Hits, was hosted by DJ Hardwell on July 5, 2012, but "'Clubland" remains hostless and as a source the best new and old EDM tracks.
MTV launched a new talk show based on hip hop music on April 12, 2012, called "Hip Hop POV", hosted by Amanda Seales, Bu Thiam, Charlamagne, Devi Dev, and Sowmya Krishnamurthy. The show featured hosted commentary on the headlines in hip hop culture, providing opinions on new music, granting insider access to major events, and including artist interviews. "Hip Hip POV" lasted several episodes before going on hiatus. The show was supposed to return in Fall 2012, but was moved to MTV2 instead, where it was rebranded and merged with "Sucker Free Countdown". The new show debuted as "The Week in Jams" on October 28, 2012.
Post-"TRL" live shows (2009–present).
MTV launched a live talk show, "It's On with Alexa Chung", on June 15, 2009. The host of the program, Alexa Chung, was described as a "younger, more Web 2.0" version of Jimmy Fallon. Although it was filmed in the same Times Square studio where "TRL" used to be broadcast, the network stated that "the only thing the two shows have in common is the studio location." "It's On" was cancelled in December of the same year, which again eliminated the only live in-studio programming from MTV's schedule, just one year after "TRL" was also cancelled.
Shortly after Michael Jackson died on June 25, 2009, the channel aired several hours of Jackson's music videos, accompanied by live news specials featuring reactions from MTV personalities and other celebrities. The temporary shift in MTV's programming culminated the following week with the channel's live coverage of Jackson's memorial service. MTV aired similar one-hour live specials with music videos and news updates following the death of Whitney Houston on February 11, 2012, and the death of Adam Yauch of the Beastie Boys on May 4, 2012.
The channel tried its hand again at live programming with the premiere of a half-hour program called "The Seven" in September 2010. The program counted down seven entertainment-related stories of interest to viewers (and included some interview segments among them), having aired weekdays at 5 p.m. with a weekend wrap-up at 10 am. ET. Shortly after its debut, the show was slightly retooled as it dropped co-host Julia Alexander but kept fellow co-host Kevin Manno; the Saturday recap show was eliminated as well. "The Seven" was cancelled on June 13, 2011. Manno's only assignment at MTV post-"Seven" was conducting an interview with a band which only aired on MTV.com. Manno is no longer employed with MTV and has since appeared as an occasional correspondent on the LXTV-produced NBC series "1st Look".
Presently, MTV airs sporadic live specials called "MTV First". The short program, produced by MTV News, debuted in early 2011 and continues to air typically once every couple of weeks on any given weekday. The specials usually begin at 7:53 pm. ET, led by one of MTV News' correspondents who will conduct a live interview with a featured artist or actor who has come to MTV to premiere a music video or movie trailer. MTV starts its next scheduled program at 8:00 pm, while the interview and chat with fans continues on MTV.com for another 30 to 60 minutes. Since its debut in 2011, "MTV First" has featured high-profile acts such as Lady Gaga, Katy Perry, Usher and Justin Bieber. In the absence of daily live programs such as "TRL", "It's on with Alexa Chung" and "The Seven" to facilitate such segments, the channel now uses "MTV First" as its newest approach to present music video premieres and bring viewers from its main television channel to its website for real-time interaction with artists and celebrities.
Other programs.
As MTV expanded, music videos were no longer the centerpiece of its programming. Conventional television shows came to replace the VJ-guided music video programming. Today, MTV presents a wide variety of non-music-related television shows aimed primarily at the 12 to 18-year-old demographic.
First format evolution (1985–91).
In 1985, Viacom bought Warner-Amex Satellite Entertainment, which owned MTV and Nickelodeon, renaming the company MTV Networks and beginning this expansion. Before 1987, MTV featured almost exclusively music videos, but as time passed, they introduced a variety of other shows, including some that were originally intended for other channels.
Non-music video programming began in the late 1980s with the introduction of a music news show "The Week in Rock", which was also the beginning of MTV's news division, MTV News. Around this time, MTV also introduced a fashion news show, "House of Style"; a dance show, "Club MTV"; and a game show, "Remote Control". Programs like these did not feature music videos, but they were still largely based around the world of music.
Following the success of the "MTV Video Music Awards", in an effort to branch out from music into movies and broader pop culture, MTV started the "MTV Movie Awards" in 1992, which continues presently. MTV also created an award show for Europe after the success of the "Video Music Awards". The "MTV Europe Music Awards", or the EMAs, were created in 1994, ten years after the debut of the VMAs.
These new shows would be just the beginning of new genres of shows to make an impact on MTV. As the format of the network continued to evolve, more genres of shows began to appear. In the early 1990s, MTV debuted its first reality shows, "The Real World" and "Road Rules".
Reality programs (1992–present).
During the latter half of the 1990s and early 2000s, MTV placed a stronger focus on reality shows and related series, building on the success of "The Real World" and "Road Rules". The first round of these shows came in the mid-1990s, with game shows such as "Singled Out", reality-based comedy shows such as "Buzzkill", and late-night talk shows such as "The Jon Stewart Show" and "Loveline".
The next round of these shows came in approximately the late 1990s, as MTV shifted its focus to prank/comedic shows such as "The Tom Green Show" and "Jackass", and game shows such as "Real World/Road Rules Challenge", "The Blame Game", "webRIOT", and "Say What? Karaoke". A year later, in 2000, "MTV's Fear" became one of the first scare-based reality shows and the first reality show in which contestants filmed themselves.
MTV continued to experiment with late night talk shows in the early 2000s with relatively short-lived programs such as "Kathy's So Called Reality", starring Kathy Griffin, and "The New Tom Green Show".
Some of the reality shows on the network also followed the lives of musicians. "The Osbournes", a reality show based on the everyday life of Black Sabbath frontman Ozzy Osbourne, his wife Sharon, and two of their children, Jack and Kelly, premiered on MTV in 2002. The show went on to become one of the network's biggest-ever successes and was also recognized for the Osbourne family members' heavy use of profanity, which MTV bleeped for broadcast. It also kick-started a musical career for Kelly Osbourne, while Sharon Osbourne went on to host her own self-titled talk show on U.S. television. Production ended on "The Osbournes" in November 2004. In the fall of 2004, Ozzy Osbourne's reality show "Battle for Ozzfest" aired; the show hosted competitions between bands vying to play as part of Ozzfest, a yearly heavy metal music tour across the United States hosted by Osbourne.
In 2003, MTV added "Punk'd", a project by Ashton Kutcher to play pranks on various celebrities, and "Pimp My Ride", a show about adding aesthetic and functional modifications to cars and other vehicles. Another show was "", a reality series that followed the lives of pop singers Jessica Simpson and Nick Lachey, a music celebrity couple. It began in 2003 and ran for four seasons, ending in early 2005; the couple later divorced. The success of "Newlyweds" was followed in June 2004 by "The Ashlee Simpson Show", which documented the beginnings of the music career of Ashlee Simpson, Jessica Simpson's younger sister.
In 2005 and 2006, MTV continued its focus on reality shows, with the debuts of shows such as "8th & Ocean", "", "NEXT", "The Hills", "Two-A-Days", "My Super Sweet 16", "Parental Control", and "Viva La Bam", featuring Bam Margera.
In 2007, MTV aired the reality show "A Shot at Love with Tila Tequila", chronicling MySpace sensation Tila Tequila's journey to find a companion. Her bisexuality played into the series – both male and female contestants were vying for love – and was the subject of criticism. It was the #2 show airing on MTV at that time, behind "The Hills". A spin-off series from "A Shot at Love", titled "That's Amoré!", followed a similar pursuit from previous "A Shot at Love" contestant Domenico Nesci.
MTV also welcomed Paris Hilton to its lineup in October 2008 with the launch of her new reality series, "Paris Hilton's My New BFF". In 2009, MTV aired Snoop Dogg's second program with the channel, "Dogg After Dark", and the show "College Life", based at the University of Wisconsin-Madison.
In late 2009, MTV shifted its focus back to "Real World"-style reality programming with the premiere of "Jersey Shore", a program that brought high ratings to the channel and also caused controversy due to some of its content.
With backlash towards what some consider too much superficial content on the network, a 2009 "New York Times" article also stated the intention of MTV to shift its focus towards more socially conscious media, which the article labels "MTV for the Obama era." Shows in that vein included "T.I.'s Road to Redemption" and Fonzworth Bentley's finishing school show "From G's to Gents".
The channel also aired a new show around this time titled "16 and Pregnant", which documented the lives of teenagers expecting babies. This had a follow-up show after the first season titled "Teen Mom", which follows some of the teens through the first stages with their newborns.
MTV found further success with "The Buried Life", a program about four friends traveling across the country to check off a list of "100 things to do before I die" and helping others along the way. Another recent reality program is MTV's "Hired", which follows the employment interviewing process; candidates meet with career coach Ryan Kahn from University of Dreams and at the end of each episode one candidate lands the job of their dreams.
In 2011, MTV premiered an amateur video clip show "Ridiculousness", in the vein of "Tosh.0" on Comedy Central. In 2012, "Punk'd" returned with a revolving door of new hosts per episode. Meanwhile, spin-offs from "Jersey Shore" such as "The Pauly D Project" and "Snooki & JWoww" were produced.
MTV announced plans to re-enter the late-night comedy space in 2012 with "Nikki & Sara Live", an unscripted series by comedians Nikki Glaser and Sara Schaefer. The program is slated to be aired weekly from MTV's studios in Times Square.
Animated programs (1991–2011).
In a continuing bid to become a more diverse network focusing on youth and culture as well as music, MTV added animated shows to its lineup in the early 1990s. The animation showcase "Liquid Television" (a co-production between BBC and MTV produced in San Francisco by Colossal Pictures) was one of the channel's first programs to focus on the medium. In addition to airing original shows created specifically for MTV, the channel also occasionally aired episodes of original cartoon series produced by sister channel Nickelodeon ("Nicktoons") in the early 1990s.
MTV has a history of cartoons with mature themes including "Beavis and Butt-head", "Æon Flux", "The Brothers Grunt", "Celebrity Deathmatch", "Undergrads", "Clone High", and "Daria". Although the channel has gone on to debut many other animated shows, few of MTV's other cartoon series have been renewed for additional seasons, regardless of their reception.
In September 2009, the channel aired "Popzilla", which showcased and imitated celebrities in an animated form. MTV again reintroduced animated programming to its lineup with the return of "Beavis and Butt-head" in 2011 after 14 years off the air, alongside brand new animated program "Good Vibes".
Scripted programs (1989–present).
MTV has a long history of airing both comedy and drama programs with scripted or improvised premises. Examples from the 1990s and 2000s include sketch-based comedies such as "Just Say Julie", "The Ben Stiller Show", "The State", "The Jenny McCarthy Show", "The Lyricist Lounge Show", and "Doggy Fizzle Televizzle", as well as soap operas such as "Undressed" and "Spyder Games".
The channel expanded its programming focus in late 2000s and early 2010s to include more scripted programs. The resurgence of scripted programming on MTV saw the introduction of comedy shows such as "Awkward." and "The Hard Times of RJ Berger", and dramas such as "Skins" and "Teen Wolf". In June 2012, MTV confirmed that it would develop a series based on the "Scream" franchise.
Rebroadcast programs.
In recent years, MTV has re-aired other programs from other Viacom-owned networks, such as BET's "College Hill" and VH1 programs "I Love New York" and "Flavor of Love". Other programs from non-Viacom networks include reruns of the shows "Fastlane" (from Fox), "Life As We Know It" (from ABC), "Scrubs" (from ABC and NBC), and CW programs "America's Next Top Model", "Beauty and the Geek", and "Hidden Palms".
MTV also began showing movies targeted toward the young adult demographic, including "8 Mile", "My Boss's Daughter", "Shaun of the Dead", and "Napoleon Dynamite" (the latter of which the network had a hand in producing). The channel has also broadcast several of its own films from its production division MTV Films, such as "Crossroads" and ', and airs original made-for-television movies from MTV Studios such as '.
Impact and recognition (2010).
In 2010, a study by the Gay and Lesbian Alliance Against Defamation found that of 207.5 hours of prime time programming on MTV, 42% included content reflecting the lives of gay, bisexual and transgender people. This was the highest in the industry and the highest percentage ever.
Image and branding.
Original logos and IDs.
The MTV logo was designed in 1981 by Manhattan Design, a collective formed by Frank Olinsky, Pat Gorman and Patty Rogoff, under the guidance of MTV's original creative director, Fred Seibert. The 'M' was sketched by Rogoff, with the 'TV' spray painted by Olinksky.
Throughout MTV's early days, the channel's main logo was a large yellow "M" with red letters "TV," but unlike most networks' logos, the MTV logo constantly morphed and adapted with different colors, patterns and images filling in the large block letter. The very first moments of MTV featured an adaptation of the first landing on the moon, directly from NASA still images (a concept of Seibert's, executed by Buzz Potamkin and Perpetual Motion Pictures). After the "moon landing," as well as the top of every hour until at least the mid-1980s (which ran "more than 15,000" times each year, according to Seibert), featured a rapidly changing network ID logo that changed its appearance several times per second. The only constant aspects of MTV's logo at the time were its general shape and proportions; everything else was dynamic.
The channel's "I want my MTV!" image and branding campaign was launched in 1982. The media strategy and creative executions were developed by George Lois, based on a cereal commercial from the 1950s, "I want my Maypo!" that George created. Over the years the campaign featured known artists and celebrities including Pete Townshend, Pat Benatar, Adam Ant, David Bowie, The Police, Kiss, Culture Club, Billy Idol, Hall & Oates, Cyndi Lauper, Madonna, Lionel Richie, Ric Ocasek, John Mellencamp, Peter Wolf, Joe Elliot, Stevie Nicks, Rick Springfield and Mick Jagger interacting with the MTV logo on-air, encouraging viewers to call their cable or satellite providers and request that MTV be added to their local channel lineups. Eventually, the slogan became so ubiquitous it became incorporated as a sung (by Sting) lyric in the Mark Knopfler penned Dire Straits' record "Money for Nothing."
1990s and 2000s updates.
Once MTV's original morphing logo had run its course, the channel began to use a solid color white logo that was otherwise the same as the original. During the late 1990s and early 2000s, MTV updated its on-air appearance at the beginning of every year and again each summer, creating a consistent brand across all of its music-related shows. This style of channel-wide branding came to an end as MTV drastically reduced its number of music-related shows in the early to mid-2000s. At this time, MTV introduced a static, single-color digital on-screen graphic during all of its other programming.
2010 rebranding.
Since the premiere of the short-lived "FNMTV" in 2008, MTV has used a revised, chopped version of its traditional logo during most of its on-air programming. This new logo was finalized and formally became MTV's official brand mark on February 8, 2010, when it debuted on MTV's website. The channel's long-running official tagline "Music Television" was officially dropped at this time. The revised logo is largely the same as MTV's original logo, but it excludes the "Music Television" caption, the bottom section of the "M" block letter, and the trailing letter "V" that branched off to the side of the original logo. However, much like the ever-changing patterns that filled MTV's original 1981 logo, the new 2010 logo is designed to be filled in with an unlimited variety of pictures and images. It is used worldwide, but not in all countries. It was first adopted for MTV Films with the 2010 release "Jackass 3D". MTV's rebranding was overseen by Popkern.
Influence and controversies.
The channel has been a target of criticism by various groups about programming choices, social issues, political correctness, sensitivity, censorship, and a perceived negative social influence on young people. Portions of the content of MTV's programs and productions have come under controversy in the general news media and among social groups that have taken offense. Some within the music industry criticized what they saw as MTV's homogenization of rock 'n' roll, including the punk band the Dead Kennedys, whose song "M.T.V. – Get Off the Air" was released on the 1985 album "Frankenchrist" just as MTV's influence over the music industry was being solidified. MTV was also the major influence on the growth of music video during the 80s.
Censorship of videos.
MTV has edited a number of music videos to remove references to drugs, sex, violence, weapons, racism, homophobia, or advertising. Many music videos aired on the channel were censored, moved to late night rotation, or banned entirely from the channel.
In the 1980s, parent-media watchdog groups such as the Parents Music Resource Center criticized MTV over certain music videos that were claimed to have explicit imagery of satanism. MTV developed a strict policy on refusal to air videos that may depict devil worship or anti-religious themes. This policy led MTV to ban music videos such as "Jesus Christ Pose" by Soundgarden in 1991 and "Megalomaniac" by Incubus in 2004.
Andrew Dice Clay.
During the "1989 MTV Video Music Awards" ceremony, comedian Andrew Dice Clay did his usual "adult nursery rhymes" routine (which he had done in his stand-up acts), after which the network executives imposed a lifetime ban. Billy Idol's music video for the song "Cradle of Love" originally had scenes from Clay's film "The Adventures of Ford Fairlane" when it was originally aired; scenes from the film were later excised. During the "2011 MTV Video Music Awards", Clay was in attendance where he confirmed that the channel lifted the ban.
"Beavis and Butt-head" "Fire".
In the wake of controversy that followed a child burning down his house after allegedly watching "Beavis and Butt-head", producers moved the show from its original 7 p.m. time slot to a late night, 11 p.m. slot. Also, Beavis' insane tendency to flick a lighter and scream the word "fire" was removed from new episodes, and controversial scenes were removed from existing episodes before rebroadcast. Some of the edits were so extensive that when series creator Mike Judge compiled his , he found out that "some of those episodes may not even exist actually in their original form."
"Dude, This Sucks".
A pilot for a show called "Dude, This Sucks" was canceled after teens attending a taping at the Snow Summit Ski Resort in January 2001 were sprayed with liquid fecal matter by the group "The Shower Rangers". The teens later sued. MTV later apologized and said that the segment would not air.
Super Bowl XXXVIII halftime show.
On the heels of parent company Viacom's purchase of CBS, MTV was selected to produce the halftime show at Super Bowl XXXV in 2001, airing on CBS and featuring Britney Spears, 'N Sync and Aerosmith. Due to its success, MTV was invited back to produce another Super Bowl halftime show in 2004, which would spark a nationwide debate and lead to sweeping changes in Super Bowl halftime shows, MTV's own programming, and even music played on the radio.
When CBS aired Super Bowl XXXVIII in 2004, its sister network was again chosen to produce the halftime show, with performances by such artists as Nelly, Diddy, Janet Jackson, and Justin Timberlake. The show became controversial after Timberlake tore off part of Jackson's outfit while performing his hit song "Rock Your Body" with her, revealing her right breast. All involved parties apologized for the incident, and Timberlake referred to the incident as a "wardrobe malfunction."
Michael Powell, then-chairman of the Federal Communications Commission, ordered an investigation of the show the day after its broadcast. In the weeks following the controversial halftime show, MTV censored much of its programming. Several music videos, including "This Love" by Maroon 5 and "I Miss You" by Blink-182, were edited for sexual content. In September 2004, the FCC ruled that the halftime show was indecent and fined CBS $550,000. The FCC upheld its decision in 2006, but federal judges reversed the fine in 2008.
The Super Bowl itself would not feature another modern act for its halftime show until The Black Eyed Peas performed at Super Bowl XLV in 2011.
Moral criticism.
The Christian right organization American Family Association has also criticized MTV from perceptions of negative moral influence, even going as far as to describe MTV as promoting a "pro-sex, anti-family, pro-choice, drug culture."
In 2005, the Parents Television Council released a study titled "MTV Smut Peddlers", which sought to expose what PTC believed was excessive sexual, profane, and violent content on the channel, based on MTV's "Spring Break" programming from 2004. Jeanette Kedas, an MTV network executive, called the PTC report "unfair and inaccurate" and "underestimating young people's intellect and level of sophistication," while L. Brent Bozell III, then-president of the PTC, stated: "the incessant sleaze on MTV presents the most compelling case yet for consumer cable choice," referring to the practice of cable and satellite companies to allow consumers to pay for channels "à la carte".
In April 2008, PTC released "The Rap on Rap", a study covering hip-hop and R&B music videos rotated on programs "106 & Park" and "Rap City", both shown on BET, and "Sucker Free" on MTV. PTC urged advertisers to withdraw sponsorship of those programs, whose videos PTC stated targeted children and teenagers containing adult content.
"Jersey Shore".
MTV received a significant amount of criticism from Italian American organizations for the show "Jersey Shore", which premiered in 2009. The controversy was due in large part to the manner in which MTV marketed the show, as it liberally used the word "guido" to describe the cast members. The word "guido" is generally regarded as an ethnic slur when referring to Italians and Italian Americans. One promotion stated that the show was to follow, "eight of the hottest, tannest, craziest Guidos," while yet another advertisement stated, ""Jersey Shore" exposes one of the tri-state area's most misunderstood species... the GUIDO. Yes, they really do exist! Our Guidos and Guidettes will move into the ultimate beach house rental and indulge in everything the Seaside Heights, New Jersey scene has to offer."
Prior to the series debut, UNICO National (which is the largest Italian American organization) formally requested that MTV cancel the show. In a letter to the network, UNICO called the show a "direct, deliberate and disgraceful attack on Italian Americans." UNICO National President Andre DiMino said in a statement, "MTV has festooned the 'bordello-like' house set with Italian flags and red, white and green maps of New Jersey while every other cutaway shot is of Italian signs and symbols. They are blatantly as well as subliminally bashing Italian Americans with every technique possible." Around this time, other Italian organizations joined the fight, including the NIAF and the Order Sons of Italy in America.
MTV responded to the controversy by issuing a press release which stated in part, "The Italian American cast takes pride in their ethnicity. We understand that this show is not intended for every audience and depicts just one aspect of youth culture." Following the calls for the show's removal, several sponsors requested that their ads not be aired during the show. These sponsors include Dell, Domino's Pizza and American Family Insurance. Despite the loss of certain advertisers, MTV did not cancel the show. Moreover, the show saw its audience increase from its premiere in 2009, and continued to place as MTV's top-rated programs during "Jersey Shore's" six-season run, ending in 2012.
Social activism.
In addition to its regular programming, MTV has a long history of promoting social, political, and environmental activism in young people. The channel's vehicles for this activism have been "Choose or Lose", encompassing political causes and encouraging viewers to vote in elections; "Fight For Your Rights", encompassing anti-violence and anti-discrimination causes; "think MTV"; and "MTV Act" and "Power of 12", the newest umbrellas for MTV's social activism.
"Choose or Lose".
In 1992, MTV started a pro-democracy campaign called "Choose or Lose", to encourage up to 20 million people to register to vote, and the channel hosted a town hall forum for then-candidate Bill Clinton.
In recent years, other politically diverse programs on MTV have included "True Life", which documents people's lives and problems, and MTV News specials, which center on very current events in both the music industry and the world. One special show covered the 2004 U.S. Presidential election, airing programs focused on the issues and opinions of young people, including a program where viewers could ask questions of Senator John Kerry. MTV worked with P. Diddy's "Vote or Die" campaign, designed to encourage young people to vote.
Additionally, MTV aired a documentary covering a trip by the musical group Sum 41 to the Democratic Republic of Congo, documenting the conflict there. The group ended up being caught in the midst of an attack outside of the hotel and were subsequently flown out of the country.
The channel also began showing presidential campaign commercials for the first time during the 2008 US presidential election. This has led to criticism from the right, with Jonah Goldberg opining that "MTV serves as the Democrats' main youth outreach program."
"Fight For Your Rights".
In the 1990s and early 2000s, MTV promoted annual campaigns known as "Fight For Your Rights", with the slogan "Speak Out/Stand Up Against Violence," to bring forth awareness on America's crime, drugs and violence issues.
On April 6, 2001, MTV voluntarily ceased regular programming for 24 hours as part of the year's hate crimes awareness campaign. On that night, MTV aired a made-for-TV movie "Anatomy of a Hate Crime", based on a true story of the 1998 murder of 21-year-old Matthew Shepard, a gay college student. After the film and a discussion, MTV went dark and showed names of hate crime victims.
"think MTV".
MTV's next activism campaign was "think MTV", which discussed current political issues such as same-sex marriage, US elections, and war in other countries. The original slogan of the program was "Reflect. Decide. Do." As part of "think MTV", the channel also aired a series of pro-conservation ads called "Break The Addiction", as a way of encouraging their viewers to find ways to use less fossil fuels and energy.
"think MTV" addressed twelve major issue areas: discrimination, environment, politics, health and self, crime and violence, poverty and disease, human rights, war and peace, relationships and sex, faith, substance abuse, and education. Young people were encouraged to choose the issues that resonate most and take action to make a positive change. The subsequent motto was, "Your cause. Your effect." "think MTV" was also integrated into MTV's programming.
"MTV Act" and "Power of 12".
In 2012, MTV launched "MTV Act" and "Power of 12", its current social activism campaigns. "MTV Act" focuses on a wide array of social issues, while "Power of 12" was a replacement for MTV's "Choose or Lose" and focused on the 2012 U.S. presidential election.
Beyond MTV.
Since its launch in 1981, the brand "MTV" has expanded to include many additional properties beyond the original MTV channel, including a variety of sister channels in the US, dozens of affiliated channels around the world, and an Internet presence through MTV.com and related websites.
Sister channels in the U.S..
MTV operates a group of channels under the name MTV Networks – a name that continues to be used for the individual units of the since renamed Viacom Media Networks, a division of its corporate parent, Viacom. In 1985, MTV saw the introduction of its first true sister channel, VH1, which was originally an acronym for "Video Hits One" and was designed to play adult contemporary music videos. Today, VH1 is aimed at celebrity and popular culture programming. Another sister channel, CMT, targets the country music and southern culture market.
The advent of satellite television and digital cable brought MTV greater channel diversity, including its current sister channels MTV2 and MTV Tr3́s (now Tr3́s), which initially played music videos exclusively but now focus on other programming. Music videos still occupy most of the schedule on two additional channels, MTV Hits and MTV Jams. MTV also broadcasts mtvU, a college-oriented channel on campus at various universities.
In the 2000s, MTV launched MTV HD, a 1080i high definition simulcast feed of MTV. Until Viacom's main master control was upgraded in 2013, only the network's original series after 2010 (with some pre-2010 content) are broadcast in high definition, while music videos, despite being filmed for high definition presentation, were presented in 4:3 standard definition, forcing them into a windowboxing type of presentation; since that time all music videos are presented in HD when available, depending on director's preference. "Jersey Shore", despite being shot with widescreen HD cameras, is also presented with SD windowboxing. Pay television providers such as DirecTV, Dish Network, Comcast, Time Warner Cable, AT&T U-verse, and Verizon FiOS carry the HD channel.
MTV Networks also operates Palladia, a high-definition channel that features original HD programming and HD versions of programs from MTV, VH1 and CMT. The channel was launched in January 2006 as MHD (Music: High Definition). The channel was officially rebranded as Palladia on September 1, 2008 to coincide with the shift to more exclusive HD programming.
In 2005 and 2006, MTV launched a series of channels for Asian Americans. The first channel was MTV Desi, launched in July 2005, dedicated toward South-Asian Americans. Next was MTV Chi, in December 2005, which catered to Chinese Americans. The third was MTV K, launched in June 2006 and targeted toward Korean Americans. Each of these channels featured music videos and shows from MTV's international affiliates as well as original US programming, promos, and packaging. All three of these channels ceased broadcasting on April 30, 2007.
Internet.
In the late 1980s, before the World Wide Web, MTV VJ Adam Curry began experimenting on the internet. He registered the then-unclaimed domain name "MTV.com" in 1993 with the idea of being MTV's unofficial new voice on the internet. Although this move was sanctioned by his supervisors at MTV Networks at the time, when Curry left to start his own web-portal design and hosting company, MTV subsequently sued him for the domain name, which led to an out-of-court settlement.
The service hosted at the domain name was originally branded "MTV Online" during MTV's first few years of control over it in the mid-1990s. It served as a counterpart to the America Online portal for MTV content, which existed at AOL keyword MTV until approximately the end of the 1990s. After this time, the website became known as simply "MTV.com" and served as the internet home base for all MTV and MTV News content.
MTV.com experimented with entirely video-based layouts between 2005 and 2007. The experiment began in April 2005 as "MTV Overdrive", a streaming video service that supplemented the regular MTV.com website. Shortly after the "2006 Video Music Awards", which were streamed on MTV.com and heavily utilized the "MTV Overdrive" features, MTV introduced a massive change for MTV.com, transforming the entire site into a Flash video-based entity. Much of users' feedback about the Flash-based site was negative, demonstrating a dissatisfaction with videos that played automatically, commercials that could not be skipped or stopped, and the slower speed of the entire website. The experiment ended in February 2006 as MTV.com reverted to a traditional HTML-based website design with embedded video clips, in the style of YouTube and some other video-based websites.
From 2006 to 2007, MTV operated an online channel, MTV International, targeted to the broad international market. The purpose of the online channel was to air commercial-free music videos once the television channels started concentrating on shows unrelated to music videos or music-related programming.
The channel responded to the rise of the Internet as the new central place to watch music videos in October 2008 by launching MTV Music, a website that features thousands of music videos from MTV and VH1's video libraries, dating back to the earliest videos from 1981.
A newly created division of the company, MTV New Media, announced in 2008 that it would produce its own original web series, in an attempt to create a bridge between old and new media. The programming is available to viewers via personal computers, cell phones, iPods, and other digital devices.
In the summer of 2012, MTV launched a music discovery web site called Artists.MTV. MTV says, "While technology has made it way easier for artists to produce and distribute their own music on their own terms, it hasn't made it any simpler to find a way to cut through all the Internet noise and speak directly to all of their potential fans. The summer launch of Artists.MTV is an attempt to help music junkies and musicians close the gap by providing a one-stop place where fans can listen to and buy music and purchase concert tickets and merchandise."
Today, MTV.com remains the official website of MTV, and it expands on the channel's broadcasts by bringing additional content to its viewers. The site's features include an online version of MTV News, podcasts, a video streaming service supported by commercials, movie features, profiles and interviews with recording artists and from MTV's television programs.
MTV around the world.
MTV Networks has launched numerous native-language regional variants of MTV-branded channels to countries around the world. For a complete list of these international MTV channels in Europe, Asia, Oceania, the Middle East, Africa, and the Americas, see List of MTV channels
References.
Bibliography.
</dl>

</doc>
<doc id="18857" url="http://en.wikipedia.org/wiki?curid=18857" title="Mustelidae">
Mustelidae

The Mustelidae (from Latin "mustela", weasel) are a family of carnivorous mammals, including the otters, badgers, weasels, martens, ferrets, minks and wolverines. Mustelids are diverse and the largest family in the order Carnivora. The internal classification still seems to be rather unsettled, with rival proposals containing between two and eight subfamilies. One study, published in 2008, questions the long-accepted Mustelinae subfamily, and suggests that Mustelidae consist of four major clades and three much smaller lineages.
Variety.
Mustelids vary greatly in size and behaviour. The least weasel is not much larger than a mouse, while the giant otter can measure up to 1.7 m in length and sea otters can exceed 45 kg in weight. The wolverine can crush bones as thick as the femur of a moose to get at the marrow, and has been seen attempting to drive bears away from their kills. The sea otter uses rocks to break open shellfish to eat. The marten is largely arboreal, while the badger digs extensive networks of tunnels, called setts. Some mustelids have been domesticated: the ferret and the tayra are kept as pets (although the tayra requires a Dangerous Wild Animals licence in the UK), or as working animals for hunting or vermin control. Others have been important in the fur trade—the mink is often raised for its fur.
As well as being one of the most species-rich families in the order Carnivora, the family Mustelidae is one of the oldest. Mustelid-like forms first appeared about 40 million years ago, roughly coinciding with the appearance of rodents. The direct ancestors of the modern mustelids first appeared about 15 million years ago.
Characteristics.
Within a large range of variation, the mustelids exhibit some common characteristics. They are typically small animals with short legs, short, round ears, and thick fur. Most mustelids are solitary, nocturnal animals, and are active year-round.
With the exception of the sea otter, they have anal scent glands that produce a strong-smelling secretion the animals use for sexual signaling and for marking territory.
Most mustelid reproduction involves embryonic diapause. The embryo does not immediately implant in the uterus, but remains dormant for a period of time. No development takes place as long as the embryo remains unattached to the uterine lining. As a result, the normal gestation period is extended, sometimes up to a year. This allows the young to be born under more favorable environmental conditions. Reproduction has a large energy cost and it is to a female's benefit to have available food and mild weather. The young are more likely to survive if birth occurs after previous offspring have been weaned.
Mustelids are predominantly carnivorous, although some will sometimes eat vegetable matter. While not all mustelids share an identical dentition, they all possess teeth adapted for eating flesh, including the presence of shearing carnassials. With variation between species, the most common dental formula is:
Ecology.
Several members of the family are aquatic to varying degrees, ranging from the semiaquatic mink, to the river otters, and to the highly aquatic sea otter. The sea otter is one of the few nonprimate mammals known to use a tool while foraging. It uses "anvil" stones to crack open the shellfish that form a significant part of its diet. It is a "keystone species", keeping its prey populations in balance so some do not outcompete the others and destroy the kelp in which they live.
The black-footed ferret is entirely dependent on another keystone species, the prairie dog. A family of four ferrets will eat 250 prairie dogs in a year; this requires a stable population of prairie dogs from an area of some 500 acre.
The mongoose and the meerkat bear a striking resemblance to many mustelids, but belong to a distinctly different suborder—the Feliformia (all those carnivores sharing more recent origins with the Felidae) and not the Caniformia (those sharing more recent origins with the Canidae). Because the mongooses and the mustelids occupy similar ecological niches, convergent evolution has led to some similarity in form and behavior.
Human uses.
Several mustelids, including the mink, the sable (a type of marten) and the stoat (ermine), boast exquisite and valuable furs, and have been accordingly hunted since prehistoric times. Since the early Middle Ages, the trade in furs was of great economic importance for northern and eastern European nations with large native populations of fur-bearing mustelids, and was a major economic impetus behind Russian expansion into Siberia and French and English expansion in North America. In recent centuries, fur farming, notably of mink, has also become widespread and provides the majority of the fur brought to market.
One species, the sea mink ("Neovison macrodon") of New England and Canada, was driven to extinction by fur trappers. Its appearance and habits are almost unknown today because no complete specimens can be found and no systematic contemporary studies were conducted.
The sea otter, which has the densest fur of any animal, narrowly escaped the fate of the sea mink. The discovery of large populations in the North Pacific was the major economic driving force behind Russian expansion into Kamchatka, the Aleutian Islands and Alaska, as well as a cause for conflict with Japan and foreign hunters in the Kuril Islands. Together with widespread hunting in California and British Columbia, the species was brought to the brink of extinction until an international moratorium came into effect in 1911.
Today, some mustelids are threatened for other reasons. Sea otters are vulnerable to oil spills and the indirect effects of overfishing; the black-footed ferret, a relative of the European polecat, suffers from the loss of American prairie; and wolverine populations are slowly declining because of habitat destruction and persecution. The rare European mink "Mustela lutreola" is one of the most endangered mustelid species.
One mustelid, the domestic ferret ("Mustela putorius furo"), has been domesticated since ancient times, originally for hunting rabbits and pest control.
Systematics.
The traditional classification of the Family has recently been questioned. The genetic studies on which the modern scheme was based did not include the genus "Lyncodon", which is therefore unplaced, but probably allied with "Mustela" and "Neovison". Older reference works generally suggest that Mustelidae should be divided into just two extant subfamilies, denoted Lutrinae and Mustelinae, although the latter now appears paraphyletic as originally described. Instead, Mustelidae is now thought to be divided into four major clades and three, much smaller, lineages. The early mustelids appear to have undergone two rapid bursts of diversification in Eurasia, with the resulting species only spreading to other continents later.
Examination of the mitochondrial DNA suggests that Taxidiinae diverged first, followed by Melinae. Lutrinae and Mustelinae are sister clades. The position of Helictidinae is unclear because the mitochondrial evidence suggests the sub-family is related to the Lutrinae-Mustelinae clade, while the intron data suggest a relationship to Martinae.
According to the older theory proposing two subfamilies, the 57 living species of mustelid are classified as:
Further reading.
</dl>

</doc>
<doc id="18858" url="http://en.wikipedia.org/wiki?curid=18858" title="Maryland">
Maryland

Maryland is a state located in the Mid-Atlantic region of the United States, bordering Virginia, West Virginia, and Washington, D.C. to its south and west; Pennsylvania to its north; and Delaware to its east. Maryland was the seventh state to ratify the United States Constitution, and has three occasionally used nicknames: the "Old Line State", the "Free State", and the "Chesapeake Bay State".
Maryland is also considered to be the birthplace of religious freedom in America, dating back to its earliest colonial days when it was made a refuge for persecuted Catholics from England by George Calvert the first Lord Baltimore, and the first English proprietor of the then-Maryland colonial grant.
Maryland is one of the smallest states in terms of area, as well as one of the most densely populated states of the United States. Maryland has the highest median household income, making it the wealthiest state in the nation. The state's largest city is Baltimore, and its capital is Annapolis. Although the state is officially claimed to be named after Queen Henrietta Maria, some Catholics believe Maryland was named after Mary, the mother of Jesus, by George Calvert, 1st Lord Baltimore prior to his death in 1632. The original intent may never be known. There is a St. Mary's County in Maryland.
Geography.
Maryland has an area of 12,406.68 sqmi and is comparable in overall area with the European country of Belgium (11787 mi2). It is the 42nd largest and 9th smallest state and is closest in size to the state of Hawaii (10930.98 mi2), the next smallest state. The next largest state, its neighbor West Virginia, is almost twice the size of Maryland (24229.76 mi2).
Maryland possesses a variety of topography within its borders, contributing to its nickname "America in Miniature". It ranges from sandy dunes dotted with seagrass in the east, to low marshlands teeming with wildlife and large bald cypress near the Chesapeake Bay, to gently rolling hills of oak forests in the Piedmont Region, and pine groves in the mountains to the west.
Maryland is bounded on its north by Pennsylvania, on its west by West Virginia, on its east by Delaware and the Atlantic Ocean, and on its south, across the Potomac River, by West Virginia and Virginia. The mid-portion of this border is interrupted by Washington, D.C., which sits on land originally part of Montgomery and Prince George's counties, including the town of Georgetown, Maryland, that was ceded to the Federal Government in 1790 to form the District of Columbia. (The Commonwealth of Virginia gave land south of the Potomac, including the town of Alexandria, Virginia, however Virginia retroceded its portion in 1846). The Chesapeake Bay nearly bisects the state and the counties east of the bay are known collectively as the "Eastern Shore".
Most of the state's waterways are part of the Chesapeake Bay watershed, with the exceptions of a tiny portion of extreme western Garrett County (drained by the Youghiogheny River as part of the watershed of the Mississippi River), the eastern half of Worcester County (which drains into Maryland's Atlantic coastal bays), and a small portion of the state's northeast corner (which drains into the Delaware River watershed). So prominent is the Chesapeake in Maryland's geography and economic life that there has been periodic agitation to change the state's official nickname to the Bay State, a nickname that has been used by Massachusetts for decades.
The highest point in Maryland, with an elevation of 3360 ft, is Hoye Crest on Backbone Mountain, in the southwest corner of Garrett County, near the border with West Virginia and near the headwaters of the North Branch of the Potomac River. Close to the small town of Hancock, in western Maryland, about two-thirds of the way across the state, there is 1.83 mi between its borders. This geographical curiosity makes Maryland the narrowest state, bordered by the Mason-Dixon Line to the north, and the northwards-arching Potomac River to the south.
Portions of Maryland are included in various official and unofficial geographic regions. For example, the Delmarva Peninsula is composed of the Eastern Shore counties of Maryland, the entire state of Delaware, and the two counties that make up the Eastern Shore of Virginia, whereas the westernmost counties of Maryland are considered part of Appalachia. Much of the Baltimore–Washington corridor lies just south of the Piedmont in the Coastal Plain, though it straddles the border between the two regions.
There are no natural lakes, though there are numerous natural ponds. During the latter Ice Ages, the glaciers did not reach as far south as Maryland, and therefore they did not carve out the deep natural lakes that exist in states farther north. There are numerous man-made lakes, the largest of these being the Deep Creek Lake, a reservoir in Garrett County in westernmost Maryland. The lack of a glacial history also accounts for Maryland's soil, which is sandier and muddier than the rocky soils farther to the north and northeast.
Flora.
As is typical of states on the East Coast, Maryland's plant life is abundant and healthy. A good dose of annual precipitation helps to support many types of plants, including seagrass and various reeds at the smaller end of the spectrum to the gigantic Wye Oak, a huge example of White oak, the state tree, which can grow in excess of 70 ft tall.
Middle Atlantic coastal forests, typical of the southeastern Atlantic coastal plain, grow around Chesapeake Bay and on the Delmarva Peninsula. Moving west, a mixture of Northeastern coastal forests and Southeastern mixed forests cover the central part of the state. The Appalachian Mountains of western Maryland are home to Appalachian-Blue Ridge forests. These give way to Appalachian mixed mesophytic forests near the West Virginia border.
Many foreign species are cultivated in the state, some as ornamentals, others as novelty species. Included among these are the Crape Myrtle, Italian Cypress, live oak in the warmer parts of the state, and even hardy palm trees in the warmer central and eastern parts of the state. USDA plant hardiness zones in the state range from Zones 5 and 6 in the extreme western part of the state to Zone 7 in the central part, and Zone 8 around the southern part of the coast, the bay area, and parts of metropolitan Baltimore. Invasive plant species, such as kudzu, tree of heaven, multiflora rose, and Japanese stiltgrass, stifle growth of endemic plant life. Maryland's state flower, the Black-eyed Susan, grows in abundance in wild flower groups throughout the state. The state insect, the Baltimore Checkerspot Butterfly, is not common as it is near the southern edge of its range. 435 species of birds have been reported from Maryland.
Fauna.
The state harbors a great number of White tailed deer, especially in the woody and mountainous west of the state, and overpopulation can become a problem from year-to-year. Mammals can be found ranging from the mountains in the west to the central areas and include black bears, bobcats, foxes, coyote, raccoons, and otters.
There is a population of rare wild horses found on Assateague Island. Every year during the last week of July, wild horses are captured and waded across a shallow bay for sale at Chincoteague, Virginia. This conservation technique ensures the tiny island is not overrun by the horses. The ponies and their sale were popularized by the children's book, "Misty of Chincoteague." They are believed to be descended from horses who escaped from shipwrecks.
The purebred Chesapeake Bay Retriever dog was bred specifically for water sports, hunting and search and rescue in the Chesapeake area. In 1878 the Chesapeake Bay Retriever was the first individual retriever breed recognized by the American Kennel Club. and was later adopted by the University of Maryland, Baltimore County as their mascot.
Maryland's reptile and amphibian population includes the Diamondback Terrapin turtle, which was adopted as the mascot of University of Maryland, College Park. The state is part of the territory of the Baltimore Oriole, which is the official state bird and mascot of the MLB team the Baltimore Orioles.
Environmental awareness.
In 2007, Forbes.com rated Maryland as the fifth "Greenest" state in the country behind three of the Pacific States and Vermont. Maryland ranks 40th in total energy consumption nationwide, and it managed less toxic waste per capita than all but six states in 2005. In April 2007 Maryland joined the Regional Greenhouse Gas Initiative (RGGI)—a regional initiative formed by all of the Northeastern states, Washington D.C., and three Canadian provinces to reduce greenhouse gas emissions.
Climate.
Maryland has a wide array of climates, due to local variances in elevation, proximity to water, and protection from colder weather due to downslope winds.
The eastern half of Maryland — which includes the cities of Ocean City, Salisbury, Annapolis, and the southern and eastern suburbs of Washington, D.C. and Baltimore — lies on the Atlantic Coastal Plain, with flat topography and sandy or muddy soil. This region has a humid subtropical climate (Köppen "Cfa"), with hot, humid summers and a short, mild to cool winter; it falls under USDA Hardiness zone 8a.
The Piedmont region — which includes northern and western greater Baltimore, Westminster, Gaithersburg, Frederick, and Hagerstown — has average seasonal snowfall totals generally exceeding 20 in and, as part of USDA Hardiness zones 7b and 7a, temperatures below 10 °F are less rare. From the Cumberland Valley on westward, the climate begins to transition to a humid continental climate (Köppen "Dfa").
In western Maryland, the higher elevations of Allegany and Garrett counties — including the cities of Cumberland, Frostburg, and Oakland — display more characteristics of the humid continental zone, due in part to elevation. They fall under USDA Hardiness zones 6b and below.
Precipitation.
Precipitation in the state is characteristic of the East Coast. Annual rainfall ranges from 35 to with more in higher elevations. Nearly every part of Maryland receives 3.5 – per month of rain. Average annual snowfall varies from 9 in in the coastal areas to over 100 in in the western mountains of the state.
Hurricanes and tornadoes.
Because of its location near the Atlantic Coast, Maryland is somewhat vulnerable to tropical cyclones, although the Delmarva Peninsula and the outer banks of North Carolina provide a large buffer, such that strikes from major hurricanes (category 3 or above) occur infrequently. More often, Maryland gets the remnants of a tropical system which has already come ashore and released most of its energy. Maryland averages around 30–40 days of thunderstorms a year, and averages around six tornado strikes annually.
Earthquakes.
Earthquakes in Maryland are infrequent due to its distance from tectonic plates and seismic/earthquake zones. The earthquakes that do occur are small, but may be felt over wide areas. The M5.8 Virginia earthquake in 2011 was felt moderately throughout Maryland. Buildings in the state are not well-designed for earthquakes and can suffer damage easily.
History.
17th century.
Maryland's first colonial settlement.
In 1629, George Calvert, 1st Lord Baltimore in the Peerage of Ireland, fresh from his failure further north with Newfoundland's Province of Avalon colony, applied to Charles I for a royal charter for what was to become the Province of Maryland. Calvert's interest in creating a colony derived from his Catholicism and his desire for the creation of a haven in the New World for Catholics, free of the persecution that was commonplace in England. He also wanted a share of fortunes, such as those made by the sale of the commodity tobacco in Virginia, and hoped to recoup some of the financial losses he had sustained in his earlier colonial venture in Newfoundland.
George Calvert died in April 1632, but a charter for "Maryland Colony" (in Latin, "Terra Maria") was granted to his son, Cecil Calvert, 2nd Baron Baltimore, on June 20, 1632. The new colony may have been named in honor of Henrietta Maria of France, wife of Charles I of England. The name recorded in the charter was phrased ""Terra Mariae, anglice", Maryland". The English name was preferred over the Latin due in part to the undesired association of "Mariae" with the Spanish Jesuit Juan de Mariana of the Inquisition.
To try to gain settlers, Maryland used what is known as the headright system, which originated in Jamestown. Settlers were given 50 acres of land for each person they brought into the colony, whether as settler, indentured servant or slave.
On November 22, 1633, Lord Baltimore sent the first settlers to the new colony, and after a long, rough sea voyage with a stopover to resupply in Barbados, they arrived in what is now Maryland in March of 1634. They made their first permanent settlement in what is now St. Mary's County choosing to settle on a bluff overlooking the St. Mary's river, a relatively calm, tidal tributary to the mouth of the Potomac river where it empties into the Chesapeake Bay. The site was already a Native American village when they arrived, occupied by members of the Piscataway Indian Nation, but the settlers had with them a former Virginia colonist who was fluent in their language and they met quickly with the paramount chief of the region. He agreed to sell the village to the settlers and ordered the area cleared. He had known of White men from communication with Native tribes to the South and West in Virginia and he was eager to gain technology, like guns and gunpowder, from the new Maryland settlers, and to trade with them as well. And so he came to the settlers shortly after their arrival and reached a treaty with them almost immediately. The new settlement was called "St. Mary's City" and it became the first capitol of Maryland, and remained so for sixty years until 1695.
More settlers soon followed and St. Mary's City quickly began to grow. The tobacco crops that they had planned from the outset were very successful and made the new colony profitable very quickly, although disease was a big killer and many colonists died in the first years until immunities built up in the population. Religious tensions would also come to challenge the colony in significant ways, making the early times very harrowing in spite of the early economic successes.
During the persecution of Catholics in the Puritan revolt, Protestants burned down all of the original Catholic churches of southern Maryland. The Puritan revolt lasted until 1658, when the Calvert family regained control of the colony and re-enacted the Toleration Act. Although most of the settlers were Protestants, Maryland soon became one of the few regions in the English Empire where Catholics held the highest positions of political authority. Maryland was also a key destination for transport of tens of thousands of English convicts to work as indentured servants.
Border disputes.
The royal charter granted Maryland the land north of the entire length of the Potomac River up to the 40th parallel. A problem arose when Charles II granted a charter for Pennsylvania. The grant defined Pennsylvania's southern border as identical to Maryland's northern border, the 40th parallel. But the terms of the grant clearly indicate that Charles II and William Penn assumed the 40th parallel would pass close to New Castle, Delaware when it falls north of Philadelphia, the site of which Penn had already selected for his colony's capital city. Negotiations ensued after the problem was discovered in 1681.
A compromise proposed by Charles II in 1682, which might have resolved the issue, was undermined by Penn's receiving the additional grant of what is now Delaware. Maryland claimed that territory for itself, but Penn successfully argued that the Maryland charter entitled Lord Baltimore only to unsettled lands, and Dutch settlement in Delaware predated his charter. The dispute remained unresolved for nearly a century, carried on by the descendants of William Penn and Lord Baltimore — the Calvert family, which controlled Maryland, and the Penn family, which controlled Pennsylvania.
Resumption of persecution of Catholics.
Maryland was founded for the purpose of providing religious toleration of England's Roman Catholic minority. With the exception of a period of armed conflict for a couple of years in the 1640s, religious tolerance was achieved for 60 years in the Maryland colony. However the English Parliament later reversed that policy and discouraged the practice of Catholicism in Maryland. This was followed by a second Protestant uprising that overthrew Maryland's Catholic leaders and ended the time of tolerance. After this, Catholics lost the right to vote and Catholic immigration to the colony was also penalized and heavily restricted until the 1820s.
However, after England's "Glorious Revolution" of 1688, when William of Orange came to the throne and established the Protestant faith in England, Maryland outlawed Catholicism. This lasted until after the American Revolutionary War. Wealthy Catholic planters built chapels on their land to practice their religion in relative secrecy.
18th century.
The border dispute with Pennsylvania continued and led to Cresap's War, a conflict between settlers from Pennsylvania and Maryland fought in the 1730s. Hostilities erupted in 1730 with a series of violent incidents prompted by disputes over property rights and law enforcement, and escalated through the first half of the decade, culminating in the deployment of military forces by Maryland in 1736 and by Pennsylvania in 1737. The armed phase of the conflict ended in May 1738 with the intervention of King George II, who compelled the negotiation of a cease-fire. A provisional agreement had been established in 1732.
Negotiations continued until a final agreement was signed in 1760. The agreement defined Maryland's border with what is now Delaware as well as Pennsylvania. The border between Maryland and Pennsylvania was defined as the line of latitude 15 mi south of the southernmost house of Philadelphia, a line now known as the Mason-Dixon Line. Maryland's border with Delaware was based on a Transpeninsular Line and the Twelve-Mile Circle around New Castle's Court House.
After Virginia made Anglicanism the established religion in the colony, numerous Puritans migrated from Virginia to Maryland. They were given land for a settlement called Providence (now Annapolis).
St. Mary's City was the first (besides St. Clement's Island, where the first colonists of Maryland landed) and largest site of the original Maryland colony, and was the seat of the colonial government until 1695, when the capitol was moved to Annapolis. St Mary's is now a state-owned archaeological site and museum adjacent to St. Mary's College of Maryland.
Most of the English colonists arrived in Maryland as indentured servants, and had to serve a several years' term as laborers to pay for their passage. In the early years, the line between indentured servants and African slaves or laborers was fluid, and white and black laborers commonly lived and worked together, and formed unions. Mixed-race children born to white mothers were considered free by the principle of "partus sequitur ventrem", by which children took the social status of their mothers, a principle of slave law that was adopted throughout the colonies, following Virginia in 1662. During the colonial era, families of free people of color were formed most often by unions of white women and African men.
Many of the free black families migrated to Delaware, where land was cheaper. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, planters in Maryland imported thousands more slaves and racial caste lines hardened. The economy's growth and prosperity was based on slave labor, devoted first to the production of tobacco as the commodity crop.
Maryland was one of the thirteen colonies that revolted against British rule in the American Revolution. On February 2, 1781, Maryland became the 13th state to approve the ratification of the Articles of Confederation which brought into being the United States as a united, sovereign and national state. It also became the seventh state admitted to the U.S. after ratifying the new Constitution. In December 1790, Maryland donated land selected by President George Washington to the federal government for the creation of the new national capital of Washington, D.C. The land was provided from Montgomery and Prince George's counties, as well as from Fairfax County and Alexandria in Virginia; however, the land donated by Virginia was later returned to that state by the District of Columbia retrocession.
19th century.
Influenced by a changing economy, revolutionary ideals, and preaching by Methodist and Quaker ministers, numerous planters in Maryland freed their slaves in the twenty years after the Revolutionary War. This was a pattern across the Upper South, in which the free black population increased markedly from less than 1% before the war to 14% by 1810. After that, increasing demand in the Deep South, which was developed for cotton plantations, resulted in slaves being sold and transported there from the Upper South, including Maryland.
During the War of 1812, the British military attempted to capture the port of Baltimore, which was protected by Fort McHenry. It was during this bombardment that the song, "Star Spangled Banner," was written by Francis Scott Key; it was later adopted as the national anthem.
The Baltimore and Ohio Railroad (B&O) was the first chartered railroad in the United States, and it opened its first section of track for regular operation in 1830, between Baltimore and Ellicott City. In 1852 it became the first rail line to reach the Ohio River from the eastern seaboard. Baltimore's seaport and good railroad connections fostered substantial growth during the Industrial Revolution of the 19th century. Many manufacturing businesses were established in Baltimore and the surrounding area after the Civil War.
Civil War.
By 1860 Maryland's free black population comprised 49% of the total of African Americans in the state. This contributed to the state's remaining loyal to the Union during the Civil War.["discuss"]
In addition, Governor Thomas Holliday Hicks temporarily suspended the state legislature, and President Abraham Lincoln had a number of its pro-slavery politicians, called "fire eaters," arrested prior to its reconvening. Lincoln ordered U.S. troops to place artillery on Federal Hill to threaten the city of Baltimore, and helped ensure the election of a new pro-union governor and legislature.
Lincoln ordered certain pro-South members of the state legislature and other prominent men jailed at Fort McHenry, including the Mayor of Baltimore, George William Brown. Historians continue to debate the constitutionality of these actions taken during the crisis of wartime. The Thomas Viaduct, which crosses the Patapsco River on the B&O Railroad, was considered so strategically important that Union troops were assigned to guard it throughout the entirety of the war.
In April 1861, Federal regular military units and state militia regiments arrived in Baltimore at the President Street Station of the Philadelphia, Wilmington and Baltimore Railroad, located east of the "Basin" (Inner Harbor). The troops, headed for Washington, D.C., marched through Baltimore towards the B&O Camden Station to continue their journey, and along the way they were attacked by an unruly mob. The incident, later known as the Baltimore riot of 1861, was the first bloodshed in the Civil War. Four soldiers and twelve civilians were killed in the riot.
Of the 115,000 men from Maryland who joined the military during the Civil War, 85,000, or 77%, joined the Union army, while the remainder joined the Confederate Army. To help ensure Maryland's inclusion in the Union, President Lincoln suspended several civil liberties, including the writ of "habeas corpus." This suspension was later deemed illegal by Chief Justice Roger Taney of the United States Supreme Court, a Maryland native.
The largest and most significant battle fought in the state was the Battle of Antietam, fought on September 17, 1862, near Sharpsburg. Although a tactical draw, the Battle of Antietam was considered a strategic Union victory and a turning point of the war.
Because Maryland remained in the Union, it was exempted from the abolition provisions of the Emancipation Proclamation of 1863, which applied only to states in rebellion. In 1864 the state held a constitutional convention that culminated in the passage of a new state constitution. Article 24 of that document abolished slavery. In 1867, following passage of constitutional amendments that granted voting rights to freedmen, the state extended suffrage to non-white males.
After the war.
The Democratic Party rapidly regained power in the state and replaced Republicans who had ruled during the war. Support for the Constitution of 1864 ended, and Democrats replaced it with the Maryland Constitution of 1867. Later, following the end of Reconstruction in 1877, Democrats devised various means of disfranchising freedmen and former free blacks, as did all the other states of the former Confederacy, initially by physical intimidation and voter fraud, later by constitutional amendments and laws. But, Maryland blacks were part of a biracial Republican coalition elected to state government in 1896-1904, and comprised 20% of the electorate. Immigrants comprised another major portion and generally also opposed disfranchisement. Both groups resisted later Democratic Party efforts in the state directed at disfranchisement.
Compared to some other states, blacks were better established both before and after the civil war. Nearly half the population was free before the war, and some had accumulated property. Half the population lived in cities, where they had more physical security than in rural areas. Literacy was quite high among blacks and, as Democrats crafted means to exclude them, suffrage campaigns helped reach blacks and teach them how to resist. Whites did impose racial segregation in public facilities and Jim Crow laws, which effectively lasted until passage of federal civil rights legislation in the mid-1960s. They tended to underfund such facilities.
As the industrial revolution swept across the northeast and midwestern United States, Baltimore continued to expand and prosper. Baltimore businessmen, including Johns Hopkins, Enoch Pratt, George Peabody, and Henry Walters, founded notable educational, health care, and cultural institutions in the city, which bear their names, including a university, library, music school and art museum. Major cities attracted European immigrants, particularly Baltimore and its environs, which had many industrial jobs.
Cumberland was Maryland's second-largest city in the 19th century, with ample nearby supplies of coal, iron ore and timber. These resources, along with railroads, the National Road and the Chesapeake and Ohio Canal, fostered its growth. The city was a major manufacturing center, with industries in glass, breweries, fabrics and tinplate.
20th century.
The Progressive Era of the late 19th and early 20th centuries brought reforms in the political arena and in working conditions for Maryland's labor force. In a series of laws passed between 1892 and 1908, reformers worked for standard state-issued ballots (rather than those distributed and pre-marked by the parties); obtained closed voting booths to prevent party workers from "assisting" voters; initiated primary elections to keep party bosses from selecting candidates; and had candidates listed without party symbols, which discouraged the illiterate from participating. These measures also had the practical effect of working against ill-educated whites and blacks, indirectly disfranchising them. Blacks resisted such efforts, with suffrage groups conducting voter education to teach people how to deal with the new rules. As noted above, in the early 20th century, blacks defeated three efforts by white Democrats to disfranchise them, making alliances with immigrants to do so and finding numerous ways to resist various Democratic campaigns.
The legislature tried to pass disfranchising bills in 1905, 1907, and 1911, but it was rebuffed on each occasion, in large part because of black opposition. Blacks comprised 20% of the electorate. In addition, immigrants comprised 15% of the voting population, and the legislature had difficulty devising requirements against blacks that did not also disadvantage immigrants.
In 1902, the state regulated conditions in mines; outlawed child laborers under the age of 12; mandated compulsory school attendance; and enacted the nation's first workers' compensation law. The workers' compensation law was overturned in the courts, but was redrafted and finally enacted in 1910.
The Great Baltimore Fire of February 8, 1904 was a momentous event for Maryland's largest city and the state as a whole. More than 1,231 firefighters, some coming from cities as far away as New York, worked to bring the blaze under control. The fire burned over 30 hours, destroying 1,526 buildings and spanning 70 city blocks.
The nation's entry into World War I in 1917 brought changes to Maryland. New military bases, such as Camp Meade (now Fort Meade) and the Aberdeen Proving Ground were established in 1917, and the Edgewood Arsenal was founded the following year. Other existing facilities, including Fort McHenry, were greatly expanded.
Maryland's urban and rural communities had different experiences during the Great Depression. In 1932 the "Bonus Army" marched through the state on its way to Washington, D.C. In addition to the nationwide New Deal reforms of President Franklin Roosevelt, which put men to work building roads and park facilities, Maryland also took steps to weather the hard times. For instance, in 1937 the state instituted its first ever income tax to generate revenue for schools and welfare.
Baltimore was a major war production center during World War II. The biggest operations were Bethlehem Steel's Fairfield Yard, which built Liberty ships; and Glenn Martin, an aircraft manufacturer.
Following World War II, Maryland experienced growth in the suburbs, particularly in the region surrounding Baltimore and Washington, D.C. Agricultural tracts gave way to residential communities such as Columbia and Montgomery Village. Concurrently the Interstate Highway System was built throughout the state, most notably I-95 and the Capital Beltway, permanently altering the landscape and travel patterns. In 1952, the eastern and western halves of Maryland were linked for the first time by the long Chesapeake Bay Bridge, which replaced a nearby ferry service. This bridge (and its later, parallel span) increased tourist traffic to Ocean City on the Atlantic Coast, which had a building boom. Soon after, the Baltimore Harbor Tunnel allowed long-distance interstate motorists to bypass downtown Baltimore, while the earlier Harry W. Nice Memorial Bridge allowed them to bypass Washington, D.C.
In a pattern similar to that of other U.S. cities, heavy manufacturing declined in Baltimore after the war, beginning in the 1950s, with far-reaching, adverse effects for working-class families. Family farms were bought up by major concerns and large-scale, mechanized poultry farms became prevalent on the lower Eastern Shore, along with irrigated vegetable farming. In Southern Maryland, tobacco farming had nearly vanished by the end of the 20th century, due to suburban housing development and a state tobacco incentive buy-out program. Industrial, railroad, and coal mining jobs in the four westernmost counties declined.
Beginning in the 1960s with Charles Center and the Baltimore World Trade Center, the city of Baltimore initiated urban renewal projects. Some resulted in the break-up of intact residential neighborhoods, producing social volatility. In 1980, the opening of Harborplace and the Baltimore Aquarium made the city a significant tourist destination. The popular Camden Yards baseball stadium opened in 1992 in the downtown area. Some residential areas of older housing around the harbor, such as Fells Point and Federal Hill, have had units renovated and have become popular with new populations. The loss of working-class industrial meant that other parts of the city suffered depopulation.
At the end of the century, Maryland joined with neighboring states to improve the health of the Chesapeake Bay. The bay's aquatic life and seafood industry have been threatened by suburban and waterfront residential development, as well as by fertilizer and livestock waste entering the bay in stormwater runoff, especially from the upper Susquehanna River in Pennsylvania.
Demographics.
The United States Census Bureau estimates that the population of Maryland was 5,976,407 on July 1, 2014, a 3.51% increase since the 2010 United States Census.
In 2014, Maryland had an estimated population of 5,976,407, which is an increase of 47,593, from the prior year and an increase of 202,855, or 3.51% percent, since 2010. This includes a natural increase since the last census of 189,158 people (that is 464,251 births minus 275,093 deaths) and an increase due to net migration of 116,713 people into the state. Immigration from outside the United States resulted in a net increase of 129,730 people, and migration within the country produced a net loss of 13,017 people.
Most of the population of Maryland lives in the central region of the state, in the Baltimore Metropolitan Area and Washington Metropolitan Area, both of which are part of the Baltimore-Washington Metropolitan Area.
The Eastern Shore is less populous and more rural, as are the counties of western and southern Maryland. The two westernmost counties of Maryland, Allegany and Garrett, are mountainous and sparsely populated, resembling West Virginia more than they do the rest of Maryland.
The center of population of Maryland is located on the county line between Anne Arundel County and Howard County, in the unincorporated community of Jessup.
The majority of Maryland's population is concentrated in the cities and suburbs surrounding Washington, D.C., as well as in and around Maryland's most populous city, Baltimore. Historically, these and many other Maryland cities developed along the Fall Line, the line along which rivers, brooks, and streams are interrupted by rapids and/or waterfalls. Maryland's capital city, Annapolis, is one exception to this pattern, since it lies along the banks of the Severn River, close to where it empties into the Chesapeake Bay.
The eastern, southern, and western portions of the state tend to be more rural, although they are dotted with cities of regional importance, such as Ocean City, Princess Anne, and Salisbury on the Eastern Shore; Lexington Park, Prince Frederick, and Waldorf in Southern Maryland; and Cumberland, Frostburg, and Hancock in Western Maryland.
Maryland's history as a border state has led it to exhibit characteristics of both the Northern and Southern regions of the United States. Generally, rural Western Maryland between the West Virginian Panhandle and Pennsylvania has an Appalachian culture; the Southern and Eastern Shore regions of Maryland embody a Southern culture, 
while densely populated Central Maryland—radiating outward from Baltimore and Washington, D.C.—has more in common with that of the Northeast. 
The U.S. Census Bureau designates Maryland as one of the South Atlantic States, but it is commonly associated with the Mid-Atlantic States and/or Northeastern United States by other federal agencies, the media, and some residents.
Racial and ethnic makeup.
In 1970, the Census Bureau reported Maryland's population as 17.8 percent African-American and 80.4 percent non-Hispanic White.
As of 2011, 58.0 percent of Maryland's population younger than age 1 were non-white.
African Americans form a sizable portion of the state's population – nearly 30 percent in 2010. Most are descendants of people transported to the area as slaves from West Africa, and many are of mixed race, including European and Native American ancestry.
Large ethnic minorities include Eastern Europeans such as Croatians, Russians and Ukrainians. The shares of European immigrants born in Eastern Europe increased significantly between 1990 and 2000, and again between 2000 and 2010. Following the dissolution of the Soviet Union, Yugoslavia, and Czechoslovakia, many immigrants from Eastern Europe came to the United States- 12 percent of which currently reside in Maryland.
New residents of African descent include 20th-century and later immigrants from Nigeria, particularly of the Igbo and Yoruba tribes. Concentrations of African Americans live in Baltimore City, Prince George's County, a suburb of Washington, DC, where many work; Charles County, Randallstown, and the southern Eastern Shore.
Irish American populations can be found throughout the Baltimore area, and the Northern and Eastern suburbs of Washington D.C. in Maryland (descents of those who moved out to the suburbs of Washington's once predominantly Irish neighborhoods), as well as Western Maryland, where Irish immigrant laborers helped to build the C & O Railroad. Smaller but much older Irish populations can be found in Southern Maryland, with some roots dating as far back as the early Maryland colony. This population however, still remains culturally very active and yearly festivals are held.
A large percentage of the population of the Eastern Shore and Southern Maryland are descendants of British American ancestry. The Eastern Shore was settled by Protestants, chiefly Methodist and the southern counties were initially settled by English Catholics. Western and northern Maryland have large German-American populations. More recent European immigrants of the late 19th and early 20th century settled first in Baltimore, attracted to its industrial jobs. Many of their ethnic Italian, Polish, Czech, and Greek descendants still live in the area.
Hispanic immigrants of the later 20th century have settled in Hyattsville/Langley Park, Wheaton, Bladensburg, Riverdale Park, Gaithersburg, and Highlandtown in East Baltimore. Salvadorans are the largest Hispanic group in Maryland. Other Hispanic groups with significant populations in the state include Mexicans and Puerto Ricans. Though the Salvadoran population is more concentrated in the area around Washington, DC, and the Puerto Rican population is more concentrated in the Baltimore area, all other major Hispanic groups in the state are evenly dispersed between these two areas. Maryland has one of the most diverse Hispanic populations in the country, with significant populations from various Caribbean and Central American nations.
Jews of European-American descent are numerous throughout Montgomery County and in Pikesville and Owings Mills northwest of Baltimore. Asian Americans are concentrated in the suburban counties surrounding Washington, D.C. and in Howard County, with Korean American and Taiwanese American communities in Rockville, Gaithersburg, and Germantown and a Filipino American community in Fort Washington. Numerous Indian Americans live across the state, especially in central Maryland. Amish/Mennonite communities are found in St. Mary's, Garrett, and Washington counties.
Attracting educated Asians and Africans to the professional jobs in the region, Maryland has the fifth-largest proportions of racial minorities in the country.
In 2006, 645,744 were counted as foreign born, which represents mainly people from Latin America and Asia. About 4.0 percent are undocumented (illegal) immigrants. Maryland also has a large Korean American population. In fact, 1.7 percent are Korean, while as a whole, almost 6.0 percent are Asian.
According to The Williams Institute's analysis of the 2010 U.S. Census, 12,538 same-sex couples are living in Maryland, representing 5.8 same-sex couples per 1,000 households.
Ancestry.
The top reported ancestries by Maryland residents were:
Religion.
More than 60% of Maryland's population does not have an affiliation with a particular religion. The largest religious groups in Maryland as of 2010 were: the Catholic Church with 837,338 adherents in Maryland, followed by non-denominational Evangelical Protestants with 298,921 members, and the United Methodist Church with 238,774. The Southern Baptist Convention has 150,345 members.
Judaism is the largest non-Christian religion in Maryland with 241,000 adherents, or 4 percent of the total population.
The Seventh-day Adventist Church's World Headquarters and Ahmadiyya Muslims national Headquarters is located in Silver Spring, just outside the District of Columbia.
Maryland has been prominent in U.S. Catholic tradition, partially because it was intended by George Calvert as a haven for English Catholics. Baltimore was the seat of the first Catholic bishop in the U.S. (1789), and Emmitsburg was the home and burial place of the first American-born citizen to be canonized, St. Elizabeth Ann Seton. Georgetown University, the first Catholic University, was founded in 1789 in what was then part of Maryland. The Basilica of the National Shrine of the Assumption of the Virgin Mary in Baltimore was the first Roman Catholic cathedral built in the United States, and the Archbishop of Baltimore is, albeit without formal primacy, the United States' quasi-primate, and often a Cardinal. Among the immigrants of the 19th and 20th century from eastern and southern Europe were many Catholics.
Economy.
The Bureau of Economic Analysis estimates that Maryland's gross state product in 2012 was US$317.7 billion. However, Maryland has been using Genuine Progress Indicator, an indicator of well-being, to guide the state's development, rather than relying only on growth indicators like GDP. According to the U.S. Census Bureau, Maryland households are currently the wealthiest in the country, with a 2009 median household income of $69,272 which puts it ahead of New Jersey and Connecticut, which are second and third respectively. Two of Maryland's counties, Howard and Montgomery, are the second and eleventh wealthiest counties in the nation respectively. Also, the state's poverty rate of 7.8 percent is the lowest in the country. Per capita personal income in 2006 was US$43,500, 5th in the nation. Maryland ranked No. 1 with the most millionaires per capita in 2013, with a ratio of 7.7 percent.
As of May 2014, the state's unemployment rate was 5.5 percent.
Maryland's economy takes advantage of the close location of the center of government in Washington, D.C. and emphasizes technical and administrative tasks for the defense/aerospace industry and bio-research laboratories, as well as staffing of satellite government headquarters in the suburban or exurban Baltimore/Washington area. Ft. Meade serves as the headquarters of the Defense Information Systems Agency, United States Cyber Command, and the National Security Agency/Central Security Service. In addition, a number of educational and medical research institutions are located in the state. In fact, the various components of The Johns Hopkins University and its medical research facilities are now the largest single employer in the Baltimore area. Altogether, white collar technical and administrative workers comprise 25 percent of Maryland's labor force, attributable in part to nearby Maryland being a part of the Washington Metro Area where the federal government office employment is relatively high.
Manufacturing, while large in dollar value, is highly diversified with no sub-sector contributing over 20 percent of the total. Typical forms of manufacturing include electronics, computer equipment, and chemicals. The once mighty primary metals sub-sector, which at one time included what was then the largest steel factory in the world at Sparrows Point, still exists, but is pressed with foreign competition, bankruptcies, and company mergers. During World War II the Glenn Martin Company (now part of Lockheed Martin) airplane factory employed some 40,000 people.
Mining other than construction materials is virtually limited to coal, which is located in the mountainous western part of the state. The brownstone quarries in the east, which gave Baltimore and Washington much of their characteristic architecture in the mid-19th century, were once a predominant natural resource. Historically, there used to be small gold-mining operations in Maryland, some near Washington, but these no longer exist.
Baltimore port.
One major service activity is transportation, centered on the Port of Baltimore and its related rail and trucking access. The port ranked 17th in the U.S. by tonnage in 2008. Although the port handles a wide variety of products, the most typical imports are raw materials and bulk commodities, such as iron ore, petroleum, sugar, and fertilizers, often distributed to the relatively close manufacturing centers of the inland Midwest via good overland transportation. The port also receives several different brands of imported motor vehicles and is the number two auto port in the U.S.
Baltimore City is the eighth largest port in the nation, and was at the center of the February 2006 controversy over the Dubai Ports World deal because it was considered to be of such strategic importance. The state as a whole is heavily industrialized, with a booming economy and influential technology centers. Its computer industries are some of the most sophisticated in the United States, and the federal government has invested heavily in the area. Maryland is home to several large military bases and scores of high level government jobs.
Agriculture and fishing.
Maryland has a large food-production sector. A large component of this is commercial fishing, centered in the Chesapeake Bay, but also including activity off the short Atlantic seacoast. The largest catches by species are the blue crab, oysters, striped bass, and menhaden. The Bay also has overwintering waterfowl in its wildlife refuges. The waterfowl support a tourism sector of sportsmen.
Maryland has large areas of fertile agricultural land in its coastal and Piedmont zones, though this land use is being encroached upon by urbanization. Agriculture is oriented to dairy farming (especially in foothill and piedmont areas) for nearby large city milksheads plus specialty perishable horticulture crops, such as cucumbers, watermelons, sweet corn, tomatoes, muskmelons, squash, and peas (Source:USDA Crop Profiles). In addition, the southern counties of the western shoreline of Chesapeake Bay are warm enough to support a tobacco cash crop zone, which has existed since early Colonial times but declined greatly after a state government buyout in the 1990s. There is also a large automated chicken-farming sector in the state's southeastern part; Salisbury is home to Perdue Farms. Maryland's food-processing plants are the most significant type of manufacturing by value in the state.
Taxation.
Maryland imposes 5 income tax brackets, ranging from 2 to 6.25 percent of personal income. The city of Baltimore and Maryland's 23 counties levy local "piggyback" income taxes at rates between 1.25 and 3.2 percent of Maryland taxable income. Local officials set the rates and the revenue is returned to the local governments quarterly. The top income tax bracket of 9.45 percent is the fifth highest combined state and local income tax rates in the country, behind New York City's 11.35 percent, California's 10.3 percent, Rhode Island's 9.9 percent, and Vermont's 9.5 percent.
Maryland's state sales tax is 6 percent. All real property in Maryland is subject to the property tax. Generally, properties that are owned and used by religious, charitable, or educational organizations or property owned by the federal, state or local governments are exempt. Property tax rates vary widely. No restrictions or limitations on property taxes are imposed by the state, meaning cities and counties can set tax rates at the level they deem necessary to fund governmental services. These rates can increase, decrease or remain the same from year to year. If the proposed tax rate increases the total property tax revenues, the governing body must advertise that fact and hold a public hearing on the new tax rate. This is called the Constant Yield Tax Rate process.
Biotechnology.
Maryland is a major center for life sciences research and development. With more than 400 biotechnology companies located there, Maryland is the fourth-largest nexus in this field in the United States.
Institutions and government agencies with an interest in research and development located in Maryland include the Johns Hopkins University, the Johns Hopkins Applied Physics Laboratory, more than one campus of the University System of Maryland, Goddard Space Flight Center, the United States Census Bureau, the National Institutes of Health (NIH), the National Institute of Standards and Technology (NIST), the National Institute of Mental Health (NIMH), the National Military Medical Center, the federal Food and Drug Administration (FDA), the Howard Hughes Medical Institute, the Celera Genomics company, the J. Craig Venter Institute (JCVI), and MedImmune - recently purchased by AstraZeneca.
Transportation.
The Maryland Department of Transportation, headquartered in the Hanover area of unincorporated Anne Arundel County, oversees most transportation in the state through its various administration-level agencies. The independent Maryland Transportation Authority, headquartered in Baltimore, maintains and operates the state's eight toll facilities.
Roads.
Maryland's Interstate highways include 110 mi of Interstate 95 (I-95), which enters the northeast portion of the state, travels through Baltimore, and becomes part of the eastern section of the Capital Beltway to the Woodrow Wilson Bridge. I-68 travels 81 mi, connecting the western portions of the state to I-70 at the small town of Hancock. I-70 enters from Pennsylvania north of Hancock and continues east for 93 mi to Baltimore, connecting Hagerstown and Frederick along the way.
I-83 has 34 mi in Maryland and connects Baltimore to southern central Pennsylvania (Harrisburg and York, Pennsylvania). Maryland also has an 11 mi portion of I-81 that travels through the state near Hagerstown. I-97, fully contained within Anne Arundel County and the second shortest (17.6 mi) one- or two-digit Interstate highway which connects the Baltimore area to the Annapolis area. Hawaii has one that is shorter.
There are also several auxiliary Interstate highways in Maryland. Among them are two beltways encircling the major cities of the region: I-695, the McKeldin (Baltimore) Beltway, which encircles Baltimore; and a portion of I-495, the Capital Beltway, which encircles Washington, D.C. I-270, which connects the Frederick area with Northern Virginia and the District of Columbia through major suburbs to the northwest of Washington, is a major commuter route and is as wide as fourteen lanes at points.
Both I-270 and the Capital Beltway are currently extremely congested; however, the Intercounty Connector (ICC; MD 200) is hoped to alleviate some of the congestion over time. Construction of the ICC was a major part of the campaign platform of former Governor Robert Ehrlich, who was in office from 2003 until 2007, and of Governor Martin O'Malley, who succeeded him. I-595, which is an unsigned highway concurrent with US 50/US 301, is the longest unsigned interstate in the country and connects Prince George's County and Washington D.C. with Annapolis and the Eastern Shore via the Chesapeake Bay Bridge.
Maryland also has a state highway system that contains routes numbered from 2 through 999, however most of the higher-numbered routes are either unsigned or are relatively short. Major state highways include Routes 2 (Governor Ritchie Highway/Solomons Island Road/Southern Maryland Blvd.), 4 (Pennsylvania Avenue/Southern Maryland Blvd./Patuxent Beach Road/St. Andrew's Church Road), 5 (Branch Avenue/Leonardtown Road/Point Lookout Road), 32, 45 (York Road), 97 (Georgia Avenue), 100 (Paul T. Pitcher Memorial Highway), 210 (Indian Head Highway), 235 (Three Notch Road), 295 (Baltimore-Washington Parkway), 355 (Wisconsin Avenue/Rockville Pike/Frederick Road), 404 (Queen Anne Highway/ Shore Highway), and 650 (New Hampshire Avenue).
Airports.
Maryland's largest airport is Baltimore-Washington International Thurgood Marshall Airport (known as Friendship Airport from its construction in 1950 and renamed in 2005 for the Baltimore-born Thurgood Marshall, the first African-American Supreme Court justice). The only other airports with commercial service are at Hagerstown and Salisbury.
The Maryland suburbs of Washington, D.C. are also served by the other two airports in the region, Ronald Reagan Washington National Airport and Dulles International Airport, both in Northern Virginia. The College Park Airport is the nation's oldest, founded in 1909, and is still used. Wilbur Wright trained military aviators at this location.
Rail.
Amtrak trains, including the high speed Acela Express serve Baltimore's Penn Station, BWI Airport, New Carrollton, and Aberdeen along the Washington D.C. to Boston Northeast Corridor. In addition, train service is provided to Rockville and Cumberland by Amtrak's Washington, D.C., to Chicago Capitol Limited.
The WMATA's Metrorail rapid transit and Metrobus local bus systems (the 2nd and 6th busiest in the nation of their respective modes) provide service in Montgomery and Prince George's counties and connect them to Washington D.C., with the express Metrobus "Route B30" serving BWI Airport. The Maryland Transit Administration (often abbreviated as "MTA Maryland"), a state agency part of the Maryland Department of Transportation also provides transit services within the state. Headquartered in Baltimore, MTA's transit services are largely focused on central Maryland, as well as some portions of the Eastern Shore and Southern MD. Baltimore's Light Rail and Metro Subway systems serve its densely populated inner-city and the surrounding suburbs. The MTA also serves the city and its suburbs with its local bus service (the 9th largest system in the nation). The MTA's Commuter Bus system provides express coach service on longer routes connecting Washington D.C. and Baltimore to parts of Central and Southern MD as well as the Eastern Shore. The commuter rail service, known as MARC, operates three lines which all terminate at Washington Union Station and provide service to Baltimore's Penn and Camden stations, Perryville, Frederick, and Martinsburg, WV. In addition, many suburban counties operate their own local bus systems which connect to and complement the larger MTA and WMATA/Metro services.
Freight rail transport is handled principally by two Class I railroads, as well as several smaller regional and local carriers. CSX Transportation has more extensive trackage throughout the state, with 560 mi, followed by Norfolk Southern Railway. Major rail yards are located in Baltimore and Cumberland, with an intermodal terminal (rail, truck and marine) in Baltimore.
Shipping canals.
The Chesapeake and Delaware Canal is a 14 mi canal on the Eastern Shore that connects the waters of the Delaware River with those of the Chesapeake Bay, and in particular with the Port of Baltimore, carrying 40 percent of the port's ship traffic.
Law and government.
The government of Maryland is conducted according to the state constitution. The government of Maryland, like the other 49 state governments, has exclusive authority over matters that lie entirely within the state's borders, except as limited by the Constitution of the United States.
Power in Maryland is divided among three branches of government: executive, legislative, and judicial. The Maryland General Assembly is composed of the Maryland House of Delegates and the Maryland Senate. Maryland's governor is unique in the United States as the office is vested with significant authority in budgeting. The legislature may not increase the governor's proposed budget expenditures. Unlike many other states, significant autonomy is granted to many of Maryland's counties.
Most of the business of government is conducted in Annapolis, the state capital. Elections for governor and most statewide offices, as well as most county elections, are held in midterm-election years (even-numbered years not divisible by four).
The judicial branch of state government consists of one united District Court of Maryland that sits in every county and Baltimore City, as well as 24 Circuit Courts sitting in each County and Baltimore City, the latter being courts of general jurisdiction for all civil disputes over $30,000.00, all equitable jurisdiction and major criminal proceedings. The intermediate appellate court is known as the Court of Special Appeals and the state supreme court is the Court of Appeals. The appearance of the judges of the Maryland Court of Appeals is unique; Maryland is the only state whose judges wear red robes.
Elections.
Since before the Civil War, Maryland's elections have been largely controlled by the Democrats, even as the party's platform has changed considerably in that time. State elections are dominated by Baltimore and the populous suburban counties bordering Washington, D.C.: Montgomery and Prince George's. Forty-three percent of the state's population resides in these three jurisdictions, each of which contain large, traditionally Democratic voting bloc(s): African Americans in Baltimore and Prince George's, federal employees in Prince George's and Montgomery, and postgraduates in Montgomery. The remainder of the state, particularly Western Maryland and the Eastern Shore, is more supportive of Republicans.
Maryland has supported the Democratic nominee in each of the last five presidential elections, by an average margin of 15.4 percent. In 1980, it was one of six states to vote for Jimmy Carter. In recent years, Maryland has been among the most reliable states for Democratic nominees. In 1992, Bill Clinton fared better in Maryland than any other state except his home state of Arkansas. In 1996, Maryland was Clinton's sixth best, in 2000 Maryland ranked fourth for Gore and in 2004 John Kerry showed his fifth best performance in Maryland. In 2008, Barack Obama won the state's 10 electoral votes with 61.9 percent of the vote to John McCain's 36.5 percent.
Both of Maryland's U.S. Senators and seven of its eight Representatives in Congress are Democrats, and Democrats hold a supermajority in the state Senate. The previous Governor, Robert Ehrlich, was the first Republican to be elected to that office in four decades, and after one term lost his seat to Baltimore Mayor Martin J. O'Malley, a Democrat. Ehrlich ran again for Governor in 2010, losing again to O'Malley.
While Republicans usually win more counties by piling up large margins in the west and east, they are usually swamped by the more densely populated and heavily Democratic Baltimore-Washington axis. In 2008, for instance, McCain won 17 counties to Obama's six; Obama also carried Baltimore City. While McCain won most of the western and eastern counties by margins of 2-to-1 or more, he was almost completely shut out in the larger counties surrounding Baltimore and Washington; every large county except Anne Arundel went for Obama.
U.S. Congressman Steny Hoyer (MD-5), a Democrat, was elected as Majority Leader for the 110th Congress of the House of Representatives, and 111th Congress, serving in that post from 2007 to 2011. His district covers parts of Anne Arundel and Prince George's counties, in addition to all of Charles, Calvert and St. Mary's counties in southern Maryland.
The 2006 election brought no significant change in this pattern of Democratic dominance. After Democratic Senator Paul Sarbanes announced that he was retiring, Democratic Congressman Benjamin Cardin defeated Republican Lieutenant Governor Michael S. Steele, with 55 percent of the vote, against Steele's 44 percent.
While Maryland is a Democratic Party stronghold, perhaps its best known political figure is a Republican – former Governor Spiro Agnew, who served as United States Vice President under Richard Nixon. He was Vice President from 1969 to 1973, when he resigned in the aftermath of revelations that he had taken bribes while he was Governor of Maryland. In late 1973, a court found Agnew guilty of violating tax laws.
In 2010 Republicans won control of most counties. The Democratic Party remained in control of eight county governments including Baltimore City.
In 2014, Larry Hogan, a Republican, was elected Governor of Maryland.
Education.
Primary and secondary education.
Education Week ranked Maryland #1 in its nationwide 2009-2013 Quality Counts reports. The College Board's 9th Annual AP Report to the Nation also ranked Maryland first. Primary and secondary education in Maryland is overseen by the Maryland State Department of Education, which is headquartered in Baltimore. The highest educational official in the state is the State Superintendent of Schools, who is appointed by the State Board of Education to a four-year term of office. The Maryland General Assembly has given the Superintendent and State Board autonomy to make educationally related decisions, limiting its own influence on the day-to-day functions of public education. Each county and county-equivalent in Maryland has a local Board of Education charged with running the public schools in that particular jurisdiction.
The budget for education was $5.5 billion in 2009, representing about 40 percent of the state's general fund.
Maryland has a broad range of private primary and secondary schools. Many of these are affiliated with various religious sects, including parochial schools of the Catholic Church, Quaker schools, Seventh-day Adventist schools, and Jewish schools. In 2003, Maryland law was changed to allow for the creation of publicly funded charter schools, although the charter schools must be approved by their local Board of Education and are not exempt from state laws on education, including collective bargaining laws.
In 2008, the state led the entire country in the percentage of students passing Advanced Placement examinations. 23.4 percent of students earned passing grades on the AP tests given in May 2008. This marks the first year that Maryland earned this honor. Three Maryland high schools (in Montgomery County) were ranked among the top 100 in the country by US News in 2009, based in large part on AP test scores.
Colleges and universities.
Maryland has several historic and renowned private colleges and universities, the most prominent of which is Johns Hopkins University, founded in 1876 with a grant from Baltimore entrepreneur Johns Hopkins.
The first public university in the state is the University of Maryland, Baltimore, which was founded in 1807 and contains the University of Maryland's only public academic health, human services, and one of two law centers (the other being the University of Baltimore School of Law). Seven professional and graduate schools train the majority of the state's physicians, nurses, dentists, lawyers, social workers, and pharmacists. The largest undergraduate institution in Maryland is the University of Maryland, College Park which was founded as the Maryland Agricultural College in 1856 and became a public land grant college in 1864. Towson University, founded in 1866, is the state's second largest university. Baltimore is home to the University of Maryland, Baltimore County and the Maryland Institute College of Art. The majority of public universities in the state are affiliated with the University System of Maryland. Two state-funded institutions, Morgan State University and St. Mary's College of Maryland, as well as two federally funded institutions, the Uniformed Services University of the Health Sciences and the United States Naval Academy, are not affiliated with the University System of Maryland.
St. John's College in Annapolis, Maryland and Washington College in Chestertown, Maryland, both private institutions, are the two oldest colleges in the state, and are among the oldest in the country. Other private institutions include Mount St. Mary's University, McDaniel College (formerly known as Western Maryland College), Hood College, Stevenson University (formerly known as Villa Julie College), Loyola University Maryland, and Goucher College, among others.
Public Libraries.
Maryland's 24 public library systems deliver public education for everyone in the state of Maryland through a curriculum that comprises three pillars: Self-Directed Education (books and materials in all formats, e-resources), Research Assistance & Instruction (individualized research assistance, classes for students of all ages), and Instructive & Enlightening Experiences (e.g., Summer Reading Clubs, author events). Many of the library systems have established formalized partnerships with other educational institutions in their counties and regions.
Sports.
With two major metropolitan areas, Maryland has a number of major and minor professional sports franchises. Two National Football League teams play in Maryland, the Baltimore Ravens in Baltimore City and the Washington Redskins in Landover. The Baltimore Colts represented the NFL in Baltimore from 1953 to 1983 before moving to Indianapolis.
The Baltimore Orioles are the state's Major League Baseball franchise. The National Hockey League's Washington Capitals and the National Basketball Association's Washington Wizards formerly played in Maryland, until the construction of an arena in Downtown D.C. in 1997 (originally known as MCI Center, renamed Verizon Center in 2006).
Maryland enjoys considerable historical repute for the talented sports players of its past, including Cal Ripken Jr. and Babe Ruth. In 2012, "The Baltimore Sun" published a list of Maryland's top ten athletes in the state's history. The list includes Ruth, Ripken, Johnny Unitas, Brooks Robinson, Frank Robinson, Ray Lewis, Michael Phelps, Jimmie Foxx, Jim Parker, and Wes Unseld.
Other professional sports franchises in the state include five affiliated minor league baseball teams, one independent league baseball team, the Baltimore Blast indoor soccer team, two indoor football teams,three low-level outdoor soccer teams, and the Chesapeake Bayhawks of Major League Lacrosse. Maryland is also home to one of the three races in horse racing's annual Triple Crown, the Preakness Stakes, which is run every spring at Pimlico Race Course in Baltimore.
The Congressional Country Club has hosted three golf tournaments for the U.S. Open and a PGA Championship.
The official state sport of Maryland, since 1962, is jousting; the official team sport since 2004 is lacrosse. The National Lacrosse Hall of Fame is located on the Johns Hopkins University campus in Baltimore. In 2008, intending to promote physical fitness for all ages, walking became the official state exercise. Maryland is the first state with an official state exercise.
Further reading.
</dl>

</doc>
<doc id="18859" url="http://en.wikipedia.org/wiki?curid=18859" title="Michigan">
Michigan

Michigan is a state in the Great Lakes region of the Midwestern United States. The name Michigan is the French form of the Ojibwa word "mishigamaa", meaning "large water" or "large lake". Michigan is the tenth most populous of the 50 United States, with the 11th most extensive total area (the largest state by total area east of the Mississippi River). Its capital is Lansing, and the largest city is Detroit.
Michigan is the only state to consist of two peninsulas. The Lower Peninsula, to which the name Michigan was originally applied, is often noted to be shaped like a mitten. The Upper Peninsula (often referred to as "the U.P.") is separated from the Lower Peninsula by the Straits of Mackinac, a 5 mi channel that joins Lake Huron to Lake Michigan. The two peninsulas are connected by the Mackinac Bridge. The state has the longest freshwater coastline of any political subdivision in the world, being bounded by four of the five Great Lakes, plus Lake Saint Clair. As a result, it is one of the leading U.S. states for recreational boating. Michigan also has 64,980 inland lakes and ponds, and a person in the state is never more than 6 mi from a natural water source or more than 85 mi from a Great Lakes shoreline.
What is now Michigan was first settled by various Native American tribes before being colonized by French explorers in the 17th century and becoming a part of New France. After the defeat of France in the French and Indian War in 1762 the region came under British rule, and was finally ceded to the newly independent United States after the British defeat in the American Revolutionary War. The area was organized as part of the larger Northwest Territory until 1800, when western Michigan became part of the Indiana Territory. Eventually, in 1805, the Michigan Territory was formed, which lasted until it was admitted into the Union on January 26, 1837, as the 26th state. The state of Michigan soon became an important center of industry and trade in the Great Lakes region and a popular immigrant destination.
Though Michigan has come to develop a diverse economy, it is widely known as the center of the U.S. automotive industry, being home to the country's three major automobile companies (whose headquarters are all located within the Detroit metropolitan area). While sparsely populated, the Upper Peninsula is economically important due to its status as a tourist destination as well as its abundance of natural resources, while the Lower Peninsula is a center of manufacturing, services, and high-tech industry.
History.
When the first European explorers arrived, the most populous tribes were Algonquian peoples, which include the Anishinaabe groups of Ojibwe (called "Chippewa" in French), Odaawaa/Odawa (Ottawa), and the Boodewaadamii/Bodéwadmi (Potawatomi). The three nations co-existed peacefully as part of a loose confederation called the Council of Three Fires. The Ojibwe, whose numbers are estimated to have been between 25,000 and 35,000, were the largest.
The Ojibwe were established in Michigan's Upper Peninsula and northern and central Michigan, and also inhabited Ontario, northern Wisconsin, southern Manitoba, and northern and north-central Minnesota. The Ottawa lived primarily south of the Straits of Mackinac in northern, western and southern Michigan, but also in southern Ontario, northern Ohio and eastern Wisconsin, while the Potawatomi were in southern and western Michigan, in addition to northern and central Indiana, northern Illinois, southern Wisconsin and southern Ontario. Other Algonquian tribes in Michigan, in the south and east, were the Mascouten, the Menominee, the Miami, the Sac (or Sauk), and the Fox, and the non-Algonquian Wyandot, who are better known by their French name, the Huron.
17th century.
French "voyageurs" and "coureurs des bois" explored and settled in Michigan in the 17th century. The first Europeans to reach what later became Michigan were those of Étienne Brûlé's expedition in 1622. The first permanent European settlement was founded in 1668 on the site where Père Jacques Marquette established Sault Ste. Marie, Michigan as a base for Catholic missions. Missionaries in 1671–75 founded outlying stations at Saint Ignace and Marquette. Jesuit missionaries were well received by the Indian populations in the area, with relatively few difficulties or hostilities. In 1679, Robert Cavelier, Sieur de la Salle built Fort Miami at present-day St. Joseph.
18th century.
In 1701, French explorer and army officer Antoine de la Mothe Cadillac founded Fort Pontchartrain du Détroit or "Fort Pontchartrain on-the-Strait" on the strait, known as the Detroit River, between lakes Saint Clair and Erie. Cadillac had convinced King Louis XIV's chief minister, Louis Phélypeaux, Comte de Pontchartrain, that a permanent community there would strengthen French control over the upper Great Lakes and discourage British aspirations.
The hundred soldiers and workers who accompanied Cadillac built a fort enclosing one arpent (about 0.85 acre, the equivalent of just under 200 ft per side) and named it Fort Pontchartrain. Cadillac's wife, Marie Thérèse Guyon, soon moved to Detroit, becoming one of the first European women to settle in the Michigan wilderness. The town quickly became a major fur-trading and shipping post. The "Église de Saint-Anne" (Church of Saint Ann) was founded the same year. While the original building does not survive, the congregation of that name continues to be active today. Cadillac later departed to serve as the French governor of Louisiana from 1710 to 1716. French attempts to consolidate the fur trade led to the Fox Wars involving the Meskwaki (Fox) and their allies versus the French and their Native allies.
At the same time, the French strengthened Fort Michilimackinac at the Straits of Mackinac to better control their lucrative fur-trading empire. By the mid-18th century, the French also occupied forts at present-day Niles and Sault Ste. Marie, though most of the rest of the region remained unsettled by Europeans. France offered free land in an effort to attract families to Detroit, which grew to 800 people in 1765, the largest city between Montreal and New Orleans.
From 1660 to the end of French rule, Michigan was part of the Royal Province of New France. In 1760, Montreal fell to the British forces ending the French and Indian War (1754–1763). Under the 1763 Treaty of Paris, Michigan and the rest of New France east of the Mississippi River passed to Great Britain. After the Quebec Act was passed in 1774, Michigan became part of the British Province of Quebec. By 1778, Detroit's population was up to 2,144 and it was the third largest city in Quebec.
During the American Revolutionary War, Detroit was an important British supply center. Most of the inhabitants were French-Canadians or Native Americans, many of whom had been allied with the French. Because of imprecise cartography and unclear language defining the boundaries in the 1783 Treaty of Paris, the British retained control of Detroit and Michigan after the American Revolution. When Quebec split into Lower and Upper Canada in 1791, Michigan was part of Kent County, Upper Canada. It held its first democratic elections in August 1792 to send delegates to the new provincial parliament at Newark (now Niagara-on-the-Lake).
Under terms negotiated in the 1794 Jay Treaty, Britain withdrew from Detroit and Michilimackinac in 1796. Questions remained over the boundary for many years, and the United States did not have uncontested control of the Upper Peninsula and Drummond Island until 1818 and 1847, respectively.
19th century.
During the War of 1812, Michigan Territory (effectively consisting of Detroit and the surrounding area) was surrendered after a nearly bloodless siege in 1812. An attempt to retake Detroit resulted in a severe American defeat in the River Raisin Massacre. This battle is still the bloodiest ever fought in the state and had the highest number of American casualties of any battle in the war. Ultimately, Michigan was recaptured by Americans in 1813 after the Battle of Lake Erie. An invasion of Canada which culminated in the Battle of the Thames was then launched from Michigan. The more northern areas were held by the British until the peace treaty restored the old boundaries. A number of forts, including Fort Wayne were built in Michigan during the 19th century out of fears of renewed fighting with Britain.
The population grew slowly until the opening in 1825 of the Erie Canal connecting the Great Lakes and the Hudson River and New York City. The new route brought a large influx of settlers, who became farmers and merchants and shipped out grain, lumber, and iron ore. By the 1830s, Michigan had 80,000 residents, more than enough to apply and qualify for statehood. In October 1835 the people approved the Constitution of 1835, thereby forming a state government, although Congressional recognition was delayed pending resolution of a boundary dispute with Ohio known as the Toledo War. Congress awarded the "Toledo Strip" to Ohio. Michigan received the western part of the Upper Peninsula as a concession and formally entered the Union on January 26, 1837. The Upper Peninsula proved to be a rich source of lumber, iron, and copper. Michigan led the nation in lumber production from the 1850s to the 1880s. Railroads became a major engine of growth from the 1850s onward, with Detroit the chief hub.
The first statewide meeting of the Republican Party took place July 6, 1854, in Jackson, Michigan, where the party adopted its platform. The state was heavily Republican until the 1930s. Michigan made a significant contribution to the Union in the American Civil War and sent more than forty regiments of volunteers to the federal armies.
Modernizers and boosters set up systems for public education, including founding the University of Michigan (1817; moved to Ann Arbor in 1837), for a classical academic education; and Michigan State Normal School, (1849) now Eastern Michigan University, for the training of teachers. In 1899, it became the first normal college in the nation to offer a four-year curriculum. Michigan Agricultural College (1855), now Michigan State University in East Lansing, was founded as the pioneer land-grant college, a model for those authorized under the Morrill Act (1862). Many other private colleges were founded as well, and the smaller cities formed high schools late in the century.
20th and 21st centuries.
Michigan's economy underwent a transformation at the turn of the 20th century. Many individuals, including Ransom E. Olds, John and Horace Dodge, Henry Leland, David Dunbar Buick, Henry Joy, Charles King, and Henry Ford, provided the concentration of engineering know-how and technological enthusiasm to start the birth of the automotive industry. Ford's development of the moving assembly line in Highland Park marked the beginning of a new era in transportation. Like the steamship and railroad, it was a far-reaching development. More than the forms of public transportation, the automobile transformed private life. It became the major industry of Detroit and Michigan, and permanently altered the socio-economic life of the United States and much of the world.
With the growth, the auto industry created jobs in Detroit that attracted immigrants from Europe and migrants from across the U.S., including those from the South. By 1920, Detroit was the fourth largest city in the U.S. Residential housing was in short supply, and it took years for the market to catch up with the population boom. By the 1930s, so many immigrants had arrived that more than 30 languages were spoken in the public schools, and ethnic communities celebrated in annual heritage festivals. Over the years immigrants and migrants contributed greatly to Detroit's diverse urban culture, including popular music trends, such as the influential Motown Sound of the 1960s led by a variety of individual singers and groups.
Grand Rapids, the second-largest city in Michigan, is also an important center of manufacturing. Since 1838, the city has also been noted for its furniture industry and is home to five of the world's leading office furniture companies. Grand Rapids is home to a number of major companies including Steelcase, Amway, and Meijer. Grand Rapids is also an important center for GE Aviation Systems.
Michigan held its first United States presidential primary election in 1910. With its rapid growth in industry, it was an important center of union industry-wide organizing, such as the rise of the United Auto Workers.
In 1920 WWJ (AM) in Detroit became the first radio station in the United States to regularly broadcast commercial programs. Throughout that decade, some of the country's largest and most ornate skyscrapers were built in the city. Particularly noteworthy are the Fisher Building, Cadillac Place, and the Guardian Building, each of which is a National Historic Landmark (NHL).
In 1927 a school bombing took place in Clinton County; the Bath School disaster, which resulted in the deaths of 38 schoolchildren, constitutes the deadliest mass murder in a school in U.S. history.
Michigan manufactured 10.9 percent of total United States military armaments produced during World War II, ranking second (behind New York) among the 48 states.
Detroit continued to expand through the 1950s, at one point doubling its population in a decade. After World War II, housing was developed in suburban areas outside city cores; newly constructed U.S. Interstate Highways allowed commuters to navigate the region more easily. Modern advances in the auto industry have resulted in increased automation, high tech industry, and increased suburban growth since 1960.
Michigan is the leading auto-producing state in the U.S., with the industry primarily located throughout the Midwestern United States, Ontario, Canada, and the Southern United States. With almost ten million residents, Michigan is a large and influential state, ranking eighth in population among the fifty states. Detroit is the centrally located metropolitan area of the Great Lakes Megalopolis and the second largest metropolitan area in the U.S. linking the Great Lakes system.
The Metro Detroit area in Southeast Michigan is the largest metropolitan area in the state (roughly 50% of the population resides there) and the eleventh largest in the USA. The Grand Rapids metropolitan area in Western Michigan is the fastest-growing metro area in the state, with over 1.3 million residents as of 2006. Metro Detroit receives more than 15 million visitors each year. Michigan has many popular tourist destinations which include areas such as Traverse City on the Grand Traverse Bay in Northern Michigan. Tourists spend about $17 billion annually in Michigan supporting 193,000 jobs.
Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the U.S. The state's leading research institutions include the University of Michigan, Michigan State University and Wayne State University which are important partners in the state's economy and the state's University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. Agriculture also serves a significant role making the state a leading grower of fruit in the U.S., including blueberries, cherries, apples, grapes and peaches.
Government.
State government.
Michigan is governed as a republic, with three branches of government: the executive branch consisting of the Governor of Michigan and the other independently elected constitutional officers; the legislative branch consisting of the House of Representatives and Senate; and the judicial branch. The Michigan Constitution allows for the direct participation of the electorate by statutory initiative and referendum, recall, and constitutional initiative and referral (Article II, § 9, defined as "the power to propose laws and to enact and reject laws, called the initiative, and the power to approve or reject laws enacted by the legislature, called the referendum. The power of initiative extends only to laws which the legislature may enact under this constitution"). Lansing is the state capital and is home to all three branches of state government.
The governor and the other state constitutional officers serve four-year terms and may be re-elected only once. The current governor is Rick Snyder. Michigan has two official Governor's Residences; one is in Lansing, and the other is at Mackinac Island. The other constitutionally elected executive officers are the lieutenant governor, who is elected on a joint ticket with the governor, the secretary of state, and the attorney general. The lieutenant governor presides over the Senate, but only voting when ties occur, and is also a member of the cabinet. The secretary of state is the chief elections officer and is charged with running many licensure programs including motor vehicles, all of which are done through the branch offices of the secretary of state.
The Michigan Legislature consists of a 38-member Senate and 110-member House of Representatives. Members of both houses of the legislature are elected through first past the post elections by single-member electoral districts of near-equal population that often have boundaries which coincide with county and municipal lines. Senators serve four-year terms concurrent to those of the governor, while representatives serve two-year terms. The Michigan State Capitol was dedicated in 1879 and has hosted the executive and legislative branches of the state ever since.
The Michigan judiciary consists of two courts with primary jurisdiction (the Circuit Courts and the District Courts), one intermediate level appellate court (the Michigan Court of Appeals), and the Michigan Supreme Court. There are several administrative courts and specialized courts. District courts are trial courts of limited jurisdiction, handling most traffic violations, small claims, misdemeanors, and civil suits where the amount contended is below $25,000. District courts are often responsible for handling the preliminary examination and for setting bail in felony cases. District court judges are elected to terms of six years. In a few locations, municipal courts have been retained to the exclusion of the establishment of district courts. There are 57 circuit courts in the State of Michigan, which have original jurisdiction over all civil suits where the amount contended in the case exceeds $25,000 and all criminal cases involving felonies. Circuit courts are also the only trial courts in the State of Michigan which possess the power to issue equitable remedies. Circuit courts have appellate jurisdiction from district and municipal courts, as well as from decisions and decrees of state agencies. Most counties have their own circuit court, but sparsely populated counties often share them. Circuit court judges are elected to terms of six years. State appellate court judges are elected to terms of six years, but vacancies are filled by an appointment by the governor. There are four divisions of the Court of Appeals, being located in Detroit, Grand Rapids, Lansing, and Marquette. Cases are heard by the Court of Appeals by panels of three judges, who examine the application of the law and not the facts of the case, unless there has been grievous error pertaining to questions of fact. The Michigan Supreme Court consists of seven members who are elected on non-partisan ballots for staggered eight-year terms. The Supreme Court has original jurisdiction only in narrow circumstances, but holds appellate jurisdiction over the entire state judicial system.
Law.
Michigan has had four constitutions, the first of which was ratified on October 5 and 6, 1835. There were also constitutions from 1850 and 1908, in addition to the current constitution from 1963. The current document has a preamble, 11 articles, and one section consisting of a schedule and temporary provisions. Michigan, like every US state except Louisiana, has a common law legal system.
Politics.
Voters in the state elect candidates from both major parties. Economic issues are important in Michigan elections.
The three-term Republican Governor John Engler (1991–2003) preceded the former two-term Democratic Governor Jennifer Granholm (2003–2011). The state has elected successive Republican attorneys general twice since 2003. The Republican Party currently holds a majority in both the House and Senate of the Michigan Legislature. Michigan supported the election of Republican Presidents Ronald Reagan and George H. W. Bush. The current Governor Rick Snyder (2011–present) is a Republican.
However, the state has supported Democrats in the last six presidential election cycles. In 2012, Barack Obama carried the state over Mitt Romney, winning Michigan's 17 electoral votes with 54% of the vote. Michigan's two U.S. Senators are both Democrats, while Republicans hold nine of the state's fourteen U.S. House seats. Michigan's current senior U.S. Senator Debbie Stabenow, a Democrat, has served since 2001 after narrowly beating former Republican U.S. Senator Spencer Abraham in the 2000 elections. Democratic U.S. Senator Gary Peters was elected in 2014, beating former Republican Michigan Secretary of State Terri Lynn Land. Congressman Fred Upton, a Republican, serves as Chairman of the U.S. House Committee on Energy and Commerce and Congressman Mike Rogers, also a Republican, serves as Chairman of U.S. House Permanent Select Committee on Intelligence. Congresswoman Debbie Dingell, a Democrat, became the first person to succeed a living spouse when she replaced fomer Dean of the House of Representatives John Dingell in 2015.
Republican strongholds of the state include rural areas of western, northern, central, and the Grand Rapids metro area.
Suburban Detroit, which includes Oakland, Macomb, Livingston, and a large portion of Wayne County has a mixed voting history.
Democratic strongholds include Wayne County, which includes the city of Detroit, Washtenaw County (Ann Arbor), Ingham County (Lansing), and Genesee County (Flint).
Historically, the first county-level meeting of the Republican Party took place in Jackson on July 6, 1854, and the party thereafter dominated Michigan until the Great Depression. In the 1912 election, Michigan was one of the six states to support progressive Republican and third-party candidate Theodore Roosevelt for president after he lost the Republican nomination to William Howard Taft.
Michigan remained fairly reliably Republican at the presidential level for much of the 20th century. It was part of Greater New England, the northern tier of states settled chiefly by migrants from New England who carried their culture with them. The state was one of only a handful to back Wendell Willkie over Franklin Roosevelt in 1940, and supported Thomas E. Dewey in his losing bid against Harry S. Truman in 1948. Michigan went to the Democrats in presidential elections during the 1960s, and voted for Republican Richard Nixon in 1972.
Michigan was the home of Gerald Ford, the 38th President of the United States. He was born in Nebraska and moved as an infant to Grand Rapids and grew up there. The Gerald R. Ford Museum is located in Grand Rapids, and the Gerald R. Ford Presidential Library is located on the campus of his alma mater, the University of Michigan in Ann Arbor.
In 1846 Michigan became the first state in the Union, as well as the first English-speaking government in the world, to abolish the death penalty. Historian David Chardavoyne has suggested that the movement to abolish capital punishment in Michigan grew as a result of enmity toward the state's neighbor, Canada. Under British rule, it made public executions a regular practice.
Michigan does not recognize or perform same-sex marriages or other unions involving same-sex couples under a 2004 state constitutional amendment; however, the legality of same-sex marriage in the state of Michigan is under debate following a May 21, 2014, ruling in the United States District Court for the Eastern District of Michigan making the 2004 state amendment invalid and rendering gay marriage legal. An appeal by state Attorney General Bill Schuette to the Sixth Circuit Court successfully reversed that decision, making same-sex marriage once again illegal in the state. The U.S. Supreme Court has agreed to hear the Michigan case, along with cases from three other states, with a ruling expected by June 2015.
Michigan has approved plans to expand Medicaid coverage in 2014 to adults with incomes up to 133% of the federal poverty level (approximately $15,500 for a single adult in 2014).
Administrative divisions.
State government is decentralized among three tiers—statewide, county and township. Counties are administrative divisions of the state, and townships are administrative divisions of a county. Both of them exercise state government authority, localized to meet the particular needs of their jurisdictions, as provided by state law. There are 83 counties in Michigan.
Cities, state universities, and villages are vested with home rule powers of varying degrees. Home rule cities can generally do anything that is not prohibited by law. The fifteen state universities have broad power and can do anything within the parameters of their status as educational institutions that is not prohibited by the state constitution. Villages, by contrast, have limited home rule and are not completely autonomous from the county and township in which they are located.
There are two types of township in Michigan: "general law" township and "charter". Charter township status was created by the Legislature in 1947 and grants additional powers and stream-lined administration in order to provide greater protection against annexation by a city. As of April 2001, there were 127 charter townships in Michigan. In general, charter townships have many of the same powers as a city but without the same level of obligations. For example, a charter township can have its own fire department, water and sewer department, police department, and so on—just like a city—but it is not "required" to have those things, whereas cities "must" provide those services. Charter townships can opt to use county-wide services instead, such as deputies from the county sheriff's office instead of a home-based force of ordinance officers.
Geography.
Marquette Park on Mackinac Island
Michigan consists of two peninsulas that lie between 82°30' to about 90°30' west longitude, and are separated by the Straits of Mackinac. The 45th parallel north runs through the state—marked by highway signs and the Polar-Equator Trail—along a line including Mission Point Light near Traverse City, the towns of Gaylord and Alpena in the Lower Peninsula and Menominee in the Upper Peninsula. With the exception of two small areas that are drained by the Mississippi River by way of the Wisconsin River in the Upper Peninsula and by way of the Kankakee-Illinois River in the Lower Peninsula, Michigan is drained by the Great Lakes-St. Lawrence watershed and is the only state with the majority of its land thus drained.
The Great Lakes that border Michigan from east to west are Lake Erie, Lake Huron, Lake Michigan and Lake Superior. It has more lighthouses than any other state. The state is bounded on the south by the states of Ohio and Indiana, sharing land and water boundaries with both. Michigan's western boundaries are almost entirely water boundaries, from south to north, with Illinois and Wisconsin in Lake Michigan; then a land boundary with Wisconsin and the Upper Peninsula, that is principally demarcated by the Menominee and Montreal Rivers; then water boundaries again, in Lake Superior, with Wisconsin and Minnesota to the west, capped around by the Canadian province of Ontario to the north and east.
The heavily forested Upper Peninsula is relatively mountainous in the west. The Porcupine Mountains, which are part of one of the oldest mountain chains in the world, rise to an altitude of almost 2,000 feet (610 m) above sea level and form the watershed between the streams flowing into Lake Superior and Lake Michigan. The surface on either side of this range is rugged. The state's highest point, in the Huron Mountains northwest of Marquette, is Mount Arvon at 1979 ft. The peninsula is as large as Connecticut, Delaware, Massachusetts, and Rhode Island combined but has fewer than 330,000 inhabitants. They are sometimes called "Yoopers" (from "U.P.'ers"), and their speech (the "Yooper dialect") has been heavily influenced by the numerous Scandinavian and Canadian immigrants who settled the area during the lumbering and mining boom of the late 19th century.
The Lower Peninsula is shaped like a mitten and many residents hold up a hand to depict where they are from. It is 277 mi long from north to south and 195 mi from east to west and occupies nearly two-thirds of the state's land area. The surface of the peninsula is generally level, broken by conical hills and glacial moraines usually not more than a few hundred feet tall. It is divided by a low water divide running north and south. The larger portion of the state is on the west of this and gradually slopes toward Lake Michigan. The highest point in the Lower Peninsula is either Briar Hill at 1705 ft, or one of several points nearby in the vicinity of Cadillac. The lowest point is the surface of Lake Erie at 571 ft.
The geographic orientation of Michigan's peninsulas makes for a long distance between the ends of the state. Ironwood, in the far western Upper Peninsula, lies 630 highway miles (1,015 km) from Lambertville in the Lower Peninsula's southeastern corner. The geographic isolation of the Upper Peninsula from Michigan's political and population centers makes the U.P. culturally and economically distinct. Occasionally U.P. residents have called for secession from Michigan and establishment as a new state to be called "Superior".
A feature of Michigan that gives it the distinct shape of a mitten is the Thumb. This peninsula projects out into Lake Huron and the Saginaw Bay. The geography of the Thumb is mainly flat with a few rolling hills. Other peninsulas of Michigan include the Keweenaw Peninsula, making up the Copper Country region of the state. The Leelanau Peninsula lies in the Northern Lower Michigan region. "See Also Michigan Regions"
Numerous lakes and marshes mark both peninsulas, and the coast is much indented. Keweenaw Bay, Whitefish Bay, and the Big and Little Bays De Noc are the principal indentations on the Upper Peninsula. The Grand and Little Traverse, Thunder, and Saginaw bays indent the Lower Peninsula. Michigan has the second longest shoreline of any state—3288 mi, including 1056 mi of island shoreline.
The state has numerous large islands, the principal ones being the North Manitou and South Manitou, Beaver, and Fox groups in Lake Michigan; Isle Royale and Grande Isle in Lake Superior; Marquette, Bois Blanc, and Mackinac islands in Lake Huron; and Neebish, Sugar, and Drummond islands in St. Mary's River. Michigan has about 150 lighthouses, the most of any U.S. state. The first lighthouses in Michigan were built between 1818 and 1822. They were built to project light at night and to serve as a landmark during the day to safely guide the passenger ships and freighters traveling the Great Lakes. See Lighthouses in the United States.
The state's rivers are generally small, short and shallow, and few are navigable. The principal ones include the Detroit River, St. Marys River, and St. Clair River which connect the Great Lakes; the Au Sable, Cheboygan, and Saginaw, which flow into Lake Huron; the Ontonagon, and Tahquamenon, which flow into Lake Superior; and the St. Joseph, Kalamazoo, Grand, Muskegon, Manistee, and Escanaba, which flow into Lake Michigan. The state has 11,037 inland lakes (totaling 1305 sqmi of inland water) in addition to 38575 sqmi of Great Lakes waters. No point in Michigan is more than six miles (10 km) from an inland lake or more than 85 mi from one of the Great Lakes.
The state is home to a number of areas maintained by the National Park Service including: Isle Royale National Park, located in Lake Superior, about 30 mi southeast of Thunder Bay, Ontario. Other national protected areas in the state include: Keweenaw National Historical Park, Pictured Rocks National Lakeshore, Sleeping Bear Dunes National Lakeshore, Huron National Forest, Manistee National Forest, Hiawatha National Forest, Ottawa National Forest and Father Marquette National Memorial. The largest section of the North Country National Scenic Trail passes through Michigan.
With 78 state parks, 19 state recreation areas, and 6 state forests, Michigan has the largest state park and state forest system of any state. These parks and forests include Holland State Park, Mackinac Island State Park, Au Sable State Forest, and Mackinaw State Forest.
Climate.
Michigan has a continental climate, although there are two distinct regions. The southern and central parts of the Lower Peninsula (south of Saginaw Bay and from the Grand Rapids area southward) have a warmer climate (Köppen climate classification "Dfa") with hot summers and cold winters. The northern part of Lower Peninsula and the entire Upper Peninsula has a more severe climate (Köppen "Dfb"), with warm, but shorter summers and longer, cold to very cold winters. Some parts of the state average high temperatures below freezing from December through February, and into early March in the far northern parts. During the winter through the middle of February the state is frequently subjected to heavy lake-effect snow. The state averages from 30 - of precipitation annually, however some areas in the northern lower peninsula and the upper peninsula average almost 160" of snowfall per year. Michigan's highest recorded temperature is 112 F at Mio on July 13, 1936, and the coldest recorded temperature is -51 F at Vanderbilt on February 9, 1934.
The entire state averages 30 days of thunderstorm activity per year. These can be severe, especially in the southern part of the state. The state averages 17 tornadoes per year, which are more common in the extreme southern portion of the state. Portions of the southern border have been almost as vulnerable historically as states further west and in Tornado Alley. For this reason, many communities in the very southern portions of the state are equipped with tornado sirens to warn residents of approaching tornadoes. Farther north, in Northern Michigan and the Upper Peninsula, tornadoes are rare.
Geology.
The geological formation of the state is greatly varied. Primary boulders are found over the entire surface of the Upper Peninsula (being principally of primitive origin), while Secondary deposits cover the entire Lower Peninsula. The Upper Peninsula exhibits Lower Silurian sandstones, limestones, copper and iron bearing rocks, corresponding to the Huronian system of Canada. The central portion of the Lower Peninsula contains coal measures and rocks of the Pennsylvanian period. Devonian and sub-Carboniferous deposits are scattered over the entire state.
Michigan rarely experiences earthquakes, thus far mostly smaller ones that do not cause significant damage. A 4.6-magnitude earthquake struck in August 1947. More recently, a 4.2-magnitude earthquake occurred on Saturday, May 2, 2015, shortly after noon, about 5 miles south of Galesburg, Michigan (9 miles southeast of Kalamazoo) in central Michigan, about 140 miles west of Detroit, according to the Colorado-based U.S. Geological Survey's National Earthquake Information Center. No major damage or injuries were reported, according to Governor Rick Snyder's office.
Demographics.
Population.
The United States Census Bureau estimates that the population of Michigan was 9,909,877 on July 1, 2014, a 0.27% increase since the 2010 United States Census.
According to the United States Census Bureau, Michigan's population was 9,938,444 in April 2000. As of the 2010 census, the state's population was 9,883,635, making Michigan the only American state to see a population decrease between 2000 and 2010.
The center of population of Michigan is located in Shiawassee County, in the southeastern corner of the civil township of Bennington, which is located northwest of the village of Morrice.
As of the 2010 American Community Survey for the U.S. Census, the state had a foreign-born population of 592,212, or 6.0% of the total. Michigan has the largest Dutch, Finnish, and Macedonian populations in the United States.
The 2010 Census reported:
In the same year Hispanics or Latinos (of any race) made up 4.4% of the population.
The large majority of Michigan's population is Caucasian. Americans of European descent live throughout Michigan and most of Metro Detroit. Large European American groups include those of German, British, Irish, Polish and Belgian ancestry. People of Scandinavian descent, and those Finnish ancestry, have a notable presence in the Upper Peninsula. Western Michigan is known for the Dutch heritage of many residents (the highest concentration of any state), especially in Holland and metropolitan Grand Rapids.
African Americans, who came to Detroit and other northern cities in the Great Migration of the early 20th century, form a majority of the population of the city of Detroit and of other cities, including Flint and Benton Harbor.
As of 2011, 34.3% of Michigan's children under the age of one belonged to racial or ethnic minority groups, meaning that they had at least one parent who was not non-Hispanic white.
As of 2007 about 300,000 people in Southeastern Michigan trace their descent from the Middle East. Dearborn has a sizeable Arab community, with many Assyrian/Chaldean/Syriac, and Lebanese who immigrated for jobs in the auto industry in the 1920s along with more recent Yemenis and Iraqis.
As of 2007 almost 8,000 Hmong people lived in the State of Michigan, about double their 1999 presence in the state. Most live in northeastern Detroit, but they have been increasingly moving to Pontiac and Warren. Lansing hosts a statewide Hmong New Year Festival.
As of April 2013, the largest Japanese national population is in Novi, with 2,666 Japanese residents, and the next largest populations are respectively in Ann Arbor, West Bloomfield Township, Farmington Hills, and Battle Creek. The state has 481 Japanese employment facilities providing 35,554 local jobs. 391 of them are in Southeast Michigan, providing 20,816 jobs, and the 90 in other regions in the state provide 14,738 jobs. The Japanese Direct Investment Survey of the Consulate-General of Japan, Detroit stated that over 2,208 additional Japanese residents were employed in the State of Michigan as of October 1, 2012, than in 2011.
A person from Michigan is called a Michigander or Michiganian; also at times, but rarely, a "Michiganite". Residents of the Upper Peninsula are sometimes referred to as "Yoopers" (a phonetic pronunciation of "U.P.ers"), and Upper Peninsula residents sometimes refer to those from the Lower Peninsula as "trolls" because they live below the bridge.
Languages.
As of 2010, 91.11% (8,507,947) of Michigan residents age 5 and older spoke English at home as a primary language, while 2.93% (273,981) spoke Spanish, 1.04% (97,559) Arabic, 0.44% (41,189) German, 0.36% (33,648) Chinese (which includes Mandarin), 0.31% (28,891) French, 0.29% (27,019) Polish, and Syriac languages (such as Modern Aramaic and Northeastern Neo-Aramaic) was spoken as a main language by 0.25% (23,420) of the population over the age of five. In total, 8.89% (830,281) of Michigan's population age 5 and older spoke a mother language other than English.
Religion.
The Catholic Church has six dioceses and one archdiocese in Michigan, the Diocese of Gaylord, Diocese of Grand Rapids, Diocese of Kalamazoo, Diocese of Lansing, Diocese of Marquette, Diocese of Saginaw and Archdiocese of Detroit. The Catholic Church is the largest denomination by number of adherents, according to the Association of Religion Data Archives (ARDA) 2010 survey, with 1,717,296 adherents. The Roman Catholic Church was the only organized religion in Michigan until the 19th century, reflecting the territory's French colonial roots. Detroit's St. Anne's parish, established in 1701 by Antoine de la Mothe Cadillac, is the second-oldest Catholic parish in the country. On March 8, 1833, the Holy See formally established a diocese in the Michigan territory, which included all of Michigan, Wisconsin, Minnesota, and the Dakotas east of the Mississippi River. When Michigan became a state in 1837, the boundary of the Diocese of Detroit was redrawn to coincide with that of the State; the other dioceses were later carved out from the Diocese of Detroit but remain part of the Ecclesiastical Province of Detroit.
In 2010, the largest Protestant denominations were the United Methodist Church with 228,521 adherents; followed by the Lutheran Church–Missouri Synod with 219,618; and the Evangelical Lutheran Church in America with 120,598 adherents. The Christian Reformed Church in North America had almost 100,000 members and over 230 congregations in Michigan. The Reformed Church in America had 76,000 members and 154 congregations in the state. In the same survey, Jewish adherents in the state of Michigan were estimated at 44,382, and Muslims at 120,351. The Lutheran Church was introduced by German and Scandinavian immigrants; Lutheranism is the second largest religious denomination in the state. The first Jewish synagogue in the state was Temple Beth El, founded by twelve German Jewish families in Detroit in 1850. In West Michigan, Dutch immigrants fled from the specter of religious persecution and famine in the Netherlands around 1850 and settled in and around what is now Holland, Michigan, establishing a "colony" on American soil that fervently held onto Calvinist doctrine that established a significant presence of Reformed churches. Islam was introduced by immigrants from the Near East during the 20th century. It is also home to the largest mosque in North America, the Islamic Center of America in Dearborn. Battle Creek, Michigan is also the birthplace of the Seventh-day Adventist Church, which was founded on May 21, 1863.
Economy.
The U.S. Economic Development Administration estimated Michigan's 2013 gross state product to be $408.218 billion, ranking 13th out of the 50 states. According to the Bureau of Labor Statistics, as of February 2015, the state's seasonally adjusted unemployment rate is estimated at 5.9%.
Products and services include automobiles, food products, information technology, aerospace, military equipment, furniture, and mining of copper and iron ore. Michigan is the third leading grower of Christmas trees with 60520 acre of land dedicated to Christmas tree farming. The beverage Vernors was invented in Michigan in 1866, sharing the title of oldest soft drink with Hires Root Beer. Faygo was founded in Detroit on November 4, 1907. Two of the top four pizza chains were founded in Michigan and are headquartered there: Domino's Pizza by Tom Monaghan and Little Caesars Pizza by Mike Ilitch. Michigan became the 24th Right to Work state in U.S. in 2012.
Since 2009, GM, Ford and Chrysler have managed a significant reorganization of their benefit funds structure after a volatile stock market which followed the September 11 attacks and early 2000s recession impacted their respective U.S. pension and benefit funds (OPEB). General Motors, Ford, and Chrysler reached agreements with the United Auto Workers Union to transfer the liabilities for their respective health care and benefit funds to a 501(c)(9) Voluntary Employee Beneficiary Association (VEBA). Manufacturing in the state grew 6.6% from 2001 to 2006, but the high speculative price of oil became a factor for the U.S. auto industry during the economic crisis of 2008 impacting industry revenues. In 2009, GM and Chrysler emerged from Chapter 11 restructurings with financing provided in part by the U.S. and Canadian governments. GM began its initial public offering (IPO) of stock in 2010. For 2010, the Big Three domestic automakers have reported significant profits indicating the beginning of rebound.
s of 2002[ [update]], Michigan ranked fourth in the U.S. in high tech employment with 568,000 high tech workers, which includes 70,000 in the automotive industry. Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the United States. Its research and development, which includes automotive, comprises a higher percentage of the state's overall gross domestic product than for any other U.S. state. The state is an important source of engineering job opportunities. The domestic auto industry accounts directly and indirectly for one of every ten jobs in the U.S.
Michigan was second in the U.S. in 2004 for new corporate facilities and expansions. From 1997 to 2004, Michigan was the only state to top the 10,000 mark for the number of major new developments; however, the effects of the late 2000s recession have slowed the state's economy. In 2008, Michigan placed third in a site selection survey among the states for luring new business which measured capital investment and new job creation per one million population. In August 2009, Michigan and Detroit's auto industry received $1.36 B in grants from the U.S. Department of Energy for the manufacture of electric vehicle technologies which is expected to generate 6,800 immediate jobs and employ 40,000 in the state by 2020. From 2007 to 2009, Michigan ranked 3rd in the U.S. for new corporate facilities and expansions.
As leading research institutions, the University of Michigan, Michigan State University, and Wayne State University are important partners in the state's economy and its University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. The National Superconducting Cyclotron Laboratory is located at Michigan State University. Michigan's workforce is well-educated and highly skilled, making it attractive to companies. It has the third highest number of engineering graduates nationally.
Detroit Metropolitan Airport is one of the nation's most recently expanded and modernized airports with six major runways, and large aircraft maintenance facilities capable of servicing and repairing a Boeing 747 and is a major hub for Delta Air Lines. Michigan's schools and colleges rank among the nation's best. The state has maintained its early commitment to public education. The state's infrastructure gives it a competitive edge; Michigan has 38 deep water ports. In 2007, Bank of America announced that it would commit $25 billion to community development in Michigan following its acquisition of LaSalle Bank in Troy.
Michigan led the nation in job creation improvement in 2010.
Taxation.
Michigan's personal income tax is set to a flat rate of 4.25%. In addition, 22 cities impose income taxes; rates are set at 1% for residents and 0.5% for non-residents in all but four cities. Michigan's state sales tax is 6%, though items such as food and medication are exempted from sales tax. Property taxes are assessed on the local level, but every property owner's local assessment contributes six mills (a rate of $6 per $1000 of property value) to the statutory State Education Tax. Property taxes are appealable to local boards of review and need the approval of the local electorate to exceed millage rates prescribed by state law and local charters. In 2011, the state repealed its business tax and replaced it with a 6% corporate income tax which substantially reduced taxes on business. Article IX of the Constitution of the State of Michigan also provides limitations on how much the state can tax.
Agriculture.
A wide variety of commodity crops, fruits, and vegetables are grown in Michigan, making it second only to California among U.S. states in the diversity of its agriculture. The state has 54,800 farms utilizing 10000000 acre of land which sold $6.49 billion worth of products in 2010. The most valuable agricultural product is milk. Leading crops include corn, soybeans, flowers, wheat, sugar beets and potatoes. Livestock in the state included 1 million cattle, 1 million hogs, 78,000 sheep and over 3 million chickens. Livestock products accounted for 38% of the value of agricultural products while crops accounted for the majority.
Michigan is a leading grower of fruit in the U.S., including blueberries, cherries, apples, grapes, and peaches. Plums, pears, and strawberries are also grown. These fruits are mainly grown in West Michigan due to the moderating effect of Lake Michigan on the climate. There is also significant fruit production, especially cherries, but also grapes, apples, and other fruits, in Northwest Michigan along Lake Michigan. Michigan produces wines, beers and a multitude of processed food products. Kellogg's cereal is based in Battle Creek, Michigan and processes many locally grown foods. Thornapple Valley, Ball Park Franks, Koegel Meat Company, and Hebrew National sausage companies are all based in Michigan.
Michigan is home to very fertile land in the Flint/Tri-Cities and "Thumb" areas. Products grown there include corn, sugar beets, navy beans, and soy beans. Sugar beet harvesting usually begins the first of October. It takes the sugar factories about five months to process the 3.7 million tons of sugarbeets into 970 million pounds of pure, white sugar. Michigan's largest sugar refiner, Michigan Sugar Company is the largest east of the Mississippi River and the fourth largest in the nation. Michigan Sugar brand names are Pioneer Sugar and the newly incorporated Big Chief Sugar. Potatoes are grown in Northern Michigan, and corn is dominant in Central Michigan. Alfalfa, cucumbers, and asparagus are also grown.
Tourism.
Michigan's tourists spend $17.2 billion per year in the state, supporting 193,000 tourism jobs. Michigan's tourism website ranks among the busiest in the nation. Destinations draw vacationers, hunters, and nature enthusiasts from across the United States and Canada. Michigan is fifty percent forest land, much of it quite remote. The forests, lakes and thousands of miles of beaches are top attractions. Event tourism draws large numbers to occasions like the Tulip Time Festival and the National Cherry Festival.
In 2006, the Michigan State Board of Education mandated that all public schools in the state hold their first day of school after the Labor Day holiday, in accordance with the new Post Labor Day School law. A survey found that 70% of all tourism business comes directly from Michigan residents, and the Michigan Hotel, Motel, & Resort Association claimed that the shorter summer in between school years cut into the annual tourism season in the state.
Tourism in metropolitan Detroit draws visitors to leading attractions, especially The Henry Ford, the Detroit Institute of Arts, the Detroit Zoo, and to sports in Detroit. Other museums include the Detroit Historical Museum, the Charles H. Wright Museum of African American History, museums in the Cranbrook Educational Community, and the Arab American National Museum. The metro area offers four major casinos, MGM Grand Detroit, Greektown, Motor City, and Caesars Windsor in Windsor, Ontario, Canada; moreover, Detroit is the largest American city and metropolitan region to offer casino resorts.
Hunting and fishing are significant industries in the state. Charter boats are based in many Great Lakes cities to fish for salmon, trout, walleye and perch. Michigan ranks first in the nation in licensed hunters (over one million) who contribute $2 billion annually to its economy. Over three-quarters of a million hunters participate in white-tailed deer season alone. Many school districts in rural areas of Michigan cancel school on the opening day of firearm deer season, because of attendance concerns.
Michigan's Department of Natural Resources manages the largest dedicated state forest system in the nation. The forest products industry and recreational users contribute $12 billion and 200,000 associated jobs annually to the state's economy. Public hiking and hunting access has also been secured in extensive commercial forests. The state has the highest number of golf courses and registered snowmobiles in the nation.
The state has numerous historical markers, which can themselves become the center of a tour. The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.
With its position in relation to the Great Lakes and the countless ships that have foundered over the many years in which they have been used as a transport route for people and bulk cargo, Michigan is a world-class scuba diving destination. The Michigan Underwater Preserves are 11 underwater areas where wrecks are protected for the benefit of sport divers.
Transportation.
Canadian international crossings.
Michigan has nine international crossings with Ontario, Canada:
A second international bridge is currently under consideration between Detroit, Michigan and Windsor, Ontario.
Railroads.
Michigan is served by four Class I railroads: the Canadian National Railway, the Canadian Pacific Railway, CSX Transportation, and the Norfolk Southern Railway. These are augmented by several dozen short line railroads. The vast majority of rail service in Michigan is devoted to freight, with Amtrak and various scenic railroads the exceptions.
Amtrak passenger rail services the state, connecting many southern and western Michigan cities to Chicago, Illinois. There are plans for commuter rail for Detroit and its suburbs (see SEMCOG Commuter Rail).
Roadways.
Interstate 75 (I-75) is the main thoroughfare between Detroit, Flint, and Saginaw extending north to Sault Sainte Marie and providing access to Sault Ste. Marie, Ontario. The freeway crosses the Mackinac Bridge between the Lower and Upper Peninsulas. Branching highways include I-275 and I-375 in Detroit; I-475 in Flint; and I-675 in Saginaw.
I-69 enters the state near the Michigan–Ohio–Indiana border, and it extends to Port Huron and provides access to the Blue Water Bridge crossing into Sarnia, Ontario.
I-94 enters the western end of the state at the Indiana border, and it travels east to Detroit and then northeast to Port Huron and ties in with I-69. I-194 branches off from this freeway in Battle Creek. I-94 is the main artery between Chicago and Detroit.
I-96 runs east–west between Detroit and Muskegon. I-496 loops through Lansing. I-196 branches off from this freeway at Grand Rapids and connects to I-94 near Benton Harbor. I-696 branches off from this freeway at Novi and connects to I-94 near St Clair Shores.
U.S. Highway 2 (US 2) enters Michigan at the city of Ironwood and travels east to the town of Crystal Falls, where it turns south and briefly re-enters Wisconsin northwest of Florence. It re-enters Michigan north of Iron Mountain and continues through the Upper Peninsula of Michigan to the cities of Escanaba, Manistique, and St. Ignace. Along the way, it cuts through the Ottawa and Hiawatha national forests and follows the northern shore of Lake Michigan. Its eastern terminus lies at exit 344 on I-75, just north of the Mackinac Bridge.
Airports.
The Detroit Metropolitan Wayne County Airport, located in the western suburb of Romulus, was in 2010 the 16th busiest airfield in North America measured by passenger traffic. The Gerald R. Ford International Airport in Grand Rapids is the next busiest airport in the state, served by eight airlines to 23 destinations. Flint Bishop International Airport is the third largest airport in the state, served by four airlines to several primary hubs. Smaller regional and local airports are located throughout the state including on several islands.
Important cities, townships, and metropolitan areas.
Other important cities include:
Half of the wealthiest communities in the state are located in Oakland County, just north of Detroit. Another wealthy community is located just east of the city, in Grosse Pointe. Only three of these cities are located outside of Metro Detroit. The city of Detroit itself, with a per capita income of $14,717, ranks 517th on the list of Michigan locations by per capita income. Benton Harbor is the poorest city in Michigan, with a per capita income of $8,965, while Barton Hills is the richest with a per capita income of $110,683.
Education.
Michigan's education system provides services to 1.6 million K-12 students in public schools. More than 124,000 students attend private schools and an uncounted number are home-schooled under certain legal requirements. The public school system has a $14.5 billion budget in 2008–2009. Michigan has a number of public universities spread throughout the state and numerous private colleges as well. Michigan State University has the eighth largest campus population of any U.S. school. Seven of the state's universities—Central Michigan University, University of Michigan, Michigan State University, Michigan Technological University, Oakland University, Wayne State University, and Western Michigan University—are classified as research universities by the Carnegie Foundation.
Professional sports.
Michigan's major-league sports teams include: Detroit Tigers baseball team, Detroit Lions football team, Detroit Red Wings ice hockey team, and the Detroit Pistons men's basketball team. All of Michigan's major league teams play in the Metro Detroit area.
The Pistons played at Detroit's Cobo Arena until 1978 and at the Pontiac Silverdome until 1988 when they moved into The Palace of Auburn Hills. The Detroit Lions played at Tiger Stadium in Detroit until 1974, then moved to the Pontiac Silverdome where they played for 27 years between 1975 and 2002 before moving to Ford Field in Detroit in 2002. The Detroit Tigers played at Tiger Stadium (formerly known as Navin Field and Briggs Stadium) from 1912 to 1999. In 2000 they moved to Comerica Park. The Red Wings played at Olympia Stadium before moving to Joe Louis Arena in 1979. Professional hockey got its start in Houghton, when the Portage Lakers were formed.
The Michigan International Speedway is the site of NASCAR races and Detroit was formerly the site of a Formula One World Championship Grand Prix race. From 1959 to 1961, Detroit Dragway hosted the NHRA's U.S. Nationals. Michigan is home to one of the major canoeing marathons: the 120 mi Au Sable River Canoe Marathon. The Port Huron to Mackinac Boat Race is also a favorite.
Thirteen-time Grand Slam champion Serena Williams was born in Saginaw. The 2011 World Champion for Women's Artistic Gymnastics, Jordyn Wieber is from DeWitt. Wieber was also a member of the gold medal winning team at the London Olympics in 2012.
Collegiate sports in Michigan are popular in addition to professional sports. The state's two largest athletic programs are the Michigan Wolverines and Michigan State Spartans, which play in the NCAA Big Ten Conference.
State symbols and nicknames.
Michigan is, by tradition, known as "The Wolverine State," and the University of Michigan takes the wolverine as its mascot. The association is well and long established: for example, many Detroiters volunteered to fight during the American Civil War and George Armstrong Custer, who led the Michigan Brigade, called them the "Wolverines". The origins of this association are obscure; it may derive from a busy trade in wolverine furs in Sault Ste. Marie in the 18th century or may recall a disparagement intended to compare early settlers in Michigan with the vicious mammal. Wolverines are, however, extremely rare in Michigan. A sighting in February 2004 near Ubly was the first confirmed sighting in Michigan in 200 years. The animal was found dead in 2010.
Further reading.
</dl>

</doc>
<doc id="18862" url="http://en.wikipedia.org/wiki?curid=18862" title="Minimum wage">
Minimum wage

A minimum wage is the lowest daily or monthly remuneration that employers may legally pay to workers. Equivalently, it is the price floor below which workers may not sell their labor. Although minimum wage laws are in effect in many jurisdictions, differences of opinion exist about the benefits and drawbacks of a minimum wage. Supporters of the minimum wage say it increases the standard of living of workers, reduces poverty, reduces inequality, boosts morale and forces businesses to be more efficient. In contrast, opponents of the minimum wage say it increases poverty, increases unemployment (particularly among unskilled or inexperienced workers) and is damaging to businesses.
History.
"It is a serious national evil that any class of His Majesty's subjects should receive less than a living wage in return for their utmost exertions. It was formerly supposed that the working of the laws of supply and demand would naturally regulate or eliminate that evil [...and...] ultimately produce a fair price. Where... you have a powerful organisation on both sides... there you have a healthy bargaining... But where you have what we call sweated trades, you have no organisation, no parity of bargaining, the good employer is undercut by the bad, and the bad employer is undercut by the worst... where those conditions prevail you have not a condition of progress, but a condition of progressive degeneration."
Winston Churchill MP, Trade Boards Bill, Hansard House of Commons (28 April 1909) vol 4, 
The precursor of modern minimum wage laws can be found in the Ordinance of Labourers (1349), which was a decree by King Edward III that set a" maximum wage" for laborers in medieval England. King Edward III, a wealthy land owner, like his lords was dependent on serfs to work his lands. In the autumn of 1348, the Black Plague reached England and decimated the population. The severe shortage of labor caused wages to soar and encouraged King Edward III to set a wage ceiling. Subsequent amendments to the ordinance, such as the Statute of Labourers (1351), increased the penalties for paying a wage above the set rates.
While the laws governing wages initially set a ceiling on compensation, they were eventually used to set a living wage. An amendment to the Statute of Labourers in 1389 effectively fixed wages to the price of food. As the centuries passed, the Justice of the Peace, who was charged with setting the maximum wage, also began to set formal minimum wages. The practice was eventually formalized with the passage of the Act Fixing a Minimum Wage in 1604 by King James I for workers in the textile industry.
By the early 19th century, the Statutes of Labourers was repealed as increasingly capitalistic England embraced "laissez-faire" policies which disfavored regulations of wages (whether upper or lower limits). The subsequent 19th century saw significant labor unrest affect many industrial nations. As trade unions were decriminalized during the century, attempts to control wages through collective agreement were made. However, this meant that a uniform minimum wage was not possible. In "Principles of Political Economy" in 1848, John Stuart Mill argued that because of the collective action problems that workers faced in organisation, it was a justified departure from "laissez faire" policies (or freedom of contract) to regulate people's wages and hours by law.
It was not until the 1890s that modern legislative attempts to regulate minimum wages were seen in New Zealand and Australia. The movement for a minimum wage was initially focused on stopping sweatshop labor and controlling the proliferation of sweatshops in manufacturing industries. The sweatshops employed large numbers of women and young workers, paying them what were considered to be substandard wages. The sweatshop owners were thought to have unfair bargaining power over their employees, and a minimum wage was proposed as a means to make them pay fairly. Over time, the focus changed to helping people, especially families, become more self sufficient.
Minimum wage laws.
 The first national minimum wage law was enacted by the government of New Zealand in 1894, followed by Australia in 1896 and the United Kingdom in 1909. In the United States, statutory minimum wages were first introduced nationally in 1938, and reintroduced and expanded in the United Kingdom in 1998. There is now legislation or binding collective bargaining regarding minimum wage in more than 90 percent of all countries. In the European Union, 21 member states currently have national minimum wages. In July 2014 Germany began legislating to introduce a federally-mandated minimum wage which would come into effect on 1 January 2015. Many countries, such as Sweden, Finland, Denmark, Switzerland, Austria, and Italy have no minimum wage laws, but rely on employer groups and trade unions to set minimum earnings through collective bargaining.
Minimum wage rates vary greatly across many different jurisdictions, not only in setting a particular amount of money – e.g. US$7.25 per hour under certain states' laws (or $2.13 for employees who receive tips, known as the tipped minimum wage), $9.47 in the US state of Washington, and £6.50 (for those aged 21+) in the United Kingdom – but also in terms of which pay period (e.g. Russia and China set monthly minimums) or the scope of coverage. Some jurisdictions allow employers to count tips given to their workers as credit towards the minimum wage levels. India was one of the first developing countries to introduce minimum wage policy. It also has one of the most complicated systems with more than 1200 minimum wage rates.
Informal minimum wages.
Customs and extra-legal pressures from governments or labor unions can produce a "de facto" minimum wage. So can international public opinion, by pressuring multinational companies to pay Third World workers wages usually found in more industrialized countries. The latter situation in Southeast Asia and Latin America was publicized in the 2000s, but it existed with companies in West Africa in the middle of the twentieth century.
Setting minimum wage.
Among the indicators that might be used to establish an initial minimum wage rate are ones that minimize the loss of jobs while preserving international competitiveness. Among these are general economic conditions as measured by real and nominal gross domestic product; inflation; labor supply and demand; wage levels, distribution and differentials; employment terms; productivity growth; labor costs; business operating costs; the number and trend of bankruptcies; economic freedom rankings; standards of living and the prevailing average wage rate.
In the business sector, concerns include the expected increased cost of doing business, threats to profitability, rising levels of unemployment (and subsequent higher government expenditure on welfare benefits raising tax rates), and the possible knock-on effects to the wages of more experienced workers who might already be earning the new statutory minimum wage, or slightly more. Among workers and their representatives, political consideration weigh in as labor leaders seek to win support by demanding the highest possible rate. Other concerns include purchasing power, inflation indexing and standardized working hours.
In the United States, the minimum wage promulgated by the Fair Labor Standards Act of 1938 was intentionally set at a high, national level to render low-technology, low-wage factories in the South obsolete. According to the Economic Policy Institute, the minimum wage in the United States would have been $18.28 in 2013 if the minimum wage kept pace with labor productivity. To adjust for increased rates of worker productivity in the United States, raising the minimum wage to $22 (or more) an hour has been presented.
Economics models.
Supply and demand.
An analysis of supply and demand of the type shown in many mainstream economics textbooks implies that by mandating a price floor above the equilibrium wage, minimum wage laws should cause unemployment. This is because a greater number of people are willing to work at the higher wage while a smaller number of jobs will be available at the higher wage. Companies can be more selective in those whom they employ thus the least skilled and least experienced will typically be excluded. An imposition or increase of a minimum wage will generally only affect employment in the low-skill labor market, as the equilibrium wage is already at or below the minimum wage, whereas in higher skill labor markets the equilibrium wage is too high for a change in minimum wage to affect employment.
According to the supply and demand model shown in many textbooks on economics, increasing the minimum wage decreases the employment of minimum-wage workers. One such textbook says:
If a higher minimum wage increases the wage rates of unskilled workers above the level that would be established by market forces, the quantity of unskilled workers employed will fall. The minimum wage will price the services of the least productive (and therefore lowest-wage) workers out of the market. …The direct results of minimum wage legislation are clearly mixed. Some workers, most likely those whose previous wages were closest to the minimum, will enjoy higher wages. This is known as the "ripple effect". The ripple effect shows that when you increase the minimum wage the wages of all others will consequently increase due the need for relativity. Others, particularly those with the lowest prelegislation wage rates, will be unable to find work. They will be pushed into the ranks of the unemployed or out of the labor force. Some argue that by increasing the federal minimum wage, however, the economy will be adversely affected due to small businesses not being able to keep up with the need to subsequently increase all workers wages.
The textbook illustrates the point with a supply and demand diagram similar to the one above. In the diagram it is assumed that workers are willing to labor for more hours if paid a higher wage. Economists graph this relationship with the wage on the vertical axis and the quantity (hours) of labor supplied on the horizontal axis. Since higher wages increase the quantity supplied, the supply of labor curve is upward sloping, and is shown as a line moving up and to the right.
A firm's cost is a function of the wage rate. It is assumed that the higher the wage, the fewer hours an employer will demand of an employee. This is because, as the wage rate rises, it becomes more expensive for firms to hire workers and so firms hire fewer workers (or hire them for fewer hours). The demand of labor curve is therefore shown as a line moving down and to the right.
Combining the demand and supply curves for labor allows us to examine the effect of the minimum wage. We will start by assuming that the supply and demand curves for labor will not change as a result of raising the minimum wage. This assumption has been questioned. If no minimum wage is in place, workers and employers will continue to adjust the quantity of labor supplied according to price until the quantity of labor demanded is equal to the quantity of labor supplied, reaching equilibrium price, where the supply and demand curves intersect. Minimum wage behaves as a classical price floor on labor. Standard theory says that, if set above the equilibrium price, more labor will be willing to be provided by workers than will be demanded by employers, creating a surplus of labor, i.e. unemployment.
In other words, the simplest and most basic economics says this about commodities like labor (and wheat, for example): Artificially raising the price of the commodity tends to cause the supply of it to increase and the demand for it to lessen. The result is a surplus of the commodity. When there is a wheat surplus, the government buys it. Since the government does not hire surplus labor, the labor surplus takes the form of unemployment, which tends to be higher with minimum wage laws than without them.
So the basic theory says that raising the minimum wage helps workers whose wages are raised, and hurts people who are not hired (or lose their jobs) because companies cut back on employment. But proponents of the minimum wage hold that the situation is much more complicated than the basic theory can account for. One complicating factor is possible monopsony in the labor market, whereby the individual employer has some market power in determining wages paid. Thus it is at least theoretically possible that the minimum wage may boost employment. Though single employer market power is unlikely to exist in most labor markets in the sense of the traditional 'company town,' asymmetric information, imperfect mobility, and the personal element of the labor transaction give some degree of wage-setting power to most firms.
Criticism of the neoclassical model.
The argument that a minimum wage decreases employment is based on a simple supply and demand model of the labor market. A number of economists (for example Pierangelo Garegnani, Robert L. Vienneau, and Arrigo Opocher & Ian Steedman), building on the work of Piero Sraffa, argue that that model, even given all its assumptions, is logically incoherent. Michael Anyadike-Danes and Wynne Godley argue, based on simulation results, that little of the empirical work done with the textbook model constitutes a potentially falsifiable theory, and consequently empirical evidence hardly exists for that model. Graham White argues, partially on the basis of Sraffianism, that the policy of increased labor market flexibility, including the reduction of minimum wages, does not have an "intellectually coherent" argument in economic theory.
Gary Fields, Professor of Labor Economics and Economics at Cornell University, argues that the standard textbook model for the minimum wage is ambiguous, and that the standard theoretical arguments incorrectly measure only a one-sector market. Fields says a two-sector market, where "the self-employed, service workers, and farm workers are typically excluded from minimum-wage coverage... [and with] one sector with minimum-wage coverage and the other without it [and possible mobility between the two]," is the basis for better analysis. Through this model, Fields shows the typical theoretical argument to be ambiguous and says "the predictions derived from the textbook model definitely do not carry over to the two-sector case. Therefore, since a non-covered sector exists nearly everywhere, the predictions of the textbook model simply cannot be relied on."
An alternate view of the labor market has low-wage labor markets characterized as monopsonistic competition wherein buyers (employers) have significantly more market power than do sellers (workers). This monopsony could be a result of intentional collusion between employers, or naturalistic factors such as segmented markets, search costs, information costs, imperfect mobility and the personal element of labor markets. In such a case a simple supply and demand graph would not yield the quantity of labor clearing and the wage rate. This is because while the upward sloping aggregate labor supply would remain unchanged, instead of using the downward labor demand curve shown in a supply and demand diagram, monopsonistic employers would use a steeper downward sloping curve corresponding to marginal expenditures to yield the intersection with the supply curve resulting in a wage rate lower than would be the case under competition. Also, the amount of labor sold would also be lower than the competitive optimal allocation.
Such a case is a type of market failure and results in workers being paid less than their marginal value. Under the monopsonistic assumption, an appropriately set minimum wage could increase both wages and employment, with the optimal level being equal to the marginal product of labor. This view emphasizes the role of minimum wages as a market regulation policy akin to antitrust policies, as opposed to an illusory "free lunch" for low-wage workers.
Another reason minimum wage may not affect employment in certain industries is that the demand for the product the employees produce is highly inelastic. For example, if management is forced to increase wages, management can pass on the increase in wage to consumers in the form of higher prices. Since demand for the product is highly inelastic, consumers continue to buy the product at the higher price and so the manager is not forced to lay off workers. Economist Paul Krugman argues this explanation neglects to explain why the firm was not charging this higher price absent the minimum wage.
Three other possible reasons minimum wages do not affect employment were suggested by Alan Blinder: higher wages may reduce turnover, and hence training costs; raising the minimum wage may "render moot" the potential problem of recruiting workers at a higher wage than current workers; and minimum wage workers might represent such a small proportion of a business's cost that the increase is too small to matter. He admits that he does not know if these are correct, but argues that "the list demonstrates that one can accept the new empirical findings and still be a card-carrying economist."
Empirical studies.
Economists disagree as to the measurable impact of minimum wages in the 'real world'. This disagreement usually takes the form of competing empirical tests of the elasticities of supply and demand in labor markets and the degree to which markets differ from the efficiency that models of perfect competition predict.
Economists have done empirical studies on different aspects of the minimum wage, including:
Until the mid-1990s, a general consensus existed among economists, both conservative and liberal, that the minimum wage reduced employment, especially among younger and low-skill workers. In addition to the basic supply-demand intuition, there were a number of empirical studies that supported this view. For example, Gramlich (1976) found that many of the benefits went to higher income families, and in particular that teenagers were made worse off by the unemployment associated with the minimum wage.
Brown et al. (1983) noted that time series studies to that point had found that for a 10 percent increase in the minimum wage, there was a decrease in teenage employment of 1–3 percent. However, the studies found wider variation, from 0 to over 3 percent, in their estimates for the effect on teenage unemployment (teenagers without a job and looking for one). In contrast to the simple supply and demand diagram, it was commonly found that teenagers withdrew from the labor force in response to the minimum wage, which produced the possibility of equal reductions in the supply as well as the demand for labor at a higher minimum wage and hence no impact on the unemployment rate. Using a variety of specifications of the employment and unemployment equations (using ordinary least squares vs. generalized least squares regression procedures, and linear vs. logarithmic specifications), they found that a 10 percent increase in the minimum wage caused a 1 percent decrease in teenage employment, and no change in the teenage unemployment rate. The study also found a small, but statistically significant, increase in unemployment for adults aged 20–24.
Wellington (1991) updated Brown et al.'s research with data through 1986 to provide new estimates encompassing a period when the real (i.e., inflation-adjusted) value of the minimum wage was declining, because it had not increased since 1981. She found that a 10% increase in the minimum wage decreased the absolute teenage employment by 0.6%, with no effect on the teen or young adult unemployment rates.
Some research suggests that the unemployment effects of small minimum wage increases are dominated by other factors. In Florida, where voters approved an increase in 2004, a follow-up comprehensive study after the increase confirmed a strong economy with increased employment above previous years in Florida and better than in the U.S. as a whole. When it comes to on-the-job training, some believe the increase in wages is taken out of training expenses. A 2001 empirical study found that there is "no evidence that minimum wages reduce training, and little evidence that they tend to increase training."
Some empirical studies have tried to ascertain the benefits of a minimum wage beyond employment effects. In an analysis of Census data, Joseph Sabia and Robert Nielson found no statistically significant evidence that minimum wage increases helped reduce financial, housing, health, or food insecurity. This study was undertaken by the Employment Policies Institute, a think tank funded by the food, beverage and hospitality industries. In 2012, Michael Reich published an economic analysis that suggested that a proposed minimum wage hike in San Diego might stimulate the city's economy by about $190 million.
"The Economist" wrote in December 2013: "A minimum wage, providing it is not set too high, could thus boost pay with no ill effects on jobs...America's federal minimum wage, at 38% of median income, is one of the rich world's lowest. Some studies find no harm to employment from federal or state minimum wages, others see a small one, but none finds any serious damage. ... High minimum wages, however, particularly in rigid labour markets, do appear to hit employment. France has the rich world’s highest wage floor, at more than 60% of the median for adults and a far bigger fraction of the typical wage for the young. This helps explain why France also has shockingly high rates of youth unemployment: 26% for 15- to 24-year-olds."
Card and Krueger.
In 1992, the minimum wage in New Jersey increased from $4.25 to $5.05 per hour (an 18.8% increase) while the adjacent state of Pennsylvania remained at $4.25. David Card and Alan Krueger gathered information on fast food restaurants in New Jersey and eastern Pennsylvania in an attempt to see what effect this increase had on employment within New Jersey. Basic economic theory would have implied that relative employment should have decreased in New Jersey. Card and Krueger surveyed employers before the April 1992 New Jersey increase, and again in November–December 1992, asking managers for data on the full-time equivalent staff level of their restaurants both times. Based on data from the employers' responses, the authors concluded that the increase in the minimum wage slightly increased employment in the New Jersey restaurants.
One possible explanation for why the current minimum wage laws may not affect unemployment in the United States is that the minimum wage is set close to the equilibrium point for low and unskilled workers. Thus, according to this explanation, in the absence of the minimum wage law unskilled workers would be paid approximately the same amount and an increase above this equilibrium point could likely bring about increased unemployment for the low and unskilled workers.
Card and Krueger expanded on this initial article in their 1995 book "Myth and Measurement: The New Economics of the Minimum Wage". They argued that the negative employment effects of minimum wage laws are minimal if not non-existent. For example, they look at the 1992 increase in New Jersey's minimum wage, the 1988 rise in California's minimum wage, and the 1990–91 increases in the federal minimum wage. In addition to their own findings, they reanalyzed earlier studies with updated data, generally finding that the older results of a negative employment effect did not hold up in the larger datasets.
Research subsequent to Card and Krueger's work.
In subsequent research, David Neumark and William Wascher attempted to verify Card and Krueger's results by using administrative payroll records from a sample of large fast food restaurant chains in order to verify employment. They found that the minimum wage increases were followed by decreases in employment. On the other hand, an assessment of data collected and analyzed by Neumark and Wascher did not initially contradict the Card and Krueger results, but in a later edited version they found a four percent decrease in employment, and reported that "the estimated disemployment effects in the payroll data are often statistically significant at the 5- or 10- percent level although there are some estimators and subsamples that yield insignificant—although almost always negative" employment effects. However, this paper's conclusions were rebutted in a 2000 paper by Card and Krueger. A 2011 paper has reconciled the difference between Card and Krueger's survey data and Neumark and Wascher's payroll-based data. The paper shows that both datasets evidence conditional employment effects that are positive for small restaurants, but are negative for large fast-food restaurants.
In 1996 and 1997, the federal minimum wage was increased from $4.25 to $5.15, thereby increasing the minimum wage by $0.90 in Pennsylvania but by just $0.10 in New Jersey; this allowed for an examination of the effects of minimum wage increases in the same area, subsequent to the 1992 change studied by Card and Krueger. A study by Hoffman and Trace found the result anticipated by traditional theory: a detrimental effect on employment.
Further application of the methodology used by Card and Krueger by other researchers yielded results similar to their original findings, across additional data sets. A 2010 study by three economists (Arindrajit Dube of the University of Massachusetts Amherst, T. William Lester of the University of North Carolina at Chapel Hill, and Michael Reich of the University of California, Berkeley), compared adjacent counties in different states where the minimum wage had been raised in one of the states. They analyzed employment trends for several categories of low-wage workers from 1990 to 2006 and found that increases in minimum wages had no negative effects on low-wage employment and successfully increased the income of workers in food services and retail employment, as well as the narrower category of workers in restaurants.
However, a 2011 study by Baskaya and Rubinstein of Brown University found that at the federal level, "a rise in minimum wage have ["sic"] an instantaneous impact on wage rates and a corresponding negative impact on employment", stating, "Minimum wage increases boost teenage wage rates and reduce teenage employment." Another 2011 study by Sen, Rybczynski, and Van De Waal found that "a 10% increase in the minimum wage is significantly correlated with a 3−5% drop in teen employment." A 2012 study by Sabia, Hansen, and Burkhauser found that "minimum wage increases can have substantial adverse labor demand effects for low-skilled individuals", with the largest effects on those aged 16 to 24.
A 2013 study by Meer and West concluded that "the minimum wage reduces net job growth, primarily through its effect on job creation by expanding establishments ... most pronounced for younger workers and in industries with a higher proportion of low-wage workers." This study by Meer and West was later critiqued for its trends of assumption in the context of narrowly defined low-wage groups. The authors replied to the critiques and released additional data which addressed the criticism of their methodology, but did not resolve the issue of whether their data showed a causal relationship. Another 2013 study by Suzana Laporšek of the University of Primorska, on youth unemployment in Europe claimed there was "a negative, statistically significant impact of minimum wage on youth employment." 
A 2013 study by labor economists Tony Fang and Carl Lin which studied minimum wages and employment in China, found that "minimum wage changes have significant adverse effects on employment in the Eastern and Central regions of China, and result in disemployment for females, young adults, and low-skilled workers".
Statistical meta-analyses.
Several researchers have conducted statistical meta-analyses of the employment effects of the minimum wage. In 1995, Card and Krueger analyzed 14 earlier time-series studies on minimum wages and concluded that there was clear evidence of publication bias (in favor of studies that found a statistically significant negative employment effect). They point out that later studies, which had more data and lower standard errors, did not show the expected increase in t-statistic (almost all the studies had a t-statistic of about two, just above the level of statistical significance at the .05 level). Though a serious methodological indictment, opponents of the minimum wage largely ignored this issue; as Thomas C. Leonard noted, "The silence is fairly deafening."
In 2005, T.D. Stanley showed that Card and Krueger's results could signify either publication bias or the absence of a minimum wage effect. However, using a different methodology, Stanley concludes that there is evidence of publication bias, and that correction of this bias shows no relationship between the minimum wage and unemployment. In 2008, Hristos Doucouliagos and T.D. Stanley conducted a similar meta-analysis of 64 U.S. studies on dis-employment effects and concluded that Card and Krueger's initial claim of publication bias is still correct. Moreover, they concluded, "Once this publication selection is corrected, little or no evidence of a negative association between minimum wages and employment remains."
Consistent with the results from Doucouliagos and Stanley, and Card and Krueger, Baskaya and Rubinstein's 2011 study, which analyzed 24 papers on the minimum wage, found "mild positive, yet statistically insignificant association between the change in the employment of teenagers" at state minimum wage levels. However, when minimum wage is set at the federal level, they found "notable wage impacts and large corresponding disemployment effects".
Debate over consequences.
Minimum wage laws affect workers in most low-paid fields of employment and have usually been judged against the criterion of reducing poverty. Minimum wage laws receive less support from economists than from the general public. Despite decades of experience and economic research, debates about the costs and benefits of minimum wages continue today.
Various groups have great ideological, political, financial, and emotional investments in issues surrounding minimum wage laws. For example, agencies that administer the laws have a vested interest in showing that "their" laws do not create unemployment, as do labor unions whose members' finances are protected by minimum wage laws. On the other side of the issue, low-wage employers such as restaurants finance the Employment Policies Institute, which has released numerous studies opposing the minimum wage. The presence of these powerful groups and factors means that the debate on the issue is not always based on dispassionate analysis. Additionally, it is extraordinarily difficult to separate the effects of minimum wage from all the other variables that affect employment.
The following table summarizes the arguments made by those for and against minimum wage laws:
A widely circulated argument that the minimum wage was ineffective at reducing poverty was provided by George Stigler in 1949:
In 2006, the International Labour Organization (ILO) argued that the minimum wage could not be directly linked to unemployment in countries that have suffered job losses. In April 2010, the Organisation for Economic Co-operation and Development (OECD) released a report arguing that countries could alleviate teen unemployment by "lowering the cost of employing low-skilled youth" through a sub-minimum training wage. A study of U.S. states showed that businesses' annual and average payrolls grow faster and employment grew at a faster rate in states with a minimum wage. The study showed a correlation, but did not claim to prove causation.
Although strongly opposed by both the business community and the Conservative Party when introduced in 1999, the Conservatives reversed their opposition in 2000. Accounts differ as to the effects of the minimum wage. The Centre for Economic Performance found no discernible impact on employment levels from the wage increases, while the Low Pay Commission found that employers had reduced their rate of hiring and employee hours employed, and found ways to cause current workers to be more productive (especially service companies). The Institute for the Study of Labor found prices in the minimum wage sector rose significantly faster than prices in non-minimum wage sectors, in the four years following the implementation of the minimum wage. Neither trade unions nor employer organizations contest the minimum wage, although the latter had especially done so heavily until 1999.
In 2014, supporters of minimum wage cited a study that found that job creation within the United States is faster in states that raised their minimum wages. In 2014, supporters of minimum wage cited news organizations who reported the state with the highest minimum-wage garnered more job creation than the rest of the United States.
In 2014, in Seattle, Washington, liberal and progressive business owners who had supported the city's new $15 minimum wage later complained that it would prevent them from expanding their businesses and creating new jobs.
Surveys of economists.
According to a 1978 article in the "American Economic Review", 90% of the economists surveyed agreed that the minimum wage increases unemployment among low-skilled workers. By 1992 the survey found 79% of economists in agreement with that statement, and by 2000, 45.6% were in full agreement with the statement and 27.9% agreed with provisos (73.5% total). The authors of the 2000 study also reweighted data from a 1990 sample to show that at that time 62.4% of academic economists agreed with the statement above, while 19.5% agreed with provisos and 17.5% disagreed. They state that the reduction on consensus on this question is "likely" due to the Card and Krueger research and subsequent debate.
A similar survey in 2006 by Robert Whaples polled PhD members of the American Economic Association (AEA). Whaples found that 46.8% respondents wanted the minimum wage eliminated, 37.7% supported an increase, 14.3% wanted it kept at the current level, and 1.3% wanted it decreased. Another survey in 2007 conducted by the University of New Hampshire Survey Center found that 73% of labor economists surveyed in the United States believed 150% of the then-current minimum wage would result in employment losses and 68% believed a mandated minimum wage would cause an increase in hiring of workers with greater skills. 31% felt that no hiring changes would result.
Surveys of labor economists have found a sharp split on the minimum wage. Fuchs et al. (1998) polled labor economists at the top 40 research universities in the United States on a variety of questions in the summer of 1996. Their 65 respondents were nearly evenly divided when asked if the minimum wage should be increased. They argued that the different policy views were not related to views on whether raising the minimum wage would reduce teen employment (the median economist said there would be a reduction of 1%), but on value differences such as income redistribution. Daniel B. Klein and Stewart Dompe conclude, on the basis of previous surveys, "the average level of support for the minimum wage is somewhat higher among labor economists than among AEA members."
In 2007, Klein and Dompe conducted a non-anonymous survey of supporters of the minimum wage who had signed the "Raise the Minimum Wage" statement published by the Economic Policy Institute. 95 of the 605 signatories responded. They found that a majority signed on the grounds that it transferred income from employers to workers, or equalized bargaining power between them in the labor market. In addition, a majority considered disemployment to be a moderate potential drawback to the increase they supported.
In 2013, a diverse group of economics experts was surveyed on their view of the minimum wage's impact on employment. 34% of respondents agreed with the statement, "Raising the federal minimum wage to $9 per hour would make it noticeably harder for low-skilled workers to find employment." 32% disagreed and the remaining respondents were uncertain or had no opinion on the question. 49% agreed with the statement, "The distortionary costs of raising the federal minimum wage to $9 per hour and indexing it to inflation are sufficiently small compared with the benefits to low-skilled workers who can find employment that this would be a desirable policy", while 11% disagree.
Alternatives.
Economists and other political commentators have proposed alternatives to the minimum wage. They argue that these alternatives may address the issue of poverty better than a minimum wage, as it would benefit a broader population of low wage earners, not cause any unemployment, and distribute the costs widely rather than concentrating it on employers of low wage workers.
Basic income.
A basic income (or negative income tax) is a system of social security that periodically provides each citizen with a sum of money that is sufficient to live on frugally. It is argued that recipients of the basic income would have considerably more bargaining power when negotiating a wage with an employer as there would be no risk of destitution for not taking the employment. As a result, the jobseeker could spend more time looking for a more appropriate or satisfying job, or they could wait until a higher-paying job appeared. Alternately, they could spend more time increasing their skills in university, which would make them more suitable for higher-paying jobs, as well as provide numerous other benefits. Experiments on Basic Income and NIT in Canada and the USA show that people spent more time studying while the program was running.
Proponents argue that a basic income that is based on a broad tax base would be more economically efficient, as the minimum wage effectively imposes a high marginal tax on employers, causing losses in efficiency.
Guaranteed minimum income.
A guaranteed minimum income is another proposed system of social welfare provision. It is similar to a basic income or negative income tax system, except that it is normally conditional and subject to a means test. Some proposals also stipulate a willingness to participate in the labor market, or a willingness to perform community services.
Refundable tax credit.
A refundable tax credit is a mechanism whereby the tax system can reduce the tax owed by a household to below zero, and result in a net payment to the taxpayer beyond their own payments into the tax system. Examples of refundable tax credits include the earned income tax credit and the additional child tax credit in the U.S., and working tax credits and child tax credits in the UK. Such a system is slightly different from a negative income tax, in that the refundable tax credit is usually only paid to households that have earned at least some income. This policy is more targeted against poverty than the minimum wage, because it avoids subsidizing low-income workers who are supported by high-income households (for example, teenagers still living with their parents).
In the United States, earned income tax credit rates, also known as EITC or EIC, vary by state − some are refundable while other states do not allow a refundable tax credit. The federal EITC program has been expanded by a number of presidents including Jimmy Carter, Ronald Reagan, George H.W. Bush, and Bill Clinton. In 1986, President Reagan described the EITC as "the best anti poverty, the best pro-family, the best job creation measure to come out of Congress." The ability of the earned income tax credit to deliver larger monetary benefits to the poor workers than an increase in the minimum wage and at a lower cost to society was documented in a 2007 report by the Congressional Budget Office.
The Adam Smith Institute prefers cutting taxes on the poor and middle class instead of raising wages as an alternative to the minimum wage.
Collective bargaining.
Italy, Sweden, Norway, Finland, and Denmark are examples of developed nations where there is no minimum wage that is required by legislation. Such nations, particularly the Nordics, have very high union participation rates. Instead, minimum wage standards in different sectors are set by collective bargaining.

</doc>
<doc id="18864" url="http://en.wikipedia.org/wiki?curid=18864" title="Mullet">
Mullet

Mullet may refer to:

</doc>
<doc id="18866" url="http://en.wikipedia.org/wiki?curid=18866" title="Macbeth">
Macbeth

Macbeth (full title The Tragedy of Macbeth) is a tragedy written by William Shakespeare, and is considered one of his darkest and most powerful works. Set in Scotland, the play dramatizes the corrosive psychological and political effects produced when evil is chosen as a way to fulfill the ambition for power.
The play is believed to have been written between 1599 and 1606, and is most commonly dated 1606. The earliest account of a performance of what was probably Shakespeare's play is the Summer of 1606, when Simon Forman recorded seeing such a play at the Globe Theatre. It was first published in the Folio of 1623, possibly from a prompt book. It was most likely written during the reign of James I, who had been James VI of Scotland before he succeeded to the English throne in 1603. James was a patron of Shakespeare's acting company, and of all the plays Shakespeare wrote during James's reign, "Macbeth" most clearly reflects the playwright's relationship with the sovereign.
"Macbeth" is Shakespeare's shortest tragedy, and tells the story of a brave Scottish general named Macbeth who receives a prophecy from a trio of witches that one day he will become King of Scotland. Consumed by ambition and spurred to action by his wife, Macbeth murders King Duncan and takes the throne for himself. He is then wracked with guilt and paranoia, and he soon becomes a tyrannical ruler as he is forced to commit more and more murders to protect himself from enmity and suspicion. The bloodbath and consequent civil war swiftly take Macbeth and Lady Macbeth into the realms of arrogance, madness, and death.
Shakespeare's source for the tragedy is the account of Macbeth, King of Scotland, Macduff, and Duncan in "Holinshed's Chronicles" (1587), a history of England, Scotland, and Ireland familiar to Shakespeare and his contemporaries, although the events in the play differ extensively from the history of the real Macbeth. In recent scholarship, the events of the tragedy are usually associated more closely with the execution of Henry Garnett for complicity in the Gunpowder Plot of 1605.
In the backstage world of theatre, some believe that the play is cursed, and will not mention its title aloud, referring to it instead as "the Scottish play". Over the course of many centuries, the play has attracted some of the most renowned actors to the roles of Macbeth and Lady Macbeth. It has been adapted to film, television, opera, novels, comic books, and other media.
Plot.
The play opens amidst thunder and lightning, and the Three Witches decide that their next meeting shall be with Macbeth. In the following scene, a wounded sergeant reports to King Duncan of Scotland that his generals—Macbeth, who is the Thane of Glamis, and Banquo—have just defeated the allied forces of Norway and Ireland, who were led by the traitorous Macdonwald and the Thane of Cawdor. Macbeth, the King's kinsman, is praised for his bravery and fighting prowess.
In the following scene, Macbeth and Banquo discuss the weather and their victory. As they wander onto a heath, the Three Witches enter and greet them with prophecies. Though Banquo challenges them first, they address Macbeth, hailing him as "Thane of Glamis," "Thane of Cawdor," and that he shall "be King hereafter." Macbeth appears to be stunned to silence. When Banquo asks of his own fortunes, the witches respond paradoxically, saying that he will be less than Macbeth, yet happier, less successful, yet more. He will father a line of kings, though he himself will not be one. While the two men wonder at these pronouncements, the witches vanish, and another thane, Ross, arrives and informs Macbeth of his newly bestowed title: Thane of Cawdor, as the previous Thane of Cawdor shall be put to death for his traitorous activities. The first prophecy is thus fulfilled, and Macbeth, previously sceptical, immediately begins to harbour ambitions of becoming king.
King Duncan welcomes and praises Macbeth and Banquo, and declares that he will spend the night at Macbeth's castle at Inverness; he also names his son Malcolm as his heir. Macbeth sends a message ahead to his wife, Lady Macbeth, telling her about the witches' prophecies. Lady Macbeth suffers none of her husband's uncertainty, and wishes him to murder Duncan in order to obtain kingship. When Macbeth arrives at Inverness, she overrides all of her husband's objections by challenging his manhood, and successfully persuades him to kill the king that very night. He and Lady Macbeth plan to get Duncan's two chamberlains drunk so that they will black out; the next morning they will blame the chamberlains for the murder. They will be defenseless, as they will remember nothing.
While Duncan is asleep, Macbeth stabs him, despite his doubts and a number of supernatural portents, including a hallucination of a bloody dagger. He is so shaken that Lady Macbeth has to take charge. In accordance with her plan, she frames Duncan's sleeping servants for the murder by placing bloody daggers on them. Early the next morning, Lennox, a Scottish nobleman, and Macduff, the loyal Thane of Fife, arrive. A porter opens the gate and Macbeth leads them to the king's chamber, where Macduff discovers Duncan's body. Macbeth murders the guards to prevent them from professing their innocence, but claims he did so in a fit of anger over their misdeeds. Duncan's sons Malcolm and Donalbain flee to England and Ireland, respectively, fearing that whoever killed Duncan desires their demise as well. The rightful heirs' flight makes them suspects and Macbeth assumes the throne as the new King of Scotland as a kinsman of the dead king. Banquo reveals this to the audience, and while sceptical of the new King Macbeth, he remembers the witches' prophecy about how his own descendants would inherit the throne, this makes him suspicious of Macbeth.
Despite his success, Macbeth, also aware of this part of the prophecy, remains uneasy. Macbeth invites Banquo to a royal banquet, where he discovers that Banquo and his young son, Fleance, will be riding out that night. Fearing Banquo's suspicions, Macbeth arranges to have him murdered, so he hires his men to kill them. The assassins succeed in killing Banquo, but Fleance escapes. Macbeth becomes furious: he fears that his power remains insecure as long as an heir of Banquo remains alive. At the banquet, Macbeth invites his lords and Lady Macbeth to a night of drinking and merriment. Banquo's ghost enters and sits in Macbeth's place. Macbeth raves fearfully, startling his guests, as the ghost is only visible to himself. The others panic at the sight of Macbeth raging at an empty chair, until a desperate Lady Macbeth tells them that her husband is merely afflicted with a familiar and harmless malady. The ghost departs and returns once more, causing the same riotous anger and fear in Macbeth. This time, Lady Macbeth tells the lords to leave, and they do so.
Macbeth, disturbed, visits the three witches once more and asks them to reveal the truth of their prophecies to him. To answer his questions, they summon horrible apparitions, each of which offers predictions and further prophecies to allay Macbeth's fears. First, they conjure an armoured head, which tells him to beware of Macduff (4.1.72). Second, a bloody child tells him that no one born of a woman shall be able to harm him. Thirdly, a crowned child holding a tree states that Macbeth will be safe until Great Birnam Wood comes to Dunsinane Hill. Macbeth is relieved and feels secure, because he knows that all men are born of women and forests cannot move. Macbeth also asks if Banquo's sons will ever reign in Scotland: the witches conjure a procession of eight crowned kings, all similar in appearance to Banquo, and the last carrying a mirror that reflects even more kings. Macbeth realises that these are all Banquo's descendants having acquired kingship in numerous countries. After the witches perform a mad dance and leave, Lennox enters and tells Macbeth that Macduff has fled to England. Macbeth orders Macduff's castle be seized, and, most cruelly, sends murderers to slaughter Macduff, as well as Macduff's wife and children. Although Macduff is no longer in the castle, everyone in Macduff's castle is put to death, including Lady Macduff and their young son.
Meanwhile, Lady Macbeth becomes wracked with guilt from the crimes she and her husband have committed. At night, in the king's palace at Dunsinane, a doctor and a gentlewoman discuss Lady Macbeth's strange habit of sleepwalking. Suddenly, Lady Macbeth enters in a trance with a candle in her hand. Bemoaning the murders of Duncan, Lady Macduff, and Banquo, she tries to wash off imaginary bloodstains from her hands, all the while speaking of the terrible things she knows she pressed her husband to do. She leaves, and the doctor and gentlewoman marvel at her descent into madness. Her belief that nothing can wash away the blood on her hands is an ironic reversal of her earlier claim to Macbeth that "[a] little water clears us of this deed" (2.2.66).
In England, Macduff is informed by Ross that his "castle is surprised; [his] wife and babes / Savagely slaughter'd" (4.3.204–5). When this news of his family's execution reaches him, Macduff is stricken with grief and vows revenge. Prince Malcolm, Duncan's son, has succeeded in raising an army in England, and Macduff joins him as he rides to Scotland to challenge Macbeth's forces. The invasion has the support of the Scottish nobles, who are appalled and frightened by Macbeth's tyrannical and murderous behaviour. Malcolm leads an army, along with Macduff and Englishmen Siward (the Elder), the Earl of Northumberland, against Dunsinane Castle. While encamped in Birnam Wood, the soldiers are ordered to cut down and carry tree limbs to camouflage their numbers.
Before Macbeth's opponents arrive, he receives news that Lady Macbeth has killed herself, causing him to sink into a deep and pessimistic despair and deliver his "Tomorrow, and tomorrow, and tomorrow" soliloquy (5.5.17–28). Though he reflects on the brevity and meaninglessness of life, he nevertheless awaits the English and fortifies Dunsinane. He is certain that the witches' prophecies guarantee his invincibility, but is struck with fear when he learns that the English army is advancing on Dunsinane shielded with boughs cut from Birnam Wood, in apparent fulfillment of one of the prophecies.
A battle culminates in Macduff's confrontation with Macbeth, who kills Young Siward in combat, the English forces overwhelm his army and castle. Macbeth boasts that he has no reason to fear Macduff, for he cannot be killed by any man born of woman. Macduff declares that he was "from his mother's womb / Untimely ripp'd" (5.8.15–16), ("i.e.," born by Caesarean section) and is not "of woman born" (an example of a literary quibble), fulfilling the second prophecy. Macbeth realises too late that he has misinterpreted the witches' words. Though he realises that he is doomed, he continues to fight. Macduff kills and beheads him, thus fulfilling the remaining prophecy.
Macduff carries Macbeth's head onstage and Malcolm discusses how order has been restored. His last reference to Lady Macbeth, however, reveals "'tis thought, by self and violent hands / Took off her life" (5.9.71–72), but the method of her suicide is undisclosed. Malcolm, now the King of Scotland, declares his benevolent intentions for the country and invites all to see him crowned at Scone.
Although Malcolm, and not Fleance, is placed on the throne, the witches' prophecy concerning Banquo ("Thou shalt get kings") was known to the audience of Shakespeare's time to be true: James VI of Scotland (later also James I of England) was supposedly a descendant of Banquo.
Sources.
"Macbeth" has been compared to Shakespeare's "Antony and Cleopatra." Both Antony and Macbeth as characters seek a new world, even at the cost of the old one. Both are fighting for a throne and have a 'nemesis' to face to achieve that throne. For Antony, the nemesis is Octavius; for Macbeth, it is Banquo. At one point Macbeth even compares himself to Antony, saying "under Banquo / My Genius is rebuk'd, as it is said / Mark Antony's was by Caesar." Lastly, both plays contain powerful and manipulative female figures: Cleopatra and Lady Macbeth.
Shakespeare borrowed the story from several tales in "Holinshed's Chronicles", a popular history of the British Isles known to Shakespeare and his contemporaries. In "Chronicles", a man named Donwald finds several of his family put to death by his king, King Duff, for dealing with witches. After being pressured by his wife, he and four of his servants kill the King in his own house. In "Chronicles", Macbeth is portrayed as struggling to support the kingdom in the face of King Duncan's ineptitude. He and Banquo meet the three witches, who make exactly the same prophecies as in Shakespeare's version. Macbeth and Banquo then together plot the murder of Duncan, at Lady Macbeth's urging. Macbeth has a long, ten-year reign before eventually being overthrown by Macduff and Malcolm. The parallels between the two versions are clear. However, some scholars think that George Buchanan's "Rerum Scoticarum Historia" matches Shakespeare's version more closely. Buchanan's work was available in Latin in Shakespeare's day.
No other version of the story has Macbeth kill the king in Macbeth's own castle. Scholars have seen this change of Shakespeare's as adding to the darkness of Macbeth's crime as the worst violation of hospitality. Versions of the story that were common at the time had Duncan being killed in an ambush at Inverness, not in a castle. Shakespeare conflated the story of Donwald and King Duff in what was a significant change to the story.
Shakespeare made another revealing change. In "Chronicles", Banquo is an accomplice in Macbeth's murder of King Duncan. He also plays an important part in ensuring that Macbeth, not Malcolm, takes the throne in the coup that follows. In Shakespeare's day, Banquo was thought to be a direct ancestor of the Stuart King James I (Banquo's Stuart descent was disproven in the 19th century, when it was discovered that the Fitzalans actually descended from a Breton family). The Banquo portrayed in historical sources is significantly different from the Banquo created by Shakespeare. Critics have proposed several reasons for this change. First, to portray the king's ancestor as a murderer would have been risky. Other authors of the time who wrote about Banquo, such as Jean de Schelandre in his "Stuartide", also changed history by portraying Banquo as a noble man, not a murderer, probably for the same reasons. Second, Shakespeare may have altered Banquo's character simply because there was no dramatic need for another accomplice to the murder; there was, however, a need to give a dramatic contrast to Macbeth—a role which many scholars argue is filled by Banquo.
Other scholars maintain that a strong argument can be made for associating the tragedy with the Gunpowder Plot of 1605. As presented by Harold Bloom in 2008: "[S]cholars cite the existence of several topical references in "Macbeth" to the events of that year, namely the execution of the Rev. Henry Garnett for his alleged complicity in the Gunpowder Plot of 1605, as referenced in the porter's scene."
Date and text.
"Macbeth" cannot be dated precisely, owing to significant evidence of later revisions. Some scholars have placed the original writing of the play as early as 1599. As the play may celebrate King James' ancestors and the Stuart accession to the throne in 1603 (James believed himself to be descended from Banquo), some scholars believe that the play is unlikely to have been composed earlier than 1603 and suggest that the parade of eight kings—which the witches show Macbeth in a vision in Act IV—is a compliment to King James. Some critics think the play was written in 1606 in the aftermath of the Gunpowder Plot because of possible internal allusions to the 1605 plot and its ensuing trials. In fact, there are a great deal of allusions and possible evidences alluding to the Plot, and, for this reason, a great number of critics agree that "Macbeth" was written in the year 1606. Lady Macbeth's instructions to her husband, "Look like the innocent flower, but be the serpent under't" (1.5.74–5), may be an allusion to a medal that was struck in 1605 to commemorate King James' escape that depicted a serpent hiding among lilies and roses . 
Particularly, the Porter's speech (2.3.1–21) in which he welcomes an "equivocator", a farmer, and a tailor to hell (2.3.8–13), has been argued to be an allusion to the 28 March 1606 trial and execution on 3 May 1606 of the Jesuit Henry Garnet, who used the alias "Farmer", with "equivocator" referring to Garnet's defence of "equivocation". The porter says that the equivocator "committed treason enough for God's sake" (2.3.9–10), which specifically connects equivocation and treason and ties it to the Jesuit belief that equivocation was only lawful when used "for God's sake", strengthening the allusion to Garnet. The porter goes on to say that the equivocator "yet could not equivocate to heaven" (2.3.10–11), echoing grim jokes that were current on the eve of Garnet's execution: i.e. that Garnet would be "hanged without equivocation" and at his execution he was asked "not to equivocate with his last breath." The "English tailor" the porter admits to hell (2.3.13), has been seen as an allusion to Hugh Griffin, a tailor who was questioned by the Archbishop of Canterbury on 27 November and 3 December 1607 for the part he played in Garnet's "miraculous straw", an infamous head of straw that was stained with Garnet's blood that had congealed into a form resembling Garnet's portrait, which was hailed by Catholics as a miracle. The tailor Griffin became notorious and the subject of verses published with his portrait on the title page.
When James became king of England, a feeling of uncertainty settled over the nation. James was a Scottish king, not to mention the son of Mary Queen of Scots, a stanch Catholic and traitor. In the words of critic Jonathan Gil Harris, “Macbeth was a play for a post-Elizabethan England facing up to what it might mean to have a Scottish king. England seems comparatively benign, while its northern neighbour is mired in a bloody, monarch-killing past...Macbeth may have been set in medieval Scotland, but it was filled with material of interest to England and England's ruler.” Critics argue that the content of the play is clearly message to James, the new Scottish Catholic King of England. Garry Wills provides further evidence that "Macbeth" is a Gunpowder Play (a type of play that emerged immediately following the events of the Gunpowder Plot). He points out that every Gunpowder Play contains "a necromancy scene, regicide attempted or completed, references to equivocation, scenes that test loyalty by use of deceptive language, and a character who sees through plots—along with a vocabulary similar to the Plot in its immediate aftermath (words like "train, blow, vault") and an ironic recoil of the Plot upon the Plotters (who fall into the pit they dug)." 
The play utilizes a few key words that the audience at the time would recognize as allusions to the Plot. In one sermon in 1605, Lancelot Andrewes stated, regarding the failure of the Plotters on God's day, "Be they fair or foul, glad or sad (as the poet calleth Him) the great Diespiter, 'the Father of days' hath made them both." Shakespeare begins the play by using the words "fair" and "foul" in the first speeches of the witches and Macbeth. In the words of Jonathan Gil Harris, the play expresses the "horror unleashed by a supposedly loyal subject who seeks to kill a king and the treasonous role of equivocation. The play even echoes certain keywords from the scandal-the 'vault' beneath the House of Parliament in which Guy Fawkes stored thirty kegs of gunpowder and the 'blow' about which one of the conspirators had secretly warned a relative who planned to attend the House of Parliament on 5 November...Even though the Plot is never alluded to directly, its presence is everywhere in the play, like a pervasive odor." Further, the play could not have been written after this time, due to references to it seen in other works, notably Francis Beaumont's "Knight of the Burning Pestle", which was written in 1607-1608. Lines 21-30 are a clear allusion to the scene in which Banquo's ghost visits and haunts Macbeth at the dinner table:
<poem>In any place, but I will visit thee
 With ghastly looks, and put into thy mind
 The great offences which thou didst to me:
 When thou art at thy table with thy friends,
 Merry in heart, and filled with swelling wine,
 I'll come in midst of all thy pride and mirth,
 Invisible to all men but thyself,
 And whisper such a sad tale in thine ear
 Shall make thee let the cup fall from thy hand,
 And stand as mute and pale as death itself.</poem>
Scholars also cite an entertainment seen by King James at Oxford in the summer of 1605 that featured three "sibyls" like the weird sisters; Kermode surmises that Shakespeare could have heard about this and alluded to it with the weird sisters. However, A. R. Braunmuller in the New Cambridge edition finds the 1605–6 arguments inconclusive, and argues only for an earliest date of 1603. The play is not considered to have been written any later than 1607, since, as Kermode notes, there are "fairly clear allusions to the play in 1607."
In addition, one suggested allusion supporting a date in late 1606 is the first witch's dialogue about a sailor's wife: "'Aroint thee, witch!' the rump-fed ronyon cries./Her husband's to Aleppo gone, master o' the "Tiger"" (1.6–7). This has been thought to allude to the "Tiger", a ship that returned to England 27 June 1606 after a disastrous voyage in which many of the crew were killed by pirates. A few lines later the witch speaks of the sailor, "He shall live a man forbid:/Weary se'nnights nine times nine" (1.21–2). The real ship was at sea 567 days, the product of 7x9x9, which has been taken as a confirmation of the allusion, which if correct, confirms that the witch scenes were either written or amended earlier than July 1606.
"Macbeth" was first printed in the First Folio of 1623 and the Folio is the only source for the text. The text that survives had been plainly altered by later hands. Most notable is the inclusion of two songs from Thomas Middleton's play "The Witch" (1615); Middleton is conjectured to have inserted an extra scene involving the witches and Hecate, for these scenes had proven highly popular with audiences. These revisions, which since the Clarendon edition of 1869 have been assumed to include all of Act III, scene v, and a portion of Act IV, scene I, are often indicated in modern texts. On this basis, many scholars reject all three of the interludes with the goddess Hecate as inauthentic. Even with the Hecate material, the play is conspicuously short, and so the Folio text may derive from a prompt book that had been substantially cut for performance, or an adapter cut the text himself.
Pronunciations.
The 'reconstructive movement' was concerned with the recreation of Elizabethan acting conditions, and would eventually lead to the creation of Shakespeare's Globe and similar replicas. One of the movement's offshoots was in the reconstruction of Elizabethan pronunciation: for example Bernard Miles' 1951 "Macbeth", for which linguists from University College London were employed to create a transcript of the play in Elizabethan English, then an audio recording of that transcription, from which the actors, in turn, learned their lines.
The pronunciation of many words evolves over time. In Shakespeare's day, for example, "heath" was pronounced as "heth" ("or a slightly elongated 'e' as in the modern 'get'"), so it rhymed with "Macbeth" in the sentences by the Witches at the beginning of the play:
Second Witch: Upon the heath.Third Witch: There to meet with Macbeth.
A scholar of antique pronunciation writes, ""Heath" would have made a close (if not exact) rhyme with the "-eth" of "Macbeth", which was pronounced with a short 'i' as in 'it'."
In the theater programme notes, "much was made of how OP [Original Pronunciation] performance reintroduces lost rhymes such as the final couplet: 'So thanks to all at once, and each to one, / Whom we invite to see us crowned at Scone'" (5.11.40-1) where 'one' sounds like 'own'. The Witches, the play's great purveyors of rhyme, benefited most in this regard. So, 'babe' (4.1.30) sounded like 'bab' and rhymed with 'drab' (4.1.31)..."
Eoin Price wrote, "I found the OP rendition of Banquo's brilliant question 'Or have we eaten on the insane root / That takes the "raison" prisoner?' unduly amusing"; and he adds,
Themes and motifs.
"Macbeth<br>The Prince of Cumberland! That is a step<br>On which I must fall down, or else o'erleap,<br>For in my way it lies. Stars, hide your fires;<br>Let not light see my black and deep desires.<br>The eye wink at the hand; yet let that be<br>Which the eye fears, when it is done, to see." 
—"Macbeth", Act I, Scene IV 
Analysis.
"Macbeth" is an anomaly among Shakespeare's tragedies in certain critical ways. It is short: more than a thousand lines shorter than "Othello" and "King Lear", and only slightly more than half as long as "Hamlet". This brevity has suggested to many critics that the received version is based on a heavily cut source, perhaps a prompt-book for a particular performance. That brevity has also been connected to other unusual features: the fast pace of the first act, which has seemed to be "stripped for action"; the comparative flatness of the characters other than Macbeth; and the oddness of Macbeth himself compared with other Shakespearean tragic heroes.
As a tragedy of character.
At least since the days of Alexander Pope and Samuel Johnson analysis of the play has centred on the question of Macbeth's ambition, commonly seen as so dominant a trait that it defines the character. Johnson asserted that Macbeth, though esteemed for his military bravery, is wholly reviled. This opinion recurs in critical literature, and, according to Caroline Spurgeon, is supported by Shakespeare himself, who apparently intended to degrade his hero by vesting him with clothes unsuited to him and to make Macbeth look ridiculous by several nimisms he applies: His garments seem either too big or too small for him – as his ambition is too big and his character too small for his new and unrightful role as king. When he feels as if "dressed in borrowed robes", after his new title as Thane of Cawdor, prophesied by the witches, has been confirmed by Ross (I, 3, ll. 108–109), Banquo comments: "New honours come upon him, / Like our strange garments, cleave not to their mould, / But with the aid of use" (I, 3, ll. 145–146). And, at the end, when the tyrant is at bay at Dunsinane, Caithness sees him as a man trying in vain to fasten a large garment on him with too small a belt: "He cannot buckle his distemper'd cause / Within the belt of rule" (V, 2, ll. 14–15), while Angus, in a similar nimism, sums up what everybody thinks ever since Macbeth's accession to power: "now does he feel his title / Hang loose about him, like a giant's robe / upon a dwarfish thief" (V, 2, ll. 18–20).
Like Richard III, but without that character's perversely appealing exuberance, Macbeth wades through blood until his inevitable fall. As Kenneth Muir writes, "Macbeth has not a predisposition to murder; he has merely an inordinate ambition that makes murder itself seem to be a lesser evil than failure to achieve the crown." Some critics, such as E. E. Stoll, explain this characterisation as a holdover from Senecan or medieval tradition. Shakespeare's audience, in this view, expected villains to be wholly bad, and Senecan style, far from prohibiting a villainous protagonist, all but demanded it.
Yet for other critics, it has not been so easy to resolve the question of Macbeth's motivation. Robert Bridges, for instance, perceived a paradox: a character able to express such convincing horror before Duncan's murder would likely be incapable of committing the crime. For many critics, Macbeth's motivations in the first act appear vague and insufficient. John Dover Wilson hypothesised that Shakespeare's original text had an extra scene or scenes where husband and wife discussed their plans. This interpretation is not fully provable; however, the motivating role of ambition for Macbeth is universally recognised. The evil actions motivated by his ambition seem to trap him in a cycle of increasing evil, as Macbeth himself recognises: "I am in blood/Stepp'd in so far that, should I wade no more,/Returning were as tedious as go o'er."
While working on Russian translations of Shakespeare's works, Boris Pasternak compared Macbeth to Raskolnikov, the protagonist of "Crime and Punishment" by Fyodor Dostoevsky. Pasternak argues that "neither Macbeth or Raskolnikov is a born criminal or a villain by nature. They are turned into criminals by faulty rationalizations, by deductions from false premises." He goes on to argue that Lady Macbeth is "feminine . . . one of those active, insistent wives" who becomes her husband's "executive, more resolute and consistent than he is himself." According to Pasternak, she is only helping Macbeth carry out his own wishes, to her own detriment.
As a tragedy of moral order.
The disastrous consequences of Macbeth's ambition are not limited to him. Almost from the moment of the murder, the play depicts Scotland as a land shaken by inversions of the natural order. Shakespeare may have intended a reference to the great chain of being, although the play's images of disorder are mostly not specific enough to support detailed intellectual readings. He may also have intended an elaborate compliment to James's belief in the divine right of kings, although this hypothesis, outlined at greatest length by Henry N. Paul, is not universally accepted. As in "Julius Caesar", though, perturbations in the political sphere are echoed and even amplified by events in the material world. Among the most often depicted of the inversions of the natural order is sleep. Macbeth's announcement that he has "murdered sleep" is figuratively mirrored in Lady Macbeth's sleepwalking.
Macbeth's generally accepted indebtedness to medieval tragedy is often seen as significant in the play's treatment of moral order. Glynne Wickham connects the play, through the Porter, to a mystery play on the harrowing of hell. Howard Felperin argues that the play has a more complex attitude toward "orthodox Christian tragedy" than is often admitted; he sees a kinship between the play and the tyrant plays within the medieval liturgical drama.
The theme of androgyny is often seen as a special aspect of the theme of disorder. Inversion of normative gender roles is most famously associated with the witches and with Lady Macbeth as she appears in the first act. Whatever Shakespeare's degree of sympathy with such inversions, the play ends with a thorough return to normative gender values. Some feminist psychoanalytic critics, such as Janet Adelman, have connected the play's treatment of gender roles to its larger theme of inverted natural order. In this light, Macbeth is punished for his violation of the moral order by being removed from the cycles of nature (which are figured as female); nature itself (as embodied in the movement of Birnam Wood) is part of the restoration of moral order.
As a poetic tragedy.
Critics in the early twentieth century reacted against what they saw as an excessive dependence on the study of character in criticism of the play. This dependence, though most closely associated with Andrew Cecil Bradley, is clear as early as the time of Mary Cowden Clarke, who offered precise, if fanciful, accounts of the predramatic lives of Shakespeare's female leads. She suggested, for instance, that the child Lady Macbeth refers to in the first act died during a foolish military action.
Witchcraft and evil.
In the play, the Three Witches represent darkness, chaos, and conflict, while their role is as agents and witnesses. Their presence communicates treason and impending doom. During Shakespeare's day, witches were seen as worse than rebels, "the most notorious traytor and rebell that can be." They were not only political traitors, but spiritual traitors as well. Much of the confusion that springs from them comes from their ability to straddle the play's borders between reality and the supernatural. They are so deeply entrenched in both worlds that it is unclear whether they control fate, or whether they are merely its agents. They defy logic, not being subject to the rules of the real world. The witches' lines in the first act: "Fair is foul, and foul is fair: Hover through the fog and filthy air" are often said to set the tone for the rest of the play by establishing a sense of confusion. Indeed, the play is filled with situations where evil is depicted as good, while good is rendered evil. The line "Double, double toil and trouble," communicates the witches' intent clearly: they seek only trouble for the mortals around them. The witches' spells are remarkably similar to the spells of the witch Medusa in Anthony Munday's play "Fidele and Fortunio" published in 1584, and Shakespeare may have been influenced by these.
While the witches do not tell Macbeth directly to kill King Duncan, they use a subtle form of temptation when they tell Macbeth that he is destined to be king. By placing this thought in his mind, they effectively guide him on the path to his own destruction. This follows the pattern of temptation used at the time of Shakespeare. First, they argued, a thought is put in a man's mind, then the person may either indulge in the thought or reject it. Macbeth indulges in it, while Banquo rejects.
According to J. A. Bryant Jr., Macbeth also makes use of Biblical parallels, notably between King Duncan's murder and the murder of Christ:
No matter how one looks at it, whether as history or as tragedy, Macbeth is distinctively Christian. One may simply count the Biblical allusions as Richmond Noble has done; one may go further and study the parallels between Shakespeare's story and the Old Testament stories of Saul and Jezebel as Miss Jane H. Jack has done; or one may examine with W. C. Curry the progressive degeneration of Macbeth from the point of view of medieval theology.
Superstition and "The Scottish Play".
While many today would say that any misfortune surrounding a production is mere coincidence, actors and other theatre people often consider it bad luck to mention "Macbeth" by name while inside a theatre, and sometimes refer to it indirectly, for example as "the Scottish play", or "MacBee", or when referring to the character and not the play, "Mr. and Mrs. M", or "The Scottish King".
This is because Shakespeare (or the play's revisers) are said to have used the spells of real witches in his text, purportedly angering the witches and causing them to curse the play. Thus, to say the name of the play inside a theatre is believed to doom the production to failure, and perhaps cause physical injury or death to cast members. There are stories of accidents, misfortunes and even deaths taking place during runs of "Macbeth".
According to the actor Sir Donald Sinden, in his Sky Arts TV series "Great West End Theatres", contrary to popular myth, Shakespeare's tragedy Macbeth is not the unluckiest play as superstition likes to portray it. Exactly the opposite! The origin of the unfortunate moniker dates back to repertory theatre days when each town and village had at least one theatre to entertain the public. If a play was not doing well, it would invariably get 'pulled' and replaced with a sure-fire audience pleaser – Macbeth guaranteed full-houses. So when the weekly theatre newspaper, "The Stage" was published, listing what was on in each theatre in the country, it was instantly noticed what shows had NOT worked the previous week, as they had been replaced by a definite crowd-pleaser. More actors have died during performances of Hamlet than in the "Scottish play" as the profession still calls it. It is forbidden to quote from it backstage as this could cause the current play to collapse and have to be replaced, causing possible unemployment.
One particular incident that lent itself to the superstition was the Astor Place Riot. The cause of the riots was based on a conflict over two performances of "Macbeth," and is usually ascribed to the curse.
Several methods exist to dispel the curse, depending on the actor. One, attributed to Michael York, is to immediately leave the building the stage is in with the person who uttered the name, walk around it three times, spit over their left shoulders, say an obscenity then wait to be invited back into the building. A related practice is to spin around three times as fast as possible on the spot, sometimes accompanied by spitting over their shoulder, and uttering an obscenity. Another popular "ritual" is to leave the room, knock three times, be invited in, and then quote a line from "Hamlet". Yet another is to recite lines from "The Merchant of Venice", thought to be a lucky play.
Legacy.
Performance history.
Shakespeare's day to the Interregnum.
The first actor to play Macbeth may have been Richard Burbage, chief tragedian of Shakespeare's company, The King's Men. The play required an effigy of his head, for its closing scene. The only eyewitness account of "Macbeth" in Shakespeare's lifetime was recorded by Simon Forman, who saw a performance at the Globe in 1610 or 1611. Scholars have struggled to explain the differences between his account and the play as it appears in the Folio; for example, the following does not accord with anything in the Folio text:
And when MackBeth had murdred the kinge, the blod on his hands could not be washed of by Any means, nor from his wiues handes, which handled the bloddi daggers in hiding them, By which means they became moch amazed and Affronted.
Conversely, he makes no mention of the apparition scene, or of Hecate, of the man not of woman born, or of Birnam Wood. As mentioned above, the Folio text is thought to be a revision of the original play, probably adapted by Thomas Middleton (and unquestionably using Middleton's material), and is very short by Shakespeare's standards, suggesting abridgement. This has led to the theory that the play as we know it from the Folio was an adaptation for indoor performance at the Blackfriars Theatre (which was operated by the King's Men from 1608) – and even speculation that it represents a specific performance before King James. The play contains more musical cues than any other play in the canon as well as a significant use of sound effects.
Restoration and Eighteenth century.
"The chill of the grave seemed about you when you looked on her; there was the hush and damp of the charnel house at midnight ... your flesh crept and your breathing became uneasy ... the scent of blood became palpable to you." 
—Sheridan Knowles on Sarah Siddons' sleepwalking scene. 
All theatres were closed down by the Puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them. Sir William Davenant, founder of the Duke's Company, adapted Shakespeare's play to the tastes of the new era, and his version would dominate on stage for around eighty years. Among the changes he made were the expansion of the role of the witches, introducing new songs, dances and 'flying', and the expansion of the role of Lady Macduff as a foil to Lady Macbeth. There were, however, performances outside the patent companies: among the evasions of the Duke's Company's monopoly was a puppet version of "Macbeth".
"Macbeth" was a favourite of the seventeenth-century diarist Samuel Pepys, who saw the play on 5 November 1664 ("admirably acted"), 28 December 1666 ("most excellently acted"), ten days later on 7 January 1667 ("though I saw it lately, yet [it] appears a most excellent play in all respects"), on 19 April 1667 ("one of the best plays for a stage ... that ever I saw"), again on 16 October 1667 ("was vexed to see Young, who is but a bad actor at best, act Macbeth in the room of Betterton, who, poor man! is sick"), and again three weeks later on 6 November 1667 ("[at] "Macbeth", which we still like mightily"), yet again on 12 August 1668 ("saw "Macbeth", to our great content"), and finally on 21 December 1668, on which date the king and court were also present in the audience.
The first professional performances of "Macbeth" in North America were probably those of The Hallam Company.
In 1744, David Garrick revived the play, abandoning Davenant's version and instead advertising it "as written by Shakespeare". In fact this claim was largely false: he retained much of Davenant's more popular business for the witches, and himself wrote a lengthy death speech for Macbeth. And he cut more than 10% of Shakespeare's play, including the drunken porter, the murder of Lady Macduff's son, and Malcolm's testing of Macduff. Hannah Pritchard was his greatest stage partner, having her premiere as his Lady Macbeth in 1747. He would later drop the play from his repertoire upon her retirement from the stage. Mrs. Pritchard was the first actress to achieve acclaim in the role of Lady Macbeth – at least partly due to the removal of Davenant's material, which made irrelevant moral contrasts with Lady Macduff. Garrick's portrayal focused on the inner life of the character, endowing him with an innocence vacillating between good and evil, and betrayed by outside influences. He portrayed a man capable of observing himself, as if a part of him remained untouched by what he had done, the play moulding him into a man of sensibility, rather than him descending into a tyrant.
John Philip Kemble first played Macbeth in 1778. Although usually regarded as the antithesis of Garrick, Kemble nevertheless refined aspects of Garrick's portrayal into his own. However it was the "towering and majestic" Sarah Siddons (Kemble's sister) who became a legend in the role of Lady Macbeth. In contrast to Hannah Pritchard's savage, demonic portrayal, Siddons' Lady Macbeth, while terrifying, was nevertheless – in the scenes in which she expresses her regret and remorse – tenderly human. And in portraying her actions as done out of love for her husband, Siddons deflected from him some of the moral responsibility for the play's carnage. Audiences seem to have found the sleepwalking scene particularly mesmerising: Hazlitt said of it that "all her gestures were involuntary and mechanical ... She glided on and off the stage almost like an apparition."
In 1794, Kemble dispensed with the ghost of Banquo altogether, allowing the audience to see Macbeth's reaction as his wife and guests see it, and relying upon the fact that the play was so well known that his audience would already be aware that a ghost enters at that point.
Ferdinand Fleck, notable as the first German actor to present Shakespeare's tragic roles in their fullness, played Macbeth at the Berlin National Theatre from 1787. Unlike his English counterparts, he portrayed the character as achieving his stature after the murder of Duncan, growing in presence and confidence: thereby enabling stark contrasts, such as in the banquet scene, which he ended babbling like a child.
Nineteenth century.
"Everyone seems to think Mrs McB is a "Monstrousness" & "I" can only see she's a "woman" – a mistaken woman – & "weak" – not a Dove – of course not – "but first of all a wife."" 
—Ellen Terry 
Performances outside the patent theatres were instrumental in bringing the monopoly to an end. Robert Elliston, for example, produced a popular adaptation of "Macbeth" in 1809 at the Royal Circus described in its publicity as "this matchless piece of pantomimic and choral performance", which circumvented the illegality of speaking Shakespeare's words through mimed action, singing, and doggerel verse written by J. C. Cross.
In 1809, in an unsuccessful attempt to take Covent Garden upmarket, Kemble installed private boxes, increasing admission prices to pay for the improvements. The inaugural run at the newly renovated theatre was "Macbeth", which was disrupted for over two months with cries of "Old prices!" and "No private boxes!" until Kemble capitulated to the protestors' demands.
Edmund Kean at Drury Lane gave a psychological portrayal of the central character, with a common touch, but was ultimately unsuccessful in the role. However he did pave the way for the most acclaimed performance of the nineteenth century, that of William Charles Macready. Macready played the role over a 30-year period, firstly at Covent Garden in 1820 and finally in his retirement performance. Although his playing evolved over the years, it was noted throughout for the tension between the idealistic aspects and the weaker, venal aspects of Macbeth's character. His staging was full of spectacle, including several elaborate royal processions.
In 1843 the Theatres Regulation Act finally brought the patent companies' monopoly to an end. From that time until the end of the Victorian era, London theatre was dominated by the actor-managers, and the style of presentation was "pictorial" – proscenium stages filled with spectacular stage-pictures, often featuring complex scenery, large casts in elaborate costumes, and frequent use of tableaux vivant. Charles Kean (son of Edmund), at London's Princess's Theatre from 1850 to 1859, took an antiquarian view of Shakespeare performance, setting his "Macbeth" in a historically accurate eleventh-century Scotland. His leading lady, Ellen Tree, created a sense of the character's inner life: The Times critic saying "The countenance which she assumed ... when luring on Macbeth in his course of crime, was actually appalling in intensity, as if it denoted a hunger after guilt." At the same time, special effects were becoming popular: for example in Samuel Phelps' "Macbeth" the witches performed behind green gauze, enabling them to appear and disappear using stage lighting.
In 1849, rival performances of the play sparked the Astor Place Riot in Manhattan. The popular American actor Edwin Forrest, whose Macbeth was said to be like "the ferocious chief of a barbarous tribe" played the central role at the Broadway Theatre to popular acclaim, while the "cerebral and patrician" English actor Macready, playing the same role at the Astor Place Opera House, suffered constant heckling. The existing enmity between the two men (Forrest had openly hissed Macready at a recent performance of "Hamlet" in Britain) was taken up by Forrest's supporters – formed from the working class and lower middle class and anti-British agitators, keen to attack the upper-class pro-British patrons of the Opera House and the colonially-minded Macready. Nevertheless, Macready performed the role again three days later to a packed house while an angry mob gathered outside. The militia tasked with controlling the situation fired into the mob. In total, 31 rioters were killed and over 100 injured.
Charlotte Cushman is unique among nineteenth century interpreters of Shakespeare in achieving stardom in roles of both genders. Her New York debut was as Lady Macbeth in 1836, and she would later be admired in London in the same role in the mid-1840s. Helen Faucit was considered the embodiment of early-Victorian notions of femininity. But for this reason she largely failed when she eventually played Lady Macbeth in 1864: her serious attempt to embody the coarser aspects of Lady Macbeth's character jarred harshly with her public image. Adelaide Ristori, the great Italian actress, brought her Lady Macbeth to London in 1863 in Italian, and again in 1873 in an English translation cut in such a way as to be, in effect, Lady Macbeth's tragedy.
Henry Irving was the most successful of the late-Victorian actor-managers, but his "Macbeth" failed to curry favour with audiences. His desire for psychological credibility reduced certain aspects of the role: He described Macbeth as a brave soldier but a moral coward, and played him untroubled by conscience – clearly already contemplating the murder of Duncan before his encounter with the witches. (Similar criticisms were made of Friedrich Mitterwurzer in Germany, whose performances of "Macbeth" had many unintentional parallels with Irving's.) Irving's leading lady was Ellen Terry, but her Lady Macbeth was unsuccessful with the public, for whom a century of performances influenced by Sarah Siddons had created expectations at odds with Terry's conception of the role.
Late nineteenth-century European Macbeths aimed for heroic stature, but at the expense of subtlety: Tommaso Salvini in Italy and Adalbert Matkowsky in Germany were said to inspire awe, but elicited little pity.
Twentieth century to present.
"And then Lady Macbeth says 'He that's coming / Must be provided for.' It's an amazing line. She's going to play hostess to Duncan at Dunsinane, and 'provide' is what gracious hostesses always do. It's a wonder of a line to play because the reverberations do the acting for you, make the audience go 'Aaaagh!'" 
—Sinéad Cusack 
Two developments changed the nature of "Macbeth" performance in the twentieth century: firstly developments in the craft of acting itself, especially the ideas of Stanislavski and Brecht, and secondly the rise of the dictator as a political icon. The latter has not always assisted the performance: it is difficult to sympathise with a Macbeth based on Hitler, Stalin or Idi Amin.
Barry Jackson, at the Birmingham Repertory Theatre in 1923, was the first of the twentieth-century directors to costume "Macbeth" in modern dress.
In 1936, a decade before his film adaptation of the play, Orson Welles directed "Macbeth" for the Negro Theatre Unit of the Federal Theatre Project at the Lafayette Theatre in Harlem, using black actors and setting the action in Haiti: with drums and Voodoo rituals to establish the Witches scenes. The production, dubbed "The Voodoo Macbeth", proved inflammatory in the aftermath of the Harlem riots, accused of making fun of black culture and as "a campaign to burlesque negroes" until Welles persuaded crowds that his use of black actors and voodoo made important cultural statements.
A performance which is frequently referenced as an example of the play's curse was the outdoor production directed by Burgess Meredith in 1953 in the British colony of Bermuda, and starring Charlton Heston. Using the imposing spectacle of Fort St. Catherine's as a key element of the set, the production was plagued by a host of mishaps, including Charlton Heston being burned when his tights caught fire.
The critical consensus is that there have been three great Macbeths on the English-speaking stage in the twentieth century, all of them commencing at Stratford-upon-Avon: Laurence Olivier in 1955, Ian McKellen in 1976 and Antony Sher in 1999. Olivier's portrayal (directed by Glen Byam Shaw, with Vivien Leigh as Lady Macbeth) was immediately hailed as a masterpiece. Kenneth Tynan expressed the view that it succeeded because Olivier built the role to a climax at the end of the play, whereas most actors spend all they have in the first two acts.
The play caused grave difficulties for the Royal Shakespeare Company, especially at the (then) Shakespeare Memorial Theatre. Peter Hall's 1967 production was (in Michael Billington's words) "an acknowledged disaster" with the use of real leaves from Birnham Wood getting unsolicited first-night laughs, and Trevor Nunn's 1974 production was (Billington again) "an over-elaborate religious spectacle". But Nunn achieved success for the RSC in his 1976 production at the intimate Other Place, with Ian McKellen and Judi Dench in the central roles. A small cast worked within a simple circle, and McKellen's Macbeth had nothing noble or likeable about him, being a manipulator in a world of manipulative characters. They were a young couple, physically passionate, "not monsters but recognisable human beings," but their relationship atrophied as the action progressed.
In Soviet-controlled Prague in 1977, faced with the illegality of working in theatres, Pavel Kohout adapted "Macbeth" into a 75-minute abridgement for five actors, suitable for "bringing a show in a suitcase to people's homes."
Spectacle became unfashionable in Western theatre throughout the twentieth century. In East Asia, however, spectacular productions have achieved great success, including Yukio Ninagawa's 1980 production with Masane Tsukayama as Macbeth, set in the sixteenth century Japanese Civil War. The same director's tour of London in 1987 was widely praised by critics, even though (like most of their audience) they were unable to understand the significance of Macbeth's gestures, the huge Buddhist altar dominating the set, or the petals falling from the cherry trees. Xu Xiaozhong's 1980 Central Academy of Drama production in Beijing made every effort to be unpolitical (necessary in the aftermath of the Cultural Revolution): yet audiences still perceived correspondences between the central character (who the director had actually modelled on Louis Napoleon) and Mao Zedong. Shakespeare has often been adapted to indigenous theatre traditions, for example the "Kunju Macbeth" of Huang Zuolin performed at the inaugural Chinese Shakespeare Festival of 1986. Similarly, B. V. Karanth's "Barnam Vana" of 1979 had adapted "Macbeth" to the Yakshagana tradition of Karnataka, India. In 1997, Lokendra Arambam created "Stage of Blood" merging a range of martial arts, dance and gymnastic styles from Manipur, performed in Imphal and in England. The stage was literally a raft on a lake.
The RSC again achieved critical success in Gregory Doran's 1999 production at The Swan, with Antony Sher and Harriet Walter in the central roles, once again demonstrating the suitability of the play to smaller venues. Doran's witches spoke their lines to a theatre in absolute darkness and the opening visual image was the entrance of Macbeth and Banquo in the berets and fatigues of modern warfare, carried on the shoulders of triumphant troops. In contrast to Nunn, Doran presented a world in which king Duncan and his soldiers were ultimately benign and honest, heightening the deviance of Macbeth (who seems genuinely surprised by the witches' prophesies) and Lady Macbeth, in plotting to kill the king. The play said little about politics, instead powerfully presenting its central characters' psychological collapse.
While the play has been translated and performed in various languages in different parts of the world, "Media Artists" was the first to stage its Punjabi adaptation in India. The adaptation by Balram and the play directed by Samuel John have been universally acknowledged as a milestone in Punjabi theatre. The unique attempt involved trained theatre experts and the actors taken from rural background in Punjab, India. Punjabi folk music imbued the play with the native ethos as the English setting of the Shakespeare's play was transposed into Punjabi milieu.
Screen.
Twentieth century.
The earliest known film "Macbeth" was 1905's American short "Death Scene From Macbeth", and short versions were produced in Italy in 1909 and France in 1910. Two notable early versions are lost: Ludwig Landmann produced a 47-minute version in Germany in 1913, and D. W. Griffith produced a 1916 version in America featuring the noted stage actor Herbert Beerbohm Tree. Tree is said to have had great difficulties adapting to the new medium, and especially in confining himself to the small number of lines in the (silent) screenplay, until an ingenious cameraman allowed him to play his entire part to an empty camera, after which a real camera shot the film.
In 1947, David Bradley produced an independent film of "Macbeth", intended for distribution to schools, most notable for the designer of its eighty-three costumes: the soon-to-be-famous Charlton Heston.
Orson Welles' 1948 "Macbeth", in the director's words a "violently sketched charcoal drawing of a great play," was filmed in only 23 days and on a budget of just $700,000. These filming conditions allowed only a single abstract set, and eclectic costumes. Dialogue was pre-recorded, enabling the actors to perform very long individual takes, including one of over ten minutes surrounding the death of Duncan. Welles himself played the central character, who dominates the film, measured both by his time on screen, and by physical presence: high-angle and low-angle shots and deep-focus close-ups are used to distort his size in comparison to other characters. Welles retained from his own 1936 stage production the image of a Voodoo doll controlling the fate of the central character: and at the end it is the doll we see beheaded. The film's allegorical aspect is heightened by Welles' introduction of a non-Shakespearean character, the Holy Father (played by Alan Napier), in opposition to the witches, speaking lines taken from Shakespeare's Ross, Angus and the Old Man. Contemporary reviews were largely negative, particularly criticising Welles' unsympathetic portrayal of the central character. "Newsweek" commented: "His Macbeth is a static, two-dimensional creature as capable of evil in the first scene as in the final hours of his bloody reign."
"Joe MacBeth" (Ken Hughes, 1955) established the tradition of resetting the "Macbeth" story among 20th-century gangsters. Others to do so include "Men of Respect" (William Reilly, 1991), "Maqbool" (Vishal Bhardwaj, 2003) and Geoffrey Wright's Australian 2006 "Macbeth".
In 1957, Akira Kurosawa used the "Macbeth" story as the basis for the "universally acclaimed" "Kumunosu-jo" (in English known as "Throne of Blood" or (the literal translation of its title) "Cobweb Castle"). The film is a Japanese period-piece (jidai-geki), drawing upon elements of Noh theatre, especially in its depiction of the evil spirit who takes the part of Shakespeare's witches, and of Asaji, the Lady Macbeth character, played by Isuzu Yamada, and upon Kabuki Theatre in its depiction of Washizu, the Macbeth character, played by Toshiro Mifune. In a twist on Shakespeare's ending, the tyrant (having witnessed Cobweb Forest come to Cobweb Castle) is killed by volleys of arrows from his own archers.
George Schaefer directed Maurice Evans and Judith Anderson in a 1960 made-for-TV film which later had a limited European theatrical release. (The three had also worked together on the earlier "Hallmark Hall of Fame" 1954 TV version of the play.) Neither of the central couple was able to adapt their stage acting style to the screen successfully, leading to their roles being described by critics as "recited" rather than "acted".
Roman Polanski's 1971 "Macbeth" was the director's first film after the brutal murder of his wife, Sharon Tate, and reflected his determination to "show ["Macbeth's"] violence the way it is ... [because] if you don't show it realistically then that's immoral and harmful." His film showed deaths only reported in the play, including the execution of Cawdor, and Macbeth stabbing Duncan, and its violence was "intense and incessant." Made in the aftermath of Zeffirelli's youthful "Romeo and Juliet", and financed by "Playboy" mogul Hugh Hefner, Polanski's film featured a young sexy lead couple, played by Jon Finch (28) and by Francesca Annis (25), who controversially performed the sleepwalking scene nude. The unsettling film score, provided by the Third Ear Band, invoked "discord and dissonance." While using Shakespeare's words, Polanski alters aspects of Shakespeare's story, turning the minor character Ross into a ruthless Machiavellian, and adding an epilogue to the play in which Donalbain (younger son of Duncan) arrives at the witches' lair, indicating that the cycle of violence will begin again.
In 1973, the Virginia Museum Theater (VMT, now the Leslie Cheek Theater), presented "Macbeth," starring E.G. Marshall. Dubbed by the "New York Times" as the "'Fowler' Macbeth" after director Keith Fowler, it was described by Clive Barnes as "splendidly vigorous, forcefully immediate... probably the goriest Shakespearean production I have seen since Peter Brook's 'Titus Andronicus'."
The Nunn/McKellen/Dench RSC Other Place stage performance discussed above was adapted for TV and broadcast by Thames Television (see "Macbeth (1978 film)").
William Reilly's 1991 "Men of Respect", another film to set the "Macbeth" story among gangsters, has been praised for its accuracy in depicting Mafia rituals, said to be more authentic than those in "The Godfather" or "GoodFellas". However the film failed to please audiences or critics: Leonard Maltin found it "pretentious" and "unintentionally comic" and Daniel Rosenthal describes it as "providing the most risible chunks of modernised Shakespeare in screen history." In 1992 S4C produced a cel-animated "Macbeth" for the series "", and in 1997 Jeremy Freeston directed Jason Connery and Helen Baxendale in a low budget, fairly full-text, version.
In Shakespeare's script, the actor playing Banquo must enter the stage as a ghost. The major film versions have usually taken the opportunity to provide a double perspective: Banquo visible to the audience from Macbeth's perspective, but invisible from the perspective of other characters. Television versions, however, have often taken the third approach of leaving Banquo invisible to viewers, thereby portraying Banquo's ghost as merely Macbeth's delusion. This approach is taken in the 1978 Thames TV production, Jack Gold's 1983 version for BBC Television Shakespeare, and in Penny Woolcock's 1997 "Macbeth on the Estate". "Macbeth on the Estate" largely dispensed with the supernatural in favour of the drug-crime driven realism of characters living on a Birmingham housing estate: except for the three "weird" (in the modern sense of the word) children who prophesy Macbeth's fate. This production used Shakespeare's language, but encouraged the actors – many of whom were locals, not professionals – to speak it naturalistically.
Twenty-first century.
Twenty-first-century cinema has re-interpreted "Macbeth", relocating "Scotland" elsewhere: "Maqbool" to Mumbai, "Scotland, PA" to Pennsylvania, Geoffrey Wright's "Macbeth" to Melbourne, and Allison L. LiCalsi's 2001 "Macbeth: The Comedy" to a location only differentiated from the reality of New Jersey, where it was filmed, through signifiers such as tartan, Scottish flags and bagpipes. Alexander Abela's 2000 "Makibefo" was set among, and starred, residents of Faux Cap, a remote fishing community in Madagascar. Leonardo Henriquez' 2000 "Sangrador" (in English: "Bleeder") set the story among Venezuelan bandits and presented a shockingly visualised horror version.
Billy Morrissette's "Scotland, PA" reframes the "Macbeth" story as a comedy-thriller set in a 1975 fast-food restaurant, and features James LeGros in the Macbeth role and Maura Tierney as Pat, the Lady Macbeth character: "We're not bad people, Mac. We're just under-achievers who have to make up for lost time." Christopher Walken plays vegetarian detective Ernie McDuff who (in the words of Daniel Rosenthal) "[applies] his uniquely offbeat menacing delivery to innocuous lines." "Scotland, PA"'s conceit of resetting the "Macbeth" story at a restaurant was followed in BBC Television's 2005 ShakespeaRe-Told adaptation.
Vishal Bhardwaj's 2003 "Maqbool", filmed in Hindi and Urdu and set in the Mumbai underworld, was produced in the Bollywood tradition, but heavily influenced by "Macbeth", by Francis Ford Coppola's 1972 "The Godfather" and by Luc Besson's 1994 "". It deviates from the "Macbeth" story in making the Macbeth character (Miyan Maqbool, played by Irfan Khan) a single man, lusting after the mistress (Nimmi, played by Tabbu) of the Duncan character (Jahangir Khan, known as Abbaji, played by Pankaj Kapoor). Another deviation is the comparative delay in the murder: Shakespeare's protagonists murder Duncan early in the play, but more than half of the film has passed by the time Nimmi and Miyan kill Abbaji.
In 2004 an "eccentric" Swedish/Norwegian film, based on Alex Scherpf's Ice Globe Theatre production of "Macbeth", was said by critic Daniel Rosenthal to owe "more to co-director Bo Landin's background in natural history documentaries than to Shakespeare." More conventional adaptations of 21st-century stage productions to television include Greg Doran's RSC production filmed in 2001 with Antony Sher and Harriet Walter in the central roles, and Rupert Goold's Chichester Festival Theatre "Macbeth" televised in 2010 with Patrick Stewart and Kate Fleetwood as the tragic couple. The cast of the latter felt that the history of their stage performance (moving from a small space at Chichester to a large proscenium arch stage in London to a huge auditorium in Brooklyn) made it easier for them to "re-scale", yet again, their performances for the cameras.
In 2006, Geoffrey Wright directed a Shakespearean-language, extremely violent "Macbeth" set in the Melbourne underworld. Sam Worthington played Macbeth. Victoria Hill played Lady Macbeth and shared the screenplay credits with Wright. The director considered her portrayal of Lady Macbeth to be the most sympathetic he had ever seen. In spite of the high level of violence and nudity (Macbeth has sex with the three naked schoolgirl witches as they prophesy his fate), intended to appeal to the young audiences that had flocked to "Romeo + Juliet", the film flopped at the box office.
Other media.
Literature.
There have been numerous literary adaptations and spin-offs from "Macbeth". Russian Novelist Nikolay Leskov told a variation of the story from Lady Macbeth's point of view in "Lady Macbeth of the Mtsensk District", which itself became a number of films and an opera by Shostakovich. Maurice Baring's 1911 "The Rehearsal" fictionalises Shakespeare's company's inept rehearsals for "Macbeth's" premiere. Gu Wuwei's 1916 play "The Usurper of State Power" adapted both "Macbeth" and "Hamlet" as a parody of contemporary events in China. The play has been used as a background for detective fiction (as in Marvin Kaye's 1976 "Bullets for Macbeth") and, in the case of Ngaio Marsh's last detective novel Light Thickens, the play takes centre stage as the rehearsal, production and run of a 'flawless' production is described in absorbing detail (so much so that her biographer describes the novel as effectively Marsh's third production of the play). But the play was also used as the basis of James Thurber's parody of the whodunit genre "The Macbeth Murder Mystery", in which the protagonist reads "Macbeth" applying the conventions of detective stories, and concludes that it must have been Macduff who murdered Duncan. Comics and graphic novels have utilised the play, or have dramatised the circumstances of its inception: Superman himself wrote the play for Shakespeare in the course of one night, in the 1947 "Shakespeare's Ghost Writer".
Macbeth has been adapted into plays dealing with the political and cultural concerns of many nations. Eugène Ionesco's "Macbett" satirised "Macbeth" as a meaningless succession of treachery and slaughter. Wale Ogunyemi's "A'are Akogun", first performed in Nigeria in 1968, mixed the English and Yoruba languages. Welcome Msomi's 1970 play "Umabatha" adapts "Macbeth" to Zulu culture, and was said by "The Independent" to be "more authentic than any modern "Macbeth"" in presenting a world in which a man's fighting ability is central to his identity. Joe de Graft adapted "Macbeth" as a battle to take over a powerful corporation in Ghana in his 1972 "Mambo" or "Let's Play Games, My Husband". Dev Virahsawmy's "Zeneral Macbeff", first performed in 1982, adapted the story to the local Creole and to the Mauritian political situation. (The same author later translated "Macbeth" itself into Mauritian creole, as "Trazedji Makbess".) And in 2000, Chuck Mike and the Nigerian Performance Studio Workshop produced "Mukbutu" as a direct commentary on the fragile nature of Nigerian democracy at the time.
Music and audio.
"Macbeth" is, with "The Tempest", one of the two most-performed Shakespeare plays on BBC Radio, with 20 productions between 1923 and 2005.
The extant version of "Macbeth", in the First Folio, contains dancing and music, including the song "Come Away Hecate" which exists in two collections of lute music (both c.1630, one of them being Drexel 4175) arranged by Robert Johnson. And, from the Restoration onwards, incidental music has frequently been composed for the play: including works by William Boyce in the eighteenth century. Davenant's use of dance in the witches' scenes was inherited by Garrick, which in turn influenced Giuseppe Verdi to incorporate a ballet around the witches' cauldron into his opera "Macbeth". Verdi's first Shakespeare-influenced opera, with libretto by Francesco Maria Piave, incorporated a number of striking arias for Lady Macbeth, giving her a prominence in the early part of the play which contrasts with the character's increasing isolation as the action continues: she ceases to sing duets and her sleepwalking confession is starkly contrasted with the "supported grief" of Macduff in the preceding scene. Other music influenced by the play includes Richard Strauss's 1890 symphonic poem "Macbeth". Duke Ellington and Billy Strayhorn incorporated themes depicting the female characters from "Macbeth" in the 1957 Shakespearean jazz suite "Such Sweet Thunder": the weird sisters juxtaposed with Iago (from "Othello"), and "Lady Mac" represented by ragtime piano because, as Ellington put it, "we suspect there was a little ragtime in her soul". Another Jazz collaboration to create hybrids of Shakespeare plays was that of Cleo Laine with Johnny Dankworth, who in Laine's 1964 "Shakespeare and All That Jazz" juxtaposed Titania's instructions to her fairies from "A Midsummer Night's Dream" with the witches' chant from "Macbeth". In 2000, Jag Panzer produced their heavy metal concept-album retelling "Thane to the Throne".
Visual arts.
The play has inspired numerous works of art. The scene in which Lady Macbeth seizes the daggers, as performed by Garrick and Mrs. Pritchard, was a touchstone throughout Henry Fuseli's career, including works in 1766, 1774 and 1812. The same performance was the subject of Johann Zoffany's painting of the Macbeths in 1768. In 1786, John Boydell announced his intention to found his Shakespeare Gallery. His chief innovation was to see the works of Shakespeare as history, rather than contemporary, so instead of including the (then fashionable) works depicting the great actors of the day on stage in modern dress, he commissioned works depicting the "action" of the plays. However the most notable works in the collection disregard this historicising principle: such as Fuseli's depiction of the naked and heroic Macbeth encountering the witches. William Blake's paintings were also influenced by Shakespeare, including his "Pity", inspired by Macbeth's "Pity, like a naked new-born babe, striding the blast." Sarah Siddons' triumph in the role of Lady Macbeth led Joshua Reynolds to depict her as "The Muse of Tragedy".
Notes.
Citations.
Unless otherwise specified, all citations of "Macbeth" refer to Muir (1984), and of other works of Shakespeare refer to Wells and Taylor (2005).
References.
</dl>

</doc>
<doc id="18870" url="http://en.wikipedia.org/wiki?curid=18870" title="Minor Threat">
Minor Threat

Minor Threat were a hardcore punk band, formed in 1980 in Washington, D.C. and disbanded in 1983. The band was relatively short-lived, but had a strong influence on the hardcore punk scene, both stylistically and in establishing a "do it yourself" (DIY) ethic for music distribution and concert promotion. Minor Threat's song "Straight Edge" became the eventual basis of the straight edge movement, while the band often professed their own "straight edge" ideals. AllMusic described Minor Threat's music as "iconic," and noted that their groundbreaking music "has held up better than [that of] most of their contemporaries."
Along with the fellow Washington, D.C. hardcore band Bad Brains and California band Black Flag, Minor Threat set the standard for many hardcore punk bands in the 1980s and 1990s. All of Minor Threat's recordings were released on Ian MacKaye and Jeff Nelson's own label, Dischord Records. The "Minor Threat" EP and their only full-length studio album "Out of Step" have received a number of accolades and are cited as landmarks of the hardcore punk genre.
History.
Formation and Early Years.
Prior to forming Minor Threat in 1980, vocalist Ian MacKaye and drummer Jeff Nelson had played bass and drums respectively in The Teen Idles while attending Wilson High School. During their two-year career within the flourishing Washington D.C. hardcore punk scene, The Teen Idles had gained a following of around one hundred fans (a sizable amount at the time), and were seen as only second within the scene to the contemporary Bad Brains. MacKaye and Nelson were strong believers in the DIY mentality and an independent, underground music scene. After the break-up of The Teen Idles, they used the money earned through the band to create Dischord Records, an independent record label that would host the releases of The Teen Idles, Minor Threat, and numerous other D.C. punk bands.
Eager to start a new band after The Teen Idles, MacKaye and Nelson recruited guitarist Lyle Preslar and bassist Brian Baker. They played their first performance in December 1980 to fifty people in a basement, opening for Bad Brains, The Untouchables, Black Market Baby and S.O.A., all D.C. bands.
The band's first 7" EPs, "Minor Threat" and "In My Eyes", were released in 1981. The group became popular regionally and toured the east coast and Midwest.
"Straight Edge," a song from the band's first EP, helped to inspire the straight edge movement. The lyrics of the song call for abstinence from alcohol and other drugs, a novel ideology for rock musicians which initially found a small but dedicated following. Other prominent groups that subsequently advocated the straight edge stance include SS Decontrol and 7 Seconds.
Another Minor Threat song from the second EP, "Out of Step", further demonstrates the belief: "Don't smoke/Don't drink/Don't fuck/At least I can fucking think/I can't keep up/I'm out of step with the world."
The "I" in the lyrics was usually only implied, mainly because it did not quite fit the rhythm of the song, like the version on the 1984 album "Minor Threat". The version on "Out of Step" is slower, allotting a bridge where MacKaye explains his philosophy of straight edge, explaining that straight edge "is not a set of rules; I'm not telling you what to do. All I'm saying is there are three things, that are like so important to the whole world that I don't happen to find much importance in, whether it's fucking, or whether it's playing golf, because of that, I feel... (chorus)". Some of the other members of Minor Threat, Jeff Nelson in particular, took exception to what they saw as MacKaye's imperious attitude on the song.
Minor Threat's song "Guilty of Being White" led to some accusations of racism, but MacKaye has strongly denied such intentions and said that some listeners misinterpreted his words. He claims that his experiences attending Wilson High School, whose student population was 70 percent black, inspired the song. There, many students bullied MacKaye and his friends. Thrash metal band Slayer later covered the song, with the last iteration of the lyric "Guilty of being white" changed to "Guilty of being right." In an interview, MacKaye stated that he was offended that some perceived racist overtones in the lyrics, saying, "To me, at the time and now, it seemed clear it's an anti-racist song. Of course, it didn't occur to me at the time I wrote it that anybody outside of my twenty or thirty friends who I was singing to would ever have to actually ponder the lyrics or even consider them."
Hiatus.
In the time between the release of the band's second seven-inch EP and the "Out of Step" record, the band briefly split when guitarist Lyle Preslar moved to Illinois to attend college for a semester at Northwestern University, Preslar was a member of Big Black for a few tempestuous rehearsals. During that period, MacKaye and Nelson put together a studio-only project called Skewbald/Grand Union; in a reflection of the slowly increasing disagreements between the two musicians, they were unable to decide on one name. The group recorded three untitled songs, which would be released posthumously as Dischord's 50th release. During Minor Threat's inactive period, Brian Baker also briefly played guitar for Government Issue and appeared on the "Make an Effort" EP.
In March 1982, at the urging of Bad Brains' H.R., Preslar left college to re-form Minor Threat. Shortly afterwards, the cuts "Minor Threat" and "In My Eyes" were rereleased as "Minor Threat" in 1984. The reunited band featured an expanded lineup: Steve Hansgen joined as the band's bassist and Baker switched to second guitar.
When "Out of Step" was rerecorded for the LP "Out of Step", MacKaye inserted a spoken section explaining, "This is not a set of rules..." An ideological door had already been opened, however, and by 1982, some straight-edge punks, such as followers of the band SS Decontrol, were swatting beers out of people's hands at clubs.
Breakup.
Minor Threat broke up in 1983. A contributing factor was disagreement over musical direction. MacKaye was allegedly skipping practice sessions towards the end of the band's career, and he wrote the lyrics to the songs on the "Salad Days" EP in the studio. That was quite a contrast with the earlier recordings, as he had written and co-written the music for much of the band's early material. Minor Threat, which had returned to being a four-piece group with the departure of Hansgen, played its last show on September 23, 1983, with go-go band Trouble Funk and the Big Boys at the Lansburgh Cultural Center in Washington, D.C. They ended their set with "Last Song", which was the original title of "Salad Days".
Alternatively, MacKaye stated that he did not "check out" on hardcore, but in fact hardcore "checked out". Explaining this, he stated that at a 1984 Minutemen show, a fan struck MacKaye's younger brother Alec in the face, and he punched the fan back, then realizing that the violence was "stupid", and that he saw his role in the stupidity. MacKaye claimed that immediately after this he decided to leave the hardcore scene.
Subsequent activities.
MacKaye went on to found Embrace with former members of The Faith, the obscure Egg Hunt with Jeff Nelson and later Fugazi and The Evens, as well as collaborating on Pailhead.
Brian Baker went on to play in Junkyard, The Meatmen, Dag Nasty and Government Issue. He currently plays in Bad Religion.
Lyle Preslar was briefly a member of Glenn Danzig's Samhain, and his playing appears on a few songs on the band's first record. He joined The Meatmen in 1984, along with fellow Minor Threat member Brian Baker. He later ran Caroline Records, signing and working with (among others) Peter Gabriel, Ben Folds, Chemical Brothers, and Idaho, and ran marketing for Sire Records. He graduated from Rutgers University law school and lives in New Jersey.
Jeff Nelson played less-frantic alternative rock with Three and The High-Back Chairs before retiring from live performance. He runs his own label, Adult Swim Records, distributed by Dischord, and is a graphic artist and a political activist in Toledo, Ohio. The band's own Dischord Records released material by many bands from the Washington, D.C., area, such as Government Issue, Void, Scream, Fugazi, Artificial Peace, Rites of Spring, Gray Matter, and Dag Nasty, and has become a respected independent record label.
Steve Hansgen formed Second Wind with Minor Threat roadie/Untouchables drummer Rich Moore. He also worked with Tool in 1992 on the production of their first EP, "Opiate".
Copyright issues.
"Major Threat".
In 2005, a mock-up of the cover of Minor Threat's first EP (also used on the "Minor Threat" LP and "Complete Discography" CD) was copied by athletic footwear manufacturer Nike for use on a promotional poster for a skateboarding tour called "Major Threat". Nike also altered Minor Threat's logo (designed by Jeff Nelson) for the same campaign, as well as featuring Nike shoes in the new picture, rather than the combat boots worn by Ian MacKaye's younger brother Alec on the original.
MacKaye issued a press statement condemning Nike's actions and said that he would discuss legal options with the other members of the band. Meanwhile, fans, at the encouragement of Dischord, organized a letter-writing campaign protesting Nike's infringement. On June 27, 2005, Nike issued a statement apologizing to Minor Threat, Dischord Records, and their fans for the "Major Threat" campaign and said that all promotional artwork (print and digital) that they could acquire were destroyed.
"Salad Days".
On October 29, 2005, Fox played the first few seconds of Minor Threat's "Salad Days" during an NFL broadcast. Use of the song was not cleared by Dischord Records or any of the members of Minor Threat. Fox claimed that the clip was too short to have violated any copyrights.
Wheelhouse Pickles.
In 2007, Brooklyn-based company Wheelhouse Pickles marketed a pepper sauce named "Minor Threat Sauce". Requesting only that the original label design (which was based on the "Bottled Violence" artwork) be amended, Ian MacKaye gave the product his endorsement. A small mention of this was made in music magazine "Revolver", where MacKaye commented "I don't really like hot sauce but I like the Minor Threat stuff".

</doc>
<doc id="18875" url="http://en.wikipedia.org/wiki?curid=18875" title="Mental event">
Mental event

A mental event is a particular occurrence of something going on in the mind or mind substitute that make up the conscious mind of an individual. It can be a thought, a dream, a feeling, a realization, or any other mental activity. Some believe that mental events are not limited to human thought but can be associated with animal and artificial intelligence as well. The issue of whether mental events are identical with complex physical events, or whether such an identity even makes sense, is central to the mind-body problem.
Mental events often occur because of physical events, however, physical events do not always occur because of mental events. 
However, some see such a distinction as erroneous, and state that the mental and the physical are the very same property which cause any event(s).

</doc>
<doc id="18878" url="http://en.wikipedia.org/wiki?curid=18878" title="Monopoly">
Monopoly

Countering monopolies.
According to professor [[Milton Friedman]], laws against monopolies cause more harm than good, but unnecessary monopolies should be countered by removing [[tariff]]s and other [[regulation]] that upholds monopolies.
A monopoly can seldom be established within a country without overt and covert government assistance in the form of a tariff or some other device. It is close to impossible to do so on a world scale. The [[De Beers]] diamond monopoly is the only one we know of that appears to have succeeded (and even De Beers are protected by various laws against so called "illicit" diamond trade). – In a world of [[free trade]], international cartels would disappear even more quickly.—Milton Friedman, "[[Free to Choose]]", p. 53–54
However, professor Steve H. Hanke believes that although private monopolies are more efficient than public ones, often by a factor of two, sometimes private natural monopolies, such as local water distribution, should be regulated (not prohibited) by, e.g., price auctions.
Thomas DiLorenzo asserts, however, that during the early days of utility companies where there was little regulation, there were no natural monopolies and there was competition. Only when companies realized that they could gain power through government did monopolies begin to form.
Further reading.
</dl>
External links.
[[Category:Commons category without a link on Wikidata]]
Criticism.
[[Category:Market structure and pricing]]
[[Category:Economic problems]]
[[Category:Monopoly (economics)| ]]

</doc>
<doc id="18879" url="http://en.wikipedia.org/wiki?curid=18879" title="Massachusetts Institute of Technology">
Massachusetts Institute of Technology

The Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts. Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. Researchers worked on computers, radar, and inertial guidance during World War II and the Cold War. Post-war defense research contributed to the rapid expansion of the faculty and campus under James Killian. The current 168 acre campus opened in 1916 and extends over 1 mi along the northern bank of the Charles River basin.
MIT, with five schools and one college which contain a total of 32 departments, is traditionally known for research and education in the physical sciences and engineering, and more recently in biology, economics, linguistics, and management as well. The "Engineers" sponsor 31 sports, most teams of which compete in the NCAA Division III's New England Women's and Men's Athletic Conference; the Division I rowing programs compete as part of the EARC and EAWRC.
MIT is often cited as among the world's top universities. s of 2014[ [update]], 81 Nobel laureates, 52 National Medal of Science recipients, 45 Rhodes Scholars, 38 MacArthur Fellows, and 2 Fields Medalists have been affiliated with MIT. MIT has a strong entrepreneurial culture and the aggregated revenues of companies founded by MIT alumni would rank as the eleventh-largest economy in the world.
History.
Foundation and vision.
In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed. A proposal by William Barton Rogers a charter for the incorporation of the Massachusetts Institute of Technology, signed by the governor of Massachusetts on April 10, 1861.
Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances. He did not wish to found a professional school, but a combination with elements of both professional and liberal education, proposing that:
"The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws."The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.
Early developments.
Two days after the charter was issued, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865. The new institute had a mission that matched the intent of the 1862 Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes", and was a land-grant school. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.
MIT was informally called "Boston Tech". The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date. Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker. Programs in electrical, chemical, marine, and sanitary engineering were introduced, new buildings were built, and the size of the student body increased to more than one thousand.
The curriculum drifted to a vocational emphasis, with less focus on theoretical science. The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School. There would be at least six attempts to absorb MIT into Harvard. In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni. However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.
In 1916, MIT moved to a spacious new campus largely consisting of filled land on a mile-long tract along the Cambridge side of the Charles River. The neoclassical "New Technology" campus was designed by William W. Bosworth and had been funded largely by anonymous donations from a mysterious "Mr. Smith," starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million in cash and Kodak stock to MIT.
Curricular reforms.
In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios. The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering." Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding. The school was elected to the Association of American Universities in 1934.
Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities. The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs. The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.
Defense research.
MIT's involvement in military research surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT. Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area. Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory; the development of a digital computer for flight simulations under Project Whirlwind; and high-speed and high-altitude photography under Harold Edgerton. By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush), employing nearly 4000 in the Radiation Laboratory alone and receiving in excess of $100 million ($ billion in 2012 dollars) before 1946. Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.
These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities. The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.
In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research. The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems. MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the Lincoln Laboratory facility in 1973 in response to the protests. The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities. Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil.
Recent history.
MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies, students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like "Spacewar!" and created much of modern hacker slang and culture. Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology; the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee; the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002; and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.
MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs. Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center. Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest. In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.
In 2001, inspired by the open source and open access movements, MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabuses, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed. While the cost of supporting and hosting the project is high, OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages. In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee. The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content.
Three days after the Boston Marathon bombings of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day. One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada. On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".
Campus.
MIT's 168 acre campus spans approximately a mile of the north side of the Charles River basin in the city of Cambridge. The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot. The Kendall MBTA Red Line station is located on the far northeastern edge of the campus in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings as well as socio-economically diverse residential neighborhoods.
Each building at MIT has a number (possibly preceded by a "W", "N", "E", or "NW") designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings. Many of the buildings are connected above ground as well as through an extensive network of underground tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.
MIT's on-campus nuclear reactor is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial, but MIT maintains that it is well-secured. In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building" that was designed by architect Frank O. Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.
Other notable campus facilities include a pressurized wind tunnel and a towing tank for testing ship and ocean structure designs. MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering 9400000 sqft of campus.
In 2001, the Environmental Protection Agency sued MIT for violating Clean Water Act and Clean Air Act with regard to its hazardous waste storage and disposal procedures. MIT settled the suit by paying a $155,000 fine and launching three environmental projects. In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.
The MIT Police with state and local authorities, in the 2009-2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.
Architecture.
MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States, and it has a history of commissioning progressive buildings. The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the US. Bosworth's design was influenced by the City Beautiful Movement of the early 1900s, and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where commencement is held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers. The imposing Building 7 atrium along Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.
Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture. More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005) and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture". These buildings have not always been well received; in 2010, "The Princeton Review" included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".
Housing.
Undergraduates are guaranteed four-year housing in one of MIT's 12 undergraduate dormitories. Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters. Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the Yale Daily News Staff's "The Insider's Guide to the Colleges, 2010", "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture." MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.
MIT has a very active Greek and co-op housing system, which includes 36 fraternities, sororities, and independent living groups (FSILGs). In 2012, 90% of all undergraduates lived on MIT-affiliated housing, 46% of the men in fraternities and 29% of the women in sororities. Most FSILGs are located across the river in the Back Bay owing to MIT's history there, and there is also a cluster of fraternities on MIT's West Campus. After the 1997 death of Scott Krueger, a new member at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002. Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy did not take effect until 2002 after Simmons Hall opened.
Organization and administration.
MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation. The current board consists of 43 members elected to five-year terms, 25 life members who vote until their 75th birthday, 3 elected officers (President, Treasurer, and Secretary), and 4 "ex officio" members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court). The board is chaired by John S. Reed, the former chairman of the New York Stock Exchange and Citigroup. The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty. MIT's endowment and other financial assets are managed through a subsidiary MIT Investment Management Company (MITIMCo). Valued at $9.7 billion in 2011, MIT's endowment is the sixth-largest among American colleges and universities.
MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine. While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs, the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President. The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.
Academics.
MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs. The university has been accredited by the New England Association of Schools and Colleges since 1929. MIT operates on a 4–1–4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester beginning in early February and ending in late May.
MIT students refer to both their majors and classes using numbers or acronyms alone. Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is Course 1, while Linguistics and Philosophy is Course 24. Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; the introductory calculus-based classical mechanics course is simply "8.01" at MIT.
Undergraduate program.
The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and is selective, admitting few transfer students and 7.7% of its applicants in the 2013–2014 application season. MIT offers 44 undergraduate degrees across its five schools. In the 2010–2011 academic year, 1,161 bachelor of science (abbreviated "SB") degrees were granted, the only type of undergraduate degree MIT now awards. In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (Course 6–2), Computer Science and Engineering (Course 6–3), Mechanical Engineering (Course 2), Physics (Course 8), and Mathematics (Course 18).
All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs). The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive", including "substantial instruction and practice in oral presentation". Finally, all students are required to complete a swimming test; non-varsity athletes must also take four quarters of physical education classes.
Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and tests. Although keeping up with the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose", the freshmen retention rate at MIT is similar to that at other national research universities. The "pass/no-record" grading system relieves some of the pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded. (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.
In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly. A substantial majority of undergraduates participate. Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.
In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published "The Hidden Curriculum," arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.
Graduate program.
MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees. The Institute offers graduate programs leading to academic degrees such as the Master of Science (MS), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD); professional degrees such as Master of Architecture (MArch), Master of Business Administration (MBA), Master of City Planning (MCP), Master of Engineering (MEng), Master of Finance (MFin) and Master of Science in Real Estate Development (MSRED),; and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School).
Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).
MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11. In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%), and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.
University rankings.
MIT places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences. For several years, "U.S. News & World Report", the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report. In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.
In 2014, "Money" magazine ranked MIT as #3 in the US "Best Colleges for Your Money", based on its assessment of "the most bang for your tuition buck", factoring in quality of education, affordability, and career outcomes. s of 2014[ [update]], "Forbes" magazine rated MIT as the #2 "Most Entrepreneurial University", based on the percentage of alumni and students self-identifying as founders or business owners on LinkedIn. In 2015, Brookings Fellow Jonathan Rothwell issued a report "Beyond College Rankings", placing MIT as #3 in the US, with an estimated 45% value-added to mid-career salary.
Collaborations.
The university historically pioneered research and training collaborations between academia, industry and government.  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.  In 1948, Compton established the MIT Industrial Liaison Program.  Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese — firms that were competing with struggling American businesses. On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940. MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.
The Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships. While the Ivy League institutions settled, MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students. MIT ultimately prevailed when the Justice Department dropped the case in 1994.
MIT's proximity to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute. In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees. A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge. MIT has more modest cross-registration programs with Boston University, Brandeis University, Tufts University, Massachusetts College of Art, and the School of the Museum of Fine Arts, Boston.
MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Singapore-MIT Alliance, MIT-Politecnico di Milano, MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.
The mass-market magazine "Technology Review" is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine. The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events, and social issues.
Libraries, collections, and museums.
The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries. Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music, the List Visual Arts Center's rotating exhibitions of contemporary art, and the Compton Gallery's cross-disciplinary exhibitions. MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.
The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The Museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT’s science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".
Research.
MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity; research expenditures totaled $718.2 million in 2009. The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million. MIT employs approximately 1300 researchers in addition to faculty. In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties. Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.
In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers. Harold Eugene Edgerton was a pioneer in high speed photography and sonar. Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory. In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography. At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.
Current and previous physics faculty have won eight Nobel Prizes, four Dirac Medals, and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory. Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods. MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology. Professor Eric Lander was one of the principal leaders of the Human Genome Project. Positronium atoms, synthetic penicillin, synthetic self-replicating molecules, and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT. Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain".
In the domain of humanities, arts, and social sciences, MIT economists have been awarded five Nobel Prizes and nine John Bates Clark Medals. Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology. The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research, has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.
Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 38 people associated with MIT. Four Pulitzer Prize–winning writers currently work at or have retired from MIT. Four current or former faculty are members of the American Academy of Arts and Letters.
Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991. Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed. Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.
Traditions and student activities.
The faculty and student body highly value meritocracy and technical proficiency. MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation. However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.
Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat". Originally created in 1929, the ring's official name is the "Standard Technology Ring." The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver. The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise," "Institute Has The Finest Professors," "It's Hard to Fondle Penguins," and other variations, has occasionally been featured on the ring given its historical prominence in student culture.
Activities.
MIT has over 380 recognized student activity groups, including a campus radio station, "The Tech" student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.
The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are the 6.270, 6.370, and MasLab competitions, the annual "mystery hunt", and Charm School. More than 250 students pursue externships annually at companies in the US and abroad.
Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes. Recent high-profile hacks have included the abduction of Caltech's cannon, reconstructing a Wright Flyer atop the Great Dome, and adorning the John Harvard statue with the Master Chief's Spartan Helmet.
Athletics.
MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs.  MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, the Pilgrim League for men's lacrosse, NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). In April 2009, budget cuts lead to MIT eliminating eight of its 41 sports, including the mixed men’s and women’s teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men’s programs in golf and wrestling.
The Institute's sports teams are called the Engineers, their mascot since 1914 being a beaver, "nature's engineer". Lester Gardner, a member of the Class of 1898, provided the following justification: "The beaver not only typifies the Tech, but his habits are particularly our own. The beaver is noted for his engineering and mechanical skills and habits of industry. His habits are nocturnal. He does his best work in the dark."
MIT fielded several dominant intercollegiate Tiddlywinks teams through 1980, winning national and world championships. MIT has produced 188 Academic All-Americans, the third largest membership in the country for any division and the highest number of members for Division III.
The Zesiger sports and fitness center (Z-Center), which opened in 2002, significantly expanded the capacity and quality of MIT's athletics, physical education, and recreation offerings to 10 buildings and 26 acre of playing fields. The 124000 sqft facility features an Olympic-class swimming pool, international-scale squash courts, and a two-story fitness center.
People.
Students.
MIT enrolled 4,384 undergraduates and 6,510 graduate students in 2011–2012. Women constituted 45 percent of undergraduate students. Undergraduate and graduate students were drawn from all 50 states as well as 115 foreign countries.
MIT received 17,909 applications for admission to the undergraduate Class of 2015; 1,742 were admitted (9.7 percent) and 1128 enrolled (64.8 percent). 19,446 applications were received for graduate and advanced degree program across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).
The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class. 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.
Undergraduate tuition and fees total $40,732 and annual expenses are estimated at $52,507 as of 2012. 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student. Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million). The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".
MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry. Female students remained a minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963. Between 1993 and 2009, the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students. Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering.
A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention to MIT's culture and student life. After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity, MIT began requiring all freshmen to live in the dormitory system. The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate. In late 2001 a task force's recommended improvements in student mental health services were implemented, including expanding staff and operating hours at the mental health center. These and later cases were significant as well because they sought to prove the negligence and liability of university administrators "in loco parentis".
Faculty and staff.
s of 2013[ [update]], MIT had 1,030 faculty members, of whom 225 were women. Faculty are responsible for lecturing classes, advising both graduate and undergraduate students, and sitting on academic committees, as well as conducting original research. Between 1964 and 2009, a total of seventeen faculty and staff members affiliated with MIT were awarded Nobel Prizes (thirteen in the last 25 years). MIT faculty members past or present have won a total of twenty-seven Nobel Prizes, the majority in Economics or Physics. s of October 2013[ [update]], among current faculty and teaching staff there are 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows. Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.
A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science, although the study's methods were controversial. Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice presidents, although allegations of sexism continue to be made. Susan Hockfield, a molecular neurobiologist, was MIT's president from 2004 to 2012 and was the first woman to hold the post.
Tenure outcomes have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble, a historian of technology, became a "cause célèbre" about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military. Former materials science professor Gretchen Kalonji sued MIT in 1994 alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments, and establishment of a project to encourage women and minorities to seek faculty positions. In 1997, the Massachusetts Commission Against Discrimination issued a probable cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure. In 2006–2007, MIT's denial of tenure to African-American stem cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger strike, and the resignation of Professor Frank L. Douglas in protest. April Simpson of The Boston Globe reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues issued a statement yesterday saying that the professor was treated fairly in tenure review."
MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty member Charles W. Eliot was recruited in 1869 to become president of Harvard University, a post he would hold for 40 years, during which he wielded considerable influence on both American higher education and secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.
s of 2014[ [update]], former provost Robert A. Brown is president of Boston University; former provost Mark Wrighton is chancellor of Washington University in St. Louis; former associate provost Alice Gast is president of Lehigh University; and former professor Suh Nam-pyo is president of KAIST. Former dean of the School of Science Robert J. Birgeneau was the chancellor of the University of California, Berkeley (2004–2013); former professor John Maeda was president of Rhode Island School of Design (RISD, 2008–2013); former professor David Baltimore was president of Caltech (1997–2006); and MIT alumnus and former assistant professor Hans Mark served as chancellor of the University of Texas system (1984–1992).
In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt was the director of the United States Geological Survey, urban studies professor Xavier de Souza Briggs is currently the associate director of the White House Office of Management and Budget, and biology professor Eric Lander is a co-chair of the President's Council of Advisors on Science and Technology. In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy. Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.
Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities as of 2013[ [update]]. Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic", but complaining about "low pay".
Alumni.
Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. s of 2014[ [update]], 27 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.
Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairwoman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of the European Central Bank Mario Draghi, Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, and former Iraqi Deputy Prime Minister Ahmed Chalabi.
MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, "The Guardian", "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year. If MIT were a country, it would have the 11th highest GDP of any nation in the world."
Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Nearby Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.
More than one third of the United States' manned spaceflights have included MIT-educated astronauts (among them Apollo 11 Lunar Module Pilot Buzz Aldrin), more than any university excluding the United States service academies. Alumnus and former faculty member Qian Xuesen was instrumental in the PRC rocket program.
Noted alumni in non-scientific fields include author Hugh Lofting, sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British "BBC" and "ITN" correspondent and political advisor David Walter, "The New York Times" columnist and Nobel Prize Winning economist Paul Krugman, "The Bell Curve" author Charles Murray, United States Supreme Court building architect Cass Gilbert, Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.
References.
Bibliography.
</dl>

</doc>
<doc id="18880" url="http://en.wikipedia.org/wiki?curid=18880" title="Monopolistic competition">
Monopolistic competition

Monopolistic competition is a type of imperfect competition such that many producers sell products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms. In the presence of coercive government, monopolistic competition will fall into government-granted monopoly. Unlike perfect competition, the firm maintains spare capacity. Models of monopolistic competition are often used to model industries. Textbook examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities. The "founding father" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, "Theory of Monopolistic Competition" (1933). Joan Robinson published a book "The Economics of Imperfect Competition" with a comparable theme of distinguishing perfect from imperfect competition.
Monopolistically competitive markets have the following characteristics:
The long-run characteristics of a monopolistically competitive market are almost the same as a perfectly competitive market. Two differences between the two are that monopolistic competition produces heterogeneous products and that monopolistic competition involves a great deal of non-price competition, which is based on subtle product differentiation. A firm making profits in the short run will nonetheless only break even in the long run because demand will decrease and average total cost will increase. This means in the long run, a monopolistically competitive firm will make zero economic profit. This illustrates the amount of influence the firm has over the market; because of brand loyalty, it can raise its prices without losing all of its customers. This means that an individual firm's demand curve is downward sloping, in contrast to perfect competition, which has a perfectly elastic demand schedule.
Major characteristics.
There are six characteristics of monopolistic competition (MC):
Product differentiation.
MC firms sell products that have real or perceived non-price differences. However, the differences are not so great as to eliminate other goods as substitutes. Technically, the cross price elasticity of demand between goods in such a market is positive. In fact, the XED would be high. MC goods are best described as close but imperfect substitutes. The goods perform the same basic functions but have differences in qualities such as type, style, quality, reputation, appearance, and location that tend to distinguish them from each other. For example, the basic function of motor vehicles is the same—to move people and objects from point to point in reasonable comfort and safety. Yet there are many different types of motor vehicles such as motor scooters, motor cycles, trucks and cars, and many variations even within these categories.
Many firms.
There are many firms in each MC product group and many firms on the side lines prepared to enter the market. A product group is a "collection of similar products". The fact that there are "many firms" gives each MC firm the freedom to set prices without engaging in strategic decision making regarding the prices of other firms and each firm's actions have a negligible impact on the market. For example, a firm could cut prices and increase sales without fear that its actions will prompt retaliatory responses from competitors.
How many firms will an MC market structure support at market equilibrium? The answer depends on factors such as fixed costs, economies of scale and the degree of product differentiation. For example, the higher the fixed costs, the fewer firms the market will support. Also the greater the degree of product differentiation—the more the firm can separate itself from the pack—the fewer firms there will be at market equilibrium.
No entry and exit costs.
In the long run there are no entry and exit costs. There are numerous firms waiting to enter the market, each with their own "unique" product or in pursuit of positive profits. Any firm unable to cover its costs can leave the market without incurring liquidation costs. This assumption implies that there are low start up costs, no sunk costs and no exit costs.
Independent decision making.
Each MC firm independently sets the terms of exchange for its product. The firm gives no consideration to what effect its decision may have on competitors. The theory is that any action will have such a negligible effect on the overall market demand that an MC firm can act without fear of prompting heightened competition. In other words each firm feels free to set prices as if it were a monopoly rather than an oligopoly.
Market power.
MC firms have some degree of market power. Market power means that the firm has control over the terms and conditions of exchange. An MC firm can raise its prices without losing all its customers. The firm can also lower prices without triggering a potentially ruinous price war with competitors. The source of an MC firm's market power is not barriers to entry since they are low. Rather, an MC firm has market power because it has relatively few competitors, those competitors do not engage in strategic decision making and the firms sells differentiated product. Market power also means that an MC firm faces a downward sloping demand curve. The demand curve is highly elastic although not "flat".
Imperfect information.
No sellers or buyers have complete market information, like market demand or market supply.
Inefficiency.
There are two sources of inefficiency in the MC market structure. First, at its optimum output the firm charges a price that exceeds marginal costs, The MC firm maximizes profits where marginal revenue = marginal cost. Since the MC firm's demand curve is downward sloping this means that the firm will be charging a price that exceeds marginal costs. The monopoly power possessed by a MC firm means that at its profit maximizing level of production there will be a net loss of consumer (and producer) surplus. The second source of inefficiency is the fact that MC firms operate with excess capacity. That is, the MC firm's profit maximizing output is less than the output associated with minimum average cost. Both a PC and MC firm will operate at a point where demand or price equals average cost. For a PC firm this equilibrium condition occurs where the perfectly elastic demand curve equals minimum average cost. A MC firm’s demand curve is not flat but is downward sloping. Thus in the long run the demand curve will be tangential to the long run average cost curve at a point to the left of its minimum. The result is excess capacity.
Problems.
Monopolistically competitive firms are inefficient, it is usually the case that the costs of regulating prices for products sold in monopolistic competition exceed the benefits of such regulation. . A monopolistically competitive firm might be said to be marginally inefficient because the firm produces at an output where average total cost is not a minimum. A monopolistically competitive market is productively inefficient market structure because marginal cost is less than price in the long run. Monopolistically competitive markets are also allocatively inefficient, as the price given is higher than Marginal cost. Product differentiation increases total utility by better meeting people's wants than homogenous products in a perfectly competitive market. 
Another concern is that monopolistic competition fosters advertising and the creation of brand names. Advertising induces customers into spending more on products because of the name associated with them rather than because of rational factors. Defenders of advertising dispute this, arguing that brand names can represent a guarantee of quality and that advertising helps reduce the cost to consumers of weighing the tradeoffs of numerous competing brands. There are unique information and information processing costs associated with selecting a brand in a monopolistically competitive environment. In a monopoly market, the consumer is faced with a single brand, making information gathering relatively inexpensive. In a perfectly competitive industry, the consumer is faced with many brands, but because the brands are virtually identical information gathering is also relatively inexpensive. In a monopolistically competitive market, the consumer must collect and process information on a large number of different brands to be able to select the best of them. In many cases, the cost of gathering information necessary to selecting the best brand can exceed the benefit of consuming the best brand instead of a randomly selected brand. The result is that the consumer is confused. Some brands gain prestige value and can extract an additional price for that.
Evidence suggests that consumers use information obtained from advertising not only to assess the single brand advertised, but also to infer the possible existence of brands that the consumer has, heretofore, not observed, as well as to infer consumer satisfaction with brands similar to the advertised brand.
Examples.
In many markets, such as toothpastes and toilet paper, producers practice product differentiation by altering the physical composition of products, using special packaging, or simply claiming to have superior products based on brand images or advertising.

</doc>
<doc id="18881" url="http://en.wikipedia.org/wiki?curid=18881" title="Mathematical induction">
Mathematical induction

Mathematical induction is a method of mathematical proof typically used to establish a given statement for all natural numbers. It is a form of direct proof, and it is done in two steps. The first step, known as the base case, is to prove the given statement for the first natural number. The second step, known as the inductive step, is to prove that the given statement for any one natural number implies the given statement for the next natural number. From these two steps, mathematical induction is the rule from which we infer that the given statement is established for all natural numbers.
The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction, in some form, is the foundation of all correctness proofs for computer programs.
Although its name may suggest otherwise, mathematical induction should not be misconstrued as a form of inductive reasoning (also see Problem of induction). Mathematical induction is an inference rule used in proofs. In mathematics, proofs including those using mathematical induction are examples of deductive reasoning, and inductive reasoning is excluded from proofs.
History.
In 370 BC, Plato's Parmenides may have contained an early example of an implicit inductive proof. The earliest implicit traces of mathematical induction can be found in Euclid's proof that the number of primes is infinite and in Bhaskara's "cyclic method". An opposite iterated technique, counting "down" rather than up, is found in the Sorites paradox, where one argued that if 1,000,000 grains of sand formed a heap, and removing one grain from a heap left it a heap, then a single grain of sand (or even no grains) forms a heap.
An implicit proof by mathematical induction for arithmetic sequences was introduced in the "al-Fakhri" written by al-Karaji around 1000 AD, who used it to prove the binomial theorem and properties of Pascal's triangle.
None of these ancient mathematicians, however, explicitly stated the inductive hypothesis. Another similar case (contrary to what Vacca has written, as Freudenthal carefully showed) was that of Francesco Maurolico in his "Arithmeticorum libri duo" (1575), who used the technique to prove that the sum of the first "n" odd integers is "n"2. The first explicit formulation of the principle of induction was given by Pascal in his "Traité du triangle arithmétique" (1665). Another Frenchman, Fermat, made ample use of a related principle, indirect proof by infinite descent. The inductive hypothesis was also employed by the Swiss Jakob Bernoulli, and from then on it became more or less well known. The modern rigorous and systematic treatment of the principle came only in the 19th century, with George Boole, Augustus de Morgan, Charles Sanders Peirce, Giuseppe Peano, and Richard Dedekind.
Description.
The simplest and most common form of mathematical induction infers that a statement involving a natural number "n" holds for all values of "n". The proof consists of two steps:
The hypothesis in the inductive step that the statement holds for some "n" is called the induction hypothesis (or inductive hypothesis). To perform the inductive step, one assumes the induction hypothesis and then uses this assumption to prove the statement for "n" + 1.
Whether "n" = 0 or "n" = 1 depends on the definition of the natural numbers. If 0 is considered a natural number, as is common in the fields of combinatorics and mathematical logic, the base case is given by "n" = 0. If, on the other hand, 1 is taken as the first natural number, then the base case is given by "n" = 1.
Example.
Mathematical induction can be used to prove that the following statement, which we will call "P"("n"), holds for all natural numbers "n".
"P"("n") gives a formula for the sum of the natural numbers less than or equal to number "n". The proof that "P"("n") is true for each natural number "n" proceeds as follows.
Basis: Show that the statement holds for "n" = 0. <br>
"P"(0) amounts to the statement:
In the left-hand side of the equation, the only term is 0, and so the left-hand side is simply equal to 0. <br>
In the right-hand side of the equation, 0·(0 + 1)/2 = 0. <br>
The two sides are equal, so the statement is true for "n" = 0. Thus it has been shown that "P"(0) holds.
Inductive step: Show that "if" "P"("k") holds, then also "P"("k" + 1) holds. This can be done as follows.
Assume "P"("k") holds (for some unspecified value of "k"). It must then be shown that "P"("k" + 1) holds, that is:
Using the induction hypothesis that "P"("k") holds, the left-hand side can be rewritten to:
Algebraically:
thereby showing that indeed "P"("k" + 1) holds.
Since both the basis and the inductive step have been performed, by mathematical induction, the statement "P"("n") holds for all natural "n". Q.E.D.
Axiom of induction.
Mathematical induction as an inference rule can be formalized as a second-order axiom. The "axiom of induction" is, in logical symbols,
where "P" is any predicate and "k" and "n" are both natural numbers.
In words, the basis "P"(0) and the inductive step (namely, that the inductive hypothesis "P"("k") implies "P"("k" + 1)) together imply that "P"("n") for any natural number "n". The axiom of induction asserts that the validity of inferring that "P"("n") holds for any natural number "n" from the basis and the inductive step.
Note that the first quantifier in the axiom ranges over "predicates" rather than over individual numbers. This is a second-order quantifier, which means that this axiom is stated in second-order logic. Axiomatizing arithmetic induction in first-order logic requires an axiom schema containing a separate axiom for each possible predicate. The article Peano axioms contains further discussion of this issue.
Characterizing the structure of formula_7 by the induction axiom.
Having proven the base case and the inductive step, then the structure of formula_7 is such that any value can be obtained by performing the inductive step repeatedly. It may be helpful to think of the domino effect. Consider a half line of dominoes each standing on end, and extending infinitely to the right (see picture). Suppose that:
With these assumptions one can conclude (using mathematical induction) that all of the dominoes will fall right.
If the dominoes are arranged in another way, this conclusion needn't hold (see Peano axioms#Formulation for a counter example). Similarly, the induction axiom describes an essential property of formula_7, viz. that each of its members can be reached from 0 by sufficiently often adding 1. While there is only one structure that satisfies all Peano axioms (including induction), there is no set of only first-order axioms that fulfils the same task.
Variants.
In practice, proofs by induction are often structured differently, depending on the exact nature of the property to be proved.
Induction basis other than 0 or 1.
If we want to prove a statement not for all natural numbers but only for all numbers greater than or equal to a certain number "b" then the proof by induction consists of:
This can be used, for example, to show that "n"2 ≥ 3"n" for "n" ≥ 3. A more substantial example is a proof that
In this way we can prove that "P"("n") holds for all "n" ≥1, or even "n" ≥−5. This form of mathematical induction is actually a special case of the previous form because if the statement that we intend to prove is "P"("n") then proving it with these two rules is equivalent with proving "P"("n" + "b") for all natural numbers "n" with the first two steps.
Induction basis equal to 2.
In mathematics, many standard functions, including operations such as "+" and relations such as "=", are binary, meaning that they take two arguments. Often these functions possess properties that implicitly extend them to more than two arguments. For example, once addition "a" + "b" is defined and is known to satisfy the associativity property ("a" + "b") + "c" = "a" + ("b" + "c"), then the ternary addition "a" + "b" + "c" makes sense, either as ("a" + "b") + "c" or as "a" + ("b" + "c"). Similarly, many axioms and theorems in mathematics are stated only for the binary versions of mathematical operations and relations, and implicitly extend to higher-arity versions.
Suppose that we wish to prove a statement about an "n"-ary operation implicitly defined from a binary operation, using mathematical induction on "n". In this case it is natural to take 2 for the induction basis.
Example: product rule for the derivative.
In this example, the binary operation in question is multiplication (of functions). The usual product rule for the derivative taught in calculus states:
or in logarithmic derivative form
This can be generalized to a product of "n" functions. One has
or in logarithmic derivative form
In each of the "n" terms of the usual form, just one of the factors is a derivative; the others are not.
When this general fact is proved by mathematical induction, the "n" = 0 case is trivial,formula_17 (since the empty product is 1, and the empty sum is 0). The "n" = 1 case is also trivial, formula_18 And for each "n" ≥ 3, the case is easy to prove from the preceding "n" − 1 case. The real difficulty lies in the "n" = 2 case, which is why that is the one stated in the standard product rule.
Induction on more than one counter.
It is sometimes desirable to prove a statement involving two natural numbers, "n" and "m", by iterating the induction process. That is, one performs a basis step and an inductive step for "n", and in each of those performs a basis step and an inductive step for "m". See, for example, the proof of commutativity accompanying "addition of natural numbers". More complicated arguments involving three or more counters are also possible.
Infinite descent.
The method of infinite descent was one of Pierre de Fermat's favorites. This method of proof can assume several slightly different forms. For example, it might begin by showing that if a statement is true for a natural number "n" it must also be true for some smaller natural number "m" ("m" < "n"). Using mathematical induction (implicitly) with the inductive hypothesis being that the statement is false for all natural numbers less than or equal to "m", we can conclude that the statement cannot be true for any natural number "n".
Although this particular form of infinite-descent proof is clearly a mathematical induction, whether one holds all proofs "by infinite descent" to be mathematical inductions depends on how one defines the term "proof by infinite descent." One might, for example, use the term to apply to proofs in which the well-ordering of the natural numbers is assumed, but not the principle of induction. Such, for example, is the usual proof that 2 has no rational square root (see Infinite descent).
Prefix induction.
The most common form of induction requires proving that
or equivalently
whereupon the induction principle "automates" n applications of this inference in getting from "P"(0) to "P"("n"). This could be called "predecessor induction" because each step proves something about a number from something about that number's predecessor.
A variant of interest in computational complexity is "prefix induction", in which one needs to prove
or equivalently
The induction principle then "automates" log "n" applications of this inference in getting from "P"(0) to "P"("n"). (It is called "prefix induction" because each step proves something about a number from something about the "prefix" of that number formed by truncating the low bit of its binary representation.)
If traditional predecessor induction is interpreted computationally as an "n"-step loop, prefix induction corresponds to a log "n"-step loop, and thus proofs using prefix induction are "more feasibly constructive" than proofs using predecessor induction.
Predecessor induction can trivially simulate prefix induction on the same statement. Prefix induction can simulate predecessor induction, but only at the cost of making the statement more syntactically complex (adding a bounded universal quantifier), so the interesting results relating prefix induction to polynomial-time computation depend on excluding unbounded quantifiers entirely, and limiting the alternation of bounded universal and existential quantifiers allowed in the statement. See 
One could take it a step farther to "prefix of prefix induction": one must prove
whereupon the induction principle "automates" log log "n" applications of this inference in getting from "P"(0) to "P"("n"). This form of induction has been used, analogously, to study log-time parallel computation.
Complete induction.
Another variant, called complete induction (or strong induction or course of values induction), says that in the second step we may assume not only that the statement holds for "n" = "m" but also that it is true for all "n" less than or equal to "m".
Complete induction is most useful when several instances of the inductive hypothesis are required for each inductive step. For example, complete induction can be used to show that
where "Fn" is the "n"th Fibonacci number, φ = (1 + √5)/2 (the golden ratio) and ψ = (1 − √5)/2 are the roots of the polynomial "x"2 − "x" − 1. By using the fact that "F""n" + 2 = "F""n" + 1 + "F""n" for each "n" ∈ N, the identity above can be verified by direct calculation for "F""n" + 2 if we assume that it already holds for both "F""n" + 1 and "F""n". To complete the proof, the identity must be verified in the two base cases "n" = 0 and "n" = 1.
Another proof by complete induction uses the hypothesis that the statement holds for "all" smaller "n" more thoroughly. Consider the statement that "every natural number greater than 1 is a product of (one or more) prime numbers", and assume that for a given "m" > 1 it holds for all smaller "n" > 1. If "m" is prime then it is certainly a product of primes, and if not, then by definition it is a product: "m" = "n"1 "n"2, where neither of the factors is equal to 1; hence neither is equal to "m", and so both are smaller than "m". The induction hypothesis now applies to "n"1 and "n"2, so each one is a product of primes. Then "m" is a product of products of primes; i.e. a product of primes.
This generalization, complete induction, is equivalent to the ordinary mathematical induction described above. Suppose P("n") is the statement that we intend to prove by complete induction. Let Q("n") mean P("m") holds for all "m" such that 0 ≤ "m" ≤ "n". Then Q("n") is true for all "n" if and only if P("n") is true for all "n", and a proof of P("n") by complete induction is just the same thing as a proof of Q("n") by (ordinary) induction.
Transfinite induction.
The last two steps can be reformulated as one step:
This form of mathematical induction is not only valid for statements about natural numbers, but for statements about elements of any well-founded set, that is, a set with an irreflexive relation < that contains no infinite descending chains.
This form of induction, when applied to ordinals (which form a well-ordered and hence well-founded class), is called "transfinite induction". It is an important proof technique in set theory, topology and other fields.
Proofs by transfinite induction typically distinguish three cases:
Strictly speaking, it is not necessary in transfinite induction to prove the basis, because it is a vacuous special case of the proposition that if "P" is true of all "n" < "m", then "P" is true of "m". It is vacuously true precisely because there are no values of "n" < "m" that could serve as counterexamples.
Equivalence with the well-ordering principle.
The principle of mathematical induction is usually stated as an axiom of the natural numbers; see Peano axioms. However, it can be proved from the well-ordering principle. Indeed, suppose the following:
To derive simple induction from these axioms, we must show that if P("n") is some proposition predicated of "n", and if:
then P("n") holds for all "n".
"Proof." Let S be the set of all natural numbers for which P("n") is false. Let us see what happens if we assert that S is nonempty. Well-ordering tells us that S has a least element, say "t". Moreover, since P(0) is true, "t" is not 0. Since every natural number is either zero or some "n"+1, there is some natural number "n" such that "n"+1="t". Now "n" is less than "t", and "t" is the least element of S. It follows that "n" is not in S, and so P("n") is true. This means that P("n"+1) is true, and so P("t") is true. This is a contradiction, since "t" was in S. Therefore, S is empty.
It can also be proved that induction, given the other axioms, implies the well-ordering principle.
Example of error in the inductive step.
This example demonstrated a subtle error in the proof of the inductive step.
Joel E. Cohen proposed the following argument, which purports to prove by mathematical induction that all horses are of the same color:
The basis case "n" = 1 is trivial (as any horse is the same color as itself), and the inductive step is correct in all cases "n" > 1. However, the logic of the inductive step is incorrect for "n" = 1, because the statement that "the two sets overlap" is false (there are only "n" + 1 = 2 horses prior to either removal, and after removal the sets of one horse each do not overlap).
References.
</dl>

</doc>
<doc id="18884" url="http://en.wikipedia.org/wiki?curid=18884" title="Matrix">
Matrix

Matrix may refer to:

</doc>
<doc id="18885" url="http://en.wikipedia.org/wiki?curid=18885" title="Morton Downey, Jr.">
Morton Downey, Jr.

Sean Morton Downey (December 9, 1932 – March 12, 2001) was an American singer, songwriter and later a television talk show host of the 1980s who pioneered the "trash TV" format on his program "The Morton Downey, Jr. Show".
The film company Ironbound Films produced a biopic about Downey titled "", which premiered April 19, 2012, at the 2012 Tribeca Film Festival.
Early life.
Morton Downey, Jr.'s parents were also in show business; his father, Morton Downey, was a popular singer, and his mother, Barbara Bennett, was a singer and dancer. Downey did not use his legal first name (Sean) in his stage name. His aunts included Hollywood film stars Constance and Joan Bennett, from whom he was estranged, and his maternal grandfather was the celebrated matinée idol Richard Bennett. Born into a life of luxury, he was raised next door to the Kennedy compound at Hyannis Port, Massachusetts. Downey attended New York University.
Career.
He was a program director and announcer at a radio station in Connecticut in the 1950s. He went on to work as a disc jockey, sometimes using the moniker "Doc" Downey, in various markets around the U.S., including Phoenix (KRIZ), Miami (WFUN), Kansas City (KUDL), San Diego (KDEO) and Seattle (KJR). Like his father, Downey pursued a career in music, recording in both pop and country styles. He sang on a few records and then began to write songs, several of which were popular in the 1950s and 1960s. He joined ASCAP as a result. In 1958, he recorded "Boulevard Of Broken Dreams", which he sang on national television on a set that resembled a dark street with one street light. In 1981, "Green Eyed Girl" charted on the "Billboard Magazine" country chart, peaking at #95.
In the 1980s, Downey was a talk show host at KFBK-AM in Sacramento, California, where he employed his abrasive style. He was fired in 1984. He was replaced on KFBK by Rush Limbaugh, who has held the time slot ever since, later via his national syndication. Downey also had a stint on WMAQ-AM in Chicago where he unsuccessfully tried to get other on air radio personalities to submit to drug testing. Downey's largest effect on American culture came from his popular, yet short-lived, syndicated late 1980s television talk show, "The Morton Downey Jr. Show".
Pro-life activism.
On January 22, 1980, Downey, a dedicated pro-life activist, hosted the California State Rally for Life at the invitation of the California ProLife Council and United Students for Life. At that time, he was also running for President of the United States, as a Democrat. The United Students for Life, at California State University, Sacramento helped organize his California presidential rallies. Downey worked to help promote anti-abortion candidates in California and around the country.
Television.
Downey headed to Secaucus, New Jersey, where his highly controversial television program "The Morton Downey Jr. Show" was taped. Starting as a local program on New York-New Jersey superstation WWOR-TV in the fall of 1987, it expanded into national syndication in early 1988. The program featured screaming matches among Downey, his guests, and audience members. Using a large silver bowl for an ashtray, he would chainsmoke during the show and blow smoke in his guests' faces. Downey's fans became known as "Loudmouths," patterned after the studio lecterns decorated with gaping cartoon mouths, from which Downey's guests would go head-to-head against each other on their respective issues.
Downey's signature phrases "pablum puking liberal" (in reference to left-liberals) and "zip it!" briefly enjoyed some popularity in the contemporary vernacular. He particularly enjoyed making his guests angry with each other, which on a few occasions resulted in physical confrontations. One that occurred on a 1988 show taped at the Apollo Theater, involving Al Sharpton and CORE National Chairman Roy Innis. The exchange between the two men culminated in Innis shoving Sharpton into his chair, knocking him to the floor and Downey intervening to separate the pair.
Because of the controversial format and content of the show, distributor MCA Television had problems selling the show to a number of stations and advertisers. Even Downey's affiliates, many of which were low-rated independent television stations in small to medium markets, were so fearful of advertiser and viewer backlash that they would air one or even two local disclaimers during the broadcast. 
During one controversial episode Downey introduced his gay brother, Tony Downey, to his studio audience and informed them Tony was HIV positive. During the episode Downey stated he was afraid his audience would abandon him if they knew he had a gay brother, but then said he did not care.
"The Washington Post" wrote about him, "Suppose a maniac got hold of a talk show. Or need we suppose?" David Letterman said, "I'm always amazed at what people will fall for. We see this every ten or twelve years, an attempt at this, and I guess from that standpoint I don't quite understand why everybody's falling over backwards over the guy."
Celebrity, cancellation, and bankruptcy.
The success of the show made Downey a pop culture celebrity, leading to an appearance on "Saturday Night Live" in 1988, WrestleMania V in 1989 in which he traded insults with Roddy Piper and Brother Love on "Piper's Pit", and later roles in movies such as "Predator 2" and "". He was also cast in several television roles, often playing tabloid TV hosts or other obnoxious media types.
In 1988, Downey recorded an album of songs based on his show, entitled "Morton Downey Jr. Sings" (released in 1989). The album's only single, "Zip It!" (a catch-phrase from the TV show, used to quiet an irate guest), became a surprise hit on some college radio stations. Over the course of the 1988–89 television season, his TV show suffered a decline in viewership, resulting from many markets downgrading its time slot; even flagship station WWOR moved Downey's program from its original 9:00 PM slot to 11:30 PM in the fall of 1988. Beginning in January 1989, the time slot immediately following Downey's program was given to the then-new "Arsenio Hall Show". However, following Hall's strong early ratings, the two series swapped time slots several weeks later, thus relegating Downey to 12:30 AM in the number-one television market. 
In late April 1989, he was involved in an incident in a San Francisco International Airport restroom in which he claimed to have been attacked by neo-Nazis who painted a swastika on his face and attempted to shave his head. Some inconsistencies in Downey's account (e.g., the swastika was painted in reverse, suggesting that Downey had drawn it himself in a mirror), and the failure of the police to find supportive evidence, led many to suspect the incident was a hoax and a plea for attention. In July 1989, his show was cancelled, with the owners of the show announcing that the last show had been taped on June 30, and that no new shows would air after September 15, 1989.
At the time of its cancellation, the show was airing on a total of 30 stations across the country (including WPHL in Philadelphia), and its advertisers had been reduced primarily to "direct-response" ads (such as 900 chat line and phone sex numbers). In February 1990, Downey filed for bankruptcy in the US Bankruptcy Court for the District of New Jersey.
Later career.
In 1990, Downey resurfaced on CNBC with an interview program called "Showdown", which was followed by three attempted talk radio comebacks: first in 1992 on Washington, D.C. radio station WWRC; then in 1993 on Dallas radio station KGBS, where he would scream insults at his callers. He was also hired as the station's VP of Operations. The following year he returned to CNBC with a short-lived television show, "Downey", in one episode, Downey claimed to have had a psychic communication with O.J. Simpson's murdered ex-wife, Nicole Brown Simpson.
His third – and final – attempt at a talk radio comeback occurred in 1997 on Cleveland radio station WTAM in a late evening time slot. It marked his return to the Cleveland market, where Downey had been a host for crosstown radio station WERE in the early 1980s prior to joining KFBK. This stint came shortly after the surgery for lung cancer that removed one of his lungs. At WTAM, Downey abandoned the confrontational schtick of his TV and previous radio shows, and conducted this program in a much more conversational and jovial manner.
On August 30, 1997, Downey quit his WTAM radio talk show to focus on pursuing legal action against Howard Stern. Downey had accused Stern of spreading rumors that he resumed his smoking habits, to which publicist Les Schecter retorted, "He hasn't picked up a cigarette." His replacement was former WERE host Rick Gilmour.
Following his death, news reports and obituaries incorrectly (according to the "Orange County Register") credited him as the composer of "Wipe Out." As of 2008, Downey's official website (and others) continue to make this claim. Prior to Downey's death, "Spin" in April 1989 had identified the "Wipe Out" authorship as a myth.
Controversies.
In 1984, at KFBK radio, Downey used the word "Chinaman" while telling a joke. His use of the word upset portions of the sizable Asian community in Sacramento. One Asian-American city councilman called for an apology and pressured the station for Downey's resignation. Downey refused to apologize and was forced to resign, with Rush Limbaugh taking his place.
Downey was sued for allegedly appropriating the words and music to his theme song from two songwriters. He was sued for $40 million after bringing then-stripper Kellie Everts onto the show and calling her a "slut," a "pig," a "hooker," and a "tramp," claiming that she had venereal diseases, and banging his pelvis against hers.
In April 1988, he was arraigned on criminal charges for allegedly attacking a gay guest on his show, in a never-aired segment. In another lawsuit, he was accused of slandering a newscaster (a former colleague), and of indecently exposing himself to her and slapping her. Downey punched Stuttering John during an interview done for "The Howard Stern Show". In various interviews, he expressed remorse for some of the extreme theatrics of his TV show, saying he had taken things too far. He added that he had been a "bastard." However, he also claimed that his show was of a higher quality and not as "sleazy" as Jerry Springer's.
Honors.
In 1998, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.
In the "Super Mario Bros." video game series, the character, Morton Koopa, Jr. was named after him.
Personal life.
Downey was married four times and had four children from three of those marriages. With wife Helen he had Melissa, with Joan he had daughters Tracey and Kelli, and, with fourth wife Lori, he had a daughter Seanna Micaela. He and Lori met when she appeared as a dancer in a show he attended in Atlantic City. According to Terry Pluto's book, "Loose Balls", Downey was one of the owners of the New Orleans Buccaneers basketball team in the American Basketball Association in the late 1960s. Downey was also president and co-founder of the proposed World Baseball Association in 1974.
"Évocateur: The Morton Downey Jr. Movie".
Released in 2012, the biopic "" touches upon Downey's upbringing and formative years in radio and politics before launching into the history of "The Morton Downey Jr. Show" and Downey's influence on trash TV. The film also looks at Downey's relationship with Al Sharpton and other important 80s figures, as well as Downey's role as a predecessor for commentators like Glenn Beck and Rush Limbaugh.
Death.
In 1996, Downey was diagnosed with lung cancer and had one of his lungs removed. He did a complete about-face on the issue of tobacco use, going from a one-time member of the National Smokers Alliance to a staunch anti-smoking activist. He continued to speak against smoking until his death from lung cancer in 2001.
After being diagnosed with lung cancer, he commented, "I had spawned a generation of kids to think it was cool to smoke a cigarette. Kids walked up to me until a matter of weeks ago, they'd have a cigarette in their hand and they'd say, 'Hey, Mort,' or, 'Hey, Mouth, autograph my cigarette.' And I'd do it." He also blamed tobacco companies for lying to consumers about cigarettes.

</doc>
<doc id="18886" url="http://en.wikipedia.org/wiki?curid=18886" title="List of male tennis players">
List of male tennis players

This is a list of top international male tennis players, both past and present.
To keep the list at a reasonable length, it includes only players who have been officially ranked among the top 25 singles players in the world during the "Open Era"; been ranked in the top 10 prior to the Open Era; have been a singles quarterfinalist or better at a Grand Slam tournament; have reached the finals of the Masters Grand Prix/ATP Tour World Championships/Tennis Masters Cup/ATP World Tour Finals; have been singles medalists at the Olympics; have won a Grand Slam or Olympic doubles title; or have been ranked world no. 1 in doubles.
Players who have won more than one Grand Slam singles title or have been ranked world no. 1 in singles, and singles Grand Slam and Olympic championships, have been put in bold font. Players who still play on the tour have been put in "italics".

</doc>
<doc id="18887" url="http://en.wikipedia.org/wiki?curid=18887" title="Metaphilosophy">
Metaphilosophy

Metaphilosophy (sometimes called philosophy of philosophy) is ‘the investigation of the nature of philosophy.’ Its subject matter includes the aims of philosophy, the boundaries of philosophy, and its methods. It is considered by some to be a subject apart from philosophy, while others see it as automatically a part of philosophy, and still others see it as a combination of these subjects. The interest in metaphilosophy led to the establishment of the journal "Metaphilosophy" in January 1970.
Relationship to philosophy.
Some philosophers consider metaphilosophy to be a subject apart from philosophy, above or beyond it, while others object to that idea. Timothy Williamson argues that the philosophy of philosophy is "automatically part of philosophy," as is the philosophy of anything else. Nicholas Bunnin and Jiyuan Yu write that the separation of first- from second-order study has lost popularity as philosophers find it hard to observe the distinction. As evidenced by these contrasting opinions, debate remains as to whether the evaluation of the nature of philosophy is 'second order philosophy' or simply 'plain philosophy'.
Many philosophers have expressed doubts over the value of metaphilosophy. Among them is Gilbert Ryle : "preoccupation with questions about methods tends to distract us from prosecuting the methods themselves. We run as a rule, worse, not better, if we think a lot about our feet. So let us... not speak of it all but just do it."
Terminology.
The designations "metaphilosophy" and "philosophy of philosophy" have a variety of meanings, sometimes taken to be synonyms, and sometimes seen as distinct.
Morris Lazerowitz claims to have created the term ‘metaphilosophy’ around 1940 and used it in print in 1942. Lazerowitz proposed that metaphilosophy is ‘the investigation of the nature of philosophy.’ Earlier uses have been found in translations from the French. The term is derived from Greek word "meta" μετά ("after", "beyond", "with") and "philosophía" φιλοσοφία ("love of wisdom").
The term ‘metaphilosophy’ is used by Paul Moser in the sense of a 'second-order' or more fundamental undertaking than philosophy itself, in the manner suggested by Charles Griswold:
"The distinction between philosophy and metaphilosophy has an analogue in the familiar distinction between mathematics and metamathematics."—Paul K. Moser, Metaphilosophy, "p. 562
This usage was considered nonsense by Ludwig Wittgenstein, who rejected the analogy between metalanguage and a metaphilosophy. As expressed by Martin Heidegger:
"When we ask, "What is philosophy?" then we are speaking "about" philosophy. By asking in this way we are obviously taking a stand above and, therefore, outside of philosophy. But the aim of our question is to enter into philosophy, to tarry in it, to conduct ourselves in its manner, that is, to "philosophize". The path of our discussion must, therefore, not only have a clear direction, but this direction must at the same time give us the guarantee that we are moving within philosophy and not outside of it and around it."—Martin Heidegger, Was Ist Das--die Philosophie? "p. 21"
Some other philosophers treat the prefix "meta" as simply meaning ‘"about..."’, rather than as referring to a metatheoretical 'second-order' form of philosophy, among them Rescher and Double. Others, such as Williamson, prefer the term "‘philosophy of philosophy’" instead of ‘metaphilosophy’ as it avoids the connotation of a 'second-order' discipline that looks down on philosophy, and instead denotes something that is a part of it. Joll suggests that to take metaphilosophy as 'the application of the methods of philosophy to philosophy itself' is too vague, while the view that sees metaphilosophy as a ‘second-order’ or more abstract discipline, outside philosophy, "is narrow and tendentious".
In the analytical tradition, the term "metaphilosophy" is mostly used to tag commenting and research on previous works as opposed to original contributions towards solving philosophical problems.
Writings.
Ludwig Wittgenstein wrote about the nature of philosophical puzzles and philosophical understanding. He suggested philosophical errors arose from confusions about the nature of philosophical inquiry. In the "Philosophical Investigations", Wittgenstein wrote that there is not a metaphilosophy in the sense of a metatheory of philosophy.
C. D. Broad distinguished Critical from Speculative philosophy in his "The Subject-matter of Philosophy, and its Relations to the special Sciences," in "Introduction to Scientific Thought", 1923. Curt Ducasse, in "Philosophy as a Science", examines several views of the nature of philosophy, and concludes that philosophy has a distinct subject matter: appraisals. Ducasse's view has been among the first to be described as 'metaphilosophy'.
Henri Lefebvre in "Metaphilosophie" (1965) argued, from a marxian standpoint, in favor of an "ontological break", as a necessary methodological approach for critical social theory (whilst criticizing Louis Althusser's "epistemological break" with subjective marxism, which represented a fundamental theoretical tool for the school of marxist structuralism).
Paul Moser writes that typical metaphilosophical discussion includes determining the conditions under which a claim can be said to be a philosophical one. He regards meta-ethics, the study of ethics, to be a form of metaphilosophy, as well as meta-epistemology, the study of epistemology.
Topics.
Many sub-disciplines of philosophy have their own branch of 'metaphilosophy', examples being Meta-aesthetics, Meta-epistemology, Meta-ethics, Meta-ontology, and so forth. However, some topics within 'metaphilosophy' cut across the various subdivisions of philosophy to consider fundamentals important to all its sub-disciplines. Some of these are mentioned below.
Aims.
Some philosophers (e.g. existentialists, pragmatists) think philosophy is ultimately a practical discipline that should help us lead meaningful lives by showing us who we are, how we relate to the world around us and what we should do. Others (e.g. analytic philosophers) see philosophy as a technical, formal, and entirely theoretical discipline, with goals such as "the disinterested pursuit of knowledge for its own sake". Other proposed goals of philosophy include "discover[ing] the absolutely fundamental reason of everything it investigates", "making explicit the nature and significance of ordinary and scientific beliefs", and unifying and transcending the insights given by science and religion. Others proposed that philosophy is a complex discipline because it has 4 or 6 different dimensions.
Boundaries.
Defining philosophy and its boundaries is itself problematic; Nigel Warburton has called it "notoriously difficult". There is no straightforward definition, and most interesting definitions are controversial. As Bertrand Russell wrote:
"We may note one peculiar feature of philosophy. If someone asks the question what is mathematics, we can give him a dictionary definition, let us say the science of number, for the sake of argument. As far as it goes this is an uncontroversial statement... Definitions may be given in this way of any field where a body of definite knowledge exists. But philosophy cannot be so defined. Any definition is controversial and already embodies a philosophic attitude. The only way to find out what philosophy is, is to do philosophy."—Bertrand Russell, "The Wisdom of the West", p.7
While there is some agreement that philosophy involves general or fundamental topics, there is no clear agreement about a series of demarcation issues, including:
Methods.
Philosophical method (or philosophical methodology) is the study of how to do philosophy. A common view among philosophers is that philosophy is distinguished by the ways that philosophers follow in addressing philosophical questions. There is not just one method that philosophers use to answer philosophical questions.
Recently, some philosophers have cast doubt about intuition as a basic tool in philosophical inquiry, from Socrates up to contemporary philosophy of language. In "Rethinking Intuition" various thinkers discard intuition as a valid source of knowledge and thereby call into question 'a priori' philosophy. Experimental philosophy is a form of philosophical inquiry that makes at least partial use of empirical research—especially "opinion polling"—in order to address persistent philosophical questions. This is in contrast with the methods found in analytic philosophy, whereby some say a philosopher will sometimes begin by appealing to his or her intuitions on an issue and then form an argument with those intuitions as premises. However, disagreement about what experimental philosophy can accomplish is widespread and several philosophers have offered criticisms. One claim is that the empirical data gathered by experimental philosophers can have an indirect effect on philosophical questions by allowing for a better understanding of the underlying psychological processes which lead to philosophical intuitions.
Progress.
A prominent question in metaphilosophy is that of whether or not philosophical progress occurs and more so, whether such progress in philosophy is even possible. It has even been disputed, most notably by Ludwig Wittgenstein, whether genuine philosophical problems actually exist. The opposite has also been claimed, for example by Karl Popper, who held that such problems do exist, that they are solvable, and that he had actually found definite solutions to some of them.

</doc>
<doc id="18888" url="http://en.wikipedia.org/wiki?curid=18888" title="Mandolin">
Mandolin

A mandolin (Italian: "mandolino"; literally "small mandola") is a musical instrument in the lute family and is usually plucked with a plectrum or "pick". It commonly has four courses of doubled strings tuned in unison (8 strings), although five (10 strings) and six (12 strings) course versions also exist. The courses are normally tuned in a succession of perfect fifths. It is the soprano member of a family that includes the mandola, octave mandolin, mandocello, and mandobass.
There are many styles of mandolin, but three are common, the "Neapolitan" or "round-backed" mandolin, the "carved-top" mandolin and the "flat-backed" mandolin. The round-back has a deep bottom, constructed of strips of wood, glued together into a bowl. The carved-top or "arch-top" mandolin has a much shallower, arched back, and an arched top—both carved out of wood. The flat-backed mandolin uses thin sheets of wood for the body, braced on the inside for strength in a similar manner to a guitar. Each style of instrument has its own sound quality and is associated with particular forms of music. Neapolitan mandolins feature prominently in European classical music and traditional music. Carved-top instruments are common in American folk music and bluegrass music. Flat-backed instruments are commonly used in Irish, British and Brazilian folk music. Some modern Brazilian instruments feature an extra fifth course tuned a fifth lower than the standard fourth course.
Other mandolin varieties differ primarily in the strings, and include Milanese, Lombard, Brescian, and other six-course types (tuned in fourths), as well as four-string models (one string per course), and the Sicilian twelve-string (three strings per course) model. There have also been made instruments with sixteen-strings (four strings per course).
Much of mandolin development revolved around the soundboard (the top). Pre-mandolin instruments were quiet instruments, strung with as many as six courses of gut strings, and were plucked with the fingers or with a quill. However, modern instruments are louder—using four courses of metal strings, which exert more pressure than the gut strings. The modern soundboard is designed to withstand the pressure of metal strings that would break earlier instruments. The soundboard comes in many shapes—but generally round or teardrop-shaped, sometimes with scrolls or other projections. There is usually one or more "sound holes" in the soundboard, either round, oval, or shaped like a calligraphic "F" (f-hole). A round or oval sound hole may be covered or bordered with decorative rosettes or purfling.
Construction.
A mandolin typically has a hollow wooden body with a neck. Strings run between mechanical tuning machines at the top of the neck to a tailpiece that anchors the other end of the strings. The strings are suspended over a soundboard and pass over a floating bridge. The bridge is kept in contact with the soundboard by the downward pressure from the strings. The neck is either flat or has a slight radius, and is covered with a fingerboard with frets.
Like any plucked instrument, mandolin notes decay to silence rather than sound out continuously as with a bowed note on a violin. Its small size and higher pitch make mandolin notes decay faster than larger stringed instruments like guitar. This encourages the use of tremolo (rapid picking of one or more pairs of strings) to create sustained notes or chords. The mandolin's paired strings facilitate this technique: the plectrum (pick) strikes each of a pair of strings alternately, providing a more full and continuous sound than a single string would.
Various design variations and amplification techniques have been used to make mandolins comparable in volume with louder instruments and orchestras, including the creation of mandolin-banjo hybrid with the louder banjo, adding metal resonators (most notably by Dobro and the National String Instrument Corporation) to make a resonator mandolin, and amplifying electric mandolins through amplifiers.
Variations.
Bowlback.
Bowlback mandolins (also known as roundbacks), are used worldwide. They are most commonly manufactured in Europe, where the long history of mandolin development has created local styles. However, Japanese luthiers also make them.
Neapolitan and Roman styles.
The Neapolitan style has an almond-shaped body resembling a bowl, constructed from curved strips of wood. It usually has a bent sound table, canted in two planes with the design to take the tension of metal strings. A hardwood fingerboard sits on top of or is flush with the sound table. Very old instruments may use wooden tuning pegs, while newer instruments tend to use geared metal tuners. The bridge is a movable length of hardwood. A pickguard is glued below the sound hole under the strings. European roundbacks commonly use a 13-inch scale instead of the 13.876 common on archtop Mandolins.
Intertwined with the Neapolitan style is the Roman style mandolin, which has influenced it. The Roman mandolin had a fingerboard that was more curved and narrow. The fingerboard was lengthened over the sound hole for the e strings, the high pitched strings. The shape of the back of the neck was different, less rounded with an edge, the bridge was curved making the g strings higher. The Roman mandolin had mechanical tuning gears before the Neapolitan.
Prominent Italian manufacturers include Vinaccia (Naples), Embergher (Rome), and Calace (Naples). Other modern manufacturers include Lorenzo Lippi (Milan), Hendrik van den Broek (Holland), Brian Dean (Canada), Salvatore Masiello and Michele Caiazza (La Bottega del Mandolino), and Ferrara, Gabriele Pandini.
Milanese and Lombardic styles.
Another family of bowlback mandolins came from Milan and Lombardy. These mandolins are closer to the mandolino or mandore than other modern mandolins. They have 6 strings, 3 treble and 3 bass. The Lombardic mandolins were tuned g b e' a' d" g". A developer of the Milanese stye was Antonio Monzino (Milan) and his family who made them for 6 generations.
Samuel Adelstein describe the Lombardi mandolin in 1893 as wider and shorter than the Neapolitan mandolin, with a shallower back and a shorter and wider neck, with six singles strings to the regular mandolin's set of 4. The Lombardi was tuned C, D, A, E, B, G. The strings were fastened to the bridge like a guitar's. There were 20 frets, covering three octaves, with an additional 5 notes. When Adelstein wrote, there were no nylon strings, and the gut and single strings "do not vibrate so clearly and sweetly as the double steel string of the Neapolitan."
Manufacturers outside of Italy.
In the United States, when the bowlback was being made in numbers, Lyon and Healy was a major manufacturer, especially under the "Washburn" brand. Other American manufacturers include Martin, Vega, and Larson Brothers.
German manufacturers include Albert & Mueller, Dietrich, Klaus Knorr, Reinhold Seiffert and Alfred Woll. The German bowlbacks use a style developed by Seiffert, with a larger and rounder body.
Japanese brands include Kunishima and Suzuki. Other Japanese manufacturers include Oona, Kawada, Noguchi, Toichiro Ishikawa, Rokutaro Nakade, Otiai Tadao, Yoshihiko Takusari, Nokuti Makoto, Watanabe, Kanou Kadama and Ochiai.
Archtop.
At the very end of the nineteenth century, a new style, with a carved top and back construction inspired by violin family instruments began to supplant the European-style bowl-back instruments, in the United States. This new style is credited to mandolins designed and built by Orville Gibson, a Kalamazoo, Michigan luthier who founded the "Gibson Mandolin-Guitar Manufacturing Co., Limited" in 1902. Gibson mandolins evolved into two basic styles: the Florentine or F-style, which has a decorative scroll near the neck, two points on the lower body, and usually a scroll carved into the headstock; and the A-style, which is pear shaped, has no points, and usually has a simpler headstock.
These styles generally have either two f-shaped soundholes like a violin (F-5 and A-5), or an oval sound hole (F-4 and A-4 and lower models) directly under the strings. Much variation exists between makers working from these archetypes, and other variants have become increasingly common. Generally, in the United States, Gibson F-hole F-5 mandolins and mandolins influenced by that design are strongly associated with bluegrass, while the A-style is associated other types of music, although it too is most often used for and associated with bluegrass. The F-5's more complicated woodwork also translates into a more expensive instrument.
Internal bracing to support the top in the F-style mandolins is usually achieved with parallel tone bars, similar to the bass bar on a violin. Some makers instead employ "x-bracing," which is two tone bars mortised together to form an X. Some luthiers now using a "modified x-bracing" that incorporates both a tone bar and x-bracing.
Numerous modern mandolin makers build instruments that largely replicate the Gibson F-5 Artist models built in the early 1920s under the supervision of Gibson acoustician Lloyd Loar. Original Loar-signed instruments are sought after and extremely valuable. Other makers from the Loar period and earlier include Lyon and Healy, Vega, and Larson Brothers. Some notable modern American carved mandolin manufacturers include, in addition to Gibson, Weber, Monteleone, and Collings. Mandolins from other countries include The Loar (China), Michael Kelly (Korea), Eastman (China), Kentucky (China), Heiden (Canada), Gilchrist (Australia), and Morgan Monroe (China).
Flatback.
Flatback mandolins use a thin sheet of wood with bracing for the back, as a guitar uses, rather than the bowl of the bowlback or the arched back of the carved mandolins.
Like the bowlback, the flatback has a round sound hole. This has been sometimes modified to an elongated hole, called a "D" hole. The body has a rounded almond shape with flat or sometimes canted soundboard.
The type was developed in Europe in the 1850s. The French and Germans called it a Portuguese mandolin, although they also developed it locally. The Germans used it in Wandervogel.
The bandolim is commonly used wherever the Spanish and Portuguese took it: in South America, in Brazil (Choro) and in the Philippines.
In Ireland, Steven Sobel created a carved soundboard for it, leaving the back flat; this is called a Celtic mandolin. The Irish also use the flat top version.
American forms include the Army-Navy mandolin, the flatiron and the pancake mandolins.
Tone.
The tone of the flatback is described as warm or mellow, suitable for folk music and smaller audiences. The instrument sound does not punch through the other players' sound like a carved top does.
Others.
Mandolinetto.
Other American-made variants include the mandolinetto or Howe-Orme guitar-shaped mandolin (manufactured by the Elias Howe Company between 1897 and roughly 1920), which featured a cylindrical bulge along the top from fingerboard end to tailpiece and the Vega mando-lute (more commonly called a cylinder-back mandolin manufactured by the Vega Company between 1913 and roughly 1927), which had a similar longitudinal bulge but on the back rather than the front of the instrument.
Banjolin or mandolin-banjo.
The mandolin was given a banjo body in an 1882 patent by Benjamin Bradbury of Brooklyn and given the name "banjolin" by John Farris in 1885. Today "banjolin" describes an instrument with four strings, while the version with the four courses of double strings is called a "mandolin-banjo".
Resonator mandolins.
A resonator mandolin or "resophonic mandolin" is a mandolin whose sound is produced by one or more metal cones (resonators) instead of the customary wooden soundboard (mandolin top/face). Historic brands include Dobro and National.
Electric mandolin.
As with almost every other contemporary string instrument, another modern variant is the electric mandolin. These mandolins can have four or five individual or double courses of strings.
They have been around since the late 1920s or early 1930s depending on the brand. They come in solid body and acoustic electric forms.
Mandolin family.
Soprano.
The mandolin is the soprano member of the mandolin family, as the violin is the soprano member of the violin family. Like the violin, its scale length is typically about 13 inches (330 mm). Modern American mandolins modeled after Gibsons have a longer scale, about 13-7/8" (352 mm).
Tuning.
A variety of different tunings are used. Usually, courses of 2 adjacent strings are tuned in unison. The most common tuning by far, GDAE, is the same as violin tuning:
Note that the numbers of Hz shown above assume a 440 Hz A, standard in most parts of the United States. Some players use A's up to 10 Hz above or below a 440, mainly in other parts of the world.
Other tunings exist, including "cross-tunings," in which the usually doubled string runs are tuned to different pitches. Additionally, guitarists may sometimes tune a mandolin to mimic a portion of the intervals on a standard guitar tuning to achieve familiar fretting patterns.
History.
Mandolins evolved from the lute family in Italy during the seventeenth and eighteenth centuries, and the deep bowled mandolin, produced particularly in Naples, became common in the nineteenth century.
Early precursors.
Dating to around c. 13000 BC, a cave painting in the Trois Frères cave in France depicts what some believe to be a musical bow, a hunting bow used as a single-stringed musical instrument.
From the musical bow, families of stringed instruments developed; since each string played a single note, adding strings added new notes, creating bow harps, harps and lyres. In turn, this led to being able to play dyads and chord. Another innovation occurred when the bow harp was straightened out and a bridge used to lift the strings off the stick-neck, creating the lute.
First lutes.
The plucked family of stringed instruments included lute-like instruments in Mesopotamia prior to 3000 BC . A cylinder seal from c. 3100 BC or earlier (now in the possession of the British Museum) shows what is thought to be a woman playing a stick lute.
The Mesopotamian lutes developed into a long variety and a short. The line of short lutes was further developed to the east of Mesopotamia, in Bactria and Gandhara, into a short, almond-shaped lute.
Persian barbat, Arab oud.
Andalusia.
Bactria and Gandhara became part of the Sasanian Empire (224–651AD). Under the Sasanians, a short almond shaped lute from Bactria came to be called the barbat or barbud, which was developed into the later Islamic world's "oud" or "ud". When the Moors conquered Andalusia in 711AD, they brought their ud along, into a country that had already known a lute tradition under the Romans, the pandura.
During the 8th and 9th centuries, many musicians and artists from across the Islamic world flocked to Iberia. Among them was Abu l-Hasan ‘Ali Ibn Nafi‘ (789-857), a prominent musician who had trained under Ishaq al-Mawsili (d. 850) in Baghdad and was exiled to Andalusia before 833 AD. He taught and has been credited with adding a fifth string to his oud and with establishing one of the first schools of music in Córdoba.
By the 11th century, Muslim Iberia had become a center for the manufacture of instruments. These goods spread gradually to Provence, influencing French troubadours and trouvères and eventually reaching the rest of Europe.
From Sicily to Germany.
Beside the introduction of the lute to Spain (Andalusia) by the Moors, another important point of transfer of the lute from Arabian to European culture was Sicily, where it was brought either by Byzantine or later by Muslim musicians. There were singer-lutenists at the court in Palermo following the Norman conquest of the island from the Muslims, and the lute is depicted extensively in the ceiling paintings in the Palermo’s royal Cappella Palatina, dedicated by the Norman King Roger II of Sicily in 1140. His Hohenstaufen grandson Frederick II, Holy Roman Emperor (1194 - 1250) continued integrating Muslims into his court, including Moorish musicians.By the fourteenth century, lutes had disseminated throughout Italy and, probably because of the cultural influence of the Hohenstaufen kings and emperor, based in Palermo, the lute had also made significant inroads into the German-speaking lands.
European lute beginnings.
A distinct European tradition of lute development is noticeable in pictures and sculpture from the thirteenth century onward. As early as the beginning of the fourteenth century, strings were doubled into courses on the miniature lute or gittern, used throughout Europe. The small zigzag-shaped soundhole became a round soundhole covered with a decoration. The "mandore", appeared in the late sixteenth century, and the mandolino or Baroque mandolin in the seventeenth century.
Development in Italy, birth of Neapolitan mandolin.
The mandore was not a final form, and the design was tinkered with wherever it was built. The Italians redesigned it and produced the "mandolino" (a small catgut-strung mandola with six strings tuned g b e' a' d" g" sometimes called the "Baroque mandolin" and played with a quill, wooden plectrum or finger-style). The mandolino was sometimes called a "mandolin" in the early eighteenth century (around 1735) Naples. At this point, all such instruments were strung with gut strings.
Vinaccia.
First metal-string mandolins.
The first evidence of modern metal-string mandolins is from literature regarding popular Italian players who travelled through Europe teaching and giving concerts. Notable are Signor Leone and Giovanni Battista Gervasio, who travelled widely between 1750 and 1810. This, with the records gleaned from the Italian Vinaccia family of luthiers in Naples, Italy, led some musicologists to believe that the modern steel-string mandolins were developed in Naples by the Vinaccia family.
There is confusion currently as to the name of the eldest Vinaccia luthier who first ran the shop. His name has been put forth as Gennaro Vinaccia (active c. 1710 to c. 1788) and Nic. Vinaccia. His son Antonio Vinaccia was active c. 1734 to c. 1796. An early extant example of a mandolin is one built by Antonio Vinaccia in 1759, which resides at the University of Edinburgh. Another is by Giuseppe Vinaccia, built in 1893, is also at the University of Edinburgh. The earliest extant mandolin was built in 1744 by Antonio's son, Gaetano Vinaccia. It resides in the Conservatoire Royal de Musique in Brussels, Belgium.
Family mandolin modified to create Neapolitan mandolin.
Gaetano's son, Pasquale Vinaccia (b.1806-d.1881), modernized the mandolin, adding features, creating the "Neapolitan" mandolin c. 1835. Pasquale remodeled, raised and extended the fingerboard to 17 frets, introduced stronger wire strings made of high-tension steel and substituted a machine head for the friction tuning pegs, then standard. The new wire strings required that he strengthen the mandolin's body, and he deepened the mandolin's bowl, giving the tonal quality more resonance.
Calace, Embergher and others.
Other luthiers who built mandolins included Raffaele Calace (1863 onwards) in Naples, Luigi Embergher (1856–1943) in Rome and Arpino, the Ferrari family (1716 onwards, also originally mandolino makers) in Rome, and De Santi (1834–1916) in Rome. The Neapolitan style of mandolin construction was adopted and developed by others, notably in Rome, giving two distinct but similar types of mandolin – Neapolitan and Roman.
Rising and falling fortunes.
First wave.
The transition from the mandolino to the mandolin began around 1744 with the designing of the metal-string mandolin by the Vinaccia family, 3 brass strings and one of gut, using friction tuning pegs on a fingerboard that sat "flush" with the sound table. The mandolin grew in popularity over the next 60 years, in the streets where it was used by young men courting and by street musicians, and in the concert hall. After the Napoleonic Wars of 1815, however, its popularity began to fall. The 19th century produced some prominent players, including Bartolomeo Bortolazzi of Venice and Pietro Vimercati. However, professional virtuosity was in decline, and the mandolin music changed as the mandolin became a folk instrument; "the large repertoire of notated instrumental music for the mandolino and the mandoline was completely forgotten". The export market for mandolins from Italy dried up around 1815. Salvador Léonardi mentioned this decline in his 1921 book, "Méthode pour Banjoline ou Mandoline-Banjo", saying that the mandolin had been declining in popularity from previous times.
It was during this slump in popularity (specifically in 1835) that Pasquale Vinaccia made his modifications to the instrument that his family made for generations, creating the Neapolitan mandolin. The mandolin was largely forgotten outside of Italy by that point, but the stage was set for it to become known again, starting with the Paris Exposition in 1878.
Second wave, the "Golden Age" of mandolins.
Beginning with the Paris Exposition of 1878, the instrument's popularity rebounded. The Exposition was one of many stops for a popular new performing group the "Estudiantes Españoles" ("Spanish Students"). They danced and played bandurria, which became confused with the mandolin. Along with the energy and awareness created by the day's hit sensation, a wave of Italian mandolinists traveled Europe in the 1880s and 1890s and in the United States by the mid-1880s, playing and teaching their instrument. The instrument's popularity continued to increase during the 1890s and mandolin popularity was at its height in "early years of the twentieth-century." Thousands were taking up the instrument as a pastime, and it became an instrument of "society", taken up by young men and women. Mandolin orchestras were formed worldwide, incorporating not only the mandolin family of instruments, but also guitars, double basses and zithers.
That era (from the late 19th-Century into the early 20th-Century) has come to be known as the "Golden Age" of the mandolin. The term is used online by mandolin enthusiasts to name the time period when the mandolin had become popular, when mandolin orchestras were being organized worldwide, and new and high-quality instruments were increasingly common.
After World War I, the instrument's popularity again fell, although gradually. Reasons cited included the rise of Jazz, for which the instrument was too quiet. Also the changing pace of life was cited, as people became busier and as modern conveniences (phonograph records, bicycle and automobiles, outdoor sports) competed with learning to play an instrument for fun.
Aftermath.
The second decline was not as complete as the first. Thousands of people had learned to play the instrument. Even as the second wave of mandolin popularity declined in the early 20th Century, new versions of the mandolin began to be used in new forms of music. Luthiers created the resonator mandolin, the flatback mandolin, the carved-top or arched-top mandolin, the mandolin-banjo and the electric mandolin. Musicians began playing it in Celtic, Bluegrass, Jazz, and Rock-n-Roll styles — and "Classical" too.
Classical.
Many classical composers composed works specifically for the Mandolin. Beethoven composed mandolin music and enjoyed playing the mandolin. The opera "Don Giovanni" by Mozart includes mandolin parts, including the accompaniment to the famous aria "Deh vieni alla finestra". Also very well known are the mandolin concerti by Vivaldi. Gustav Mahler used the mandolin in his Symphony No. 7, Symphony No. 8 and Das Lied von der Erde. Some 20th-century classical composers also used the mandolin as their instrument of choice (amongst these are: Schoenberg, Webern, Stravinsky).
The mandolin orchestras that play classical remain especially popular in Japan and Germany, and exist throughout the United States, Europe and the rest of the world. They often perform works composed for mandolin family instruments, or re-orchestrations of traditional pieces. They tend to consist of first and second mandolins, mandolas (either octave mandolas, tuned an octave below the mandolin, or tenor mandolas, tuned like the viola), and mandocellos (tuned like the cello). These are also may be used as quartets composed of two mandolins, tenor mandola, and mandocello.
Mandolin traditions in Italy and worldwide.
The international repertoire of music for mandolin is almost unlimited, and musicians use it to play various types of music. This is especially true of violin music, since the mandolin has the same tuning of the violin. Following its invention and early development in Italy the mandolin spread throughout the European continent. The instrument was primarily used in a classical tradition with Mandolin orchestras, so called "Estudiantinas" or in Germany "Zupforchestern" appearing in many cities. Following this continental popularity of the mandolin family local traditions appeared outside of Europe in the Americas and in Japan. Traveling mandolin virtuosi like Giuseppe Pettine, Raffaele Calace and Silvio Ranieri contributed to the mandolin becoming a "fad" instrument in the early 20th century. This "mandolin craze" was fading by the 1930s, but just as this practice was falling into disuse, the mandolin found a new niche in American country, old-time music, bluegrass, and folk music. More recently, the Baroque and Classical mandolin repertory and styles have benefited from the raised awareness of and interest in Early music, with media attention to classical players such as Israeli Avi Avital.
Australia.
The earliest references to the mandolin in Australia come from Phil Skinner (1903–1991). In his article "Recollections" he mentions a Walter Stent, who was “active in the early part of the century and organised possibly the first Mandolin Orchestra in Sydney.”
Phil Skinner played a key role in 20th century development of the mandolin movement in Australia, and was awarded an MBE in 1979 for services to music and the community. He was born Harry Skinner in Sydney in 1903 and started learning music at age 10 when his uncle tutored him on the banjo. Skinner began teaching part-time at age 18, until the Great Depression forced him to begin teaching full-time and learn a broader range of instruments. Skinner founded the Sydney Mandolin Orchestra, the oldest surviving mandolin orchestra in Australia.
The Sydney Mandolins (Artistic Director: Adrian Hooper) have contributed greatly to the repertoire through commissioning over 200 works by Australian and International composers. Most of these works have been released on Compact Disks and can regularly be heard on radio stations on the ABC and MBS networks. One of their members, mandolin virtuoso Paul Hooper, has had a number of Concertos written for him by composers such as Eric Gross. He has performed and recorded these works with the Sydney Symphony Orchestra and the Tasmanian Symphony Orchestra. As well, Paul Hooper has had many solo works dedicated to him by Australian composers e.g., Caroline Szeto, Ian Shanahan, Larry Sitsky and Michael Smetanin.
In January 1979, the Federation of Australian Mandolin Ensembles (FAME) Inc. formed. Bruce Morey from Melbourne is the first FAME President. An Australian Mandolin Orchestra toured Germany in May 1980.
Australian popular groups such as My Friend The Chocolate Cake use the mandolin extensively. The McClymonts also use the mandolin, as do Mic Conway's National Junk Band and the Blue Tongue Lizards. Nevertheless, in folk and traditional styles, the mandolin remains more popular in Irish Music and other traditional repertoires.
Belgium.
In the early 20th century several mandolin orchestras (Estudiantinas) were active in Belgium. Today only a few groups remain: (founded in 1904) in Antwerp, in Brasschaat and an orchestra in Mons (Bergen). Gerda Abts is a well known mandolin virtuoso in Belgium. She is also mandolin teacher and gives lessons in the music academies of Lier, Wijnegem and Brasschaat. She is now also professor mandolin at the music high school “Koninklijk Conservatorium Artesis Hogeschool Antwerpen”. She also gives various concerts each year in different ensembles. She is in close contact to the Brasschaat mandolin Orchestra. Her site is www.gevoeligesnaar.be
Brazil.
The mandolin has a long and rich tradition in Brazilian folk music (where it is called "bandolim"), especially in the style called choro. The composer and mandolin virtuoso Jacob do Bandolim did much to popularize the instrument through many recordings, and his influence continues to the present day. Some contemporary mandolin players in Brazil include Jacob's disciple Déo Rian, and Hamilton de Holanda (the former, a traditional choro-style player, the latter an eclectic innovator).
The mandolin came into Brazil by way of Portugal. Portuguese music has a long tradition of mandolins and mandolin-like instruments (see, for example, the Portuguese guitar).
In Brazilian music, the mandolin is almost exclusively a melody instrument. The cavaquinho, a steel stringed instrument similar to a ukulele provides chordal accompaniment. The mandolin's popularity has risen and fallen with instrumental folk music styles, especially choro. The later part of the 20th century saw a renaissance of choro in Brazil, and with it, a revival of the country's mandolin tradition.
Czech and Slovak republics.
From Italy mandolin music extended in popularity throughout Europe in the early 20th century, with mandolin orchestras appearing throughout the continent.
In the 21st century an increased interest in bluegrass music, especially in Central European countries such as the Czech Republic and Slovak Republic, has inspired many new mandolin players and builders. These players often mix traditional folk elements with bluegrass.
Finland.
Finland has mandolin players rooted in the folk music scene. Prominent names include Petri Hakala, Seppo Sillanpää and Heikki Lahti, who have taught and recorded albums.
Greece.
The mandolin has a long tradition in the Ionian islands (the "Heptanese") and Crete. It has long been played in the Aegean islands outside of the control of the Ottoman Empire. It is common to see choirs accompanied by mandolin players (the "mandolinátes") in the Ionian islands and especially in the cities of Corfu, Zakynthos, and Kefalonia. The evolution of the repertoire for choir and mandolins ("kantádes") occurred during Venetian rule over the islands.
On the island of Crete, along with the lyra and the laouto (lute), the mandolin is one of the main instruments used in Cretan Music. It appeared on Crete around the time of the Venetian rule of the island. Different variants of the mandolin, such as the "mantola," were used to accompany the lyra, the violin, and the laouto. Stelios Foustalierakis reported that the mandolin and the "mpoulgari" were used to accompany the lyra in the beginning of the 20th century in the city of Rethimno. There are also reports that the mandolin was mostly a woman's musical instrument. Nowadays it is played mainly as a solo instrument in personal and family events on the Ionian islands and Crete.
India.
Mandolin music was used in Indian Movies as far back as the 1940s by the Raj Kapoor Studios in movies such as Barsaat. The movie Dilwale Dulhania Le Jayenge (1995) used mandolin in several places.
Adoption of the mandolin in Carnatic music is recent and involves an electric instrument. U. Srinivas has, over the last couple of decades, made his version of the mandolin very popular in India and abroad.
Many adaptations of the instrument have been done to cater to the special needs of Indian Carnatic music.
In Indian classical music and Indian light music, the mandolin, which bears little resemblance to the European mandolin, is usually tuned E-B-E-B. As there is no concept of absolute pitch in Indian classical music, any convenient tuning maintaining these relative pitch intervals between the strings can be used. Another prevalent tuning with these intervals is C-G-C-G, which corresponds to Sa-Pa-Sa-Pa in the Indian carnatic classical music style. This tuning corresponds to the way violins are tuned for carnatic classical music. This type of mandolin is also used in Bhangra, dance music popular in Punjabi culture.
Use of the mandolin also spread into Afghanistan and the mandolin is often used in Afghan popular music.
Ireland.
The mandolin has become a more common instrument amongst Irish traditional musicians. Fiddle tunes are readily accessible to the mandolin player because of the equivalent tuning and range of the two instruments, and the practically identical (allowing for the lack of frets on the fiddle) left hand fingerings.
Though almost any variety of acoustic mandolin might be adequate for Irish traditional music, virtually all Irish players prefer flat-backed instruments with oval sound holes to the Italian-style bowl-back mandolins or the carved-top mandolins with f-holes favoured by bluegrass mandolinists. The former are often too soft-toned to hold their own in a session (as well as having a tendency to not stay in place on the player's lap), whilst the latter tend to sound harsh and overbearing to the traditional ear. The f-hole mandolin, however, does come into its own in a traditional session, where its brighter tone cuts through the sonic clutter of a pub. Greatly preferred for formal performance and recording are flat-topped "Irish-style" mandolins (reminiscent of the WWI-era Martin Army-Navy mandolin) and carved (arch) top mandolins with oval soundholes, such as the Gibson A-style of the 1920s.
Noteworthy Irish mandolinists include Andy Irvine (who, like Johnny Moynihan, almost always tunes the top E down to D, to achieve an open tuning of GDAD), Paul Brady, Mick Moloney, Paul Kelly, and Claudine Langille. John Sheahan and the late Barney McKenna, respectively fiddle player and tenor banjo player with The Dubliners, are also accomplished Irish mandolin players. The instruments used are either flat-backed, oval hole examples as described above (made by UK luthier Roger Bucknall of Fylde Guitars), or carved-top, oval hole instruments with arched back (made by Stefan Sobell in Northumberland). The Irish guitarist Rory Gallagher often played the mandolin on stage, and he most famously used it in the song "Going To My Hometown."
Israel.
Israel has two world class mandolinists, Avi Avital and Alon Sariel. Both have performed around the world.
Italy.
Important performers in the Italitan tradition include Raffaele Calace (luthier, virtuoso and composer of 180 works for many instruments including mandolin), Pietro Denis (whole also composed "Sonata for mandolin & continuo No. 1 in D major" and "Sonata No. 3"), Giovanni Fouchetti, Gabriele Leone, Carlo Munier (1859-1911), Giuseppe Branzoli (1835-1909), Giovanni Gioviale (1885-1949) and Silvio Ranieri (1882-1956).
Antonio Vivaldi composed a mandolin concerto ("Concerto in C major Op.3 6") and two concertos for two mandolins and orchestra. Wolfgang Amadeus Mozart placed it in his 1787 work "Don Giovanni" and Beethoven created four variations of it. Antonio Maria Bononcini composed "La conquista delle Spagne di Scipione Africano il giovane" in 1707 and George Frideric Handel composed "Alexander Balus" in 1748. Others include Giovani Battista Gervasio ("Sonata in D major for Mandolin and Basso Continuo"), Giuseppe Giuliano ("Sonata in D major for Mandolin and Basso Continuo"), Emanuele Barbella ("Sonata in D major for Mandolin and Basso Continuo"), Domenico Scarlatti ("Sonata n.54 (K.89) in D minor for Mandolin and Basso Continuo"), and Addiego Guerra ("Sonata in G major for Mandolin and Basso Continuo").
More contemporary composers for the mandolin include Giuseppe Anedda (a virtuoso performer and teacher of the first chair of the Conservatory of Italian mandolin), Carlo Aonzo and Dorina Frati.
Japan.
Instruments of the mandolin family are popular in Japan, particularly Neapolitan (round-back) style instruments, and Roman-Embergher style mandolins are still being made there. Japan became seriously interested in mandolins at the beginning of the 20th Century during a process of becoming westernized.
Where interest in the mandolin declined in the United States and parts of Europe after World War I, in Japan there was a boom, with orchestras being formed all over the country.
Connections to the West, including cultural connections with World War II ally Italy, were forming. One musical connection which encouraged mandolin music growth was a visit by mandolin virtuoso Raffaele Calace, who toured extensively at the end of 1924, into 1925, and who gave a performance for the Japanese emperor. Another visiting mandolin virtuoso, Samuel Adelstein, toured from his home in the United States.
The expansion of mandolin use continued after World War II through the late 1960s, and Japan still maintains a strong classical music tradition using mandolins, with active orchestras and university music programs. New orchestras were founded and new orchestral compositions composed. Japanese mandolin orchestras today may consist of up to 40 or 50 members, and can include woodwind, percussion, and brass sections. Japan also maintains an extensive collection of 20th Century mandolin music from Europe and one of the most complete collections of mandolin magazines from mandolin's golden age, purchased by Morishige Takei.
Morishige Takei (1890–1949), who studied Italian in Tokyo College of Language and was a member of the court of Emperor Hirohito, established the mandolin orchestra in the Italian style before World War II. He was also a major composer, with 114 compositions for mandolin.
The military government could not persecute Japanese mandolinists by the authority of Takei So the Japanese mandolin orchestras continued to perform old Italian works after World War II, and they are prosperous today.
Another composer, Jiro Nakano (1902–2000), arranged many of the Italian works for regular orchestras or winds composed before World War II as new repertoires for Japanese mandolin orchestras.
Original compositions for mandolin orchestras were composed increasingly after World War II. Seiichi Suzuki (1901–1980) composed music for early Kurosawa films. Others include Tadashi Hattori (1908–2008), and Hiroshi Ohguri (1918–1982). Ohguri was influenced by Béla Bartók and composed many symphonic works for Japanese mandolin orchestras. Yasuo Kuwahara (1946–2003) used German techniques. Many of his works were published in Germany.
New Zealand.
The Auckland Mandolinata mandolin orchestra was formed in 1969 by Doris Flameling (1932–2004). Soon after arriving from the Netherlands with her family, Doris started teaching guitar and mandolin in West Auckland. In 1969, she formed a small ensemble for her pupils. This ensemble eventually developed into a full size mandolin orchestra, which survives today. Doris was the musical director and conductor of this orchestra for many years. The orchestra is currently led by Bryan Holden (conductor).
The early history of the mandolin in New Zealand is currently being researched by members of the Auckland Mandolinata.
Portugal.
The "bandolim" (Portuguese for "mandolin") was a favorite instrument within the Portuguese bourgeoisie of the 19th century, but its rapid spread took it to other places, joining other instruments. Today you can see mandolins as part of the traditional and folk culture of Portuguese singing groups and the majority of the mandolin scene in Portugal is in Madeira Island. Madeira has over 17 active mandolin Orchestras and Tunas. The mandolin virtuoso Fabio Machado is one of Portugal's most accomplished mandolin players. The Portuguese influence brought the mandolin to Brazil.
South Africa.
Mandolin has been a prominent instrument in the recordings of Johnny Clegg and his bands Juluka and Savuka. Since 1992, Andy Innes has been the mandolinist for Johnny Clegg and Savuka.
United Kingdom.
The mandolin has been used extensively in the traditional music of England and Scotland for generations. Simon Mayor is a prominent British player who has produced six solo albums, instructional books and DVDs, as well as recordings with his mandolin quartet The Mandolinquents. The instrument has also found its way into British rock music. The mandolin was played by Mike Oldfield (and introduced by Vivian Stanshall) on Oldfield's album "Tubular Bells", as well as on a number of his subsequent albums (particularly prominently on "Hergest Ridge" (1974) and "Ommadawn" (1975)). It was used extensively by the British folk-rock band Lindisfarne, who featured two members on the instrument, Ray Jackson and Simon Cowe, and whose "Fog on the Tyne" was the biggest selling UK album of 1971-1972. The instrument was also used extensively in the UK folk revival of the 1960s and 1970s with bands such as Fairport Convention and Steeleye Span taking it on as the lead instrument in many of their songs. "Maggie May" by Rod Stewart, which hit No. 1 on both the British charts and the Billboard Hot 100, also featured Jackson's playing. It has also been used by other British rock musicians. Led Zeppelin's bassist John Paul Jones is an accomplished mandolin player and has recorded numerous songs on mandolin including "Going to California" and "That's the Way"; the mandolin part on "The Battle of Evermore" is played by Jimmy Page, who composed the song. Other Led Zeppelin songs featuring mandolin are "Hey Hey What Can I Do", and "Black Country Woman." Pete Townshend of The Who played mandolin on the track "Mike Post Theme", along with many other tracks on Endless Wire. McGuinness Flint, for whom Graham Lyle played the mandolin on their most successful single, "When I'm Dead And Gone", is another example. Lyle was also briefly a member of Ronnie Lane's Slim Chance, and played mandolin on their hit "How Come." One of the more prominent early mandolin players in popular music was Robin Williamson in The Incredible String Band. Ian Anderson of Jethro Tull is a highly accomplished mandolin player (beautiful track "Pussy Willow"), as is his guitarist Martin Barre. The popular song "Please Please Please Let Me Get What I Want" by The Smiths featured a mandolin solo played by Johnny Marr. More recently, the Glasgow-based band Sons and Daughters featured the mandolin, played by Ailidh Lennon, on tracks such as "Fight," "Start to End," and "Medicine." British folk-punk icons the Levellers also regularly use the mandolin in their songs. Current bands are also beginning to use the Mandolin and its unique sound - such as South London's Indigo Moss who use it throughout their recordings and live gigs. The mandolin has also featured in the playing of Matthew Bellamy in the rock band Muse. It also forms the basis of Paul McCartney's 2007 hit "Dance Tonight." That was not the first time a Beatle played a mandolin, however; that distinction goes to George Harrison on "Gone Troppo," the title cut from the 1982 album of the same name. The mandolin is taught in Lanarkshire by the Lanarkshire Guitar and Mandolin Association to over 100 people. Also more recently hard rock supergroup Them Crooked Vultures have been playing a song based primarily using a mandolin. This song was left off their debut album, and features former Led Zeppelin bassist John Paul Jones.
In the Classical style, performers such as Hugo D'Alton, Alison Stephens and Michael Hooper have continued to play music by British composers such as Michael Finnissy, James Humberstone and Elspeth Brooke.
United States.
Mandolin orchestras and classical-music virtuosos.
The mandolin's popularity in the United States was spurred by the success of a group of touring young European musicians known as the Estudiantina Figaro, or in the United States, simply the "Spanish Students." The group landed in the U.S. on January 2, 1880 in New York City, and played in Boston and New York to wildly enthusiastic crowds. Ironically, this ensemble did not play mandolins but rather bandurrias, which are also small, double-strung instruments that resemble the mandolin. The success of the Figaro Spanish Students spawned other groups who imitated their musical style and costumes. An Italian musician, Carlo Curti, hastily started a musical ensemble after seeing the Figaro Spanish Students perform; his group of Italian born Americans called themselves the "Original Spanish Students," counting on the American public to not know the difference between the Spanish bandurrias and Italian mandolins. The imitators' use of mandolins helped to generate enormous public interest in an instrument previously relatively unknown in the United States.
Mandolin awareness in the United States blossomed in the 1880s, as the instrument became part of a fad that continued into the mid-1920s. According to Clarence L. Partee, the first mandolin made in the United States was made in 1883 or 1884 by Joseph Bohmann, who was an established maker of violins in Chicago. Partee characterized the early instrument as being larger than the European instruments he was used to, with a "peculiar shape" and "crude construction," and said that the quality improved, until American instruments were "superior" to imported instruments. At the time, Partee was using an imported French-made mandolin.
Instruments were marketed by teacher-dealers, much as the title character in the popular musical "The Music Man". Often, these teacher-dealers conducted mandolin orchestras: groups of 4-50 musicians who played various mandolin family instruments. However, alongside the teacher-dealers were serious musicians, working to create a spot for the instrument in classical music, ragtime and jazz. Like the teacher-dealers, they traveled the U.S., recording records, giving performances and teaching individuals and mandolin orchestras. Samuel Siegel played mandolin in Vaudeville and became one of America's preeminent mandolinists. Seth Weeks was an African American who not only taught and performed in the United States, but also in Europe, where he recorded records. Another pioneering African American musician and director who made his start with a mandolin orchestra was composer James Reese Europe. W. Eugene Page toured the country with a group, and was well known for his mandolin and mandola performances. Other names include Valentine Abt, Samuel Adelstein, William Place, Jr., and Aubrey Stauffer.
The instrument was primarily used in an ensemble setting well into the 1930s, and although the fad died out at the beginning of the 1930s, the instruments that were developed for the orchestra found a new home in bluegrass. The famous Lloyd Loar Master Model from Gibson (1923) was designed to boost the flagging interest in mandolin ensembles, with little success. However, The "Loar" became the defining instrument of bluegrass music when Bill Monroe purchased F-5 S/N 73987 in a Florida barbershop in 1943 and popularized it as his main instrument.
The mandolin orchestras never completely went away, however. In fact, along with all the other musical forms the mandolin is involved with, the mandolin ensemble (groups usually arranged like the string section of a modern symphony orchestra, with first mandolins, second mandolins, mandolas, mandocellos, mando-basses, and guitars, and sometimes supplemented by other instruments) continues to grow in popularity. Since the mid-nineties, several public-school mandolin-based guitar programs have blossomed around the country, including Fretworks Mandolin and Guitar Orchestra, the first of its kind. The national organization, Classical Mandolin Society of America, founded by Norman Levine, represents these groups. Prominent modern mandolinists and composers for mandolin in the classical music tradition include Samuel Firstman, Howard Fry, Rudy Cipolla, Dave Apollon, Neil Gladd, Evan Marshall, Marilynn Mair and Mark Davis (the Mair-Davis Duo), Brian Israel, David Evans, Emanuil Shynkman, Radim Zenkl, David Del Tredici and Ernst Krenek.
Bluegrass, Blues, and the jug band.
Single mandolins were first used in southern string band music in the 1930s, most notably by brother duets such as the sedate Blue Sky Boys (Bill Bolick and Earl Bolick) and the more hard-driving Monroe Brothers (Bill Monroe and Charlie Monroe). However, the mandolin's modern popularity in country music can be directly traced to one man: Bill Monroe, the father of bluegrass music. After the Monroe Brothers broke up in 1939, Bill Monroe formed his own group, after a brief time called the Blue Grass Boys, and completed the transition of mandolin styles from a "parlor" sound typical of brother duets to the modern "bluegrass" style. He joined the Grand Ole Opry in 1939 and its powerful clear-channel broadcast signal on WSM-AM spread his style throughout the South, directly inspiring many musicians to take up the mandolin. Monroe famously played Gibson F-5 mandolin, signed and dated July 9, 1923, by Lloyd Loar, chief acoustic engineer at Gibson. The F-5 has since become the most imitated tonally and aesthetically by modern builders.
Monroe's style involved playing lead melodies in the style of a fiddler, and also a percussive chording sound referred to as "the chop" for the sound made by the quickly struck and muted strings. He also perfected a sparse, percussive blues style, especially up the neck in keys that had not been used much in country music, notably B and E. He emphasized a powerful, syncopated right hand at the expense of left-hand virtuosity. Monroe's most influential follower of the second generation is Frank Wakefield and nowadays Mike Compton of the Nashville Bluegrass Band and David Long, who often tour as a duet. Tiny Moore of the Texas Playboys developed an electric five-string mandolin and helped popularize the instrument in Western Swing music.
Other major bluegrass mandolinists who emerged in the early 1950s and are still active include Jesse McReynolds (of Jim and Jesse) who invented a syncopated banjo-roll-like style called crosspicking—and Bobby Osborne of the Osborne Brothers, who is a master of clarity and sparkling single-note runs. Highly respected and influential modern bluegrass players include Herschel Sizemore, Doyle Lawson, and the multi-genre Sam Bush, who is equally at home with old-time fiddle tunes, rock, reggae, and jazz. Ronnie McCoury of the Del McCoury Band has won numerous awards for his Monroe-influenced playing. The late John Duffey of the original Country Gentlemen and later the Seldom Scene did much to popularize the bluegrass mandolin among folk and urban audiences, especially on the east coast and in the Washington, D.C. area.
Jethro Burns, best known as half of the comedy duo Homer and Jethro, was also the first important jazz mandolinist. Tiny Moore popularized the mandolin in Western swing music. He initially played an 8-string Gibson but switched after 1952 to a 5-string solidbody electric instrument built by Paul Bigsby. Modern players David Grisman, Sam Bush, and Mike Marshall, among others, have worked since the early 1970s to demonstrate the mandolin's versatility for all styles of music. Chris Thile of California is a well-known player, and has accomplished many feats of traditional bluegrass, classical, contemporary pop and rock; the band Nickel Creek featured his playing in its blend of traditional and pop styles, and he now plays in his band Punch Brothers. Most commonly associated with bluegrass, mandolin has been used a lot in country music over the years. Some well-known players include Marty Stuart and Vince Gill.
Mandolin has also been used in blues music, most notably by Ry Cooder, who performed outstanding covers on his very first recordings, Yank Rachell, Johnny "Man" Young, Carl Martin, and Gerry Hundt.
It saw some use in jug band music, since that craze began as the mandolin fad was waning, and there were plenty of instruments available at relatively low cost.
Rock and Celtic.
The mandolin has been used occasionally in rock music, first appearing in the psychedelic era of the late 1960s. Levon Helm of The Band occasionally moved from his drum kit to play mandolin, most notably on "Rag Mama Rag," "Rockin' Chair," and "Evangeline." Ian Anderson of Jethro Tull played mandolin on "Fat Man," from their second album, "Stand Up", and also occasionally on later releases. Rod Stewart's still-played 1971 No. 1 hit "Maggie May" features a significant mandolin riff in its motif. David Grisman played mandolin on two Grateful Dead songs on the "American Beauty" album, "Friend of the Devil" and "Ripple", which became instant favorites among amateur pickers at jam sessions and campground gatherings. John Paul Jones and Jimmy Page both played mandolin on a few Led Zeppelin songs. Dash Crofts of the soft rock duo Seals and Crofts extensively used mandolin in their repertoire during the 1970s.
Some rock musicians today use mandolins, often single-stringed electric models rather than double-stringed acoustic mandolins. One example is Tim Brennan of the Irish-American punk rock band Dropkick Murphys. In addition to electric guitar, bass, and drums, the band uses several instruments associated with traditional Celtic music, including mandolin, tin whistle, and Great Highland bagpipes. The band explains that these instruments accentuate the growling sound they favor. The 1991 R.E.M. hit "Losing My Religion" was driven by a few simple mandolin licks played by guitarist Peter Buck, who also played the mandolin in nearly a dozen other songs. The single peaked at No. 4 on the Billboard Hot 100 chart (#1 on the rock and alternative charts), Luther Dickinson of North Mississippi Allstars and The Black Crowes has made frequent use of the mandolin, most notably on the Black Crowes song "Locust Street." Armenian American Rock group System of A Down makes extensive use of the mandolin on their 2005 double album Mezmerize/Hypnotize. Pop punk band Green Day has used a mandolin in several occasions, especially on their 2000 album, "Warning". Boyd Tinsley, violin player of the Dave Matthews Band has been using an electric mandolin since 2005. Frontman Colin Meloy and guitarist Chris Funk of The Decemberists regularly employ the mandolin in the band's music. Nancy Wilson, rhythm guitarist of Heart, uses a mandolin in Heart's song "Dream of the Archer" from the album "Little Queen", as well as in Heart's cover of Led Zeppelin's song "The Battle of Evermore." "Show Me Heaven" by Maria McKee, the theme song to the film "Days of Thunder", prominently features a mandolin.
Many folk punk bands will also feature the mandolin. One such band is Days N' Daze, who make regular use of the mandolin, banjo, ukelele, as well as several other acoustic plucked string instruments. Other folk punk acts include Blackbird Raum, and Johnny Hobo and the Freight Trains.
Venezuela.
As in Brazil, the mandolin has played an important role in the Music of Venezuela. It has enjoyed a privileged position as the main melodic instrument in several different regions of the country. Specifically, the eastern states of Sucre, Nueva Esparta, Anzoategui and Monagas have made the mandolin the main instrument in their versions of Joropo as well as Puntos, Jotas, Polos, Fulias, Merengues and Malagueñas. Also, in the west of the country the sound of the mandolin is intrinsically associated with the regional genres of the Venezuelan Andes: Bambucos, Pasillos, Pasodobles, and Waltzes. In the western city of Maracaibo the Mandolin has been played in Decimas, Danzas and Contradanzas Zulianas; in the capital, Caracas, the Merengue Rucaneao, Pasodobles and Waltzes have also been played with mandolin for almost a century. Today, Venezuelan mandolists include an important group of virtuoso players and ensembles such as Alberto Valderrama, Jesus Rengel, Ricardo Sandoval, Saul Vera, and Cristobal Soto.
Further reading.
Chord dictionaries
Method and instructional guides

</doc>
<doc id="18889" url="http://en.wikipedia.org/wiki?curid=18889" title="Microphotonics">
Microphotonics

Microphotonics is a branch of technology that deals with directing light on a microscopic scale. It is used in optical networking.
Microphotonics employs at least two different materials with a large differential index of refraction to squeeze the light down to a small size. Generally speaking virtually all of microphotonics relies on Fresnel reflection to guide the light. If the photons reside mainly in the higher index material, the confinement is due to total internal reflection. If the confinement is due many distributed Fresnel reflections, the device is termed a photonic crystal. There are many different types of geometries used in microphotonics including optical waveguides, optical microcavities, and Arrayed waveguide gratings.
Photonic crystals.
Photonic crystals are non-conducting materials that reflect various wavelengths of light almost perfectly. Such a crystal can be referred to as a perfect mirror. Other devices employed in microphotonics include micromirrors and photonic wire waveguides. These tools are used to "mold the flow of light", a famous phrase for describing the goal of microphotonics.
Currently microphotonics technology is being developed to replace electronics devices. For instance, the long-standing goal of an all-optical router would eliminate electronic bottlenecks, speeding up the network. Perfect mirrors are being developed for use in fiber optic cables.
Microdisks, microtoroids, and microspheres.
An optical microdisk, optical microtoroid, or optical microsphere uses internal reflection in a circular geometry to hold on to the photons. This type of circularly symmetric optical resonance is called a Whispering gallery mode, after Lord Rayleigh coined the term.
See also.
<br>

</doc>
<doc id="18890" url="http://en.wikipedia.org/wiki?curid=18890" title="Microsoft Windows">
Microsoft Windows

Microsoft Windows (or simply Windows) is a metafamily of graphical operating systems developed, marketed, and sold by Microsoft. It consists of several families of operating systems, each of which cater to a certain sector of the computing industry. Active Windows families include Windows NT, Windows Embedded and Windows Phone; these may encompass subfamilies, e.g. Windows Embedded Compact (Windows CE) or Windows Server. Defunct Windows families include Windows 9x and Windows Mobile.
Microsoft introduced an operating environment named "Windows" on November 20, 1985 as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. However, since 2012, it sells less than Android, which became the most popular operating system in 2014, when counting all of the computing platforms Windows runs on (same as Android); in 2014, the number of Windows device sold were less than 25% of Android devices sold.
s of April 2014[ [update]], the most recent versions of Windows for personal computers, smartphones, server computers and embedded devices are respectively Windows 8.1, Windows Phone 8.1, Windows Server 2012 R2 and Windows Embedded 8. A specialized version of Windows runs on the Xbox One game console.
The next version of Windows is Windows 10 and is currently available as a technical preview; it is set for release for phones, tablets, laptops, and PCs in mid 2015. The next server version of Windows is Windows Server 2016, which is expected to be released in early 2016.
Genealogy.
By marketing role.
Microsoft, the developer of Windows, has registered several trademarks each of which denote a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families are being actively developed:
The following Windows families are no longer being developed:
Version history.
The term "Windows" collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:
Early versions.
The history of Windows dates back to September 1981, when Chase Bishop, a computer scientist, designed the first model of an electronic device and project Interface Manager was started. It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985. Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows.
Windows 2.0 was released in December 1987 and was more popular than its predecessor. It features several improvements to the user interface and memory management. Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights. Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.
Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area.
In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.
The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services. However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.
Windows 3.x.
Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications. Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.
Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.
Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1. The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language. Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.
Windows 9x.
The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world." Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer. Mainstream support for Windows 95 ended on December 31, 2000 and extended support for Windows 95 ended on December 31, 2001.
Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002 and extended support for Windows 98 ended on July 11, 2006.
On September 14, 2000, Microsoft released Windows ME (Millennium Edition), the last DOS-based version of Windows. Windows ME incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs), expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools. However, Windows ME was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. "PC World" considered Windows ME to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.
Windows NT.
Early versions.
In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.
The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993 with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.
Windows XP.
The next major version of Windows, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, modes to help provide compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.
At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications). Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.
After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003. It was followed in December 2005 by Windows Server 2003 R2.
Windows Vista and later.
After a lengthy development process, Windows Vista was released on November 30, 2006 for volume licensing and January 30, 2007 for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism. Vista's server counterpart, Windows Server 2008 was released in early 2008.
On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible. Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar, a home networking system called HomeGroup, and performance improvements.
Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own SkyDrive and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture. An update to Windows 8, called Windows 8.1, was released on October 17, 2013, and includes features such as new live tile sizes, deeper SkyDrive integration, and many other revisions.
On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It will be released in late 2015 and addresses shortcomings in the user interface first introduced with Windows 8. Changes include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode.
Multilingual support.
Multilingual support is built into Windows. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.
Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) - they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).
The interface language of installed applications are not affected by changes in the Windows interface language. Availability of languages depends on the application developers themselves.
Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.
Platform support.
Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Window NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.
With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continued to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.
On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or simply x64), the eighth generation of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.
An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture.
Windows CE.
Windows CE (officially known as "Windows Embedded Compact"), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.
Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.
Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.
Xbox OS.
Xbox OS is an unofficial name given to the version of Windows that runs on the Xbox One. It is a more specific implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.
Timeline of releases.
Windows timeline: Table 
Windows timeline: Bar chart 
The Windows family tree
Usage share.
Usage share as a general platform.
According to Gartner, Windows (including all platforms) sold less than a quarter of Android (the so-called mobile operating system) in 2014, and has been outsold every year since 2012. Android runs on tablets ("personal computers"), smartphones (and traditional PCs but few users do). In 2013, Android-based devices outsold Windows' 2.8:1 (2:1 in mature markets) or by 573 million units (estimating the gap to widen much in 2015 as it did in 2014).
Strategy Analytics estimates the installed base of desktop and mobile PCs (Windows is mostly used on those) to about 1300 million at best; they also estimate the overall tablet installed base to be already of comparable size to the PC market and predict tablets will have surpassed them by 2018. Most of the tablet installed base consists of Android-based devices that add to the following Android smartphone numbers. To put Strategy Analytics estimates in context: by Statistica's estimate, Android smartphones have an installed base, in 2014, of 1600 million Androids smartphones (75% of the estimated 2.2 billion);
Android has the largest installed base of any mobile operating system (in fact of any OS including "non-mobile" ones); as of 2013[ [update]], devices running it also sell more than Windows, iOS and Mac OS X devices combined. In the third quarter of 2013, Android's share of the global smartphone shipment market was 81.3%, An estimated three billion Android smartphones only have been sold by the end of 2014.
Unlike Android, programs may not work across different versions of Windows, but that is planned in Windows 10.
Security.
Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset. However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.
These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's "Counterpane Internet Security" reported that it had seen over 1,000 new viruses and worms in the previous six months. In 2005, Kaspersky Lab found around 11,000 malicious programs—viruses, Trojans, back-doors, and exploits written for Windows.
Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary. In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.
While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.
The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.
Windows Vista changes this by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows Shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.
File permissions.
All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGLP (Accounts, Global, Local, Permissions) AGDLP which in essence where file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directory to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.
Windows Defender.
On January 6, 2005, Microsoft released a Beta version of Microsoft AntiSpyware, based upon the previously released Giant AntiSpyware. On February 14, 2006, Microsoft AntiSpyware became Windows Defender with the release of Beta 2. Windows Defender is a freeware program designed to protect against spyware and other unwanted software. Windows XP and Windows Server 2003 users who have genuine copies of Microsoft Windows can freely download the program from Microsoft's web site, and Windows Defender ships as part of Windows Vista and 7. In Windows 8, Windows Defender and Microsoft Security Essentials have been combined into a single program, named Windows Defender. It is based on Microsoft Security Essentials, borrowing its features and user interface. Although it is enabled by default, it can be turned off to use another anti-virus solution. Windows Malicious Software Removal Tool and the optional Microsoft Safety Scanner are two other free security products offered by Microsoft.
Third-party analysis.
In an article based on a report by Symantec, internetnews.com has described Microsoft Windows as having the "fewest number of patches and the shortest average patch development time of the five operating systems it monitored in the last six months of 2006."
A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004 found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only 4 minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours. This study does not apply to Windows XP systems running the Service Pack 2 update (released in late 2004), which vastly improved the security of Windows XP. The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004 determined that 80% of Windows users were infected by at least one spyware/adware product. Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update.
Alternative implementations.
Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:

</doc>
<doc id="18892" url="http://en.wikipedia.org/wiki?curid=18892" title="Mojo (African-American culture)">
Mojo (African-American culture)

Mojo , in the African-American folk belief called hoodoo, is an amulet consisting of a flannel bag containing one or more magical items. It is a "prayer in a bag", or a spell that can be carried with or on the host's body.
Alternative American names for the mojo bag include hand, mojo hand, conjure hand, lucky hand, conjure bag, trick bag, root bag, toby, jomo, and gris-gris bag.
Ideology.
The most common synonym for the word mojo is "gris-gris", which literally means "fetish" or "charm", thus a gris-gris bag is a charm bag. In the Caribbean, an almost identical African-derived bag is called a "wanga" or "oanga" bag, but that term is uncommon in the United States. The word "conjure" is an ancient alternative to "hoodoo", which is a direct variation of African-American folklore. Because of this, a conjure hand is also considered a hoodoo bag; usually made by a respected community conjure doctor.
The word "hand" in this context is defined as a combination of ingredients. The term may derive from the use of finger and hand bones from the dead in mojo bags, or from ingredients such as the lucky hand root (favored by gamblers). The latter suggests an analogy between the varied bag ingredients and the several cards that make up a "hand" in card games. Mojo reaches as far back as West African culture, where it is said to drive away evil spirits, keep good luck in the household, manipulate a fortune, and lure and persuade lovers. The ideology of the ancestors and the descendants of the mojo hand used this "prayer in a bag" based on their belief of spiritual inheritance, where the omniscient forefathers of their families would provide protection and favor; especially when they used the mojo. Through this, a strong belief was placed in the idealism of whomever used mojo, creating a spiritual trust in the magic itself.
Making.
Although most Southern-style conjure bags are made of red flannel material, most seasoned conjurers use color-symbolism. This practice embodies itself in the practice of hoodoo, where green flannel is used for a money mojo, white flannel is used for a baby-blessing mojo, red flannel is used for love mojo, and so on. West Indians also use mojo bags, but often use leather instead of flannel.
The contents of each bag vary directly with the aim of the conjurer. For example, a mojo carried for love-drawing will contain different ingredients than one for gambling luck or magical protection. Ingredients can include roots, herbs, animal parts, minerals, coins, crystals, good luck tokens, and carved amulets. The more personalized objects are used to add extra power because of the symbolic value.
Maintenance.
Fixing and feeding a mojo hand.
There is a process to fixing a proper mojo. A ritual must be put in place in order to successfully prepare a mojo by being filled and awakened to life. This can be done by smoking incense and candles, or breathed upon to bring it to life. Prayers may be said, and other methods may be used to accomplish this essential step. Once prepared, the mojo is "dressed" or "fed" with a liquid such as alcohol, perfume, water, or bodily fluids. The reason why it is said to feed the mojo to keep it working is because it is alive with spirit. One story from the work entitled "From My People" describes a slave who went out and sought a mojo conjurer that gave him a mojo to run away from home. The story describes the slave's mojo as fixing him into many formations, and he ultimately dies because he misuses its power. Had he fixed and believed in the specific mojo himself, he might have escaped the plantation alive.
Hiding the mojo.
Mojos are traditionally made for an individual, and so must be concealed on the person at all times. Men usually keep the trinkets hidden in the pants pocket, while women are more prone to clip it to the bra. They are also commonly pinned to clothes below the waist. Depending on the type of mojo, the hiding place will be crucial to its success, as those who make conjure bags to carry love spells sometimes specify that the mojo must be worn next to the skin. A story from the book "From My People" described the story of Moses, and the task he went through to bring his people out of slavery. It described how "Hoodoo Lost his Hand", as Moses' mojo was hidden through his staff. When he turned it into a snake, the pharaoh made his soothsayers and magicians create the same effect. As a result, the Pharaoh's snake was killed by Moses' snake; and that is how Hoodoo lost his hand.

</doc>
<doc id="18894" url="http://en.wikipedia.org/wiki?curid=18894" title="Matt Groening">
Matt Groening

Matthew Abram "Matt" Groening ( ; born February 15, 1954) is an American cartoonist, screenwriter, and producer. He is the creator of the comic strip "Life in Hell" (1977–2012) as well as the co-creator of two successful television series, "The Simpsons" (1989–present), "Futurama" (1999–2003, 2008–2013), and a Christmas special "Olive, the Other Reindeer". "The Simpsons" has gone on to become the longest running U.S. primetime television series in history, as well as the longest running animated series and sitcom.
Groening made his first professional cartoon sale of "Life in Hell" to the avant-garde "Wet" magazine in 1978. At its peak, the cartoon was carried in 250 weekly newspapers. "Life in Hell" caught the attention of James L. Brooks. In 1985, Brooks contacted Groening with the proposition of working in animation for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Fearing the loss of ownership rights, Groening decided to create something new and came up with a cartoon family, the Simpson family, and named the members after his own parents and sisters—while Bart was an anagram of the word brat. The shorts would be spun off into their own series "The Simpsons", which has since aired 573 episodes. In 1997, Groening and former "Simpsons" writer David X. Cohen developed "Futurama", an animated series about life in the year 3000, which premiered in 1999, running for four years on Fox, then picked up by Comedy Central for additional seasons.
Groening has won 12 Primetime Emmy Awards, ten for "The Simpsons" and two for "Futurama" as well as a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2002, he won the National Cartoonist Society Reuben Award for his work on "Life in Hell". He received a star on the Hollywood Walk of Fame on February 14, 2012.
Early life.
Groening was born on February 15, 1954 in Portland, Oregon, the middle of five children (older brother Mark and sister Patty were born in 1950 and 1952, while the younger sisters Lisa and Maggie in 1956 and 1958, respectively). His Norwegian-American mother, Margaret Ruth (née Wiggum; March 23, 1919 – April 22, 2013), was once a teacher, and his German American father, Homer Philip Groening (December 30, 1919 – March 15, 1996), was a filmmaker, advertiser, writer and cartoonist. Homer, born in Main Centre, Saskatchewan, Canada, grew up in a Mennonite, Plautdietsch-speaking family.
Matt's grandfather, Abram Groening, was a professor at Tabor College, a Mennonite Brethren liberal arts college in Hillsboro, Kansas before moving to Albany College (now known as Lewis and Clark College) in Oregon in 1930.
Groening grew up in Portland, and attended Ainsworth Elementary School and Lincoln High School. From 1972 to 1977, Groening attended The Evergreen State College in Olympia, Washington, a liberal arts school that he described as "a hippie college, with no grades or required classes, that drew every weirdo in the Northwest." He served as the editor of the campus newspaper, "The Cooper Point Journal", for which he also wrote articles and drew cartoons. He befriended fellow cartoonist Lynda Barry after discovering that she had written a fan letter to Joseph Heller, one of Groening's favorite authors, and had received a reply. Groening has credited Barry with being "probably [his] biggest inspiration." He first became interested in cartoons after watching the Disney animated film "One Hundred and One Dalmatians", and he has also cited "Peanuts" and its creator Charles M. Schulz as inspirations.
Career.
Early career.
In 1977, at the age of 23, Groening moved to Los Angeles to become a writer. He went through what he described as "a series of lousy jobs," including being an extra in the television movie "When Every Day Was the Fourth of July", busing tables, washing dishes at a nursing home, clerking at the Hollywood Licorice Pizza record store, landscaping in a sewage treatment plant, and chauffeuring and ghostwriting for a retired Western director.
"Life in Hell".
Groening described life in Los Angeles to his friends in the form of the self-published comic book "Life in Hell", which was loosely inspired by the chapter "How to Go to Hell" in Walter Kaufmann's book "Critique of Religion and Philosophy". Groening distributed the comic book in the book corner of Licorice Pizza, a record store in which he worked. He made his first professional cartoon sale to the avant-garde "Wet" magazine in 1978. The strip, titled "Forbidden Words," appeared in the September/October issue of that year.
Groening had gained employment at the "Los Angeles Reader", a newly formed alternative newspaper, delivering papers, typesetting, editing and answering phones. He showed his cartoons to the editor, James Vowell, who was impressed and eventually gave him a spot in the paper. "Life in Hell" made its official debut as a comic strip in the "Reader" on April 25, 1980. Vowell also gave Groening his own weekly music column, "Sound Mix," in 1982. However, the column would rarely actually be about music, as he would often write about his "various enthusiasms, obsessions, pet peeves and problems" instead. In an effort to add more music to the column, he "just made stuff up," concocting and reviewing fictional bands and non-existent records. In the following week's column, he would confess to fabricating everything in the previous column and swear that everything in the new column was true. Eventually, he was finally asked to give up the "music" column. Among the fans of the column was Harry Shearer, who would later become a voice on "The Simpsons".
"Life in Hell" became popular almost immediately. In November 1984, Deborah Caplan, Groening's then-girlfriend and co-worker at the "Reader", offered to publish "Love is Hell", a series of relationship-themed "Life in Hell" strips, in book form. Released a month later, the book was an underground success, selling 22,000 copies in its first two printings. "Work is Hell" soon followed, also published by Caplan. Soon afterward, Caplan and Groening left and put together the Life in Hell Co., which handled merchandising for "Life in Hell". Groening also started Acme Features Syndicate, which syndicated "Life in Hell", Lynda Barry and John Callahan, but now only syndicates "Life in Hell". At the end of its run, "Life in Hell" was carried in 250 weekly newspapers and has been anthologized in a series of books, including "School is Hell", "Childhood is Hell", "The Big Book of Hell", and "The Huge Book of Hell". Although Groening has stated, "I'll never give up the comic strip. It's my foundation," he announced that the June 16, 2012 strip would mark "Life in Hell"‍ '​s conclusion. After Groening ended the strip, the Center for Cartoon Studies commissioned a poster that was presented to Groening in honor of his work. The poster contained tribute cartoons by 22 of Groening's cartoonist friends who were influenced by "Life in Hell".
"The Simpsons".
Creation.
"Life in Hell" caught the eye of Hollywood writer-producer and Gracie Films founder James L. Brooks, who had been shown the strip by fellow producer Polly Platt. In 1985, Brooks contacted Groening with the proposition of working in animation on an undefined future project, which would turn out to be developing a series of short animated skits, called "bumpers," for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Groening feared that he would have to give up his ownership rights, and that the show would fail and would take down his comic strip with it. Groening conceived of the idea for The Simpsons in the lobby of James L. Brooks's office and hurriedly sketched out his version of a dysfunctional family: Homer, the overweight father; Marge, the slim mother; Bart, the bratty oldest child; Lisa, the intelligent middle child; and Maggie, the baby. Groening famously named the main Simpson characters after members of his own family: his parents, Homer and Margaret (Marge or Marjorie in full), and his younger sisters, Lisa and Margaret (Maggie). Claiming that it was a bit too obvious to name a character after himself, he chose the name "Bart," an anagram of brat. However, he stresses that aside from some of the sibling rivalry, his family is nothing like the Simpsons. Groening also has an older brother and sister, Mark and Patty, and in a 1995 interview Groening divulged that Mark "is the actual inspiration for Bart."
Maggie Groening has co-written a few "Simpsons" books featuring her cartoon namesake.
"The Tracey Ullman Show".
The family was crudely drawn, because Groening had submitted basic sketches to the animators, assuming they would clean them up; instead, they just traced over his drawings. The entire Simpson family was designed so that they would be recognizable in silhouette. When Groening originally designed Homer, he put his own initials into the character's hairline and ear: the hairline resembled an 'M', and the right ear resembled a 'G'. Groening decided that this would be too distracting though, and redesigned the ear to look normal. He still draws the ear as a 'G' when he draws pictures of Homer for fans. Marge's distinct beehive hairstyle was inspired by "Bride of Frankenstein" and the style that Margaret Groening wore during the 1960s, although her hair was never blue. Bart's original design, which appeared in the first shorts, had spikier hair, and the spikes were of different lengths. The number was later limited to nine spikes, all of the same size. At the time Groening was primarily drawing in black and "not thinking that [Bart] would eventually be drawn in color" gave him spikes that appear to be an extension of his head. Lisa's physical features are generally not used in other characters; for example, in the later seasons, no character other than Maggie shares her hairline. While designing Lisa, Groening "couldn't be bothered to even think about girls' hair styles". When designing Lisa and Maggie, he "just gave them this kind of spiky starfish hair style, not thinking that they would eventually be drawn in color". Groening storyboarded and scripted every short (now known as "The Simpsons shorts"), which were then animated by a team including David Silverman and Wes Archer, both of whom would later become directors on the series.
The Simpsons shorts first appeared in "The Tracey Ullman Show" on April 19, 1987. Another family member, Grampa Simpson, was introduced in the later shorts. Years later, during the early seasons of "The Simpsons", when it came time to give Grampa a first name, Groening says he refused to name him after his own grandfather, Abraham Groening, leaving it to other writers to choose a name. By coincidence, they chose Abraham, unaware that it was the name of Groening's grandfather.
Half-hour.
Although "The Tracey Ullman Show" was not a big hit, the popularity of the shorts led to a half-hour spin-off in 1989. A team of production companies adapted "The Simpsons" into a half-hour series for the Fox Broadcasting Company. The team included what is now the Klasky Csupo animation house. James L. Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content. Groening said his goal in creating the show was to offer the audience an alternative to what he called "the mainstream trash" that they were watching. The half-hour series premiered on December 17, 1989 with "Simpsons Roasting on an Open Fire", a Christmas special. "Some Enchanted Evening" was the first full-length episode produced, but it did not broadcast until May 1990, as the last episode of the first season, because of animation problems.
The series quickly became a worldwide phenomenon, to the surprise of many. Groening said: "Nobody thought "The Simpsons" was going to be a big hit. It sneaked up on everybody." "The Simpsons" was co-developed by Groening, Brooks, and Sam Simon, a writer-producer with whom Brooks had worked on previous projects. Groening and Simon, however, did not get along and were often in conflict over the show; Groening once described their relationship as "very contentious." Simon eventually left the show in 1993 over creative differences.
Like the main family members, several characters from the show have names that were inspired by people, locations or films. The name "Wiggum" for police chief Chief Wiggum is Groening's mother's maiden name. The names of a few other characters were taken from major street names in Groening's hometown of Portland, Oregon, including Flanders, Lovejoy, Powell, Quimby and Kearney. Despite common fan belief that Sideshow Bob Terwilliger was named after SW Terwilliger Boulevard in Portland, he was actually named after the character Dr. Terwilliker from the film "The 5,000 Fingers of Dr. T".
Although Groening has pitched a number of spin-offs from "The Simpsons", those attempts have been unsuccessful. In 1994, Groening and other "Simpsons" producers pitched a live-action spin-off about Krusty the Clown (with Dan Castellaneta playing the lead role), but were unsuccessful in getting it off the ground. Groening has also pitched "Young Homer" and a spin-off about the non-Simpsons citizens of Springfield.
In 1995, Groening got into a major disagreement with Brooks and other "Simpsons" producers over "A Star Is Burns", a crossover episode with "The Critic", an animated show also produced by Brooks and staffed with many former "Simpsons" crew members. Groening claimed that he feared viewers would "see it as nothing but a pathetic attempt to advertise "The Critic" at the expense of "The Simpsons"," and was concerned about the possible implication that he had created or produced "The Critic". He requested his name be taken off the episode.
Groening is credited with writing or co-writing the episodes "Some Enchanted Evening", "The Telltale Head", "Colonel Homer" and "22 Short Films About Springfield", as well as "The Simpsons Movie", released in 2007. He has had several cameo appearances in the show, with a speaking role in the episode "My Big Fat Geek Wedding". He currently serves at "The Simpsons" as an Executive Producer and Creative Consultant.
"Futurama".
After spending a few years researching science fiction, Groening got together with "Simpsons" writer/producer David X. Cohen (still known as David S. Cohen at the time) in 1997 and developed "Futurama", an animated series about life in the year 3000. By the time they pitched the series to Fox in April 1998, Groening and Cohen had composed many characters and storylines; Groening claimed they had gone "overboard" in their discussions. Groening described trying to get the show on the air as "by far the worst experience of [his] grown-up life." The show premiered on March 28, 1999. Groening's writing credits for the show are for the premiere episode, "Space Pilot 3000" (co-written with Cohen), "Rebirth" (story) and "In-A-Gadda-Da-Leela" (story).
After four years on the air, the show was canceled by Fox. In a situation similar to "Family Guy", however, strong DVD sales and very stable ratings on Adult Swim brought Futurama back to life. When Comedy Central began negotiating for the rights to air "Futurama" reruns, Fox suggested that there was a possibility of also creating new episodes. When Comedy Central committed to sixteen new episodes, it was decided that four straight-to-DVD films—' (2007), ' (2008), ' (2008) and ' (2009)—would be produced. Since no new "Futurama" projects were in production, the movie "Into the Wild Green Yonder" was designed to stand as the Futurama series finale. However, Groening had expressed a desire to continue the "Futurama" franchise in some form, including as a theatrical film. In an interview with CNN, Groening said that "we have a great relationship with Comedy Central and we would love to do more episodes for them, but I don't know...We're having discussions and there is some enthusiasm but I can't tell if it's just me." Comedy Central commissioned an additional 26 new episodes, and began airing them in 2010. The show continued in to 2013, before Comedy Central announced in April 2013 that they would not be renewing it beyond its seventh season. The final episode aired on September 4, 2013.
Other pursuits.
In 1994, Groening formed Bongo Comics (named after the character Bongo from "Life in Hell") with Steve Vance, Cindy Vance and Bill Morrison, which publishes comic books based on "The Simpsons" and "Futurama" (including "Futurama Simpsons Infinitely Secret Crossover Crisis", a crossover between the two), as well as a few original titles. According to Groening, the goal with Bongo is to "[try] to bring humor into the fairly grim comic book market." He also formed Zongo Comics in 1995, an imprint of Bongo that published comics for more mature readers, which included three issues of Mary Fleener's "Fleener" and seven issues of his close friend Gary Panter's "Jimbo" comics.
Groening is known for his eclectic taste in music. His favorite band is Frank Zappa and The Mothers of Invention and his favorite album is "Trout Mask Replica" by Captain Beefheart (which was produced by Zappa). He guest-edited Da Capo Press's "Best Music Writing 2003" and curated a US All Tomorrow's Parties music festival in 2003. In May 2010, he curated another edition of All Tomorrow's Parties in Minehead, England. He also plays the drums in the all-author rock and roll band The Rock Bottom Remainders (although he is listed as the cowbell player), whose other members include Dave Barry, Ridley Pearson, Scott Turow, Amy Tan, James McBride, Mitch Albom, Roy Blount Jr., Stephen King, Kathi Kamen Goldmark, Sam Barry and Greg Iles. In July 2013, Groening co-authored "Hard Listening" (2013) with the rest of the Rock Bottom Remainders (published by Coliloquy, LLC).
Personal life.
Groening and Deborah Caplan married in 1986 and had two sons together, Homer (who goes by Will) and Abe, both of whom Groening occasionally portrays as rabbits in "Life in Hell". The couple divorced in 1999 after thirteen years of marriage. In 2011, Groening married Argentinian artist Agustina Picasso after a four-year relationship, and became stepfather to her daughter Camille. In May 2013, Picasso gave birth to Nathaniel Philip Picasso Groening, named after writer Nathanael West. She joked that "His godfather is SpongeBob's creator [Stephen Hillenburg]." Matt is the brother-in-law of "Hey Arnold!" creator, Craig Bartlett, who is married to Groening's sister, Lisa. Arnold used to appear in "Simpsons Illustrated".
Groening identifies himself as agnostic and a liberal and has often made campaign contributions to Democratic Party candidates. His first cousin, Laurie Monnes Anderson, is a member of the Oregon State Senate representing eastern Multnomah County.
Awards.
Groening has been nominated for 25 Emmy Awards and has won twelve: ten for "The Simpsons" and two for "Futurama" in the "Outstanding Animated Program (for programming one hour or less)" category. Groening received the 2002 National Cartoonist Society Reuben Award, and had been nominated for the same award in 2000. He received a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2007, he was ranked fourth (and highest American by birth) in a list of the "top 100 living geniuses", published by British newspaper "The Daily Telegraph".
He received the 2,459th star on the Hollywood Walk of Fame on February 14, 2012.

</doc>
<doc id="18895" url="http://en.wikipedia.org/wiki?curid=18895" title="Metaphysics">
Metaphysics

Metaphysics is a traditional branch of philosophy concerned with explaining the fundamental nature of being and the world that encompasses it, although the term is not easily defined. Traditionally, metaphysics attempts to answer two basic questions in the broadest possible terms:
A person who studies metaphysics is called a "metaphysicist" or a "metaphysician". The metaphysician attempts to clarify the fundamental notions by which people understand the world, e.g., existence, objects and their properties, space and time, cause and effect, and possibility. A central branch of metaphysics is ontology, the investigation into the basic categories of being and how they relate to each other. Another central branch of metaphysics is cosmology, the study of the origin, fundamental structure, nature, and dynamics of the universe. Some include Epistemology as another central focus of metaphysics, but this can be questioned.
Prior to the modern history of science, scientific questions were addressed as a part of metaphysics known as natural philosophy. Originally, the term "science" (Latin "scientia") simply meant "knowledge". The scientific method, however, transformed natural philosophy into an empirical activity deriving from experiment unlike the rest of philosophy. By the end of the 18th century, it had begun to be called "science" to distinguish it from philosophy. Thereafter, metaphysics denoted philosophical enquiry of a non-empirical character into the nature of existence. Some philosophers of science, such as the neo-positivists, say that natural science rejects the study of metaphysics, while other philosophers of science strongly disagree.
Etymology.
The word "metaphysics" derives from the Greek words μετά ("metá", "beyond", "upon" or "after") and φυσικά ("physiká", "physics"). It was first used as the title for several of Aristotle's works, because they were usually anthologized after the works on physics in complete editions. The prefix "meta-" ("after") indicates that these works come "after" the chapters on physics. However, Aristotle himself did not call the subject of these books "Metaphysics": he referred to it as "first philosophy." The editor of Aristotle's works, Andronicus of Rhodes, is thought to have placed the books on first philosophy right after another work, "Physics", and called them τὰ μετὰ τὰ φυσικὰ βιβλία ("ta meta ta physika biblia") or "the books that come after the [books on] physics". This was misread by Latin scholiasts, who thought it meant "the science of what is beyond the physical".
However, once the name was given, the commentators sought to find intrinsic reasons for its appropriateness. For instance, it was understood to mean "the science of the world beyond nature" ("physis" in Greek), that is, the science of the immaterial. Again, it was understood to refer to the chronological or pedagogical order among our philosophical studies, so that the "metaphysical sciences" would mean "those that we study after having mastered the sciences that deal with the physical world" (St. Thomas Aquinas, "Expositio in librum Boethii De hebdomadibus", V, 1).
There is a widespread use of the term in current popular literature which replicates this understanding, i.e. that the metaphysical equates to the non-physical: thus, "metaphysical healing" means healing by means of remedies that are not physical."
Origins and nature of metaphysics.
Although the "word" "metaphysics" goes back to Aristotelean philosophy, Plato himself credited earlier philosophers with dealing with metaphysical questions. The first known philosopher, according to Aristotle, is Thales of Miletus, who taught that all things derive from a single first cause or "Arche".
Metaphysics as a discipline was a central part of academic inquiry and scholarly education even before the age of Aristotle, who considered it "the Queen of Sciences." Its issues were considered no less important than the other main formal subjects of physical science, medicine, mathematics, poetics and music. Since the beginning of modern philosophy during the seventeenth century, problems that were not originally considered within the bounds of metaphysics have been added to its purview, while other problems considered metaphysical for centuries are now typically subjects of their own separate regions in philosophy, such as philosophy of religion, philosophy of mind, philosophy of perception, philosophy of language, and philosophy of science.
Central questions.
Most positions that can be taken with regards to any of the following questions are endorsed by one or another notable philosopher. It is often difficult to frame the questions in a non-controversial manner.
Being, existence and reality.
The nature of Being is a perennial topic in metaphysics. For instance, Parmenides taught that reality was a single unchanging Being. The 20th century philosopher Heidegger thought previous philosophers had lost sight of the question of Being (qua Being) in favour of the questions of beings (existing things), so that a return to the Parmenidean approach was needed. An ontological catalogue is an attempt to list the fundamental constituents of reality. The question of whether or not existence is a predicate has been discussed since the Early Modern period, not least in relation to the ontological argument for the existence of God. Existence, "that" something is, has been contrasted with "essence", the question of "what" something is. Reflections on the nature of the connection and distinction between existence and essence dates back to Aristotle's "Metaphysics", and later found one of its most influential interpretations in the ontology of the eleventh century metaphysician Avicenna (Ibn Sina). Since existence without essence seems blank, it is associated with nothingness by philosophers such as Hegel.
Empirical and conceptual objects.
Objects and their properties.
The world seems to contain many individual things, both physical, like apples, and abstract, such as love and the number 3; the former objects are called particulars. Particulars are said to have attributes, e.g., size, shape, color, location, and two particulars may have some such attributes in common. Such attributes are also termed Universals or Properties; the nature of these, and whether they have any real existence and if so of what kind, is a long-standing issue, realism and nominalism representing opposing views.
Metaphysicians concerned with questions about universals or particulars are interested in the nature of objects and their properties, and the relationship between the two. Some, e.g., Plato, argue that properties are abstract objects, existing outside of space and time, to which particular objects bear special relations. David Armstrong holds that universals exist in time and space but only at their instantiation and their discovery is a function of science. Others maintain that particulars are a bundle or collection of properties (specifically, a bundle of properties they have).
Biological literature contains abundant references to taxa (singular "taxon"), groups like the mammals or the poppies. Some authors claim (or at least presuppose) that taxa are real entities, that to say that an animal is included in Mammalia (the scientific name for the mammal group) is to say that it bears a certain relation to Mammalia, an abstract object. Advocates of phylogenetic nomenclature, a more nominalistic view, oppose this reading; in their opinion, calling an animal a mammal is a shorthand way of saying that it is descended from the last common ancestor of, say, humans and platypuses.
Cosmology and cosmogony.
Metaphysical Cosmology is the branch of metaphysics that deals with the world as the totality of all phenomena in space and time. Historically, it has had quite a broad scope, and in many cases was founded in religion. The ancient Greeks drew no distinction between this use and their model for the cosmos. However, in modern times it addresses questions about the Universe which are beyond the scope of the physical sciences. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods (e.g. dialectics).
Cosmogony deals specifically with the origin of the universe.
Modern metaphysical cosmology and cosmogony try to address questions such as:
Determinism and free will.
Determinism is the philosophical proposition that every event, including human cognition, decision and action, is causally determined by an unbroken chain of prior occurrences. It holds that nothing happens that has not already been determined. The principal consequence of the deterministic claim is that it poses a challenge to the existence of free will.
The problem of free will is the problem of whether rational agents exercise control over their own actions and decisions. Addressing this problem requires understanding the relation between freedom and causation, and determining whether the laws of nature are causally deterministic. Some philosophers, known as Incompatibilists, view determinism and free will as mutually exclusive. If they believe in determinism, they will therefore believe free will to be an illusion, a position known as "Hard Determinism". Proponents range from Baruch Spinoza to Ted Honderich.
Others, labeled Compatibilists (or "Soft Determinists"), believe that the two ideas can be coherently reconciled. Adherents of this view include Thomas Hobbes and many modern philosophers such as John Martin Fischer.
Incompatibilists who accept free will but reject determinism are called Libertarians, a term not to be confused with the political sense. Robert Kane and Alvin Plantinga are modern defenders of this theory.
Identity and change.
The Greeks took some extreme positions on the nature of change: Parmenides denied that change occurs at all, while Heraclitus thought change was ubiquitous: "[Y]ou cannot step into the same river twice."
Identity, sometimes called Numerical Identity, is the relation that a "thing" bears to itself, and which no "thing" bears to anything other than itself (cf. sameness). According to Leibniz, if some object x is identical to some object y, then any property that x has, y will have as well. However, it seems, too, that objects can change over time. If one were to look at a tree one day, and the tree later lost a leaf, it would seem that one could still be looking at that same tree. Two rival theories to account for the relationship between change and identity are Perdurantism, which treats the tree as a series of tree-stages, and Endurantism, which maintains that the tree—the same tree—is present at every stage in its history.
Mind and matter.
The nature of matter was a problem in its own right in early philosophy. Aristotle himself introduced the idea of matter in general to the Western world, adapting the term "hyle", which originally meant "lumber." Early debates centered on identifying a single underlying principle. Water was claimed by Thales, air by Anaximenes, "Apeiron" (the Boundless) by Anaximander, fire by Heraclitus. Democritus, in conjunction with his mentor, Leucippus, conceived of an atomic theory many centuries before it was accepted by modern science. It is worth noting, however, that the grounds necessary to ensure validity to the proposed theory's veridical nature were not scientific, but just as philosophical as those traditions espoused by Thales and Anaximander.
The nature of the mind and its relation to the body has been seen as more of a problem as science has progressed in its mechanistic understanding of the brain and body. Proposed solutions often have ramifications about the nature of mind as a whole. René Descartes proposed substance dualism, a theory in which mind and body are essentially different, with the mind having some of the attributes traditionally assigned to the soul, in the seventeenth century. This creates a conceptual puzzle about how the two interact (which has received some strange answers, such as occasionalism). Evidence of a close relationship between brain and mind, such as the Phineas Gage case, have made this form of dualism increasingly unpopular.
Another proposal discussing the mind-body problem is idealism, in which the material is sweepingly eliminated in favor of the mental. Idealists, such as George Berkeley, claim that material objects do not exist unless perceived and only as perceptions. The "German idealists" such as Fichte, Hegel and Schopenhauer took Kant as their starting-point, although it is debatable how much of an idealist Kant himself was. Idealism is also a common theme in Eastern philosophy. Related ideas are panpsychism and panexperientialism, which say everything "has" a mind rather than everything exists "in" a mind. Alfred North Whitehead was a twentieth-century exponent of this approach.
Idealism is a monistic theory which holds that there is a single universal substance or principle. Neutral monism, associated in different forms with Baruch Spinoza and Bertrand Russell, seeks to be less extreme than idealism, and to avoid the problems of substance dualism. It claims that existence consists of a single substance that in itself is neither mental nor physical, but is capable of mental and physical aspects or attributes – thus it implies a dual-aspect theory.
For the last one hundred years, the dominant metaphysics has without a doubt been materialistic monism. Type identity theory, token identity theory, functionalism, reductive physicalism, nonreductive physicalism, eliminative materialism, anomalous monism, property dualism, epiphenomenalism and emergence are just some of the candidates for a scientifically informed account of the mind. (It should be noted that while many of these positions are dualisms, none of them are "substance" dualism.)
Prominent recent philosophers of mind include David Armstrong, Ned Block, David Chalmers, Patricia and Paul Churchland, Donald Davidson, Daniel Dennett, Fred Dretske, Douglas Hofstadter, Jerry Fodor, David Lewis, Thomas Nagel, Hilary Putnam, John Searle, John Smart, Ludwig Wittgenstein, and Fred Alan Wolf.
Necessity and possibility.
Metaphysicians investigate questions about the ways the world could have been. David Lewis, in "On the Plurality of Worlds," endorsed a view called Concrete Modal realism, according to which facts about how things could have been are made true by other concrete worlds, just like ours, in which things are different. Other philosophers, such as Gottfried Leibniz, have dealt with the idea of possible worlds as well. The idea of necessity is that any necessary fact is true across all possible worlds. A possible fact is true in some possible world, even if not in the actual world. For example, it is possible that cats could have had two tails, or that any particular apple could have not existed. By contrast, certain propositions seem necessarily true, such as analytic propositions, e.g., "All bachelors are unmarried." The particular example of analytic truth being necessary is not universally held among philosophers. A less controversial view might be that self-identity is necessary, as it seems fundamentally incoherent to claim that for any x, it is not identical to itself; this is known as the "law of identity", a putative "first principle". Aristotle describes the "principle of non-contradiction", "It is impossible that the same quality should both belong and not belong to the same thing ... This is the most certain of all principles ... Wherefore they who demonstrate refer to this as an ultimate opinion. For it is by nature the source of all the other axioms."
Religion and spirituality.
Theology is the study of a god or gods and the nature of the divine. Whether there is a god (monotheism), many gods (polytheism) or no gods (atheism), or whether it is unknown or unknowable whether any gods exist (agnosticism; apophatic theology), and whether a divine entity directly intervenes in the world (theism), or its sole function is to be the first cause of the universe (deism); these and whether a god or gods and the world are different (as in panentheism and dualism), or are identical (as in pantheism), are some of the primary metaphysical questions concerning philosophy of religion.
Within the standard Western philosophical tradition, theology reached its peak under the medieval school of thought known as scholasticism, which focused primarily on the metaphysical aspects of Christianity. The work of the scholastics is still an integral part of modern philosophy, with key figures such as Thomas Aquinas still playing an important role in the philosophy of religion.
Space and time.
In Book XI of the Confessions, Saint Augustine of Hippo asked the fundamental question about the nature of time. A traditional realist position in ontology is that time and space have existence apart from the human mind. Idealists, including Kant, claim that space and time are mental constructs used to organize perceptions, or are otherwise surreal.
Suppose that one is sitting at a table, with an apple in front of him or her; the apple exists in space and in time, but what does this statement indicate? Could it be said, for example, that space is like an invisible three-dimensional grid in which the apple is positioned? Suppose the apple, and all physical objects in the universe, were removed from existence entirely. Would space as an "invisible grid" still exist? René Descartes and Leibniz believed it would not, arguing that without physical objects, "space" would be meaningless because space is the framework upon which we understand how physical objects are related to each other. Newton, on the other hand, argued for an absolute "container" space. The pendulum swung back to relational space with Einstein and Ernst Mach.
While the absolute/relative debate, and the realism debate are equally applicable to time and space, time presents some special problems of its own. The "flow" of time has been denied in ancient times by Parmenides and more recently by J. M. E. McTaggart in his paper "The Unreality of Time".
The "direction" of time, also known as "time's arrow", is also a puzzle, although physics is now driving the debate rather than philosophy. It appears that fundamental laws are time-reversible and the arrow of time must be an "emergent" phenomenon, perhaps explained by a statistical understanding of thermodynamic entropy.
Common sense tells us that objects "persist" across time, that there is some sense in which you are the same person you were yesterday, in which the oak is the same as the acorn, in which you perhaps even can step into the same river twice. Philosophers have developed two rival theories for how this happens, called "endurantism" and "perdurantism". Broadly speaking, endurantists hold that a whole object exists at each moment of its history, and the same object exists at each moment. Perdurantists believe that objects are four-dimensional entities made up of a series of temporal parts like the frames of a movie.
The Nature of Metaphysics.
The phenomena into which metaphysics enquires are puzzling; that's why metaphysicians enquire into them. Yet the attempt to enquire into those phenomena can also seem puzzling. Take change over time. Objects are continually changing before our very eyes: it is hard to see how anything about the process could remain mysterious, or what more we could learn about change to dispel any mystery remaining. This puzzlement is apt to generate reflection on the nature of metaphysics itself: that is, metametaphysical reflection. Does metaphysics discover truths? If so, what does most of the work in making metaphysical discoveries true: the way the world is, or the way we use language? If not, is it because there simply are no truths of the sort the metaphysician seeks, or is it because there is no reliable method of finding them? Some philosophers, such as Amie Thomasson, have argued that many metaphysical questions can be dissolved just by looking at the way we use words; others, such as Ted Sider, have argued that metaphysical questions are substantive, and that we can make progress towards answering them by comparing theories according to a range of theoretical virtues inspired by the sciences, such as simplicity and explanatory power.
History and schools of metaphysics.
Pre-Socratic metaphysics in Greece.
The first known philosopher, according to Aristotle, is Thales of Miletus. Rejecting mythological and divine explanations, he sought a single first cause or "Arche" (origin or beginning) under which all phenomena could be explained, and concluded that this first cause was in fact moisture or water. Thales also taught that the world is harmonious, has a harmonious structure, and thus is intelligible to rational understanding. Other Miletians, such as Anaximander and Anaximenes, also had a monistic conception of the first cause.
Another school was the Eleatics, Italy. The group was founded in the early fifth century BCE by Parmenides, and included Zeno of Elea and Melissus of Samos. Methodologically, the Eleatics were broadly rationalist, and took logical standards of clarity and necessity to be the criteria of truth. Parmenides' chief doctrine was that reality is a single unchanging and universal Being. Zeno used "reductio ad absurdum", to demonstrate the illusory nature of change and time in his paradoxes.
Heraclitus of Ephesus, in contrast, made change central, teaching that "all things flow". His philosophy, expressed in brief aphorisms, is quite cryptic. For instance, he also taught the unity of opposites.
Democritus and his teacher Leucippus, are known for formulating an atomic theory for the cosmos. They are considered forerunners of the scientific method.
Socrates and Plato.
Socrates is known for his dialectic or questioning approach to philosophy rather than a positive metaphysical doctrine.
His pupil, Plato is famous for his theory of forms (which he confusingly places in the mouth of Socrates in the dialogues he wrote to expound it). Platonic realism (also considered a form of idealism) is considered to be a solution to the problem of universals; i.e., what particular objects have in common is that they share a specific Form which is universal to all others of their respective kind.
The theory has a number of other aspects:
Platonism developed into Neoplatonism, a philosophy with a monotheistic and mystical flavour that survived well into the early Christian era.
Aristotle.
Plato's pupil Aristotle wrote widely on almost every subject, including metaphysics. His solution to the problem of universals contrasts with Plato's. Whereas Platonic Forms exist in a separate realm, and can exist uninstantiated in visible things, Aristotelean essences "indwell" in particulars.
Potentiality and Actuality are principles of a dichotomy which Aristotle used throughout his philosophical works to analyze motion, causality and other issues.
The Aristotelean theory of change and causality stretches to four causes: the material, formal, efficient and final. The efficient cause corresponds to what is now known as a cause "simpliciter". Final causes are explicitly teleological, a concept now regarded as controversial in science. The Matter/Form dichotomy was to become highly influential in later philosophy as the substance/essence distinction.
The opening arguments in Aristotle's "Metaphysics", Book I, revolve around the senses, knowledge, experience, theory, and wisdom. The first main focus in the Metaphysics is attempting to determine how intellect “advances from sensation through memory, experience, and art, to theoretical knowledge”. Aristotle claims that eyesight provides us with the capability to recognize and remember experiences, while sound allows us to learn.
Scholasticism and the Middle Ages.
Between about 1100 and 1500, philosophy as a discipline took place as part of the Catholic church's teaching system, known as scholasticism. Scholastic philosophy took place within an established
framework blending Christian theology with Aristotelean teachings. Although fundamental orthodoxies could not be challenged, there were nonetheless deep metaphysical disagreements, particularly over the problem of universals, which engaged Duns Scotus and Pierre Abelard. William of Ockham is remembered for his principle of ontological parsimony.
Rationalism and Continental Rationalism.
In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, that is the technique of deducing the nature of the world by pure reason. The scholastic concepts of substance and accident were employed.
British empiricism.
British empiricism marked something of a reaction to rationalist and system-building philosophy, or "speculative" metaphysics as it was pejoratively termed. The sceptic David Hume famously declared that most metaphysics should be consigned to the flames (see below). Hume was notorious among his contemporaries as one of the first philosophers to openly doubt religion, but is better known now for his critique of causality. John Stuart Mill, Thomas Reid and John Locke were less sceptical, embracing a more cautious style of metaphysics based on realism, common sense and science. Other philosophers, notably George Berkeley were led from empiricism to idealistic metaphysics.
Kant.
Immanuel Kant attempted a grand synthesis and revision of the trends already mentioned: scholastic philosophy, systematic metaphysics, and skeptical empiricism, not to forget the burgeoning science of his day. Like the systems builders, he had an overarching framework in which all questions were to be addressed. Like Hume, who famously woke him from his 'dogmatic slumbers', he was suspicious of metaphysical speculation, and also places much emphasis on the limitations of the human mind.
Kant saw rationalist philosophers as aiming for a kind of metaphysical knowledge he defined as the "synthetic apriori"—that is knowledge that does not come from the senses (it is a priori) but is nonetheless about reality (synthetic). Inasmuch as it is about reality, it is unlike abstract mathematical propositions (which he terms analytical apriori), and being apriori it is distinct from empirical, scientific knowledge (which he terms synthetic aposteriori). The only synthetic apriori knowledge we can have is of how our minds organise the data of the senses; that organising framework is space and time, which for Kant have no mind-independent existence, but nonetheless operate uniformly in all humans. Apriori knowledge of space and time is all that remains of metaphysics as traditionally conceived. There "is" a reality beyond sensory data or phenomena, which he calls the realm of noumena; however, we cannot know it as it is in itself, but only as it appears to us. He allows himself to speculate that the origins of God, morality, and free will "might" exist in the noumenal realm, but these possibilities have to be set against its basic unknowability for humans. Although he saw himself as having disposed of metaphysics, in a sense, he has generally been regarded in retrospect as having a metaphysics of his own.
19th Century philosophy was overwhelmingly influenced by Kant and his successors. Schopenhauer, Schelling, Fichte and Hegel all purveyed their own panoramic versions of German Idealism, Kant's own caution about metaphysical speculation, and refutation of idealism, having fallen by the wayside. The idealistic impulse continued into the early 20th century with British idealists such as F. H. Bradley and J. M. E. McTaggart.
Followers of Karl Marx took Hegel's dialectic view of history and re-fashioned it as materialism.
Early analytical philosophy and positivism.
During the period when idealism was dominant in philosophy, science had been making great advances. The arrival of a new generation of scientifically minded philosophers led to a sharp decline in the popularity of idealism during the 1920s.
Analytical philosophy was spearheaded by Bertrand Russell and G. E. Moore. Russell and William James tried to compromise between idealism and materialism with the theory of neutral monism.
The early to mid 20th century philosophy also saw a trend to reject metaphysical questions as meaningless. The driving force behind this tendency was the philosophy of Logical Positivism as espoused by the Vienna Circle.
At around the same time, the American pragmatists were steering a middle course between materialism and idealism.
System-building metaphysics, with a fresh inspiration from science, was revived by A. N. Whitehead and Charles Hartshorne.
Continental philosophy.
The forces that shaped analytical philosophy—the break with idealism, and the influence of science — were much less significant outside the English speaking world, although there was a shared turn toward language. Continental philosophy continued in a trajectory from post Kantianism.
The phenomenology of Husserl and others was intended as a collaborative project for the investigation of the features and structure of consciousness common to all humans, in line with Kant's basing his synthetic apriori on the uniform operation of consciousness. It was officially neutral with regards to ontology, but was nonetheless to spawn a number of metaphysical systems. Brentano's concept of intentionality would become widely influential, including on analytical philosophy.
Heidegger, author of Being and Time, saw himself as re-focusing on Being-qua-being, introducing the novel concept of "Dasein" in the process. Classing himself an existentialist, Sartre wrote an extensive study of "Being and Nothingness".
The speculative realism movement marks a return to full blooded realism.
Process Metaphysics.
There are two fundamental aspects of everyday experience: change and persistence. Until recently, the Western philosophical tradition has arguably championed substance and persistence, with some notable exceptions however. According to process thinkers, novelty, flux and accident do matter, and sometimes they constitute the ultimate reality.
Lato sensu, process metaphysics is as old as Western philosophy, with figures such as Heraclitus, Plotinus, Duns Scotus, Leibniz, David Hume, Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph von Schelling, Gustav Theodor Fechner, Friedrich Adolf Trendelenburg, Charles Renouvier, Karl Marx, Ernst Mach, Friedrich Wilhelm Nietzsche, Émile Boutroux, Henri Bergson, Samuel Alexander and Nicolas Berdyaev. It seemingly remains an open question whether major “Continental” figures like the late Martin Heidegger, Maurice Merleau-Ponty, Gilles Deleuze, Michel Foucault, or Jacques Derrida should be included.
Stricto sensu, process metaphysics can be limited to the works of a few founding fathers : G. W. F. Hegel, Charles Sanders Peirce, William James, Henri Bergson, A. N. Whitehead and John Dewey. From a European perspective, there was a very significant and early Whiteheadian influence on the works of outstanding scholars like Émile Meyerson (1859–1933), Louis Couturat (1868–1914), Jean Wahl (1888–1974), Robin George Collingwood (1889–1943), Philippe Devaux (1902–1979), Hans Jonas (1903–1993), Dorothy M. Emmett (1904–2000), Maurice Merleau Ponty (1908–1961), Enzo Paci (1911–1976), Charlie Dunbar Broad (1887–1971), Wolfe Mays (1912–), Ilya Prigogine (1917–2003), Jules Vuillemin (1920–2001), Jean Ladrière (1921–), Gilles Deleuze (1925–1995), Wolfhart Pannenberg (1928–) and Reiner Wiehl (1929–2010).
Later analytical philosophy.
While early analytic philosophy tended to reject metaphysical theorizing, under the influence of logical positivism, it was revived in the second half of the twentieth century. Philosophers such as David K. Lewis and David Armstrong developed elaborate theories on a range of topics such as universals, causation, possibility and necessity and abstract objects. However, the focus of analytical philosophy is generally away from the construction of all-encompassing systems and towards close analysis of individual ideas.
Among the developments that led to the revival of metaphysical theorizing were Quine's attack on the analytic-synthetic distinction, which was generally taken to undermine Carnap's distinction between existence questions internal to a framework and those external to it.
The philosophy of fiction, the problem of empty names, and the debate over existence's status as a property have all risen out of relative obscurity to become central concerns, while perennial issues such as free will, possible worlds, and the philosophy of time have had new life breathed into them.
Rejections of metaphysics.
A number of individuals have suggested that much of metaphysics should be rejected. In the 18th century, David Hume took an extreme position, arguing that all genuine knowledge involves either mathematics or matters of fact and that metaphysics, which goes beyond these, is worthless. He concludes his "Enquiry Concerning Human Understanding" with the statement:
In the 1930s, A. J. Ayer and Rudolf Carnap endorsed Hume's position; Carnap quoted the passage above. They argued that metaphysical statements are neither true nor false but meaningless since, according to their verifiability theory of meaning, a statement is meaningful only if there can be empirical evidence for or against it. Thus, while Ayer rejected the monism of Spinoza, noted above, he avoided a commitment to pluralism, the contrary position, by holding both views to be without meaning. Carnap took a similar line with the controversy over the reality of the external world.
33 years after Hume's "Enquiry" appeared, Immanuel Kant published his "Critique of Pure Reason". Though he followed Hume in rejecting much of previous metaphysics, he argued that there was still room for some "synthetic a priori" knowledge, concerned with matters of fact yet obtainable independent of experience. These included fundamental structures of space, time, and causality. He also argued for the freedom of the will and the existence of "things in themselves", the ultimate (but unknowable) objects of experience.
The logical atomist Ludwig Wittgenstein introduced the concept that metaphysics could be influenced by theories of Aesthetics, via Logic, vis. a world composed of "atomical facts".
Arguing against such rejections, the Scholastic philosopher Edward Feser has observed that Hume’s critique of metaphysics, and specifically Hume's fork, is “notoriously self-refuting”. Feser argues that Hume’s fork itself is not a conceptual truth and is not empirically testable.
Metaphysics in science.
Much recent work has been devoted to analyzing the role of metaphysics in scientific theorizing. Alexandre Koyré led this movement, declaring in his book "Metaphysics and Measurement", "It is not by following experiment, but by outstripping experiment, that the scientific mind makes progress." Imre Lakatos maintained that all scientific theories have a metaphysical "hard core" essential for the generation of hypotheses and theoretical assumptions. Thus, according to Lakatos, "scientific changes are connected with vast cataclysmic metaphysical revolutions."
An example from biology of Lakatos' thesis: David Hull has argued that changes in the ontological status of the species concept have been central in the development of biological thought from Aristotle through Cuvier, Lamarck, and Darwin. Darwin's ignorance of metaphysics made it more difficult for him to respond to his critics because he could not readily grasp the ways in which their underlying metaphysical views differed from his own.
In physics, new metaphysical ideas have arisen in connection with quantum mechanics, where subatomic particles arguably do not have the same sort of individuality as the particulars with which philosophy has traditionally been concerned. Also, adherence to a deterministic metaphysics in the face of the challenge posed by the quantum-mechanical uncertainty principle led physicists like Albert Einstein to propose alternative theories that retained determinism. A. N. Whitehead is famous for creating a metaphysics inspired by electromagnetism and special relativity.
In chemistry, Gilbert Newton Lewis addressed the nature of motion, arguing that an electron should not be said to move when it has none of the properties of motion.
Katherine Hawley notes that the metaphysics even of a widely accepted scientific theory may be challenged if it can be argued that the metaphysical presuppositions of the theory make no contribution to its predictive success.

</doc>
<doc id="18896" url="http://en.wikipedia.org/wiki?curid=18896" title="Human spaceflight">
Human spaceflight

Human spaceflight (also referred to as manned spaceflight) is space travel with a crew aboard the spacecraft. When a spacecraft is crewed, it can be operated directly, as opposed to being remotely operated or autonomous.
The first human spaceflight was launched by the Soviet Union on 12 April 1961 as a part of the Vostok program, with cosmonaut Yuri Gagarin aboard. Humans have been continually present in space for on the International Space Station.
Since the retirement of the US Space Shuttle in 2011, only Russia and China have maintained domestic human spaceflight capability with the Soyuz program and Shenzhou program. Currently, all crewed flights to the International Space Station use Soyuz vehicles, which remain attached to the station to allow quick return if needed. The United States is developing commercial crew transportation to facilitate domestic access to ISS and low Earth orbit, as well as the Orion vehicle for beyond-low Earth orbit applications.
While spaceflight has typically been a government-directed activity, commercial spaceflight has gradually been taking on a greater role. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight, and a number of non-governmental companies have been working to develop a space tourism industry. NASA has also played a role to stimulate private spaceflight through programs such as Commercial Orbital Transportation Services (COTS) and Commercial Crew Development (CCDev). With its 2011 budget proposals released in 2010, the Obama administration moved towards a model where commercial companies would supply NASA with transportation services of both crew and cargo to low Earth orbit. The vehicles used for these services could then serve both NASA and potential commercial customers. Commercial resupply of ISS began two years after the retirement of the Shuttle, and commercial crew launches could begin by 2017.
History.
First human spaceflights.
The first human spaceflight took place on 12 April 1961, when cosmonaut Yuri Gagarin made one orbit around the Earth aboard the Vostok 1 spacecraft, launched by the Soviet space program. Valentina Tereshkova became the first woman in space aboard Vostok 6 on 16 June 1963. Both spacecraft were launched by Vostok 3KA launch vehicles. Alexei Leonov made the first spacewalk when he left Voskhod 2 on 8 March 1965. Svetlana Savitskaya became the first woman to do so on 25 July 1984.
The United States became the second nation to put a human in space with the suborbital flight of astronaut Alan Shepard aboard "Freedom 7" as part of Project Mercury. The spacecraft was launched on 5 May 1961 on a Redstone rocket. The first U.S. orbital flight was that of John Glenn aboard "Friendship 7", launched 20 February 1962 on an Atlas rocket. From 1981 to 2011, the U.S. conducted all its human spaceflight missions with reusable space shuttles. Sally Ride became the first American woman in space in 1983. Eileen Collins was the first female shuttle pilot, and with shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.
China became the third nation to achieve independent human spaceflight capability when Yang Liwei launched into space on a Chinese-made vehicle, the Shenzhou 5, on 15 October 2003. The first Chinese woman, Liu Yang, was launched in June 2012 aboard Shenzhou 9. Previous European (Hermes) and Japanese (HOPE-X) domestic human spaceflight programs were abandoned after years of development, as was the first Chinese attempt, the Shuguang spacecraft.
The farthest destination for a human spaceflight mission has been the Moon. The only manned missions to the Moon have been those conducted by NASA as part of the Apollo program. The first such mission, Apollo 8, orbited the Moon but did not land. The first Moon landing mission was Apollo 11, during which—on 20 July 1969—Neil Armstrong and Buzz Aldrin became the first people to set foot on the Moon. Six missions landed in total, numbered Apollo 11–17, excluding Apollo 13. Altogether 12 men walked on the Moon, the only humans to have been on an extraterrestrial body. The Soviet Union discontinued its program for lunar orbiting and landing of human spaceflight missions in 1974 when Valentin Glushko became General Designer of NPO Energiya.
The longest single human spaceflight is that of Valeriy Polyakov, who left Earth on 8 January 1994, and did not return until 22 March 1995 (a total of 437 days 17 h 58 min 16 s). Sergei Krikalyov has spent the most time of anyone in space, 803 days, 9 hours, and 39 minutes altogether. The longest period of continuous human presence in space is on the International Space Station, exceeding the previous record of almost 10 years (or 3,634 days) held by Mir, spanning the launch of Soyuz TM-8 on 5 September 1989 to the landing of Soyuz TM-29 on 28 August 1999.
For many years beginning in 1961, only two countries, the USSR (later Russia) and the United States, had their own astronauts. Citizens of other nations flew in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on 2 March 1978. s of 2010[ [update]], citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.
Post-shuttle gap in United States human spaceflight capability.
Under the Bush administration, the Constellation Program included plans for retiring the Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration cancelled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. For beyond low earth orbit human spaceflight NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA will rely on transportation services provided by the private sector to reach low earth orbit, such as Space X's Falcon 9/Dragon V2, Sierra Nevada Corporation's Dream Chaser, or Boeing's CST-100. The period between the retirement of the shuttle in 2011 and the initial operational capability of new systems in 2017, similar to the gap between the end of Apollo in 1975 and the first space shuttle flight in 1981, is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap. Commercial sub-orbital spacecraft aimed at the space tourism market such as Scaled Composites SpaceshipTwo to be operated by Virgin Galactic, and XCOR's Lynx spaceplane are under development and could reach space before 2017.
Space programs.
Human spaceflight programs have been conducted by the former Soviet Union/Russian Federation, the United States, the People's Republic of China and by private spaceflight company Scaled Composites.
The Indian Space Research Organization (ISRO) begun work on pre project activities of human space flight mission programme. The objective of Human Spaceflight Programme is to undertake a human spaceflight mission to carry a crew of two to Low Earth Orbit (LEO) and return them safely to a predefined destination on earth. The programme is proposed to be implemented in defined phases. Currently, the pre project activities are progressing with a focus on the development of critical technologies for subsystems such as Crew Module (CM), Environmental control and Life Support System (ECLSS), Crew Escape System, etc. A study for undertaking human space flight to carry human beings to low earth orbit and ensure their safe return has been made by the department. The department has initiated pre-project activities to study technical and managerial issues related to undertaking manned mission with an aim to build and demonstrate the country’s capability. The programme envisages the development of a fully autonomous orbital vehicle carrying 2 or 3 crew members to about 300 km low earth orbit and their safe return.
Several other countries and space agencies have announced and begun human spaceflight programs by their own technology, Japan (JAXA), Iran (ISA) and Malaysia (MNSA).
Currently the following spacecraft and spaceports are used for launching human spaceflights:
Historically, the following spacecraft and spaceports have also been used for human spaceflight launches:
Numerous private companies attempted human spaceflight programs in an effort to win the $10 million Ansari X Prize. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight. SpaceShipOne captured the prize on 4 October 2004, when it accomplished two consecutive flights within one week. SpaceShipTwo, launching from the carrier aircraft White Knight Two, is planned to conduct regular suborbital space tourism.
Most of the time, the only humans in space are those aboard the ISS, whose crew of six spends up to six months at a time in low Earth orbit.
NASA and ESA use the term "human spaceflight" to refer to their programs of launching people into space. These endeavors have also been referred to as "manned space missions."
Safety concerns.
Planners of human spaceflight missions face a number of safety concerns.
Life support.
The immediate needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.
Medical issues.
Medical consequences such as possible blindness and bone loss have been associated with human space flight.
On 31 December 2012, a NASA-supported study reported that spaceflight may harm the brain of astronauts and accelerate the onset of Alzheimer's disease.
Effects of microgravity.
Medical data from astronauts in low earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts’ performance or increase their risk of injury.
In a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up. Those muscles then start to weaken and eventually get smaller. If there is an emergency at landing, the loss of muscles, and consequently the loss of strength can be a serious problem. Sometimes, astronauts can lose up to 25% of their muscle mass on long term flights. When they get back to ground, they will be considerably weakened and will be out of action for a while.
Astronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, or any other mass with gravity, they have to readjust to the gravity and may have problems standing up, focusing their gaze, walking and turning. Importantly, those body motor disturbances after changing from different gravities only get worse the longer the exposure to little gravity. These changes will affect operational activities including approach and landing, docking, remote manipulation, and emergencies that may happen while landing. This can be a major roadblock to mission success.
In addition, after long space flight missions, male astronauts may experience severe eyesight problems. Such eyesight problems may be a major concern for future deep space flight missions, including a manned mission to the planet Mars.
Radiation.
Without proper shielding, the crews of missions beyond low Earth orbit (LEO) might be at risk from high-energy protons emitted by solar flares. Lawrence Townsend of the University of Tennessee and others have studied the most powerful solar flare ever recorded. That flare was seen by the British astronomer Richard Carrington in September 1859. Radiation doses astronauts would receive from a Carrington-type flare could cause acute radiation sickness and possibly even death.
Another type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond LEO.
Radiation damage to the immune system.
There is also some scientific concern that extended spaceflight might slow down the body’s ability to protect itself against diseases. Some of the problems are a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short and long term consequences to the bone marrow stem cells which create the blood and immune systems. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.
Isolation.
During long missions, astronauts are isolated and confined into small spaces. Depression, cabin fever and other psychological problems may impact the crew's safety and mission success.
Astronauts may not be able to quickly return to Earth or receive medical supplies, equipment or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on their limited existing resources and medical advice from the ground.
Fatality risk.
s of 2010[ [update]], 18 crew members have died during actual spaceflight missions (see table). Over 100 others have died in accidents during activity directly related to spaceflight missions or testing.

</doc>
<doc id="18899" url="http://en.wikipedia.org/wiki?curid=18899" title="Mendelevium">
Mendelevium

Mendelevium is a synthetic element with chemical symbol Md (formerly Mv) and atomic number 101. A metallic radioactive transuranic element in the actinide series, it is the first element that currently cannot be produced in macroscopic quantities through neutron bombardment of lighter elements. It is the antepenultimate actinide and the ninth transuranic element. It can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of sixteen mendelevium isotopes are known, the most stable being 258Md with a half-life of 51 days; nevertheless, the shorter-lived 256Md (half-life 1.27 hours) is most commonly used in chemistry because it can be produced on a larger scale.
Mendelevium was discovered by bombarding einsteinium with alpha particles in 1955, the same method still used to produce it today. It was named after Dmitri Mendeleev, father of the periodic table of the chemical elements. Using available microgram quantities of the isotope einsteinium-253, over a million mendelevium atoms may be produced each hour. The chemistry of mendelevium is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced mendelevium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside of basic scientific research.
Discovery.
Mendelevium was the ninth transuranic element to be synthesized. It was first synthesized by Albert Ghiorso, Glenn T. Seaborg, Gregory R. Choppin, Bernard G. Harvey, and team leader Stanley G. Thompson in early 1955 at the University of California, Berkeley. The team produced 256Md (half-life of 87 minutes) when they bombarded an 253Es target consisting of only a billion (109) einsteinium atoms with alpha particles (helium nuclei) in the Berkeley Radiation Laboratory's 60-inch cyclotron, thus increasing the target's atomic number by two. 256Md thus became the first isotope of any element to be synthesized one atom at a time. In total, seventeen mendelevium atoms were produced. This discovery was part of a program, begun in 1952, that irradiated plutonium with neutrons to transmute it into heavier actinides. This method was necessary as the previous method used to synthesize transuranic elements, neutron capture, could not work because of a lack of beta decaying isotopes of fermium that would produce isotopes of the next element, mendelevium, and also due to the very short half-life to spontaneous fission of fermium-258 that thus constituted a hard limit to the success of the neutron capture process.
To predict if the production of mendelevium would be possible, the team made use of a rough calculation. The number of atoms that would be produced would be approximately equal to the product of the number of atoms of target material, the target's cross section, the ion beam intensity, and the time of bombardment; this last factor was related to the half-life of the product when bombarding for a time on the order of its half-life. This gave one atom per experiment. Thus under optimum conditions, the preparation of only one atom of element 101 per experiment could be expected. This calculation demonstrated that it was feasible to go ahead with the experiment. The target material, einsteinium-253, could be produced readily from irradiating plutonium: one year of irradiation would give a billion atoms, and its three-week half-life meant that the element 101 experiments could be conducted in one week after the produced einsteinium was separated and purified to make the target. However, it was necessary to upgrade the cyclotron to obtain the needed intensity of 1014 alpha particles per second; Seaborg applied for the necessary funds.
While Seaborg applied for funding, Harvey worked on the einsteinium target, while Thomson and Choppin focused on methods for chemical isolation. Choppin suggested using α-hydroxyisobutyric acid to separate the mendelevium atoms from those of the lighter actinides. The actual synthesis was done by a recoil technique, introduced by Albert Ghiorso. In this technique, the einsteinium was placed on the opposite side of the target from the beam, so that the recoiling mendelevium atoms would get enough momentum to leave the target and be caught on a catcher foil made of gold. This recoil target was made by an electroplating technique, developed by Alfred Chetham-Strode. This technique gave a very high yield, which was absolutely necessary when working with such a rare and valuable product as the einsteinium target material. The recoil target consisted of 109 atoms of 253Es which were deposited electrolytically on a thin gold foil. It was bombarded by 41 MeV alpha particles in the Berkeley cyclotron with a very high beam density of 6×1013 particles per second over an area of 0.05 cm2. The target was cooled by water or liquid helium, and the foil could be replaced.
Initial experiments were carried out in September 1954. No alpha decay was seen from mendelevium atoms; thus, Ghiorso suggested that the mendelevium had all decayed by electron capture to fermium and that the experiment should be repeated to search instead for spontaneous fission events. The repetition of the experiment happened in February 1955.
On the day of discovery, 19 February, alpha irradiation of the einsteinium target occurred in three three-hour sessions. The cyclotron was in the University of California campus, while the Radiation Laboratory was on the next hill. To deal with this situation, a complex procedure was used: Ghiorso took the catcher foils (there were three targets and three foils) from the cyclotron to Harvey, who would use aqua regia to dissolve it and pass it through an anion-exchange resin column to separate out the transuranium elements from the gold and other products. The resultant drops entered a test tube, which Choppin and Ghiorso took in a car to get to the Radiation Laboratory as soon as possible. There Thompson and Choppin used a cation-exchange resin column and the α-hydroxyisobutyric acid. The solution drops were collected on platinum disks and dried under heat lamps. The three disks were expected to contain respectively the fermium, no new elements, and the mendelevium. Finally, they were placed in their own counters, which were connected to recorders such that spontaneous fission events would be recorded as huge deflections in a graph showing the number and time of the decays. There thus was no direct detection, but by observation of spontaneous fission events arising from its electron-capture daughter 256Fm. The first one was identified with a "hooray" followed by a "double hooray" and a "triple hooray". The fourth one eventually officially proved the chemical identification of the 101st element, mendelevium. In total, five decays were reported up till 4 a.m. Seaborg was notified and the team left to sleep. Additional analysis and further experimentation showed the produced mendelevium isotope to have mass 256 and to decay by electron capture to fermium-256 with a half-life of 1.5 h.
 We thought it fitting that there be an element named for the Russian chemist Dmitri Mendeleev, who had developed the periodic table. In nearly all our experiments discovering transuranium elements, we'd depended on his method of predicting chemical properties based on the element's position in the table. But in the middle of the Cold War, naming an element for a Russian was a somewhat bold gesture that did not sit well with some American critics.
 — Glenn T. Seaborg
Being the first of the second hundred of the chemical elements, it was decided that the element would be named "mendelevium" after the Russian chemist Dmitri Mendeleev, father of the periodic table. Due to the fact that this discovery came during the Cold War, Seaborg had to request permission of the government of the United States to propose that the element be named for a Russian, but it was granted. The name "mendelevium" was accepted by the International Union of Pure and Applied Chemistry (IUPAC) in 1955 with symbol "Mv", which was changed to "Md" in the next IUPAC General Assembly (Paris, 1957).
Characteristics.
Physical.
In the periodic table, mendelevium is located to the right of the actinide fermium, to the left of the actinide nobelium, and below the lanthanide thulium. Mendelevium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.
The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have f"n"d1s2 configurations, whereas the latter have f"n"+1s2 configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f126d17s2 configuration over the [Rn]5f137s2 configuration for mendelevium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number. Thermochromatographic studies with trace quantities of mendelevium by Zvara and Hübener from 1976 to 1982 confirmed this prediction. In 1990, Haire and Gibson estimated mendelevium metal to have an enthalpy of sublimation between 134 and 142 kJ·mol−1. Divalent mendelevium metal should have a metallic radius of around (194 ± 10) pm. Mendelevium's melting point has been estimated at 827 °C, the same value as that predicted for the neighboring element nobelium.
Chemical.
The chemistry of mendelevium is mostly known only in solution, in which it can take on the +3 or +2 oxidation states. The +1 state has also been reported, but has not yet been confirmed.
Before mendelevium's discovery, Seaborg and Katz predicted that it should be predominantly trivalent in aqueous solution and hence should behave similarly to other tripositive lanthanides and actinides. After the synthesis of mendelevium in 1955, these predictions were confirmed, first in the observation at its discovery that it eluted just after fermium in the trivalent actinide elution sequence from a cation-exchange column of resin, and later the 1967 observation that mendelevium could form insoluble hydroxides and fluorides that coprecipitated with trivalent lanthanide salts. Cation-exchange and solvent extraction studies led to the conclusion that mendelevium was a trivalent actinide with an ionic radius somewhat smaller than that of the previous actinide, fermium. Mendelevium can form coordination complexes with 1,2-cyclohexanedinitrilotetraacetic acid (DCTA).
In reducing conditions, mendelevium(III) can be easily reduced to mendelevium(II), which is stable in aqueous solution. The standard reduction potential of the "E"°(Md3+→Md2+) couple has been variously estimated as −0.10 V or −0.20 V. In comparison, "E"°(Md3+→Md0) should be around −1.74 V, and "E"°(Md2+→Md0) should be around −2.5 V. Mendelevium(II)'s elution behavior has been compared with that of strontium(II) and europium(II).
In 1973, mendelevium(I) was reported to have been produced by Russian scientists, who obtained it by reducing higher oxidation states of mendelevium with samarium(II). It was found to be stable in neutral water–ethanol solution and be homologous to caesium(I). However, later experiments found no evidence for mendelevium(I) and found that mendelevium behaved like divalent elements when reduced, not like the monovalent alkali metals. Nevertheless, the Russian team conducted further studies on the thermodynamics of cocrystallizing mendelevium with alkali metal chlorides, and concluded that mendelevium(I) had formed and could form mixed crystals with divalent elements, thus cocrystallizing with them. The status of the +1 oxidation state is still tentative.
Although "E"°(Md4+→Md3+) was predicted in 1975 to be +5.4 V, suggesting that mendelevium(III) could be easily oxidized to mendelevium(IV), 1967 experiments with the strong oxidizing agent sodium bismuthate were unable to oxidize mendelevium(III) to mendelevium(IV).
Atomic.
A mendelevium atom has 101 electrons, of which at least three (and perhaps four) can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f137s2 (ground state term symbol 2F7/2), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, three valence electrons may be lost, leaving behind a [Rn]5f12 core: this conforms to the trend set by the other actinides with their [Rn] 5f"n" electron configurations in the tripositive state. The first ionization potential of mendelevium was measured to be at most (6.58 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to mendelevium's scarcity and high radioactivity. The ionic radius of hexacoordinate Md3+ had been preliminarily estimated in 1978 to be around 91.2 pm; 1988 calculations based on the logarithmic trend between distribution coefficients and ionic radius produced a value of 89.6 pm, as well as an enthalpy of hydration of −(3654 ± 12) kJ·mol−1. Md2+ should have an ionic radius of 115 pm and hydration enthalpy −1413 kJ·mol−1; Md+ should have ionic radius 117 pm.
Isotopes.
Sixteen isotopes of mendelevium are known, with mass numbers from 245 to 260; all are radioactive. Additionally, five nuclear isomers are known: 245mMd, 247mMd, 249mMd, 254mMd, and 258mMd. Of these, the longest-lived isotope is 258Md with a half-life of 51.5 days, and the longest-lived isomer is 258mMd with a half-life of 58.0 minutes. Nevertheless, the slightly shorter-lived 256Md (half-life 1.27 hours) is more often used in chemical experimentation because it can be produced in larger quantities from alpha particle irradiation of einsteinium. After 258Md, the next most stable mendelevium isotopes are 260Md with a half-life of 31.8 days, 257Md with a half-life of 5.52 hours, 259Md with a half-life of 1.60 hours, and 256Md with a half-life of 1.27 hours. All of the remaining mendelevium isotopes have half-lives that are less than an hour, and the majority of these have half-lives that are less than 5 minutes.
The half-lives of mendelevium isotopes mostly increase smoothly from 245Md onwards, reaching a maximum at 258Md. Experiments and predictions suggest that the half-lives will then decrease, apart from 260Md with a half-life of 31.8 days, as spontaneous fission becomes the dominant decay mode due to the mutual repulsion of the protons posing a limit to the island of relative stability of long-lived nuclei in the actinide series.
Mendelevium-256, the chemically most important isotope of mendelevium, decays through electron capture 90.7% of the time and alpha decay 9.9% of the time. It is most easily detected through the spontaneous fission of its electron-capture daughter fermium-256, but in the presence of other nuclides that undergo spontaneous fission, alpha decays at the characteristic energies for mendelevium-256 (7.205 and 7.139 MeV) can provide more useful identification.
Production and isolation.
The lightest mendelevium isotopes (245Md to 247Md) are mostly produced through bombardment of bismuth targets with heavy argon ions, while slightly heavier ones (248Md to 253Md) are produced by bombarding plutonium and americium targets with lighter ions of carbon and nitrogen. The most important and most stable isotopes are in the range from 254Md to 258Md and are produced through bombardment of einsteinium isotopes with alpha particles: einsteinium-253, -254, and -255 can all be used. 259Md is produced as a daughter of 259No, and 260Md can be produced in a transfer reaction between einsteinium-254 and oxygen-18. Typically, the most commonly used isotope 256Md is produced by bombarding either einsteinium-253 or -254 with alpha particles: einsteinium-254 is preferred when available because it has a longer half-life and therefore can be used as a target for longer. Using available microgram quantities of einsteinium, femtogram quantities of mendelevium-256 may be produced.
The recoil momentum of the produced mendelevium-256 atoms is used to bring them physically far away from the einsteinium target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum. This eliminates the need for immediate chemical separation, which is both costly and prevents reusing of the expensive einsteinium target. The mendelevium atoms are then trapped in a gas atmosphere (frequently helium), and a gas jet from a small opening in the reaction chamber carries the mendelevium along. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the mendelevium atoms can be transported over tens of meters to be chemically analyzed and have their quantity determined. The mendelevium can then be separated from the foil material and other fission products by applying acid to the foil and then coprecipitating the mendelevium with lanthanum fluoride, then using a cation-exchange resin column with a 10% ethanol solution saturated with hydrochloric acid, acting as an eluant. However, if the foil is made of gold and thin enough, it is enough to simply dissolve the gold in aqua regia before separating the trivalent actinides from the gold using anion-exchange chromatography, the eluant being 6 M hydrochloric acid.
Mendelevium can finally be separated from the other trivalent actinides using selective elution from a cation-exchange resin column, the eluant being ammonia α-HIB. Using the gas-jet method often renders the first two steps unnecessary. The above procedure is the most commonly used one for the separation of transeinsteinium elements.
Another possible way to separate the trivalent actinides is via solvent extraction chromatography using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column, so that the heavier actinides elute later. The mendelevium separated by this method has the advantage of being free of organic complexing agent compared to the resin column; the disadvantage is that mendelevium then elutes very late in the elution sequence, after fermium.
Another method to isolate mendelevium exploits the distinct elution properties of Md2+ from those of Es3+ and Fm3+. The initial steps are the same as above, and employs HDEHP for extraction chromatography, but coprecipitates the mendelevium with terbium fluoride instead of lanthanum fluoride. Then, 50 mg of chromium is added to the mendelevium to reduce it to the +2 state in 0.1 M hydrochloric acid with zinc or mercury. The solvent extraction then proceeds, and while the trivalent and tetravalent lanthanides and actinides remain on the column, mendelevium(II) does not and stays in the hydrochloric acid. It is then reoxidized to the +3 state using hydrogen peroxide and then isolated by selective elution with 2 M hydrochloric acid (to remove impurities, including chromium) and finally 6 M hydrochloric acid (to remove the mendelevium). It is also possible to use a column of cationite and zinc amalgam, using 1 M hydrochloric acid as an eluant, reducing Md(III) to Md(II) where it behaves like the alkaline earth metals. Thermochromatographic chemical isolation could be achieved using the volatile mendelevium hexafluoroacetylacetonate: the analogous fermium compound is also known and is also volatile.

</doc>
<doc id="18900" url="http://en.wikipedia.org/wiki?curid=18900" title="Modus ponens">
Modus ponens

In propositional logic, modus ponendo ponens (Latin for "the way that affirms by affirming"; often abbreviated to MP or modus ponens) or implication elimination is a valid, simple argument form and rule of inference. It can be summarized as ""P" implies "Q"; "P" is asserted to be true, so therefore "Q" must be true." The history of "modus ponens" goes back to antiquity.
While "modus ponens" is one of the most commonly used concepts in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the "rule of definition" and the "rule of substitution". "Modus ponens" allows one to eliminate a conditional statement from a logical proof or argument (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the rule of detachment. Enderton, for example, observes that "modus ponens can produce shorter formulas from longer ones", and Russell observes that "the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q [the consequent] . . . an inference is the dropping of a true premise; it is the dissolution of an implication".
A justification for the "trust in inference is the belief that if the two former assertions [the antecedents] are not in error, the final assertion [the consequent] is not in error". In other words: if one statement or proposition implies a second one, and the first statement or proposition is true, then the second one is also true. If "P" implies "Q" and "P" is true, then "Q" is true. An example is:
"Modus ponens" can be stated formally as:
where the rule is that whenever an instance of ""P" → "Q" and "P"" appear by themselves on lines of a logical proof, "Q" can validly be placed on a subsequent line; furthermore, the premise "P" and the implication "dissolves", their only trace being the symbol "Q" that is retained for use later e.g. in a more complex deduction.
It is closely related to another valid form of argument, "modus tollens". Both have apparently similar but invalid forms such as affirming the consequent, denying the antecedent, and evidence of absence. Constructive dilemma is the disjunctive version of modus ponens. Hypothetical syllogism is closely related to modus ponens and sometimes thought of as "double modus ponens."
Formal notation.
The "modus ponens" rule may be written in sequent notation:
where ⊢ is a metalogical symbol meaning that "Q" is a syntactic consequence of "P" → "Q" and "P" in some logical system;
or as the statement of a truth-functional tautology or theorem of propositional logic:
where "P", and "Q" are propositions expressed in some formal system.
Explanation.
The argument form has two premises (hypothesis). The first premise is the "if–then" or conditional claim, namely that "P" implies "Q". The second premise is that "P", the antecedent of the conditional claim, is true. From these two premises it can be logically concluded that "Q", the consequent of the conditional claim, must be true as well. In artificial intelligence, "modus ponens" is often called forward chaining.
An example of an argument that fits the form "modus ponens":
This argument is valid, but this has no bearing on whether any of the statements in the argument are true; for "modus ponens" to be a sound argument, the premises must be true for any true instances of the conclusion. An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid "and" all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is not only sound on Tuesdays (when John goes to work), but valid on every day of the week. A propositional argument using "modus ponens" is said to be deductive.
In single-conclusion sequent calculi, "modus ponens" is the Cut rule. The cut-elimination theorem for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is admissible.
The Curry–Howard correspondence between proofs and programs relates "modus ponens" to function application: if "f" is a function of type "P" → "Q" and "x" is of type "P", then "f x" is of type "Q".
Justification via truth table.
The validity of "modus ponens" in classical two-valued logic can be clearly demonstrated by use of a truth table.
<br>
In instances of "modus ponens" we assume as premises that "p" → "q" is true and "p" is true. Only one line of the truth table—the first—satisfies these two conditions ("p" and "p" → "q"). On this line, "q" is also true. Therefore, whenever "p" → "q" is true and "p" is true, "q" must also be true.

</doc>
<doc id="18901" url="http://en.wikipedia.org/wiki?curid=18901" title="Modus tollens">
Modus tollens

In propositional logic, modus tollens (or modus tollendo tollens and also denying the consequent) (Latin for "the way that denies by denying") is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contra-positive.
The first to explicitly state the argument form "modus tollens" were the Stoics.
The inference rule "modus tollens", also known as the law of contrapositive, validates the inference from formula_1 implies formula_2 and the contradictory of formula_2, to the contradictory of formula_1.
The "modus tollens" rule can be stated formally as:
where formula_6 stands for the statement "P implies Q" (and formula_7 is called the "contrapositive"). formula_8 stands for "it is not the case that Q" (or in brief "not Q"). Then, whenever "formula_6" and "formula_10" each appear by themselves as a line of a proof, then "formula_11" can validly be placed on a subsequent line. The history of the inference rule "modus tollens" goes back to antiquity.
"Modus tollens" is closely related to "modus ponens". There are two similar, but invalid, forms of argument: affirming the consequent and denying the antecedent. See also contraposition and proof by contraposition.
Formal notation.
The "modus tollens" rule may be written in sequent notation:
where formula_13 is a metalogical symbol meaning that formula_11 is a syntactic consequence of formula_6 and formula_10 in some logical system;
or as the statement of a functional tautology or theorem of propositional logic:
where formula_1 and formula_2 are propositions expressed in some formal system;
or including assumptions:
though since the rule does not change the set of assumptions, this is not strictly necessary.
More complex rewritings involving "modus tollens" are often seen, for instance in set theory:
Also in first-order predicate logic:
Strictly speaking these are not instances of "modus tollens", but they may be derived using "modus tollens" using a few extra steps.
Explanation.
The argument has two premises. The first premise is a conditional or "if-then" statement, for example that if P then Q. The second premise is that it is not the case that Q . From these two premises, it can be logically concluded that it is not the case that P.
Consider an example:
Supposing that the premises are both true (the dog will bark if it detects an intruder, and does indeed not bark), it follows that no intruder has been detected. This is a valid argument since it is not possible for the conclusion to be false if the premises are true. (It is conceivable that there may have been an intruder that the dog did not detect, but that does not invalidate the argument; the first premise is "if the watch-dog detects an intruder." The thing of importance is that the dog detects or doesn't detect an intruder, not if there is one.)
Another example:
Relation to "modus ponens".
Every use of "modus tollens" can be converted to a use of "modus ponens" and one use of transposition to the premise which is a material implication. For example:
Likewise, every use of "modus ponens" can be converted to a use of "modus tollens" and transposition.
Justification via truth table.
The validity of "modus tollens" can be clearly demonstrated through a truth table.
In instances of "modus tollens" we assume as premises that p → q is true and q is false. There is only one line of the truth table—the fourth line—which satisfies these two conditions. In this line, p is false. Therefore, in every instance in which p → q is true and q is false, p must also be false.

</doc>
<doc id="18902" url="http://en.wikipedia.org/wiki?curid=18902" title="Mathematician">
Mathematician

A mathematician is someone who uses an extensive knowledge of mathematics in his or her work, typically to solve mathematical problems. Mathematics is concerned with numbers, data, quantity, structure, space, models and change.
History.
One of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.
The number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).
Science and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on Optics, Math and Astronomy of Ibn al-Haytham.
The Renaissance brought an increased emphasis on mathematics and science to Europe. Many well known mathematicians had other occupations: Luca Pacioli (founder of accounting); Galileo Galilei; Niccolò Fontana Tartaglia (notable engineer and bookkeeper); and Gerolamo Cardano (one of the founders of probability).
As time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had notably already begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the fathers of British scientific methodology Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag[ing] productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve. 
British universities of this period adopted some approaches familiar to the German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt’s idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”
Notable mathematicians.
Some notable mathematicians include Baudhayana, Johann Bernoulli, Jacob Bernoulli, Aryabhata, Bhāskara II, Nilakantha Somayaji, Andrey Kolmogorov, Alexander Grothendieck, John von Neumann, Alan Turing, Kurt Gödel, Augustin-Louis Cauchy, Georg Cantor, William Rowan Hamilton, Carl Jacobi, Nikolai Lobachevsky, Joseph Fourier, Pierre-Simon Laplace, Alonzo Church, and Nikolay Bogolyubov.
Required education.
Mathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate-level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.
Activities.
Applied mathematics.
Mathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers. 
The discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, "applied mathematicians" look into the "formulation, study, and use of mathematical models" in science, engineering, business, and other areas of mathematical practice.
Abstract mathematics.
Pure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as "speculative mathematics", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and so on. 
Another insightful view put forth is that "pure mathematics is not necessarily applied mathematics": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.
To develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be "pure" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research. 
Mathematics teaching.
Many professional mathematicians also engage in the teaching of mathematics. Duties may include:
Consulting.
Many careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.
As another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock ("see: Valuation of options; Financial modeling").
Occupations.
According to the Dictionary of Occupational Titles occupations in mathematics include the following.
Quotations about mathematicians.
The following are quotations about mathematicians, or by mathematicians.
Women in mathematics.
While the majority of mathematicians are male, there have been some demographic changes since World War II. For example in Europe, from 1992 onwards, several women have been laureates of the prestigious EMS Prize. Some prominent female mathematicians throughout History are Hypatia of Alexandria (ca. 400 AD), Ada Lovelace (1815–1852), Maria Gaetana Agnesi (1718–1799), Emmy Noether (1882–1935), Sophie Germain (1776–1831), Sofia Kovalevskaya (1850–1891), Alicia Boole Stott (1860–1940), Rózsa Péter (1905–1977), Julia Robinson (1919–1985), Olga Taussky-Todd (1906–1995), Émilie du Châtelet (1706–1749), Mary Cartwright (1900–1998), Olga Ladyzhenskaya (1922–2004), Olga Oleinik (1925–2001) and Maryam Mirzakhani.
The Association for Women in Mathematics is a professional society whose purpose is "to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity and the equal treatment of women and girls in the mathematical sciences."
The American Mathematical Society and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.
Prizes in mathematics.
There is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.
References.
</dl>

</doc>
<doc id="18905" url="http://en.wikipedia.org/wiki?curid=18905" title="Armed forces">
Armed forces

The armed forces of a country are its government-sponsored defense, fighting forces, and organizations. They exist to further the foreign and domestic policies of their governing body and to defend that body and the nation it represents from external and internal aggressors. In broad usage, the terms "armed forces" and "military" are often treated synonymously, although in technical usage a distinction is sometimes made in which a country's armed forces may include both its military and other paramilitary forces. Armed force is the use of armed forces to achieve political objectives.
The study of the use of armed forces is called military science. Broadly speaking, this involves considering offense and defense at three "levels": strategy, operational art, and tactics. All three levels study the application of the use of force in order to achieve a desired objective.
Organization.
In most countries the basis of the armed forces is the military, divided into basic military branches. However, armed forces can include other paramilitary structures.
Benefits and costs.
The obvious benefit to a country in maintaining armed forces is in providing protection from foreign threats and from internal conflict. In recent decades armed forces personnel have also been used as emergency civil support roles in post-disaster situations. On the other hand, they may also harm a society by engaging in counter-productive (or merely unsuccessful) warfare. Expenditure on science and technology to develop weapons and systems sometimes produces side benefits, although some claim that greater benefits could come from targeting the money directly.

</doc>
<doc id="18906" url="http://en.wikipedia.org/wiki?curid=18906" title="Microfluidics">
Microfluidics

Microfluidics is a multidisciplinary field intersecting engineering, physics, chemistry, biochemistry, nanotechnology, and biotechnology, with practical applications to the design of systems in which small volumes of fluids will be handled. Microfluidics emerged in the beginning of the 1980s and is used in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies. 
It deals with the behavior, precise control and manipulation of fluids that are geometrically constrained to a small, typically sub-millimeter, scale.
Typically, micro means one of the following features:
Typically fluids are moved, mixed, separated or otherwise processed. Numerous applications employ passive fluid control techniques like capillary forces. In some applications external actuation means are additionally used for a directed transport of the media. Examples are rotary drives applying centrifugal forces for the fluid transport on the passive chips. Active microfluidics refers to the defined manipulation of the working fluid by active (micro) components such as micropumps or micro valves. Micro pumps supply fluids in a continuous manner or are used for dosing. Micro valves determine the flow direction or the mode of movement of pumped liquids. Often processes which are normally carried out in a lab are miniaturized on a single chip in order to enhance efficiency and mobility as well as reducing sample and reagent volumes.
Microscale behavior of fluids.
The behavior of fluids at the microscale can differ from 'macrofluidic' behavior in that factors such as surface tension, energy dissipation, and fluidic resistance start to dominate the system. Microfluidics studies how these behaviors change, and how they can be worked around, or exploited for new uses.
At small scales (channel diameters of around 100 nanometers to several hundred micrometers) some interesting and sometimes unintuitive properties appear. In particular, the Reynolds number (which compares the effect of momentum of a fluid to the effect of viscosity) can become very low. A key consequence of this is that fluids, when side-by-side, do not necessarily mix in the traditional sense, as flow becomes laminar rather than turbulent; molecular transport between them must often be through diffusion.
High specificity of chemical and physical properties (concentration, pH, temperature, shear force, etc.) can also be ensured resulting in more uniform reaction conditions and higher grade products in single and multi-step reactions.
Key application areas.
Microfluidic structures include micropneumatic systems, i.e. microsystems for the handling of off-chip fluids (liquid pumps, gas valves, etc.), and microfluidic structures for the on-chip handling of nano- and picolitre volumes. To date, the most successful commercial application of microfluidics is the inkjet printhead. Significant research has also been applied to microfluidic synthesis and production of various biofunctionalized nanoparticles including quantum dots (QDs) and metallic nanoparticles, and other industrially relevant materials (e.g., polymer particles). Additionally, advances in microfluidic manufacturing allow the devices to produced in low-cost plastics and part quality may be verified automatically.
Advances in microfluidics technology are revolutionizing molecular biology procedures
for enzymatic analysis (e.g., glucose and lactate assays), DNA analysis
(e.g., polymerase chain reaction and high-throughput sequencing), and proteomics.
The basic idea of microfluidic biochips is to integrate assay operations such as detection,
as well as sample pre-treatment and sample preparation on one chip.
An emerging application area for biochips is clinical pathology,
especially the immediate point-of-care diagnosis of diseases.
In addition, microfluidics-based devices, capable of continuous sampling and real-time
testing of air/water samples for biochemical toxins and other dangerous
pathogens, can serve as an always-on "bio-smoke alarm" for early warning.
Continuous-flow microfluidics.
These technologies are based on the manipulation of continuous
liquid flow through microfabricated channels.
Actuation of liquid flow is implemented either by external pressure sources, external mechanical pumps,
integrated mechanical micropumps, or by combinations of capillary forces and electrokinetic mechanisms. Continuous-flow microfluidic operation is the mainstream approach because it is easy to implement and less sensitive to protein fouling problems. Continuous-flow devices
are adequate for many well-defined and simple biochemical applications, and for certain tasks such
as chemical separation, but they are less suitable for tasks requiring a high
degree of flexibility or ineffect fluid manipulations. These closed-channel
systems are inherently difficult to integrate and scale because the parameters
that govern flow field vary along the flow path making the fluid flow at any
one location dependent on the properties of the entire system. Permanently etched microstructures also lead to limited reconfigurability and poor fault tolerance capability.
Process monitoring capabilities in continuous-flow systems can be achieved with highly sensitive microfluidic flow sensors based on MEMS technology which offer resolutions down to the nanoliter range.
Droplet-based microfluidics.
Droplet-based microfluidics as a subcategory of microfluidics in contrast with continuous microfluidics has the distinction of manipulating discrete volumes of fluids in immiscible phases with low Reynolds number and laminar flow regimes. Interest in droplet-based microfluidics systems has been growing substantially in past decades. Microdroplets offer the feasibility of handling miniature volumes of fluids conveniently, provide better mixing and are suitable for high throughput experiments. Exploiting the benefits of droplet based microfluidics efficiently requires a deep understanding of droplet generation, droplet motion, droplet merging, and droplet breakup
One of the key advantages of droplet-based microfluidics is the ability to use droplets as incubators for single cells.
Devices capable of generating thousands of droplets per second opens new ways characterize cell population, not only based on a specific marker measured at a specific time point, but also based on cells kinetic behavior such as protein secretion, enzyme activity or proliferation. 
Recently, a method was found to generate a stationary array of microscopic droplets for single-cell incubation that does not require the use of a surfactant 
Digital microfluidics.
Alternatives to the above closed-channel continuous-flow systems include novel open structures, where discrete, independently controllable droplets
are manipulated on a substrate using electrowetting. Following the analogy of digital microelectronics, this approach is referred to as digital microfluidics. Le Pesant et al. pioneered the use of electrocapillary forces to move droplets on a digital track. The "fluid transistor" pioneered by Cytonix also played a role. The technology was subsequently commercialized by Duke University. By using discrete unit-volume droplets, a microfluidic function can be reduced to a set of repeated basic operations, i.e., moving one unit of fluid over one unit of
distance. This "digitization" method facilitates the use of a hierarchical
and cell-based approach for microfluidic biochip design. Therefore, digital
microfluidics offers a flexible and scalable system architecture as well as
high fault-tolerance capability. Moreover, because each droplet can be
controlled independently, these systems also have dynamic reconfigurability,
whereby groups of unit cells in a microfluidic array can be reconfigured to
change their functionality during the concurrent execution of a set of
bioassays. Although droplets are manipulated in confined microfluidic channels, since the control on droplets is not independent, it should not be confused as "digital microfluidics". One common actuation method for digital microfluidics is electrowetting-on-dielectric (EWOD). Many lab-on-a-chip applications have been demonstrated within the digital microfluidics paradigm using electrowetting. However, recently other techniques for droplet manipulation have also been demonstrated using surface acoustic waves, optoelectrowetting, mechanical actuation, etc.
DNA chips (microarrays).
Early biochips were based on the idea of a DNA microarray,
e.g., the GeneChip DNAarray from Affymetrix, which is a piece of glass,
plastic or silicon substrate on which pieces of DNA (probes) are affixed in a microscopic
array. Similar to a DNA microarray, a protein array is a miniature array
where a multitude of different capture agents, most frequently monoclonal
antibodies, are deposited on a chip surface; they are used to determine the
presence and/or amount of proteins in biological samples, e.g., blood. A
drawback of DNA and protein arrays is that they are neither
reconfigurable nor scalable after manufacture. Digital microfluidics has been described as a means for carrying out Digital PCR.
Molecular biology.
In addition to microarrays, biochips have been designed for two-dimensional electrophoresis, transcriptome analysis, and PCR amplification. Other applications include various electrophoresis and liquid chromatography applications for proteins and DNA, cell separation, in particular blood cell separation, protein analysis, cell manipulation and analysis including cell viability analysis and microorganism capturing.
Evolutionary biology.
By combining microfluidics with landscape ecology and nanofluidics, a nano/micro fabricated fluidic landscape can be constructed by building local patches of bacterial habitat and connecting them by dispersal corridors. The resulting landscapes can be used as physical implementations of an adaptive landscape, by generating a spatial mosaic of patches of opportunity distributed in space and time. The patchy nature of these fluidic landscapes allows for the study of adapting bacterial cells in a metapopulation system. The evolutionary ecology of these bacterial systems in these synthetic ecosystems allows for using biophysics to address questions in evolutionary biology.
Cell behavior.
The ability to create precise and carefully controlled chemoattractant gradients makes microfluidics the ideal tool to study motility, chemotaxis and the ability to evolve / develop resistance to antibiotics in small populations of microorganisms and in a short period of time. These microorganisms including bacteria and the broad range of organisms that form the marine microbial loop, responsible for regulating much of the oceans' biogeochemistry.
Microfluidics has also greatly aided the study of durotaxis by facilitating the creation of durotactic (stiffness) gradients.
Cellular biophysics.
By rectifying the motion of individual swimming bacteria, microfluidic structures can be used to extract mechanical motion from a population of motile bacterial cells. This way, bacteria-powered rotors can be built.
Optics.
The merger of microfluidics and optics is typical known as optofluidics. Examples of optofluidic devices are tuneable microlens arrays and 
optofluidic microscopes.
Acoustic droplet ejection (ADE).
Acoustic droplet ejection uses a pulse of ultrasound to move low volumes of fluids (typically nanoliters or picoliters) without any physical contact. This technology focuses acoustic energy into a fluid sample in order to eject droplets as small as a millionth of a millionth of a liter (picoliter = 10−12 liter). ADE technology is a very gentle process, and it can be used to transfer proteins, high molecular weight DNA and live cells without damage or loss of viability. This feature makes the technology suitable for a wide variety of applications including proteomics and cell-based assays.
Fuel cells.
Microfluidic fuel cells can use laminar flow to separate the fuel and its oxidant to control the interaction of the two fluids without a physical barrier as would be required in conventional fuel cells.
A tool for cell biological research.
Microfluidic technology is creating powerful tools for cell biologists to control the complete cellular environment, leading to new questions and new discoveries. Many diverse advantages of this technology for microbiology are listed below:

</doc>
