<doc id="15406" url="http://en.wikipedia.org/wiki?curid=15406" title="Irgun">
Irgun

The Irgun (Hebrew: אִרְגּוּן; full title: הָאִרְגּוּן הַצְּבָאִי הַלְּאֻמִּי בְאֶרֶץ יִשְׂרָאֵל "Hā-ʾIrgun Ha-Tzvaʾī Ha-Leūmī b-Ērētz Yiśrāʾel", lit. "The National Military Organization in the Land of Israel"), was a Zionist paramilitary group that operated in Mandate Palestine between 1931 and 1948. It was an offshoot of the older and larger Jewish paramilitary organization Haganah (Hebrew: "Defense", הגנה). When the group broke from the Haganah it became known as the "Haganah Bet" (Hebrew: literally "Defense 'B' " or "Second Defense", הגנה ב), or alternatively as haHaganah haLeumit (ההגנה הלאומית) or Hama'amad (המעמד‎). Irgun members were absorbed into the Israel Defense Forces at the start of the 1948 Arab–Israeli war. The Irgun is also referred to as Etzel (אצ"ל), an acronym of the Hebrew initials, or by the abbreviation IZL.
The Irgun policy was based on what was then called Revisionist Zionism founded by Ze'ev Jabotinsky. According to Howard Sachar, "The policy of the new organization was based squarely on Jabotinsky's teachings: every Jew had the right to enter Palestine; only active retaliation would deter the Arabs; only Jewish armed force would ensure the Jewish state".
Two of the operations for which the Irgun is best known are the bombing of the King David Hotel in Jerusalem on 22 July 1946 and the Deir Yassin massacre, carried out together with Lehi on 9 April 1948.
The Irgun has been viewed as a terrorist organization or organization which carried out terrorist acts. In particular the Irgun was described as a terrorist organization by Britain, the 1946 Zionist Congress and the Jewish Agency. Irgun's tactics appealed to a certain segment of the Jewish community that believed that any action taken in the cause of the creation of a Jewish state was justified, including terrorism.
The Irgun was a political predecessor to Israel's right-wing "Herut" (or "Freedom") party, which led to today's Likud party. Likud has led or been part of most Israeli governments since 1977.
Nature of the Movement.
Members of the Irgun came mostly from Betar and from the Revisionist Party both in Palestine and abroad. The Revisionist Movement made up a popular backing for the underground organization. Ze'ev Jabotinsky, founder of Revisionist Zionism, was the commander of the organization until he died. He formulated the general realm of operation, regarding "Restraint" and the end thereof, and was the inspiration for the organization overall. An additional major source of ideological inspiration was the poetry of Uri Zvi Greenberg. The symbol of the organization, with the motto רק כך (only thus), underneath a hand holding a rifle in the foreground of a map showing both Mandatory Palestine and the Emirate of Transjordan (at the time, both were administered under the terms of the British Mandate for Palestine), implying that force was the only way to "liberate the homeland".
The number of members of the Irgun varied from a few hundred to a few thousand. Most of its members were people who joined the organization's command, under which they carried out various operations and filled positions, largely in opposition to British law. Most of them were "ordinary" people, who held regular jobs, and only a few dozen worked full-time in the Irgun.
The Irgun disagreed with the policy of the Yishuv and with the World Zionist Organization, both with regard to strategy and basic ideology and with regard to PR and military tactics, such as use of armed force to accomplish the Zionist ends, operations against the Arabs during the riots, and relations with the British mandatory government. Therefore the Irgun tended to ignore the decisions made by the Zionist leadership and the Yishuv's institutions. This fact caused the elected bodies not to recognize the independent organization, and during most of the time of its existence the organization was seen as irresponsible, and its actions thus worthy of thwarting. Therefore the Irgun accompanied its armed operations with public relations campaigns, in order to convince the public of the Irgun's way and the problems with the official political leadership of the Yishuv. The Irgun put out numerous advertisements, an underground newspaper and even ran the first independent Hebrew radio station – Kol Zion HaLochemet.
Structure, command, and organization.
As an underground armed organization, members did not normally call it by its name, but rather used other names. In the first years of its existence it was known primarily as "Ha-Haganah Leumit"' (The National Defense), and also by names such as "Haganah Bet" ("Second Defense"), "Irgun Bet" ("Second Irgun"), the "Parallel Organization" and the "Rightwing Organization". Later on it was most widely known as המעמד (the Stand). The anthem adopted by the Irgun was "Anonymous Soldiers", written by Avraham (Yair) Stern who was at the time a commander in the Irgun. Later on Stern defected from the Irgun and founded Lehi, and the song became the anthem of the Lehi. The Irgun's new anthem then became the third verse of the "Betar Song", by Ze'ev Jabotinsky.
The Irgun gradually evolved from its humble origins into a serious and well-organized paramilitary organization. The movement developed a series of ranks and a sophisticated command structure, and came to demand serious military training and strict discipline from its members. It developed clandestine networks of hidden arms caches and weapons-production workshops, safe-houses, and training camps.
The ranks of the Irgun were (in ascending order): 
The Irgun was led by a High Command, which set policy and gave orders. Directly underneath it was a General Staff, which oversaw the activities of the Irgun. The General Staff was divided into a military and support staff. The military staff was divided into operational units that oversaw operations and support units in charge of planning, instruction, weapons caches and manufacture, and first aid. The military and support staff never met jointly and communicated through the High Command. Beneath the General Staff were six district commands: Jerusalem, Tel Aviv, Haifa-Galilee, Southern, Sharon, and Shomron, each led by a district commander. A local Irgun district unit was called a "Branch". A "brigade" in the Irgun was made up of three sections. A section was made up of two groups, at the head of each was a "Group Head", and a deputy. Eventually, various units were established, which answered to a "Center" or "Staff".
The head of the Irgun High Command was the overall commander of the organization, but the name of his rank varied. During the revolt against the British, Irgun commander Menachem Begin and the entire High Command held the rank of "Gundar Rishon". His predecessors, however, had held their own ranks. A rank of Military Commander (Seren) was awarded to the Irgun commander Yaakov Meridor and a rank of High Commander (Aluf) to David Raziel. Until his death in 1940, Jabotinsky was known as the "Military Commander of the Etzel" or the "Ha-Matzbi Ha-Elyon" ("Supreme Commander").
Under the command of Menachem Begin, the Irgun was divided into different corps: 
In theory, the Irgun was supposed to have a regular combat force, a reserve, and shock units, but in practice there were not enough personnel for a reserve or a shock force.
The Irgun emphasized that it's fighters be highly disciplined. Strict drill exercises were carried out at ceremonies at different times, and strict attention was given to discipline, formal ceremonies and military relationships between the various ranks. The Irgun put out professional publications on combat doctrine, weaponry, leadership, drill exercises, etc. Among these publications were three books written by David Raziel, who had studied military history, techniques, and strategy: "The Pistol" (written in collaboration with Avraham Stern), "The Theory of Training", and "Parade Ground and Field Drill". A British analysis noted that the Irgun's discipline was "as strict as any army in the world."
The Irgun operated a sophisticated recruitment and military training regime. Those wishing to join had to find and make contact with a member, meaning only those who personally knew a member or were persistent could find their way in. Once contact had been established, a meeting was set up with the three-member selection committee at a safe-house, where the recruit was interviewed in a darkened room, with the committee either positioned behind a screen, or with a flashlight shone into the recruit's eyes. The interviewers asked basic biographical questions, and then asked a series of questions designed to weed out romantics and adventurers and those who had not seriously contemplated the potential sacrifices. Those selected attended a four-month series of indoctrination seminars in groups of five to ten, where they were taught the Irgun's ideology and the code of conduct it expected of its members. These seminars also had another purpose - to weed out the impatient and those of flawed purpose who had gotten past the selection interview. Then, members were introduced to other members, were taught the locations of safe-houses, and given military training. Irgun recruits trained with firearms, hand grenades, and were taught how to conduct combined attacks on targets. Arms handling and tactics courses were given in clandestine training camps, while practice shooting took place in the desert or by the sea. Eventually, separate training camps were established for heavy-weapons training. The most rigorous course was the explosives course for bomb-makers, which lasted a year. The British authorities believed that some Irgun members enlisted in the Jewish section of the Palestine Police Force for a year as part of their training, during which they also passed intelligence. In addition to the Irgun's sophisticated training program, many Irgun members were veterans of the Haganah (including the Palmach), the British Armed Forces, and Jewish partisan groups that had waged guerrilla warfare in Nazi-occupied Europe, thus bringing significant military training and combat experience into the organization. The Irgun also operated a course for its intelligence operatives, in which recruits were taught espionage, cryptography, and analysis techniques.
Of the Irgun's members, almost all were part-time members. They were expected to maintain their civilian lives and jobs, dividing their time between their civilian lives and underground activities. There were never more than 40 full-time members, who were given a small expense stipend on which to live on. Upon joining, every member received an underground name. The Irgun's members were divided into cells, and worked with the members of their own cells. The identities of Irgun members in other cells were withheld. This ensured that an Irgun member taken prisoner could betray no more than a few comrades.
In addition to the Irgun's members in Palestine, underground Irgun cells composed of local Jews were established in Europe following World War II. An Irgun cell was also established in Shanghai, home to many European-Jewish refugees. The Irgun also set up a Swiss bank account. Eli Tavin, the former head of Irgun intelligence, was appointed commander of the Irgun abroad.
In November 1947, the Jewish insurgency came to an end as the UN approved of the partition of Palestine, and the British had announced their intention to withdraw the previous month. As the British left and the 1947-48 Civil War in Mandatory Palestine got underway, the Irgun came out of the underground and began to function more as a standing army rather an underground organization. It began openly recruiting, training, and raising funds, and established bases, including training facilities. It also introduced field communications and created a medical unit and supply service.
Until World War II the group armed itself with weapons purchased in Europe, primarily Italy and Poland, and smuggled to Palestine. The Irgun also established workshops that manufactured spare parts and attachments for the weapons. Also manufactured were land mines and simple hand grenades. Another way in which the Irgun armed itself was theft of weapons from the British Police and military.
Prior to World War II.
Founding.
The Irgun's first steps were in the aftermath of the Riots of 1929. In the Jerusalem branch of the Haganah there were feelings of disappointment and internal unrest towards the leadership of the movements and the Histadrut (at that time the organization running the Haganah). These feelings were a result of the view that the Haganah was not adequately defending Jewish interests in the region. Likewise, critics of the leadership spoke out against alleged failures in the number of weapons, readiness of the movement and its policy of restraint and not fighting back. On April 10, 1931, commanders and equipment managers announced that they refuse to return weapons to the Haganah that had been issued to them earlier, prior to the Nebi Musa holiday. These weapons were later returned by the commander of the Jerusalem branch, Avraham Tehomi, a.k.a. "Gideon". However, the commanders who decided to rebel against the leadership of the Haganah relayed a message regarding their resignations to the Vaad Leumi, and thus this schism created a new independent movement.
The leader of the new underground movement was Avraham Tehomi, alongside other founding members who were all senior commanders in the Haganah, members of Hapoel Hatzair and of the Histadrut. Also among them was Eliyahu Ben Horin, an activist in the Revisionist Party. This group was known as the "Odessan Gang", because they previously had been members of the "Haganah Ha'Atzmit" of Jewish Odessa. The new movement was named "Irgun Tsvai Leumi", ("National Military Organization") in order to emphasize its active nature in contrast to the Haganah. Moreover, the organization was founded with the desire to become a true military organization and not just a militia as the Haganah was at the time.
In the autumn of that year the Jerusalem group merged with other armed groups affiliated with Betar. The Betar groups' center of activity was in Tel Aviv, and they began their activity in 1928 with the establishment of "Officers and Instructors School of Betar". Students at this institution had broken away from the Haganah earlier, for political reasons, and the new group called itself the "National Defense", הגנה הלאומית. During the riots of 1929 Betar youth participated in the defense of Tel Aviv neighborhoods under the command of Yermiyahu Halperin, at the behest of the Tel Aviv city hall. After the riots the Tel Avivian group expanded, and was known as "The Right Wing Organization".
After the Tel Aviv expansion another branch was established in Haifa. Towards the end of 1932 the Haganah branch of Safed also defected and joined the Irgun, as well as many members of the Maccabi sports association. At that time the movement's underground newsletter, "Ha'Metsudah" (the Fortress) also began publication, expressing the active trend of the movement. The Irgun also increased its numbers by expanding draft regiments of Betar – groups of volunteers, committed to two years of security and pioneer activities. These regiments were based in places that from which stemmed new Irgun strongholds in the many places, including the settlements of Yesod HaMa'ala, Mishmar HaYarden, Rosh Pina, Metula and Nahariya in the north; in the center – Hadera, Binyamina, Herzliya, Netanya and Kfar Saba, and south of there – Rishon LeZion, Rehovot and Ness Ziona. Later on regiments were also active in the Old City of Jerusalem ("the Kotel Brigades") among others. Primary training centers were based in Ramat Gan, Qastina (by Kiryat Mal'akhi of today) and other places.
Under Tehomi's command.
In 1933 there were some signs of unrest, seen by the incitement of the local Arab leadership to act against the authorities. The strong British response put down the disturbances quickly. During that time the Irgun operated in a similar manner to the Haganah and was a guarding organization. The two organizations cooperated in ways such as coordination of posts and even intelligence sharing.
Within the Irgun, Tehomi was the first to serve as "Head of the Headquarters" or "Chief Commander". Alongside Tehomi served the senior commanders, or "Headquarters" of the movement. As the organization grew, it was divided into district commands.
In August 1933 a "Supervisory Committee" for the Irgun was established, which included representatives from most of the Zionist political parties. The members of this committee were Meir Grossman (of the Hebrew State Party), Rabbi Meir Bar-Ilan (of the Mizrachi Party, either Immanuel Neumann or Yehoshua Supersky (of the General Zionists) and Ze'ev Jabotinsky or Eliyahu Ben Horin (of Hatzohar).
In protest against, and with the aim of ending Jewish immigration to Palestine, the Great Arab Revolt of 1936–1939 broke out on April 19, 1936. The riots took the form of attacks by Arab rioters ambushing main roads, bombing of roads and settlements as well as property and agriculture vandalism. In the beginning, the Irgun and the Haganah generally maintained a policy of restraint, apart from a few instances. Some expressed resentment at this policy, leading up internal unrest in the two organizations. The Irgun tended to retaliate more often, and sometimes Irgun members patrolled areas beyond their positions in order to encounter attackers ahead of time. However, there were differences of opinion regarding what to do in the Haganah, as well. Due to the joining of many Betar Youth members, Jabotinsky (founder of Betar) had a great deal of influence over Irgun policy. Nevertheless, Jabotinsky was of the opinion that for moral reasons violent retaliation was not to be undertaken.
In November 1936 the Peel Commission was sent to inquire regarding the breakout of the riots and propose a solution to end the Revolt. In early 1937 there were still some in the Yishuv who felt the commission would recommend a partition of Mandatory Palestine (the land west of the Jordan River), thus creating a Jewish state on part of the land. The Irgun leadership, as well as the "Supervisory Committee" held similar beliefs, as did some members of the Haganah and the Jewish Agency. This belief strengthened the policy of restraint and led to the position that there was no room for defense institutions in the future Jewish state. Tehomi was quoted as saying: "We stand before great events: a Jewish state and a Jewish army. There is a need for a single military force". This position intensified the differences of opinion regarding the policy of restraint, both within the Irgun and within the political camp aligned with the organization. The leadership committee of the Irgun supported a merger with the Haganah. On April 24, 1937 a referendum was held among Irgun members regarding its continued independent existence. David Raziel and Avraham (Yair) Stern came out publicly in support for the continued existence of the Irgun:
The first split.
In April 1937 the Irgun split after the referendum. Approximately 1,500–2,000 people, about half of the Irgun's membership, including the senior command staff, regional committee members, along with most of the Irgun's weapons, returned to the Haganah, which at that time was under the Jewish Agency's leadership. The Supervisory Committee's control over the Irgun ended, and Jabotinsky assumed command. In their opinion, the removal of the Haganah from the Jewish Agency's leadership to the national institutions necessitated their return. Furthermore, they no longer saw significant ideological differences between the movements. Those who remained in the Irgun were primarily young activists, mostly laypeople, who sided with the independent existence of the Irgun. In fact, most of those who remained were originally Betar people. Moshe Rosenberg estimated that approximately 1,800 members remained. In theory, the Irgun remained an organization not aligned with a political party, but in reality the supervisory committee was disbanded and the Irgun's continued ideological path was outlined according to Ze'ev Jabotinsky's school of thought and his decisions, until the movement eventually became Revisionist Zionism's military arm. One of the major changes in policy by Jabotinsky was the end of the policy of restraint.
On April 27, 1937 the Irgun founded a new headquarters, staffed by Moshe Rosenberg at the head, Avraham (Yair) Stern as secretary, David Raziel as head of the Jerusalem branch, Hanoch Kalai as commander of Haifa and Aharon Haichman as commander of Tel Aviv. On 20 Tammuz, (June 29) the day of Theodor Herzl's death, a ceremony was held in honor of the reorganization of the underground movement. For security purposes this ceremony was held at a construction site in Tel Aviv.
Ze'ev Jabotinsky placed Col. Robert Bitker at the head of the Irgun. Bitker had previously served as Betar commissioner in China and had military experience. A few months later, probably due to total incompatibility with the position, Jabotinsky replaced Bitker with Moshe Rosenberg. When the Peel Commission report was published a few months later, the Revisionist camp decided not to accept the commission's recommendations. Moreover, the organizations of Betar, Hatzohar and the Irgun began to increase their efforts to bring Jews to the land of Israel, illegally. This Aliyah was known as the עליית אף על פי "Af Al Pi (Nevertheless) Aliyah". As opposed to this position, the Jewish Agency began acting on behalf of the Zionist interest on the political front, and continued the policy of restraint. From this point onwards the differences between the Haganah and the Irgun were much more obvious.
Illegal immigration.
According to Jabotinsky's "Evacuation Plan", which called for millions of European Jews to be brought to Palestine at once, the Irgun helped the illegal immigration of European Jews to the land of Israel. This was named by Jabotinsky the "National Sport". The most significant part of this immigration prior to World War II was carried out by the Revisionist camp, largely because the Yishuv institutions and the Jewish Agency shied away from such actions on grounds of cost and their belief that Britain would in the future allow widespread Jewish immigration.
The Irgun joined forces with Hatzohar and Betar in September 1937, when it assisted with the landing of a convoy of 54 Betar members at Tantura Beach (near Haifa.) The Irgun was responsible for discreetly bringing the Olim, or Jewish immigrants, to the beaches, and dispersing them among the various Jewish settlements. The Irgun also began participating in the organisation of the immigration enterprise and undertook the process of accompanying the ships. This began with the ship "Draga" which arrived at the coast of British Palestine in September 1938. In August of the same year, an agreement was made between Ari Jabotinsky (the son of Ze'ev Jabotinsky), the Betar representative and Hillel Kook, the Irgun representative, to coordinate the immigration (also known as Ha'apala). This agreement was also made in the "Paris Convention" in February 1939, at which Ze'ev Jabotinsky and David Raziel were present. Afterwards, the "Aliyah Center" was founded, made up of representatives of Hatzohar, Betar, and the Irgun, thereby making the Irgun a full participant in the process.
The difficult conditions on the ships demanded a high level of discipline. The people on board the ships were often split into units, led by commanders. In addition to having a daily roll call and the distribution of food and water (usually very little of either), organized talks were held to provide information regarding the actual arrival in Palestine. One of the largest ships was the "Sakaria", with 2,300 passengers, which equalled about 0.5% of the Jewish population in Palestine. The first vessel arrived on April 13, 1937, and the last on February 13, 1940. All told, about 18,000 Jews immigrated to Palestine with the help of the Revisionist organizations and private initiatives by other Revisionists. Most were not caught by the British.
End of restraint.
Irgun members continued to defend settlements, but at the same time began attacks on Arab villages, thus ending the policy of restraint. These attacks were intended to instill fear in the Arab side, in order to cause the Arabs to wish for peace and quiet. In March 1938, David Raziel wrote in the underground newspaper "By the Sword" a constitutive article for the Irgun overall, in which he coined the term "Active Defense":
The first attacks began around April 1936, and by the end of World War II, more than 250 Arabs had been killed. Examples include:
During 1936, Irgun members carried out approximately ten attacks.
Throughout 1937 the Irgun continued this line of operation.
A more complete list can be found here.
At that time, however, these acts were not yet a part of a formulated policy of the Irgun. Not all of the aforementioned operations received a commander's approval, and Jabotinsky was not in favor of such actions at the time. Jabotinsky still hoped to establish a Jewish force out in the open that would not have to operate underground. However, the failure, in its eyes, of the Peel Commission and the renewal of violence on the part of the Arabs caused the Irgun to rethink its official policy.
Increase in operations.
14 November 1937 was a watershed in Irgun activity. From that date, the Irgun increased its reprisals. Following an increase in the number of attacks aimed at Jews, including the killing of five kibbutz members near Kiryat Anavim (today kibbutz Ma'ale HaHamisha), the Irgun undertook a series of attacks in various places in Jerusalem, killing five Arabs. Operations were also undertaken in Haifa (shooting at the Arab-populated Wadi Nisnas neighborhood) and in Herzliya. The date is known as the day the policy of restraint (Havlagah) ended, or as "Black Sunday". This is when the organization fully changed its policy, with the approval of Jabotinsky and Headquarters to the policy of "active defense" in respect of Irgun actions.
The British responded with the arrest of Betar and Hatzohar members as suspected members of the Irgun. Military courts were allowed to act under "Time of Emergency Regulations" and even sentence people to death. In this manner Yehezkel Altman, a guard in a Betar battalion in the Nahalat Yizchak neighborhood of Tel Aviv, shot at an Arab bus, without his commanders' knowledge. Altman was acting in response to a shooting at Jewish vehicles on the Tel Aviv–Jerusalem road the day before. He turned himself in later and was sentenced to death, a sentence which was later commuted to a life sentence.
Despite the arrests, Irgun members continued fighting. Jabotinsky lent his moral support to these activities. In a letter to Moshe Rosenberg on 18 March 1938 he wrote:
Although the Irgun continued activities such as these, following Rosenberg's orders, they were greatly curtailed. Furthermore, in fear of the British threat of the death sentence for anyone found carrying a weapon, all operations were suspended for eight months. However, opposition to this policy gradually increased. In April, 1938, responding to the killing of six Jews, Betar members from the Rosh Pina Brigade went on a reprisal mission, without the consent of their commander, as described by historian Avi Shlaim:
Although the incident ended without casualties, the three were caught, and one of them – Shlomo Ben-Yosef was sentenced to death. Demonstrations around the country, as well as pressure from institutions and people such as Dr. Chaim Weizmann and the Chief Rabbi of Mandatory Palestine, Yitzhak HaLevi Herzog did not reduce his sentence. In Shlomo Ben-Yosef's writings in Hebrew were later found:
On 29 June 1938 he was executed, and was the first of the Olei Hagardom. The Irgun revered him after his death and many regarded him as an example.
In light of this, and due to the anger of the Irgun leadership over the decision to adopt a policy of restraint until that point, Jabotinsky relieved Rosenberg of his post and replaced him with David Raziel, who proved to be the most prominent Irgun commander until Menachem Begin. Jabotinsky simultaneously instructed the Irgun to end its policy of restraint, leading to armed offensive operations until the end of the Arab Revolt in 1939. In this time, the Irgun mounted about 40 operations against Arabs and Arab villages, for instance:
This action led the British Parliament to discuss the disturbances in Palestine. On 23 February 1939 the Secretary of State for the Colonies, Malcolm MacDonald revealed the British intention to cancel the mandate and establish a state that would preserve Arab rights. This caused a wave of riots and attacks by Arabs against Jews. The Irgun responded four days later with a series of attacks on Arab buses and other sites. The British used military force against the Arab rioters and in the latter stages of the revolt by the Arab community in Palestine, it deteriorated into a series of internal gang wars.
During the same period.
At the same time, the Irgun also established itself in Europe. The Irgun built underground cells that participated in organizing migration to Palestine. The cells were made up almost entirely of Betar members, and their primary activity was military training in preparation for emigration to Palestine. Ties formed with the Polish authorities brought about courses in which Irgun commanders were trained by Polish officers in advanced military issues such as guerrilla warfare, tactics and laying land mines. Avraham (Yair) Stern was notable among the cell organizers in Europe. In 1937 the Polish authorities began to deliver large amounts of weapons to the underground. The transfer of handguns, rifles, explosives and ammunition stopped with the outbreak of World War II. Another field in which the Irgun operated was the training of pilots, so they could serve in the Air Force in the future war for independence, in the flight school in Lod.
Towards the end of 1938 there was progress towards aligning the ideologies of the Irgun and the Haganah. Many abandoned the belief that the land would be divided and a Jewish state would soon exist. The Haganah founded פו"מ, a special operations unit, (pronounced "poom"), which carried out reprisal attacks following Arab violence. These operations continued into 1939. Furthermore, the opposition within the Yishuv to illegal immigration significantly decreased, and the Haganah began to bring Jews to Palestine using rented ships, as the Irgun had in the past.
First operations against the British.
The publishing of the MacDonald White Paper of 1939 brought with it new edicts that were intended to lead to a more equitable settlement between Jews and Arabs. However, it was considered by some Jews to have an adverse effect on the continued development of the Jewish community in Palestine. Chief among these was the prohibition on selling land to Jews, and the smaller quotas for Jewish immigration. The entire Yishuv was furious at the contents of the White Paper. There were demonstrations against the "Treacherous Paper", as it was considered that it would preclude the establishment of a Jewish homeland in Palestine.
Under the temporary command of Hanoch Kalai, the Irgun began sabotaging strategic infrastructure such as electricity facilities, radio and telephone lines. It also started publicizing its activity and its goals. This was done in street announcements, newspapers, as well as the underground radio station Kol Zion HaLochemet. On August 26, 1939, the Irgun killed Ralph Cairns, a British police officer who, as head of the Jewish Department in the Palestine Police, had tortured a number of youths who were underground members. Cairns and Ronald Barker, another British police officer, were killed by an Irgun IED.
The British increased their efforts against the Irgun. As a result, on August 31 the British police arrested members meeting in the Irgun headquarters. On the next day, September 1, 1939, World War II broke out.
During World War II.
Following the outbreak of war, Ze'ev Jabotinsky and the New Zionist Organization voiced their support for Britain and France. In mid-September 1939 Raziel was moved from his place of detention in Tzrifin. This, among other events, encouraged the Irgun to announce a cessation of its activities against the British so as not to hinder Britain's effort to fight "the Hebrew's greatest enemy in the world – German Nazism". This announcement ended with the hope that after the war a Hebrew state would be founded "within the historical borders of the liberated homeland". After this announcement Irgun, Betar and Hatzohar members, including Raziel and the Irgun leadership, were gradually released from detention. The Irgun did not rule out joining the British army and the Jewish Brigade. Irgun members did enlist in various British units. Irgun members also assisted British forces with intelligence in Romania, Bulgaria, Morocco and Tunisia. An Irgun unit also operated in Syria and Lebanon. David Raziel later died during one of these operations.
During the Holocaust, Betar members revolted numerous times against the Nazis in occupied Europe. The largest of these revolts was the Warsaw Ghetto Uprising, in which an armed underground organization fought, formed by Betar and Hatzoar and known as the "Żydowski Związek Wojskowy (ŻZW)" (Jewish Military Union). Despite its political origins, the ŻZW accepted members without regard to political affiliation, and had contacts established before the war with elements of the Polish military. Because of differences over objectives and strategy, the ŻZW was unable to form a common front with the mainstream ghetto fighters of the Żydowska Organizacja Bojowa, and fought independently under the military leadership of Paweł Frenkiel and the political leadership of Dawid Wdowiński.
There were instances of Betar members enlisted in the British military smuggling British weapons to the Irgun. [ref?]
From 1939 onwards, an Irgun delegation in the United States worked for the creation of a Jewish army made up of Jewish refugees and Jews from Palestine, to fight alongside the Allied Forces. In July 1943 the "Emergency Committee to Save the Jewish People in Europe" was formed, and worked until the end of the war to rescue the Jews of Europe from the Nazis and to garner public support for a Jewish state. However, it was not until January 1944 that US President Franklin Roosevelt established the War Refugee Board, which achieved some success in saving European Jews.
Second split.
Throughout this entire period, the British continued enforcing the White Paper's provisions, which included a ban on the sale of land, restrictions on Jewish immigration and increased vigilance against illegal immigration. Part of the reason why the British banned land sales (to anyone) was the confused state of the post Ottoman land registry; it was difficult to determine who actually owned the land that was for sale.
Within the ranks of the Irgun this created much disappointment and unrest, at the center of which was disagreement with the leadership of the New Zionist Organization, David Raziel and the Irgun Headquarters. On June 18, 1939, Avraham (Yair) Stern and others of the leadership were released from prison and a rift opened between them the Irgun and Hatzohar leadership. The controversy centred on the issues of the underground movement submitting to public political leadership and fighting the British. On his release from prison Raziel resigned from Headquarters. To his chagrin, independent operations of senior members of the Irgun were carried out and some commanders even doubted Raziel's loyalty.
In his place, Stern was elected to the leadership. n the past, Stern had founded secret Irgun cells in Poland without Jabotinsky's knowledge, in opposition to his wishes. Furthermore, Stern was in favor of removing the Irgun from the authority of the New Zionist Organization, whose leadership urged Raziel to return to the command of the Irgun. He finally consented. Jabotinsky wrote to Raziel and to Stern, and these letters were distributed to the branches of the Irgun:
Stern was sent a telegram with an order to obey Raziel, who was reappointed. However, these events did not prevent the splitting of the organization. Suspicion and distrust were rampant among the members. Out of the Irgun a new organization was created on July 17, 1940, which was first named "The National Military Organization in Israel" (as opposed to the "National Military Organization in the Land of Israel") and later on changed its name to Lehi, an acronym for Lohamei Herut Israel, "Fighters for the Freedom of Israel", (לח"י – לוחמי חירות ישראל). Jabotinsky died in New York on August 4, 1940, yet this did not prevent the Lehi split. Following Jabotinsky's death, ties were formed between the Irgun and the New Zionist Organization. These ties would last until 1944, when the Irgun declared a revolt against the British.
The primary difference between the Irgun and the newly formed organization was its intention to fight the British in Palestine, regardless of their war against Germany. Later, additional operational and ideological differences developed that contradicted some of the Irgun's guiding principles. For example, the Lehi, unlike the Irgun, supported a population exchange with local Arabs. 
Change of policy.
The split damaged the Irgun both organizationally and from a morale point of view. As their spiritual leader, Jabotinsky's death also added to this feeling. Together, these factors brought about a mass abandonment by members. The British took advantage of this weakness to gather intelligence and arrest Irgun activists. The new Irgun leadership, which included Meridor, Yerachmiel Ha'Levi, Moshe Segal and others used the forced hiatus in activity to rebuild the injured organization. This period was also marked by more cooperation between the Irgun and the Jewish Agency, however David Ben-Gurion's uncompromising demand that Irgun accept the Agency's command foiled any further cooperation.
In both the Irgun and the Haganah more voices were being heard opposing any cooperation with the British. Nevertheless, an Irgun operation carried out in the service of Britain was aimed at sabotaging pro-Nazi forces in Iraq, including the assassination of Haj Amin al-Husayni. Among others, Raziel and Yaakov Meridor participated. On April 20, 1941, during a Luftwaffe air raid on RAF Hannaniya near Baghdad, David Raziel, commander of the Irgun, was killed during the operation.
In late 1943 a joint Haganah – Irgun initiative was developed, to form a single fighting body, unaligned with any political party, by the name of עם לוחם ("Fighting Nation"). The new body's first plan was to kidnap the British High Commissioner of Palestine, Sir Harold MacMichael and take him to Cyprus. However, the Haganah leaked the planned operation and it was thwarted before it got off the ground. Nevertheless, at this stage the Irgun ceased its cooperation with the British. As Eliyahu Lankin tells in his book:
The "Revolt".
In 1943 the Polish II Corps, commanded by Władysław Anders, arrived in Palestine from Iraq. The British insisted that no Jewish units of the army be created. Eventually, many of the soldiers of Jewish origin that arrived with the army were released and allowed to stay in Palestine. One of them was Menachem Begin, whose arrival in Palestine created new-found expectations within the Irgun and Betar. Begin had served as head of the Betar movement in Poland, and was a respected leader. Yaakov Meridor, then the commander of the Irgun, raised the idea of appointing Begin to the post. In late 1943, when Begin accepted the position, a new leadership was formed. Meridor became Begin's deputy, and other members of the board were Aryeh Ben Eliezer, Eliyahu Lankin, and Shlomo Lev Ami.
On February 1, 1944 the Irgun put up posters all around the country, proclaiming a revolt against the British mandatory government. The posters began by saying that all of the Zionist movements stood by the Allied Forces and over 25,000 Jews had enlisted in the British military. The hope to establish a Jewish army had died. European Jewry was trapped and was being destroyed, yet Britain, for its part, did not allow any rescue missions. This part of the document ends with the following words:
The Irgun then declared that, for its part, the ceasefire was over and they were now at war with the British. It demanded the transfer of rule to a Jewish government, to implement ten policies. Among these were the mass evacuation of Jews from Europe, the signing of treaties with any state that recognized the Jewish state's sovereignty, including Britain, granting social justice to the state's residents, and full equality to the Arab population. The proclamation ended with:
The Irgun began this campaign rather weakly. At the time of the start of the revolt, it was only about 1,000 strong, including some 200 fighters. It possessed about 4 submachine guns, 40 rifles, 60 pistols, 150 hand grenades, and 2,000 kilograms of explosive material, and it's funds were about £800.
Struggle against the British.
The Irgun began a militant operation against the symbols of government, in an attempt to harm the regime's operation as well as its reputation. The first attack was on February 12, 1944 at the government immigration offices, a symbol of the immigration laws. The attacks went smoothly and ended with no casualties—as they took place on a Saturday night, when the buildings were empty—in the three largest cities: Jerusalem, Tel Aviv, and Haifa. On February 27 the income tax offices were bombed. Parts of the same cities were blown up, also on a Saturday night; prior warnings were put up near the buildings. On March 23 the national headquarters building of the British police in the Russian Compound in Jerusalem was attacked, and part of it was blown up. These attacks in the first few months were sharply condemned by the organized leadership of the Yishuv and by the Jewish Agency, who saw them as dangerous provocations.
At the same time the Lehi also renewed its attacks against the British. The Irgun continued to attack police stations and headquarters, and Tegart Fort, a fortified police station (today the location of Latrun). One relatively complex operation was the takeover of the radio station in Ramallah, on May 17, 1944.
One symbolic act by the Irgun happened before Yom Kippur of 1944. They plastered notices around town, warning that no British officers should come to the Western Wall on Yom Kippur, and for the first time since the mandate began no British police officers were there to prevent the Jews from the traditional Shofar blowing at the end of the fast. After the fast that year the Irgun attacked four police stations in Arab settlements. In order to obtain weapons, the Irgun carried out "confiscation" operations – they robbed British armouries and smuggled stolen weapons to their own hiding places. During this phase of activity the Irgun also cut all of its official ties with the New Zionist Organization, so as not to tie their fate in the underground organization.
Begin wrote in his memoirs, "The Revolt":
Underground exiles.
In October 1944 the British began expelling hundreds of arrested Irgun and Lehi members to detention camps in Africa. 251 detainees from Latrun were flown on thirteen planes, on October 19 to a camp in Asmara, Eritrea. Eleven additional transports were made. Throughout the period of their detention, the detainees often initiated rebellions and hunger strikes. Many escape attempts were made until July 1948 when the exiles were returned to Israel. While there were numerous successful escapes from the camp itself, only nine men actually made it back all the way. One noted success was that of Yaakov Meridor, who escaped nine times before finally reaching Europe in April 1948. These tribulations were the subject of his book "Long is the Path to Freedom: Chronicles of one of the Exiles".
Hunting Season.
On November 6, 1944, Lord Moyne, British Deputy Resident Minister of State in Cairo was assassinated by Lehi members Eliyahu Hakim and Eliyahu Bet-Zuri. This act raised concerns within the Yishuv from the British regime's reaction to the underground's violent acts against them. Therefore the Jewish Agency decided on starting a "Hunting Season", known as the "saison", (from the French "la saison de chasse").
The Irgun's recuperation was noticeable when it began to renew its cooperation with the Lehi in May 1945, when it sabotaged oil pipelines, telephone lines and railroad bridges. All in all, over 1,000 members of the Irgun and Lehi were arrested and interred in British camps during the "Saison". Eventually the Hunting Season died out, and there was even talk of cooperation with the Haganah leading to the formation of the Jewish Resistance Movement.
The Jewish Resistance Movement.
Towards the end of July 1945 the Labour party in Britain was elected to power. The Yishuv leadership had high hopes that this would change the anti-Zionist policy that the British maintained at the time. However, these hopes were quickly dashed when the government limited Jewish immigration, with the intention that the population of Mandatory Palestine (the land west of the Jordan River) would not be more than one third of the total. This, along with the stepping up of arrests and their pursuit of underground members and illegal immigration organizers led to the formation of the Jewish Resistance Movement. This body consolidated the armed resistance to the British of the Irgun, Lehi, and Haganah. For ten months the Irgun and the Lehi cooperated and they carried out nineteen attacks and defense operations. The Haganah and Palmach carried out ten such operations. The Haganah also assisted in landing 13,000 illegal immigrants.
Tension between the underground movements and the British increased with the increase in operations. On April 23, 1945 an operation undertaken by the Irgun to gain weapons from the Tegart fort at Ramat Gan resulted in a firefight. One Irgun member was killed and his body was later hanged on the fort's fence. Another fighter, Yizchak Bilu, was killed as well in a diversionary ploy – an explosive device fell out of his hand, and he leapt onto it in order to save his comrades, who were also carrying explosives. A third fighter, Dov Gruner, was caught. He stood trial and was sentenced to be death by hanging, refusing to sign a pardon request.
In 1946, British relations with the Yishuv worsened, building up to Operation Agatha of June 29. The authorities ignored the Anglo-American Committee of Inquiry's recommendation to allow 100,000 Jews into Palestine at once. As a result of the discovery of documents tying the Jewish Agency to the Jewish Resistance Movement, the Irgun was asked to speed up the plans for the King David Hotel bombing of July 22. The hotel was where the documents were located, the base for the British Secretariat, the military command and a branch of the Criminal Investigation Division of the police. The Irgun later claimed to have sent a warning that was ignored. 91 people were killed in the attack where a 350 kg bomb was placed in the basement of the hotel and caused a large section of it to collapse. Only 13 were British soldiers.
Further struggle against the British.
The King David Hotel bombing and the arrest of Jewish Agency and other Yishuv leaders as part of Operation Agatha caused the Haganah to cease their armed activity against the British. Yishuv and Jewish Agency leaders were released from prison. From then until the end of the British mandate, resistance activities were led by the Irgun and Lehi. In early September 1946 the Irgun renewed its attacks against civil structures, railroads, communication lines and bridges. One operation was the attack on the train station in Jerusalem, in which Meir Feinstein was arrested and later committed suicide awaiting execution. According to the Irgun these sort of armed attacks were legitimate, since the trains primarily served the British, for redeployment of their forces. The Irgun also publicized leaflets, in three languages, not to use specific trains in danger of being attacked. For a while the British stopped train traffic at night. The Irgun also carried out repeated attacks against military and police traffic using disguised, electronically-detonated roadside mines which could be detonated by an operator hiding nearby as a vehicle passed, carried out arms raids against military bases and police stations (often disguised as British soldiers), launched bombing, shooting, and mortar attacks against military and police installations and checkpoints, and robbed banks to gain funds as a result of losing access to Haganah funding following the collapse of the Jewish Resistance Movement.
On October 31, 1946, in response to the British barring entry of Jews from Palestine, the Irgun blew up the British embassy in Rome, a center of British efforts to monitor and stop Jewish immigration. The Irgun also carried out a few other operations in Europe: a British troop train was derailed and an attempt against another troop train failed. An attack on a British officers club in Vienna took place in 1947, and an attack on another British officer's club in Vienna and a sergeant's club in Germany took place in 1948.
In December 1946 a sentence of 18 years and 18 beatings was handed down to a young Irgun member. The Irgun made good on a threat they made and after the detainee was whipped, Irgun members kidnapped British officers and beat them in public. The operation, known as the "Night of the Beatings" brought an end to British punitive beatings. The British, taking these acts seriously, moved many British families in Palestine into the confines of military bases, and some moved home.
On February 14, 1947, Ernest Bevin announced that the Jews and Arabs would not be able to agree on any British proposed solution for the land, and therefore the issue must be brought to the United Nations (UN) for a final decision. The Yishuv thought of the idea to transfer the issue to the UN as a British attempt to achieve delay while a UN inquiry commission would be established, and its ideas discussed, and all the while the Yishuv would weaken. Foundation for Immigration B increased the number of ships bringing in Jewish refugees. The British still strictly enforced the policy of limited Jewish immigration and illegal immigrants were placed in detention camps in Cyprus, which increased the anger of the Jewish community towards the mandate government.
The Irgun stepped up its activity and from February 19 until March 3 it attacked 18 British military camps, convoy routes, vehicles, and other facilities. The most notable of these attacks was the bombing of a British officer's club located in Goldschmidt House in Jerusalem, which was in a heavily guarded security zone. Covered by machine-gun fire, an Irgun assault team in a truck penetrated the security zone and lobbed explosives into the building. Thirteen people, including two officers, were killed. As a result, martial law was imposed over much of the country, enforced by approximately 20,000 British soldiers. Despite this, attacks continued throughout the martial law period. The most notable one was an Irgun attack against the Royal Army Pay Corps base at the Schneller Orphanage, in which a British soldier was killed. 
Throughout its struggle against the British, the Irgun sought to publicize its cause around the world. By humiliating the British, it attempted to focus global attention on Palestine, hoping that any British overreaction would be widely reported, and thus result in more political pressure against the British. Begin described this strategy as turning Palestine into a "glass house". The Irgun also re-established many representative offices internationally, and by 1948 operated in 23 states. In these countries the Irgun sometimes acted against the local British representatives or led public relations campaigns against Britain. According to Bruce Hoffman: ""In an era long before the advent of 24/7 global news coverage and instantaneous satellite-transmitted broadcasts, the Irgun deliberately attempted to appeal to a worldwide audience far beyond the immediate confines of its local struggle, and beyond even the ruling regime's own homeland"."
The Acre Prison break.
On April 16, 1947, Dov Gruner, Yehiel Drezner, Eliezer Kashani, and Mordechai El'kachi were hanged, while singing Hatikvah. On April 21 Meir Feinstein and Lehi member Moshe Barazani blew themselves up, using an improvised explosive device (IED), hours before their scheduled hanging. And on May 4 one of the Irgun's largest operations took place – the raid of the prison in the citadel in Acre. The operation was carried out by 23 men, commanded by Dov Cohen – AKA "Shimshon", along with the help of the Irgun and Lehi prisoners inside the prison. The raid allowed 41 underground members to escape, although some were caught outside of the prison, and some were killed in the escape. Along with the underground movement members, other criminals – including 214 Arabs – also escaped. Five of the attackers were caught and three of them – Avshalom Haviv, Meir Nakar, and Yaakov Weiss, were sentenced to death.
The Sergeants affair.
After the death sentences of the three were confirmed, the Irgun tried to save them by kidnapping hostages — British sergeants Clifford Martin and Mervyn Paice — in the streets of Netanya. British forces closed off and combed the area in search of the two, but did not find them. On July 29, 1947, in the afternoon, Meir Nakar, Avshalom Haviv, and Yaakov Weiss were executed. Approximately thirteen hours later the hostages were hanged in retaliation by the Irgun and their bodies, booby-trapped with an explosive, afterwards strung up from trees in woodlands south of Netanya. This action caused an outcry in Britain and was condemned both there and by Jewish leaders in Palestine.
This episode has been given as a major influence on the British decision to terminate the Mandate and leave Palestine. The United Nations Special Committee on Palestine (UNSCOP) was also influenced by this and other actions. At the same time another incident was developing – the events of the ship "Exodus 1947". The 4,500 Holocaust survivors on board were not allowed to enter Palestine. UNSCOP also covered the events. Some of its members were even present at Haifa port when the putative immigrants were forcefully removed from their ship (later found to have been rigged with an IED by some of its passengers) onto the deportation ships, and later commented that this strong image helped them press for an immediate solution for Jewish immigration and the question of Palestine.
Two weeks later, the House of Commons convened for a special debate on events in Palestine, and concluded that their soldiers should be withdrawn as soon as possible.
The 1948 Palestine War.
UNSCOP's conclusion was a unanimous decision to end the British mandate and majority opinion to divide the Mandatory Palestine (the land west of the Jordan River) between a Jewish state and an Arab state. During the UN's deliberations regarding the committee's recommendations the Irgun avoided initiating any attacks, so as not to influence the UN negatively on the idea of a Jewish state. On November 29 the UN General Assembly voted in favor of ending the mandate and establishing two states on the land. That very same day the Irgun and the Lehi renewed their attacks on British targets. The next day the local Arabs began attacking the Jewish community, thus beginning the first stage of the 1948 Palestine War. The first attacks on Jews were in Jewish neighborhoods of Jerusalem, in and around Jaffa, Bat Yam, Holon, and the Ha'Tikvah neighborhood in Tel Aviv.
In the autumn of 1947 the Irgun membership was approximately 4,000 people. The goal of the organization at that point was the conquest of the land between the Jordan River and the Mediterranean Sea for the sake of the future Jewish state and preventing the Arab Legion from driving out the Jewish community. The Irgun became almost an overt organization, establishing military bases in Ramat Gan and Petah Tikva. It began recruiting openly, thus significantly increasing in size. During the war the Irgun fought alongside the Lehi and the Haganah in the front against the Arab attacks. At first the Haganah maintained a defensive policy, as it had until then, but after the Convoy of 35 incident it completely abandoned its policy of restraint: "Distinguishing between individuals is no longer possible, for now – it is a war, and the even the innocent shall not be absolved."
The Irgun also began carrying out reprisal missions, as it had under David Raziel's command. At the same time though, it published announcements calling on the Arabs to lay down their weapons and maintain a ceasefire:
However the mutual attacks continued. The Irgun attacked the Arab villages of Tira near Haifa, Yehudiya ('Abassiya) in the center, and Shuafat by Jerusalem. The Irgun also attacked in the Wadi Rushmiya neighborhood in Haifa and Abu Kabir in Jaffa. On December 29 Irgun units arrived by boat to the Jaffa shore and a gunfight between them and Arab gangs ensued. The following day a bomb was thrown from a speeding Irgun car at a group of Arab men waiting to be hired for the day at the Haifa oil refinery, resulting in seven Arabs killed, and dozens injured. In response, some Arab workers attacked Jews in the area, killing 41. This sparked a Haganah response in Balad al-Sheykh, which resulted in the deaths of 60 civilians. The Irgun's goal in the fighting was to move the battles from Jewish populated areas to Arab populated areas. On January 1, 1948 the Irgun attacked again in Jaffa, its men entering the city dressed as British troops; later in the month it attacked in Beit Nabala, a base for many Arab fighters. On 5 January 1948 the Irgun detonated a lorry bomb outside Jaffa's Ottoman built Town Hall, killing 14 and injuring 19. In Jerusalem, two days later, Irgun members in a stolen police van rolled a barrel bomb into a large group of civilians who were waiting for a bus by the Jaffa Gate, killing around sixteen. In the pursuit that followed three of the attackers were killed and two taken prisoner.
On 6 April 1948, the Irgun raided the British Army camp at Pardes Hanna killing six British soldiers and their commanding officer.
The Deir Yassin massacre was carried out in a village west of Jerusalem that had signed a non-belligerency pact with its Jewish neighbors and the Haganah, and repeatedly had barred entry to foreign irregulars. On 9 April approximately 120 Irgun and Lehi members began an operation to capture the village. During the operation, the villagers fiercely resisted the attack, and a battle broke out. In the end, the Irgun and Lehi forces advanced gradually through house-to-house fighting. The village was only taken after the Irgun began systematically dynamiting houses, and after a Palmach unit intervened and employed mortar fire to silence the villagers' sniper positions. The operation resulted in five Jewish fighters dead and 40 injured. Some 100 to 120 villagers were also killed.
There are allegations that Irgun and Lehi forces committed war crimes during and after the capture of the village. These allegations include reports that fleeing individuals and families were fired at, and prisoners of war were killed after their capture. A Haganah report writes:
Some say that this incident was an event that accelerated the Arab exodus from Palestine.
The Irgun cooperated with the Haganah in the conquest of Haifa. At the regional commander's request, on April 21 the Irgun took over an Arab post above Hadar Ha'Carmel as well as the Arab neighborhood of Wadi Nisnas, adjacent to the Lower City.
The Irgun acted independently in the conquest of Jaffa (part of the proposed Arab State according to the UN Partition Plan). On April 25 Irgun units, about 600 strong, left the Irgun base in Ramat Gan towards Arab Jaffa. Difficult battles ensued, and the Irgun faced resistance from the Arabs as well as the British. Under the command of Amichai "Gidi" Paglin, the Irgun's chief operations officer, the Irgun captured the neighborhood of Manshiya, which threatened the city of Tel Aviv. Afterwards the force continued to the sea, towards the area of the port, and using mortars, shelled the southern neighborhoods.
 In his report concerning the fall of Jaffa the local Arab military commander, Michel Issa, writes: 'Continuous shelling with mortars of the city by Jews for four days, beginning 25 April, [...] caused inhabitants of city, unaccustomed to such bombardment, to panic and flee.' According to Morris the shelling was done by the Irgun. Their objective was 'to prevent constant military traffic in the city, to break the spirit of the enemy troops [and] to cause chaos among the civilian population in order to create a mass flight'. High Commissioner Cunningham wrote a few days later 'It should be made clear that IZL attack with mortars was indiscriminate and designed to create panic among the civilian inhabitants'. The British demanded the evacuation of the newly conquered city, and militarily intervened. Heavy British shelling against Irgun positions in Jaffa failed to dislodge them, and when British armor pushed into the city, the Irgun resisted; a bazooka team managed to knock out one tank, buildings were blown up and collapsed onto the streets as the armor advanced, and Irgun men crawled up and tossed live dynamite sticks onto the tanks. The British withdrew, and opened negotiations with the Jewish authorities. The Irgun, which had previously agreed with the Haganah that British pressure would not lead to withdrawal from Jaffa and that custody of captured areas would be turned over to the Haganah. The city ultimately fell on May 13 after Haganah forces entered the city and took control of the rest of the city, from the south – part of the "Hametz Operation" which included the conquest of a number of villages in the area. The battles in Jaffa were a great victory for the Irgun. This operation was the largest in the history of the organization, which took place in highly built up area that had many militants in shooting positions. During the battles explosives were used in order to break into homes and continue forging a way though them. Furthermore, this was the first occasion in which the Irgun had directly fought British forces, reinforced with armor and heavy weaponry. The city began these battles with an Arab population estimated at 70,000, which shrank to some 4,100 Arab residents by the end of major hostilities. Since the Irgun captured the neighborhood of Manshiya on its own, causing the flight of many of Jaffa's residents, the Irgun took credit for the conquest of Jaffa. It had lost 42 dead and about 400 wounded during the battle.
Integration with the IDF and the Altalena Affair.
On May 14, 1948 the establishment of the State of Israel was proclaimed. The declaration of independence was followed by the establishment of the Israel Defense Forces (IDF), and the process of absorbing all military organizations into the IDF started. On June 1, an agreement had been signed Between Menachem Begin and Yisrael Galili for the absorption of the Irgun into the IDF. One of the clauses stated that the Irgun had to stop smuggling arms. Meanwhile in France, Irgun representatives purchased a ship, renamed "Altalena" (a pseudonym of Ze'ev Jabotinsky), and weapons. The ship sailed on June 11 and arrived at the Israeli coast on June 20 in violation of the four-week ceasefire agreement in the ongoing war with the neighbouring Arab states and the United Nations Security Council Resolution 50.
When the ship arrived the Israeli government, headed by Ben-Gurion, was adamant in its demand that the Irgun surrender and hand over all of the weapons. Ben-Gurion said: "We must decide whether to hand over power to Begin or to order him to cease his activities. If he does not do so, we will open fire! Otherwise, we must decide to disperse our own army."
There were two confrontations between the newly formed IDF and the Irgun: when "Altalena" reached Kfar Vitkin in the late afternoon of Sunday, June 20 many Irgun militants, including Begin, waited on the shore. A clash with the Alexandroni Brigade, commanded by Dan Even (Epstein), occurred. Fighting ensued and there were a number of casualties on both sides. The clash ended in a ceasefire and the transfer of the weapons on shore to the local IDF commander, and with the ship, now reinforced with local Irgun members, including Begin, sailing to Tel Aviv, where the Irgun had more supporters.
Many Irgun members, who joined the IDF earlier that month, left their bases and concentrated on the Tel Aviv beach. A confrontation between them and the IDF units started. In response, Ben-Gurion ordered Yigael Yadin (acting Chief of Staff) to concentrate large forces on the Tel Aviv beach and to take the ship by force. Heavy guns were transferred to the area and at four in the afternoon, Ben-Gurion ordered the shelling of the "Altalena". One of the shells hit the ship, which began to burn.
Sixteen Irgun fighters were killed in the confrontation with the army; six were killed in the Kfar Vitkin area and ten on Tel Aviv beach. Three IDF soldiers were killed: two at Kfar Vitkin and one in Tel Aviv.
After the shelling of the "Altalena", more than 200 Irgun fighters were arrested. Most of them were freed several weeks later. The Irgun militants were then fully integrated with the IDF and not kept in separate units.
The initial agreement for the integration of the Irgun into the IDF did not include Jerusalem, which was under siege. The Irgun operated an armed group known as the "Jerusalem Battalion", numbering around 400 fighters. Following the assassination of UN Envoy for Peace Folke Bernadotte by the LEHI in September 1948, this separate unit collapsed and integrated into the IDF.
Criticism.
References to the Irgun as a terrorist organization came from sources including the Anglo-American Committee of Inquiry, newspapers and a number of prominent world and Jewish figures.
Leaders within the mainstream Jewish organizations, the Jewish Agency, Haganah and Histadrut, as well as the British authorities, routinely condemned Irgun operations as terrorism and branded it an illegal organization as a result of the group's attacks on civilian targets. However, privately at least the Haganah kept a dialogue with the dissident groups.
Ironically, in early 1947, "the British army in Mandate Palestine banned the use of the term 'terrorist' to refer to the Irgun zvai Leumi ... because it implied that British forces had reason to be terrified."
Irgun attacks prompted a formal declaration from the World Zionist Congress in 1946, which strongly condemned "the shedding of innocent blood as a means of political warfare."
The Israeli government, in September 1948, acting in response to the assassination of Count Folke Bernadotte, outlawed the Irgun and Lehi groups, declaring them terrorist organizations under the Prevention of Terrorism Ordinance.
In 1948, "The New York Times" published a letter signed by a number of prominent Jewish figures including Hannah Arendt, Albert Einstein, Sidney Hook, and Rabbi Jessurun Cardozo, which described Irgun as "a terrorist, right-wing, chauvinist organization in Palestine". The letter went on to state that Irgun and the Stern gang "inaugurated a reign of terror in the Palestine Jewish community. Teachers were beaten up for speaking against them, adults were shot for not letting their children join them. By gangster methods, beatings, window-smashing, and widespread robberies, the terrorists intimidated the population and exacted a heavy tribute."
Soon after World War II, Winston Churchill said "we should never have stopped immigration before the war", but that the Irgun were "the vilest gangsters" and that he would "never forgive the Irgun terrorists."
A US military intelligence report, dated January 1948, described Irgun recruiting tactics amongst Displaced Persons (DP) in the camps across Germany:
Clare Hollingworth, the "Daily Telegraph" and "The Scotsman" correspondent in Jerusalem during 1948 wrote several outspoken reports after spending several weeks in West Jerusalem:
'The shopkeepers are afraid not so much of shells as of raids by Irgun Zvai Leumi and the Stern Gang. These young toughs, who are beyond whatever law there is have cleaned out most private houses of the richer classes & started to prey upon the shopkeepers.'—Clare Hollingworth reporting on West Jerusalem June 2, 1948
In 2006, Simon McDonald, the British ambassador in Tel Aviv, and John Jenkins, the Consul-General in Jerusalem, wrote in response to a pro-Irgun commemoration of the King David Hotel bombing: "We do not think that it is right for an act of terrorism, which led to the loss of many lives, to be commemorated." They also called for the removal of plaques at the site which presented as a fact that the deaths were due to the British ignoring warning calls. The plaques, in their original version, read:
Warning phone calls had been made urging the hotel's occupants to leave immediately. For reasons known only to the British the hotel was not evacuated and after 25 minutes the bombs exploded, and to the Irgun's regret and dismay 91 persons were killed.
McDonald and Jenkins said that no such warning calls were made, adding that even if they had, "this does not absolve those who planted the bomb from responsibility for the deaths."
"Ha'aretz" columnist and Israeli historian Tom Segev wrote of the Irgun: "In the second half of 1940, a few members of the Irgun Zvai Leumi (National Military Organization) – the anti-British terrorist group sponsored by the Revisionists and known by its acronym Etzel, and to the British simply as the Irgun – made contact with representatives of Fascist Italy, offering to cooperate against the British."
Alan Dershowitz wrote in his book "The Case for Israel" that unlike the Haganah, the policy of the Irgun had been to encourage the flight of local Arabs.

</doc>
<doc id="15408" url="http://en.wikipedia.org/wiki?curid=15408" title="Isoroku Yamamoto">
Isoroku Yamamoto

Isoroku Yamamoto (山本 五十六, Yamamoto Isoroku, April 4, 1884 – April 18, 1943) was a Japanese Marshal Admiral and the commander-in-chief of the Combined Fleet during World War II until his death.
Yamamoto held several important posts in the Imperial Japanese Navy, and undertook many of its changes and reorganizations, especially its development of naval aviation. He was the commander-in-chief during the decisive early years of the Pacific War and so was responsible for major battles such as Pearl Harbor and Midway. He died when American codebreakers identified his flight plans and his plane was shot down. His death was a major blow to Japanese military morale during World War II.
Family background.
Yamamoto was born Isoroku Takano (高野 五十六, "Takano Isoroku") in Nagaoka, Niigata. His father was Sadayoshi Takano (高野 貞吉), an intermediate-rank "samurai" of the Nagaoka Domain. "Isoroku" is an old Japanese term meaning "56"; the name referred to his father's age at Isoroku's birth.
In 1916, Isoroku was adopted into the Yamamoto family (another family of former Nagaoka samurai) and took the Yamamoto name. It was a common practice for samurai families lacking sons to adopt suitable young men in this fashion to carry on the family name, the rank and the income that comes with it. In 1918 Isoroku married Reiko Mihashi, with whom he had two sons and two daughters.
Early career.
After graduating from the Imperial Japanese Naval Academy in 1904, Yamamoto served on the armored cruiser "Nisshin" during the Russo-Japanese War. He was wounded at the Battle of Tsushima, losing two fingers (the index and middle fingers) on his left hand, as the cruiser was hit repeatedly by the Russian battleline. He returned to the Naval Staff College in 1914, emerging as a Lieutenant Commander in 1916.
1920s and 1930s.
Yamamoto was part of the Japanese Navy establishment, who were rivals of the more aggressive Army establishment, especially the officers of the Kwantung Army. As such he promoted a policy of a strong fleet to project force through gunboat diplomacy, rather than a fleet used primarily for transport of invasion land forces, as some of his political opponents in the army wanted. This stance led him to oppose the invasion of China. He also opposed war against the United States partly because of his studies at Harvard University (1919–1921) and his two postings as a naval attaché in Washington, D.C. He learned to speak fluent English as a result.
Yamamoto traveled extensively in the United States during his tour of duty there, where he studied American customs and business practices.
He was promoted to Captain in 1923. On April 13, 1924 at the rank of captain, he was part of the Japanese delegation visiting the U.S. Naval War College. Later that year, he changed his specialty from gunnery to naval aviation. His first command was the cruiser "Isuzu" in 1928, followed by the aircraft carrier "Akagi".
He participated in the second London Naval Conference of 1930 as a Rear Admiral and the 1934 London Naval Conference as a Vice Admiral, as the growing military influence on the government at the time deemed that a career military specialist needed to accompany the diplomats to the arms limitations talks. Yamamoto was a strong proponent of naval aviation, and served as head of the Aeronautics Department before accepting a post as commander of the First Carrier Division. Yamamoto personally opposed the invasion of Manchuria in 1931, the subsequent land war with China (1937), and the 1940 Tripartite Pact with Nazi Germany and Fascist Italy. As Deputy Navy Minister, he apologized to United States Ambassador Joseph C. Grew for the bombing of the gunboat USS "Panay" in December 1937. These issues made him a target of assassination threats by pro-war militarists.
Throughout 1938, many young army and naval officers began to speak publicly against Yamamoto and certain other Japanese admirals such as Mitsumasa Yonai and Shigeyoshi Inoue for their strong opposition towards a Tripartite pact with Nazi Germany for reportedly being against "Japan's natural interests." Yamamoto himself received a steady stream of hate mail and death threats from Japanese nationalists but his reaction to the prospect of death by assassination was passive and accepting. The Admiral wrote:
To die for Emperor and Nation is the highest hope of a military man. After a brave hard fight the blossoms are scattered on the fighting field. But if a person wants to take a life instead, still the fighting man will go to eternity for Emperor and country. One man's life or death is a matter of no importance. All that matters is the Empire. As Confucius said, "They may crush cinnabar, yet they do not take away its color; one may burn a fragrant herb, yet it will not destroy the scent." They may destroy my body, yet they will not take away my will.
The Japanese army, annoyed at Yamamoto's unflinching opposition to a Rome-Berlin-Tokyo treaty, dispatched military police to "guard" Yamamoto; this was an attempt by the Army to keep an eye on him. He was later reassigned from the Navy Ministry to sea as the Commander-in-Chief of the Combined Fleet on (August 30, 1939). This was done as one of the last acts of the then-acting Navy Minister Mitsumasa Yonai, under Baron Hiranuma's short-lived administration partly to make it harder for assassins to target Yamamoto; Yonai was certain that if Yamamoto remained ashore, he would be killed before the year (1939) ended.
1940–1941.
Yamamoto was promoted to Admiral on November 15, 1940. This, in spite of the fact that when Hideki Tōjō was appointed Prime Minister on October 18, 1941, many political observers thought that Yamamoto's career was essentially over. Tōjō had been Yamamoto's old opponent from the time when the latter served as Japan's deputy navy minister and Tōjō was the prime mover behind Japan's takeover of Manchuria. It was believed that Yamamoto would be appointed to command the Yokosuka Naval Base, "a nice safe demotion with a big house and no power at all." After the new Japanese cabinet was announced, however, Yamamoto found himself left alone in his position despite his open conflicts with Tōjō and other members of the army's oligarchy who favored war with the European powers and America. Two of the main reasons for Yamamoto's political survival were his immense popularity within the fleet, where he commanded the respect of his men and officers, and his close relations with the imperial family. He also had the acceptance by Japan's naval hierarchy:
"There was no officer more competent to lead the Combined Fleet to victory than Admiral Yamamoto. His daring plan for the Pearl Harbor attack had passed through the crucible of the Japanese naval establishment, and after many expressed misgivings, his fellow admirals had realized that Yamamoto spoke no more than the truth when he said that Japan's hope for victory in this [upcoming] war was limited by time and oil. Every sensible officer of the navy was well aware of the perennial oil problems. Also, it had to be recognized that if the enemy could seriously disturb Japanese merchant shipping, then the fleet would be endangered even more."
Consequently, Yamamoto stayed in his post. With Tōjō now in charge of Japan's highest political office, it became clear the army would lead the navy into a war about which Yamamoto had serious reservations. He wrote to an ultranationalist:
Should hostilities once break out between Japan and the United States, it would not be enough that we take Guam and the Philippines, nor even Hawaii and San Francisco. To make victory certain, we would have to march into Washington and dictate the terms of peace in the White House. I wonder if our politicians (who speak so lightly of a Japanese-American war) have confidence as to the final outcome and are prepared to make the necessary sacrifices.
This quote was spread by the militarists, minus the last sentence, where it was interpreted in America as a boast that Japan would conquer the entire continental United States. The omitted sentence showed Yamamoto's counsel of caution towards a war that could cost Japan dearly. Nevertheless, Yamamoto accepted the reality of impending war and planned for a quick victory by destroying the US fleet at Pearl Harbor in a preventive strike while simultaneously thrusting into the oil and rubber resource rich areas of Southeast Asia, especially the Dutch East Indies, Borneo and Malaya. In naval matters, Yamamoto opposed the building of the super-battleships "Yamato" and "Musashi" as an unwise investment of resources.
Yamamoto was responsible for a number of innovations in Japanese naval aviation. Although remembered for his association with aircraft carriers due to Pearl Harbor and Midway, Yamamoto did more to influence the development of land-based naval aviation, particularly the Mitsubishi G3M and G4M medium bombers. His demand for great range and the ability to carry a torpedo was intended to conform to Japanese conceptions of attriting the American fleet as it advanced across the Pacific in war. The planes did achieve long range, but long-range fighter escorts were not available. These planes were lightly constructed and when fully fueled, they were especially vulnerable to enemy fire. This earned the G4M the sardonic nickname "the Flying Cigarette Lighter." Yamamoto would eventually die in one of these aircraft.
The range of the G3M and G4M contributed to a demand for great range in a fighter aircraft. This partly drove the requirements for the A6M Zero which was as noteworthy for its range as for its maneuverability. Both qualities were again purchased at the expense of light construction and flammability that later contributed to the A6M's high casualty rates as the war progressed.
As Japan moved toward war during 1940, Yamamoto gradually moved toward strategic as well as tactical innovation, again with mixed results. Prompted by talented young officers such as Lieutenant Commander Minoru Genda, Yamamoto approved the reorganization of Japanese carrier forces into the First Air Fleet, a consolidated striking force that gathered Japan's six largest carriers into one unit. This innovation gave great striking capacity, but also concentrated the vulnerable carriers into a compact target; both boon and bane would be realized in war. Yamamoto also oversaw the organization of a similar large land-based organization in the 11th Air Fleet, which would later use the G3M and G4M to neutralize American air forces in the Philippines and sink the British Force "Z".
In January 1941, Yamamoto went even further and proposed a radical revision of Japanese naval strategy. For two decades, in keeping with the doctrine of Captain Alfred T. Mahan, the Naval General Staff had planned in terms of Japanese light surface forces, submarines and land-based air units whittling down the American Fleet as it advanced across the Pacific until the Japanese Navy engaged it in a climactic "Decisive Battle" in the northern Philippine Sea (between the Ryukyu Islands and the Marianas Islands), with battleships meeting in the traditional exchange between battle lines.
Correctly pointing out this plan had never worked even in Japanese war games, and painfully aware of American strategic advantages in military productive capacity, Yamamoto proposed instead to seek a decision with the Americans by first reducing their forces with a preventive strike, and following it with a "Decisive Battle" fought offensively, rather than defensively. Yamamoto hoped, but probably did not believe, if the Americans could be dealt such terrific blows early in the war, they might be willing to negotiate an end to the conflict. As it turned out, however, the note officially breaking diplomatic relations with the United States was delivered late, and he correctly perceived the Americans would be resolved upon revenge and unwilling to negotiate. At the end of the attack upon Pearl Harbor, upon hearing of the mis-timing of the communique breaking diplomatic relations with the United States earlier that day, it is reputed Yamamoto said, "I fear all we have done today is to awaken a great, sleeping giant and fill him with a terrible resolve."; however, there is no documented evidence the statement was made.
The Naval General Staff proved reluctant to go along and Yamamoto was eventually driven to capitalize on his popularity in the fleet by threatening to resign to get his way. Admiral Osami Nagano and the Naval General Staff eventually caved in to this pressure, but only insofar as approving the attack on Pearl Harbor.
The First Air Fleet commenced preparations for the Pearl Harbor Raid, solving a number of technical problems along the way, including how to launch torpedoes in the shallow water of Pearl Harbor and how to craft armor-piercing bombs by machining down battleship gun projectiles.
Attack on Pearl Harbor.
As Yamamoto had planned, the First Air Fleet of six carriers commenced hostilities against the Americans on December 7, 1941, launching 353 aircraft against Pearl Harbor in two waves. The attack was a complete success according to the parameters of the mission which sought to sink at least four American battleships and prevent the U.S. Fleet from interfering in Japan's southward advance for at least six months. American aircraft carriers were also considered a choice target, but these were not in port at the time of the attack.
In the end, five American battleships were sunk, three were damaged, and eleven other cruisers, destroyers and auxiliaries were sunk or seriously damaged. The Japanese lost only 29 aircraft, while 74 were damaged from anti-aircraft fire from the ground. The damaged aircraft were disproportionately dive and torpedo bombers, seriously impacting available firepower to exploit the first two waves' success, so the commander of the First Air Fleet, Naval Lieutenant-General Chuichi Nagumo, withdrew. Yamamoto later lamented Nagumo's failure to seize the initiative to seek out and destroy the American carriers, absent from the harbor, or further bombard various strategically important facilities on Oahu. Nagumo had absolutely no idea where the American carriers might be, and remaining on station while his forces cast about looking for them ran the risk of his own forces being found first and attacked while his aircraft were absent searching. In any case, insufficient daylight remained after recovering the aircraft from the first two waves for the carriers to launch and recover a third before dark, and Nagumo's escorting destroyers lacked the fuel capacity for him to loiter long. Much has been made of Yamamoto's hindsight, but (in keeping with Japanese military tradition not to criticize the commander on the spot), he did not punish Nagumo in any way for his withdrawal.
On the strategic level, the attack was a disaster for Japan, rousing American passions for revenge due to it being a "sneak attack". The shock of the attack coming in an unexpected place, with such devastating results and without the expected "fair play" of a declaration of war galvanized the American public's determination to avenge the attack. When asked by Prime Minister Fumimaro Konoe in mid-1941 concerning the outcome of a possible war with the United States, Yamamoto made a well-known and prophetic statement: If ordered to fight, "I shall run wild considerably for the first six months or a year, but I have utterly no confidence for the second and third years." His prediction would be vindicated as Japan easily conquered territories and islands for the first six months of the war until it suffered a shattering defeat at the Battle of Midway on June 4–7, 1942, which ultimately tilted the balance of power in the Pacific towards the U.S.
As a strategic blow intended to prevent American interference in the Dutch East Indies for six months, the Pearl Harbor attack was a success, but unbeknownst to Yamamoto, it was a pointless one. The U.S. Navy had abandoned any intention of attempting to charge across the Pacific towards the Philippines at the outset of war in 1935 (in keeping with the evolution of War Plan Orange). In 1937, the U.S. Navy had further determined even fully manning the fleet to wartime levels could not be accomplished in less than six months, and myriad other logistic assets needed to execute a trans-Pacific movement simply did not exist and would require two years to construct after the onset of war. In 1940, U.S. Chief of Naval Operations, Admiral Harold Stark had penned a Plan Dog memorandum, which emphasized a defensive war in the Pacific while the U.S. concentrated on defeating Nazi Germany first, and consigned Admiral Husband Kimmel's Pacific Fleet to merely keeping the Imperial Japanese Navy (IJN) out of the eastern Pacific and away from the shipping lanes to Australia. Moreover, it is in question whether the U.S. would have gone to war at all had Japan only attacked British and Dutch possessions in the Far East.
December 1941 to May 1942.
With the American Fleet largely neutralized at Pearl Harbor, Yamamoto's Combined Fleet turned to the task of executing the larger Japanese war plan devised by the Imperial Japanese Army and Navy General Staff. The First Air Fleet proceeded to make a circuit of the Pacific, striking American, Australian, Dutch and British installations from Wake Island to Australia to Ceylon in the Indian Ocean. The 11th Air Fleet caught the American 5th Air Force on the ground in the Philippines hours after Pearl Harbor, and then proceeded to sink the British Force "Z" (battleship HMS "Prince of Wales" and battlecruiser HMS "Repulse") underway at sea.
Under Yamamoto's able subordinates, Naval Lieutenant-Generals Jisaburō Ozawa, Nobutake Kondō and Ibō Takahashi, the Japanese swept the inadequate remaining American, British, Dutch and Australian naval assets from the Dutch East Indies in a series of amphibious landings and surface naval battles that culminated in the Battle of the Java Sea on February 27, 1942. With the occupation of the Dutch East Indies, and the reduction of the remaining American positions in the Philippines on the Bataan Peninsula and Corregidor island, the Japanese had secured their oil- and rubber-rich "Southern Resources Area".
By late March, having achieved their initial aims with surprising speed and little loss (albeit against enemies ill-prepared to resist them), the Japanese paused to consider their next moves. Yamamoto and a few Japanese military leaders and officials waited, hoping that the United States or Great Britain would negotiate for an armistice or a peace treaty to end the war in their favour. But when the British, as well as the Americans, expressed no interest in negotiating with Japan for any cease fire, the Japanese thoughts turned to securing their newly seized territory and acquiring more with an eye toward attempting to force one or more of their enemies out of the war.
Competing plans were developed at this stage, including thrusts to the west against India, the south against Australia and the east against the United States. Yamamoto was involved in this debate, supporting different plans at different times with varying degrees of enthusiasm and for varying purposes, including "horse-trading" for support of his own objectives.
Plans included ideas as ambitious as invading India or Australia, or seizing Hawaii. These grandiose ventures were inevitably set aside as the army could not spare enough troops from China for the first two (which would require a minimum of 250,000 men), nor shipping to support the latter two. (Shipping was allocated separately to IJN & IJA, and jealously guarded.) Instead, the Imperial General Staff supported an army thrust into Burma in hopes of linking up with Indian Nationalists revolting against British rule, and attacks in New Guinea and the Solomon Islands designed to imperil Australia's sea line of communication with the United States. Yamamoto agitated for an offensive decisive battle in the east to finish off the American fleet, but the more conservative Naval General Staff officers were unwilling to risk it.
On April 18, in the midst of these debates, the Doolittle Raid struck Tokyo and the surrounding areas, galvanizing the threat posed by the American aircraft carriers in the minds of staff officers, and giving Yamamoto an event he could exploit to get his way as further debate over military strategy came to a quick end. The Naval General Staff agreed to Yamamoto's Midway (MI) Operation, subsequent to the first phase of the operations against Australia's link with America, and concurrent with their own plan to seize positions in the Aleutian Islands.
Yamamoto rushed planning for the Midway and Aleutians missions, while dispatching a force under Naval Major-General Takeo Takagi, including the Fifth Carrier Division (the large, new carriers "Shōkaku" and "Zuikaku"), to support the effort to seize the islands of Tulagi and Guadalcanal for seaplane and aeroplane bases, and the town of Port Moresby on Papua New Guinea's south coast facing Australia.
The Port Moresby (MO) Operation proved an unwelcome setback. Although Tulagi and Guadalcanal were taken, the Port Moresby invasion fleet was compelled to turn back when Takagi clashed with an American carrier task force in the Battle of the Coral Sea in early May. Although the Japanese sank the American carrier USS "Lexington" and damaged the , the Americans damaged the carrier "Shōkaku" so badly that she required dockyard repairs, and the Japanese lost the light carrier "Shoho". Just as importantly, Japanese operational mishaps and American fighters and anti-aircraft fire devastated the dive bomber and torpedo plane formations of both "Shōkaku"‍ '​s and "Zuikaku"‍ '​s air groups. These losses sidelined "Zuikaku" while she awaited replacement aircraft and aircrews, and saw to tactical integration and training. These two ships would be sorely missed a month later at Midway.
Battle of Midway, June 1942.
Yamamoto's plan for Midway Island was an extension of his efforts to knock the U.S. Pacific Fleet out of action long enough for Japan to fortify her defensive perimeter in the Pacific island chains. Yamamoto felt it necessary to seek an early, offensive decisive battle.
This plan was long believed to have been to draw American attention—and possibly carrier forces—north from Pearl Harbor by sending his Fifth Fleet (two light carriers, five cruisers, 13 destroyers, and four transports) against the Aleutians, raiding Dutch Harbor on Unalaska Island and invading the more distant islands of Kiska and Attu. Recent scholarship using Japanese language documents has revealed it was, rather, an unrelated venture of the Naval General Staff which Yamamoto agreed to conduct concurrently with the Midway operation, in exchange for the latter's approval.
While Fifth Fleet attacked the Aleutians, First Mobile Force (4 carriers, 2 battleships, 3 cruisers, and 12 destroyers) would raid Midway and destroy its air force. Once this was neutralized, Second Fleet (1 light carrier, 2 battleships, 10 cruisers, 21 destroyers, and 11 transports) would land 5,000 troops to seize the atoll from the American Marines.
The seizure of Midway was expected to draw the American carriers west into a trap where the First Mobile Force would engage and destroy them. Afterward, First Fleet (1 light carrier, 7 battleships, 3 cruisers and 13 destroyers), in conjunction with elements of Second Fleet, would mop up remaining American surface forces and complete the destruction of the Pacific Fleet.
To guard against mischance, Yamamoto initiated two security measures. The first was an aerial reconnaissance mission (Operation K) over Pearl Harbor to ascertain if the American carriers were there. The second was a picket line of submarines to detect the movement of the American carriers toward Midway in time for First Mobile Force, First Fleet, and Second Fleet to combine against it. In the event, the first was aborted and the second delayed until after American carriers had sortied.
The plan was a compromise and hastily prepared (apparently so it could be launched in time for the anniversary of Tsushima), but appeared well thought out, well organized, and finely timed when viewed from a Japanese viewpoint. Against four carriers, two light carriers, 11 battleships, 16 cruisers and 46 destroyers likely to be in the area of the main battle the Americans could field only three carriers, eight cruisers, and 15 destroyers. The disparity appeared crushing. Only in numbers of carrier decks, available aircraft, and submarines was there near parity between the two sides. Despite various frictions developed in the execution, it appeared—barring something extraordinary—Yamamoto held all the cards.
The Americans were able to learn of the Japanese plans thanks to code breaking of Japanese naval code D (known to the U.S. as JN-25). As a result, Admiral Chester Nimitz, the Pacific Fleet commander, was able to circumvent both of Yamamoto's security measures and position his outnumbered forces in the exact position to conduct a devastating ambush. By Nimitz's calculation, his three available carrier decks, plus Midway, gave him rough parity with Nagumo's First Mobile Force.
Following a nuisance raid by Japanese flying boats in May, Nimitz dispatched a minesweeper to guard the intended refueling point for Operation K near French Frigate Shoals, causing the reconnaissance mission to be aborted and leaving Yamamoto ignorant of whether Pacific Fleet carriers were still at Pearl Harbor. (It remains unclear why Yamamoto permitted the earlier attack, and why his submarines did not sortie sooner, as reconnaissance was essential to the success of MI.) He also dispatched his carriers toward Midway early, and they passed the intended picket line force of submarines "en route" to their station, negating Yamamoto's back-up security measure. Nimitz's carriers positioned themselves to ambush the "Kido Butai" (Striking Force) when it struck Midway. A token cruiser and destroyer force was sent toward the Aleutians, but otherwise Nimitz ignored them. On June 4, 1942, days before Yamamoto expected them to interfere in the Midway operation, American carrier-based aircraft destroyed the four carriers of the "Kido Butai", catching the Japanese carriers at an especially vulnerable moment.
With his air power destroyed and his forces not yet concentrated for a fleet battle, Yamamoto attempted to maneuver his remaining forces, still strong on paper, to trap the American forces. He was unable to do so because his initial dispositions had placed his surface combatants too far from Midway, and because Admiral Raymond Spruance prudently withdrew to the east in a position to further defend Midway Island, believing (based on a mistaken submarine report) the Japanese still intended to invade. Not knowing several battleships, including the powerful "Yamato", were on the Japanese order of battle, he did not comprehend the severe risk of a night surface battle, in which his carriers and cruisers would be at a disadvantage. However, his move to the east did avoid the possibility of such a battle taking place. Correctly perceiving he had lost and could not bring surface forces into action, Yamamoto aborted the invasion of Midway and withdrew. The defeat marked the high tide of Japanese expansion.
Yamamoto's plan for MI has been the subject of much criticism. Many commentators state it violated the principle of concentration of force, and was overly complex. Others point out similarly complex Allied operations (such as Operation MB8) that were successful, and note the extent to which the American intelligence "coup" derailed the operation before it began. Had Yamamoto's dispositions not denied Nagumo adequate pre-attack reconnaissance assets, both the American cryptanalytic success and the unexpected appearance of Fletcher's carriers would have been irrelevant.
Actions after Midway.
The Battle of Midway solidly checked Japanese momentum, but the IJN was still a powerful force and capable of regaining the initiative. They planned to resume the thrust with "Operation FS" aimed at eventually taking Samoa and Fiji to cut the American life-line to Australia. This was expected to short-circuit the threat posed by General Douglas MacArthur and his American and Australian forces in New Guinea. To this end, development of the airfield on Guadalcanal continued and attracted the baleful eye of Yamamoto's opposite number, Admiral Ernest King.
To prevent the Japanese from regaining the initiative, King ramrodded the idea of an immediate American counterattack through the Joint Chiefs of Staff. This precipitated the American invasion of Guadalcanal and beat the Japanese to the punch, with Marines landing on the island in August 1942 and starting a bitter struggle that lasted until February 1943 and commenced a battle of attrition Japan could ill afford.
Yamamoto remained in command as Commander-in-Chief, retained at least partly to avoid diminishing the morale of the Combined Fleet. However, he had lost face as a result of the Midway defeat and the Naval General Staff were disinclined to indulge in further gambles. This reduced Yamamoto to pursuing the classic defensive Decisive Battle strategy he had attempted to overturn.
The naval and land battles at Guadalcanal caught the Japanese over-extended and attempting to support fighting in New Guinea while guarding the Central Pacific and preparing to conduct Operation FS. The FS operation was abandoned and the Japanese attempted to fight in both New Guinea and Guadalcanal at the same time. Already stretched thin, they suffered repeated setbacks due to a lack of shipping, a lack of troops, and a disastrous inability to coordinate Army and Navy activities.
Yamamoto committed Combined Fleet units to a series of small attrition actions across the south and central Pacific that stung the Americans, but suffered losses he could ill afford in return. Three major efforts to carry the island precipitated a pair of carrier battles that Yamamoto commanded personally at the Eastern Solomons and Santa Cruz Islands in September and October, and finally a wild pair of surface engagements in November, all timed to coincide with Japanese Army pushes. The timing of each major battle was successively derailed when the army could not hold up its end of the operation. Yamamoto's naval forces won a few victories and inflicted considerable losses and damage to the U.S. Fleet in several naval battles around Guadalcanal which included the battles of Savo Island, Cape Esperance, and Tassafaronga, but he could never draw the Americans into a decisive fleet action. As a result, the Japanese Navy's strength began to bleed off.
There were severe losses of carrier dive-bomber and torpedo-bomber crews in the carrier battles, emasculating the already depleted carrier air groups. Japan could not hope to match the United States in quantities of well-trained replacement pilots, and the quality of both Japanese land-based and naval aviation began declining. Particularly harmful, however, were losses of numerous destroyers in the unsuccessful Tokyo Express supply runs. The IJN already faced a shortage of such ships, and these losses further exacerbated Japan's already weakened commerce defense. With Guadalcanal lost in February 1943, there was no further attempt by the Japanese navy to seek a major battle in the Solomon Islands against the U.S. fleet, although smaller attrition battles continued. Yamamoto shifted the load of the air battle away from the depleted carriers groups and to the land-based naval air forces.
Death.
To boost morale following the defeat at Guadalcanal, Yamamoto decided to make an inspection tour throughout the South Pacific. On April 14, 1943, the US naval intelligence effort, code-named "Magic", intercepted and decrypted a message containing specific details regarding Yamamoto's tour, including arrival and departure times and locations, as well as the number and types of aircraft that would transport and accompany him on the journey. Yamamoto, the itinerary revealed, would be flying from Rabaul to Balalae Airfield, on an island near Bougainville in the Solomon Islands, on the morning of April 18, 1943.
U.S. President Franklin D. Roosevelt ordered Secretary of the Navy Frank Knox to "Get Yamamoto." Knox instructed Chief of Naval Operations Admiral Ernest J. King of Roosevelt's wishes. Admiral King telephoned Admiral Chester W. Nimitz at Pearl Harbor. This mission would be Top Secret and Urgent. Admiral Nimitz consulted Admiral William F. Halsey, Jr., Commander, South Pacific, then authorized a mission on April 17 to intercept Yamamoto's flight "en route" and shoot it down. A squadron of USAAF Lockheed P-38 Lightning aircraft were assigned the task as only they possessed the range to intercept and engage. Select pilots from three units were informed that they were intercepting an "important high officer" with no specific name given.
On the morning of April 18, despite urgings by local commanders to cancel the trip for fear of ambush, Yamamoto's two Mitsubishi G4M bombers, used as fast transport aircraft without bombs, left Rabaul as scheduled for the 315 mi trip. Sixteen Lightnings intercepted the flight over Bougainville and a dogfight ensued between them and the six escorting Mitsubishi A6M Zeroes. First Lieutenant Rex T. Barber engaged the first of the two Japanese transports which turned out to be "T1-323" (Yamamoto's aircraft). He targeted the aircraft with gunfire until it began to spew smoke from its left engine. Barber turned away to attack the other transport as Yamamoto's plane crashed into the jungle.
The crash site and body of Yamamoto were found the next day in the jungle north of Buin, Papua New Guinea, by a Japanese search and rescue party, led by army engineer, Lieutenant Hamasuna. According to Hamasuna, Yamamoto had been thrown clear of the plane's wreckage, his white-gloved hand grasping the hilt of his katana, still upright in his seat under a tree. Hamasuna said Yamamoto was instantly recognizable, head dipped down as if deep in thought. A post-mortem of the body disclosed that Yamamoto had received two 0.50-caliber bullet wounds, one to the back of his left shoulder and another to his left lower jaw that exited above his right eye. The Japanese navy doctor examining the body determined that the head wound killed Yamamoto. The more violent details of Yamamoto's death were hidden from the Japanese public; the medical report was whitewashed, changed "on orders from above", according to biographer Hiroyuki Agawa.
His staff cremated his remains at Buin, and the ashes were returned to Tokyo aboard the battleship "Musashi", Yamamoto's last flagship. Yamamoto was given a full state funeral on June 5, 1943, where he received, posthumously, the title of Marshal and was awarded the Order of the Chrysanthemum (1st Class). He was also awarded Nazi Germany's Knight's Cross of the Iron Cross with Oak Leaves and Swords. Part of his ashes were buried in the public Tama Cemetery, Tokyo (多摩霊園), and the remainder at his ancestral burial grounds at the temple of Chuko-ji in Nagaoka City. He was succeeded as commander-in-chief of the Combined Fleet by Admiral Mineichi Koga.
The wreck of the aircraft that carried Yamamoto remains as a tourist attraction in the Bougainville jungle near Moila Point, a few kilometers off the Panguna-Buin road. Signposts can be found near the village of Aku, 24 km outside Buin. A path to the wreck has been cut through the jungle, an hour's walk from the road. Other artifacts from the crash site, including the outer wing panel and the Admiral's seat, are at the Isoroku Yamamoto Memorial Hall and Museum in Nagaoka, Niigata, Japan.
Personal life.
While other military leaders avoided the image of being "soft", Yamamoto continued to practice calligraphy. He and his wife, Reiko, had four children: two sons and two daughters. Yamamoto was an avid gambler, enjoying "shogi", billiards, bridge, mah jong, poker, and other games that tested his wits and sharpened his mind. He frequently made jokes about moving to Monaco and starting his own casino. He enjoyed the company of "geisha", and his wife Reiko revealed to the Japanese public in 1954 that Yamamoto was closer to his favorite "geisha" Kawai Chiyoko than to her, which stirred some controversy. After his death, his funeral procession passed by Kawai's quarters on the way to the cemetery.
Media portrayals.
Since the end of the Second World War, a number of Japanese and American films have depicted the character of Isoroku Yamamoto.
One of the most notable films is the 1970 movie "Tora! Tora! Tora!", which stars Japanese actor Sô Yamamura as Yamamoto, who states after the attack on Pearl Harbor:
I fear that all we have done is to awaken a sleeping giant and fill him with a terrible resolve.—attributed to Yamamoto in "Tora! Tora! Tora!" (1970), in reference to the attack on Pearl Harbor.
 There is no evidence that Yamamoto said this in reality despite the film calling it a quote. (See Isoroku Yamamoto's sleeping giant quote for further discussion.)
The first film to feature Yamamoto was Toho's 1953 film "", (later released in the United States as "Eagle of the Pacific"), in which Yamamoto was portrayed by Denjirô Ôkôchi.
The 1960 film "The Gallant Hours" depicts the battle of wits between Vice-Admiral William Halsey, Jr. and Yamamoto from the start of the Guadalcanal Campaign in August 1942 to Yamamoto's death in April 1943. The film, however, portrays Yamamoto's death as occurring in November 1942, the day after the Naval Battle of Guadalcanal, and the P-38 aircraft that killed him as coming from Guadalcanal.
In Daiei Studios's 1969 film "Aa, kaigun" (later released in the United States as "Gateway to Glory"), Yamamoto was portrayed by Shôgo Shimada.
Award-winning Japanese actor Toshiro Mifune (star of "The Seven Samurai") portrayed Yamamoto in three films:
In Shūe Matsubayashi's 1981 film "" (lit. "Combined Fleet", later released in the United States as "The Imperial Navy"), Yamamoto was portrayed by Keiju Kobayashi.
In the 2001 film "Pearl Harbor", a Jerry Bruckheimer-produced epic, Yamamoto was portrayed by Oscar-nominated Japanese-born American actor Mako Iwamatsu. Like "Tora! Tora! Tora!", this film also features the sleeping giant quote.
In Toei's 2011 war film "", Yamamoto was portrayed by Kōji Yakusho.
Alternate history.
In the 1993 OVA series "Konpeki no Kantai" (lit. "Deep Blue Fleet"), the original timeline proceeds until Yamamoto's death in April 1943. However, instead of dying in the crash, Yamamoto blacks out and suddenly wakes up as his younger self, Isoroku Takano, after the Battle of Tsushima in 1905. His memory from the original timeline intact, Yamamoto uses his knowledge of the future to help Japan become a stronger military power, and eventually launching a "coup d'état" against Hideki Tōjō's government. In the subsequent Pacific war, Japan's technologically advanced navy decisively defeats the United States, and grants all of the former European and American colonies in Asia full independence. Later on, Yamamoto convinces Japan to join forces with the United States and Britain to defeat Nazi Germany.
In the 2004 anime series "Zipang", Yamamoto (who is voiced by ) works to develop the uneasy partnership with the crew of the JMSDF "Mirai", which has been transported back sixty years through time to the year 1942.
In the Axis of Time trilogy by author John Birmingham, which depicts an alternate history of World War II, after a naval task force from the year 2021 is accidentally transported back through time to 1942, Yamamoto assumes a leadership role in the dramatic alteration of Japan's war strategy.
In Douglas Niles' 2007 book "MacArthur's War: A Novel of the Invasion of Japan" (written with Michael Dobson), which focuses on General Douglas MacArthur and an alternate history of the Pacific War (following a considerably different outcome of the Battle of Midway), Yamamoto is portrayed sympathetically, with much of the action in the Japanese government seen through his eyes, though he could not change the major decisions of Japan in World War II.
In Robert Conroy's 2011 book "Rising Sun", Yamamoto directs the IJN to launch a series of attacks on the American West Coast, in the hope that the United States can be convinced to sue for peace and securing Japan's place as a world power; but he cannot escape his lingering fear that the war will ultimately doom Japan.
In Neal Stephenson's 1999 book "Cryptonomicon", Yamamoto's final moments are depicted as him realising that Japan's cryptographic codes have been broken, and that he must inform them, right up until he and his chair hit the tree.

</doc>
<doc id="15412" url="http://en.wikipedia.org/wiki?curid=15412" title="Infrared spectroscopy">
Infrared spectroscopy

Infrared spectroscopy (IR spectroscopy) is the spectroscopy that deals with the infrared region of the electromagnetic spectrum, that is light with a longer wavelength and lower frequency than visible light. It covers a range of techniques, mostly based on absorption spectroscopy. As with all spectroscopic techniques, it can be used to identify and study chemicals. For a given sample which may be solid, liquid, or gaseous, the method or technique of infrared spectroscopy uses an instrument called an infrared spectrometer (or spectrophotometer) to produce an infrared spectrum. A basic IR spectrum is essentially a graph of infrared light absorbance (or transmittance) on the vertical axis vs. frequency or wavelength on the horizontal axis. Typical units of frequency used in IR spectra are reciprocal centimeters (sometimes called wave numbers), with the symbol cm−1. Units of IR wavelength are commonly given in micrometers (formerly called "microns"), symbol μm, which are related to wave numbers in a reciprocal way. A common laboratory instrument that uses this technique is a Fourier transform infrared (FTIR) spectrometer. Two-dimensional IR is also possible as discussed below.
The infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14000–4000 cm−1 (0.8–2.5 μm wavelength) can excite overtone or harmonic vibrations. The mid-infrared, approximately 4000–400 cm−1 (2.5–25 μm) may be used to study the fundamental vibrations and associated rotational-vibrational structure. The far-infrared, approximately 400–10 cm−1 (25–1000 μm), lying adjacent to the microwave region, has low energy and may be used for rotational spectroscopy. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties.
Theory.
Infrared spectroscopy exploits the fact that molecules absorb specific frequencies that are characteristic of their structure. These absorptions are resonant frequencies, i.e. the frequency of the absorbed radiation matches the transition energy of the bond or group that vibrates. The energies are determined by the shape of the molecular potential energy surfaces, the masses of the atoms, and the associated vibronic coupling.
In particular, in the Born–Oppenheimer and harmonic approximations, i.e. when the molecular Hamiltonian corresponding to the electronic ground state can be approximated by a harmonic oscillator in the neighborhood of the equilibrium molecular geometry, the resonant frequencies are associated with the normal modes corresponding to the molecular electronic ground state potential energy surface. The resonant frequencies are also related to the strength of the bond and the mass of the atoms at either end of it. Thus, the frequency of the vibrations are associated with a particular normal mode of motion and a particular bond type.
Number of vibrational modes.
In order for a vibrational mode in a molecule to be "IR active", it must be associated with changes in the dipole. A permanent dipole is not necessary, as the rule requires only a change in dipole moment.
A molecule can vibrate in many ways, and each way is called a "vibrational mode." For molecules with N number of atoms in them, linear molecules have 3N – 5 degrees of vibrational modes, whereas nonlinear molecules have 3N – 6 degrees of vibrational modes (also called vibrational degrees of freedom). As an example H2O, a non-linear molecule, will have 3 × 3 – 6 = 3 degrees of vibrational freedom, or modes.
Simple diatomic molecules have only one bond and only one vibrational band. If the molecule is symmetrical, e.g. N2, the band is not observed in the IR spectrum, but only in the Raman spectrum. Asymmetrical diatomic molecules, e.g. CO, absorb in the IR spectrum. More complex molecules have many bonds, and their vibrational spectra are correspondingly more complex, i.e. big molecules have many peaks in their IR spectra.
The atoms in a CH2X2 group, commonly found in organic compounds and where X can represent any other atom, can vibrate in nine different ways. Six of these involve only the CH2 portion: symmetric and antisymmetric stretching, scissoring, rocking, wagging and twisting, as shown below. (Note, that because CH2 is attached to X2 it has 6 modes, unlike H2O, which only has 3 modes. The rocking, wagging, and twisting modes do not exist for H2O, since they are rigid body translations and no relative displacements exist.)
These figures do not represent the "recoil" of the C atoms, which, though necessarily present to balance the overall movements of the molecule, are much smaller than the movements of the lighter H atoms.
Special effects.
The simplest and most important IR bands arise from the "normal modes," the simplest distortions of the molecule. In some cases, "overtone bands" are observed. These bands arise from the absorption of a photon that leads to a doubly excited vibrational state. Such bands appear at approximately twice the energy of the normal mode. Some vibrations, so-called 'combination modes," involve more than one normal mode. The phenomenon of Fermi resonance can arise when two modes are similar in energy; Fermi resonance results in an unexpected shift in energy and intensity of the bands etc.
Practical IR spectroscopy.
The infrared spectrum of a sample is recorded by passing a beam of infrared light through the sample. When the frequency of the IR is the same as the vibrational frequency of a bond, absorption occurs. Examination of the transmitted light reveals how much energy was absorbed at each frequency (or wavelength). This can be achieved by scanning the wavelength range using a monochromator. Alternatively, the whole wavelength range is measured at once using a Fourier transform instrument and then a transmittance or absorbance spectrum is generated using a dedicated procedure. Analysis of the position, shape and intensity of peaks in this spectrum reveals details about the molecular structure of the sample.
This technique works almost exclusively on samples with covalent bonds. Simple spectra are obtained from samples with few IR active bonds and high levels of purity. More complex molecular structures lead to more absorption bands and more complex spectra. The technique has been used for the characterization of very complex mixtures . Spectra issues with infrared fluorescence are rare.
Sample preparation.
Gaseous samples require a sample cell with a long pathlength to compensate for the diluteness. The pathlength of the sample cell depends on the concentration of the compound of interest. A simple glass tube with length of 5 to 10 cm equipped with infrared-transparent windows at the both ends of the tube can be used for concentrations down to several hundred ppm. Sample gas concentrations well below ppm can be measured with a White's cell in which the infrared light is guided with mirrors to travel through the gas. White's cells are available with optical pathlength starting from 0.5 m up to hundred meters.
Liquid samples can be sandwiched between two plates of a salt (commonly sodium chloride, or common salt, although a number of other salts such as potassium bromide or calcium fluoride are also used).
The plates are transparent to the infrared light and do not introduce any lines onto the spectra.
Solid samples can be prepared in a variety of ways. One common method is to crush the sample with an oily mulling agent (usually Nujol) in a marble or agate mortar, with a pestle. A thin film of the mull is smeared onto salt plates and measured. The second method is to grind a quantity of the sample with a specially purified salt (usually potassium bromide) finely (to remove scattering effects from large crystals). This powder mixture is then pressed in a mechanical press to form a translucent pellet through which the beam of the spectrometer can pass. A third technique is the "cast film" technique, which is used mainly for polymeric materials. The sample is first dissolved in a suitable, non hygroscopic solvent. A drop of this solution is deposited on surface of KBr or NaCl cell. The solution is then evaporated to dryness and the film formed on the cell is analysed directly. Care is important to ensure that the film is not too thick otherwise light cannot pass through. This technique is suitable for qualitative analysis. The final method is to use microtomy to cut a thin (20–100 µm) film from a solid sample. This is one of the most important ways of analysing failed plastic products for example because the integrity of the solid is preserved.
In photoacoustic spectroscopy the need for sample treatment is minimal. The sample, liquid or solid, is placed into the sample cup which is inserted into the photoacoustic cell which is then sealed for the measurement. The sample may be one solid piece, powder or basically in any form for the measurement. For example, a piece of rock can be inserted into the sample cup and the spectrum measured from it.
It is important to note that spectra obtained from different sample preparation methods will look slightly different from each other due to differences in the samples' physical states.
Comparing to a reference.
To take the infrared spectrum of a sample, it is necessary to measure both the sample and a "reference" (or "control"). This is because each measurement is affected by not only the light-absorption properties of the sample, but also the properties of the instrument (for example, what light source is used, what infrared detector is used, etc.). The reference measurement makes it possible to eliminate the instrument influence. Mathematically, the sample transmission spectrum is divided by the reference transmission spectrum.
The appropriate "reference" depends on the measurement and its goal. The simplest reference measurement is to simply remove the sample (replacing it by air). However, sometimes a different reference is more useful. For example, if the sample is a dilute solute dissolved in water in a beaker, then a good reference measurement might be to measure pure water in the same beaker. Then the reference measurement would cancel out not only all the instrumental properties (like what light source is used), but also the light-absorbing and light-reflecting properties of the water and beaker, and the final result would just show the properties of the solute (at least approximately).
A common way to compare to a reference is sequentially: first measure the reference, then replace the reference by the sample and measure the sample. This technique is not perfectly reliable; if the infrared lamp is a bit brighter during the reference measurement, then a bit dimmer during the sample measurement, the measurement will be distorted. More elaborate methods, such as a "two-beam" setup (see figure), can correct for these types of effects to give very accurate results. The Standard addition method can be used to statistically cancel these errors.
FTIR.
Fourier transform infrared (FTIR) spectroscopy is a measurement technique that allows one to record infrared spectra. Infrared light is guided through an interferometer and then through the sample (or vice versa). A moving mirror inside the apparatus alters the distribution of infrared light that passes through the interferometer. The signal directly recorded, called an "interferogram", represents light output as a function of mirror position. A data-processing technique called Fourier transform turns this raw data into the desired result (the sample's spectrum): Light output as a function of infrared wavelength (or equivalently, wavenumber). As described above, the sample's spectrum is always compared to a reference.
There is an alternate method for taking spectra (the "dispersive" or "scanning monochromator" method), where one wavelength at a time passes through the sample. The dispersive method is more common in UV-Vis spectroscopy, but is less practical in the infrared than the FTIR method. One reason that FTIR is favored is called "Fellgett's advantage" or the "multiplex advantage": The information at all frequencies is collected simultaneously, improving both speed and signal-to-noise ratio. Another is called "Jacquinot's Throughput Advantage": A dispersive measurement requires detecting much lower light levels than an FTIR measurement. There are other advantages, as well as some disadvantages, but virtually all modern infrared spectrometers are FTIR instruments.
Absorption bands.
IR spectroscopy is often used to identify structures because functional groups give rise to characteristic bands both in terms of intensity and position (frequency). The positions of these bands is summarized in correlation tables as shown below.
Wavenumbers listed in cm−1.
Badger's rule.
For many kinds of samples, the assignments are known, i.e. which bond deformation(s) are associated with which frequency. In such cases further information can be gleaned about the strength on a bond, relying on the empirical guideline called Badger's Rule. Originally published by Richard Badger in 1934, this rule states that the strength of a bond correlates with the frequency of its vibrational mode. That is, increase in bond strength leads to corresponding frequency increase and vice versa.
Uses and applications.
Infrared spectroscopy is a simple and reliable technique widely used in both organic and inorganic chemistry, in research and industry. It is used in quality control, dynamic measurement, and monitoring applications such as the long-term unattended measurement of CO2 concentrations in greenhouses and growth chambers by infrared gas analyzers.
It is also used in forensic analysis in both criminal and civil cases, for example in identifying polymer degradation. It can be used in determining the blood alcohol content of a suspected drunk driver.
A useful way of analysing solid samples without the need for cutting samples uses ATR or attenuated total reflectance spectroscopy. Using this approach, samples are pressed against the face of a single crystal. The infrared radiation passes through the crystal and only interacts with the sample at the interface between the two materials.
With increasing technology in computer filtering and manipulation of the results, samples in solution can now be measured accurately (water produces a broad absorbance across the range of interest, and thus renders the spectra unreadable without this computer treatment).
Some instruments will also automatically tell you what substance is being measured from a store of thousands of reference spectra held in storage.
Infrared spectroscopy is also useful in measuring the degree of polymerization in polymer manufacture. Changes in the character or quantity of a particular bond are assessed by measuring at a specific frequency over time. Modern research instruments can take infrared measurements across the range of interest as frequently as 32 times a second. This can be done whilst simultaneous measurements are made using other techniques. This makes the observations of chemical reactions and processes quicker and more accurate.
Infrared spectroscopy has also been successfully utilized in the field of semiconductor microelectronics: for example, infrared spectroscopy can be applied to semiconductors like silicon, gallium arsenide, gallium nitride, zinc selenide, amorphous silicon, silicon nitride, etc.
The instruments are now small, and can be transported, even for use in field trials.
In February 2014, NASA announced a , based on IR spectroscopy, for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
Isotope effects.
The different isotopes in a particular species may exhibit different fine details in infrared spectroscopy. For example, the O–O stretching frequency (in reciprocal centimeters) of oxyhemocyanin is experimentally determined to be 832 and 788 cm−1 for ν(16O–16O) and ν(18O–18O), respectively.
By considering the O–O bond as a spring, the wavenumber of absorbance, ν can be calculated:
where "k" is the spring constant for the bond, "c" is the speed of light, and "μ" is the reduced mass of the A–B system:
(formula_3 is the mass of atom formula_4).
The reduced masses for 16O–16O and 18O–18O can be approximated as 8 and 9 respectively. Thus
Where formula_6 is the wavenumber; [wavenumber = frequency/(speed of light)]
The effect of isotopes, both on the vibration and the decay dynamics, has been found to be stronger than previously thought. In some systems, such as silicon and germanium, the decay of the anti-symmetric stretch mode of interstitial oxygen involves the symmetric stretch mode with a strong isotope dependence. For example, it was shown that for a natural silicon sample, the lifetime of the anti-symmetric vibration is 11.4 ps. When the isotope of one of the silicon atoms is increased to 29Si, the lifetime increases to 19 ps. In similar manner, when the silicon atom is changed to 30Si, the lifetime becomes 27 ps.
Two-dimensional IR.
Two-dimensional infrared correlation spectroscopy analysis combines multiple samples of infrared spectra to reveal more complex properties. By extending the spectral information of a perturbed sample, spectral analysis is simplified and resolution is enhanced. The 2D synchronous and 2D asynchronous spectra represent a graphical overview of the spectral changes due to a perturbation (such as a changing concentration or changing temperature) as well as the relationship between the spectral changes at two different wavenumbers.
Nonlinear two-dimensional infrared spectroscopy is the infrared version of correlation spectroscopy. Nonlinear two-dimensional infrared spectroscopy is a technique that has become available with the development of femtosecond infrared laser pulses. In this experiment, first a set of pump pulses is applied to the sample. This is followed by a waiting time during which the system is allowed to relax. The typical waiting time lasts from zero to several picoseconds, and the duration can be controlled with a resolution of tens of femtoseconds. A probe pulse is then applied, resulting in the emission of a signal from the sample. The nonlinear two-dimensional infrared spectrum is a two-dimensional correlation plot of the frequency ω1 that was excited by the initial pump pulses and the frequency ω3 excited by the probe pulse after the waiting time. This allows the observation of coupling between different vibrational modes; because of its extremely fine time resolution, it can be used to monitor molecular dynamics on a picosecond timescale. It is still a largely unexplored technique and is becoming increasingly popular for fundamental research.
As with two-dimensional nuclear magnetic resonance (2DNMR) spectroscopy, this technique spreads the spectrum in two dimensions and allows for the observation of cross peaks that contain information on the coupling between different modes. In contrast to 2DNMR, nonlinear two-dimensional infrared spectroscopy also involves the excitation to overtones. These excitations result in excited state absorption peaks located below the diagonal and cross peaks. In 2DNMR, two distinct techniques, COSY and NOESY, are frequently used. The cross peaks in the first are related to the scalar coupling, while in the latter they are related to the spin transfer between different nuclei. In nonlinear two-dimensional infrared spectroscopy, analogs have been drawn to these 2DNMR techniques. Nonlinear two-dimensional infrared spectroscopy with zero waiting time corresponds to COSY, and nonlinear two-dimensional infrared spectroscopy with finite waiting time allowing vibrational population transfer corresponds to NOESY. The COSY variant of nonlinear two-dimensional infrared spectroscopy has been used for determination of the secondary structure content of proteins.

</doc>
<doc id="15414" url="http://en.wikipedia.org/wiki?curid=15414" title="Irenaeus">
Irenaeus

Irenaeus (; Greek: Εἰρηναῖος) (early 2nd century – c. AD 202), also referred to as Saint Irenaeus, was Bishop of Lugdunum in Gaul, then a part of the Roman Empire (now Lyon, France). He was an early Church Father and apologist, and his writings were formative in the early development of Christian theology. He was a hearer of Polycarp, who in turn was traditionally a disciple of John the Evangelist.
Irenaeus' best-known book, "Adversus Haereses" or "Against Heresies" (c. 180), is a detailed attack on Gnosticism, which was then a serious threat to the Church, and especially on the system of the Gnostic Valentinus. As one of the first great Christian theologians, he emphasized the traditional elements in the Church, especially the episcopate, Scripture, and tradition. Against the Gnostics, who said that they possessed a secret oral tradition from Jesus himself, Irenaeus maintained that the bishops in different cities are known as far back as the Apostles and that the bishops provided the only safe guide to the interpretation of Scripture. His writings, with those of Clement and Ignatius, are taken as among the earliest signs of the developing doctrine of the primacy of the Roman see. Irenaeus is the earliest witness to recognition of the canonical character of all four gospels.
Irenaeus is recognized as a saint in both Roman Catholicism and Eastern Orthodoxy. His feast day is on June 28 in the General Roman Calendar, where it was inserted for the first time in 1920; in 1960 the Catholic Church transferred it to July 3, leaving June 28 for the Vigil of the Feast of Saints Peter and Paul, but in 1969 it was returned to June 28, the day of his death. The Lutheran Church commemorates Irenaeus on that same date for his life of exemplary Christian witness. In the Orthodox Church his feast day is 23 August.
Biography.
Irenaeus was born during the first half of the 2nd century (the exact date is disputed: between the years 115 and 125 according to some, or 130 and 142 according to others), and he is thought to have been a Greek from Polycarp's hometown of Smyrna in Asia Minor, now İzmir, Turkey. Unlike many of his contemporaries, he was brought up in a Christian family rather than converting as an adult.
During the persecution of Marcus Aurelius, the Roman Emperor from 161–180, Irenaeus was a priest of the Church of Lyon. The clergy of that city, many of whom were suffering imprisonment for the faith, sent him in 177 to Rome with a letter to Pope Eleuterus concerning the heresy Montanism, and that occasion bore emphatic testimony to his merits. While Irenaeus was in Rome, a massacre took place in Lyon. Returning to Gaul, Irenaeus succeeded the martyr Saint Pothinus and became the second Bishop of Lyon.
During the religious peace which followed the persecution of Marcus Aurelius, the new bishop divided his activities between the duties of a pastor and of a missionary (as to which we have but brief data, late and not very certain). Almost all his writings were directed against Gnosticism. The most famous of these writings is "Adversus haereses" ("Against Heresies"). Irenaeus alludes to coming across Gnostic writings, and holding conversations with Gnostics, and this may have taken place in Asia Minor or in Rome. However, it also appears that Gnosticism was present near Lyon: he writes that there were followers of 'Magus the Magician' living and teaching in the Rhone valley.
Little is known about the career of Irenaeus after he became bishop. The last action reported of him (by Eusebius, 150 years later) is that in 190 or 191, he exerted influence on Pope Victor I not to excommunicate the Christian communities of Asia Minor which persevered in the practice of the Quartodeciman celebration of Easter.
Nothing is known of the date of his death, which must have occurred at the end of the 2nd or the beginning of the 3rd century. A few within the Roman Catholic Church and Orthodox Church celebrate him as a martyr. He was buried under the Church of Saint John in Lyon, which was later renamed St Irenaeus in his honour. The tomb and his remains were utterly destroyed in 1562 by the Huguenots.
Writings.
Irenaeus wrote a number of books, but the most important that survives is the "Against Heresies" (or, in its Latin title, "Adversus Haereses"). In Book I, Irenaeus talks about the Valentinian Gnostics and their predecessors, who go as far back as the magician Simon Magus. In Book II he attempts to provide proof that Valentinianism contains no merit in terms of its doctrines. In Book III Irenaeus purports to show that these doctrines are false, by providing counter-evidence gleaned from the Gospels. Book IV consists of Jesus' sayings, and here Irenaeus also stresses the unity of the Old Testament and the Gospel. In the final volume, Book V, Irenaeus focuses on more sayings of Jesus plus the letters of Paul the Apostle.
The purpose of "Against Heresies" was to refute the teachings of various Gnostic groups; apparently, several Greek merchants had begun an oratorial campaign in Irenaeus' bishopric, teaching that the material world was the accidental creation of an evil god, from which we are to escape by the pursuit of "gnosis". Irenaeus argued that the true gnosis is in fact knowledge of Christ, which redeems rather than escapes from bodily existence. Until the discovery of the Library of Nag Hammadi in 1945, "Against Heresies" was the best-surviving description of Gnosticism. According to some biblical scholars, the findings at Nag Hammadi have shown Irenaeus' description of Gnosticism to be largely inaccurate and polemic in nature. Though correct in some details about the belief systems of various groups, Irenaeus' main purpose was to warn Christians against Gnosticism, rather than catalog those beliefs. He described Gnostic groups as sexual libertines, for example, when some of their own writings advocated chastity more strongly than did orthodox texts—yet the gnostic texts cannot be taken as guides to their actual practices, about which almost nothing is reliably known today. However, at least one scholar, Rodney Stark, claims that it is the same Nag Hammadi library that proves Irenaeus right.
It seemed that Irenaeus's critiques against the gnostics were exaggerated, which led to his scholarly dismissal for a long time. For example, he wrote: "They declare that Judas the traitor was thoroughly acquainted with these things, and that he alone, knowing the truth as no other did, accomplished the mystery of betrayal; by him all things were thus thrown into confusion. They produce a fictitious history of this kind, which they style the Gospel of Judas." These claims turned out to be truly mentioned in the Gospel of Judas where Jesus asked Judas to betray him. In any case the gnostics were not a single group, but a wide array of sects. Some groups were indeed libertine because they considered bodily existence meaningless; others praise chastity, and strongly prohibited any sexual activity, even within marriage.
Irenaeus also wrote "The Demonstration of the Apostolic Preaching" (also known as "Proof of the Apostolic Preaching"), an Armenian copy of which was discovered in 1904. This work seems to have been an instruction for recent Christian converts.
Eusebius attests to other works by Irenaeus, today lost, including "On the Ogdoad," an untitled letter to Blastus regarding schism, "On the Subject of Knowledge", "On the Monarchy" or "How God is not the Cause of Evil".
Irenaeus exercised wide influence on the generation which followed. Both Hippolytus and Tertullian freely drew on his writings. However, none of his works aside from "Against Heresies" and "The Demonstration of the Apostolic Preaching" survive today, perhaps because his literal hope of an earthly millennium may have made him uncongenial reading in the Greek East. Even though no complete version of "Against Heresies" in its original Greek exists, we possess the full ancient Latin version, probably of the third century, as well as thirty-three fragments of a Syrian version and a complete Armenian version of books 4 and 5.
Irenaeus' works were first translated into English by John Keble and published in 1872 as part of the Library of the Fathers series.
Scripture.
Irenaeus pointed to Scripture as a proof of orthodox Christianity against heresies, classifying as Scripture not only the Old Testament but most of the books now known as the New Testament, while excluding many works, a large number by Gnostics, that flourished in the 2nd century and claimed scriptural authority.
Before Irenaeus, Christians differed as to which gospel they preferred. The Christians of Asia Minor preferred the Gospel of John. The Gospel of Matthew was the most popular overall. Irenaeus asserted that four Gospels, Matthew, Mark, Luke, and John, were canonical scripture. Thus Irenaeus provides the earliest witness to the assertion of the four canonical Gospels, possibly in reaction to Marcion's edited version of the Gospel of Luke, which Marcion asserted was the one and only true gospel.
Based on the arguments Irenaeus made in support of only four authentic gospels, some interpreters deduce that the "fourfold Gospel" must have still been a novelty in Irenaeus' time. "Against Heresies" 3.11.7 acknowledges that many heterodox Christians use only one gospel while 3.11.9 acknowledges that some use more than four. The success of Tatian's Diatessaron in about the same time period is "... a powerful indication that the fourfold Gospel contemporaneously sponsored by Irenaeus was not broadly, let alone universally, recognized."
Irenaeus is also our earliest attestation that the Gospel of John was written by John the Apostle, and that the Gospel of Luke was written by Luke, the companion of Paul.
The apologist and ascetic Tatian had previously harmonized the four gospels into a single narrative, the "Diatesseron" ("c" 150–160).
Scholars contend that Irenaeus quotes from 21 of the 27 New Testament Texts:
Matthew ("Book 3, Chapter 16")
Mark ("Book 3, Chapter 10")
Luke ("Book 3, Chapter 14")
John ("Book 3, Chapter 11")
Acts of the Apostles ("Book 3, Chapter 14")
Romans ("Book 3, Chapter 16")
1 Corinthians ("Book 1, Chapter 3")
2 Corinthians ("Book 3, Chapter 7")
Galatians ("Book 3, Chapter 22")
Ephesians ("Book 5, Chapter 2")
Philippians ("Book 4, Chapter 18")
Colossians ("Book 1, Chapter 3")
1 Thessalonians ("Book 5, Chapter 6")
2 Thessalonians ("Book 5, Chapter 25")
1 Timothy ("Book 1, Preface")
2 Timothy ("Book 3, Chapter 14")
Titus ("Book 3, Chapter 3")
1 Peter ("Book 4, Chapter 9")
1 John ("Book 3, Chapter 16")
2 John ("Book 1, Chapter 16")
Revelation to John ("Book 4, Chapter 20")
He may refer to Hebrews ("Book 2, Chapter 30") and James ("Book 4, Chapter 16") and maybe even 2 Peter ("Book 5, Chapter 28") but does not cite Philemon, 3 John or Jude.
Apostolic authority.
In his writing against the Gnostics, who claimed to possess a secret oral tradition from Jesus himself, Irenaeus maintained that the bishops in different cities are known as far back as the Apostles and that the bishops provided the only safe guide to the interpretation of Scripture. In a passage that became a "locus classicus" of Catholic-Protestant polemics, he cited the Roman church as an example of the unbroken chain of authority which text Western polemics would use to assert the primacy of Rome over Eastern churches by virtue of its "preeminent authority".
With the lists of bishops to which Irenaeus referred, the doctrine of the apostolic succession, firmly established in the Church at this time, of the bishops could be linked. This succession was important to establish a chain of custody for orthodoxy. He felt it important, however, to also speak of a succession of elders (presbyters).
Irenaeus' point when refuting the Gnostics was that all of the Apostolic churches had preserved the same traditions and teachings in many independent streams. It was the unanimous agreement between these many independent streams of transmission that proved the orthodox Faith, current in those churches, to be true. Had any error crept in, the agreement would be immediately destroyed.
Irenaeus' theology and contrast with Gnosticism.
The central point of Irenaeus' theology is the unity and the goodness of God, in opposition to the Gnostics' theory of God; a number of divine emanations (Aeons) along with a distinction between the Monad and the Demiurge. Irenaeus uses the Logos theology he inherited from Justin Martyr. Irenaeus was a student of Polycarp, who was said to have been tutored by John the Apostle. (John had used Logos terminology in the Gospel of John and the letter of 1 John). Irenaeus prefers to speak of the Son and the Spirit as the "hands of God".
His emphasis on the unity of God is reflected in his corresponding emphasis on the unity of salvation history. Irenaeus repeatedly insists that God began the world and has been overseeing it ever since this creative act; everything that has happened is part of his plan for humanity. The essence of this plan is a process of maturation: Irenaeus believes that humanity was created immature, and God intended his creatures to take a long time to grow into or assume the divine likeness. Thus, Adam and Eve were created as children. Their Fall was thus not a full-blown rebellion but rather a childish spat, a desire to grow up before their time and have everything with immediacy.
Everything that has happened since has therefore been planned by God to help humanity overcome this initial mishap and achieve spiritual maturity. The world has been intentionally designed by God as a difficult place, where human beings are forced to make moral decisions, as only in this way can they mature as moral agents. Irenaeus likens death to the big fish that swallowed Jonah: it was only in the depths of the whale's belly that Jonah could turn to God and act according to the divine will. Similarly, death and suffering appear as evils, but without them we could never come to know God.
According to Irenaeus, the high point in salvation history is the advent of Jesus. Irenaeus believed that Christ would always have been sent, even if humanity had never sinned; but the fact that they "did" sin determines his role as a savior. He sees Christ as the new Adam, who systematically "undoes" what Adam did: thus, where Adam was disobedient concerning God's edict concerning the fruit of the Tree of Knowledge of Good and Evil, Christ was obedient even to death on the wood of a tree. Irenaeus is the first to draw comparisons between Eve and Mary, contrasting the faithlessness of the former with the faithfulness of the latter. In addition to reversing the wrongs done by Adam, Irenaeus thinks of Christ as "recapitulating" or "summing up" human life. This means that Christ goes through every stage of human life, from infancy to old age, and simply by living it, sanctifies it with his divinity. Although it is sometimes claimed that Irenaeus believed Christ did not die until he was older than is conventionally portrayed, the bishop of Lyons simply pointed out that because Jesus turned the permissible age for becoming a rabbi (30 years old and above), he recapitulated and sanctified the period between 30 and 50 years old, as per the Jewish custom of periodization of human life, and so touches the beginning of old age when one becomes 50 years old. (see Adversus Haereses, book II, chapter 22).
In the passage of "Adversus Haereses" under consideration, Irenaeus is clear that after receiving baptism at the age of thirty, citing Luke 3:23, Gnostics then falsely assert that "He [Jesus] preached only one year reckoning from His baptism," and also, "On completing His thirtieth year He [Jesus] suffered, being in fact still a young man, and who had by no means attained to advanced age." Irenaeus argues against the Gnostics by using scripture to show that Jesus lives at least several years after his baptism by referencing 3 distinctly separate visits to Jerusalem. The first is when Jesus makes wine out of water, He goes up to the Paschal feast-day, after which He withdraws and is found in Samaria. The second is when Jesus goes up to Jerusalem for Passover and cures the paralytic, after which He withdraws over the sea of Tiberias. The third mention is when He travels to Jerusalem, eats the Passover, and suffers on the following day.
Irenaeus quotes scripture, which we reference as John 8:57, to suggest that Jesus ministers while in his 40's. In this passage, Jesus' opponents want to argue that Jesus has not seen Abraham, because Jesus is too young. Jesus' opponents argue that Jesus is not yet 50 years old. Irenaeus argues that if Jesus was in his thirties, his opponents would've argued that He's not yet 40 years, since that would make Him even younger. Irenaeus' argument is that they would not weaken their own argument by adding years to Jesus' age. Irenaeus also writes that "The Elders witness to this, who in Asia conferred with John the Lord's disciple, to the effect that John had delivered these things unto them: for he abode with them until the times of Trajan. And some of them saw not only John, but others also of the Apostles, and had this same account from them, and witness to the aforesaid relation."
In Demonstration (74) Irenaeus reinforced his view that Jesus was at least 45 with the statement "For Herod the king of the Jews and Pontius Pilate, the governor of Claudius Caesar, came together and condemned Him to be crucified." This would place the crucifixion no earlier than AD 42.
Irenaeus conceives of our salvation as essentially coming about through the incarnation of God as a man. He characterizes the penalty for sin as death and corruption. God, however, is immortal and incorruptible, and simply by becoming united to human nature in Christ he conveys those qualities to us: they spread, as it were, like a benign infection. Irenaeus therefore understands the atonement of Christ as happening through his incarnation rather than his crucifixion, although the latter event is an integral part of the former.
By comparison, according to the Gnostic view of Salvation, creation was perfect to begin with; it did not need time to grow and mature. For the Valentinians, the material world is the result of the loss of perfection which resulted from Sophia's desire to understand the Forefather. Therefore, one is ultimately redeemed, through secret knowledge, to enter the pleroma of which the Achamoth originally fell.
According to the Valentinian Gnostics, there are three classes of human beings. They are the material, who cannot attain salvation; the psychic, who are strengthened by works and faith (they are part of the church); and the spiritual, who cannot decay or be harmed by material actions. 
Essentially, ordinary humans—those who have faith but do not possess the special knowledge—will not attain salvation. Spirituals, on the other hand—those who obtain this great gift—are the only class that will eventually attain salvation.
In his article entitled "The Demiurge," J.P. Arendzen sums up the Valentinian view of the salvation of man. He writes, "The first, or carnal men, will return to the grossness of matter and finally be consumed by fire; the second, or psychic men, together with the Demiurge as their master, will enter a middle state, neither heaven (pleroma) nor hell (whyle); the purely spiritual men will be completely freed from the influence of the Demiurge and together with the Saviour and Achamoth, his spouse, will enter the pleroma divested of body (húle) and soul (psuché)."
Irenaeus is also known as one of the first theologians to use the principle of apostolic succession to refute his opponents.
In his criticism of Gnosticism, Irenaeus made reference to a Gnostic gospel which portrayed Judas in a positive light, as having acted in accordance with Jesus' instructions. The recently discovered Gospel of Judas dates close to the period when Irenaeus lived (late 2nd century), and scholars typically regard this work as one of many Gnostic texts, showing one of many varieties of Gnostic beliefs of the period.
Irenaeus mariology.
Irenaeus of Lyons is perhaps the earliest of the Church Fathers to develop a thorough mariology. It is certain that, while still very young, Irenaeus had seen and heard Bishop Polycarp (d. 155) at Smyrna. Irenaeus sets out a forthright account of Mary's role in the economy of salvation.
According to Irenaeus, Christ, being born out of the Virgin Mary, created a totally new historical situation. This view influences later Ambrose of Milan and Tertullian, who wrote about the virgin birth of the Mother of God. The donor of a new birth had to be born in a totally new way. The new birth being that what was lost through a woman, is now saved by a woman.
Prophetic exegesis.
The first four books of "Against Heresies" constitute a minute analysis and refutation of the Gnostic doctrines. The fifth is a statement of positive belief contrasting the constantly shifting and contradictory Gnostic opinions with the steadfast faith of the church. He appeals to the prophecies to demonstrate the truthfulness of Christianity.
Rome and Ten Horns.
Irenaeus shows the close relationship between the predicted events of Daniel 2 and 7. Rome, the fourth prophetic kingdom, would end in a tenfold partition. The ten divisions of the empire are the "ten horns" of Daniel 7 and the "ten horns" in Revelation 17. A "little horn," which is to supplant three of Rome's ten divisions, is also the still future "eighth" in Revelation. Irenaeus climaxes with the destruction of all kingdoms at the Second Advent, when Christ, the prophesied "stone," cut out of the mountain without hands, smites the image after Rome's division.
Antichrist.
Irenaeus identified the Antichrist, another name of the apostate Man of Sin, with Daniel's Little Horn and John's Beast of Revelation 13. He sought to apply other expressions to the Antichrist, such as "the abomination of desolation," mentioned by Christ (Matt. 24:15) and the "king of a most fierce countenance," in Gabriel's explanation of the Little Horn of Daniel 8. But he is not very clear how "the sacrifice and the libation shall be taken away" during the "half-week," or three and one-half years of the Antichrist's reign.
Under the notion that the Antichrist, as a single individual, might be of Jewish origin, he fancies that the mention of "Dan," in Jeremiah 8:16, and the omission of that name from those tribes listed in Revelation 7, might indicate the Antichrist's tribe. This surmise became the foundation of a series of subsequent interpretations by others.
Time, Times and Half a Time.
Like the other early church fathers, Irenaeus interpreted the three and one-half "times" of the Little Horn of Daniel 7 as three and one-half literal years. Antichrist's three and a half years of sitting in the temple are placed immediately before the Second Coming of Christ.
They are identified as the second half of the "one week" of Daniel 9. Irenaeus says nothing of the seventy weeks; we do not know whether he placed the "one week" at the end of the seventy or whether he had a gap.
666.
Irenaeus is the first of the church fathers to consider the mystic number 666. While Irenaeus did propose some solutions of this numerical riddle, his interpretation was quite reserved. Thus, he cautiously states:
But knowing the sure number declared by Scripture, that is six hundred sixty and six, let them await, in the first place, the division of the kingdom into ten; then, in the next place, when these kings are reigning, and beginning to set their affairs in order, and advance their kingdom, [let them learn] to acknowledge that he who shall come claiming the kingdom for himself, and shall terrify those men of whom we have been speaking, have a name containing the aforesaid number, is truly the abomination of desolation.
Although Irenaeus did speculate upon three names to symbolize this mystical number, namely Euanthas, Teitan, and Lateinos, nevertheless he was content to believe that the Antichrist would arise some time in the future after the fall of Rome and then the meaning of the number would be revealed.
Millennium.
Irenaeus declares that the Antichrist's future three-and-a-half-year reign, when he sits in the temple at Jerusalem, will be terminated by the second advent, with the resurrection of the just, the destruction of the wicked, and the millennial reign of the righteous. The general resurrection and the judgment follow the descent of the New Jerusalem at the end of the millennial kingdom.
Irenaeus calls those "heretics" who maintain that the saved are immediately glorified in the kingdom to come after death, before their resurrection. He avers that the millennial kingdom and the resurrection are actualities, not allegories, the first resurrection introducing this promised kingdom in which the risen saints are described as ruling over the renewed earth during the millennium, between the two resurrections.
Irenaeus held to the old Jewish tradition that the first six days of creation week were typical of the first six thousand years of human history, with Antichrist manifesting himself in the sixth period. And he expected the millennial kingdom to begin with the second coming of Christ to destroy the wicked and inaugurate, for the righteous, the reign of the kingdom of God during the seventh thousand years, the millennial Sabbath, as signified by the Sabbath of creation week.
In common with many of the fathers, Irenaeus did not distinguish between the new earth re-created in its eternal state—the thousand years of Revelation 20—when the saints are with Christ after His second advent, and the Jewish traditions of the Messianic kingdom. Hence, he applies Biblical and traditional ideas to his descriptions of this earth during the millennium, throughout the closing chapters of Book 5. This conception of the reign of resurrected and translated saints with Christ on this earth during the millennium-popularly known as chiliasm—was the increasingly prevailing belief of this time. Incipient distortions due to the admixture of current traditions, which figure in the extreme forms of chiliasm, caused a reaction against the earlier interpretations of Bible prophecies.
Irenaeus was not looking for a Jewish kingdom. He interpreted Israel as the Christian church, the spiritual seed of Abraham.
At times his expressions are highly fanciful. He tells, for instance, of a prodigious fertility of this earth during the millennium, after the resurrection of the righteous, "when also the creation, having been renovated and set free, shall fructify with an abundance of all kinds of food." In this connection, he attributes to Christ the saying about the vine with ten thousand branches, and the ear of wheat with ten thousand grains, and so forth, which he quotes from Papias of Hierapolis.
Exegesis.
Irenaeus' exegesis does not give complete coverage. On the seals, for example, he merely alludes to Christ as the rider on the white horse. He stresses five factors with greater clarity and emphasis than Justin:

</doc>
<doc id="15416" url="http://en.wikipedia.org/wiki?curid=15416" title="Involuntary commitment">
Involuntary commitment

Involuntary commitment or civil commitment is a legal process through which an individual with symptoms of severe mental illness is court-ordered into treatment in a hospital (inpatient) or in the community (outpatient).
Criteria for civil commitment are established by laws, which vary between nations. Commitment proceedings often follow a period of emergency hospitalization, during which an individual with acute psychiatric symptoms is confined for a relatively short duration (e.g. 72 hours) in a treatment facility for evaluation and stabilization by mental health professionals—who may then determine whether further civil commitment is appropriate or necessary. If civil commitment proceedings follow, then the evaluation is presented in a formal court hearing where testimony and other evidence may also be submitted. The subject of the hearing is typically entitled to legal counsel and may challenge a commitment order through habeas corpus rules.
Historically, until the first third of the twentieth century or later in most jurisdictions, all committals to public psychiatric facilities and most committals to private ones were involuntary. Since then, there have been alternating trends towards the abolition or substantial reduction of involuntary commitment, a trend known as "deinstitutionalisation."
Purpose.
In most jurisdictions, involuntary commitment is specifically applied to individuals found to be suffering from a mental illness that impairs their reasoning ability to such an extent that the laws, state, or courts find that decisions must or should be made for them under a legal framework. (In some jurisdictions, this is a distinct proceeding from being "found incompetent.")
Involuntary commitment is used to some degree for each of the following headings although different jurisdictions have different criteria. Some jurisdictions limit court-ordered treatment to individuals who meet statutory criteria for presenting a danger "to self or others." Other jurisdictions have broader criteria.
First aid.
Training is gradually becoming available in mental health first aid to equip community members such as teachers, school administrators, police officers, and medical workers in recognizing and managing situations where evaluations of behavior might be appropriate. The extension of first aid training to cover mental health problems and crises is a quite recent development. A mental health first aid training course was developed in Australia in 2001 and has been found to improve assistance provided to persons with a mental illness or in a mental health crisis. This form of training has now spread to a number of other countries (Canada, Finland, Hong Kong, Ireland, Singapore, Scotland, England, Wales, and the United States). Mental health triage may be used in an emergency room to evaluate the degree of risk and prioritize treatment.
Observation.
Observation is sometimes used to determine whether a person warrants involuntary commitment. It is not always clear on a relatively brief examination whether a person is psychotic or otherwise warrants commitment.
Containment of danger.
Austria, Belgium, Germany, Israel, the Netherlands, Northern Ireland, Russia, Taiwan, Ontario (Canada), and the United States have adopted commitment criteria based on the presumed danger of the defendant to self or to others. People with suicidal thoughts may act on these impulses and harm or kill themselves. People with psychosis are occasionally driven by their delusions or hallucinations to harm themselves or others. People with certain types of personality disorders can occasionally present a danger to themselves or others.
This concern has found expression in the standards for involuntary commitment in every U.S. state and in other countries as the "danger to self or others" standard, sometimes supplemented by the requirement that the danger be "imminent." In some jurisdictions, the "danger to self or others" standard has been broadened in recent years to include need-for-treatment criteria such as "gravely disabled."
Deinstitutionalization.
Starting in the 1960s, there has been a worldwide trend toward moving psychiatric patients from hospital settings to less restricting settings in the community, a shift known as "deinstitutionalization." Because the shift was typically not accompanied by a commensurate development of community-based services, critics say that deinstitutionalization has led to large numbers of people who would once have been inpatients being incarcerated in jails and prisons or becoming homeless. These scenarios occurred when outpatient services were not available or patients chose not to adhere to treatment outside the hospital. In some jurisdictions, laws authorizing court-ordered outpatient treatment have been passed in an effort to compel individuals with chronic, untreated severe mental illness to accept treatment while living outside the hospital (e.g. see Laura's Law, Kendra's Law).
Since the late 1960s the Italian physician Giorgio Antonucci questioned the basis themselves of psychiatry through the dismantling of the psychiatric hospitals "Osservanza" and "Luigi Lolli" and the liberation – and restitution to life – of the people there secluded.
Before the 1960s deinstitutionalization there were earlier efforts to free psychiatric patients. Doctor Philippe Pinel (1745–1826) ordered the removal of chains from patients.
There was a study of 269 back-ward patients from Vermont State Hospital done by Courtenay M. Harding, PhD and associates, about two-thirds of the ex-patients did well after deinstitutionalization.
Around the world.
United Nations.
United Nations General Assembly (resolution 46/119 of 1991), "Principles for the Protection of Persons with Mental Illness and the Improvement of Mental Health Care" is a non-binding resolution advocating certain broadly drawn procedures for the carrying out of involuntary commitment. These principles have been used in many countries where local laws have been revised or new ones implemented. The UN runs programs in some countries to assist in this process.
Politically motivated abuses.
At certain places and times, the practice of involuntary commitment has been used for the suppression of dissent, or in a punitive way.
In the former Soviet Union, psychiatric hospitals were used as prisons to isolate political prisoners from the rest of society. The official explanation was that no sane person would oppose the Soviet government and Communism. British playwright Tom Stoppard wrote "Every Good Boy Deserves Favour" about the relationship between a patient and his doctor in one of these hospitals. Stoppard was inspired by a meeting with a Russian exile.
In 1927, after the execution of Sacco and Vanzetti in the United States, a demonstrator named Aurora D'Angelo was sent to a mental health facility for psychiatric evaluation after she participated in a rally in support of the anarchists.

</doc>
<doc id="15417" url="http://en.wikipedia.org/wiki?curid=15417" title="Intermolecular force">
Intermolecular force

Intermolecular forces are forces of attraction or repulsion which act between neighboring particles (atoms, molecules or ions). They are weak compared to the intramolecular forces, the forces which keep a molecule together. For example the covalent bond, involving the sharing of electron pairs between atoms is much stronger than the forces present between the neighboring molecules. They are an essential part of force fields used in molecular mechanics.
The investigation of intermolecular forces starts from macroscopic observations which point out the existence and action of forces at a molecular level. These observations include non-ideal gas thermodynamic behavior reflected by virial coefficients, vapor pressure, viscosity, superficial tension and adsorption data.
The first reference to the nature of microscopic forces is found in Alexis Clairaut's work Theorie de la Figure de la Terre. Other scientists who have contributed to the investigation of microscopic forces include: Laplace, Gauss, Maxwell and Boltzmann.
Attractive intermolecular forces are considered by the following types:
Information on intermolecular force is obtained by macroscopic measurements of properties like viscosity, PVT data. The link to microscopic aspects is given by virial coefficients and Lennard-Jones potentials.
Dipole-dipole interactions.
Dipole-dipole interactions are electrostatic interactions between permanent dipoles in molecules. These interactions tend to align the molecules to increase attraction (reducing potential energy). An example of a dipole-dipole interaction can be seen in hydrogen chloride (HCl): the positive end of a polar molecule will attract the negative end of the other molecule and influence its position. Polar molecules have a net attraction between them. Examples of polar molecules include hydrogen chloride (HCl) and chloroform (CHCl3).
Often molecules contain dipolar groups, but have no overall dipole moment. This occurs if there is symmetry within the molecule that causes the dipoles to cancel each other out. This occurs in molecules such as tetrachloromethane and carbon dioxide. Note that the dipole-dipole interaction between two individual atoms is usually zero, since atoms rarely carry a permanent dipole.
Ion-dipole and ion-induced dipole forces.
Ion-dipole and ion-induced dipole forces are similar to dipole-dipole and induced-dipole interactions but involve ions, instead of only polar and non-polar molecules. Ion-dipole and ion-induced dipole forces are stronger than dipole-dipole interactions because the charge of any ion is much greater than the charge of a dipole moment. Ion-dipole bonding is stronger than hydrogen bonding.
An ion-dipole force consists of an ion and a polar molecule interacting. They align so that the positive and negative groups are next to one another, allowing for maximum attraction.
An ion-induced dipole force consists of an ion and a non-polar molecule interacting. Like a dipole-induced dipole force, the charge of the ion causes distortion of the electron cloud on the non-polar molecule.
Hydrogen bonding.
A hydrogen bond is the attraction between the lone pair of an electronegative atom and a hydrogen atom that is bonded to either nitrogen, oxygen, or fluorine. The hydrogen bond is often described as a strong electrostatic dipole-dipole interaction. However, it also has some features of covalent bonding: it is directional, stronger than a van der Waals interaction, produces interatomic distances shorter than the sum of van der Waals radius, and usually involves a limited number of interaction partners, which can be interpreted as a kind of valence.
Intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides, which have no hydrogen bonds. Intramolecular hydrogen bonding is partly responsible for the secondary, tertiary, and quaternary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.
van der Waals forces.
The van der Waals forces arise from interaction between uncharged atoms or molecules, leading not only to such phenomena as the cohesion of condensed phases and physical adsorption of gases, but also to a universal force of attraction between macroscopic bodies.
Keesom (permanent-permanent dipoles) interaction.
The first contribution to van der Waals forces is due to electrostatic interactions between charges (in molecular ions), dipoles (for polar molecules), quadrupoles (all molecules with symmetry lower than cubic), and permanent multipoles. It is referred to as Keesom interactions(named after Willem Hendrik Keesom). These forces originate from the attraction between permanent dipoles (dipolar molecules) and are temperature dependent.
They consists in attractive interactions between dipoles that are ensemble averaged over different rotational orientations of the dipoles. It is assumed that the molecules are constantly rotating and never get locked into place. This is a good assumption, but at some point molecules do get locked into place. The energy of a Keesom interaction depends on the inverse sixth power of the distance, unlike the interaction energy of two spatially fixed dipoles, which depends on the inverse third power of the distance. The Keesom interaction can only occur among molecules that possess permanent dipole moments a.k.a. two polar molecules. Also Keesom interactions are very weak van der Waals interactions and do not occur in aqueous solutions that contain electrolytes. The angle averaged interaction is given by the following equation:
formula_1
Where m = charge per length, formula_2 = permitivity of free space, formula_3 = dielectric constant of surrounding material, T = temperature, formula_4 = Boltzmann constant, and r = distance between molecules.
Debye (permanent-induced dipoles) force.
The second contribution is the induction (also known as polarization) or Debye force, arising from interactions between rotating permanent dipoles and from the polarizability of atoms and molecules (induced dipoles). These induced dipoles occur when one molecule with a permanent dipole repels another molecule’s electrons. A molecule with permanent dipole can induce a dipole in a similar neighboring molecule and cause mutual attraction. Debye forces cannot occur between atoms. The forces between induced and permanent dipoles are not as temperature dependent as Keesom interactions because the induced dipole is free to shift and rotate around the non-polar molecule. The Debye induction effects and Keesom orientation effects are referred to as polar interactions.
The induced dipole forces appear from the induction (also known as polarization), which is the attractive interaction between a permanent multipole on one molecule with an induced (by the former di/multi-pole) multipole on another. This interaction is called the Debye force, named after Peter J.W. Debye.
One example of an induction-interaction between permanent dipole and induced dipole is the interaction between HCl and Ar. In this system, Ar experiences a dipole as its electrons are attracted (to the H side of HCl) or repelled (from the Cl side) by HCl. The angle averaged interaction is given by the following equation.
formula_5
Where formula_6= polarizability
This kind of interaction can be expected between any polar molecule and non-polar/symmetrical molecule. The induction-interaction force is far weaker than dipole-dipole interaction, but stronger than the London dispersion force.
London dispersion force (induced-induced dipoles interaction).
The third and dominant contribution is the dispersion or London force (fluctuating dipole-induced dipole), which arises due to the non-zero instantaneous dipole moments of all atoms and molecules. Such polarization can be induced either by a polar molecule or by the repulsion of negatively charged electron clouds in non-polar molecules. Thus, London interactions are caused by random fluctuations of electron density in an electron cloud. An atom with a large number of electrons will have a greater associated London force than an atom with fewer electrons. The dispersion (London) force is the most important component because all materials are polarizable, whereas Keesom and Debye forces require permanent dipoles. The London interaction is universal and is present in atom-atom interactions as well. For various reasons, London interactions (dispersion) have been considered relevant for interactions between macroscopic bodies in condensed systems. Hamaker developed the theory of van der Waals between macroscopic bodies in 1937 and showed that the additivity of these interactions renders them considerably more long-range.
Relative strength of forces.
Note: this comparison is only approximate – the actual relative strengths will vary depending on the molecules involved. Ionic and covalent bonding will always be stronger than intermolecular forces in any given substance.
Quantum mechanical theories.
Intermolecular forces observed between atoms and molecules can be described phenomenologically as occurring between permanent and instantaneous dipoles, as outlined above. Alternatively, one may seek a fundamental, unifying theory that is able to explain the various types of interactions such as hydrogen bonding, van der Waals forces and dipole-dipole interactions. Typically, this is done by applying the ideas of quantum mechanics to molecules, and Rayleigh–Schrödinger perturbation theory has been especially effective in this regard. When applied to existing quantum chemistry methods, such a quantum mechanical explanation of intermolecular interactions, this provides an array of approximate methods that can be used to analyze intermolecular interactions.

</doc>
<doc id="15420" url="http://en.wikipedia.org/wiki?curid=15420" title="IRQ">
IRQ

IRQ may refer to:

</doc>
<doc id="15422" url="http://en.wikipedia.org/wiki?curid=15422" title="List of Internet top-level domains">
List of Internet top-level domains

This is a list of Internet top-level domain (TLDs). A top-level domain is a domain name in the Domain Name System that is a direct subdomain of the DNS root zone. The official list of all top-level domains is maintained by the Internet Assigned Numbers Authority (IANA). In an initiative to expand the domain name space, IANA is considering approval of several proposed top-level domains. s of February 2015[ [update]], the root domain contains 810 top-level domains, while a few have been retired and are no longer functional.
Original top-level domains.
Seven generic top-level domains were created early in that development of the Internet, and pre-date the creation of ICANN in 1998.
Country code top-level domains.
"Note:" the country code domain system was created in the early days of the Domain Name System, and pre-dates ICANN.
Internationalized country code top-level domains.
Source:
Internationalized generic top-level domains.
All of these TLDs are internationalized domain names (IDN) and support second-level IDNs.
Test TLDs.
ICANN created a set of top-level Internationalized domain names in October 2007 for the purpose of testing the use of IDNA in the root zone and within those domains. These testing domains were abolished on 31 October 2013. Each of these TLDs encoded a word meaning "test" in the respective language.
Each of these domains contained only one site with the word "example" encoded in the respective script and language. These "example.test" sites were test wikis used by ICANN.

</doc>
<doc id="15428" url="http://en.wikipedia.org/wiki?curid=15428" title="Idealism">
Idealism

In philosophy, idealism is the group of philosophies which assert that reality, or reality as we can know it, is fundamentally mental, mentally constructed, or otherwise immaterial. Epistemologically, idealism manifests as a skepticism about the possibility of knowing any mind-independent thing. In a sociological sense, idealism emphasizes how human ideas—especially beliefs and values—shape society. As an ontological doctrine, idealism goes further, asserting that all entities are composed of mind or spirit. Idealism thus rejects physicalist and dualist theories that fail to ascribe priority to the mind.
The earliest extant arguments that the world of experience is grounded in the mental derive from India and Greece. The Hindu idealists in India and the Greek Neoplatonists gave panentheistic arguments for an all-pervading consciousness as the ground or true nature of reality. In contrast, the Yogācāra school, which arose within Mahayana Buddhism in India in the 4th century CE, based its "mind-only" idealism to a greater extent on phenomenological analyses of personal experience. This turn toward the subjective anticipated empiricists such as George Berkeley, who revived idealism in 18th-century Europe by employing skeptical arguments against materialism.
Beginning with Immanuel Kant, German idealists such as G. W. F. Hegel, Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Arthur Schopenhauer dominated 19th-century philosophy. This tradition, which emphasized the mental or "ideal" character of all phenomena, birthed idealistic and subjectivist schools ranging from British idealism to phenomenalism to existentialism. The historical influence of this branch of idealism remains central even to the schools that rejected its metaphysical assumptions, such as Marxism, pragmatism and positivism.
Definitions.
"Idealism" is a term with several related meanings. It comes via "idea" from the Greek "idein" (ἰδεῖν), meaning "to see". The term entered the English language by 1743. In ordinary use, as when speaking of Woodrow Wilson's political idealism, it generally suggests the priority of ideals, principles, values, and goals over concrete realities. Idealists are understood to represent the world as it might or should be, unlike pragmatists, who focus on the world as it presently is. In the arts, similarly, idealism affirms imagination and attempts to realize a mental conception of beauty, a standard of perfection, juxtaposed to aesthetic naturalism and realism.
Any philosophy that assigns crucial importance to the ideal or spiritual realm in its account of human existence may be termed "idealist". Metaphysical idealism is an ontological doctrine that holds that reality itself is incorporeal or experiential at its core. Beyond this, idealists disagree on which aspects of the mental are more basic. Platonic idealism affirms that abstractions are more basic to reality than the things we perceive, while subjective idealists and phenomenalists tend to privilege sensory experience over abstract reasoning. Epistemological idealism is the view that reality can only be known through ideas, that only psychological experience can be apprehended by the mind.
Subjective idealists like George Berkeley are anti-realists in terms of a mind-independent world, whereas transcendental idealists like Immanuel Kant are strong skeptics of such a world, affirming epistemological and not metaphysical idealism. Thus Kant defines "idealism" as "the assertion that we can never be certain whether all of our putative outer experience is not mere imagining". He claimed that, according to "idealism", "the reality of external objects does not admit of strict proof. On the contrary, however, the reality of the object of our internal sense (of myself and state) is clear immediately through consciousness." However, not all idealists restrict the real or the knowable to our immediate subjective experience. Objective idealists make claims about a transempirical world, but simply deny that this world is essentially divorced from or ontologically prior to the mental. Thus Plato and Gottfried Leibniz affirm an objective and knowable reality transcending our subjective awareness—a rejection of epistemological idealism—but propose that this reality is grounded in ideal entities, a form of metaphysical idealism. Nor do all metaphysical idealists agree on the nature of the ideal; for Plato, the fundamental entities were non-mental abstract forms, while for Leibniz they were proto-mental and concrete monads.
As a rule, transcendental idealists like Kant affirm idealism's epistemic side without committing themselves to whether reality is "ultimately" mental; objective idealists like Plato affirm reality's metaphysical basis in the mental or abstract without restricting their epistemology to ordinary experience; and subjective idealists like Berkeley affirm both metaphysical and epistemological idealism.
Classical idealism.
Monistic idealism holds that consciousness, not matter, is the ground of all being. It is monist because it holds that there is only one type of thing in the universe and idealist because it holds that one thing to be consciousness.
Anaxagoras (480 BC) was known as "Nous" ("Mind") because he taught that "all things" were created by Mind, that Mind held the cosmos together and gave human beings a connection to the cosmos or a pathway to the divine.
Many religious philosophies are specifically idealist. The belief that beings with knowledge (God/s, angels & spirits) preceded insentient matter seems to suggest that an experiencing subject is a necessary reality. Hindu idealism is central to Vedanta philosophy and to such schools as Kashmir Shaivism. Proponents include P.R. Sarkar and his disciple Sohail Inayatullah.
Christian theologians have held idealist views, often based on Neoplatonism, despite the influence of Aristotelian scholasticism from the 12th century onward. Later western theistic idealism such as that of Hermann Lotze offers a theory of the "world ground" in which all things find their unity: it has been widely accepted by Protestant theologians. Several modern religious movements, for example the organizations within the New Thought Movement and the Unity Church, may be said to have a particularly idealist orientation. The theology of Christian Science includes a form of idealism: it teaches that all that truly exists is God and God's ideas; that the world as it appears to the senses is a distortion of the underlying spiritual reality, a distortion that may be corrected (both conceptually and in terms of human experience) through a reorientation (spiritualization) of thought.
Wang Yangming, a Ming Chinese neo-Confucian philosopher, official, educationist, calligraphist and general, held that objects do not exist entirely apart from the mind because the mind shapes them. It is not the world that shapes the mind but the mind that gives reason to the world, so the mind alone is the source of all reason, having an inner light, an innate moral goodness and understanding of what is good.
The consciousness-only approach of the Yogācāra school of Mahayana Buddhism is not true metaphysical idealism as Yogācāra thinkers did not focus on consciousness to assert it as ultimately real, it is only conventionally real since it arises from moment to moment due to fluctuating causes and conditions and is significant because it is the cause of karma and hence suffering.
Platonism and neoplatonism.
Plato's theory of forms or "ideas" describes ideal forms (for example the platonic solids in geometry or abstracts like Goodness and Justice), as universals existing independently of any particular instance. Arne Grøn calls this doctrine "the classic example of a metaphysical idealism as a "transcendent" idealism", while Simone Klein calls Plato "the earliest representative of metaphysical objective idealism". Nevertheless Plato holds that matter is real, though transitory and imperfect, and is perceived by our body and its senses and given existence by the eternal ideas that are perceived directly by our rational soul. Plato was therefore a metaphysical and epistemological dualist, an outlook that modern idealism has striven to avoid: Plato's thought cannot therefore be counted as idealist in the modern sense.
With the neoplatonist Plotinus, wrote Nathaniel Alfred Boll; "there even appears, probably for the first time in Western philosophy, "idealism" that had long been current in the East even at that time, for it taught... that the soul has made the world by stepping from eternity into time...". Similarly, in regard to passages from the Enneads, "The only space or place of the world is the soul" and "Time must not be assumed to exist outside the soul", Ludwig Noiré wrote: "For the first time in Western philosophy we find idealism proper in Plotinus, However, Plotinus does not address whether we know external objects, unlike Schopenhauer and other modern philosophers.
Subjective idealism.
Subjective Idealism (immaterialism or phenomenalism) describes a relationship between experience and the world in which objects are no more than collections or "bundles" of sense data in the perceiver. Proponents include Berkeley, Bishop of Cloyne, an Irish philosopher who advanced a theory he called immaterialism, later referred to as "subjective idealism", contending that individuals can only know sensations and ideas of objects directly, not abstractions such as "matter", and that ideas also depend upon being perceived for their very existence - "esse est percipi"; "to be is to be perceived".
Arthur Collier published similar assertions though there seems to have been no influence between the two contemporary writers. The only knowable reality is the represented image of an external object. Matter as a cause of that image, is unthinkable and therefore nothing to us. An external world as absolute matter unrelated to an observer does not exist as far as we are concerned. The universe cannot exist as it appears if there is no perceiving mind. Collier was influenced by "An Essay Towards the Theory of the Ideal or Intelligible World" by "Cambridge Platonist" John Norris (1701).
Bertrand Russell's popular book "The Problems of Philosophy" highlights Berkeley's tautological premise for advancing idealism;
The Australian philosopher David Stove harshly, arguing that it rests on what he called "the worst argument in the world". Stove claims that Berkeley tried to derive a non-tautological conclusion from tautological reasoning. He argued that in Berkeley's case the fallacy is not obvious and this is because one premise is ambiguous between one meaning which is tautological and another which, Stove argues, is logically equivalent to the conclusion.
Alan Musgrave argues that conceptual idealists compound their mistakes with use/mention confusions;
and proliferation of hyphenated entities such as "thing-in-itself" (Immanuel Kant), "things-as-interacted-by-us" (Arthur Fine), "table-of-commonsense" and "table-of-physics" (Sir Arthur Eddington) which are "warning signs" for conceptual idealism according to Musgrave because they allegedly do not exist but only highlight the numerous ways in which people come to know the world. This argument does not take into account the issues pertaining to hermeneutics, especially at the backdrop of analytic philosophy. Musgrave criticized Richard Rorty and Postmodernist philosophy in general for confusion of use and mention.
A. A. Luce and John Foster are other subjectivists. Luce, in "Sense without Matter" (1954), attempts to bring Berkeley up to date by modernising his vocabulary and putting the issues he faced in modern terms, and treats the Biblical account of matter and the psychology of perception and nature. Foster's "The Case for Idealism" argues that the physical world is the logical creation of natural, non-logical constraints on human sense-experience. Foster's latest defence of his views is in his book "A World for Us: The Case for Phenomenalistic Idealism".
Paul Brunton, a British philosopher, mystic, traveler, and guru, taught a type of idealism called "mentalism", similar to that of Bishop Berkeley, proposing a master world-image, projected or manifested by a world-mind, and an infinite number of individual minds participating. A tree does not cease to exist if nobody sees it because the world-mind is projecting the idea of the tree to all minds.
John Searle criticising some versions of idealism, summarises two important arguments for subjective idealism. The first is based on our perception of reality;
therefore;
Whilst agreeing with (2) Searle argues that (1) is false and points out that (3) does not follow from (1) and (2). The second argument runs as follows;
Searle contends that "Conclusion 2" does not follow from the premises.
Epistemological idealism is a subjectivist position in epistemology that holds that what one knows about an object exists only in one's mind. Proponents include Brand Blanshard.
Transcendental idealism.
Transcendental idealism, founded by Immanuel Kant in the eighteenth century, maintains that the mind shapes the world we perceive into the form of space-and-time.
 ... if I remove the thinking subject, the whole material world must at once vanish because it is nothing but a phenomenal appearance in the sensibility of ourselves as a subject, and a manner or species of representation.
 — "Critique of Pure Reason" A383
The 2nd edition (1787) contained a "Refutation of Idealism" to distinguish his transcendental idealism from Descartes's Sceptical Idealism and Berkeley's Dogmatic Idealism. The section "Paralogisms of Pure Reason" is an implicit critique of Descartes' idealism. Kant says that it is not possible to infer the 'I' as an object (Descartes' "cogito ergo sum") purely from "the spontaneity of thought". Kant focused on ideas drawn from British philosophers such as Locke, Berkeley and Hume but distinguished his transcendental or critical idealism from previous varieties;
 The dictum of all genuine idealists, from the Eleatic school to Bishop Berkeley, is contained in this formula: “All knowledge through the senses and experience is nothing but sheer illusion, and only in the ideas of the pure understanding and reason is there truth.” The principle that throughout dominates and determines my [transcendental] idealism is, on the contrary: “All knowledge of things merely from pure understanding or pure reason is nothing but sheer illusion, and only in experience is there truth.”
 — "Prolegomena", 374
Kant distinguished between things as they appear to an observer and things in themselves, "that is, things considered without regard to whether and how they may be given to us". We cannot approach the "noumenon", the "thing in Itself" (German: "Ding an sich") without our own mental world. He added that the mind is not a blank slate, "tabula rasa" but rather comes equipped with categories for organising our sense impressions.
In the first volume of his "Parerga and Paralipomena", Schopenhauer wrote his "Sketch of a History of the Doctrine of the Ideal and the Real". He defined the ideal as being mental pictures that constitute subjective knowledge. The ideal, for him, is what can be attributed to our own minds. The images in our head are what comprise the ideal. Schopenhauer emphasized that we are restricted to our own consciousness. The world that appears is only a representation or mental picture of objects. We directly and immediately know only representations. All objects that are external to the mind are known indirectly through the mediation of our mind. He offered a history of the concept of the "ideal" as "ideational" or "existing in the mind as an image".
 [T]rue philosophy must at all costs be "idealistic"; indeed, it must be so merely to be honest. For nothing is more certain than that no one ever came out of himself in order to identify himself immediately with things different from him; but everything of which he has certain, sure, and therefore immediate knowledge, lies within his consciousness. Beyond this consciousness, therefore, there can be no "immediate" certainty ... There can never be an existence that is objective absolutely and in itself; such an existence, indeed, is positively inconceivable. For the objective, as such, always and essentially has its existence in the consciousness of a subject; it is therefore the subject's representation, and consequently is conditioned by the subject, and moreover by the subject's forms of representation, which belong to the subject and not to the object.
 — "The World as Will and Representation", Vol. II, Ch. 1
Charles Bernard Renouvier was the first Frenchman after Nicolas Malebranche to formulate a complete idealistic system, and had a vast influence on the development of French thought. His system is based on Immanuel Kant's, as his chosen term "néo-criticisme" indicates; but it is a transformation rather than a continuation of Kantianism.
Friedrich Nietzsche argued that Kant commits an agnostic tautology and does not offer a satisfactory answer as to the "source" of a philosophical right to such-or-other metaphysical claims; he ridicules his pride in tackling "the most difficult thing that could ever be undertaken on behalf of metaphysics." The famous "thing-in-itself" was called a product of philosophical habit, which seeks to introduce a grammatical subject: because wherever there is cognition, there must be a "thing" that is cognized and allegedly it must be added to ontology as a being (whereas, to Nietzsche, only the world as ever changing appearances can be assumed). Yet he attacks the idealism of Schopenhauer and Descartes with an argument similar to Kant's critique of the latter "(see above)".
Objective idealism.
Objective idealism asserts that the reality of experiencing combines and transcends the realities of the object experienced and of the mind of the observer. Proponents include Thomas Hill Green, Josiah Royce, Benedetto Croce and Charles Sanders Peirce.
Absolute idealism.
Schelling (1775–1854) claimed that the Fichte's "I" needs the Not-I, because there is no subject without object, and vice versa. So there is no difference between the subjective and the objective, that is, the ideal and the real. This is Schelling's "absolute identity": the ideas or mental images in the mind are identical to the extended objects which are external to the mind.
Absolute idealism is G. W. F. Hegel's account of how existence is comprehensible as an all-inclusive whole. Hegel called his philosophy "absolute" idealism in contrast to the "subjective idealism" of Berkeley and the "transcendental idealism" of Kant and Fichte, which were not based on a critique of the finite and a dialectical philosophy of history as Hegel's idealism was. The exercise of reason and intellect enables the philosopher to know ultimate historical reality, the phenomenological constitution of self-determination, the dialectical development of self-awareness and personality in the realm of History.
In his "Science of Logic" (1812–1814) Hegel argues that finite qualities are not fully "real" because they depend on other finite qualities to determine them. Qualitative "infinity", on the other hand, would be more self-determining and hence more fully real. Similarly finite natural things are less "real"—because they are less self-determining—than spiritual things like morally responsible people, ethical communities and God. So any doctrine, such as materialism, that asserts that finite qualities or natural objects are fully real is mistaken.
Hegel certainly intends to preserve what he takes to be true of German idealism, in particular Kant's insistence that ethical reason can and does go beyond finite inclinations. For Hegel there must be some identity of thought and being for the "subject" (any human observer)) to be able to know any observed "object" (any external entity, possibly even another human) at all. Under Hegel's concept of "subject-object identity," subject and object both have Spirit (Hegel's ersatz, redefined, nonsupernatural "God") as their "conceptual" (not metaphysical) inner reality—and in that sense are identical. But until Spirit's "self-realization" occurs and Spirit graduates from Spirit to "Absolute" Spirit status, subject (a human mind) mistakenly thinks every "object" it observes is something "alien," meaning something separate or apart from "subject." In Hegel's words, "The object is revealed to it [to "subject"] by [as] something alien, and it does not recognize itself." Self-realization occurs when Hegel (part of Spirit's nonsupernatural Mind, which is the collective mind of all humans) arrives on the scene and realizes that every "object" is "himself", because both subject and object are essentially Spirit. When self-realization occurs and Spirit becomes "Absolute" Spirit, the "finite" (man, human) becomes the "infinite" ("God," divine), replacing the imaginary or "picture-thinking" supernatural God of theism: man becomes God. Tucker puts it this way: "Hegelianism . . . is a religion of self-worship whose fundamental theme is given in Hegel's image of the man who aspires to be God himself, who demands 'something more, namely infinity.'" The picture Hegel presents is "a picture of a self-glorifying humanity striving compulsively, and at the end successfully, to rise to divinity."
Kierkegaard criticised Hegel's idealist philosophy in several of his works, particularly his claim to a comprehensive system that could explain the whole of reality. Where Hegel argues that an ultimate understanding of the logical structure of the world is an understanding of the logical structure of God's mind, Kierkegaard asserting that for God reality can be a system but it cannot be so for any human individual because both reality and humans are incomplete and all philosophical systems imply completeness. A logical system is possible but an existential system is not. "What is rational is actual; and what is actual is rational". Hegel's absolute idealism blurs the distinction between existence and thought: our mortal nature places limits on our understanding of reality;
So-called systems have often been characterized and challenged in the assertion that they abrogate the distinction between good and evil, and destroy freedom. Perhaps one would express oneself quite as definitely, if one said that every such system fantastically dissipates the concept existence. ... Being an individual man is a thing that has been abolished, and every speculative philosopher confuses himself with humanity at large; whereby he becomes something infinitely great, and at the same time nothing at all.
A major concern of Hegel's "Phenomenology of Spirit" (1807) and of the philosophy of Spirit that he lays out in his "Encyclopedia of the Philosophical Sciences" (1817–1830) is the interrelation between individual humans, which he conceives in terms of "mutual recognition." However, what Climacus means by the aforementioned statement, is that Hegel, in the "Philosophy of Right", believed the best solution was to surrender one's individuality to the customs of the State, identifying right and wrong in view of the prevailing bourgeois morality. Individual human will ought, at the State's highest level of development, to properly coincide with the will of the State. Climacus rejects Hegel's suppression of individuality by pointing out it is impossible to create a valid set of rules or system in any society which can adequately describe existence for any one individual. Submitting one's will to the State denies personal freedom, choice, and responsibility.
In addition, Hegel does believe we can know the structure of God's mind, or ultimate reality. Hegel agrees with Kierkegaard that both reality and humans are incomplete, inasmuch as we are in time, and reality develops through time. But the relation between time and eternity is outside time and this is the "logical structure" that Hegel thinks we can know. Kierkegaard disputes this assertion, because it eliminates the clear distinction between ontology and epistemology. Existence and thought are not identical and one cannot possibly think existence. Thought is always a form of abstraction, and thus not only is pure existence impossible to think, but all forms in existence are unthinkable; thought depends on language, which merely abstracts from experience, thus separating us from lived experience and the living essence of all beings. In addition, because we are finite beings, we cannot possibly know or understand anything that is universal or infinite such as God, so we cannot know God exists, since that which transcends time simultaneously transcends human understanding.
Bradley saw reality as a monistic whole apprehended through "feeling", a state in which there is no distinction between the perception and the thing perceived. Like Berkeley, Bradley thought that nothing can be known to exist unless it is known by a mind.
 We perceive, on reflection, that to be real, or even barely to exist, must be to fall within sentience ... . Find any piece of existence, take up anything that any one could possibly call a fact, or could in any sense assert to have being, and then judge if it does not consist in sentient experience. Try to discover any sense in which you can still continue to speak of it, when all perception and feeling have been removed; or point out any fragment of its matter, any aspect of its being, which is not derived from and is not still relative to this source. When the experiment is made strictly, I can myself conceive of nothing else than the experienced.
 — F.H. Bradley, 'Appearance and Reality', Chapter 14
Bradley was the apparent target of G. E. Moore's radical rejection of idealism. Moore claimed that Bradley did not understand the statement that something is real. We know for certain, through common sense and prephilosophical beliefs, that some things are real, whether they are objects of thought or not, according to Moore. The 1903 article "The Refutation of Idealism" is one of the first demonstrations of Moore's commitment to analysis. He examines each of the three terms in the Berkeleian aphorism "esse est percipi", "to be is to be perceived", finding that it must mean that the object and the subject are "necessarily" connected so that "yellow" and "the sensation of yellow" are identical - "to be yellow" is "to be experienced as yellow". But it also seems there is a difference between "yellow" and "the sensation of yellow" and "that "esse" is held to be "percipi", solely because what is experienced is held to be identical with the experience of it". Though far from a complete refutation, this was the first strong statement by analytic philosophy against its idealist predecessors, or at any rate against the type of idealism represented by Berkeley. This argument did not show that the GEM (in post–Stove vernacular, see below) is logically invalid.
Actual idealism.
Actual Idealism is a form of idealism developed by Giovanni Gentile that grew into a "grounded" idealism contrasting Kant and Hegel.
Pluralistic idealism.
Pluralistic idealism such as that of Gottfried Leibniz takes the view that there are many individual minds that together underlie the existence of the observed world and make possible the existence of the physical universe. Unlike absolute idealism, pluralistic idealism does not assume the existence of a single ultimate mental reality or "Absolute". Leibniz' form of idealism, known as Panpsychism, views "monads" as the true atoms of the universe and as entities having perception. The monads are "substantial forms of being",elemental, individual, subject to their own laws, non-interacting, each reflecting the entire universe. Monads are centers of force, which is substance while space, matter and motion are phenomenal and their form and existence is dependent on the simple and immaterial monads. There is a pre-established harmony established by God, the central monad, between the world in the minds of the monads and the external world of objects. Leibniz's cosmology embraced traditional Christian Theism. The English psychologist and philosopher James Ward inspired by Leibniz had also defended a form of pluralistic idealism. According to Ward the universe is composed of "psychic monads" of different levels, interacting for mutual self- betterment.
Personalism is the view that the minds that underlie reality are the minds of persons. Borden Parker Bowne, a philosopher at Boston University, a founder and popularizer of personal idealism, presented it as a substantive reality of persons, the only reality, as known directly in self-consciousness. Reality is a society of interacting persons dependent on the Supreme Person of God. Other proponents include George Holmes Howison and J. M. E. McTaggart.
Howison's personal idealism was also called "California Personalism" by others to distinguish it from the "Boston Personalism" which was of Bowne. Howison maintained that both impersonal, monistic idealism and materialism run contrary to the experience of moral freedom. To deny freedom to pursue truth, beauty, and "benignant love" is to undermine every profound human venture, including science, morality, and philosophy. Personalistic idealists Borden Parker Bowne and Edgar S. Brightman and realistic personal theist Saint Thomas Aquinas address a core issue, namely that of dependence upon an infinite personal God.
Howison, in his book "The Limits of Evolution and Other Essays Illustrating the Metaphysical Theory of Personal Idealism", created a democratic notion of personal idealism that extended all the way to God, who was no more the ultimate monarch but the ultimate democrat in eternal relation to other eternal persons. J. M. E. McTaggart's idealist atheism and Thomas Davidson's Apeirionism resemble Howisons personal idealism.
J. M. E. McTaggart of Cambridge University, argued that minds alone exist and only relate to each other through love. Space, time and material objects are unreal. In "The Unreality of Time" he argued that time is an illusion because it is impossible to produce a coherent account of a sequence of events. "The Nature of Existence" (1927) contained his arguments that space, time, and matter cannot possibly be real. In his "Studies in Hegelian Cosmology" (Cambridge, 1901, p196) he declared that metaphysics are not relevant to social and political action. McTaggart "thought that Hegel was wrong in supposing that metaphysics could show that the state is more than a means to the good of the individuals who compose it". For McTaggart "philosophy can give us very little, if any, guidance in action... Why should a Hegelian citizen be surprised that his belief as to the organic nature of the Absolute does not help him in deciding how to vote? Would a Hegelian engineer be reasonable in expecting that his belief that all matter is spirit should help him in planning a bridge?
Thomas Davidson taught a philosophy called "apeirotheism", a "form of pluralistic idealism...coupled with a stern ethical rigorism" which he defined as "a theory of Gods infinite in number." The theory was indebted to Aristotle's pluralism and his concepts of Soul, the rational, living aspect of a living substance which cannot exist apart from the body because it is not a substance but an essence, and "nous", rational thought, reflection and understanding. Although a perennial source of controversy, Aristotle arguably views the latter as both eternal and immaterial in nature, as exemplified in his theology of unmoved movers. Identifying Aristotle's God with rational thought, Davidson argued, contrary to Aristotle, that just as the soul cannot exist apart from the body, God cannot exist apart from the world.
Idealist notions took a strong hold among physicists of the early 20th century confronted with the paradoxes of quantum physics and the theory of relativity. In "The Grammar of Science", Preface to the 2nd Edition, 1900, Karl Pearson wrote, "There are many signs that a sound idealism is surely replacing, as a basis for natural philosophy, the crude materialism of the older physicists." This book influenced Einstein's regard for the importance of the observer in scientific measurements. In § 5 of that book, Pearson asserted that "...science is in reality a classification and analysis of the contents of the mind..." Also, "...the field of science is much more consciousness than an external world."
Sir Arthur Eddington, a British astrophysicist of the early 20th century, wrote in his book "The Nature of the Physical World"; "The stuff of the world is mind-stuff";
"The mind-stuff of the world is, of course, something more general than our individual conscious minds... The mind-stuff is not spread in space and time; these are part of the cyclic scheme ultimately derived out of it... It is necessary to keep reminding ourselves that all knowledge of our environment from which the world of physics is constructed, has entered in the form of messages transmitted along the nerves to the seat of consciousness... Consciousness is not sharply defined, but fades into subconsciousness; and beyond that we must postulate something indefinite but yet continuous with our mental nature... It is difficult for the matter-of-fact physicist to accept the view that the substratum of everything is of mental character. But no one can deny that mind is the first and most direct thing in our experience, and all else is remote inference."
Ian Barbour in his book "Issues in Science and Religion" (1966), p. 133, cites Arthur Eddington's "The Nature of the Physical World" (1928) for a text that argues The Heisenberg Uncertainty Principles provides a scientific basis for "the defense of the idea of human freedom" and his "Science and the Unseen World" (1929) for support of philosophical idealism "the thesis that reality is basically mental".
Sir James Jeans wrote; "The stream of knowledge is heading towards a non-mechanical reality; the Universe begins to look more like a great thought than like a great machine. Mind no longer appears to be an accidental intruder into the realm of matter... we ought rather hail it as the creator and governor of the realm of matter."
Jeans, in an interview published in The Observer (London), when asked the question:
"Do you believe that life on this planet is the result of some sort of accident, or do you believe that it is a part of some great scheme?" 
replied:
"I incline to the idealistic theory that consciousness is fundamental, and that the material universe is derivative from consciousness, not consciousness from the material universe... In general the universe seems to me to be nearer to a great thought than to a great machine. It may well be, it seems to me, that each individual consciousness ought to be compared to a brain-cell in a universal mind."
"What remains is in any case very different from the full-blooded matter and the forbidding materialism of the Victorian scientist. His objective and material universe is proved to consist of little more than constructs of our own minds. To this extent, then, modern physics has moved in the direction of philosophic idealism. Mind and matter, if not proved to be of similar nature, are at least found to be ingredients of one single system. There is no longer room for the kind of dualism which has haunted philosophy since the days of Descartes." Sir James Jeans addressing the British Association in 1934.
"Finite picture whose dimensions are a certain amount of space and a certain amount of time; the protons and electrons are the streaks of paint which define the picture against its space-time background. Traveling as far back in time as we can, brings us not to the creation of the picture, but to its edge; the creation of the picture lies as much outside the picture as the artist is outside his canvas. On this view, discussing the creation of the universe in terms of time and space is like trying to discover the artist and the action of painting, by going to the edge of the canvas. This brings us very near to those philosophical systems which regard the universe as a thought in the mind of its Creator, thereby reducing all discussion of material creation to futility." Sir James Jeans "The Universe Around Us" page 317.
The chemist Ernest Lester Smith wrote a book "Intelligence Came First" (1975) in which he claimed that consciousness is a fact of nature and that the cosmos is grounded in and pervaded by mind and intelligence.
Bernard d'Espagnat a French theoretical physicist best known for his work on the nature of reality wrote a paper titled "The Quantum Theory and Reality" according to the paper: "The doctrine that the world is made up of objects whose existence is independent of human consciousness turns out to be in conflict with quantum mechanics and with facts established by experiment." In an article in the Guardian titled "Quantum weirdness: What we call 'reality' is just a state of mind" d'Espagnat wrote that:
"What quantum mechanics tells us, I believe, is surprising to say the least. It tells us that the basic components of objects – the particles, electrons, quarks etc. – cannot be thought of as "self-existent". He further writes that his research in quantum physics has led him to conclude that an "ultimate reality" exists, which is not embedded in space or time.
References.
Further reading

</doc>
<doc id="15430" url="http://en.wikipedia.org/wiki?curid=15430" title="Inheritance">
Inheritance

Inheritance is the practice of passing on property, titles, debts, rights and obligations upon the death of an individual. It has long played an important role in human societies. The rules of inheritance differ between societies and have changed over time.
Terminology.
In law, an "heir" is a person who is entitled to receive a share of the deceased's (the person who died) property, subject to the rules of inheritance in the jurisdiction where the deceased (decedent) died or owned property at the time of death. A person does not become an heir before the death of the deceased, since the exact identity of the persons entitled to inherit is determined only then. Members of ruling noble or royal houses expected to become heirs are called heirs apparent if first in line and incapable of being displaced from inheriting by another claim; otherwise, they are heirs presumptive. There is a further concept of joint inheritance, pending renunciation by all but one, which is called coparceny.
In modern law, the terms "inheritance" and "heir" refer exclusively to succession to property by descent from a deceased dying intestate. Takers in property succeeded to under a will are termed generally "beneficiaries," and specifically "devisees" for real property, "bequestees" for personal property, or "legatees" for money.
History.
Detailed anthropological and sociological studies have been made about customs of patrilineal inheritance, where only male children can inherit. Some cultures also employ matrilineal succession, where property can only pass along the female line, most commonly going to the sister's sons of the decedent; but also, in some societies, from the mother to her daughters. Some ancient societies and most modern states employ egalitarian inheritance, without discrimination based on gender and/or birth order.
Islamic laws of inheritance.
The Quran introduced a number of different rights and restrictions on matters of inheritance, including general improvements to the treatment of women and family life compared to the pre-Islamic societies that existed in the Arabian Peninsula at the time. The Quran also presented efforts to fix the laws of inheritance, and thus forming a complete legal system. This development was in contrast to pre-Islamic societies where rules of inheritance varied considerably. Furthermore, the Quran introduced additional heirs that were not entitled inheritance in pre-Islamic times, mentioning nine relatives specifically of which six were female and three were male. In addition to the above changes, the Quran imposed restrictions on testamentary powers of a Muslim in disposing his or her property. In their will, a Muslim can only give out a maximum of one third of their property.
The Quran contains only three verses that give specific details of inheritance and shares, in addition to few other verses dealing with testamentary. #Redirect But this information was used as a starting point by Muslim jurists who expounded the laws of inheritance even further using Hadith, as well as methods of juristic reasoning like Qiyas. Nowadays, inheritance is considered an integral part of Shariah Law and its application for Muslims is mandatory, though many peoples (see Historical inheritance systems), despite being Muslim, have other inheritance customs.
Jewish laws of inheritance.
The inheritance is patrilineal. The father —that is, the owner of the land— bequeaths only to his male descendants, so the Promised Land passes from one Jewish father to his sons. 
If there were no living sons and no descendants of any previously living sons, daughters could inherit. In , the daughters of Zelophehad (Mahlah, Noa, Hoglah, Milcah, and Tirzah) of the tribe of Manasseh come to Moses and ask for their father's inheritance, as they have no brothers. The order of inheritance is set out in : a man's sons inherit first, daughters if no sons, brothers if he has no children, and so on.
Later, in , some of the heads of the families of the tribe of Manasseh come to Moses and point out that, if a daughter inherits and then marries a man not from her paternal tribe, her land will pass from her birth-tribe's inheritance into her marriage-tribe's. So a further rule is laid down: if a daughter inherits land, she must marry someone within her father's tribe. (The daughters of Zelophehad marry the sons' of their father's brothers. There is "no" indication that this was not their choice.)
The tractate Baba Bathra, written during late Antiquity in Babylon, deals extensively with issues of property ownership and inheritance according to Jewish Law. Other works of Rabbinical Law, such as the Hilkhot naḥalot : mi-sefer Mishneh Torah leha-Rambam, and the Sefer ha-yerushot: ʻim yeter ha-mikhtavim be-divre ha-halakhah be-ʻAravit uve-ʻIvrit uve-Aramit also deal with inheritance issues. The first, often abbreviated to Mishneh Torah, was written by Maimonides and was very important in Jewish tradition. 
All these sources agree that the firstborn son is entitled to a double portion of his father's estate: . This means that, for example, if a father left five sons, the firstborn receives a third of the estate and each of the other four receives a sixth. If he left nine sons, the firstborn receives a fifth and each of the other eight receive a tenth. If the eldest surviving son is not the firstborn son, he is not entitled to the double portion.
Philo of Alexandria and Josephus also comment on the Jewish laws of inheritance, praising them above other law codes of their time. They also agreed that the firstborn son must receive a double portion of his father's estate.
Inheritance inequality.
The distribution of the inherited wealth is often unequal. The majority might receive little while only a small number inherit a larger amount, with the lesser amount given to daughter in the family. The amount of inheritance is often far less than the value of a business initially given to the son, especially when a son takes over a thriving multi-million dollar business, yet the daughter is given the balance of the actual inheritance amounting to far less than the value of business that was initially given to the son. This is especially seen in old world cultures, but continues in many families to this day.
Arguments for eliminating the disparagement of inheritance inequality include the right to property and the merit of individual allocation of capital over government wealth confiscation and redistribution, but this does not resolve the problem of unequal inheritance. In terms of inheritance inequality, some economists and sociologists focus on the inter generational transmission of income or wealth which is said to have a direct impact on one's mobility (or immobility) and class position in society. Nations differ on the political structure and policy options that govern the transfer of wealth.
According to the American federal government statistics compiled by Mark Zandi, currently of "Moody's Economy.com", back in 1985, the average inheritance was $39,000. In subsequent years, the overall amount of total annual inheritance was more than doubled, reaching nearly $200 billion. By 2050, there is an estimated $25 trillion average inheritance transmitted across generations. Some researchers have attributed this rise to the baby boomer generation. Historically, the baby boomers were the largest influx of children conceived after WW2. For this reason, Thomas Shapiro suggests that this generation "is in the midst of benefiting from the greatest inheritance of wealth in history."
Inherited wealth may help explain why many Americans who have become rich may have had a "substantial head start". In September 2012, according to the Institute for Policy Studies, "over 60 percent" of the Forbes richest 400 Americans "grew up in substantial privilege".
Inheritance and social stratification.
Inheritance inequality has a significant effect on stratification. Inheritance is an integral component of family, economic, and legal institutions, and a basic mechanism of class stratification. It also affects the distribution of wealth at the societal level. The total cumulative effect of inheritance on stratification outcomes takes three forms. The first form of inheritance is the inheritance of cultural capital (i.e. linguistic styles, higher status social circles, and aesthetic preferences). The second form of inheritance is through familial interventions in the form of "inter vivos" transfers (i.e. gifts between the living), especially at crucial junctures in the life courses. Examples include during a child's milestone stages, such as going to college, getting married, getting a job, and purchasing a home. The third form of inheritance is the transfers of bulk estates at the time of death of the testators, thus resulting in significant economic advantage accruing to children during their adult years. The origin of the stability of inequalities is material (personal possessions one is able to obtain) and is also cultural, rooted either in varying child-rearing practices that are geared to socialization according to social class and economic position. Child-rearing practices among those who inherit wealth may center around favoring some groups at the expense of others at the bottom of the social hierarchy.
Sociological and economic effects of inheritance inequality.
The degree to which economic status and inheritance is transmitted across generations determines one's life chances in society. Although many have linked one's social origins and educational attainment to life chances and opportunities, education cannot serve as the most influential predictor of economic mobility. In fact, children of well-off parents generally receive better schooling and benefit from material, cultural, and genetic inheritances. Likewise, schooling attainment is often persistent across generations and families with higher amounts of inheritance are able to acquire and transmit higher amounts of human capital. Lower amounts of human capital and inheritance can perpetuate inequality in the housing market and higher education. Research reveals that inheritance plays an important role in the accumulation of housing wealth. Those who receive an inheritance are more likely to own a home than those who do not regardless of the size of the inheritance.
Often, minorities and individuals from socially disadvantaged backgrounds receive less inheritance and wealth. As a result, mixed races might be excluded in inheritance privilege and are more likely to rent homes or live in poorer neighborhoods, as well as achieve lower educational attainment compared with whites in America. Individuals with a substantial amount of wealth and inheritance often intermarry with others of the same social class to protect their wealth and ensure the continuous transmission of inheritance across generations; thus perpetuating a cycle of privilege. For this reason, it can even be argued that one's inheritance places them in a specific social class position that requires a level of participation in certain activities that promote the oppression of lower-class individuals in terms of the social hierarchy and system of stratification.
Nations with the highest income and wealth inequalities often have the highest rates of homicide and disease (such as obesity, diabetes, and hypertension). A New York Times article reveals that the U.S. is the world's wealthiest nation, but "ranks twenty-ninth in life expectancy, right behind Jordan and Bosnia." This has been regarded as highly attributed to the significant gap of inheritance inequality in the country, although there are clearly other factors such as the healthcare system.
When social and economic inequalities centered on inheritance are perpetuated by major social institutions such as family, education, religion, etc., these differing life opportunities are transmitted from each generation. As a result, this inequality becomes part of the overall social structure.
Taxation.
Many states have inheritance taxes or death duties, under which a portion of any estate goes to the government.

</doc>
<doc id="15432" url="http://en.wikipedia.org/wiki?curid=15432" title="ISO 6166">
ISO 6166

ISO 6166 defines the structure of an International Securities Identifying Number (ISIN). An ISIN uniquely identifies a fungible security. Securities with which ISINs can be used are equities, debts, ETFs, options, derivatives, and futures.
ISINs consist of two alphabetic characters, which are the ISO 3166-1 alpha-2 code for the issuing country, nine alpha-numeric digits (the National Securities Identifying Number, or NSIN, which identifies the security), and one numeric check digit. The NSIN is issued by a national numbering agency (NNA) for that country. Regional substitute NNAs have been allocated the task of functioning as NNAs in those countries where NNAs have not yet been established.
NNAs cooperate through the Association of National Numbering Agencies (ANNA). ANNA also functions as the ISO 6166 Maintenance Agency (MA).

</doc>
<doc id="15435" url="http://en.wikipedia.org/wiki?curid=15435" title="Ignatius of Antioch">
Ignatius of Antioch

Ignatius of Antioch (Ancient Greek: Ἰγνάτιος Ἀντιοχείας, "Ignátios Antiokheías"; AD c. 35 or 50 – 98 to 117), also known as Ignatius Theophorus (Ιγνάτιος ὁ Θεοφόρος, "Ignátios ho Theophóros", "the God-bearing"), was an Apostolic Father and student of John the Apostle and was the third bishop of Antioch. En route to Rome, where he met his martyrdom by being fed to wild beasts, he wrote a series of letters which have been preserved as an example of very early Christian theology. Important topics addressed in these letters include ecclesiology, the sacraments, and the role of bishops.
Life.
Ignatius converted to Christianity at a young age. Later in his life he was chosen to serve as a Bishop of Antioch, succeeding Saint Peter and St. Evodius (who died around AD 67). The 4th-century Church historian Eusebius records that Ignatius succeeded Evodius. Making his apostolic succession even more immediate, Theodoret of Cyrrhus reported that St. Peter himself appointed Ignatius to the episcopal see of Antioch. Ignatius called himself "Theophorus" (God Bearer). A tradition arose that he was one of the children whom Jesus took in his arms and blessed.
Ignatius is one of the five Apostolic Fathers (the earliest authoritative group of the Church Fathers). He based his authority on being a bishop of the Church, living his life in the imitation of Christ. It is believed that St. Ignatius, along with his friend Polycarp, with great probability were disciples of the Apostle St. John.
Epistles attributed to Ignatius report his arrest by the authorities and travel to Rome:
From Syria even to Rome I fight with wild beasts, by land and sea, by night and by day, being bound amidst ten leopards, even a company of soldiers, who only grow worse when they are kindly treated.—"Ignatius to the Romans", 5.
Along the route he wrote six letters to the churches in the region and one to a fellow bishop. He was sentenced to die at the Colosseum. In his "Chronicle", Eusebius gives the date of Ignatius's death as AA 2124 (2124 years after Adam), which would amount to the 11th year of Trajan's reign; i.e., AD 108.
After Ignatius' martyrdom in the Colosseum his remains were carried back to Antioch by his companions and were interred outside the city gates. The reputed remains of Ignatius were moved by the Emperor Theodosius II to the Tychaeum, or Temple of Tyche, which had been converted into a church dedicated to Ignatius. In 637 the relics were transferred to the Basilica di San Clemente in Rome.
Veneration.
Ignatius' feast day was kept in his own Antioch on 17 October, the day on which he is now celebrated in the Catholic Church and generally in western Christianity, although from the 12th century until 1969 it was put at 1 February in the General Roman Calendar. 
In the Eastern Orthodox Church it is observed on 20 December. The Synaxarium of the Coptic Orthodox Church of Alexandria places it on the 24th of the Coptic Month of Koiak, corresponding in three years out of every four to 20 December in the Julian Calendar, which currently falls on 2 January of the Gregorian Calendar.
Letters.
The following seven letters preserved under the name of Ignatius:
By the 5th century, this collection had been enlarged by spurious letters, and some of the original letters were, at one point, believed to had been changed with interpolations, created to posthumously enlist Ignatius as an unwitting witness in theological disputes of that age, but that position was vigorously combated by several British and German critics, including the Catholics Denzinger and Hefele, who defended the genuineness of the entire seven epistles. At the same time, the purported eye-witness account of his martyrdom is also thought to be a forgery from around the same time. A detailed but spurious account of Ignatius' arrest and his travails and martyrdom is the material of the "Martyrium Ignatii" which is presented as being an eyewitness account for the church of Antioch, and attributed to Ignatius' companions, Philo of Cilicia, deacon at Tarsus, and Rheus Agathopus, a Syrian.
Although James Ussher regarded it as genuine, if there is any genuine nucleus of the "Martyrium", it has been so greatly expanded with interpolations that no part of it is without questions. Its most reliable manuscript is the 10th-century "Codex Colbertinus" (Paris), in which the "Martyrium" closes the collection. The "Martyrium" presents the confrontation of the bishop Ignatius with Trajan at Antioch, a familiar trope of "Acta" of the martyrs, and many details of the long, partly overland voyage to Rome. The Synaxarium of the Coptic Orthodox Church of Alexandria says that he was thrown to the wild beasts that devoured him and rent him to pieces.
Ignatius's letters proved to be important testimony to the development of Christian theology, since the number of extant writings from this period of Church history is very small. They bear signs of being written in great haste and without a proper plan, such as run-on sentences and an unsystematic succession of thought. Ignatius is the earliest known Christian writer to emphasize loyalty to a single bishop in each city (or diocese) who is assisted by both presbyters (possibly elders) and deacons. Earlier writings only mention "either" bishops "or" presbyters, and give the impression that there was usually more than one bishop per congregation. 
For instance, his writings on bishops, presbyters and deacons: Take care to do all things in harmony with God, with the bishop presiding in the place of God, and with the presbyters in the place of the council of the apostles, and with the deacons, who are most dear to me, entrusted with the business of Jesus Christ, who was with the Father from the beginning and is at last made manifest — "Letter to the Magnesians 2", 6:1
Ignatius is known to have taught the deity of Christ: There is one Physician who is possessed both of flesh and spirit; both made and not made; God existing in flesh; true life in death; both of Mary and of God; first passible and then impassible, even Jesus Christ our Lord. —"Letter to the Ephesians", ch. 7, shorter version, Roberts-Donaldson translation
 He stressed the value of the Eucharist, calling it a "medicine of immortality" ("Ignatius to the Ephesians" 20:2). The very strong desire for bloody martyrdom in the arena, which Ignatius expresses rather graphically in places, may seem quite odd to the modern reader. An examination of his theology of soteriology shows that he regarded salvation as one being free from the powerful fear of death and thus to bravely face martyrdom.
Ignatius is claimed to be the first known Christian writer to argue in favor of Christianity's replacement of the Sabbath with the Lord's Day: Be not seduced by strange doctrines nor by antiquated fables, which are profitless. For if even unto this day we live after the manner of Judaism, we avow that we have not received grace ... If then those who had walked in ancient practices attained unto newness of hope, no longer observing Sabbaths but fashioning their lives after the Lord's day, on which our life also arose through Him and through His death which some men deny ... how shall we be able to live apart from Him? ... It is monstrous to talk of Jesus Christ and to practise Judaism. For Christianity did not believe in Judaism, but Judaism in Christianity — "Ignatius to the Magnesians" 8:1, 9:1-2, 10:3, Lightfoot translation.
He is also responsible for the first known use of the Greek word "katholikos" (καθολικός), meaning "universal", "complete" and "whole" to describe the church, writing: Wherever the bishop appears, there let the people be; as wherever Jesus Christ is, there is the Catholic Church. It is not lawful to baptize or give communion without the consent of the bishop. On the other hand, whatever has his approval is pleasing to God. Thus, whatever is done will be safe and valid. — "Letter to the Smyrnaeans" 8, J.R. Willis translation.
It is from the word "katholikos" ("according to the whole") that the word "catholic" comes. When Ignatius wrote the Letter to the Smyrnaeans in about the year 107 and used the word "catholic", he used it as if it were a word already in use to describe the Church. This has led many scholars to conclude that the appellation "Catholic Church" with its ecclesial connotation may have been in use as early as the last quarter of the 1st century. On the Eucharist, he wrote in his letter to the Smyrnaeans: Take note of those who hold heterodox opinions on the grace of Jesus Christ which has come to us, and see how contrary their opinions are to the mind of God ... They abstain from the Eucharist and from prayer because they do not confess that the Eucharist is the flesh of our Savior Jesus Christ, flesh which suffered for our sins and which that Father, in his goodness, raised up again. They who deny the gift of God are perishing in their disputes. — "Letter to the Smyrnaeans" 6:2–7:1
Ignatius modeled his writings after Paul, Peter, and John, and even quoted or paraphrased their own works freely, such as when he quoted 1 Cor 1:18, in his letter to the Ephesians: Let my spirit be counted as nothing for the sake of the cross, which is a stumbling-block to those that do not believe, but to us salvation and life eternal. - "Letter to the Ephesians" 18, Roberts and Donaldson translation
Saint Ignatius's most famous quotation, however, comes from his letter to the Romans: I am writing to all the Churches and I enjoin all, that I am dying willingly for God's sake, if only you do not prevent it. I beg you, do not do me an untimely kindness. Allow me to be eaten by the beasts, which are my way of reaching to God. I am God's wheat, and I am to be ground by the teeth of wild beasts, so that I may become the pure bread of Christ. — "Letter to the Romans"
Letters of Pseudo-Ignatius.
Epistles attributed to Saint Ignatius but of spurious origin include

</doc>
<doc id="15437" url="http://en.wikipedia.org/wiki?curid=15437" title="ITU prefix">
ITU prefix

The International Telecommunication Union (ITU) allocates call sign prefixes for radio and television stations of all types. They also form the basis for, but do not exactly match, aircraft registration identifiers. These prefixes are agreed upon internationally, and are a form of country code. A call sign can be any number of letters and numerals but each country must only use call signs that begin with the characters allocated for use in that country.
A few countries do not fully comply with these rules. Australian broadcast stations officially have—but do not use—the VL prefix, and Canada uses Chile's CB for its own Canadian Broadcasting Corporation stations. This is through a special agreement with the government of Chile, which is officially assigned the CB prefix.
With regard to the second and/or third letters in the prefixes in the list below, if the country in question is allocated all callsigns with A to Z in that position, then that country can also use call signs with the digits 0 to 9 in that position. For example, the United States is assigned KA–KZ, and therefore can also use prefixes like KW0 or K1.
Many large countries in turn have internal rules on how and where specific subsets of their callsigns can be used (such as Mexico's XE for AM and XH for FM radio and television broadcasting), which are not covered here.
Unallocated and unavailable call sign prefixes.
Unallocated: The following call sign prefixes are available for future allocation by the ITU. ("x" represents any letter; "n" represents any digit from 2–9.)
Unavailable: Under present ITU guidelines the following call sign prefixes shall not be allocated . They are sometimes used unofficially - such as amateur radio operators operating in a disputed territory or in a nation state that has no official prefix (e.g. S0 in Western Sahara or station 1A0 at Knights of Malta headquarters in Rome). ("x" represents any letter; "n" represents any digit from 2–9.)
Table of Allocation of International Call Sign Series.
Linked country codes are from ISO 3166-1.
       Series allocated to an international organization.
       Provisional allocation in accordance with No. S19.33: "Between radiocommunication conferences, the Secretary-General is authorized to deal with questions relating to changes in the allocation of series of call signs, on a provisional basis, and subject to confirmation by the following conference."
       Half series allocation. The first country listed uses all callsigns beginning with the listed prefix followed by A-M, and the second country listed uses N-Z.<br>

</doc>
<doc id="15440" url="http://en.wikipedia.org/wiki?curid=15440" title="IBM PC keyboard">
IBM PC keyboard

The keyboards for IBM PC compatible computers are standardized. However, during the more than 30 years of PC architecture being constantly updated, multiple types of keyboard layout variations have been developed.
A well-known class of IBM PC keyboards is the Model M. Introduced in 1986 and manufactured by IBM, Lexmark, Maxi-Switch and Unicomp, the vast majority of Model M keyboards feature a buckling spring key design and many have fully swappable keycaps.
Keyboard layouts.
The PC keyboard changed over the years, often at the launch of new IBM PC versions.
Common additions to the standard layouts include additional power management keys, volume controls, media player controls, and miscellaneous user-configurable shortcuts for email client, World Wide Web browser, etc.
The IBM PC layout, particularly the Model M, has been extremely influential, and today most keyboards use some variant of it. This has caused problems for applications developed with alternative layouts, which require keys that are in awkward positions on the Model M layout – often requiring the pinkie to operate – and thus require remapping for comfortable use. One notable example is the Escape key, used by the vi editor: on the ADM-3A terminal this was located where the Tab key is on the IBM PC, but on the IBM PC the Escape key is in the corner; this is typically solved by remapping Caps Lock to Escape. Another example is the Emacs editor, which makes extensive use of modifier keys, and uses the Control key more than the Meta key (IBM PC instead has the Alt key) – these date to the Knight keyboard, which had the Control key on the "inside" of the Meta key, opposite to the Model M, where it is on the "outside" of the Alt key; and to the space-cadet keyboard, where the four bucky bit keys (Control, Meta, Super, Hyper) are in a row, allowing easy chording to press several, unlike on the Model M layout. This results in the "Emacs pinky" problem.
Reception.
Although "PC Magazine" praised most aspects of the 1981 IBM PC keyboard's hardware design, it questioned "how IBM, that ultimate pro of keyboard manufacture, could put the left-hand SHIFT key at the awkward reach they did". The magazine reported in 1982 that it received more letters to its "Wish List" column asking for the ability to determine the status of the three lock keys than on any other topic. "BYTE" columnist Jerry Pournelle praised the keyboard's feel as "excellent" but complained that the Shift and other keys' locations were "enough to make a saint weep", and denounced the trend of PC compatible computers to emulate the layout but not the feel. "BYTE"‍ '​s review was more sanguine. It praised the keyboard as "bar none, the best ... on any microcomputer" and described the unusual Shift key locations as "minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard".
Standard key meanings.
The PC keyboard with its various keys has a long history of evolution reaching back to teletypewriters. In addition to the 'old' standard keys, the PC keyboard has accumulated several special keys over the years. Some of the additions have been inspired by the opportunity or requirement for improving user productivity with general office application software, while other slightly more general keyboard additions have become the factory standards after being introduced by certain operating system or GUI software vendors such as Microsoft.

</doc>
<doc id="15441" url="http://en.wikipedia.org/wiki?curid=15441" title="Italian battleship Giulio Cesare">
Italian battleship Giulio Cesare

Giulio Cesare was one of three "Conte di Cavour"-class dreadnought battleships built for the Royal Italian Navy ("Regia Marina") in the 1910s. She served in both World Wars, although she was little used and saw no combat during the former. The ship supported operations during the Corfu Incident in 1923 and spent much of the rest of the decade in reserve. She was rebuilt between 1933 and 1937 with more powerful guns, additional armor and considerably more speed than before.
Both "Giulio Cesare" and her sister ship, "Conte di Cavour", participated in the Battle of Calabria in July 1940, when the former was lightly damaged. They were both present when British torpedo bombers attacked the fleet at Taranto in November 1940, but "Giulio Cesare" was not damaged. She escorted several convoys to North Africa and participated in the Battle of Cape Spartivento in late 1940 and the First Battle of Sirte in late 1941. She was designated as a training ship in early 1942, and escaped to Malta after Italy surrendered. The ship was transferred to the Soviet Union in 1949 and renamed Novorossiysk. The Soviets also used her for training until she was sunk when an old German mine exploded in 1955. She was salvaged the following year and later scrapped.
Description.
"Giulio Cesare" was 168.9 m long at the waterline, and 176 m overall. The ship had a beam of 28 m, and a draft of 9.3 m. She displaced 23088 LT at normal load, and 25086 LT at deep load. She had a crew of 31 officers and 969 enlisted men. The ship's machinery consisted of four Parsons steam turbines, each driving one propeller shaft. Steam for the turbines was provided by 24 Babcock & Wilcox boilers, half of which burned fuel oil and the other half burning both oil and coal. Designed to reach a maximum speed of 22.5 kn from 31000 shp, "Giulio Cesare" failed to reach this goal on her sea trials, despite generally exceeding the rated power of her turbines. The ship only made a maximum speed of 21.56 kn using 30700 shp. She had a cruising radius of 4800 nmi at 10 kn.
The ship was armed with a main battery of thirteen 305 mm guns in three triple-gun turret and two twin-gun turrets, designated 'A', 'B', 'Q', 'X', and 'Y' from front to rear. The secondary battery comprised eighteen 120 mm guns, all mounted in casemates in the sides of the hull. "Giulio Cesare" was also armed with fourteen 76 mm guns. As was customary for capital ships of the period, she was equipped with three submerged 450 mm torpedo tubes. She was protected with Krupp cemented steel manufactured by Terni. The belt armor was 250 mm thick and the main deck was 40 mm thick. The conning tower and main battery turrets were protected with 280 mm worth of armor plating.
Modifications and reconstruction.
Shortly after the end of World War I, the number of 50-caliber 76 mm guns was reduced to 13, all mounted on the turret tops, and six new 40-caliber 76 mm anti-aircraft (AA) guns were installed abreast the aft funnel. In addition two license-built 2-pounder AA guns were mounted on the forecastle deck. In 1925–26 the foremast was replaced by a four-legged mast, which was moved forward of the funnels, the rangefinders were upgraded, and the ship was equipped to handle a Macchi M.18 seaplane mounted on the center turret. Around that same time, either one or both of the ships was equipped with a fixed aircraft catapult on the port side of the forecastle.
"Giulio Cesare" began an extensive reconstruction in October 1933 at the Cantieri del Tirreno shipyard in Genoa that lasted until October 1937. A new bow section was grafted over the existing bow which increased her length by 10.31 m to 186.4 m and her beam increased to 28.6 m. The ship's draft at deep load increased to 10.02 m. All of the changes made increased her displacement to 26140 LT at standard load and 29100 LT at deep load. The ship's crew increased to 1,260 officers and enlisted men. Two of the propeller shafts were removed and the existing turbines were replaced by two Belluzzo geared steam turbines rated at 75000 shp. The boilers were replaced by eight Yarrow boilers. On her sea trials in December 1936, before her reconstruction was fully completed, "Giulio Cesare" reached a speed of 28.24 kn from 93430 shp. In service her maximum speed was about 27 kn and she had a range of 6400 nmi at a speed of 13 kn.
The main guns were bored out to 320 mm and the center turret and the torpedo tubes were removed. All of the existing secondary armament and AA guns were replaced by a dozen 120 mm guns in six twin-gun turrets and eight 102 mm AA guns in twin turrets. In addition the ship was fitted with a dozen Breda 37 mm light AA guns in six twin-gun mounts and twelve 13.2 mm Breda M31 anti-aircraft machine guns, also in twin mounts. In 1940 the 13.2 mm machine guns were replaced by 20 mm AA guns in twin mounts. "Giulio Cesare" received two more twin mounts as well as four additional 37 mm guns in twin mounts on the forecastle between the two turrets in 1941. The tetrapodal mast was replaced with a new forward conning tower, protected with 260 mm thick armor. Atop the conning tower there was a fire-control director fitted with two large stereo-rangefinders, with a base length of 7.2 m.
The deck armor was increased during the reconstruction to a total of 135 mm over the engine and boiler rooms and 166 mm over the magazines, although its distribution over three decks, each with multiple layers, meant that it was considerably less effective than a single plate of the same thickness. The armor protecting the barbettes was reinforced with 50 mm plates. All this armor weighed a total of 3227 LT. The existing underwater protection was replaced by the Pugliese torpedo defense system that consisted of a large cylinder surrounded by fuel oil or water that was intended to absorb the blast of a torpedo warhead. It lacked, however, enough depth to be fully effective against contemporary torpedoes. A major problem of the reconstruction was that the ship's increased draft meant that their waterline armor belt was almost completely submerged with any significant load.
Construction and service.
"Giulio Cesare", named after Julius Caesar, was laid down at the Gio. Ansaldo & C. shipyard in Genoa on 24 June 1910 and launched on 15 October 1911. She was completed on 14 May 1914 and served as a flagship in the southern Adriatic Sea during World War I. She saw no action, however, and spent little time at sea. Admiral Paolo Thaon di Revel, the Italian naval chief of staff, believed that Austro-Hungarian submarines and minelayers could operate too effectively in the narrow waters of the Adriatic. The threat from these underwater weapons to his capital ships was too serious for him to use the fleet in an active way. Instead, Revel decided to implement a blockade at the relatively safer southern end of the Adriatic with the battle fleet, while smaller vessels, such as the MAS torpedo boats, conducted raids on Austro-Hungarian ships and installations. Meanwhile, Revel's battleships would be preserved to confront the Austro-Hungarian battle fleet in the event that it sought a decisive engagement.
"Giulio Cesare" made port visits in the Levant in 1919 and 1920. Both "Giulio Cesare" and "Conte di Cavour" supported Italian operations on Corfu in 1923 after an Italian general and his staff were murdered on Corfu; Benito Mussolini was not satisfied with the Greek government's response so he ordered Italian troops to occupy the island. "Cesare" became a gunnery training ship in 1928, after having been in reserve since 1926. She was reconstructed at Cantieri del Tirreno, Genoa, between 1933 and 1937. Both ships participated in a naval review by Adolf Hitler in the Bay of Naples in May 1938 and covered the invasion of Albania in May 1939.
World War II.
Early in World War II, the ship took part in the Battle of Calabria (also known as the Battle of Punto Stilo), together with "Conte di Cavour", on 9 July 1940, as part of the 1st Battle Squadron, commanded by Admiral Inigo Campioni, during which she engaged major elements of the British Mediterranean Fleet. The British were escorting a convoy from Malta to Alexandria, while the Italians had finished escorting another from Naples to Benghazi, Libya. Admiral Andrew Cunningham, commander of the Mediterranean Fleet, attempted to interpose his ships between the Italians and their base at Taranto. Crew on the fleets spotted each other in the middle of the afternoon and the battleships opened fire at 15:53 at a range of nearly 29000 yd. The two leading British battleships, HMS "Warspite" and "Malaya", replied a minute later. Three minutes after she opened fire, shells from "Giulio Cesare" began to straddle "Warspite" which made a small turn and increased speed, to throw off the Italian ship's aim, at 16:00. At that same time, a shell from "Warspite" struck "Giulio Cesare" at a distance of about 26000 yd. The shell pierced the rear funnel and detonated inside it, blowing out a hole nearly 20 ft across. Fragments started several fires and their smoke was drawn into the boiler rooms, forcing four boilers off-line as their operators could not breathe. This reduced the ship's speed to 18 kn. Uncertain how severe the damage was, Campioni ordered his battleships to turn away in the face of superior British numbers and they successfully disengaged. Repairs to "Giulio Cesare" were completed by the end of August and both ships unsuccessfully attempted to intercept British convoys to Malta in August and September.
On the night of 11 November 1940, "Giulio Cesare" and the other Italian battleships were at anchor in Taranto harbor when they were attacked by 21 Fairey Swordfish torpedo bombers from the British aircraft carrier HMS "Illustrious", along with several other warships. One torpedo sank Conte di Cavour in shallow water, but "Giulio Cesare" was not hit during the attack. She participated in the Battle of Cape Spartivento on 27 November 1940, but never got close enough to any British ships to fire at them. The ship was damaged in January 1941 by splinters from a near miss during an air raid on Naples by Vickers Wellington bombers of the Royal Air Force; repairs at Genoa were completed in early February. On 8 February, she sailed from to the Straits of Bonifacio to intercept what the Italians thought was a Malta convoy, but was actually a raid on Genoa. She failed to make contact with any British forces. She participated in the First Battle of Sirte on 17 December 1941, providing distant cover for a convoy bound for Libya, again never firing her main armament. She also provided distant cover for another convoy to North Africa in early January 1942. "Giulio Cesare" was reduced to a training ship afterwards at Taranto and later Pola. The unsuccessfully attacked the ship in the Gulf of Taranto in early March 1944. After the Italian surrender on 9 September 1943, she steamed to Taranto, putting down a mutiny and enduring an ineffective attack by five German aircraft en route. She then sailed for Malta where she arrived on 12 September to be interned. The ship remained there until 17 June 1944 when she returned to Taranto where she remained for the next four years.
Soviet service.
After the war, "Giulio Cesare" was allocated to the Soviet Union as part of the war reparations and she was moved to Augusta, Sicily on 9 December 1948 where an unsuccessful attempt was made to sabotage the ship. The ship was stricken from the naval register on 15 December and turned over to the Soviets on 6 February 1949 under the temporary name of "Z11" in Vlorë, Albania. She was renamed by them as "Novorossiysk", after the Soviet city on the Black Sea. The Soviets used her as a training ship when she was not undergoing one of her eight refits in their hands. In 1953, all remaining Italian light AA guns were replaced by eighteen 37 mm 70-K AA guns in six twin mounts and six singles. They also replaced her fire-control systems and added radars, although the exact changes are unknown. The Soviets intended to rearm her with their own 305 mm guns, but this was forestalled by her loss. While at anchor in Sevastopol on the night of 28/29 October 1955, she most likely detonated a large German mine left over from World War II. The explosion blew a hole completely through the ship, making a 4 by hole in the forecastle forward of 'A' turret. The flooding could not be controlled and she later capsized with the loss of 608 men, including men sent from other ships to assist.
The cause of the explosion is still unclear. The officially named cause, regarded as the most probable, was a magnetic RMH or LMB bottom mine, laid by the Germans during World War II and triggered by the dragging of the battleship's anchor chain before mooring for the last time. Subsequent searches located 32 mines of these types, some of them within 50 m of the explosion. The damage was consistent with an explosion of 1000 - of TNT and more than one mine may have detonated. Nonetheless, other explanations for the ship's loss have been proposed and the most popular of these is that she was sunk by Italian frogmen of the wartime special operations unit "Decima Flottiglia MAS" who — more than ten years after the cessation of hostilities — were either avenging the transfer of the former Italian battleship to the USSR or sinking it on behalf of NATO. "Novorossiysk" was stricken from the naval register on 24 February 1956, salvaged on 4 May 1957, and subsequently scrapped.

</doc>
<doc id="15442" url="http://en.wikipedia.org/wiki?curid=15442" title="INS Vikrant (R11)">
INS Vikrant (R11)

INS "Vikrant" (Hindi : भा नौ पो विक्रान्‍त)(Sanskrit: विक्रान्‍त, for "courageous") was a "Majestic"-class aircraft carrier of the Indian Navy. She played a key role in enforcing the naval blockade on East Pakistan during the Indo-Pakistan War of 1971.
The ship was built under the name Hercules for the British Royal Navy during World War II, but construction was put on hold after the war's end, and she never entered British service. India purchased the incomplete carrier from the United Kingdom in 1957, and construction was completed in 1961. The Indian Navy sent Raghunath Mishra and a small crew to bring the ship to India. INS "Vikrant" was commissioned as the first aircraft carrier of the Indian Navy. After years of distinguished service, she was decommissioned in January 1997.
From 1997 to 2012, she was preserved as a museum ship in Cuffe Parade, Mumbai, until it was closed in 2012 due to safety concerns. At the end of January 2014, "Vikrant" was sold through an online auction to a Darukhana ship-breaker, where she underwent preparations to be broken up. Although a public-interest litigation was filed and heard by the Supreme Court of India challenging "Vikrant" 's sale and scrapping, on 14 August 2014, the Supreme Court rejected the PIL and cleared the way for the warship to be scrapped. Vikrant remained beached off Darukhana in Mumbai Port while awaiting the final clearances of the Mumbai Port Trust. On 12 November 2014, the Supreme Court gave its final approval for the carrier to be scrapped. The scrapping of "Vikrant" began on 22 November, and is intended to be completed by mid-2015.
History.
"Vikrant" was ordered as "Hercules" by the Royal Navy. She was laid down on 12 November 1943 by Vickers-Armstrong on the River Tyne. She was launched on 22 September 1945. However, with the end of World War II, her construction was suspended in May 1946 and she was laid up for possible future use.
In January 1957 she was sold to India. She was towed to Belfast to complete her construction and for modifications by Harland and Wolff. A number of improvements to the original design were ordered by the Indian Navy, including an angled deck, steam catapults and a modified island.
"Vikrant" was commissioned into the Indian Navy by then Indian High Commissioner to the United Kingdom, Vijayalakshmi Pandit on 4 March 1961 in Belfast. The name "Vikrant" was taken from Sanskrit "vikrānta" meaning "stepping beyond", i.e. "courageous" or "bold". Captain Raghunath Mishra was the first commanding officer of the carrier.
"Vikrant"‍ '​s initial air wing consisted of British Hawker Sea Hawk fighter-bombers and a French Alize anti-submarine aircraft. On 18 May 1961, the first jet landed on her deck piloted by Lieutenant (later Admiral) Radhakrishna Hariram Tahiliani. She formally joined the Indian Navy's Fleet in Bombay on 3 November 1961, when she was received at Ballard Pier by Prime Minister Jawaharlal Nehru.
During the Indo-Pakistan War of 1965, Pakistan reported that it had sunk "Vikrant". However, at the time the ship was in dry dock undergoing modifications.
In June 1970, "Vikrant" was at the Naval Dockyard for repairs due to a crack in a water drum of one of the boilers powering her steam catapult. Unable to procure a replacement drum from the United Kingdom due to an embargo, Admiral Sardarilal Mathradas Nanda ordered the routing of steam from her forward machinery to the steam catapult to bypass the damaged boiler. This repair enabled her to launch both the Sea Hawks as well as the Breguet Alizé, although she lost some cruising power. In March 1971, she was put through trials to test the fix. These modifications turned out to be valuable, enabling "Vikrant" to enter combat against East Pakistan in the Indo-Pakistani War of 1971 despite the cracked boiler.
Stationed off the Andaman & Nicobar Islands along with frigates, INS "Brahmaputra" and INS "Beas", "Vikrant" redeployed towards Chittagong at the outbreak of hostilities. Based on naval intelligence reports that the Pakistan Navy intended to break through the Indian Naval blockade using camouflaged merchant ships, the Sea Hawks struck shipping in the Chittagong and Cox's Bazar harbours, sinking or incapacitating most ships there. On the morning of 4 December 1971, "Vikrant"‍ '​s eight Sea Hawk aircraft launched an air raid on Cox's Bazar from 60 nmi offshore. On the evening of 4 December, the air group struck Chittagong Harbour. Later strikes targeted Khulna and Port of Mongla. A Press Trust of India report of 4 December read, "Chittagong harbour ablaze as ships and aircraft of the Eastern Naval Fleet bombed and rocketed. Not a single vessel can be put to sea from Chittagong." Air strikes continued until 10 December 1971 with not a single Sea Hawk lost.
The Pakistan Navy deployed the submarine "Ghazi" to specifically target and sink "Vikrant". However, "Ghazi" sank off Visakhapatnam harbour, probably due to depth charge attacks by INS "Rajput". During the war, the crew of "Vikrant" earned two Mahavir Chakras and 12 Vir Chakras.
Air Arm.
"Vikrant" had four squadrons on board :
Squadron insignia
Subsequent service.
"Vikrant" was given an extensive refit, including new engines and modernization between 1979 and 3 January 1982. Between December 1982 and February 1983 she was refitted again to enable her to operate BAe Sea Harriers which replaced the Sea Hawk. After the retirement of the Breguet Alizé from carrier service in 1989, she received a 'ski jump' for more efficient use of her Sea Harriers.
"Vikrant" was India's only carrier for over twenty years, but by the early 1990s she was effectively out of service because of her poor condition. Even following major overhauls she was rarely put to sea. She was formally decommissioned on 31 January 1997.
Museum ship.
Following her decommissioning, "Vikrant" was marked for preservation as a museum ship in Mumbai, although a lack of funding prevented progress on the ship's conversion for this role. Similarly, speculation that the ship would be made into a training ship in 2006 came to nothing. From 2001, "Vikrant" was made open to the public by the Indian Navy for short periods, but as of April 2010, the Government of Maharashtra was unable to find an industrial partner to operate the museum on a permanent, long-term basis. In 2012, the museum was closed after "Vikrant" was deemed unsafe.
"Vikrant" was the only World War II-era British-built aircraft carrier to be preserved as a museum.
Auction and scrapping.
In August 2013, Vice-Admiral Shekhar Sinha, chief of the Western Naval Command, said the Ministry of Defence would scrap "Vikrant" as she had become "very difficult to maintain," and as no private bidders had offered to fund the museum's operations. On 3 December 2013 the Indian government decided to auction the ship, due to maintenance difficulties. The Bombay High Court dismissed a public-interest litigation filed by Kiran Paigankar, founder of the "Save Vikrant Committee," stating the vessel's dilapidated condition did not warrant her preservation, nor were the necessary funds or government support available.
At the end of January 2014, "Vikrant" was sold through an online auction to a Darukhana ship-breaker for Rs.60 crores (Indian Express-21-Nov-2014: http://indianexpress.com/article/india/india-others/dismantling-of-iconic-warship-ins-vikrant-begins/ ). Although a public-interest litigation was filed and heard by the Supreme Court of India challenging "Vikrant" 's sale and scrapping, on 14 August 2014, the Supreme Court rejected the PIL and cleared the way for the warship to be scrapped. Vikrant remained beached off Darukhana in Mumbai Port while awaiting the final clearances of the Mumbai Port Trust. On 12 November 2014, the Supreme Court gave its final approval for the carrier to be scrapped. The scrapping of "Vikrant" began on 22 November, and is intended to be completed by mid-2015.

</doc>
<doc id="15443" url="http://en.wikipedia.org/wiki?curid=15443" title="Western imperialism in Asia">
Western imperialism in Asia

Western imperialism in Asia as presented in this article pertains to Western European entry into what was first called the East Indies. This was sparked early in the 15th century by the search for trade routes to China that led directly to the Age of Discovery, and the introduction of early modern warfare into what was then called the Far East. By the early 16th century the Age of Sail greatly expanded Western European influence and development of the Spice Trade under colonialism. There has been a presence of Western European colonial empires and imperialism in Asia throughout six centuries of colonialism, formally ending with the independence of the Portuguese Empire's last colony East Timor in 2002. The empires introduced Western concepts of nation and the multinational state. This article attempts to outline consequently development of the Western concept of the nation state.
The thrust of European political power, commerce, and culture in Asia gave rise to growing trade in commodities—a key development in the rise of today's modern world free market economy. In the 16th century, the Portuguese broke the (overland) monopoly of the Arabs and Italians of trade between Asia and Europe by the discovery of the sea route to India around the Cape of Good Hope. With the ensuing rise of the rival Dutch East India Company, Portuguese influence in Asia was gradually eclipsed. Dutch forces first established independent bases in the East (most significantly Batavia, the heavily fortified headquarters of the Dutch East India Company) and then between 1640 and 1660 wrestled Malacca, Ceylon, some southern Indian ports, and the lucrative Japan trade from the Portuguese. Later, the English and the French established settlements in India and established a trade with China and their own acquisitions would gradually surpass those of the Dutch. Following the end of the Seven Years' War in 1763, the British eliminated French influence in India and established the British East India Company as the most important political force on the Indian Subcontinent.
Before the Industrial Revolution in the mid-to-late 19th century, demand for oriental goods such as (porcelain, silk, spices and tea) remained the driving force behind European imperialism, and (with the important exception of British East India Company rule in India) the European stake in Asia remained confined largely to trading stations and strategic outposts necessary to protect trade. Industrialisation, however, dramatically increased European demand for Asian raw materials; and the severe Long Depression of the 1870s provoked a scramble for new markets for European industrial products and financial services in Africa, the Americas, Eastern Europe, and especially in Asia. This scramble coincided with a new era in global colonial expansion known as "the New Imperialism," which saw a shift in focus from trade and indirect rule to formal colonial control of vast overseas territories ruled as political extensions of their mother countries. Between the 1870s and the beginning of World War I in 1914, the United Kingdom, France, and the Netherlands—the established colonial powers in Asia—added to their empires vast expanses of territory in the Middle East, the Indian Subcontinent, and South East Asia. In the same period, the Empire of Japan, following the Meiji Restoration; the German Empire, following the end of the Franco-Prussian War in 1871; Tsarist Russia; and the United States, following the Spanish–American War in 1898, quickly emerged as new imperial powers in East Asia and in the Pacific Ocean area.
In Asia, World War I and World War II were played out as struggles among several key imperial powers—conflicts involving the European powers along with Russia and the rising American and Japanese powers. None of the colonial powers, however, possessed the resources to withstand the strains of both world wars and maintain their direct rule in Asia. Although nationalist movements throughout the colonial world led to the political independence of nearly all of the Asia's remaining colonies, decolonisation was intercepted by the Cold War; and South East Asia, South Asia, the Middle East, and East Asia remained embedded in a world economic, financial, and military system in which the great powers compete to extend their influence. However, the rapid post-war economic development of the East Asian Tigers, India, the People's Republic of China, along with the collapse of the Soviet Union, have loosened European and North American influence in Asia, generating speculation today about emergence of modern India and China as potential superpowers.
Early European exploration of Asia.
European exploration of Asia started in ancient Roman times. Knowledge of lands as distant as China were held by the Romans. Trade with India through the Roman Egyptian Red Sea ports was significant in the first centuries of the Common Era.
Medieval European exploration of Asia.
In the 13th and 14th centuries, a number of Europeans, many of them Christian missionaries, had sought to penetrate into China. The most famous of these travelers was Marco Polo. But these journeys had little permanent effect on East-West trade because of a series of political developments in Asia in the last decades of the 14th century, which put an end to further European exploration of Asia. The Yuan dynasty in China, which had been receptive to European missionaries and merchants, was overthrown, and the new Ming rulers were found to be unreceptive of religious proselytism. Meanwhile, the Turks consolidated control over the eastern Mediterranean, closing off key overland trade routes. Thus, until the 15th century, only minor trade and cultural exchanges between Europe and Asia continued at certain terminals controlled by Muslim traders.
Oceanic voyages to Asia.
Western European rulers determined to find new trade routes of their own. The Portuguese spearheaded the drive to find oceanic routes that would provide cheaper and easier access to South and East Asian goods. This chartering of oceanic routes between East and West began with the unprecedented voyages of Portuguese and Spanish sea captains. Their voyages were influenced by medieval European adventurers, who had journeyed overland to the Far East and contributed to geographical knowledge of parts of Asia upon their return.
In 1488, Bartolomeu Dias rounded the southern tip of Africa under the sponsorship of Portugal's John II, from which point he noticed that the coast swung northeast (Cape of Good Hope). While Dias' crew forced him to turn back, by 1497, Portuguese navigator Vasco da Gama made the first open voyage from Europe to India. In 1520, Ferdinand Magellan, a Portuguese navigator in the service of Spain, found a sea route into the Pacific Ocean.
Portuguese and Spanish trade and colonization in Asia.
Portuguese monopoly over trade in the Indian Ocean and Asia.
Early in the 16th century Afonso de Albuquerque (left) emerged as the Portuguese colonial viceroy most instrumental in consolidating Portugal's holdings in Africa and in Asia. He understood that Portugal could wrest commercial supremacy from the Arabs only by force, and therefore devised a plan to establish forts at strategic sites which would dominate the trade routes and also protect Portuguese interests on land. In 1510, he conquered Goa in India, which enabled him to gradually consolidate control of most of the commercial traffic between Europe and Asia, largely through trade; Europeans started to carry on trade from forts, acting as foreign merchants rather than as settlers. In contrast, early European expansion in the "West Indies", (later known to Europeans as a separate continent from Asia that they would call the "Americas") following the 1492 voyage of Christopher Columbus, involved heavy settlement in colonies that were treated as political extensions of the mother countries.
Lured by the potential of high profits from another expedition, the Portuguese established a permanent base south of the Indian trade port of Calicut in the early 15th century. In 1510, the Portuguese seized Goa on the coast of India, which Portugal held until 1961. The Portuguese soon acquired a monopoly over trade in the Indian Ocean.
Portuguese viceroy Afonso de Albuquerque (1509–1515) resolved to consolidate Portuguese holdings in Africa and Asia, and secure control of trade with the East Indies and China. His first objective was Malacca, which controlled the narrow strait through which most Far Eastern trade moved. Captured in 1511, Malacca became the springboard for further eastward penetration; several years later the first trading posts were established in the Moluccas, or "Spice Islands," which was the source for some of the world's most hotly demanded spices. By 1516, the first Portuguese ships had reached Canton on the southern coasts of China.
In 1513, after the failed attempt to conquer Aden, Albuquerque entered with an armada, for the first time for Europeans by the ocean via, on the Red Sea; and in 1515, Albuquerque consolidated the Portuguese hegemony in the Persian Gulf gates, already begun by him in 1507, with the domain of Muscat and Ormuz.
By 1557, the Portuguese gained a permanent base in China at Macau, which they held until 1999. The Portuguese, based at Goa and Malacca, had now established a lucrative maritime empire in the Indian Ocean meant to monopolise the spice trade. The Portuguese also began a channel of trade with the Japanese, becoming the first recorded Westerners to have visited Japan. This contact introduced Christianity and fire-arms into Japan.
The energies of Spain, the other major colonial power of the 16th century, were largely concentrated on the Americas, not South and East Asia. But the Spanish did establish a footing in the Far East in the Philippines. After 1565, cargoes of Chinese goods were transported from the Philippines to Mexico and from there to Spain. By this long route, Spain reaped some of the profits of Far Eastern commerce. Spanish officials converted the islands to Christianity and established some settlements, permanently establishing the Philippines as the area of East Asia most oriented toward the West in terms of culture and commerce.
Decline of Portugal's Asian empire since the 17th century.
The lucrative trade was vastly expanded when the Portuguese began to export slaves from Africa in 1541; however, over time, the rise of the slave trade left Portugal over-extended, and vulnerable to competition from other Western European powers. Envious of Portugal's control of trade routes, other Western European nations—mainly the Netherlands, France, and England—began to send in rival expeditions to Asia. In 1642, the Dutch drove the Portuguese out of the Gold Coast in Africa, the source of the bulk of Portuguese slave labourers, leaving this rich slaving area to other Europeans, especially the Dutch and the English.
Rival European powers began to make inroads in Asia as the Portuguese and Spanish trade in the Indian Ocean declined primarily because they had become hugely over-stretched financially due to the limitations on their investment capacity and contemporary naval technology. Both of these factors worked in tandem, making control over Indian Ocean trade extremely expensive.
The existing Portuguese interests in Asia proved sufficient to finance further colonial expansion and entrenchment in areas regarded as of greater strategic importance in Africa and Brazil. Portuguese maritime supremacy was lost to the Dutch in the 17th century, and with this came serious challenges for the Portuguese. However, they still clung to Macau, and settled a new colony on the island of Timor. It was as recent as the 1960s and 1970s that the Portuguese began to relinquish their colonies in Asia. Goa was invaded by India in 1961 and became an Indian state in 1987; Portuguese Timor was abandoned in 1975 and was then invaded by Indonesia. It became an independent country in 2002; and Macau was handed back to the Chinese as per a treaty in 1999.
Dutch trade and colonization in Asia.
Rise of Dutch control over Asian trade in the 17th century.
Portuguese decline in Asia was accelerated by the attacks on their commercial empire by the Dutch and the English, which began a global struggle over empire in Asia that lasted until the end of the Seven Years' War in 1763. The Netherlands revolt against Spanish rule facilitated Dutch encroachment of the Portuguese monopoly over South and East Asian trade. The Dutch looked on Spain's trade and colonies as potential spoils in war. When the two crowns of the Iberian peninsula were joined in 1581, the Dutch felt free to attack Portuguese territories in Asia.
By the 1590s, a number of Dutch companies were formed to finance trading expeditions in Asia. Because competition lowered their profits, and because of the doctrines of mercantilism, in 1602 the companies united into a cartel and formed the Dutch East India Company, and received from the government the right to trade and colonise territory in the area stretching from the Cape of Good Hope eastward to the Strait of Magellan.
In 1605, armed Dutch merchants captured the Portuguese fort at Amboyna in the Moluccas, which was developed into the first secure base of the company. Over time, the Dutch gradually consolidated control over the great trading ports of the East Indies. Control over the East Indies trading ports allowed the company to monopolise the world spice trade for decades. Their monopoly over the spice trade became complete after they drove the Portuguese from Malacca in 1641 and Ceylon in 1658.
Dutch East India Company colonies or outposts were later established in Atjeh (Aceh), 1667; Macassar, 1669; and Bantam, 1682. The company established its headquarters at Batavia (today Jakarta) on the island of Java. Outside the East Indies, the Dutch East India Company colonies or outposts were also established in Persia (Iran), Bengal (now Bangladesh and part of India), Mauritius (1638-1658/1664-1710), Siam (now Thailand), Guangzhou (Canton, China), Taiwan (1624–1662), and southern India (1616–1795). In 1662, Zheng Chenggong (also known as Koxinga) expelled the Dutch from Taiwan. ("see" History of Taiwan) Further, the Dutch East India Company trade post on Dejima (1641–1857), an artificial island off the coast of Nagasaki, was for a long time the only place where Europeans could trade with Japan.
In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, currently in South Africa) to restock company ships on their journey to East Asia. This post later became a fully-fledged colony, the Cape Colony (1652–1806). As Cape Colony attracted increasing Dutch and European settlement, the Dutch founded the city of Kaapstad (Cape Town).
By 1669, the Dutch East India Company was the richest private company in history, with a huge fleet of merchant ships and warships, tens of thousands of employees, a private army consisting of thousands of soldiers, and a reputation on the part of its stockholders for high dividend payments.
Dutch New Imperialism in Asia.
The company was in almost constant conflict with the English; relations were particularly tense following the Amboyna Massacre in 1623. During the 18th century, Dutch East India Company possessions were increasingly focused on the East Indies. After the fourth war between the United Provinces and England (1780–1784), the company suffered increasing financial difficulties. In 1799, the company was dissolved, commencing official colonisation of the East Indies. During the era of New Imperialism the territorial claims of the Dutch East India Company (VOC) expanded into a fully fledged colony named the Dutch East Indies. Partly driven by re-newed colonial aspirations of fellow European nation states the Dutch strived to establish unchallenged control of the archipelago now known as Indonesia.
Six years into formal colonisation of the East Indies, in Europe the Dutch Republic was occupied by the French forces of Napoleon. The Dutch government went into exile in England and formally ceded its colonial possessions to Great Britain. The pro-French Governor General of Java Jan Willem Janssens, resisted a British invasion force in 1811 until forced to surrender. British Governor Raffles, who the later founded the city of Singapore, ruled the colony the following 10 years of the British interregnum (1806–1816).
After the defeat of Napoleon and the Anglo-Dutch Treaty of 1814 colonial government of the East Indies was ceded back to the Dutch in 1817. The loss of South Africa and the continued scramble for Africa stimulated the Dutch to secure unchallenged dominion over its colony in the East Indies. The Dutch started to consolidate its power base through extensive military campaigns and elaborate diplomatic alliances with indigenous rulers ensuring the Dutch tricolor was firmly planted in all corners of the Archipelago. These military campaigns included: the Padri War (1821–1837), the Java War (1825–1830) and the Aceh War (1873–1904). This raised the need for a considerable military buildup of the colonial army (KNIL). From all over Europe soldiers were recruited to join the KNIL.
The Dutch concentrated their colonial enterprise in the Dutch East Indies (Indonesia) throughout the 19th century. The Dutch lost control over the East Indies to the Japanese during much of World War II. Following the war, the Dutch fought Indonesian independence forces after Japan surrendered to the Allies in 1945. In 1949 most of what was known as the Dutch East Indies was ceded to the independent Republic of Indonesia. In 1962 also Dutch New Guinea was annexed by Indonesia de facto ending Dutch imperialism in Asia.
British in India.
Portuguese, French, and British competition in India (1600–1763).
The English sought to stake out claims in India at the expense of the Portuguese dating back to the Elizabethan era. In 1600, Queen Elizabeth I incorporated the English East India Company (later the British East India Company), granting it a monopoly of trade from the Cape of Good Hope eastward to the Strait of Magellan. In 1639 it acquired Madras on the east coast of India, where it quickly surpassed Portuguese Goa as the principal European trading centre on the Indian Subcontinent.
Through bribes, diplomacy, and manipulation of weak native rulers, the company prospered in India, where it became the most powerful political force, and outrivaled its Portuguese and French competitors. For more than one hundred years, English and French trading companies had fought one another for supremacy, and, by the middle of the 18th century, competition between the British and the French had heated up. French defeat by the British under the command of Robert Clive during the Seven Years' War (1756–1763) marked the end of the French stake in India.
Collapse of Mughal India.
The British East India Company, although still in direct competition with French and Dutch interests until 1763, was able to extend its control over almost the whole of India in the century following the subjugation of Bengal at the 1757 Battle of Plassey. The British East India Company made great advances at the expense of a Mughal dynasty.
The reign of Aurangzeb had marked the height of Mughal power, By 1690. Mughal territorial expansion reached its greatest extent, Aurangzeb's Empire encompassed the entire Indian Subcontinent. But this period of power was followed by one of decline. Fifty years after the death of Aurangzeb, the great Mughal empire had crumbled. Meanwhile, marauding warlords, nobles, and others bent on gaining power left the Subcontinent increasingly anarchic. Although the Mughals kept the imperial title until 1858, the central government had collapsed, creating a power vacuum.
From Company to Crown.
Aside from defeating the French during the Seven Years' War, Robert Clive, the leader of the Company in India, defeated a key Indian ruler of Bengal at the decisive Battle of Plassey (1757), a victory that ushered in the beginning of a new period in Indian history, that of informal British rule. While still nominally the sovereign, the Mughal Indian emperor became more and more of a puppet ruler, and anarchy spread until the company stepped into the role of policeman of India. The transition to formal imperialism, characterised by Queen Victoria being crowned "Empress of India" in the 1870s was a gradual process. The first step toward cementing formal British control extended back to the late 18th century. The British Parliament, disturbed by the idea that a great business concern, interested primarily in profit, was controlling the destinies of millions of people, passed acts in 1773 and 1784 that gave itself the power to control company policies and to appoint the highest company official in India, the Governor-General. (This system of dual control lasted until 1858.) By 1818 the East India Company was master of all of India. Some local rulers were forced to accept its overlordship; others were deprived of their territories. Some portions of India were administered by the British directly; in others native dynasties were retained under British supervision.
Until 1858, however, much of India was still officially the dominion of the Mughal emperor. Anger among some social groups, however, was seething under the governor-generalship of James Dalhousie (1847–1856), who annexed the Punjab (1849) after victory in the Second Sikh War, annexed seven princely states on the basis of lapse, annexed the key state of Oudh on the basis of misgovernment, and upset cultural sensibilities by banning Hindu practices such as Sati. The 1857 Sepoy Rebellion, or Indian Mutiny, an uprising initiated by Indian troops, called sepoys, who formed the bulk of the Company's armed forces, was the key turning point. Rumour had spread among them that their bullet cartridges were lubricated with pig and cow fat. The cartridges had to be bit open, so this upset the Hindu and Muslim soldiers. The Hindu religion held cows sacred, and for Muslims pork was considered Haraam. In one camp, 85 out of 90 sepoys would not accept the cartridges from their garrison officer. The British harshly punished those who would not by jailing them. The Indian people were outraged, and on May 10, 1857, sepoys marched to Delhi, and, with the help of soldiers stationed there, captured it. Fortunately for the British, many areas remained loyal and quiescent, allowing the revolt to be crushed after fierce fighting. One important consequence of the revolt was the final collapse of the Mughal dynasty. The mutiny also ended the system of dual control under which the British government and the British East India Company shared authority. The government relieved the company of its political responsibilities, and in 1858, after 258 years of existence, the company relinquished its role. Trained civil servants were recruited from graduates of British universities, and these men set out to rule India. Lord Canning (created earl in 1859), appointed Governor-General of India in 1856, became known as "Clemency Canning" as a term of derision for his efforts to restrain revenge against the Indians during the Indian Mutiny. When the Government of India was transferred from the Company to the Crown, Canning became the first viceroy of India.
The Company initiated the first of the Anglo-Burmese wars in 1824, which led to total annexation of Burma by the Crown in 1885. The British ruled Burma as a province of British India until 1937, then administered her separately under the Burma Office except during the Japanese occupation of Burma, 1942–1945, until granted independence on 4 January 1948. (Unlike India, Burma opted not to join the Commonwealth of Nations.)
Rise of Indian nationalism.
The denial of equal status to Indians was the immediate stimulus for the formation in 1885 of the Indian National Congress, initially loyal to the Empire but committed from 1905 to increased self-government and by 1930 to outright independence. The "Home charges", payments transferred from India for administrative costs, were a lasting source of nationalist grievance, though the flow declined in relative importance over the decades to independence in 1947.
Although majority Hindu and minority Muslim political leaders were able to collaborate closely in their criticism of British policy into the 1920s, British support for a distinct Muslim political organisation, the Muslim League from 1906 and insistence from the 1920s on separate electorates for religious minorities, is seen by many in India as having contributed to Hindu-Muslim discord and the country's eventual Partition.
France in Indochina.
France, which had lost its empire to the British by the end of the 18th century, had little geographical or commercial basis for expansion in Southeast Asia. After the 1850s, French imperialism was initially impelled by a nationalistic need to rival the United Kingdom and was supported intellectually by the notion that French culture was superior to that of the people of Annam, and its "mission civilisatrice"—or its "civilizing mission" of the Annamese through their assimilation to French culture and the Catholic religion. The pretext for French expansionism in Indochina was the protection of French religious missions in the area, coupled with a desire to find a southern route to China through Tonkin, the European name for the northern region of northern Vietnam.
French religious and commercial interests were established in Indochina as early as the 17th century, but no concerted effort at stabilizing the French position was possible in the face of British strength in the Indian Ocean and French defeat in Europe at the beginning of the 19th century. A mid-19th century religious revival under the Second Empire provided the atmosphere within which interest in Indochina grew. Anti-Christian persecutions in the Far East provided the pretext for the bombardment of Tourane (Da Nang) in 1847, and invasion and occupation of Da Nang in 1857 and Saigon in 1858. Under Napoleon III, France decided that French trade with China would be surpassed by the British, and accordingly the French joined the British against China in the Second Opium War from 1857 to 1860, and occupied parts of Vietnam as its gateway to China.
By the Treaty of Saigon in 1862, on June 5, the Vietnamese emperor ceded France three provinces of southern Vietnam to form the French colony of Cochinchina; France also secured trade and religious privileges in the rest of Vietnam and a protectorate over Vietnam's foreign relations. Gradually French power spread through exploration, the establishment of protectorates, and outright annexations. Their seizure of Hanoi in 1882 led directly to war with China (1883–1885), and the French victory confirmed French supremacy in the region. France governed Cochinchina as a direct colony, and central and northern Vietnam under the protectorates of Annam and Tonkin, and Cambodia as protectorates in one degree or another. Laos too was soon brought under French "protection."
By the beginning of the 20th century, France had created an empire in Indochina nearly 50 percent larger than the mother country. A Governor-General in Hanoi ruled Cochinchina directly and the other regions through a system of residents. Theoretically, the French maintained the precolonial rulers and administrative structures in Annam, Tonkin, Cochinchina, Cambodia, and Laos, but in fact the governor-generalship was a centralised fiscal and administrative regime ruling the entire region. Although the surviving native institutions were preserved in order to make French rule more acceptable, they were almost completely deprived of any independence of action. The ethnocentric French colonial administrators sought to assimilate the upper classes into France's "superior culture." While the French improved public services and provided commercial stability, the native standard of living declined and precolonial social structures eroded. Indochina, which had a population of over eighteen million in 1914, was important to France for its tin, pepper, coal, cotton, and rice. It is still a matter of debate, however, whether the colony was commercially profitable.
Russia and "The Great Game".
Tsarist Russia is not often regarded as a colonial power such as the United Kingdom or France because of the manner of Russian expansions: unlike the United Kingdom, which expanded overseas, the Russian empire grew from the centre outward by a process of accretion, like the United States. In the 19th century, Russian expansion took the form of a struggle of an effectively landlocked country for access to a warm water port.
While the British were consolidating their hold on India, Russian expansion had moved steadily eastward to the Pacific, then toward the Middle East. In the early 19th century it succeeded in conquering the South Caucasus and Dagestan from Qajar Persia following the Russo-Persian War (1804–13), the Russo-Persian War (1826–28) and the out coming treaties of Gulistan and Turkmenchay, giving Russia direct borders with both Persia's as well as Ottoman Turkey's heartlands. Later, they eventually reached the frontiers of Afghanistan as well (which had the largest foreign border adjacent to British holdings in India). In response to Russian expansion, the defense of India's land frontiers and the control of all sea approaches to the Subcontinent via the Suez Canal, the Red Sea, and the Persian Gulf became preoccupations of British foreign policy in the 19th century.
Anglo-Russian rivalry in the Middle East and Central Asia led to a brief confrontation over Afghanistan in the 1870s. In Persia (Iran), both nations set up banks to extend their economic influence. The United Kingdom went so far as to invade Tibet, a land subordinate to the Chinese empire, in 1904, but withdrew when it became clear that Russian influence was insignificant and when Chinese resistance proved tougher than expected.
In 1907, the United Kingdom and Russia signed an agreement which — on the surface —ended their rivalry in Central Asia. ("see" Anglo-Russian Entente) As part of the entente, Russia agreed to deal with the sovereign of Afghanistan only through British intermediaries. In turn, the United Kingdom would not annex or occupy Afghanistan. Chinese suzerainty over Tibet also was recognised by both Russia and the United Kingdom, since nominal control by a weak China was preferable to control by either power. Persia was divided into Russian and British spheres of influence and an intervening "neutral" zone. The United Kingdom and Russia chose to reach these uneasy compromises because of growing concern on the part of both powers over German expansion in strategic areas of China and Africa.
Following the entente, Russia increasingly intervened in Persian domestic politics and suppressed nationalist movements that threatened both St. Petersburg and London. After the Russian Revolution, Russia gave up its claim to a sphere of influence, though Soviet involvement persisted alongside the United Kingdom's until the 1940s.
In the Middle East, a German company built a railroad from Constantinople to Baghdad and the Persian Gulf. In Persia (Iran), it built a railroad from the north of the country to the south, connecting the Caucasus with the Persian Gulf. Germany wanted to gain economic influence in the region and then, perhaps, move on to India. This was met with bitter resistance by the United Kingdom, Russia, and France who divided the region among themselves.
European intrusions into China.
The 16th century brought many Jesuit missionaries to China, such as Matteo Ricci, who established missions where Western science was introduced, and where Europeans gathered knowledge of Chinese society, history, culture, and science. During the 18th century, merchants from Western Europe came to China in increasing numbers. However, merchants were confined to Guangzhou and the Portuguese colony of Macau, as they had been since the 16th century. European traders were increasingly irritated by what they saw as the relatively high customs duties they had to pay and by the attempts to curb the growing import trade in opium. By 1800, its importation was forbidden by the imperial government. However, the opium trade continued to boom.
Early in the 19th century, serious internal weaknesses developed in the Qing dynasty that left China vulnerable to Western, Meiji period Japanese, and Russian imperialism. In 1839, China found itself fighting the First Opium War with Britain. China was defeated, and in 1842, signed the provisions of the Treaty of Nanjing which were first of the unequal treaties signed during the Qing Dynasty. Hong Kong Island was ceded to Britain, and certain ports, including Shanghai and Guangzhou, were opened to British trade and residence. In 1856, the Second Opium War broke out. The Chinese were again defeated, and now forced to the terms of the 1858 Treaty of Tientsin. The treaty opened new ports to trade and allowed foreigners to travel in the interior. In addition, Christians gained the right to propagate their religion. The United States Treaty of Wanghia and Russia later obtained the same prerogatives in separate treaties.
Toward the end of the 19th century, China appeared on the way to territorial dismemberment and economic vassalage—the fate of India's rulers that played out much earlier. Several provisions of these treaties caused long-standing bitterness and humiliation among the Chinese: extraterritoriality (meaning that in a dispute with a Chinese person, a Westerner had the right to be tried in a court under the laws of his own country), customs regulation, and the right to station foreign warships in Chinese waters, including its navigable rivers.
The rise of Japan since the Meiji Restoration as an imperial power led to further subjugation of China. In a dispute over China's longstanding claim of suzerainty in Korea, war broke out between China and Japan, resulting in humiliating defeat for the Chinese. By the Treaty of Shimonoseki (1895), China was forced to recognize effective Japanese rule of Korea and Taiwan was ceded to Japan until its recovery in 1945 at the end of the WWII by the Republic of China.
China's defeat at the hands of Japan was another trigger for future aggressive actions by Western powers. In 1897, Germany demanded and was given a set of exclusive mining and railroad rights in Shandong province. Russia obtained access to Dairen and Port Arthur and the right to build a railroad across Manchuria, thereby achieving complete domination over a large portion of northwestern China. The United Kingdom and France also received a number of concessions. At this time, much of China was divided up into "spheres of influence": Germany dominated Jiaozhou (Kiaochow) Bay, Shandong, and the Huang He (Hwang-Ho) valley; Russia dominated the Liaodong Peninsula and Manchuria; the United Kingdom dominated Weihaiwei and the Yangtze Valley; and France dominated the Guangzhou Bay and several other southern provinces.
China continued to be divided up into these spheres until the United States, which had no sphere of influence, grew alarmed at the possibility of its businessmen being excluded from Chinese markets. In 1899, Secretary of State John Hay asked the major powers to agree to a policy of equal trading privileges. In 1900, several powers agreed to the U.S.-backed scheme, giving rise to the "Open Door" policy, denoting freedom of commercial access and non-annexation of Chinese territory. In any event, it was in the European powers' interest to have a weak but independent Chinese government. The privileges of the Europeans in China were guaranteed in the form of treaties with the Qing government. In the event that the Qing government totally collapsed, each power risked losing the privileges that it already had negotiated.
The erosion of Chinese sovereignty contributed to a spectacular anti-foreign outbreak in June 1900, when the "Boxers" (properly the society of the "righteous and harmonious fists") attacked European legations in Beijing, provoking a rare display of unity among the powers, whose troops landed at Tianjin and marched on the capital. British and French forces looted, plundered and burned the Old Summer Palace to the ground for the second time (the first time being in 1860, following the Second Opium War), as a form of threat to force the Qing empire to give in to their demands. German forces were particularly severe in exacting revenge for the killing of their ambassador, while Russia tightened its hold on Manchuria in the northeast until its crushing defeat by Japan in the war of 1904–1905.
Although extraterritorial jurisdiction was abandoned by the United Kingdom and the United States in 1943, foreign political control of parts of China only finally ended with the incorporation of Hong Kong and the small Portuguese territory of Macau into the People's Republic of China in 1997 and 1999 respectively.
U.S. imperialism in Asia.
As the United States emerged as a new imperial power in the Pacific and Asia, one of the two oldest Western imperialist powers in the regions, Spain, was finding it increasingly difficult to maintain control of territories it had held in the regions since the 16th century. In 1896, a widespread revolt against Spanish rule broke out in the Philippines. Meanwhile, the recent string of U.S. territorial gains in the Pacific posed an even greater threat to Spain's remaining colonial holdings.
As the U.S. continued to expand its economic and military power in the Pacific, it declared war against Spain in 1898. During the Spanish–American War, U.S. Admiral Dewey destroyed the Spanish fleet at Manila and U.S. troops landed in the Philippines. Spain later agreed by treaty to cede the Philippines in Asia and Guam in the Pacific. In the Caribbean, Spain ceded Puerto Rico to the U.S. The war also marked the end of Spanish rule in Cuba, which was to be granted nominal independence but remained heavily influenced by the U.S. government and U.S. business interests. One year following its treaty with Spain, the U.S. occupied the small Pacific outpost of Wake Island.
The Filipinos, who assisted U.S. troops in fighting the Spanish, wished to establish an independent state and, on June 12, 1898, declared independence from Spain. In 1899, fighting between the Filipino nationalists and the U.S. broke out; it took the U.S. almost fifteen years to fully subdue the insurgency. The U.S. sent 70,000 troops and suffered thousands of casualties. The Filipinos insurgents, however, suffered considerably higher casualties than the Americans. Most casualties in the war were civilians dying primarily from disease.
U.S. attacks into the countryside often included scorched earth campaigns where entire villages were burned and destroyed, and concentrated civilians into camps known as "protected zones." Most of these civilian casualties resulted from disease and famine. Reports of the execution of U.S. soldiers taken prisoner by the Filipinos led to disproportionate reprisals by American forces.
In 1914, Dean C. Worcester, U.S. Secretary of the Interior for the Philippines (1901–1913) described "the regime of civilisation and improvement which started with American occupation and resulted in developing naked savages into cultivated and educated men." Nevertheless, some Americans, such as Mark Twain, deeply opposed American involvement/imperialism in the Philippines, leading to the abandonment of attempts to construct a permanent U.S. naval base and using it as an entry point to the Chinese market. In 1916, Congress guaranteed the independence of the Philippines by 1945.
World War I: Changes in Imperialism.
World War I brought about the fall of several empires in Europe. This had repercussions around the world. The defeated Central Powers included Germany and the Turkish Ottoman Empire. Germany lost all of its colonies in Asia. German New Guinea, a part of Papua New Guinea, became administered by Australia. German possessions and concessions in China, including Qingdao, became the subject of a controversy during the Paris Peace Conference when the Beiyang government in China agreed to cede these interests to Japan, to the anger of many Chinese people. Although the Chinese diplomats refused to sign the agreement, these interests were ceded to Japan with the support of the United States and the United Kingdom.
Turkey gave up her provinces; Syria, Palestine, and Mesopotamia (now Iraq) came under French and British control as League of Nations Mandates. The discovery of petroleum first in Iran and then in the Arab lands in the interbellum provided a new focus for activity on the part of the United Kingdom, France, and the United States.
Japan.
In 1641, all Westerners were thrown out of Japan. For the next two centuries, Japan was free from Western influence, except for at the port of Nagasaki, which Japan allowed Dutch merchant vessels to enter on a limited basis.
Japan's freedom from Western penetration ended on 8 July 1853, when Commodore Matthew Perry of the U.S. Navy sailed a squadron of black-hulled warships into Edo (modern Tokyo) harbor. The Japanese told Perry to sail to Nagasaki but he refused. Perry sought to present a letter from U.S. President Millard Fillmore to the emperor which demanded concessions from Japan. Japanese authorities responded by stating that they could not present the letter directly to the emperor, but scheduled a meeting on July 14 with a representative of the emperor. On 14 July, the squadron sailed towards the shore, giving a demonstration of their cannon's firepower thirteen times. Perry landed with a large detachment of Marines and presented the emperor's representative with Fillmore's letter. Perry said he would return, and did so, this time with even more war ships. The U.S. show of force led to Japan's concession to the Convention of Kanagawa on 31 March 1854. This treaty conferred extraterritoriality on American nationals, as well as, opening up further treaty ports beyond Nagasaki. This treaty was followed up by similar treaties with the United Kingdom, Holland, Russia and France. These events made Japanese authorities aware that the country was lacking technologically and needed the strength of industrialism in order to keep their power. This realisation eventually led to a civil war and political reform known the Meiji Restoration.
The Meiji Restoration of 1868 led to administrative overhaul, deflation and subsequent rapid economic development. Japan had limited natural resources of her own and sought both overseas markets and sources of raw materials, fuelling a drive for imperial conquest which began with the defeat of China in 1895.
Taiwan, ceded by Qing Dynasty China, became the first Japanese colony. In 1899, Japan won agreements from the great powers' to abandon extraterritoriality for their citizens, and an alliance with the United Kingdom established it in 1902 as an international power. Its spectacular defeat of Russia's navy in 1905 gave it the southern half of the island of Sakhalin; exclusive Japanese influence over Korea (propinquity); the former Russian lease of the Liaodong Peninsula with Port Arthur (Lüshunkou); and extensive rights in Manchuria (see the Russo-Japanese War).
The Empire of Japan and the Joseon Dynasty in Korea formed bilateral diplomatic relations in 1876. China lost its suzerainty of Korea after defeat in the Sino-Japanese War in 1894. Russia also lost influence on the Korean peninsula with the Treaty of Portsmouth as a result of the Russo-Japanese war in 1904. The Joseon Dynasty became increasingly dependent on Japan. Korea became a protectorate of Japan with the Japan–Korea Treaty of 1905. Korea was then "de jure" annexed to Japan with the Japan–Korea Treaty of 1910.
Japan was now one of the most powerful forces in the Far East, and in 1914, it entered World War I on the side of the Allies, seizing German-occupied Kiaochow and subsequently demanding Chinese acceptance of Japanese political influence and territorial acquisitions (Twenty-One Demands, 1915). Mass protests in Peking in 1919 coupled with Allied (and particularly U.S.) opinion led to Japan's abandonment of most of the demands and Joseon's 1922 return to China. Japan received the German territory from the Treaty of Versailles, 1919, sparking widespread Chinese nationalism.
Tensions with China increased over the 1920s, and in 1931 Japanese army units based in Manchuria seized control of the region without direction from Tokyo. Intermittent conflict with China led to full-scale war in mid-1937, drawing Japan toward an overambitious bid for Asian hegemony (Greater East Asia Co-Prosperity Sphere), which ultimately led to defeat and the loss of all its overseas territories after World War II (see Japanese expansionism and Japanese nationalism).
Post World War II era.
Decolonisation and the rise of nationalism in Asia.
In the aftermath of World War II, European colonies, controlling more than one billion people throughout the world, still ruled most of the Middle East, South East Asia, and the Indian Subcontinent. However, the image of European pre-eminence was shattered by the wartime Japanese occupations of large portions of British, French, and Dutch territories in the Pacific. The destabilisation of European rule led to the rapid growth of nationalist movements in Asia—especially in Indonesia, Malaya, Burma, and French Indochina.
The war, however, only accelerated forces already in existence undermining Western imperialism in Asia. Throughout the colonial world, the processes of urbanisation and capitalist investment created professional merchant classes that emerged as new Westernised elites. While imbued with Western political and economic ideas, these classes increasingly grew to resent their unequal status under European rule.
British in India and the Middle East.
In India, the westward movement of Japanese forces towards Bengal during World War II had led to major concessions on the part of British authorities to Indian nationalist leaders. In 1947, the United Kingdom, devastated by war and embroiled in economic crisis at home, granted British India its independence as two nations: India and Pakistan. The following year independence was granted to Burma and Ceylon. In the Middle East, the United Kingdom granted independence to Jordan in 1946 and two years later ended its mandate of Palestine.
Dutch East Indies.
Following the end of the war, nationalists in Indonesia demanded complete independence from the Netherlands. A brutal conflict ensued, and finally, in 1949, through United Nations mediation, the Dutch East Indies achieved independence, becoming the new nation of Indonesia. Dutch imperialism moulded this new multi-ethnic state comprising roughly 3,000 islands of the Indonesian archipelago with a population at the time of over 100 million.
The end of Dutch rule opened up latent tensions between the roughly 300 distinct ethnic groups of the islands, with the major ethnic fault line being between the Javanese and the non-Javanese.
United States in Asia.
In the Philippines, the U.S. remained committed to its previous pledges to grant the islands their independence, and the Philippines became the first of the Western-controlled Asian colonies to be granted independence post-World War II. However, the Philippines remained under pressure to adopt a political and economic system similar to their old imperial master.
This aim was greatly complicated by the rise of new political forces. During the war, the "Hukbalahap" (People's Army), which had strong ties to the Communist Party of the Philippines (PKP), fought against the Japanese occupation of the Philippines and won strong popularity among many sectors of the Filipino working class and peasantry. In 1946, the PKP participated in elections as part of the Democratic Alliance. However, with the onset of the Cold War, its growing political strength drew a reaction from the ruling government and the United States, resulting in the repression of the PKP and its associated organisations. In 1948, the PKP began organizing an armed struggle against the government and continued U.S. military presence. In 1950, the PKP created the People's Liberation Army ("Hukbong Mapagpalaya ng Bayan"), which mobilised thousands of troops throughout the islands. The insurgency lasted until 1956, when the PKP gave up armed struggle.
In 1968, the PKP underwent a split, and in 1969 the Maoist faction of the PKP created the New People's Army. Maoist rebels re-launched an armed struggle against the government and the U.S. military presence in the Philippines, which continues to this day.
France in Indochina.
Post-war resistance to French rule.
France remained determined to retain its control of Indochina. However, in Hanoi, in 1945, a broad front of nationalists and communists led by Ho Chi Minh declared an independent Republic of Vietnam, commonly referred to as the Viet Minh regime by Western outsiders. France, seeking to regain control of Vietnam, countered with a vague offer of self-government under French rule. France's offers were unacceptable to Vietnamese nationalists; and in December 1946 the Việt Minh launched a rebellion against the French authority governing the colonies of French Indochina. The first few years of the war involved a low-level rural insurgency against French authority. However, after the Chinese communists reached the Northern border of Vietnam in 1949, the conflict turned into a conventional war between two armies equipped with modern weapons supplied by the United States and the Soviet Union. Meanwhile, the France granted the State of Vietnam based in Saigon independence in 1949 whilst Laos and Cambodia received independence in 1953. The US recognized the regime in Saigon, and provided the French military effort with military aid.
Meanwhile, in Vietnam, the French war against the Viet Minh continued for nearly eight years. The French were gradually worn down by guerrilla and jungle fighting. The turning point for France occurred at Dien Bien Phu in 1954, which resulted in the surrender of ten thousand French troops. Paris was forced to accept a political settlement that year at the Geneva Conference, which led to a precarious set of agreements regarding the future political status of Laos, Cambodia, and Vietnam.

</doc>
<doc id="15445" url="http://en.wikipedia.org/wiki?curid=15445" title="Entropy (information theory)">
Entropy (information theory)

In information theory, entropy is the average amount of information contained in each message received. Here, "message" stands for an event, sample or character drawn from a distribution or data stream. Entropy thus characterizes our uncertainty about our source of information. (Entropy is best understood as a measure of uncertainty rather than certainty as entropy is larger for more random sources). The source is also characterized by the probability distribution of the samples drawn from it. The idea here is that the less likely an event is, the more information it provides when it occurs. For some other reasons (explained below) it makes sense to define information as the negative of the logarithm of the probability distribution. The probability distribution of the events, coupled with the information amount of every event, forms a random variable whose average (also termed expected value) is the average amount of information, a.k.a. entropy, generated by this distribution. The units of entropy are commonly referred to as bits, but entropy is also measured in shannons, nats, or hartleys, depending on the base of the logarithm used to define it.
The logarithm of the probability distribution is useful as a measure of information because it is additive. For instance, flipping a coin provides 1 shannon of information whereas "m" tosses gather "m" bits. Generally, you need log2("n") bits to represent a variable that can take one of "n" values. Since 1 of "n" outcomes is possible when you apply a scale graduated with "n" marks, you receive log2("n") bits of information with every such measurement. The log2("n") rule holds only while all outcomes are equally probable. If one of the events occurs more often than others, observation of that event is less informative. Conversely, observing rarer events compensate by providing more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as the average information) received from non-uniformly distributed data is less than log2("n"). Entropy is zero when only one certain outcome is expected. Shannon entropy quantifies all these considerations exactly when a probability distribution of the source is provided. It is important to note that the meaning of the events observed (a.k.a. the meaning of "messages") do not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.
Generally, "entropy" stands for "disorder" or uncertainty. The entropy we talk about here was introduced by Claude E. Shannon in his 1948 paper "A Mathematical Theory of Communication". We also call it Shannon entropy to distinguish from other occurrences of the term, which appears in various parts of physics in different forms. Shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of any communication, assuming that the communication may be represented as a sequence of independent and identically distributed random variables.
Introduction.
Entropy is a measure of "unpredictability" of "information content". To get an informal, intuitive understanding of the connection between these three English terms, consider the example of a poll on some political issue. Usually, such polls happen because the outcome of the poll isn't already known. In other words, the outcome of the poll is relatively "unpredictable", and actually performing the poll and learning the results gives some new "information"; these are just different ways of saying that the "entropy" of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the entropy of the second poll result relative to the first is small.
Now consider the example of a coin toss. When the coin is fair, that is, when the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time—the best we can do is predict that the coin will come up heads, and our prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. Contrarily, a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly.
English text has fairly low entropy. In other words, it is fairly predictable. Even if we don't know exactly what is going to come next, we can be fairly certain that, for example, there will be many more e's than z's, that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy for each character of message.
The Chinese version of Wikipedia points out that Chinese characters have a much higher entropy than English. Each character of Chinese has about -log2(1/2500)=11.3bits which is almost three times higher than English. However, the discussion could be much more sophisticated than this simple calculation because in English the usage of words, not only characters, and redundancy factors could be considered.
If a compression scheme is lossless—that is, you can always recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a lossless compression scheme cannot compress messages, on average, to have "more" than one bit of information per bit of message, but that any value "less" than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.
Shannon's theorem also implies that no lossless compression scheme can shorten "all" messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music that is already in the FLAC audio format is unlikely to achieve much extra saving in space.
Definition.
Named after Boltzmann's H-theorem, Shannon defined the entropy Η (Greek letter Eta) of a discrete random variable "X" with possible values {"x"1, ..., "x""n"} and probability mass function P("X") as:
Here E is the expected value operator, and I is the information content of "X".
I("X") is itself a random variable.
When taken from a finite sample, the entropy can explicitly be written as
where "b" is the base of the logarithm used. Common values of "b" are 2, Euler's number "e", and 10, and the unit of entropy is shannon for "b" = 2, nat for "b" = "e", and hartley for "b" = 10. When "b" = 2, the units of entropy are also commonly referred to as bits.
In the case of "p"("x""i") = 0 for some "i", the value of the corresponding summand 0 log"b"(0) is taken to be 0, which is consistent with the limit:
One may also define the conditional entropy of two events "X" and "Y" taking values "xi" and "yj" respectively, as
where "p"("xi", "yj") is the probability that "X" = "xi" and "Y" = "yj". This quantity should be understood as the amount of randomness in the random variable "X" given that you know the value of "Y".
Example.
Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this is known as the Bernoulli process.
The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information.
However, if we know the coin is not fair, but comes up heads or tails with probabilities "p" and "q", where "p" ≠ "q", then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information.
The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain. In this respect, entropy can be normalized by dividing it by information length. This ratio is called metric entropy and is a measure of the randomness of the information.
Rationale.
To understand the meaning of formula_5, at first, try to define an information function, I, in terms of an event "i" with probability formula_6. How much information is acquired due to the observation of event "i"? Shannon's solution follows from the fundamental properties of information:
The last is a crucial property. It states that joint probability communicates as much information as two individual events separately. Particularly, if the first event can yield one of n equiprobable outcomes and another has one of "m" equiprobable outcomes then there are "mn" possible outcomes of the joint event. This means that if log2("n") bits are needed to encode the first value and log2("m") to encode the second, one needs log2("mn") = log2("m") + log2("n") to encode both. Shannon discovered that the proper choice of function to quantify information, preserving this additivity, is logarithmic, i.e.,
The base of the logarithm can be any fixed real number greater than 1. The different units of information (bits for log2, trits for log3, nats for the natural logarithm ln and so on) are just constant multiples of each other. (In contrast, the entropy would be negative if the base of the logarithm were less than 1.) For instance, in case of a fair coin toss, heads provides log2(2) = 1 bit of information, which is approximately 0.693 nats or 0.631 trits. Because of additivity, "n" tosses provide "n" bits of information, which is approximately 0.693"n" nats or 0.631"n" trits.
Now, suppose we have a distribution where event "i" can happen with probability "pi". Suppose we have sampled it "N" times and outcome "i" was, accordingly, seen formula_8 times. The total amount of information we have received is 
The average amount of information that we receive with every event is therefore
Aspects.
Relationship to thermodynamic entropy.
The inspiration for adopting the word "entropy" in information theory came from the close resemblance between Shannon's formula and very similar known formulae from statistical mechanics.
In statistical thermodynamics the most general formula for the thermodynamic entropy "S" of a thermodynamic system is the Gibbs entropy,
where "k"B is the Boltzmann constant, and "pi" is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Boltzmann (1872).
The Gibbs entropy translates over almost unchanged into the world of quantum physics to give the von Neumann entropy, introduced by John von Neumann in 1927,
where ρ is the density matrix of the quantum mechanical system and Tr is the trace.
At an everyday practical level the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in "changes" in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. And, as the minuteness of Boltzmann's constant "k"B indicates, the changes in "S"/"k"B for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. Furthermore, in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.
At a multidisciplinary level, however, connections can be made between thermodynamic and informational entropy, although it took many years in the development of the theories of statistical mechanics and information theory to make the relationship fully apparent. In fact, in the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an "application" of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the Boltzmann constant. For example, adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, thus making any complete state description longer. (See article: "maximum entropy thermodynamics"). Maxwell's demon can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as Landauer (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). Landauer's principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.
Entropy as information content.
Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.
The entropy rate of a data source means the average number of bits per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text.
From the preceding example, note the following points:
Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits (see caveat below in italics). The formula can be derived by calculating the mathematical expectation of the "amount of information" contained in a digit from the information source. "See also" Shannon-Hartley theorem.
Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). "Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc." See Markov chain.
Data compression.
Entropy effectively bounds the performance of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. The performance of existing data compression algorithms is often used as a rough estimate of the entropy of a block of data. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.
World's technological capacity to store and communicate information.
A 2011 study in "Science" estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources. 
The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories—to store information on a medium, to receive information through a one-way broadcast networks, or to exchange information through two-way telecommunication networks.
Limitations of entropy as information content.
There are a number of entropy-related concepts that mathematically quantify information content in some way:
(The "rate of self-information" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a stationary process.) Other quantities of information are also used to compare or relate different sources of information.
It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the "entropy" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy "rate".
Although entropy is often used as a characterization of the information content of a data source, this information content is not absolute: it depends crucially on the probabilistic model. A source that always generates the same symbol has an entropy rate of 0, but the definition of what a symbol is depends on the alphabet. Consider a source that produces the string ABABABABAB... in which A is always followed by B and vice versa. If the probabilistic model considers individual letters as independent, the entropy rate of the sequence is 1 bit per character. But if the sequence is considered as "AB AB AB AB AB..." with symbols as two-character blocks, then the entropy rate is 0 bits per character.
However, if we use very large blocks, then the estimate of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are "N" published books, and each book is only published once, the estimate of the probability of each book is 1/"N", and the entropy (in bits) is −log2(1/"N") = log2("N"). As a practical code, this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.
For example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, ... . Treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately log2("n"). So the first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [F("n") = F("n"−1) + F("n"−2) for "n"={3,4,5...}, F(1)=1, F(2)=1] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.
Limitations of entropy as a measure of unpredictability.
In cryptanalysis, entropy is often roughly used as a measure of the unpredictability of a cryptographic key. For example, a 128-bit key that is randomly generated has 128 bits of entropy. It takes (on average) formula_13 guesses to break by brute force. If the key's first digit is 0, and the others random, then the entropy is 127 bits, and it takes (on average) formula_14 guesses.
However, entropy fails to capture the number of guesses required if the possible keys are not of equal probability. If the key is half the time "password" and half the time a true random 128-bit key, then the entropy is approximately 65 bits. Yet half the time the key may be guessed on the first try, if your first guess is "password", and on average, it takes around formula_15 guesses (not formula_16) to break this password.
Similarly, consider a 1000000-digit binary one-time pad. If the pad has 1000000 bits of entropy, it is perfect. If the pad has 999999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may still be considered very good. But if the pad has 999999 bits of entropy, where the first digit is fixed and the remaining 999999 digits are perfectly random, then the first digit of the ciphertext will not be encrypted at all.
Data as a Markov process.
A common way to define entropy for text is based on the Markov model of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:
where "p""i" is the probability of "i". For a first-order Markov source (one in which the probability of selecting a character is dependent only on the immediately preceding character), the entropy rate is:
where "i" is a state (certain preceding characters) and formula_19 is the probability of "j" given "i" as the previous character.
For a second order Markov source, the entropy rate is
"b"-ary entropy.
In general the "b"-ary entropy of a source formula_21 = ("S","P") with source alphabet "S" = {"a"1, ..., "an"} and discrete probability distribution "P" = {"p"1, ..., "pn"} where "pi" is the probability of "ai" (say "pi" = "p"("ai")) is defined by:
Note: the "b" in ""b"-ary entropy" is the number of different symbols of the "ideal alphabet" used as a standard yardstick to measure source alphabets. In information theory, two symbols are necessary and sufficient for an alphabet to encode information. Therefore, the default is to let "b" = 2 ("binary entropy"). Thus, the entropy of the source alphabet, with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the source alphabet. Also note that "optimal probability distribution" here means a uniform distribution: a source alphabet with "n" symbols has the highest possible entropy (for an alphabet with "n" symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be log"b"("n").
Efficiency.
A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency:
Efficiency has utility in quantifying the effective use of a communications channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy formula_24.
Characterization.
Shannon entropy is characterized by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form
where "K" is a constant corresponding to a choice of measurement units.
In the following, "pi" = Pr("X" = "xi") and formula_26.
Continuity.
The measure should be continuous, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.
Symmetry.
The measure should be unchanged if the outcomes "xi" are re-ordered.
Maximum.
The measure should be maximal if all the outcomes are equally likely (uncertainty is highest when all possible events are equiprobable).
For equiprobable events the entropy should increase with the number of outcomes.
Additivity.
The amount of entropy should be independent of how the process is regarded as being divided into parts.
This last functional relationship characterizes the entropy of a system with sub-systems. It demands that the entropy of a system can be calculated from the entropies of its sub-systems if the interactions between the sub-systems are known.
Given an ensemble of "n" uniformly distributed elements that are divided into "k" boxes (sub-systems) with "b"1, ..., "bk" elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.
For positive integers "bi" where "b"1 + ... + "bk" = "n",
Choosing "k" = "n", "b"1 = ... = "bn" = 1 this implies that the entropy of a certain outcome is zero: Η1(1) = 0. This implies that the efficiency of a source alphabet with "n" symbols can be defined simply as being equal to its "n"-ary entropy. See also Redundancy (information theory).
Further properties.
The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable "X":
Proving this mathematically follows easily from the previous two properties of entropy.
Extending discrete entropy to the continuous case.
Differential entropy.
The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with probability density function "f(x)" with finite or infinite support formula_37 on the real line is defined by analogy, using the above form of the entropy as an expectation:
This formula is usually referred to as the continuous entropy, or differential entropy. A precursor of the continuous entropy "h"["f"] is the expression for the functional "H" in the H-theorem of Boltzmann.
Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and thus corrections have been suggested, notably limiting density of discrete points.
To answer this question, we must establish a connection between the two functions:
We wish to obtain a generally finite measure as the bin size goes to zero. In the discrete case, the bin size is the (implicit) width of each of the "n" (finite or infinite) bins whose probabilities are denoted by "pn". As we generalize to the continuous domain, we must make this width explicit.
To do this, start with a continuous function "f" discretized into bins of size formula_39.
By the mean-value theorem there exists a value "xi" in each bin such that
and thus the integral of the function "f" can be approximated (in the Riemannian sense) by
where this limit and "bin size goes to zero" are equivalent.
We will denote
and expanding the logarithm, we have
As Δ → 0, we have
But note that log(Δ) → −∞ as Δ → 0, therefore we need a special definition of the differential or continuous entropy:
which is, as said before, referred to as the differential entropy. This means that the differential entropy "is not" a limit of the Shannon entropy for "n" → ∞. Rather, it differs from the limit of the Shannon entropy by an infinite offset.
It turns out as a result that, unlike the Shannon entropy, the differential entropy is "not" in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations.
Relative entropy.
Another useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution. It is defined as the Kullback–Leibler divergence from the distribution to a reference measure "m" as follows. Assume that a probability distribution "p" is absolutely continuous with respect to a measure "m", i.e. is of the form "p"("dx") = "f"("x")"m"("dx") for some non-negative "m"-integrable function "f" with "m"-integral 1, then the relative entropy can be defined as
In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure "m" is the counting measure, and the differential entropy, where the measure "m" is the Lebesgue measure. If the measure "m" is itself a probability distribution, the relative entropy is non-negative, and zero if "p" = "m" as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure "m". The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure "m".
Use in combinatorics.
Entropy has become a useful quantity in combinatorics.
Loomis-Whitney inequality.
A simple example of this is an alternate proof of the Loomis-Whitney inequality: for every subset "A" ⊆ Z"d", we have
where "Pi" is the orthogonal projection in the ith coordinate:
The proof follows as a simple corollary of Shearer's inequality: if "X"1, ..., "Xd" are random variables and "S"1, ..., "Sn" are subsets of {1, ..., "d"} such that every integer between "1" and "d" lies in exactly "r" of these subsets, then
where formula_50 is the Cartesian product of random variables "Xj" with indexes "j" in "Si" (so the dimension of this vector is equal to the size of "Si").
We sketch how Loomis-Whitney follows from this: Indeed, let "X" be a uniformly distributed random variable with values in "A" and so that each point in "A" occurs with equal probability. Then (by the further properties of entropy mentioned above) Η("X") = log|"A"|, where |"A"| denotes the cardinality of "A". Let "Si" = {1, 2, ..., "i"−1, "i"+1, ..., "d"}. The range of formula_51 is contained in "Pi"("A") and hence formula_52. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.
Approximation to binomial coefficient.
For integers 0 < "k" < "n" let "q" = "k"/"n". Then
where 
Here is a sketch proof. Note that formula_55 is one term of the expression
Rearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then,
since there are "n"+1 terms in the summation. Rearranging gives the lower bound.
A nice interpretation of this is that the number of binary strings of length "n" with exactly "k" many 1's is approximately formula_58.
References.
"This article incorporates material from on PlanetMath, which is licensed under the ."

</doc>
<doc id="15446" url="http://en.wikipedia.org/wiki?curid=15446" title="Ithaca College">
Ithaca College

Ithaca College is a coeducational, nonsectarian private college located on the South Hill of Ithaca, New York, United States. The school was founded by William Egbert in 1892 as a conservatory of music. The college has a strong liberal arts core, but also offers several pre-professional programs and some graduate programs. The college is also known internationally for its communications program, the Roy H. Park School of Communications, which was most recently ranked as a top school for journalism, film and media. The college is set against the backdrop of Cayuga Lake, the city of Ithaca, and several waterfalls and gorges. The college is perhaps best known for its large list of alumni who play or have played substantial roles in the worlds of media and entertainment. 
Ithaca College has been ranked among the top ten master's universities in the North by "U.S. News & World Report" every year since 1996. For the 2014 rankings, the college was ranked eighth in this category. Ithaca College is also consistently named among the best colleges in the nation by Princeton Review, with the annual guide also ranking the college at #1 for radio and #7 for theater. In 2015, Ithaca College was ranked #49 in New York State by average professor salaries.
History.
Beginnings.
Ithaca College was founded as the Ithaca Conservatory of Music in 1892 when a local violin teacher, William Grant Egbert, rented four rooms and arranged for the instruction of eight students. For nearly seven decades the institution flourished in the city of Ithaca, adding to its music curriculum the study of elocution, dance, physical education, speech correction, radio, business, and the liberal arts. In 1931 the conservatory was chartered as a private college. The college was originally housed in the Boardman House, that later became the Ithaca College Museum of Art, and it was listed on the National Register of Historic Places in 1971.
Modern era.
By 1960, some 2,000 students were in attendance. A modern campus was built on South Hill in the sixties, and students were shuttled between the old and new during the construction. The hillside campus continued to grow in the ensuing 30 years to accommodate more than 6,000 students.
As the campus expanded, the college also began to expand its curriculum. By the 1990s, some 2,000 courses in more than 100 programs of study were available in the college's five schools. The school attracts a multicultural student body with representatives from almost every state and from 78 foreign countries.
Campus.
Ithaca College's current campus was built in the 1960s on South Hill. In 1968 the College's final academic department moved to the South Hill campus from downtown, making the move complete.
Satellite campuses.
Besides its Ithaca campus, Ithaca College has also operated satellite campuses in other cities. The Ithaca College London Center has been in existence since 1972. Ithaca runs the Ithaca College Los Angeles Program at the James B. Pendleton Center. Additionally, there is an Ithaca College Washington Semester Program, and a recently launched Ithaca College New York City Center.
Former programs include the Ithaca College Antigua Program and the Ithaca College Walkabout Down Under Program in Australia.
Ithaca College also operates direct enrollment exchange programs with several universities, including Griffith University, La Trobe University, Murdoch University, and University of Tasmania (Australia); Chengdu Sport University and Beijing Sport University (China); University of Hong Kong; Masaryk University (Czech Republic); Akita International University and University of Tsukuba (Japan); Hanyang University (Korea); Nanyang Technological University (Singapore); University of Valencia (Spain); and Jönköping University (Sweden).
Academics.
The college offers a curriculum with more than 100 degree programs in its five schools.
Until recently, several cross-disciplinary degree programs along with the Center for the Study of Culture, Race, and Ethnicity were housed in the Division of Interdisciplinary and International Studies; however, starting spring 2011, the division was eliminated and its programs, centers and institutes were absorbed within other schools.
As of the 2012-2013 academic year, Television-Radio and Business Administration were the two most popular majors, while the School of Humanities & Sciences had the most students overall.
Student life.
Media and publications.
With its top-ranked Roy H. Park School of Communications, Ithaca College is well known for its several prominent student-run media vehicles, including:
Greek life.
Historically, various independent and national fraternities and sororities had active chapters at Ithaca College. However, due to a series of highly publicized hazing incidents in the 1980s, including one that was responsible for the death of a student, the College administration removed all but five Greek letter organizations from campus, and adopted a non-expansion policy, prohibiting any new Greek houses from affiliating with the College. As of 2014, three recognized Greek organizations remain on campus, all of which are music-oriented:
A fourth house, performing arts fraternity Kappa Gamma Psi (Iota Chapter) became inactive in 2008. Although there are potentially plans to reactivate the chapter, it is unclear whether this will be permitted or not due to the college's non-expansionist policy.
However, there are various Greek letter organizations at Ithaca College that are unaffiliated with the school, and therefore not subject to the same housing privileges or rules that contribute to the safety of their members such as non-hazing and non-drinking policies. Additionally, while not particularly common, Ithaca College students may rush for Greek houses affiliated with Cornell University, subject to the rules of each individual fraternity or sorority. Some Cornell affiliated Greek organizations actively recruit Ithaca College students.
Athletics.
The Ithaca athletics nickname "Bombers" is unique in NCAA athletics, and the origins of the nickname are obscure. Ithaca College's sports teams were originally named the Cayugas, but the name was changed to the Bombers sometime in the 1930s. Several possibilities for the change have been posited. The most common explanation is that the school's baseball uniforms—white with navy blue pinstripes and an interlocking "IC" on the left chest—bear a striking resemblance to the distinctive home uniforms of the New York Yankees, who are known as the Bronx Bombers. It may also have referred to the Ithaca basketball team of that era and its propensity for half-court "bombs". Grumman Aircraft also manufactured airplanes including bombers in Ithaca for many years. The first “Bombers” reference on record was in the December 17, 1938 issue of the "Rochester Times-Union" in a men’s basketball article.
The name has at times sparked controversy for its perceived martial connotations. It is an occasional source of umbrage from Ithaca's prominent pacifist community, but the athletics department has consistently stated it has no interest in changing the name. The athletics logo has in the past incorporated World War II era fighter planes, but currently does not, and the school does not currently have a physical mascot to personify the name. In 2010 the school launched a contest to choose one. It received over 250 suggestions and narrowed the field down to three: a phoenix, a flying squirrel, and a Lake Beast. In June 2011, President Rochon announced that the school would discontinue the search due to opposition in the alumni community.
Ithaca is a member of the NCAA's Division III, the Empire Eight Conference, and the Eastern College Athletic Conference. Ithaca has one of Division III's strongest athletic programs. The Bombers have won a total of 15 national titles in seven team sports and five individual sports.
Ithaca College recently remodeled the Hill Center in 2013. The building features hardwood floors (Ben Light Gymnasium) as well as coaches offices. The building is home to Ithaca's men's and women's basketball teams, women's volleyball team, wrestling, and gymnastics. Ithaca also opened the Athletics & Events Center in 2011, a $65.5 million facility funded by donors. The facility is mainly used by the school's varsity athletes. It has a 47,000 square foot, 9-lane 50 meter Olympic-size pool. The building also has Glazer Arena, a 130,000 square foot event space. It is a track and field center that doubles as a practice facility for lacrosse, field hockey, soccer, baseball, tennis, and football.
Coached by Jim Butterfield for 27 years, the football team has won three NCAA Division III National Football Championships in 1979, 1988 and 1991 (a total surpassed only by Augustana, Mount Union and Wisconsin-Whitewater). Bomber football teams made a record seven appearances in the Division III national championship game, the Amos Alonzo Stagg Bowl, which has since been surpassed by Mount Union in 2003. The Bombers play the SUNY Cortland Red Dragons for the Cortaca Jug, which was added in 1959 to an already competitive rivalry. The matchup is one of the most prominent in Division III college football.
Most recently, the women's crew won back-to-back NCAA Division III championships in 2004 and 2005.
Women's soccer has won two national championships in Division III and is consistently ranked in the top 20 nationally.
The Men's Wrestling team won NCAA Division III National Championships in 1989, 1990 and 1994.
This past year in 2013, Paula Miller, head of Woman's Swimming team completed her 30th year as head coach of the Ithaca Bombers. She has led the team to many victories. The past four years the Bombers have been undefeated throughout their season defeating tough competition. Ithaca has finished first or second at 25 of the past 29 state meets. The Bombers have also won the Empire 8 crown in each of the past nine seasons.
The 2013-2014 season ended with regaining the NCAA Division III Championship trophy.
Ithaca is also home to more than 60 club sports, many of which compete regularly against other colleges in leagues and tournaments.
"Ithaca Forever".
"Ithaca Forever" is the official alma mater or school song of Ithaca College.
Intramurals.
Along with Intercollegiate athletics, Ithaca College has a rather large Intramural sport program. This extracurricular program serves approximately 25% of the undergraduate population yearly. Fourteen traditional team activities are offered throughout the year and include: basketball, flag football, kickball, soccer, softball, ultimate frisbee, ski racing, and volleyball.
For most activities divisions are offered for men’s, women’s, and co-recreational teams. Throughout the year usually two or more activities run concurrently and participants are able to play on a single sex team and co-recreational team for each activity.
The most popular activities recently have been 5-on-5 basketball with over forty teams entered for the past three years, for the past two years there have been over thirty indoor flag football teams and teams have been turned away.
During 08-09 new records were established for total teams in both 4-person and 6-person volleyball, 3-on-3 basketball, and co-recreational indoor soccer. During the 08-09 year there were 1,559 intramural participants and over 500 female participants. It was estimated that the 2009–10 year and the 2010–11 have even more participants in intramural sports.
Sustainability.
Ithaca's School of Business was the first college or university business school in the world to achieve LEED Platinum Certification alongside Yale University, which had the second. Ithaca's Peggy Ryan Williams Center is also LEED Platinum certified. It makes extensive use of day light in occupied spaces. There are sensors that regulate lighting and ventilation based on occupancy and natural light. Over 50% of the building energy comes from renewable sources such as wind power. The college also has a LEED Gold Certified building, the Athletics & Events Center. The College composts its dining hall waste, runs a "Take It or Leave It" Green move-out program, and offers a sustainable living option. It also operates an office supply collection and reuse program, as well as a sustainability education program during new student orientation. Ithaca received a B- grade on the Sustainable Endowments Institute's 2009 College Sustainability Report Card and an A- for 2010.
Environmental record.
Commitments to action on climate change.
In Spring 2007, then-President Peggy R. Williams signed the American College and University President's Climate Commitment (ACUPCC), pledging Ithaca College to the task of developing a strategy and long-range plan to achieve "carbon neutrality" at some point in the future. In 2009 the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050. In 2009, the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050 and offers a 40-year action plan to work toward that ambitious goal.
Energy profile.
The college purchases 14 percent of its electricity from renewable sources and offsets 3 percent of its energy use with renewable energy credits.
Energy investments.
The college aims to optimize investment returns and does not invest the endowment in on-campus sustainability projects, renewable energy funds, or community development loan funds. The college's investment policy reserves the right of the investment committee to restrict investments for any reason, which could include environmental and sustainability factors.
Community impact.
While the Ithaca College Natural Lands has issued a statement that Ithaca College should join efforts calling for a moratorium on horizontal drilling and high volume (“slick water”) hydraulic fracturing, or fracking, the college as a whole has refused to issue a statement regarding the issue.
Leadership.
Current president.
Ithaca's current president is Thomas Rochon. Thomas Rochon was named the eighth president of Ithaca College on April 11, 2008. Rochon took over as president of the college following Peggy Williams, who had announced on July 12, 2007, that she would retire from the presidency post effective May 31, 2009, following a one-year sabbatical.
Alumni.
Ithaca College has 49,570 alumni in the United States. There are alumni clubs for Boston, Chicago, Connecticut, Los Angeles, Metro New York, National Capital, N. & S. Carolina, Philadelphia, Rochester (NY), San Diego, and Southern Florida. Alumni events are hosted in cooperation with the specific clubs and also through a program called "IC on the Road".
Following is a brief list of noteworthy Ithaca College alumni. 
For a more extensive list, see main entry List of Ithaca College alumni.
Faculty.
Following is a brief list of noteworthy Ithaca College faculty.

</doc>
<doc id="15447" url="http://en.wikipedia.org/wiki?curid=15447" title="Differential psychology">
Differential psychology

Differential psychology studies the ways in which individuals differ in their behavior and the processes that underlie it. This is distinguished from other aspects of psychology in that although psychology is ostensibly a study of individuals, modern psychologists often study groups or biological underpinnings of cognition. 
For example, in evaluating the effectiveness of a new therapy, the mean performance of the therapy in one treatment group might be compared to the mean effectiveness of a placebo (or a well-known therapy) in a second, control group. In this context, differences between individuals in their reaction to the experimental and control manipulations are actually treated as errors rather than as interesting phenomena to study. 
This is because psychological research depends upon statistical controls that are only defined upon groups of people. Individual difference psychologists usually express their interest in individuals while studying groups by seeking dimensions shared by all individuals but upon which individuals differ.
Importance of individual differences.
Individual differences are essential whenever we wish to explain how 
individuals differ in their behavior. In any study, significant 
variation exists between individuals. Reaction time, preferences, 
values, and health-linked behaviors are just a few examples. Individual 
differences in factors such as personality, intelligence, 
memory, or physical factors such as body size, sex, age, and other 
factors can be studied and used in understanding this large source of 
variance. Importantly, individuals can also differ not only in their 
current state, but in the magnitude or even direction of response to a 
given stimulus. Such phenomena, often 
explained in terms of inverted-U response curves, 
place differential psychology at an important location in such 
endeavours as personalized medicine, in which diagnoses are 
customised for an individual's response profile.
Areas of study.
Individual differences research typically includes personality, motivation, intelligence, ability, IQ, interests, values, self-concept, self-efficacy, and self-esteem (to name just a few). There are few remaining "differential psychology" programs in the United States, although research in this area is very active. Current researchers are found in a variety of applied and experimental programs, including educational psychology, Industrial and organizational psychology, personality psychology, social psychology, and developmental psychology programs, in the neo-Piagetian theories of cognitive development in particular.

</doc>
<doc id="15450" url="http://en.wikipedia.org/wiki?curid=15450" title="Industrial and organizational psychology">
Industrial and organizational psychology

Industrial and organizational psychology (also known as I–O psychology, occupational psychology, work psychology, WO psychology, IWO psychology and business psychology) is the scientific study of human behavior in the workplace and applies psychological theories and principles to organizations.
I-O psychologists are trained in the scientist–practitioner model. I-O psychologists contribute to an organization's success by improving the performance, satisfaction, safety, health and well-being of its employees. An I–O psychologist conducts research on employee behaviors and attitudes, and how these can be improved through hiring practices, training programs, feedback, and management systems. I–O psychologists also help organizations and their employees transition among periods of change and organization development.
I-O psychology is one of the 14 recognized specialties and proficiencies in professional psychology in the United States and is represented by Division 14 of the American Psychological Association (APA), known formally as the Society for Industrial and Organizational Psychology (SIOP).
In the UK, industrial and organizational psychologists are referred to as occupational psychologists and one of 7 'protected titles' and specializations in psychology regulated by the Health and Care Professions Council.
In Australia, the title organizational psychologist is also protected by law and is regulated by the Australian Health Practitioner Regulation Agency (AHPRA). Organizational psychology is one of nine areas of specialist endorsement for psychology practice in Australia. Graduate programs at both the Masters and Doctorate level are offered worldwide.
In the UK graduate degrees are accredited by the British Psychological Society and required as part of the process to become an occupational psychologist.
In Europe someone with a specialist EuroPsy Certificate in Work and Organisational Psychology is a fully qualified psychologist and an expert in the work psychology field with further advanced education and training.
Historical overview.
The historical development of I–O psychology had parallel developments in the United States and other countries, such as the UK, Australia, Germany, the Netherlands, and eastern European countries such as Romania. However, many foreign countries do not have a published English language account of their development of I–O psychology. The roots of I-O psychology trace back nearly to the beginning of psychology as a science, when Wilhelm Wundt founded one of the first psychological laboratories in 1876 in Leipzig, Germany. In the mid 1880s, Wundt trained two psychologists who had a major influence on the eventual emergence of I–O Psychology: Hugo Münsterberg and James McKeen Cattell. Instead of viewing differences as “errors”, Cattell was one of the first to recognize the importance of these differences among individuals as a way of predicting and better understanding their behavior. Walter Dill Scott, who was a contemporary of Cattell, was elected President of the American Psychological Association (APA) in 1919, was arguably the most prominent I–O psychologist of his time. Scott, along with Walter Van Dyke Bingham worked at the Carnegie Institute of Technology, developing methods for selecting and training sales personnel
The "industrial" side of I–O psychology has its historical origins in research on individual differences, assessment, and the prediction of work performance. This branch crystallized during World War I, in response to the need to rapidly assign new troops to duty stations. Scott and Bingham volunteered to help with the testing and placement of more than a million army recruits. In 1917, together, along with other prominent psychologists, adapted a well-known intelligence test, (the Stanford-Binet test, designed for testing one individual at a time) to make it suitable for mass group testing. This new test form was called the Army Alpha. After the War, the growing industrial base in the US added impetus to I–O psychology. The private industry set out to emulate the successful testing of army personnel, and mental ability testing soon became a commonplace in the work setting. Industrial psychology began to gain prominence when Elton Mayo arrived in the United States in 1924. Mayo was fascinated by not the efficiency of workers, but their emotions and how work may cause workers to act in particular pathological ways. These observations of workers’ thoughts and emotions were studied to see how prone employees would be to resist management attempts to increase productivity and how sympathetic to labor unions they would become. These studies are known as Hawthorne studies. The results of these studies ushered in a radically new movement known as the Human Relations Movement. This movement was interested in the more complicated theories of motivation, the emotional world of the worker, job satisfaction, and interviews with workers.
World War II brought in new problems that led to I–O Psychology's continued development. The war brought renewed interest in ability testing (to accurately place recruits in these new technologically advanced military jobs), the introduction of the assessment center, concern with morale and fatigue of war industry workers, and military intelligence. Post-Second World War years were a boom time for industry with many jobs to be filled and applicants to be tested. Interestingly, however, when the war ended and the soldiers came back to work, there was an increasing trend towards labor unrest with rising numbers of authorized and unauthorized work stoppages staged by unions and workers. This caused management to grow concern about work productivity and worker attitude surveys became of much interest in the field. Following Industrial Organizational Psychology's admission into Division 14 of the American Psychological Association, there continued to be an influx of new tests for selection, productivity, and workforce stability. This influx continued unabated until the passage of the Civil Rights Act of 1964. Section, Title VII dealt with employment discrimination and required employers to justify and show relevance for the use of tests for selection.
The mid-1960s seemed to mark a line of demarcation between "classic" and "modern" thinking. During this period, the name changed from just industrial psychology to industrial and organizational psychology. The earlier periods addressed work behavior from the individual perspective, examining performance and attitudes of individual workers. Although this was a valuable approach, it became clear that there were other, broader influences not only on individual, but also on group behavior in the work place. Thus, in 1973, "organizational" was added to the name to emphasize the fact that when an individual joins an organization (e.g., the organization that hired him or her), he or she will be exposed to a common goal and a common set of operating procedures.
In the 1970s in the United Kingdom, references to occupational psychology became more common than I-O psychology. Rigor and methods of psychology are applied to issues of critical relevance to business, including talent management, coaching, assessment, selection, training, organizational development, performance, well-being and work-life balance. During the 1990s references to "business psychology" became increasingly common. Business psychology is defined as the study and practice of improving working life. It combines an understanding of the science of human behavior with experience of the world of work to attain effective and sustainable performance for both individuals and organizations.
Research methods.
As described above, I–O psychologists are trained in the scientist–practitioner model. I–O psychologists rely on a variety of methods to conduct organizational research. Study designs employed by I–O psychologists include surveys, experiments, quasi-experiments, and observational studies. I–O psychologists rely on diverse data sources including human judgments, historical databases, objective measures of work performance (e.g., sales volume), and questionnaires and surveys.
I–O researchers employ both quantitative and qualitative research methods. Quantitative methods used in I–O psychology include both descriptive statistics and inferential statistics (e.g., correlation, multiple regression, and analysis of variance). More advanced statistical methods employed by some I–O psychologists include logistic regression, multivariate analysis of variance, structural equation modeling, and hierarchical linear modeling (HLM; also known as multilevel modeling). HLM is particularly applicable to research on team- and organization-level effects on individuals. I–O psychologists also employ psychometric methods including methods associated with classical test theory (CTT), generalizability theory, and item response theory (IRT). In the 1990s, a growing body of empirical research in I–O psychology was influential in the application of meta-analysis, particularly in the area of the stability of research findings across contexts. The most well-known meta-analytic approaches are those associated with Hunter & Schmidt, Rosenthal, and Hedges & Olkin. With the help of meta-analysis, Hunter & Schmidt advanced the idea of validity generalization, which suggests that some performance predictors, specifically cognitive ability tests (see especially Hunter [1986] and Hunter & Schmidt [1996]) have a relatively stable and positive relation to job performance across all jobs. Although not unchallenged, validity generalization has broad acceptance with regard to many selection instruments (e.g. cognitive ability tests, job knowledge tests, work samples, and structured interviews) across a broad range of jobs.
Qualitative methods employed in I–O psychology include content analysis, focus groups, interviews, case studies, and several other observational techniques. I–O research on organizational culture research has employed ethnographic techniques and participant observation to collect data. One well-known qualitative technique employed in I–O psychology is John Flanagan's Critical Incident Technique, which requires "qualified observers" (e.g., pilots in studies of aviation, construction workers in studies of construction projects) to describe a work situation that resulted in a good or bad outcome. Objectivity is ensured when multiple observers identify the same incidents. The observers are also asked to provide information about what the actor in the situation could have done differently to influence the outcome. This technique is then used to describe the critical elements of performance in certain jobs and how worker behavior relates to outcomes. Most notably, this technique has been employed to improve performance among aircraft crews and surgical teams, literally saving thousands of lives since its introduction. An application of the technique in research on coping with job stress comes from O'Driscoll & Cooper. The resistance to qualitative research resulted from viewing it too excessively subjective. This concern, however, is misplaced due to all methods of research, either qualitative or quantitative, ultimately requiring some sort of interpretation. When a researcher is developing and researching a phenomenon, all information available should be used, regardless of its form. The key is triangulation, which is an approach looking for converging information from different sources to develop that theory.
I–O psychologists sometimes use quantitative and qualitative methods in concert. The two are not mutually exclusive. For example, when constructing behaviorally-anchored rating scales (BARS), a job analyst may use qualitative methods, such as critical incidents interviews and focus groups to collect data bearing on performance. Then the analyst would have SMEs rate those examples on a Likert scale and compute inter-rater agreement statistics to judge the adequacy of each item. Each potential item would additionally be correlated with an external criterion in order to evaluate its usefulness if it were to be selected to be included in a BARS metric. As a simpler example, consider an extended observation of a worker, which might include videotaped episodes of performance - a qualitative measure. The qualitative video could easily be used to develop a frequency count of a particular behavior - a quantitative measure.
Topics.
Job analysis.
Job analysis has a few different methods but it primarily involves the systematic collection of information about a job. The task-oriented job analysis, involves an examination of the duties, tasks, and/or competencies required by a job, whereas a worker-oriented job analysis, involves an examination of the knowledge, skills, abilities, and other characteristics (KSAOs) required to successfully perform the work. Job analysis information is used for many purposes, including the creation of job-relevant selection procedures, performance appraisals and criteria, or training programs. Position analysis questionnaire is a particular analysis that is used to determined an individuals job characteristics and relates them to human characteristics.
Personnel recruitment and selection.
I–O psychologists typically work with HR specialists to design (a) recruitment processes and (b) personnel selection systems. Personnel recruitment is the process of identifying qualified candidates in the workforce and getting them to apply for jobs within an organization. Personnel recruitment processes include developing job announcements, placing ads, defining key qualifications for applicants, and screening out unqualified applicants.
Personnel selection is the systematic process of hiring and promoting personnel. Personnel selection systems employ evidence-based practices to determine the most qualified candidates. Personnel selection involves both the newly hired and individuals who can be promoted from within the organization. Common selection tools include ability tests (e.g., cognitive, physical, or psycho-motor), knowledge tests, personality tests, structured interviews, the systematic collection of biographical data, and work samples. I–O psychologists must evaluate evidence regarding the extent to which selection tools predict job performance, evidence that bears on the validity of selection tools.
Personnel selection procedures are usually validated, i.e., shown to be job relevant, using one or more of the following types of validity: content validity, construct validity, and/or criterion-related validity. I–O psychologists adhere to professional standards, such as the Society for Industrial and Organizational Psychology's (SIOP) "Principles for Validation and Use of Personnel Selection Procedures" and the Standards for Educational and Psychological Testing. The Equal Employment Opportunity Commission's "Uniform Guidelines" are also influential in guiding personnel selection although they have been criticized as outdated when compared to the current state of knowledge in I–O psychology.
I–O psychologists not only help in the selection and assessment of personnel for jobs, but also assist in the selection of students for admission to colleges, universities, and graduate and professional schools as well as the assessment of student achievement, student aptitude, and the performance of teachers and K–12 schools. Increasingly, I–O psychologists are working for educational assessment and testing organizations and divisions.
A meta-analysis of selection methods in personnel psychology found that general mental ability was the best overall predictor of job performance and training performance.
Performance appraisal/management.
Performance appraisal or performance evaluation is the process of measuring an individual's or a group's work behaviors and outcomes against the expectations of the job. Performance appraisal is frequently used in promotion and compensation decisions, to help design and validate personnel selection procedures, and for performance management. Performance management is the process of providing performance feedback relative to expectations and improvement information (e.g., coaching, mentoring). Performance management may also include documenting and tracking performance information for organization-level evaluation purposes.
An I–O psychologist would typically use information from the job analysis to determine a job's performance dimensions, and then construct a rating scale to describe each level of performance for the job. Often, the I–O psychologist would be responsible for training organizational personnel how to use the performance appraisal instrument, including ways to minimize bias when using the rating scale, and how to provide effective performance feedback. Additionally, the I–O psychologist may consult with the organization on ways to use the performance appraisal information for broader performance management initiatives.
Individual assessment and psychometrics.
Individual assessment involves the measurement of individual differences. I–O psychologists perform individual assessments in order to evaluate differences among candidates for employment as well as differences among employees. The constructs measured pertain to job performance. With candidates for employment, individual assessment is often part of the personnel selection process. These assessments can include written tests, aptitude tests, physical tests, psycho-motor tests, personality tests, integrity and reliability tests, work samples, simulation and assessment centres.
Psychometrics is the science of measuring psychological variables, such as knowledge, skills, and abilities. I–O psychologists are generally well-trained in psychometric psychology.
Occupational health and wellbeing.
I/O psychologists and researchers are also concerned with occupational health and wellbeing. Researchers have examined the effect of physical exercise, and staying vigorous at work. Sonnentag and Niessen (2008) found that staying vigorous during working hours is important for work-related behaviour, subjective well-being, and for effective functioning in the family domain. Individuals high on their general level of vigour at work, benefited most from recovery experienced over the course of several days. A 2010 study found positive relationships between job satisfaction and life satisfaction, happiness, positive affect, and the absence of negative affect and feelings of positive wellbeing. Other researchers have looked at the negative health impacts of mature-aged unemployment. Another recent study conducted by Potocnik & Sonnentag (2013) examined the impact of engaging in seven types of activities on depression and quality of life in older workers over a period of 2 years, using a sample from the Survey of Health, Ageing and Retirement in Europe. Results indicated that I/O psychologists should make attempts to reduce physical demands over older employees at work, to help improve their health and well-being. Practitioners should also design intervention programmes and preventive measures that focus on how to stimulate older employees' engagement in community activities. I/O research has also examined effects of job mobility and negative health effects, including burnout in workers.
Workplace bullying, aggression and violence.
I/O psychology and I/O psychologists are also concerned with the related topics of workplace bullying, aggression and violence. This 2010 study investigated the impact of the larger organizational context on bullying as well as the group-level processes that impact on the incidence, and maintenance of bullying behaviour. The impact of engaging in certain thought patterns after exposure to workplace violence has also been examined. This 2011 research examines the detrimental effect that interpersonal aggressive behaviours may have on dimensions of team effectiveness particularly team performance and team viability.
Remuneration and compensation.
Compensation includes wages or salary, bonuses, pension/retirement contributions, and perquisites that can be converted to cash or replace living expenses. I–O psychologists may be asked to conduct a job evaluation for the purpose of determining compensation levels and ranges. I–O psychologists may also serve as expert witnesses in pay discrimination cases when disparities in pay for similar work are alleged.
Training and training evaluation.
Training is the systematic acquisition of skills, concepts, or attitudes that results in improved performance in another environment. Most people hired for a job are not already versed in all the tasks required to perform the job effectively. Evidence indicates that training is effective and that these training expenditures are paying off in terms of higher net sales and gross profitability per employee. Training can be beneficial for the organization and for employees in terms of increasing their value to their organization as well as their employability in the broader marketplace. Many organizations are using training and development as a way to attract and retain their most successful employees.
Similar to performance management (see above), an I–O psychologist would employ a job analysis in concert with principles of instructional design to create an effective training program. A training program is likely to include a summative evaluation at its conclusion in order to ensure that trainees have met the training objectives and can perform the target work tasks at an acceptable level. Training programs often include formative evaluations to assess the impact of the training as the training proceeds. Formative evaluations can be used to locate problems in training procedures and help I–O psychologists make corrective adjustments while the training is ongoing.
The basic foundation for training programs is learning. Learning outcomes can be organized into three broad categories: cognitive, skill-based, and affective outcomes. Cognitive is a type of learning outcome that includes declarative knowledge or the knowledge of rules, facts, and principles. An example is police officers acquire declarative knowledge about laws and court procedures. Skill-based is a learning outcome that concerns procedural knowledge and the development of motor and technical skills. An example is motor skills that involve the coordination of physical movements such as using a special tool or flying a certain aircraft, whereas technical skills might include understanding a certain software program, or exhibiting effective customer relations behaviors. Affective is a type of learning outcome that includes attitudes or beliefs that predispose a person to behave in a certain way. Attitudes can be developed or changed through training programs. Examples of these attitudes are organizational commitment and appreciation of diversity.
Before training design issues are considered, a careful needs analysis is required to develop a systematic understanding of where training is needed, what needs to be taught or trained, and who will be trained. Training needs analysis typically involves a three-step process that includes organizational analysis, task analysis and person analysis. Organizational analysis examines organizational goals, available resources, and the organizational environment to determine where training should be directed. This analysis identifies the training needs of different departments or subunits and systematically assessing manager, peer, and technological support for transfer of training. Organizational analysis also takes into account the climate of the organization and its subunits. For example, if a climate for safety is emphasized throughout the organization or in particular parts of the organization (e.g., production), then training needs will likely reflect this emphasis. Task analysis uses the results from job analysis on determining what is needed for successful job performance and then determines what the content of training should be. Task analysis can consist of developing task statements, determining homogeneous task clusters, and identifying KSAOs (knowledge, skills, abilities, other characteristics) required for the job. With organizations increasingly trying to identify "core competencies" that are required for all jobs, task analysis can also include an assessment of competencies. Person analysis identifies which individuals within an organization should receive training and what kind of instruction they need. Employee needs can be assessed using a variety of methods that identify weaknesses that training and development can address. The needs analysis makes it possible to identify the training program's objectives, which in turn, represents the information for both the trainer and trainee about what is to be learned for the benefit of the organization.
Therefore with any training program it is key to establish specify training objectives.
Schultz & Schultz (2010) states that need assessment is an analysis of corporate and individual goals undertaken before designing a training program. Examples of need assessment are based on organizational, task, and work analysis is conducted using job analysis critical incidents, performance appraisal, and self-assessment techniques.(p164)
But with any training there are always challenges that one faces. Challenges which I–O psychologists face:(p185)
Motivation in the workplace.
Work motivation "is a set of energetic forces that originate both within as well as beyond an individual's being, to initiate work-related behavior, and to determine its form, direction, intensity, and duration" Understanding what motivates an organization's employees is central to the study of I–O psychology. Motivation is a person's internal disposition to be concerned with an approach positive incentives and avoid negative incentives. To further this, an "incentive" is the anticipated reward or aversive event available in the environment. While motivation can often be used as a tool to help predict behavior, it varies greatly among individuals and must often be combined with ability and environmental factors to actually influence behavior and performance. Because of motivation's role in influencing workplace behavior and performance, it is key for organizations to understand and to structure the work environment to encourage productive behaviors and discourage those that are unproductive.
There is general consensus that motivation involves three psychological processes: arousal, direction, and intensity. Arousal is what initiates action. It is fueled by a person's need or desire for something that is missing from their lives at a given moment, either totally or partially. Direction refers to the path employees take in accomplishing the goals they set for themselves. Finally, intensity is the vigor and amount of energy employees put into this goal-directed work performance. The level of intensity is based on the importance and difficulty of the goal. These psychological processes result in four outcomes. First, motivation serves to direct attention, focusing on particular issues, people, tasks, etc. It also serves to stimulate an employee to put forth effort. Next, motivation results in persistence, preventing one from deviating from the goal-seeking behavior. Finally, motivation results in task strategies, which as defined by Mitchell & Daniels, are "patterns of behavior produced to reach a particular goal."
Occupational stress.
I/O psychologists are involved in the research and the practice of occupational stress and design of individual and organizational interventions to manage and reduce the stress levels and increase productivity, performance, health and wellbeing. Occupational stress is concerned with physical and psychosocial working conditions (termed stressors) that can elicit negative responses (termed strains) from employees. Occupational stress can have implications for organizational performance because of the emotions job stress evokes. For example, a job stressor such as conflict with a supervisor can precipitate anger that in turn motivates counterproductive workplace behaviors. Job-related hindrance stressors are directly (and challenge stressors inversely) related to turnover and turnover intentions. I/O research has examined the relations among work stressors and workplace aggression, withdrawal, theft, and substance abuse, strategies that individuals use to cope with work stress and prevent occupational burnout, and the relation of work stress to depressive symptoms.
A number of models have been developed to explain the job stress process. Examples of models that have influenced research include the person-environment fit model and the demand-control model. Research has also examined the interaction among personality variables and stressors and their effects on employee strains. I/O psychology is also concerned with the physical health outcomes caused by occupational stress. For instance, researchers at the institute of work psychology (IWP) examined the mediating role of psychological strain in relation to musculoskeletal disorders.
Research has also examined occupational stress in specific occupations. For example, there has been research on job stress in police, teachers, general practitioners, and dentists. Another concern has been the relation of occupational stress to family life. Other research has examined gender differences in leadership style and job stress and strain in the context of male- and female-dominated industries, burnout in the human services and other occupations, and unemployment-related distress. I/O psychology is also concerned with the relation of occupational stress to career advancement.
Occupational health and safety.
Occupational health and safety is concerned with how the work environment contributes to illness and injury of workers. Of particular importance are psychosocial hazards or risk factors that include fatigue, workplace violence, workplace bullying. Other factors important to employee health and well-being include work schedules (e.g., night shifts), work/family conflict, and burnout. Tools have been developed by I/O researchers and psychologists to measure these psychosocial risk factors in the workplace and "stress audits" can be used to help organizations remain compliant with various occupational health and safety regulations around the world.
Another area of concern is the high rate of occupational fatalities and injuries due to accidents. There is also research interest in how psychosocial hazards affect physical ailments like musculoskeletal disorder. A contributing psychosocial factor to accidents is safety climate, that concerns organizational policies and practices concerning safe behavior at work. A related concept that has to do with psychological well-being as opposed to accidents is psychosocial safety climate (PSC). PSC refers to policies, practices, and procedures for the protection of worker psychological health and safety. Safety leadership is another area of occupational health and safety I/O psychology is concerned with, where specific leadership styles affect safety compliance and safety participation.
Organizational culture.
Organizational culture can be described as a set of assumptions shared by the individuals in an organization that directs interpretation and action by defining appropriate behavior for various situations. There are three levels of organizational culture: artifacts, shared values, and basic beliefs and assumptions. Artifacts comprise the physical components of the organization that relay cultural meaning. Shared values are individuals' preferences regarding certain aspects of the organization's culture (e.g., loyalty, customer service). Basic beliefs and assumptions include individuals' impressions about the trustworthiness and supportiveness of an organization, and are often deeply ingrained within the organization's culture.
In addition to an overall culture, organizations also have subcultures. Examples of subcultures include corporate culture, departmental culture, local culture, and issue-related culture. While there is no single "type" of organizational culture, some researchers have developed models to describe different organizational cultures.
Organizational culture has been shown to have an impact on important organizational outcomes such as performance, attraction, recruitment, retention, employee satisfaction, and employee well-being. Also, organizations with an adaptive culture tend to perform better than organizations with an maladaptive culture.
Group behavior.
Group behavior is the interaction between individuals of a collective and the processes such as opinions, attitudes, growth, feedback loops, and adaptations that occur and change as a result of this interaction. The interactions serve to fulfill some need satisfaction of an individual who is part of the collective and helps to provide a basis for his interaction with specific members of the group.
A specific area of research in group behavior is the dynamics of teams. Team effectiveness refers to the system of getting people in a company or institution to work together effectively. The idea behind team effectiveness is that a group of people working together can achieve much more than if the individuals of the team were working on their own.
Team effectiveness.
Organizations support the use of teams, because teams can accomplish a much greater amount of work in a short period of time than can be accomplished by an individual contributor, and because the collective results of a group of contributors can produce higher quality deliverables. Five elements that are contributors to team effectiveness include: 
I/O research has looked at the negative impacts of workplace aggression on team performance and particularly team effectiveness as was evidenced in a recent study by Aube and Rousseau.
Team composition.
The composition of teams is initially decided during the selection of individual contributors that are to be assigned to specific teams and has a direct bearing on the resulting effectiveness of those teams. Aspects of team composition that should be considered during the team selection process include team member: knowledge, skills and abilities (KSAs), personalities, and attitudes.
As previously stated, one of the reasons organizations support the use of teams is the expectation of the delivery of higher quality results. To achieve these types of results, highly skilled members are more effective than teams built around those with lesser skills, and teams that include a diversity of skills have improved team performance (Guzzo & Shea, 1992). Additionally, increased average cognitive ability of team members has been shown to consistently correlate to increased work group effectiveness (Sundstrom et al., 2000). Therefore, organizations should seek to assign teams with team members that have a mix of KSAs. Teams that are composed of members that have the same KSAs may prove to be ineffective in meeting the team goals, no matter how talented the individual members are.
The personalities and attitudes of the individuals that are selected as team members are other aspects that should be taken into consideration when composing teams, since these individual traits have been found to be good indicators of team effectiveness. For example, a positive relationship between the team-level traits of agreeableness and conscientiousness and the team performance has been shown to exist (Van Vianen & De Dreu, 2001). Differing personalities of individual team members can affect the team climate in a negative way as members may clash and reduce team performance (Barrick, et al., 1998).
Task design.
A fundamental question in team task design is whether or not a task is even appropriate for a team. Those tasks that require predominantly independent work are best left to individuals, and team tasks should include those tasks that consist primarily of interdependent work. When a given task is appropriate for a team, task design can play a key role in team effectiveness (Sundstrom, et al., 2000).
The Job Characteristics Theory of motivation identifies core job dimensions that provide motivation for individuals and include: skill variety, task identity, task significance, autonomy and feedback (Hackman & Oldham, 1980). These dimensions map well to the team environment. Individual contributors that perform team tasks that are challenging, interesting, and engaging are more likely to be motivated to exert greater effort and perform better than those team members that are working on those tasks that do not have these characteristics.
Interrelated to the design of various tasks is the implementation method for the tasks themselves. For example, certain team members may find it challenging to cross train with other team members that have subject matter expertise in areas in which they are not familiar. In utilizing this approach, greater motivation is likely to result for both parties as the expert becomes the mentor and trainer and the cross-training team member finds learning new tasks to be an interesting change of pace. Such expansions of team task assignments can make teams more effective and require teams to spend greater amounts of time discussing and planning strategies and approaches for completing assigned tasks (Hackman, et al., 1976).
Organizational resources.
Organizational support systems impact the effectiveness of teams (Sundstrum, et al., 1990) and provide resources for teams operating in the multi-team environment. In this case, the provided resources include various resource types that teams require to be effective. During the chartering of new teams, organizational enabling resources are first identified. Examples of enabling resources include facilities, equipment, information, training and leadership. Also identified during team chartering are team-specific resources (e.g., budgetary resources, human resources). Team-specific human resources represent the individual contributors that are selected for each team as team members. Intra-team processes (e.g., task design, task assignment) are sufficient for effective utilization of these team-specific resources.
Teams also function in multi-team environments that are dynamic in nature and require teams to respond to shifting organizational contingencies (Salas, et al., 2004). In regards to resources, such contingencies include the constraints imposed by organizational resources that are not specifically earmarked for the exclusive use of certain teams. These types of resources are scarce in nature and must be shared by multiple teams. Examples of these scarce resources include subject matter experts, simulation and testing facilities, and limited amounts of time for the completion of multi-team goals. For these types of shared resources inter-team management processes (e.g.: constraint resource scheduling) must be provided to enable effective multi-team utilization.
Team rewards.
Organizational reward systems are a driver for strengthening and enhancing individual team member efforts that contribute towards reaching collective team goals (Luthans & Kreitner, 1985). In other words, rewards that are given to individual team members should be contingent upon the performance of the entire team (Sundstrom, et al., 1990).
Several design elements of organizational reward systems are needed to meet this objective. The first element for reward systems design is the concept that for a collective assessment to be appropriate for individual team members, the group's tasks must be highly interdependent. If this is not the case, individual assessment is more appropriate than team assessment (Wageman & Baker, 1997). A second design element is the compatibility between individual-level reward systems and team-level reward systems (DeMatteo, Eby, & Sundstrom, 1998). For example, it would be an unfair situation to reward the entire team for a job well done if only one team member did the great majority of the work. That team member would most likely view teams and team work in a negative fashion and not want to participate in a team setting in the future. A final design element is the creation of an organizational culture that supports and rewards employees who believe in the value of teamwork and who maintain a positive mental attitude towards team-based rewards (Haines and Taggar, 2006).
Team goals.
Goals for individual contributors have been shown to be motivating when they contain three elements: (1) difficulty, (2) acceptance, and (3) specificity (Lock & Latham, 1990). In the team setting, goal difficulty is related to group belief that the team can accomplish the tasks required to meet the assigned goal (Whitney, 1994). This belief (collective efficacy) is somewhat counterintuitive, but rests on team member perception that they now view themselves as more competent than others in the organization who were not chosen to complete such difficult goals. This in turn, can lead to higher levels of performance. Goal acceptance and specificity is also applicable to the team setting. When team members individually and collectively commit to team goals, team effectiveness is increased and is a function of increased supportive team behaviors (Aube & Rousseau, 2005).
As related to the team setting, it is also important to be aware of the interplay between the goals of individual contributors that participate on teams and the goals of the teams themselves. The selection of team goals must be done in coordination with the selection of goals for individuals. Individual goals must be in line with team goals (or not exist at all) to be effective (Mitchell & Silver, 1990). For example, a professional ball player that does well in his/her sport is rewarded individually for excellent performance. This individual performance generally contributes to improved team performance which can, in turn, lead to team recognition, such as a league championship.
Job satisfaction and commitment.
Job satisfaction reflects an employee's overall assessment of their job, particularly their emotions, behaviors, and attitudes about their work experience. It is one of the most heavily researched topics in industrial–organizational psychology with several thousand published studies. Job satisfaction has theoretical and practical utility for the field of psychology and has been linked to important job outcomes including attitudinal variables, absenteeism, employee turnover, and job performance. For instance, job satisfaction is strongly correlated with attitudinal variables such as job involvement, organizational commitment, job tensions, frustration, and feelings of anxiety. A 2010 meta-analyses found positive relationships between job satisfaction and life satisfaction, happiness, positive affect, and the absence of negative affect. Job satisfaction also has a weak correlation with employee's absentee behaviors and turnover from an organization with employees more likely to miss work or find other jobs if they are not satisfied. Finally, research has found that although a positive relationship exists between job satisfaction and performance, it is moderated by the use of rewards at an organization and the strength of employee's attitudes about their job.
Productive behavior.
Productive behavior is defined as employee behavior that contributes positively to the goals and objectives of an organization. When an employee begins a new job, there is a transition period during which he or she is not contributing positively to the organization. To successfully transition from being an outsider to a full-fledged member of an organization, an employee typically needs job-related training as well as more general information about the culture of the organization. In financial terms, productive behavior represents the point at which an organization begins to achieve some return on the investment it has made in a new employee. Industrial–organizational psychologists are typically more focused on productive behavior rather than simple job or task performance because of the ability to account for extra-role performance in addition to in-role performance. While in-role performance tells managers or researchers how well the employee performs the required technical aspects of the job, extra-role performance includes behaviors not necessarily required as part of the job but still contribute to organizational effectiveness. By taking both in-role and extra-role performance into account, industrial–organizational psychologists are able to assess employees' effectiveness (how well they do what they were hired to do), efficiency (their relative outputs to relative inputs), and their productivity (how much they help the organization reach its goals). Jex & Britt outline three different forms of productive behavior that industrial–organizational psychologists frequently evaluate in organizations: job performance; organizational citizenship behavior; and innovation.
Job performance.
Job performance represents behaviors employees engage in while at work which contribute to organizational goals. These behaviors are formally evaluated by an organization as part of an employee's responsibilities. In order to understand and ultimately predict job performance, it is important to be precise when defining the term. Job performance is about behaviors that are within the control of the employee and not about results (effectiveness), the costs involved in achieving results (productivity), the results that can be achieved in a period of time (efficiency), or the value an organization places on a given level of performance, effectiveness, productivity or efficiency (utility).
To model job performance, researchers have attempted to define a set of dimensions that are common to all jobs. Using a common set of dimensions provides a consistent basis for assessing performance and enables the comparison of performance across jobs. Performance is commonly broken into two major categories: in-role (technical aspects of a job) and extra-role (non-technical abilities such as communication skills and being a good team member). While this distinction in behavior has been challenged it is commonly made by both employees and management. A model of performance by Campbell breaks performance into in-role and extra-role categories. Campbell labeled job-specific task proficiency and non-job-specific task proficiency as in-role dimensions, while written and oral communication, demonstrating effort, maintaining personal discipline, facilitating peer and team performance, supervision and leadership and management and administration are labeled as extra-role dimensions. Murphy's model of job performance also broke job performance into in-role and extra-role categories. However, task-orientated behaviors composed the in-role category and the extra-role category included interpersonally-oriented behaviors, down-time behaviors and destructive and hazardous behaviors. However, it has been challenged as to whether the measurement of job performance is usually done through pencil/paper tests, job skills tests, on-site hands-on tests, off-site hands-on tests, high-fidelity simulations, symbolic simulations, task ratings and global ratings. These various tools are often used to evaluate performance on specific tasks and overall job performance. Van Dyne and LePine developed a measurement model in which overall job performance was evaluated using Campbell's in-role and extra-role categories. Here, in-role performance was reflected through how well "employees met their performance expectations and performed well at the tasks that made up the employees' job." Dimensions regarding how well the employee assists others with their work for the benefit of the group, if the employee voices new ideas for projects or changes to procedure and whether the employee attends functions that help the group composed the extra-role category.
To assess job performance, reliable and valid measures must be established. While there are many sources of error with performance ratings, error can be reduced through rater training and through the use of behaviorally-anchored rating scales. Such scales can be used to clearly define the behaviors that constitute poor, average, and superior performance. Additional factors that complicate the measurement of job performance include the instability of job performance over time due to forces such as changing performance criteria, the structure of the job itself and the restriction of variation in individual performance by organizational forces. These factors include errors in job measurement techniques, acceptance and the justification of poor performance and lack of importance of individual performance.
The determinants of job performance consist of factors having to do with the individual worker as well as environmental factors in the workplace. According to Campbell's Model of The Determinants of Job Performance, job performance is a result of the interaction between declarative knowledge (knowledge of facts or things), procedural knowledge (knowledge of what needs to be done and how to do it), and motivation (reflective of an employee's choices regarding whether to expend effort, the level of effort to expend, and whether to persist with the level of effort chosen). The interplay between these factors show that an employee may, for example, have a low level of declarative knowledge, but may still have a high level of performance if the employee has high levels of procedural knowledge and motivation.
Regardless of the job, three determinants stand out as predictors of performance: (1) general mental ability (especially for jobs higher in complexity); (2) job experience (although there is a law of diminishing returns); and (3) the personality trait of conscientiousness (people who are dependable and achievement-oriented, who plan well). These determinants appear to influence performance largely through the acquisition and usage of job knowledge and the motivation to do well. Further, an expanding area of research in job performance determinants includes emotional intelligence.
Organizational citizenship behavior.
Organizational citizenship behaviors ("OCBs") are another form of productive behavior, having been shown to be beneficial to both organization and team effectiveness. Dennis Organ is often thought of as the father of OCB research and defines OCBs as "individual behavior that is discretionary, not directly or explicitly recognized by the formal reward system, and that in the aggregate promotes the effective functioning of the organization." Behaviors that qualify as OCBs can fall into one of the following five categories: altruism, courtesy, sportsmanship, conscientiousness, and civic virtue.
Researchers have adapted, elaborated, or otherwise changed Organ's (1988) five OCB categories, but they remain popular today. The categories and their descriptions are as follows:
OCBs are also categorized using other methods. For example, Williams and Anderson categorize OCBs by their intended target, separating them into those targeted at individuals ("OCBIs"), supervisors ("OCBSs"), and those targeted at the organization as a whole ("OCBOs"). Additionally, Vigoda-Gadot uses a sub-category of OCBs called CCBs, or "compulsory OCBs" which is used to describe OCBs that are done under the influence of coercive persuasion or peer pressure rather than out of good will. This theory stems from debates concerning the reasons for conducting OCBs and whether or not they are truly voluntary in nature.
Jex & Britt offer three explanations as to why employees engage in organizational citizenship behavior. One relates to positive affect; for example, an overall positive mood tends to change the frequency of helping behavior to a higher rate. This theory stems from a history of numerous studies indicating that positive mood increases the frequency of helping and prosocial behaviors.
A second explanation, which stems from equity theory, is that employees reciprocate fair treatment that they received from the organization. Equity theory researchers found that certain forms of fairness or justice predict OCB better than others. For example, Jex & Britt mention research that indicates that interactional justice is a better predictor than procedural justice, which is in turn a better predictor than distributive justice.
A third explanation Jex & Britt offer is that, on the one hand, some employees hold personal values that tend to skew their behavior positively to participate in organizational citizenship activities. On the other hand, Jex & Britt's interpretation of research results suggest that other employees will tend to perform organizational citizenship behavior merely to influence how they are viewed within the organization, not because it reflects their personally held values. While these behaviors are not formally part of the job description, performing them can certainly influence performance appraisals. In contrast to this view, some I–O psychologists believe that employees engage in OCBs as a form of "impression management," a term coined by Erving Goffman in his 1959 book "The Presentation of Self in Everyday Life". Goffman defines impression management as "the way in which the individual ... presents himself and his activity to others, the ways in which he guides and controls the impression they form of him, and the kinds of things he may and may not do while sustaining his performance before them." Researchers such as Bolino have hypothesized that the act of performing OCBs is not done out of goodwill, positive affect, etc., but instead as a way of being noticed by superiors and looking good in the eyes of others. The key difference between this view and those mentioned by Jex & Britt is that the intended beneficiary of the behavior is the individual who engages in it, rather than another individual, the organization, or the supervisor.
With this research on why employees engage in OCBs comes the debate among I–O psychologists about the voluntary or involuntary nature of engaging in OCBs. Many researchers, including the "father of OCB research," Dennis Organ have consistently portrayed OCBs as voluntary behaviors done at the discretion of the individual. However, more recently researchers have brought attention to potential underlying causes of OCBs, including social pressure, coercion, and other external forces. For example, Eran Vigoda-Gadot suggests that some, but not all, OCBs may be performed voluntarily out of goodwill, but many may be more involuntary in nature and "may arise from coercive managerial strategies or coercive social pressure by powerful peers." As mentioned previously, Vigoda-Gadot categorizes these behaviors in a separate category of OCBs as "compulsory OCBs" or CCBs, which he suggests are a form of "abusive supervision" and will result in poorer organizational performance, similar to what has been seen in other research on abusive supervision and coercive persuasion.
Innovation.
Industrial and Organizational Psychologists consider innovation, more often than not, a variable of less importance and often a counter-productive one to include in conducting job performance appraisals when irrelevant to the major job functions for which a given job exists. Nonetheless, Industrial and Organizational Psychologists see the value of that variable where its consideration would, were its reliability and validity questioned, achieve a statistically significant probability that its results are not due to chance, and that it can be replicated reliably with a statistically significant ratio of reliability, and that were a court to raise a question on its reliability and validity testing, the Industrial and Organizational Psychologist behind its use would be able to defend it before a court of justice with the belief that it will stand before such a court as reliable, and valid.
With the above in mind, innovation is often considered a form of productive behavior that employees exhibit when they come up with novel ideas that further the goals of the organization. This section will discuss three topics of interest: research on innovation; characteristics of an individual that may predict innovation; and how organizations may be structured to promote innovation. According to Jex & Britt, individual and organization research can be divided into four unique research focuses. 
As indicated above, the first focus looks specifically to find certain attributes of an individual that may lead to innovation, therefore, one must ask, "Are there quantifiable predictors that an individual will be innovative?" Research indicates if various skills, knowledge, and abilities are present then an individual will be more apt to innovation. These qualities are generally linked to creativity. A brief overview of these characteristics are listed below.
In addition to the role and characteristics of the individual, one must consider what it is that may be done on an organizational level to develop and reward innovation. A study by Damanpour identified four specific characteristics that may predict innovation within an organization. They are the following ones:
Additionally, organizations could use and institutionalize many participatory system-processes, which could breed innovation in the workplace. Some of these items include providing creativity training, having leaders encourage and model innovation, allowing employees to question current procedures and rules, seeing that the implementation of innovations had real consequences, documenting innovations in a professional manner, allowing employees to have autonomy and freedom in their job roles, reducing the number of obstacles that may be in the way of innovation, and giving employees access to resources (whether these are monetary, informational, or access to key people inside or outside of the organization).
According to the American Productivity & Quality Center ("APQC") there are basic principles an organization can develop to encourage and reward innovation.
innovation.
In discussing innovation for a Best-Practice report, APQC Knowledge Management expert, Kimberly Lopez, stated, "It requires a blending of creativity within business processes to ensure good ideas become of value to the company ... Supporting a creative environment requires innovation to be recognized, nurtured, and rewarded."
Counterproductive work behavior.
Counterproductive work behavior (CWB) can be defined as employee behavior that goes against the goals of an organization. These behaviors can be intentional or unintentional and result from a wide range of underlying causes and motivations. Some CWBs have instrumental motivations (e.g., theft). It has been proposed that a person-by-environment interaction can be utilized to explain a variety of counterproductive behaviors (Fox and Spector, 1999). For instance, an employee who sabotages another employee's work may do so because of lax supervision (environment) and underlying psychopathology (person) that work in concert to result in the counterproductive behavior. There is evidence that an emotional response (e.g., anger) to job stress (e.g., unfair treatment) can motivate CWBs.
The forms of counterproductive behavior with the most empirical examination are ineffective job performance, absenteeism, job turnover, and accidents. Less common but potentially more detrimental forms of counterproductive behavior have also been investigated including violence and sexual harassment.
Leadership.
In I–O psychology, leadership can be defined as a process of influencing others to agree on a shared purpose, and to work towards shared objectives. A distinction should be made between leadership and management. Managers process administrative tasks and organize work environments. Although leaders may be required to undertake managerial duties as well, leaders typically focus on inspiring followers and creating a shared organizational culture and values. Managers deal with complexity, while leaders deal with initiating and adapting to change. Managers undertake the tasks of planning, budgeting, organizing, staffing, controlling and problem solving. In contrast, leaders undertake the tasks of setting a direction or vision, aligning people to shared goals, communicating, and motivating.
Approaches to studying leadership in I–O psychology can be broadly classified into three categories: Leader-focused approaches, Contingency-focused approaches, and Follower-focused approaches.
Leader-focused approaches.
Leader-focused approaches look to organizational leaders to determine the characteristics of effective leadership. According to the trait approach, more effective leaders possess certain traits that less effective leaders lack. More recently, this approach is being used to predict leader emergence. The following traits have been identified as those that predict leader emergence when there is no formal leader: high intelligence, high needs for dominance, high self-motivation, and socially perceptive. Another leader-focused approached is the behavioral approach which focuses on the behaviors that distinguish effective from ineffective leaders. There are two categories of leadership behaviors: (1) consideration; and (2) initiating structure. Behaviors associated with the category of consideration include showing subordinates they are valued and that the leader cares about them. An example of a consideration behavior is showing compassion when problems arise in or out of the office. Behaviors associated with the category of initiating structure include facilitating the task performance of groups. One example of an initiating structure behavior is meeting one-on-one with subordinates to explain expectations and goals. The final leader-focused approach is power and influence. To be most effective a leader should be able to influence others to behave in ways that are in line with the organization's mission and goals. How influential a leader can be depends on their social power or their potential to influence their subordinates. There are six bases of power: coercive power, reward power, legitimate power, expert power, referent power, and informational power. A leader can use several different tactics to influence others within an organization. These common tactics include: rational persuasion, inspirational appeal, consultation, ingratiation, exchange, personal appeal, coalition, legitimating, and pressure.
Contingency-focused approaches.
Of the 3 approaches to leadership, contingency-focused approaches have been the most prevalent over the past 30 years. Contingency-focused theories base a leader's effectiveness on their ability to assess a situation and adapt their behavior accordingly. These theories assume that an effective leader can accurately "read" a situation and skillfully employ a leadership style that meets the needs of the individuals involved and the task at hand. A brief introduction to the most prominent contingency-focused theories will follow.
Fiedler's Contingency Theory holds that a leader's effectiveness depends on the interaction between their characteristics and the characteristics of the situation. Path–Goal Theory asserts that the role of the leader is to help his or her subordinates achieve their goals. To effectively do this, leaders must skillfully select from four different leadership styles to meet the situational factors. The situational factors are a product of the characteristics of subordinates and the characteristics of the environment. The Leader-Member Exchange (LMX) Model focuses on how leader–subordinate relationships develop. Generally speaking, when a subordinate performs well or when there are positive exchanges between a leader and a subordinate, their relationship is strengthened, performance and job satisfaction are enhanced, and the subordinate will feel more commitment to the leader and the organization as a whole. Vroom-Yetton-Jago Model focuses on decision making with respect to a "feasibility set" which is composed of the situational attributes.
In addition to the contingency-focused approaches mentioned, there has been a high degree of interest paid to three novel approaches that have recently emerged. The first is transformational leadership, which posits that there are certain leadership traits that inspire subordinates to perform beyond their capabilities. The second is transactional leadership, which is most concerned with keeping subordinates in-line with deadlines and organizational policy. This type of leader fills more of a managerial role and lacks qualities necessary to inspire subordinates and induce meaningful change. And the third is authentic leadership which is centered around empathy and a leader's values or character. If the leader understands their followers, they can inspire subordinates by cultivating a personal connection and leading them to share in the vision and goals of the team. Although there has been a limited amount of research conducted on these theories, they are sure to receive continued attention as the field of I–O psychology matures.
Follower-focused approaches.
Follower-focused approaches look at the processes by which leaders motivate followers, and lead teams to achieve shared goals. Understandably, the area of leadership motivation draws heavily from the abundant research literature in the domain of motivation in I–O psychology. Because leaders are held responsible for their followers' ability to achieve the organization's goals, their ability to motivate their followers is a critical factor of leadership effectiveness. Similarly, the area of team leadership draws heavily from the research in teams and team effectiveness in I–O psychology. Because organizational employees are frequently structured in the form of teams, leaders need to be aware of the potential benefits and pitfalls of working in teams, how teams develop, how to satisfy team members' needs, and ultimately how to bring about team effectiveness and performance. An emerging area of research in the area of team leadership is in leading virtual teams, where people in the team are geographically-distributed across various distances and sometimes even countries. While technological advances have enabled the leadership process to take place in such virtual contexts, they present new challenges for leaders as well, such as the need to use technology to build relationships with followers, and influencing followers when faced with limited (or no) face-to-face interaction.
Organizational change/development.
Organizational development.
Industrial-organizational psychologists have displayed a great deal of consideration for the problems of total organizational change and systematic ways to bring about planned change. This effort, called organizational development (OD), involves techniques such as:
Within the survey feedback technique, surveys after being answered by employees periodically, are assessed for their emotions and attitudes which are then communicated to various members within the organization. The team building technique was created due to realization that most tasks within the organization are completed by small groups and/or teams. In order to further enhance a team's or group's morale and problem-solving skills, OD consultants (called change agents) help the groups to build their self-confidence, group cohesiveness, and working effectiveness. A change agent's impartiality, gives the managers within the organization a new outlook of the organization's structure, functions, and culture. A change agent's first task is diagnosis, where questionnaires and interviews are used to assess the problems and needs of the organization. Once analyzed, the strengths and weaknesses of the organization are presented and used to create strategies for solving problems and coping with future changes.(pp216–217)
Flexibility and adaptability are some strengths of the OD process, as it possesses the ability to conform to the needs of the situation. Regardless of the specific techniques applied, the OD process helps to free the typical bureaucratic organization from its rigidity and formality, hereby allowing more responsiveness and open participation. Public and private organizations both have employed OD techniques, despite their varied results in research conducted. However, the use of the techniques are justified by the significant increases in productivity that was proven by various studies.(p217)
Relation to organizational behavior.
The i/o psychology and organizational behavior have manifested some overlap. The overlap has led to some confusion regarding how the two disciplines differ.
Training and outlook.
Graduate programs.
Schultz and Schultz (2010) states that modern I–O Psychology is a complex and intricate position. It requires intense university training, and hands on experience. Individuals who choose I–O psychology as a profession should also be aware that they will be constantly studying to learn about new developments that may emerge. The minimum requirement for working as an I–O psychologist is a Master's Degree. Normally, this degree requires 42 semester hours and takes about 2–3 years to complete. Most Master's Degree students work, either full-time or part-time, while studying to become an I–O psychologist. Of all the degrees granted in I–O psychology, each year approximately two thirds are at the master's level.(p18)
A comprehensive list of US and Canadian master's and doctoral programs can be found at the web site of the Society for Industrial and Organizational Psychology (SIOP). Some helpful ways to learn about graduate programs include visiting the web sites on the SIOP list and speaking to I–O faculty at the institutions listed. Admission into I–O psychology PhD programs is highly competitive given that many programs accept a small number of applicants every year.
There are graduate degree programs in I–O psychology outside of the US and Canada. The SIOP web site also provides a comprehensive list of I–O programs in many other countries.
Job outlook.
According to the United States Department of Labor's Bureau of Labor Statistics, I-O psychology is the fastest growing occupation in the United States, based on projections between 2012 and 2022.
According to recent salary and employment surveys conducted by SIOP, the median salary for a PhD in I–O psychology was $98,000; for a master's level I–O psychologist was $72,000. The highest paid PhD I–O psychologists in private industry worked in pharmaceuticals and averaged approximately $151,000 per year; the median salary for self-employed consultants was $150,000; those employed in retail, energy, and manufacturing followed closely behind, averaging approximately $133,000. The lowest earners were found in state and local government positions, averaging approximately $77,000. I–O psychologists whose primary responsibility is teaching at private and public colleges and universities often earn additional income from consulting with government and industry.
Pros and cons of an industrial and organizational psychology career.
Pros of a Career in I–O Psychology:
Cons of a Career in I–O Psychology:
Ethics.
In the consulting field, it is important for the consultant to maintain high ethical standards in all aspects of relationships: consultant to client, consultant to consultant, and client to consultant. After all, all decisions made and actions taken by the consultant will reflect what kind of consultant he or she is. Although ethical situations can be more intricate in the business world, American Psychology Association (APA)’s Ethical Principles of Psychologists and Code of Conduct can be applied to I–O consultants as well. For example, the consultant should only accept projects for which he or she is qualified; the consultant should also avoid all conflicts of interest and being in multiple relationships with those he or she is working with. On the other hand, some might disagree that it is the consultant’s responsibility to actively promote the application of moral and ethical standards in the consultation and examine ethical issues in organizational decisions and policies. It is an ongoing controversial issue in the consulting field. In addition, as more and more organizations are becoming global, it is imperative for consultants working abroad to quickly become aware of rules, regulations, and cultures of the organizations and countries they are in as well as not to ignore ethical standards and codes just because they are abroad.
Industrial/organizational consultancy.
Definition.
An industrial/organizational (I–O) consultant helps clients and organizations improve productivity and create an optimal working environment through human capital consulting and strategies. Areas of consulting include but are not limited to selection and recruiting, training, leadership, and development, compensation and benefits, employee relations, performance management, succession planning, and executive coaching.
Types.
Consultants can be categorized as internal or external to an organization. An internal consultant is someone who is working specifically for an organization that he or she is a part of whereas an external consultant can be either a sole proprietor or an employee of a consulting firm who is hired by another organization on a project basis or for a certain period of time. There are different types of I–O consultants:
Services offered.
Kurpius (1978; as cited in Hedge & Borman, 2009) gave four general types of consultation:
Consultants offer these consulting services to all kinds of organizations, such as profit and nonprofit sectors, public and private sectors, and a government organization.
Pros and cons.
Like any other careers, there are many benefits and downsides of consulting. Some advantages are substantial material rewards, trust and respect from clients, and personal satisfaction. Some disadvantages are traveling (the number one complaint of all I/O consultants), uncertainty in business especially for external consultants, and marginality which is not belonging to any group or organization that the consultant works for.
Competencies.
There are many different sets of competencies for different specializations within I–O psychology and I–O psychologists are versatile behavioral scientists. For example, an I–O psychologist specializing in selection and recruiting should have expertise in finding the best talent for the organization and getting everyone on board while he or she might not need to know much about executive coaching. Some consultants tend to specialize in specific areas of consulting whereas others tend to generalize their areas of expertise. However, Cummings and Worley (2009) claimed that there are basic skills and knowledge, which most consultants agree, needed to be effective consultants: 
Stages.
Block (2011) identified the following five stages of consulting.
Entry and contracting.
This stage is where the consultant makes the initial contact with the client about the project, and it includes setting up the first meeting, exploring more about the project and the client, roles, responsibilities, and expectations about the consultant, the client, and the project, and whether the consultant’s expertise and experience fit with what the client wants out of the project. This is the most important part of the consulting, and most consultants agree that most mistakes in the project can essentially be traced back to the faulty contracting stage.
Discovery and diagnosis.
This stage is where the consultant makes his or her own judgment about the problem identified by the client and about the project. Sometimes, the problem presented by the client is not the actual problem but a symptom of a true cause. Then, the consultant collects more information about the situation.
Analysis and planning.
This stage is where the consultant analyzes the data and presents the results to the client. The consultant needs to reduce a large amount of data into a manageable size and present them to the client in a clear and simple way. After presenting the results, the consultant helps the client make plans and goals for actions to be taken as a next step to solve the identified problem.
Engagement and implementation.
This stage sometimes falls entirely on the client or the organization, and the consultant’s job might be completed at the end of third stage. However, it is important for the consultant to be present at the fourth stage since without implementing the changes suggested by the consultant, the problem is not likely to be solved. Moreover, despite how good the consultant’s advice might be, employees are actually the ones who need to live the changes. So, in this fourth stage, the consultant needs to get everyone on board with the changes and help implement the changes.
Extension or termination.
This final stage is where the consultant and the client evaluate the project, and it is usually the most neglected yet important stage. Then, the project is completed or extended depending on the client’s needs.
Future trends.
Teachout and Vequist (2008) identified driving forces affecting future trends in the business consulting: 
They also discussed three trends in the field as a result of these forces – people, process, and technology.
Human capital or people.
In terms of human capital or people consulting, there are major forces for future trends:
As a result, trends, such as major talent management, selection and recruiting, workplace education and training, and planning for next generation, have emerged. In addition, change management also becomes important in organizations in order to innovate and implement new technology, tools, and systems to cope with changes in the business.
Process.
In terms of process consulting, because of an increase in competition, it becomes important to identify and improve key processes that meet customer values and demands as well as that are faster and cheaper.
Technology.
In terms of technology consulting, there is an increased need to automate processes or data so that employees can focus on actually doing work and focusing on business rather than doing the manual labor. The consultant can add value to these technologies by providing training, communication plan, and change management as well as to incorporate these technologies into organizational culture. So, regardless of how advanced technology is, consultants are still needed in making sure that these advanced technologies have positive effects on employees and organizations in both technical and social aspects.
Aside from technology consulting, there is a future trend for the interaction that comes with technology. This includes, human-technology interaction, technology-technology interaction, and human-human interaction through technology. Due to the evolving technology throughout the globe, communication and relationships in the workplace are dramatically changing. Technology consultants help organizations cope with the interjection of technology in the work place. However, their job description will eventually expand to include proper technology communication styles and when technology does or does not have a place in an interaction. This delicate subject alters the meanings and interpretations behind social interactions and creating concise guidelines to technological interactions is essential.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="15451" url="http://en.wikipedia.org/wiki?curid=15451" title="International Council of Unitarians and Universalists">
International Council of Unitarians and Universalists

The International Council of Unitarians and Universalists (ICUU) is an umbrella organization founded in 1995 bringing together many Unitarian, Universalist and Unitarian Universalist organizations. 
The size of the affiliated organizations varies widely. Some groups represent only a few hundred people; while the largest, the Unitarian Universalist Association, has over 160,000 members and is larger than all the other groups put together.
History.
The original initiative for its establishment was contained in a resolution of the General Assembly of Unitarian and Free Christian Churches (British Unitarians) in 1987. This led to the establishment of the Advocates for the Establishment of an International Organization of Unitarians (AEIOU), which worked towards creating the council. However, the General Assembly resolution provided no funding.
The Unitarian Universalist Association (UUA) became particularly interested in the establishment of a council when it had to deal with an increasing number of applications for membership from congregations outside North America. It had already granted membership to congregations in Adelaide, Auckland, the Philippines and Pakistan, and congregations in Sydney, Russia and Spain had applied for membership. Rather than admit congregations from all over the world, the UUA hoped that they would join a world council instead. The UUA thus became willing to provide funding for the council's establishment.
As a result, the council was finally established at a meeting in Essex, Massachusetts, United States on 23–26 March 1995. The Rev. David Usher, a British Unitarian minister of Australian origin who had proposed the original motion eight years previously, became the ICUU's first President.
Principles and purposes.
The Preamble to the Constitution of the International Council of Unitarians and Universalists reads:
We, the member groups of the International Council of Unitarians and Universalists, affirming our belief in religious community based on:
declare our purposes to be:
Members.
Reorganizing.
Spain and the Polish Unitarians have reported a need for a period of reorganization, and that at this time they are unable to maintain the level of activity needed to be full Council members, be it moved that membership of these groups be suspended. This action is taken with regret and the ICUU looks forward to welcoming Spain and Poland back into membership at the earliest possible date.
The Unitarian Universalists of Russia were a founding member of the ICUU. Its membership in the Council was officially dropped in 2007 because of persistent lack of activity.
Provisional members.
Churches and religious associations which have expressed their will to become members of the Council may be admitted as "Provisional Members" for a period of time (generally two or four years), until the Council decides that they have shown their organizational stability, affinity with the ICUU principles and commitment to deserve becoming Full Members of the Council. Provisional Members are invited to Council meetings through a delegate but cannot vote.
The following organizations have been accepted by the ICUU Executive Committee but they have not yet been ratified by the Council Meeting:
Emerging groups.
According to the , Emerging Groups are "applicants that are deemed to be reasonable prospects for membership, but do not fulfil the conditions of either Provisional membership or Full Membership". These groups may be designated as Emerging Groups by the Executive Committee upon its sole discretion. Emerging Groups may be invited as observers to General Meetings.
The current list of Emerging Groups after the last meeting of the Executive Committee (London, 22–25 November 2008) is as follows:
Associates.
Organizations with beliefs and purposes closely akin to those of ICUU but which by nature of their constitution are not eligible for full membership or which do not wish to become full members now or in the foreseeable future, may become Associates of the ICUU. The application must be approved by the ICUU Council Meeting.
The current list of Associates is as follows:
The following organizations have been accepted by the ICUU Executive Committee but they have not yet been ratified by the Council Meeting:

</doc>
<doc id="15454" url="http://en.wikipedia.org/wiki?curid=15454" title="Itanium">
Itanium

Itanium ( ) is a family of 64-bit Intel microprocessors that implement the Intel Itanium architecture (formerly called IA-64). Intel markets the processors for enterprise servers and high-performance computing systems. The Itanium architecture originated at Hewlett-Packard (HP), and was later jointly developed by HP and Intel.
Itanium-based systems have been produced by HP (the HP Integrity Servers line) and several other manufacturers. s of 2008[ [update]], Itanium was the fourth-most deployed microprocessor architecture for enterprise-class systems, behind x86-64, Power Architecture, and SPARC.
The most recent processor, was released on November 8, 2012.
Market reception.
High-end server market.
When first released in 2001, Itanium's performance, compared to better-established RISC and CISC processors, was disappointing. Emulation to run existing x86 applications and operating systems was particularly poor, with one benchmark in 2001 reporting that it was equivalent at best to a 100 MHz Pentium in this mode (1.1 GHz Pentiums were on the market at that time).
Itanium failed to make significant inroads against IA-32 or RISC, and then suffered from the successful introduction of x86-64 based systems into the high-end server market, systems which were more compatible with the older x86 applications. Journalist John C. Dvorak, commenting in 2009 on the history of the Itanium processor, said "This continues to be one of the great fiascos of the last 50 years" in an article titled "How the Itanium Killed the Computer Industry". Tech columnist Ashlee Vance commented that the delays and underperformance "turned the product into a joke in the chip industry." In an interview, Donald Knuth said "The Itanium approach...was supposed to be so terrific—until it turned out that the wished-for compilers were basically impossible to write."
Both Red Hat and Microsoft announced plans to drop Itanium support in their operating systems due to lack of market interest; however, other Linux distributions such as Gentoo and Debian remain available for Itanium. On March 22, 2011, Oracle announced discontinuation of development on Itanium, although its technical support for its existing products would continue. In October 2013, Oracle committed to release Oracle Database 12.1.0.1.0 on HP-UX Itanium 11.31 by early 2014.
A former Intel official reported that the Itanium business had become profitable for Intel in late 2009. By 2009, the chip was almost entirely deployed on servers made by HP, which had over 95% of the Itanium server market share, making the main operating system for Itanium HP-UX. On March 22, 2011 Intel reaffirmed its commitment to Itanium with multiple generations of chips in development and on schedule.
Other markets.
Although Itanium did attain limited success in the niche market of high-end computing, Intel had originally hoped it would find broader acceptance as a replacement for the original x86 architecture.
AMD chose a different direction, designing the less radical x86-64, a 64-bit extension to the existing x86 architecture, which Microsoft then supported, forcing Intel to introduce the same extensions in its own x86-based processors. These designs can run existing 32-bit applications at native hardware speed, while offering support for 64-bit memory addressing and other enhancements to new applications. This architecture has now become the predominant 64-bit architecture in the desktop and portable market. Although some Itanium-based workstations were initially introduced by companies such as SGI, they are no longer available.
History.
Development: 1989–2000.
In 1989, HP determined that Reduced Instruction Set Computing (RISC) architectures were approaching a processing limit at one instruction per cycle. HP researchers investigated a new architecture, later named Explicitly Parallel Instruction Computing (EPIC), that allows the processor to execute multiple instructions in each clock cycle. EPIC implements a form of Very Long Instruction Word (VLIW) architecture, in which a single instruction word contains multiple instructions. With EPIC, the compiler determines in advance which instructions can be executed at the same time, so the microprocessor simply executes the instructions and does not need elaborate mechanisms to determine which instructions to execute in parallel.
The goal of this approach is twofold: to enable deeper inspection of the code at compile time to identify additional opportunities for parallel execution, and to simplify processor design and reduce energy consumption by eliminating the need for runtime scheduling circuitry.
HP believed that it was no longer cost-effective for individual enterprise systems companies such as itself to develop proprietary microprocessors, so it partnered with Intel in 1994 to develop the IA-64 architecture, derived from EPIC. Intel was willing to undertake a very large development effort on IA-64 in the expectation that the resulting microprocessor would be used by the majority of enterprise systems manufacturers. HP and Intel initiated a large joint development effort with a goal of delivering the first product, Merced, in 1998.
During development, Intel, HP, and industry analysts predicted that IA-64 would dominate in servers, workstations, and high-end desktops, and eventually supplant RISC and Complex Instruction Set Computing (CISC) architectures for all general-purpose applications.
Compaq and Silicon Graphics decided to abandon further development of the Alpha and MIPS architectures respectively in favor of migrating to IA-64.
Several groups developed operating systems for the architecture, including Microsoft Windows, OpenVMS, Linux, and UNIX variants such as HP-UX, Solaris,
Tru64 UNIX, and Monterey/64
(the last three were canceled before reaching the market). By 1997, it was apparent that the IA-64 architecture and the compiler were much more difficult to implement than originally thought, and the delivery of Merced began slipping.
Technical difficulties included the very high transistor counts needed to support the wide instruction words and the large caches. There were also structural problems within the project, as the two parts of the joint team used different methodologies and had slightly different priorities. Since Merced was the first EPIC processor, the development effort encountered more unanticipated problems than the team was accustomed to. In addition, the EPIC concept depends on compiler capabilities that had never been implemented before, so more research was needed.
Intel announced the official name of the processor, "Itanium", on October 4, 1999.
Within hours, the name "Itanic" had been coined on a Usenet newsgroup, a reference to "Titanic", the "unsinkable" ocean liner that sank in 1912.
"Itanic" has since often been used by "The Register",
and others,
to imply that the multibillion dollar investment in Itanium—and the early hype associated with it—would be followed by its relatively quick demise.
Itanium (Merced): 2001.
By the time Itanium was released in June 2001, its performance was not superior to competing RISC and CISC processors.
Itanium competed at the low-end (primarily 4-CPU and smaller systems) with servers based on x86 processors, and at the high end with IBM's POWER architecture and Sun Microsystems' SPARC architecture. Intel repositioned Itanium to focus on high-end business and HPC computing, attempting to duplicate x86's successful "horizontal" market (i.e., single architecture, multiple systems vendors). The success of this initial processor version was limited to replacing PA-RISC in HP systems, Alpha in Compaq systems and MIPS in SGI systems, though IBM also delivered a supercomputer based on this processor.
POWER and SPARC remained strong, while the 32-bit x86 architecture continued to grow into the enterprise space, building on economies of scale fueled by its enormous installed base.
Only a few thousand systems using the original "Merced" Itanium processor were sold, due to relatively poor performance, high cost and limited software availability.
Recognizing that the lack of software could be a serious problem for the future, Intel made thousands of these early systems available to independent software vendors (ISVs) to stimulate development. HP and Intel brought the next-generation Itanium 2 processor to market a year later.
Itanium 2: 2002–2010.
The Itanium 2 processor was released in 2002, and was marketed for enterprise servers rather than for the whole gamut of high-end computing. The first Itanium 2, code-named "McKinley", was jointly developed by HP and Intel. It relieved many of the performance problems of the original Itanium processor, which were mostly caused by an inefficient memory subsystem. "McKinley" contained 221 million transistors (of which 25 million were for logic), measured 19.5 mm by 21.6 mm (421 mm2) and was fabricated in a 180 nm, bulk CMOS process with six layers of aluminium metallization.
In 2003, AMD released the Opteron, which implemented its own 64-bit architecture (AMD64). Opteron gained rapid acceptance in the enterprise server space because it provided an easy upgrade from x86. Intel responded by implementing x86-64 in its Xeon microprocessors in 2004.
Intel released a new Itanium 2 family member, codenamed "Madison", in 2003. Madison used a 130 nm process and was the basis of all new Itanium processors until Montecito was released in June 2006.
In March 2005, Intel announced that it was working on a new Itanium processor, codenamed "Tukwila", to be released in 2007. Tukwila would have four processor cores and would replace the Itanium bus with a new Common System Interface, which would also be used by a new Xeon processor.
Later that year, Intel revised Tukwila's delivery date to late 2008.
In November 2005, the major Itanium server manufacturers joined with Intel and a number of software vendors to form the Itanium Solutions Alliance to promote the architecture and accelerate software porting.
The Alliance announced that its members would invest $10 billion in Itanium solutions by the end of the decade.
In 2006, Intel delivered "Montecito" (marketed as the Itanium 2 9000 series), a dual-core processor that roughly doubled performance and decreased energy consumption by about 20 percent.
Intel released the Itanium 2 9100 series, codenamed "Montvale", in November 2007.
In May 2009 the schedule for Tukwila, its follow-on, was revised again, with release to OEMs planned for the first quarter of 2010.
Itanium 9300 (Tukwila): 2010.
The Itanium 9300 series processor, codenamed "Tukwila", was released on 8 February 2010 with greater performance and memory capacity.
The device uses a 65 nm process, includes two to four cores, up to 24 MB on-die caches, Hyper-Threading technology and integrated memory controllers. It implements double-device data correction, which helps to fix memory errors. Tukwila also implements Intel QuickPath Interconnect (QPI) to replace the Itanium bus-based architecture. It has a peak interprocessor bandwidth of 96 GB/s and a peak memory bandwidth of 34 GB/s. With QuickPath, the processor has integrated memory controllers and interfaces the memory directly, using QPI interfaces to directly connect to other processors and I/O hubs. QuickPath is also used on Intel processors using the "Nehalem" microarchitecture, making it probable that Tukwila and Nehalem will be able to use the same chipsets.
Tukwila incorporates four memory controllers, each of which supports multiple DDR3 DIMMs via a separate memory controller,
much like the Nehalem-based Xeon processor code-named "Beckton".
Itanium 9500 (Poulson): 2012.
The Itanium 9500 series processor, codenamed "Poulson", is the follow-on processor to Tukwila and was released on November 8, 2012.
According to Intel, it skips the 45 nm process technology and uses a 32 nm process technology; it features eight cores, has a 12-wide issue architecture, multithreading enhancements, and new instructions to take advantage of parallelism, especially in virtualization.
The Poulson L3 cache size is 32 MB. L2 cache size is 6 MB, 512 I KB, 256 D KB per core. Die size is 544 mm², less than its predecessor Tukwila (698.75 mm²).
At ISSCC 2011, Intel presented a paper called, "A 32nm 3.1 Billion Transistor 12-Wide-Issue Itanium Processor for Mission Critical Servers."
Given Intel's history of disclosing details about Itanium microprocessors at ISSCC, this paper most likely refers to Poulson. Analyst David Kanter speculates that Poulson will use a new microarchitecture, with a more advanced form of multi-threading that uses as many as two threads, to improve performance for single threaded and multi-threaded workloads.
Some new information was released at Hot Chips conference.
New information presents improvements in multithreading, resilency improvements (Instruction Replay RAS) and few new instructions (thread priority, integer instruction, cache prefetching, data access hints).
In Intel's Product Change Notification (PCN) 111456-01, it listed 4 models of Itanium 9500 series CPU, which was later removed in a revised document. The parts were later listed in Intel's Material Declaration Data Sheets (MDDS) database. Intel later posted Itanium 9500 reference manual.
The models are:
Market share.
In comparison with its Xeon family of server processors, Itanium has never been a high-volume product for Intel. Intel does not release production numbers. One industry analyst estimated that the production rate was 200,000 processors per year in 2007.
According to Gartner Inc., the total number of Itanium servers (not processors) sold by all vendors in 2007 was about 55,000. (It is unclear whether clustered servers counted as a single server or not.) This compares with 417,000 RISC servers (spread across all RISC vendors) and 8.4 million x86 servers. IDC reports that a total of 184,000 Itanium-based systems were sold from 2001 through 2007. For the combined POWER/SPARC/Itanium systems market, IDC reports that POWER captured 42% of revenue and SPARC captured 32%, while Itanium-based system revenue reached 26% in the second quarter of 2008.
According to an IDC analyst, in 2007 HP accounted for perhaps 80% of Itanium systems revenue.
According to Gartner, in 2008 HP accounted for 95% of Itanium sales. HP's Itanium system sales were at an annual rate of $4.4Bn at the end of 2008, and declined to $3.5Bn by the end of 2009,
compared to a 35% decline in UNIX system revenue for Sun and an 11% drop for IBM, with an x86-64 server revenue increase of 14% during this period.
In Dec 2012, IDC released a research report stating that Itanium server shipments would remain flat through 2016, with annual shipment of 26,000 systems (a decline of over 50% compared to shipments in 2008).
Hardware support.
Systems.
s of 2012[ [update]] only a few manufacturers offer Itanium systems, including HP, Bull, NEC, Inspur and Huawei. In addition, Intel offers a chassis that can be used by system integrators to build Itanium systems.
HP, the only one of the industry's top four server manufacturers to offer Itanium-based systems today, manufactures at least 80% of all Itanium systems. HP sold 7200 systems in the first quarter of 2006.
The bulk of systems sold are enterprise servers and machines for large-scale technical computing, with an average selling price per system in excess of US$200,000. A typical system uses eight or more Itanium processors.
Chipsets.
The Itanium bus interfaces to the rest of the system via a chipset. Enterprise server manufacturers differentiate their systems by designing and developing chipsets that interface the processor to memory, interconnections, and peripheral controllers. The chipset is the heart of the system-level architecture for each system design. Development of a chipset costs tens of millions of dollars and represents a major commitment to the use of the Itanium. IBM created a chipset in 2003, and Intel in 2002, but neither of them has developed chipsets to support newer technologies such as DDR2 or PCI Express.
Currently, modern chipsets for Itanium supporting such technologies are manufactured by HP, Fujitsu, SGI, NEC, and Hitachi.
The "Tukwila" Itanium processor model had been designed to share a common chipset with the Intel Xeon processor EX (Intel’s Xeon processor designed for four processor and larger servers). The goal is to streamline system development and reduce costs for server OEMs, many of whom develop both Itanium- and Xeon-based servers. However in 2013 this goal was pushed back to "evaluated for future implementation opportunities".
Software support.
Itanium is supported by the following operating systems:
Itanium was also supported by these operating systems:
Microsoft announced that Windows Server 2008 R2 would be the last version of Windows Server to support the Itanium, and that it would also discontinue development of the Itanium versions of Visual Studio and SQL Server.
Likewise, Red Hat Enterprise Linux 5 (first released in March 2007) was the last Itanium edition of Red Hat Enterprise Linux
and Canonical's Ubuntu 10.04 LTS (released in April 2010) was the last supported Ubuntu release on Itanium.
HP will not be supporting or certifying Linux on Itanium 9300 (Tukwila) servers.
In late September 2012, NEC announced a return from IA64 to the previous NOAH line of proprietary mainframe processors, now produced in a quad-core variant on 40 nm, called NOAH-6.
Oracle Corporation announced in March 2011 that it would drop development of application software for Itanium platforms, with the explanation that "Intel management made it clear that their strategic focus is on their x86 microprocessor and that Itanium was nearing the end of its life." However, a California state judge ruled that Oracle will have to continue supporting and releasing new versions of its software designed for Intel Itanium-based servers sold by Hewlett-Packard, after a settlement and release agreement between HP, Oracle and Mark Hurd had revealed that Oracle must continue to offer its product suite on HP's Itanium-based server platforms and does not confer on Oracle the discretion to decide whether to do so or not. Oracle's obligation to continue to offer its products on HP's Itanium-based server platforms lasts until such time as HP discontinues the sales of its Itanium-based servers. Oracle was ordered to port its products to HP's Itanium-based servers without charge to HP.
HP sells a virtualization technology for Itanium called Integrity Virtual Machines.
To allow more software to run on the Itanium, Intel supported the development of compilers optimized for the platform, especially its own suite of compilers.
Starting in November 2010, with the introduction of new product suites, the Intel Itanium Compilers were no longer bundled with the Intel x86 compilers in a single product. Intel offers Itanium tools and Intel x86 tools, including compilers, independently in different product bundles.
GCC,
Open64 and Microsoft Visual Studio 2005 (and later)
are also able to produce machine code for Itanium. According to the Itanium Solutions Alliance over 13,000 applications were available for Itanium based systems in early 2008,
though Sun has contested Itanium application counts in the past.
The ISA also supported Gelato, an Itanium HPC user group and developer community that ported and supported open source software for Itanium.
Emulation.
Emulation is a technique that allows a computer to execute binary code that was compiled for a different type of computer. Before IBM's acquisition of QuickTransit in 2009, application binary software for IRIX/MIPS and Solaris/SPARC could run via type of emulation called "dynamic binary translation" on Linux/Itanium. Similarly, HP implemented a method to execute PA-RISC/HP-UX on the Itanium/HP-UX via emulation, to simplify migration of its PA-RISC customers to the radically different Itanium instruction set. Itanium processors can also run the mainframe environment GCOS from Groupe Bull and several x86 operating systems via instruction set simulators.
Competition.
Itanium is aimed at the enterprise server and high-performance computing (HPC) markets. Other enterprise- and HPC-focused processor lines include Oracle Corporation's SPARC T5 and M6, Fujitsu's SPARC64 X+ and IBM's POWER8. Measured by quantity sold, Itanium's most serious competition comes from x86-64 processors including Intel's own Xeon line and AMD's Opteron line. Since 2009, most servers were being shipped with x86-64 processors.
In 2005, Itanium systems accounted for about 14% of HPC systems revenue, but the percentage has declined as the industry shifts to x86-64 clusters for this application.
An October 2008 paper by Gartner on the Tukwila processor stated that "...the future roadmap for Itanium looks as strong as that of any RISC peer like Power or SPARC."
Supercomputers and high-performance computing.
An Itanium-based computer first appeared on the list of the TOP500 supercomputers in November 2001. The best position ever achieved by an "Itanium 2" based system in the list was #2, achieved in June 2004, when Thunder (LLNL) entered the list with an Rmax of 19.94 Teraflops. In November 2004, Columbia entered the list at #2 with 51.8 Teraflops, and there was at least one Itanium-based computer in the top 10 from then until June 2007. The peak number of Itanium-based machines on the list occurred in the November 2004 list, at 84 systems (16.8%); by June 2012, this had dropped to one system (0.2%), and no Itanium system remained on the list in November 2012.
Processors.
Released processors.
The Itanium processors show a progression in capability. Merced was a proof of concept. McKinley dramatically improved the memory hierarchy and allowed Itanium to become reasonably competitive. Madison, with the shift to a 130 nm process, allowed for enough cache space to overcome the major performance bottlenecks. Montecito, with a 90 nm process, allowed for a dual-core implementation and a major improvement in performance per watt. Montvale added three new features: core-level lockstep, demand-based switching and front-side bus frequency of up to 667 MHz.
Future processors.
During the HP vs. Oracle support lawsuit, court documents unsealed by Santa Clara County Court judge revealed in 2008, Hewlett-Packard had paid Intel Corp. around $440 million to keep producing and updating Itanium microprocessors from 2009 to 2014. In 2010, the two companies signed another $250 million deal, which obliged Intel to continue making Itanium central processing units for HP's machines until 2017. Under the terms of the agreements, HP has to pay for chips it gets from Intel, while Intel launches Tukwila, Poulson, Kittson and Kittson+ chips in a bid to gradually boost performance of the platform.
Kittson.
"Kittson" was planned to follow Poulson in 2015. Kittson, like Poulson, will be manufactured using Intel's 32 nm process. Few other details are known beyond the existence of the codename and the binary and socket compatibility with Poulson and Tukwila, though moving to a common socket with x86 Xeon "will be evaluated for future implementation opportunities" after Kittson.

</doc>
<doc id="15459" url="http://en.wikipedia.org/wiki?curid=15459" title="International Statistical Classification of Diseases and Related Health Problems">
International Statistical Classification of Diseases and Related Health Problems

The International Statistical Classification of Diseases and Related Health Problems, usually called by the short-form name International Classification of Diseases (ICD), is the international "standard diagnostic tool for epidemiology, health management and clinical purposes". The ICD is maintained by the World Health Organization, the directing and coordinating authority for health within the United Nations System. The ICD is designed as a health care classification system, providing a system of diagnostic codes for classifying diseases, including nuanced classifications of a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. This system is designed to map health conditions to corresponding generic categories together with specific variations, assigning for these a designated code, up to six characters long. Thus, major categories are designed to include a set of similar diseases.
The International Classification of Diseases is published by the World Health Organization (WHO) and used worldwide for morbidity and mortality statistics, reimbursement systems, and automated decision support in health care. This system is designed to promote international comparability in the collection, processing, classification, and presentation of these statistics. As in the case of the analogous (but limited to mental and behavioral disorders) Diagnostic and Statistical Manual of Mental Disorders (DSM, currently in version 5), the ICD is a major project to statistically classify health disorders, and provide diagnostic assistance. The ICD is a core statistically-based classificatory diagnostic system for health care related issues of the WHO Family of International Classifications (WHO-FIC).
The ICD is revised periodically and is currently in its tenth revision. The ICD-10, as it is therefore known, was developed in 1992 to track health statistics. ICD-11 is planned for 2017. s of 2007[ [update]], development plans included using Web 2.0 principles to support detailed revision. Annual minor updates and triennial major updates are published by the WHO. The ICD is part of a "family" of guides that can be used to complement each other, including also the International Classification of Functioning, Disability and Health which focuses on the domains of functioning (disability) associated with health conditions, from both medical and social perspectives.
Historical synopsis.
In 1860, during the international statistical congress held in London, Florence Nightingale made a proposal that was to result in the development of the first model of systemic collection of hospital data. 
In 1893, a French physician, Jacques Bertillon, introduced the "Bertillon Classification of Causes of Death" at a congress of the International Statistical Institute in Chicago. A number of countries and cities adopted Dr. Bertillon’s system, which was based on the principle of
distinguishing between general diseases and those localized to a particular organ or anatomical site, as used by the City of Paris for classifying deaths. Subsequent revisions represented a synthesis of English, German and Swiss classifications, expanding from the original 44 titles to 161 titles. In 1898, the American Public Health Association (APHA) recommended that the registrars of Canada, Mexico, and the United States also adopt it. The APHA also recommended revising the system every ten-years to ensure the system remained current with medical practice advances. As a result, the first international conference to revise the International Classification of Causes of Death took place in 1900; with revisions occurring every ten-years thereafter. At that time the classification system was contained in one book, which included an Alphabetic Index as well as a Tabular List. The book was small compared with current coding texts.
The revisions that followed contained minor changes, until the sixth revision of the classification system. With the sixth revision, the classification system expanded to two volumes. The sixth revision included morbidity and mortality conditions, and its title was modified to reflect the changes: International Statistical Classification of Diseases, Injuries and Causes of Death (ICD). Prior to the sixth revision, responsibility for ICD revisions fell to the Mixed Commission, a group composed of representatives from the International Statistical Institute and the Health Organization of the League of Nations. In 1948, the World Health Organization (WHO) assumed responsibility for preparing and publishing the revisions to the ICD every ten-years. WHO sponsored the seventh and eighth revisions in 1957 and 1968, respectively. It later become clear that the established ten-year interval between revisions was too short.
The ICD is currently the most widely used statistical classification system for diseases in the world. International health statistics using this system are available at the ()
In addition, some countries—including Australia, Canada and the United States—have developed their own adaptations of ICD, with more procedure codes for classification of operative or diagnostic procedures.
Versions of ICD.
ICD-6.
The ICD-6, published in 1949, was the first to be shaped to become suitable for morbidity reporting. Accordingly the name changed from International List of Causes of Death to International Statistical Classification of Diseases. The combined code section for injuries and their associated accidents was split into two, a chapter for injuries, and a chapter for their external causes. With use for morbidity there was a need for coding mental conditions, and for the first time a section on mental disorders was added.
ICD-7.
The International Conference for the Seventh Revision of the International Classification of Diseases was held in Paris under the auspices of WHO in February 1955. In accordance with a recommendation of the WHO Expert Committee on Health Statistics, this revision was limited to essential changes and amendments of errors and inconsistencies.
ICD-8a.
The Eighth Revision Conference convened by WHO met in Geneva, from 6 to 12 July 1965. This revision was more radical than the Seventh but left unchanged the basic structure of the Classification and the general philosophy of classifying diseases, whenever possible, according to their etiology rather than a particular manifestation.
During the years that the Seventh and Eighth Revisions of the ICD were in force, the use of the ICD for indexing hospital medical records increased rapidly and some countries prepared national adaptations which provided the additional detail needed for this application of the ICD. 
In the USA, a group of consultants was asked to study the 8th revision of ICD (ICD-8a) for its applicability to various users in the United States. This group recommended that further detail be provided for coding hospital and morbidity data. The American Hospital Association’s “Advisory Committee to the Central Office on ICDA” developed the needed adaptation proposals, resulting in the publication of the International Classification of Diseases, Adapted (ICDA). In 1968, the United States Public Health Service published the International Classification of Diseases, Adapted, 8th Revision for use in the United States (ICDA-8a). Beginning in 1968, ICDA-8a served as the basis for coding diagnostic data for both official morbidity [and mortality] statistics in the United States.
ICD-9.
The International Conference for the Ninth Revision of the International Classification of Diseases, convened by WHO, met in Geneva from 30 September to 6 October 1975. In the discussions leading up to the conference, it had originally been intended that there should be little change other than updating of the classification. This was mainly because of the expense of adapting data processing systems each time the classification was revised.
There had been an enormous growth of interest in the ICD and ways had to be found of responding to this, partly by modifying the classification itself and partly by introducing special coding provisions. A number of representations were made by specialist bodies which had become interested in using the ICD for their own statistics. Some subject areas in the classification were regarded as inappropriately arranged and there was considerable pressure for more detail and for adaptation of the classification to make it more relevant for the evaluation of medical care, by classifying conditions to the chapters concerned with the part of the body affected rather than to those dealing with the underlying generalized disease.
At the other end of the scale, there were representations from countries and areas where a detailed and sophisticated classification was irrelevant, but which nevertheless needed a classification based on the ICD in order to assess their progress in health care and in the control of disease. A field test with a bi-axial classification approach - one axis for anatomy, another for etiology - showed the impracticability of such approach for routine use.
The final proposals presented to and accepted by the Conference in 1978 retained the basic structure of the ICD, although with much additional detail at the level of the four digit subcategories, and some optional five digit subdivisions. For the benefit of users not requiring such detail, care was taken to ensure that the categories at the three digit level were appropriate.
For the benefit of users wishing to produce statistics and indexes oriented towards medical care, the Ninth Revision included an optional alternative method of classifying diagnostic statements, including information about both an underlying general disease and a manifestation in a particular organ or site. This system became known as the dagger and asterisk system and is retained in the Tenth Revision. A number of other technical innovations were included in the Ninth Revision, aimed at increasing its flexibility for use in a variety of situations.
It was eventually replaced by ICD-10, the version currently in use by the WHO and most countries. Given the widespread expansion in the tenth revision, it is not possible to convert ICD-9 data sets directly into ICD-10 data sets, although some tools are available to help guide users.
Publication of ICD-9 without IP restrictions in a world with evolving electronic data systems led to a range of products based on ICD-9, such as MeDRA or the Read directory.
ICPM.
When ICD-9 was published by the World Health Organization (WHO), the International Classification of Procedures in Medicine (ICPM) was also developed (1975) and published (1978). The ICPM surgical procedures fascicle was originally created by the United States, based on its adaptations of ICD (called ICDA), which had contained a procedure classification since 1962. ICPM is published separately from the ICD disease classification as a series of supplementary documents called fascicles (bundles or groups of items). Each fascicle contains a classification of modes of laboratory, radiology, surgery, therapy, and other diagnostic procedures. Many countries have adapted and translated the ICPM in parts or as a whole and are using it with amendments since then.
ICD-9-CM.
"International Classification of Diseases, Clinical Modification" (ICD-9-CM) is an adaption created by the U.S. National Center for Health Statistics (NCHS) and used in assigning diagnostic and procedure codes associated with inpatient, outpatient, and physician office utilization in the United States. The ICD-9-CM is based on the ICD-9 but provides for additional morbidity detail. It is updated annually on October 1.
It consists of two or three volumes: 
The NCHS and the Centers for Medicare and Medicaid Services are the U.S. governmental agencies responsible for overseeing all changes and modifications to the ICD-9-CM.
ICD-10.
Work on ICD-10 began in 1983, and the new revision was endorsed by the Forty-third World Health Assembly in May 1990. The latest version came into use in WHO Member States starting in 1994. The classification system allows more than 155,000 different codes and permits tracking of many new diagnoses and procedures, a significant expansion on the 17,000 codes available in ICD-9.
Adoption was relatively swift in most of the world. Several materials are made available online by WHO to facilitate its use, including a manual, training guidelines, a browser, and files for download. Some countries have adapted the international standard, such as the "ICD-10-AM" published in Australia in 1998 (also used in New Zealand), and the "ICD-10-CA" introduced in Canada in 2000.
ICD-10-CM.
Adoption of ICD-10-CM has been slow in the United States. Since 1979, the USA had required ICD-9-CM codes for Medicare and Medicaid claims, and most of the rest of the American medical industry followed suit.
On 1 January 1999 the ICD-10 (without clinical extensions) was adopted for reporting mortality, but ICD-9-CM was still used for morbidity. Meanwhile, NCHS received permission from the WHO to create a clinical modification of the ICD-10, and has production of all these systems:
On August 21, 2008, the US Department of Health and Human Services (HHS) proposed new code sets to be used for reporting diagnoses and procedures on health care transactions. Under the proposal, the ICD-9-CM code sets would be replaced with the ICD-10-CM code sets, effective October 1, 2013. On April 17, 2012 the Department of Health and Human Services (HHS) published a proposed rule that would delay, from October 1, 2013 to October 1, 2014,the compliance date for the ICD-10-CM and PCS. Once again, Congress delayed implementation date to October 1, 2015, after it was inserted into "Doc Fix" Bill without debate over objections of many.
Revisions to ICD-10-CM Include:
ICD-10-CA.
ICD-10-CA is a clinical modification of ICD-10 developed by the Canadian Institute for Health Information for morbidity classification in Canada. ICD-10-CA applies beyond acute hospital care, and includes conditions and situations that are not diseases but represent risk factors to health, such as occupational and environmental factors, lifestyle and psycho-social circumstances.
ICD-11.
The World Health Organization is currently revising the International Classification of Diseases (ICD) towards the ICD-11. The development is taking place on an internet-based workspace, called iCAT (Collaborative Authoring Tool) Platform, somewhat similar to a wiki – yet it requires more structure and peer review process. The WHO collaborates through this platform with all interested parties.
The final draft of the ICD-11 system is expected to be submitted to WHO's World Health Assembly (WHA) for official endorsement by 2017. The beta draft was made available online in May 2012 for initial consultation and commenting.
In ICD-11 each disease entity will have definitions that give key descriptions and guidance on what the meaning of the entity/category is in human readable terms - to guide users. This is an advancement over ICD-10, which had only title headings. The Definitions have a standard structure according to a template with standard definition templates and further features exemplified in a “Content Model”. The Content Model is a structured framework that captures the knowledge that underpins the definition of an ICD entity. The Content Model therefore allows computerization (with links to ontologies and SNOMED CT). Each ICD entity can be seen from different dimensions or “parameters”. For example, there are currently 13 defined main parameters in the Content Model (see below) to describe a category in ICD. 
ICD exists in 41 Languages in electronic versions and its expression in multiple languages will be systematically pursued in ICD11.
Usage and current topics.
History and usage in the United States.
In the United States, the U.S. Public Health Service published "The International Classification of Diseases, Adapted for Indexing of Hospital Records and Operation Classification (ICDA)," completed in 1962 and expanding the ICD-7 in a number of areas to more completely meet the indexing needs of hospitals. The U.S. Public Health Service later published the "Eighth Revision, International Classification of Diseases, Adapted for Use in the United States," commonly referred to as ICDA-8, for official national morbidity and mortality statistics. This was followed by the "ICD, 9th Revision, Clinical Modification", known as ICD-9-CM, published by the U.S. Department of Health and Human Services and used by hospitals and other healthcare facilities to better describe the clinical picture of the patient. The diagnosis component of ICD-9-CM is completely consistent with ICD-9 codes, and remains the data standard for reporting morbidity. National adaptations of the ICD-10 progressed to incorporate both clinical code (ICD-10-CM) and procedure code (ICD-10-PCS) with the revisions completed in 2003. In 2009, the U.S. Centers for Medicare and Medicaid Services announced that it would begin using ICD-10 on April 1, 2010, with full compliance by all involved parties by 2013.
The years for which causes of death in the United States have been classified by each revision as follows:
Mental and behavioral disorders.
The ICD includes a section classifying mental and behavioral disorders (Chapter V). This has developed alongside the American Psychiatric Association's "Diagnostic and Statistical Manual of Mental Disorders" (DSM) and the two manuals seek to use the same codes. There are significant differences, however, such as the ICD including personality disorders in the same way as other mental disorders, while the DSM-IV-TR lists them on a separate 'axis'. The WHO is revising their classifications in these sections as part the development of the ICD-11 (scheduled for 2015), and an "International Advisory Group" has been established to guide this. An international survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found that the former was more often used for clinical diagnosis while the latter was more valued for research. The ICD is actually the official system for the US, although many mental health professionals do not realize this due to the dominance of the DSM. Psychologists state, "Serious problems with the clinical utility of both the ICD and the DSM are widely acknowledged."

</doc>
<doc id="15462" url="http://en.wikipedia.org/wiki?curid=15462" title="Integral domain">
Integral domain

In mathematics, and specifically in abstract algebra, an integral domain is a nonzero commutative ring in which the product of any two nonzero elements is nonzero. Integral domains are generalizations of the ring of integers and provide a natural setting for studying divisibility.
In an integral domain the cancellation property holds for multiplication by a nonzero element "a", that is, if a ≠ 0, an equality implies .
"Integral domain" is defined almost universally as above, but there is some variation. This article follows the convention that rings have a 1, but some authors who do not follow this also do not require integral domains to have a 1. Noncommutative integral domains are sometimes admitted. This article, however, follows the much more usual convention of reserving the term "integral domain" for the commutative case and using "domain" for the general case including noncommutative rings.
Some sources, notably Lang, use the term entire ring for integral domain.
Some specific kinds of integral domains are given with the following chain of class inclusions:
Definitions.
There are a number of equivalent definitions of integral domain:
Non-examples.
The following rings are "not" integral domains.
Divisibility, prime elements, and irreducible elements.
In this section, "R" is an integral domain.
Given elements "a" and "b" of "R", we say that "a" divides "b", or that "a" is a divisor of "b", or that "b" is a multiple of "a", if there exists an element "x" in "R" such that "ax" = "b".
The elements that divide 1 are called the units of "R"; these are precisely the invertible elements in "R". Units divide all other elements.
If "a" divides "b" and "b" divides "a", then we say "a" and "b" are associated elements or associates. Equivalently, "a" and "b" are associates if "a"="ub" for some unit "u". 
If "q" is a nonzero non-unit, we say that "q" is an irreducible element if "q" cannot be written as a product of two non-units. 
If "p" is a nonzero non-unit, we say that "p" is a prime element if, whenever "p" divides a product "ab", then "p" divides "a" or "p" divides "b". Equivalently, an element "p" is prime if and only if the principal ideal ("p") is a nonzero prime ideal. The notion of prime element generalizes the ordinary definition of prime number in the ring Z, except that it allows for negative prime elements. 
Every prime element is irreducible. The converse is not true in general: for example, in the quadratic integer ring formula_7 the element 3 is irreducible (if it factored nontrivially, the factors would each have to have norm 3, but there are no norm 3 elements since formula_8 has no integer solutions), but not prime (since 3 divides formula_9 without dividing either factor). In a unique factorization domain (or more generally, a GCD domain), an irreducible element is a prime element. 
While unique factorization does not hold in formula_7, there is unique factorization of ideals. See Lasker–Noether theorem.
Field of fractions.
The field of fractions "K" of an integral domain "R" is the set of fractions "a"/"b" with "a" and "b" in "R" and "b" ≠ 0 modulo an appropriate equivalence relation, equipped with the usual addition and multiplication operations. It is "the smallest field containing "R"" in the sense that there is an injective ring homomorphism "R" → "K" such that any injective ring homomorphism from "R" to a field factors through "K".
The field of fractions of the ring of integers Z is the field of rational numbers Q. The field of fractions of a field is isomorphic to the field itself.
Algebraic geometry.
Integral domains are characterized by the condition that they are reduced (that is "x"2 = 0 implies "x" = 0) and irreducible (that is there is only one minimal prime ideal). The former condition ensures that the nilradical of the ring is zero, so that the intersection of all the ring's minimal primes is zero. The latter condition is that the ring have only one minimal prime. It follows that the unique minimal prime ideal of a reduced and irreducible ring is the zero ideal, so such rings are integral domains. The converse is clear: an integral domain has no nonzero nilpotent elements, and the zero ideal is the unique minimal prime ideal.
This translates, in algebraic geometry, into the fact that the coordinate ring of an affine algebraic set is an integral domain if and only if the algebraic set is an algebraic variety.
More generally, a commutative ring is an integral domain if and only if its spectrum is an integral affine scheme.
Characteristic and homomorphisms.
The characteristic of an integral domain is either 0 or a prime number.
If "R" is an integral domain of prime characteristic "p", then the Frobenius endomorphism "f"("x") = "x" "p" is injective.

</doc>
<doc id="15466" url="http://en.wikipedia.org/wiki?curid=15466" title="Infundibulum">
Infundibulum

An infundibulum (Latin for "funnel"; plural, "infundibula") is a funnel-shaped cavity or organ.

</doc>
<doc id="15467" url="http://en.wikipedia.org/wiki?curid=15467" title="Interrupt latency">
Interrupt latency

In computing, interrupt latency is the time that elapses from when an interrupt is generated to when the source of the interrupt is serviced. For many operating systems, devices are serviced as soon as the device's interrupt handler is executed. Interrupt latency may be affected by microprocessor design, interrupt controllers, interrupt masking, and the operating system's (OS) interrupt handling methods.
Background.
There is usually a trade-off between interrupt latency, throughput, and processor utilization. Many of the techniques of CPU and OS design that improve interrupt latency will decrease throughput and increase processor utilization. Techniques that increase throughput may increase interrupt latency and increase processor utilization. Lastly, trying to reduce processor utilization may increase interrupt latency and decrease throughput.
Minimum interrupt latency is largely determined by the interrupt controller circuit and its configuration. They can also affect the jitter in the interrupt latency, which can drastically affect the real-time schedulability of the system. The Intel APIC Architecture is well known for producing a huge amount of interrupt latency jitter.
Maximum interrupt latency is largely determined by the methods an OS uses for interrupt handling. For example, most processors allow programs to disable interrupts, putting off the execution of interrupt handlers, in order to protect critical sections of code. During the execution of such a critical section, all interrupt handlers that cannot execute safely within a critical section are blocked (they save the minimum amount of information required to restart the interrupt handler after all critical sections have exited). So the interrupt latency for a blocked interrupt is extended to the end of the critical section, plus any interrupts with equal and higher priority that arrived while the block was in place.
Many computer systems require low interrupt latencies, especially embedded systems that need to control machinery in real-time. Sometimes these systems use a real-time operating system (RTOS). An RTOS makes the promise that no more than an agreed upon maximum amount of time will pass between executions of subroutines. In order to do this, the RTOS must also guarantee that interrupt latency will never exceed a predefined maximum.
Considerations.
There are many methods that hardware may use to increase the interrupt latency that can be tolerated. These include buffers, and flow control. For example, most network cards implement transmit and receive ring buffers, interrupt rate limiting, and hardware flow control. Buffers allow data to be stored until it can be transferred, and flow control allows the network card to pause communications without having to discard data if the buffer is full.
Modern hardware also implements interrupt rate limiting. This helps prevent interrupt storms or "live lock" by having the hardware wait a programmable minimum amount of time between each interrupt it generates. Interrupt rate limiting reduces the amount of time spent servicing interrupts, allowing the processor to spend more time doing useful work. Exceeding this time results in a soft (recoverable) or hard (non-recoverable) error.

</doc>
<doc id="15468" url="http://en.wikipedia.org/wiki?curid=15468" title="İskender kebap">
İskender kebap

İskender kebap is one of the most famous meat foods of northwestern Turkey and takes its name from its inventor, İskender Efendi, who lived in Bursa in the late 19th century. It can be assumed to be derived from a verticalized version of Cağ kebabı.
It is a kind of döner kebab prepared from thinly cut grilled lamb basted with hot tomato sauce over pieces of lavas bread and generously slathered with melted sheep butter and yogurt. Additionally, one cylindrical köfte can be placed on top. It is commonly consumed with şıra as a drink to aid digestion. Tomato sauce and melting butter are generally poured over the dish, at the table.
"Kebapçı İskender" is trademarked by Yavuz İskenderoğlu, whose family still runs the restaurant in Bursa. This dish is available in many restaurants throughout the country mostly under the name "İskender kebap", "Bursa kebabı", or at times with an alternative one made up by the serving restaurant such as "Uludağ kebabı".
Differences from döner kebab.
A serving of İskender kebap contains thin and wide strips of meat, on the other hand döner kebap has smaller pieces of meat. İskender kebap is prepared by stacking large pieces of meat vertically, which is why the meat strips are larger. In lower quality restaurants, ground meat is used, which is not suitable for İskender kebap. The fat content of Iskender meat is lower than that of döner meat. İskender kebap is served with yogurt and sauce, while döner is not.

</doc>
<doc id="15471" url="http://en.wikipedia.org/wiki?curid=15471" title="LGBT in Islam">
LGBT in Islam

LGBT and Islam is influenced by the religious, legal and cultural history of the nations with a sizable Muslim population, along with specific passages in the Qur'an and statements attributed to the Islamic prophet Muhammad (hadith). Hadiths traditionally are not interpreted because their language is understood to be simple matter-of-fact language. Orthodox Islam is not only a system of beliefs, but also a legal system.
The traditional schools of Islamic law based on Qur'anic verses and hadith consider homosexual acts a punishable crime and a sin, and influenced by Islamic scholars such as Imam Malik and Imam Shafi. The Qur'an cites the story of the "people of Lot" destroyed by the wrath of God because they engaged in "lustful" carnal acts between men.
Nevertheless, homoerotic themes were present in poetry and other literature written by some Muslims from the medieval period onwards and sometimes homoeroticism in the form of pederasty was seen in a positive way.
Today in most of the Islamic world homosexuality is not socially or legally accepted. In some of these countries, Afghanistan, Brunei, Iran, Mauritania, Nigeria, Saudi Arabia, Sudan and Yemen, homosexual activity carries the death penalty. In others, such as Somalia, it is illegal.
Same-sex sexual intercourse is legal in 19 Muslim-majority nations (Albania, Azerbaijan, Bahrain, Burkina Faso, Chad, Djibouti, Guinea-Bissau, Iraq, Jordan, Kazakhstan, Kosovo, Kyrgyzstan, Mali, Niger, Northern Cyprus, Tajikistan, Turkey, West Bank (State of Palestine), and most of Indonesia). In Albania, Lebanon, and Turkey, there have been discussions about legalizing same-sex marriage. Homosexual relations between females are legal in Kuwait, but homosexual acts between males are illegal. Lebanon has had recent internal efforts to legalize homosexuality.
Most Muslim-majority countries have opposed moves to advance LGBT rights at the United Nations, in the General Assembly and/or the UNHRC. However, Albania and Sierra Leone have signed a UN Declaration supporting LGBT rights. OIC member-state Mozambique provides LGBT rights protections in law in the form of non-discrimination laws, and discussions on legally recognizing same-sex marriage have been held in the country.
Law.
Sharia punishments.
There are several methods by which sharia jurists have advocated the punishment of gays or lesbians who are sexually active. One form of execution involves an LGBT person being stoned to death by a crowd of Muslims; this precedence was set in the hadith Abu Dawud (discretion advised; he has history of using da'if hadiths) which states "Whoever you find doing the action of the people of Loot, execute the one who does it and the one to whom it is done", then elaborates they be "stoned to death". The majority of Muslim jurists established an ijma ruling that LGBT people be thrown from rooftops or high places, and this is the perspective of most Salafists.
The Quran.
The Quran contains seven references to "the people of Lut", the biblical Lot, but meaning the residents of Sodom and Gomorrah (references 7:80–84, 11:77–83, 21:74, 22:43, 26:165–175, 27:56–59, and 29:27–33), and their destruction by Allah is associated explicitly with their sexual practices: 
"And (We sent) Lot when he said to his people: What! do you commit an indecency which any one in the world has not done before you? Most surely you come to males in lust besides females; nay you are an extravagant people. And the answer of his people was no other than that they said: Turn them out of your town, surely they are a people who seek to purify (themselves). So We delivered him and his followers, except his wife; she was of those who remained behind. And We rained upon them a rain; consider then what was the end of the guilty."[ (Translated by Shakir)]
The sins of the people of Lot became proverbial, and the Arabic words for homosexual behaviour ("liwat") and for a person who performs such acts ("luti") both derive from his name. There is, however, only one passage in the Qur'an which can be interpreted as prescribing a legal position, and is not restricted to homosexual behaviour - in fact it deals with public practice of adultery:
"And as for those who are guilty of an indecency from among your women, call to witnesses against them four (witnesses) from among you; then if they bear witness confine them to the houses until death takes them away or Allah opens some way for them. And as for the two who are guilty of indecency from among you, give them both a punishment; then if they repent and amend, turn aside from them; surely Allah is oft-returning (to mercy), the Merciful."[ (Translated by Shakir)]
Islamist journalist Muhammad Jalal Kishk found no prescribed punishment for homosexuality in Islamic law Several modern day scholars, including Scott Kugle, argue for a different interpretation of the Lot narrative focusing not on the sexual act but on the infidelity of the tribe and their rejection of Lot's Prophethood.
The Hadith and Seerah.
The hadith (sayings and actions of Muhammad) show that homosexuality was not unknown in Arabia. Given that the Qur'an is allegedly vague regarding the punishment of homosexual sodomy, Islamic jurists turned to the collections of the hadith and seerah (accounts of Muhammad's life) to support their argument for Hudud punishment.
Ibn al-Jawzi records Muhammad as cursing sodomites in several hadith, and recommending the death penalty for both the active and passive partners in same-sex acts.
Sunan al-Tirmidhi, compiling his work two centuries after the death of Muhammad, wrote that Muhammad had prescribed the death penalty for both the active and the passive partner: "Whoever you find committing the sin of the people of Lut (Lot), kill them, both the one who does it and the one to whom it is done." The overall moral or theological principle is that a person who performs such actions ("luti") challenges the harmony of God's creation, and is therefore a revolt against God.
Al-Nuwayri in his "Nihaya" reports that Muhammad is alleged to have said what he feared most for his community were the practices of the people of Lot (although he seems to have expressed the same idea in regard to wine and female seduction).
 Narrated by Abdullah ibn Abbas: The Prophet (peace be upon him) said: If you find anyone doing as Lot's people did, kill the one who does it, and the one to whom it is done.
 — Sunan Abu Dawood, see also Sunan Abu Dawood, Sunan Abu Dawood, Sunan Abu Dawood, Sunan Abu Dawood, Sunan Abu Dawood, Sunan Abu Dawood, 
 Narrated by Abdullah ibn Abbas: The Prophet cursed effeminate men; those men who are in the similitude (assume the manners of women) and those women who assume the manners of men, and he said, "Turn them out of your houses." The Prophet turned out such-and-such man, and 'Umar turned out such-and-such woman.
 — Sahih al-Bukhari, see also Sahih al-Bukhari, 
Medieval jurisprudence.
The four schools of shari'a (Islamic law) disagreed on what punishment is appropriate for "liwat". Abu Bakr Al-Jassas (d. 981 AD/370 AH) argued that the two hadiths on killing homosexuals "are not reliable by any means and no legal punishment can be prescribed based on them", and the Hanafi school held that it does not merit any physical punishment, on the basis of a hadith that "Muslim blood can only be spilled for adultery, apostasy and homicide"; against this the Hanbali school inferred that sodomy is a form of adultery and must incur the same penalty, i.e. death.
There were varying opinions on how the death penalty is to be carried out. Abu Bakr recommended toppling a wall on the evil-doer, or else burning alive, while Ali bin Abi Talib ordered death by stoning for one "luti" and had another thrown head-first from the top of a minaret—according to Ibn Abbas, this last punishment must be followed by stoning.
Rulings by modern scholars of Islam.
With few exceptions all scholars of Sharia, or Islamic law, interpret homosexual activity as a punishable offence as well as a sin. There is no specific punishment prescribed, however, and this is usually left to the discretion of the local authorities on Islam. Mohamed El-Moctar El-Shinqiti, a contemporary Mauritanian scholar, has argued that "[even though] homosexuality is a grievous sin...[a] no legal punishment is stated in the Qur'an for homosexuality...[b] it is not reported that Prophet Muhammad has punished somebody for committing homosexuality...[c] there is no authentic hadith reported from the Prophet prescribing a punishment for the homosexuals..." Hadith scholars such as Al-Bukhari, Yahya ibn Ma'in, Al-Nasa'i, Ibn Hazm, Al-Tirmidhi, and others have impugned these statements.
Faisal Kutty, a professor of Islamic law at Indiana-based Valparaiso University Law School and Toronto-based Osgoode Hall Law School, commented on the contemporary same-sex marriage debate in a March 27, 2014 essay in the Huffington Post. He acknowledged that while Islamic law iterations prohibits pre- and extra-marital as well as same-sex sexual activity, it does not attempt to "regulate feelings, emotions and urges, but only its translation into action that authorities had declared unlawful." Kutty, who teaches comparative law and legal reasoning, also wrote that many Islamic scholars have "even argued that homosexual tendencies themselves were not haram [prohibited] but had to be suppressed for the public good." He claimed that this may not be "what the LGBTQ community wants to hear," but he wrote that, "it reveals that even classical Islamic jurists struggled with this issue and had a more sophisticated attitude than many contemporary Muslims." Kutty who in the past wrote in support of allowing Islamic principles in dispute resolution also noted that "most Muslims have no problem extending full human rights to those - even Muslims - who live together 'in sin'." He argued that therefore it seems hypocritical to deny fundamental rights to same-sex couples. Moreover, he argued as pointed out by Islamic legal scholar Mohamed Fadel, this is not about changing Islamic marriage (nikah), but about making "sure that all citizens have access to the same kinds of public benefits.
Views of homosexuality in Islamic societies.
Modern day.
During the Ottoman Empire, homosexuality was decriminalized in 1858, as part of wider reforms during the Tanzimat.
Raphael Patai in "The Arab Mind" has argued that among some Arabs and Turks homosexuality can be justified as an expression of power. The "active homosexual act is considered as an assertion of one's aggressive masculine superiority, while the acceptance of the role of the passive homosexual is considered extremely degrading and shameful because it casts the man or youth into a submissive, feminine role".
In 2011, the UN Human Rights Council passed its first resolution recognizing LGBT rights, which was followed up with a report from the UN Human Rights Commission documenting violations of the rights of LGBT people. The two world maps of religions of the world and the countries that support LGBT rights at the UN give an impression of the official attitude towards homosexuality in the Muslim world.
Rejection of homosexuality according to opinion polls.
In a study on the global acceptance of homosexuality by Pew Research Center, released June 4, 2013 it is stated:
The survey of publics in 39 countries finds broad acceptance of homosexuality in North America, the European Union, and much of Latin America, but equally widespread rejection in predominantly Muslim nations and in Africa, as well as in parts of Asia and in Russia.
In the UK, a Gallup poll showed that none of the 500 British Muslims polled believed homosexuality to be "morally acceptable", compared with 35% of the 1001 French Muslims polled. A 2007 survey of British Muslims showed that 61% believe homosexuality should be illegal, with up to 71% of young British Muslims holding this belief. According to a 2012 poll, 51% of the Turks in Germany, who account for nearly two thirds of the total Muslim population in Germany, believe that homosexuality is a sickness.
Sex between boys and men.
Despite the formal disapproval of religious authority, the segregation of women in Muslim societies and the strong emphasis on male virility leads adolescent males and unmarried young men to seek sexual outlets with boys younger than themselves—in one study in Morocco, with boys in the age-range 7 to 13. Men have sex with other males so long as they are the penetrators and their partners are boys, or in some cases effeminate men. "Liwat" is regarded as a temptation, and anal intercourse is not seen as repulsively unnatural so much as dangerously attractive. They believe "one has to avoid getting buggered precisely in order not to acquire a taste for it and thus become addicted." Not all sodomy is homosexual: one Moroccan sociologist, in a study of sex education in his native country, notes that for many young men heterosexual sodomy is considered better than vaginal penetration, and female prostitutes likewise report the demand for anal penetration from their (male) clients.
It is not so much the penetration as the enjoyment that is considered bad. Deep shame attaches to the passive partner: "for this reason men stop getting laid at the age of 15 or 16 and 'forget' that they ever allowed/suffered/enjoyed it earlier." Similar sexual sociologies are reported for other Muslim societies from North Africa to Pakistan and the Far East. In Afghanistan in 2009, the British Army was forced to commission a report into the sexuality of the local men after British soldiers reported the discomfort at witnessing adult males involved in sexual relations with boys. The report stated that though illegal, there was a tradition of such relationships in the country, known as "bache bazi" or "boy play", and that it was especially strong around North Afghanistan. http://www.bbc.co.uk/news/world-south-asia-11217772
Gay marriage.
In 2007 there was a gay party in the Moroccan town of al-Qasr al-Kabir. Rumours spread that this was a gay marriage and more than 600 people took to the streets, condemning the alleged event and protesting against leniency towards homosexuals. Several persons who attended the party were detained and eventually six Moroccan men were sentenced to between four and ten months in prison for "homosexuality".
In France there was an Islamic same-sex marriage on February 18, 2012. In Paris in November 2012 a room in a Buddhist prayer hall was used by gay Muslims and called a "gay-friendly mosque", and a French Islamic website is supporting religious same-sex marriage.
The first American Muslim in the United States Congress, Keith Ellison (D-MN) said in 2010 that all discrimination against LGBT people is wrong. He further expressed support for gay marriage stating:
I believe that the right to marry someone who you please is so fundamental it should not be subject to popular approval any more than we should vote on whether blacks should be allowed to sit in the front of the bus.
In 2014 eight men were jailed for three years by a Cairo court after the circulation of a video of them allegedly taking part in a private wedding ceremony between two men on a boat on the Nile.
Medieval era.
Increasing prosperity resulting from Muslim conquests in the centuries following Muhammad's death was accompanied by what some Muslims bemoaned as a general "corruption" of morals in the two holy cities of Mecca and Medina.
Therefore, despite its condemnation by religious authorities, homosexuality persisted in a subterranean manner. It seems to have become less of a rarity as the process of acculturation sped up. Information relating to the development of music and song reveals the presence of "mukhannathun", who were apparently for the most part of foreign origin. The arrival of the Abbasid army to Arabia in the 8th century seems to have meant that tolerance for homosexual practice subsequently spread more widely under the new dynasty. The ruler Al-Amin, for example, was said to have required slave women to be dressed in masculine clothing in the hope of inducing him to adopt more conventional morals.
There are other examples from the following centuries. The Aghlabid Emir, Ibrahim II of Ifriqiya (ruled 875–902), was said to have been surrounded by some sixty catamites, yet whom he was said to have treated in a most horrific manner. Caliph al-Mutasim in the 9th century and some of his successors were accused of homosexuality. The popular stories says that Cordoba, Abd al-Rahman III had executed a young man from León who was held as a hostage, because he had refused his advances during the Reconquista.
Mehmed the Conqueror, the Ottoman sultan living in the 15th century, European sources say "who was known to have ambivalent sexual tastes, sent a eunuch to the house of Notaras, demanding that he supply his good looking fourteen year old son for the Sultan’s pleasure. When he refused, the Sultan instantly ordered the decapitation of Notaras, together with that of his son and his son-in-law; and their three heads … were placed on the banqueting table before him". Another youth Mehmed found attractive, and who was presumably more accommodating, was Radu III the Fair, the brother of the famous Vlad the Impaler, "Radu, a hostage in Istanbul whose good looks had caught the Sultan’s fancy, and who was thus singled out to serve as one of his most favored pages." After the defeat of Vlad, Mehmed placed Radu on the throne of Wallachia as a vassal ruler. However, Turkish sources deny these stories.
The objectivity of the European sources that claim Mehmed had homosexual tendencies cannot always be verified. Commonly, contemporary writers would embellish stories to add sensual imagery and homosexual behavior and attribute them to Ottoman sultans in an attempt to rile up European opposition to the Ottomans. Furthermore, with regards to the story about Notaras' son, Ottoman sources assert that the boy was being recruited to be an "iç oğlan", meaning an "inner servant". Mehmed employed a corps of inner servants, whose role was to serve in the innermost chambers of the palace, not for the sexual pleasure of the sultan.
In literature.
According to the "Encyclopedia of Islam and the Muslim World"
Whatever the legal strictures on sexual activity, the positive expression of male homeoerotic sentiment in literature was accepted, and assiduously cultivated, from the late eighth century until modern times. First in Arabic, but later also in Persian, Turkish and Urdu, love poetry by men about boys more than competed with that about women, it overwhelmed it. Anecdotal literature reinforces this impression of general societal acceptance of the public celebration of male-male love (which hostile Western caricatures of Islamic societies in medieval and early modern times simply exaggerate).
Legal status in nations.
The Ottoman Empire (predecessor of Turkey) decriminalizes homosexuality in 1858 and In Turkey where 99.8% of the population is Muslim, homosexuality has never criminalized since the day it has founded in 1923. and LGBT people also have right to seek asylum to Turkey under the Geneva Convention since 1951. On the other hand, The death penalty for homosexuality is currently in place in Saudi Arabia, Yemen, Iran, Afghanistan, Mauritania, Sudan and northern Nigeria.
The legal situation in the United Arab Emirates is that it is illegal and can get you the death penalty, but some regions just give jail time, mutilations, and fines. In Qatar, Algeria, Uzbekistan and the Maldives, homosexuality is punished with jail time or fines. This has led to controversy regarding Qatar, which is due to stage the 2022 FIFA World Cup. Human rights groups have questioned the awarding in 2010 of the right to host the competition, due to the possibility that gay football fans may be jailed. In response, Sepp Blatter, head of FIFA, joked that they would have to "refrain from sexual activity" while in Qatar. He later withdrew the remarks after condemnation from rights groups.
In Saudi Arabia, while the maximum punishment for homosexual acts is public execution, the government will generally use lesser punishments—e.g., fines, jail time, and whipping—as alternatives, unless it feels that individuals are challenging state authority by engaging in LGBT social movements. Iran is perhaps the nation to execute the largest number of its citizens for homosexual acts. Since the 1979 Islamic revolution, the Iranian government has executed more than 4,000 such people.
In Egypt, openly gay men have been prosecuted under general public morality laws. (See Cairo 52.) In stark contrast, homosexuality is both legal and tolerated in Lebanon and other states in the Levant, like Palestine, Jordan.
In 20 out of 57 Muslim-majority nations same-sex intercourse is not forbidden by law. In Albania there have been discussions about legalizing same-sex marriage. Homosexual relations between females are legal in Kuwait but homosexual acts between males are illegal. Lebanon has an internal effort to legalize homosexuality.
Homosexuality laws in majority Muslim countries.
According to the International Lesbian and Gay Association (ILGA) seven countries still retain capital punishment for homosexual behavior: Afghanistan, Mauritania, Pakistan, Saudi Arabia, Sudan, and Yemen. The situation in the United Arab Emirates (UAE) is that is punished by corporal or capital punishment, depending on the region.
Homosexuality laws in India.
In India, where Muslims form a large minority, the largest Islamic seminary (Darul Uloom Deoband) has vehemently opposed recent government moves to abrogate and liberalize laws from the British Raj era that banned homosexuality.
LGBT movements within Islam.
The Al-Fatiha Foundation is an organization which tries to advance the cause of gay, lesbian, and transgender Muslims. It was founded in 1998 by Faisal Alam, a Pakistani American, and is registered as a nonprofit organization in the United States. The organization was an offshoot of an internet listserve that brought together many gay, lesbian and questioning Muslims from various countries. The Foundation accepts and considers homosexuality as natural, either regarding Qur'anic verses as obsolete in the context of modern society, or stating that the Qu'ran speaks out against homosexual lust and is silent on homosexual love. In 2001, Al-Muhajiroun, a banned and now defunct international organization who sought the establishment of a global Islamic caliphate, issued a fatwa declaring that all members of Al-Fatiha were "murtadd", or apostates, and condemning them to death. Because of the threat and coming from conservative societies, many members of the foundation's site still prefer to be anonymous so as to protect their identity while continuing a tradition of secrecy. Al-Fatiha has fourteen chapters in the United States, as well as offices in England, Canada, Spain, Turkey, and South Africa. In addition, Imaan, a social support group for Muslim LGBT people and their families, exists in the UK. Both of these groups were founded by gay Pakistani activists. The UK also has the Safra Project for women.
Some Muslims such as the lesbian writer Irshad Manji and academic author Scott Kugle argue that Islam does not condemn homosexuality. Kugle, South Asian scholar and author Ruth Vanita, and Muslim scholar and writer Saleem Kidwai also contend that ancient Islam has a rich history of homoerotic literature.
There are also a number of Islamic ex-gay (i.e. people claiming to have experienced a basic change in sexual orientation from exclusive homosexuality to exclusive heterosexuality) groups aimed at attempting to guide homosexuals towards heterosexuality. A large body of research and global scientific consensus indicates that being gay, lesbian, or bisexual is compatible with normal mental health and social adjustment. Because of this, major mental health professional organizations discourage and caution individuals against attempting to change their sexual orientation to heterosexual, and warn that attempting to do so can be harmful.
The religious conflicts and inner turmoil that Islamic homosexuals struggle over has been addressed in various media, such as the 2006 Channel 4 documentary "Gay Muslims", and the 2007 documentary film "A Jihad for Love". The latter was produced by Sandi Simcha DuBowski, who also made a Jewish-themed documentary on the same topic ("Trembling Before G-d") 6 years before.
In November 2012, a prayer room was set up in Paris by gay Islamic scholar and founder of the group 'Homosexual Muslims of France' Ludovic-Mohamed Zahed. It was described by the press as the first gay-friendly mosque in Europe, but traditional Islamic scholars disagree.
Gender variant and transgender people.
In Islam, the term mukhannathun is used to describe gender-variant people, usually male-to-female transgender. Neither this term nor the equivalent for "eunuch" occurs in the Qur'an, but the term does appear in the Hadith, the sayings of Muhammad, which have a secondary status to the central text. Moreover, within Islam, there is a tradition on the elaboration and refinement of extended religious doctrines through scholarship. This doctrine contains a passage by the scholar and hadith collector An-Nawawi:A mukhannath is the one ("male") who carries in his movements, in his appearance and in his language the characteristics of a woman. There are two types; the first is the one in whom these characteristics are innate, he did not put them on by himself, and therein is no guilt, no blame and no shame, as long as he does not perform any (illicit) act or exploit it for money (prostitution etc.). The second type acts like a woman out of immoral purposes and he is the sinner and blameworthy.
While Iran has outlawed homosexuality, Iranian Shi'a thinkers such as Ayatollah Khomeini have allowed for transgender people to change their sex so that they can enter heterosexual relationships. This position has been confirmed by the Supreme Leader of Iran, Ayatollah Ali Khamenei, and is also supported by many other Iranian clerics.
Iran carries out more sex change operations than any other nation in the world except for Thailand. It is regarded as a cure for homosexuality, which is punishable by death under Iranian law. The government even provides up to half the cost for those needing financial assistance and a sex change is recognized on the birth certificate.

</doc>
<doc id="15474" url="http://en.wikipedia.org/wiki?curid=15474" title="Infanticide">
Infanticide

Infanticide (or infant homicide) is the intentional killing of children under the age of 12 months according to the Infanticide Act 1938 in the UK. Parental infanticide are more commonly done by fathers than mothers but vice versa for neonaticide.
In many past societies, certain forms of infanticide were considered permissible. In some countries, female infanticide is more common than the killing of male offspring, due to sex-selective infanticide. In China for example, the gender gap between males and females aged 0–19 year old were estimated to be 25 million in 2010 by the United Nations Population Fund.
History and pre-history.
The practice of infanticide has taken many forms. Child sacrifice to supernatural figures or forces, such as that practiced in ancient Carthage, may be only the most notorious example in the ancient world. Anthropologist Laila Williamson notes that "Infanticide has been practiced on every continent and by people on every level of cultural complexity, from hunter gatherers to high civilizations, including our own ancestors. Rather than being an exception, then, it has been the rule."
A frequent method of infanticide in ancient Europe and Asia was simply to abandon the infant, leaving it to die by exposure (i.e. hypothermia, hunger, thirst, or animal attack). Infant abandonment still occurs in modern societies.
In at least one island in Oceania, infanticide was carried out until the 20th century by suffocating the infant, while in pre-Columbian Mesoamerica and in the Inca Empire it was carried out by sacrifice (see below).
Paleolithic and Neolithic.
Many Neolithic groups routinely resorted to infanticide in order to control their numbers so that their lands could support them. Joseph Birdsell believed that infanticide rates in prehistoric times were between 15% and 50% of the total number of births, while Laila Williamson estimated a lower rate ranging from 15% to 20%. Both anthropologists believed that these high rates of infanticide persisted until the development of agriculture during the Neolithic Revolution. Comparative anthropologists have calculated that 50% of female newborn babies were killed by their parents during the Paleolithic era. Decapitated skeletons of hominid children have been found with evidence of cannibalism. The children were not necessarily actively killed, but neglect and intentional malnourishment may also have occurred, as proposed by Vicente Lull as an explanation for an apparent surplus of men and the below average height of women in prehistoric Menorca.
In ancient history.
In the New World.
Archaeologists have uncovered physical evidence of child sacrifice at several locations. Some of the best attested examples are the diverse rites which were part of the religious practices in Mesoamerica and the Inca Empire.
In the Old World.
Three thousand bones of young children, with evidence of sacrificial rituals, have been found in Sardinia. Infants were offered to the Babylonian goddess Ishtar. Pelasgians offered a sacrifice of every tenth child during difficult times. Syrians sacrificed children to Jupiter and Juno. Many remains of children have been found in Gezer excavations with signs of sacrifice. Child skeletons with the marks of sacrifice have been found also in Egypt dating 950-720 BCE. In Carthage "[child] sacrifice in the ancient world reached its infamous zenith." Besides the Carthaginians, other Phoenicians, and the Canaanites, Moabites and Sepharvites offered their first-born as a sacrifice to their gods.
Ancient Egypt.
In Egyptian households, at all social levels, children of both sexes were valued and there is no evidence of infanticide. The religion of the Ancient Egyptians forbade infanticide and during the Greco-Roman period they rescued abandoned babies from manure heaps, a common method of infanticide by Greeks or Romans, and were allowed to either adopt them as foundlings or raise them as slaves, often giving them names such as "copro -" to memorialise their rescue. Strabo considered it a peculiarity of the Egyptians that every child must be reared. Diodorus indicates infanticide was a punishable offence. Egypt was heavily dependent on the annual flooding of the Nile to irrigate the land and in years of low inundation severe famine could occur with breakdowns in social order resulting, notably between 930-1070 AD and 1180-1350 AD. Instances of cannibalism are recorded during these periods but it is unknown if this happened during the pharaonic era of Ancient Egypt. Beatrix Midant-Reynes describes human sacrifice as having occurred at Abydos in the early dynastic period (c. 3150-2850 BCE), while Jan Assmann asserts there is no clear evidence of human sacrifice ever happening in Ancient Egypt.
Carthage.
According to Shelby Brown, Carthaginians, descendants of the Phoenicians, sacrificed infants to their gods. Charred bones of hundreds of infants have been found in Carthaginian archaeological sites. One such area harbored as many as 20,000 burial urns. Skeptics suggest that the bodies of children found in Carthaginian and Phoenician cemeteries were merely the cremated remains of children that died naturally.
Plutarch (ca. 46–120 AD) mentions the practice, as do Tertullian, Orosius, Diodorus Siculus and Philo. The Hebrew Bible also mentions what appears to be child sacrifice practiced at a place called the Tophet (from the Hebrew "taph" or "toph", to burn) by the Canaanites. Writing in the 3rd century BCE, Kleitarchos, one of the historians of Alexander the Great, described that the infants rolled into the flaming pit. Diodorus Siculus wrote that babies were roasted to death inside the burning pit of the god Baal Hamon, a bronze statue.
Greece and Rome.
The historical Greeks considered the practice of adult and child sacrifice barbarous. However, exposure of newborns was widely practiced in ancient Greece. In Greece the decision to expose a child was typically the father's, although in Sparta the decision was made by a group of elders. Exposure was the preferred method of disposal, as that act in itself was not murder; moreover, the exposed child technically had a chance of being rescued by the gods or any passersby. This very situation was a recurring motif in Greek mythology.
To notify the neighbors of a birth of a child, a woolen strip was hung over the front door to indicate a female baby and an olive branch to indicate a boy had been born. Families did not always keep their new child. After a woman had a baby, she would show it to her husband. If the husband accepted it, it would live, but if he refused it, it would die. Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example hunger, asphyxiation or exposure to the elements.
The practice was prevalent in ancient Rome, as well. Philo was the first philosopher to speak out against it. A letter from a Roman citizen to his sister, dating from 1 BCE, demonstrates the casual nature with which infanticide was often viewed:
In some periods of Roman history it was traditional for a newborn to be brought to the "pater familias", the family patriarch, who would then decide whether the child was to be kept and raised, or left to die by exposure. The Twelve Tables of Roman law obliged him to put to death a child that was visibly deformed. The concurrent practices of slavery and infanticide contributed to the "background noise" of the crises during the Republic.
Infanticide became a capital offense in Roman law in 374 AD, but offenders were rarely if ever prosecuted.
According to mythology, Romulus and Remus, twin infant sons of the war god Mars, survived near-infanticide after being tossed into the Tiber River. According to the myth, they were raised by wolves, and later founded the city of Rome.
Judaism.
Judaism prohibits infanticide, and has for some time, dating back to at least early Common Era. Roman historians wrote about the ideas and customs of other peoples, which often diverged from their own. Tacitus recorded that the Jews "regard it as a crime to kill any late-born children." Josephus, whose works give an important insight into 1st-century Judaism, wrote that God "forbids women to cause abortion of what is begotten, or to destroy it afterward."
Pagan European tribes.
In his book "Germania", Tacitus wrote that the ancient Germanic tribes enforced a similar prohibition. He found such mores remarkable and commented: "[The Germani] hold it shameful to kill any unwanted child." Modern scholarship differs. John Boswell believed that in ancient Germanic tribes unwanted children were exposed, usually in the forest. "It was the custom of the [Teutonic] pagans, that if they wanted to kill a son or daughter, they would be killed before they had been given any food." Usually children born out of wedlock were disposed that way.
In his highly influential "Pre-historic Times", John Lubbock described burnt bones indicating the practice of child sacrifice in pagan Britain.
The last canto, "Marjatan poika" (Son of Marjatta), of Finnish national epic Kalevala describes an assumed infanticide. Väinämöinen orders the infant bastard son of Marjatta to be drowned in marsh.
The Íslendingabók, a main source for the early history of Iceland, recounts that on the Conversion of Iceland to Christianity in 1000 it was provided - in order to make the transition more palatable to Pagans - that "(...)the old laws allowing exposure of newborn children will remain in force".
However, this provision - like other concessions made at the time to the Pagans - was abolished some years later.
Christianity.
Christianity rejects infanticide. The "Teachings of the Apostles" or "Didache" said "You shall not kill that which is born." The "Epistle of Barnabas" stated an identical command. Apologists Tertullian, Athenagoras, Minucius Felix, Justin Martyr and Lactantius also maintained that exposing a baby to death was a wicked act. In 318 AD, Constantine I considered infanticide a crime, and in 374 AD, Valentinian I mandated the rearing of all children (exposing babies, especially girls, was still common). The Council of Constantinople declared that infanticide was homicide, and in 589 AD, the Third Council of Toledo took measures against the Spanish custom of killing their own children.
Middle Ages.
Whereas theologians and clerics preached sparing their lives, newborn abandonment continued as registered in both the literature record and in legal documents. According to William L. Langer, exposure in the Middle Ages "was practiced on gigantic scale with absolute impunity, noticed by writers with most frigid indifference". At the end of the 12th century, notes Richard Trexler, Roman women threw their newborns into the Tiber river in daylight.
Unlike other European regions, in the Middle Ages the German mother had the right to expose the newborn. In Gotland, Sweden, children were also sacrificed.
In the High Middle Ages, abandoning unwanted children finally eclipsed infanticide. Unwanted children were left at the door of church or abbey, and the clergy was assumed to take care of their upbringing. This practice also gave rise to the first orphanages.
However, very high sex ratios were common in even late medieval Europe, which may indicate sex-selective infanticide.
Arabia.
The pre-Islamic Arabian society practiced infanticide as a form of "post-partum birth control". Regarding the prevalence of this practice, we know it was "common enough among the pre-Islamic Arabs to be assigned a specific term, "waʾd"". Infanticide was practiced either out of destitution (thus practiced on males and females alike), or as sacrifices to gods, or as "disappointment and fear of social disgrace felt by a father upon the birth of a daughter".
Some authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine. Others state that "female infanticide was common all over Arabia during this period of time" (pre-Islamic Arabia), especially by burying alive a female newborn.
Islam.
Islam
Infanticide is explicitly prohibited by the Qur'an. "And do not kill your children for fear of poverty; We give them sustenance and yourselves too; surely to kill them is a great wrong."
The Qur'an rejects the practice of infanticide. Together with polytheism and homicide, infanticide is regarded as a grave sin (see and ). Infanticide is also implicitly denounced in the story of Pharaoh's slaughter of the male children of Israelites (see ; ; ; ; ;).
Ukraine and Russia.
Infanticide may have been practiced as human sacrifice, as part of the pagan cult of Perun. Ibn Fadlan describes sacrificial practices at the time of his trip to Kiev Rus (present day Ukraine) in 921-922 CE, and describes an incident of a woman voluntarily sacrificing her life as part of a funeral rite for a prominent leader, but makes no mention of infanticide. The Primary Chronicle, one of the most important literary sources before the 12th century, indicates that human sacrifice to idols may have been introduced by Vladimir the Great in 980 CE. The same Vladimir the Great formally converted Kiev Rus into Christianity just 8 years later, but pagan cults continued to be practiced clandestinely in remote areas as late as the 13th century.
In Kamchatka, babies were killed and thrown to the dogs. American explorer George Kennan noted that among the Koryaks, a Mongoloid people of north-eastern Siberia, infanticide was still common in the 19th century. One of the twins was always sacrificed.
Georgia.
The Svans killed newborn females by filling their mouths with hot ashes. 
Asia.
China.
Marco Polo, the famed explorer, saw newborns exposed in Manzi. China's society practiced sex selective infanticide. Philosopher Han Fei Tzu, a member of the ruling aristocracy of the 3rd century BC, who developed a school of law, wrote: "As to children, a father and mother when they produce a boy congratulate one another, but when they produce a girl they put it to death." Among the Hakka people, and in Yunnan, Anhui, Sichuan, Jiangxi and Fujian a method of killing the baby was to put her into a bucket of cold water, which was called "baby water".
Infanticide was known in China as early as the 3rd century BC, and, by the time of the Song Dynasty (960-1279 AD), it was widespread in some provinces. Buddhist belief in transmigration allowed poor residents of the country to kill their newborn children if they felt unable to care for them, hoping that they would be reborn in better circumstances. Furthermore, some Chinese did not consider newborn children fully "human", and saw "life" beginning at some point after the sixth month after birth.
Contemporary writers from the Song Dynasty note that, in Hubei and Fujian provinces, residents would only keep three sons and two daughters (among poor farmers, two sons and one daughter), and kill all babies beyond that number at birth. Initially the sex of the child was only one factor to consider. By the time of the Ming Dynasty, however (1368–1644), male infanticide was becoming increasingly uncommon. The prevalence of female infanticide remained high much longer. The magnitude of this practice is subject to some dispute; however, one commonly quoted estimate is that, by late Qing, between one fifth and one quarter of all newborn girls, across the entire social spectrum, were victims of infanticide. If one includes excess mortality among female children under 10 (ascribed to gender-differential neglect), the share of victims rises to one third.
Scottish Physician John Dudgeon, who worked in Beijing, China, during the Qing Dynasty said that in China, "Infanticide does not prevail to the extent so generally believed among us, and in the north it does not exist at all."
Gender-selected abortion, abandonment, and infanticide are illegal in present-day China. Nevertheless, the US State Department, and the human rights organization Amnesty International have all declared that China's family planning programs, called the one child policy, contribute to infanticide.
Japan.
Since feudal Japan the common slang for infanticide was "mabiki" (間引き) which means to pull plants from an overcrowded garden. A typical method in Japan was smothering through wet paper on the baby's mouth and nose. Mabiki persisted in the 19th century and early 20th century. To bear twins was perceived as barbarous and unlucky and efforts were made to hide or kill one or both twins.
India.
Female infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held "in one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death". The practice of female infanticide was also common among the Kutch, Kehtri, Nagar, Bengal, Miazed, Kalowries in India inhabitants, and also among the Sindh in British India.
It was not uncommon that parents threw a child to the sharks in the Ganges River as a sacrificial offering. The British colonists were unable to outlaw the custom until the beginnings of the 19th century.
According to social activists, female infanticide has remained a problem in India into the 21st century, with both NGOs and the government conducting awareness campaigns to combat it.
Africa.
In some African societies some neonates were killed because of beliefs in evil omens or because they were considered unlucky. Twins were usually put to death in Arebo; as well as by the Nama Hottentots of South West Africa; in the Lake Victoria Nyanza region; by the Tswana in Portuguese East Africa; in some parts of Igboland, Nigeria twins were sometimes abandoned in a forest at birth, often times one twin was killed or hidden by midwives of wealthier mothers; and by the !Kung people of the Kalahari Desert. The Kikuyu, Kenya's most populous ethnic group, practiced ritual killing of twins.
Australia.
Literature suggests infanticide may have occurred reasonably commonly amongst Indigenous Australians, in all areas of Australia prior to European settlement. Infanticide may have continued to occur quite often up until the 1960s. An 1866 issue of 'The Australian News for Home Readers' informed readers that "the crime of infanticide is so prevalent amongst the natives that it is rare to see an infant."
Author Susanna de Vries in 2007 told a newspaper that her accounts of Aboriginal violence, including infanticide, was censored by publishers in the 1980s and 1990s. She told reporters that the censorship "stemmed from guilt over the stolen children question." Keith Windschuttle weighed in on the conversation, saying this type of censorship started in the 1970s. In the same article Louis Nowra suggested that infanticide in customary Aboriginal law may have been because it was difficult to keep an abundant number of Aboriginal children alive; there were life-and-death decisions modern-day Australians no longer have to face.
South Australia and Victoria.
According to William D. Rubinstein, "Nineteenth-century European observers of Aboriginal life in South Australia and Victoria reported that about 30% of Aboriginal infants were killed at birth."
James Dawson wrote a passage about infanticide amongst Indigenous people in the western district of Victoria, which stated that "Twins are as common among them as among Europeans; but as food is occasionally very scarce, and a large family troublesome to move about, it is lawful and customary to destroy the weakest twin child, irrespective of sex. 
It is usual also to destroy those which are malformed."
He also wrote "When a woman has children too rapidly for the convenience and necessities of the parents, she makes up her mind to let one be killed, and consults with her husband which it is to be. As the strength of a tribe depends more on males than females, the girls are generally sacrificed.
The child is put to death and buried, or burned without ceremony; not, however, by its father or mother, but by relatives. No one wears mourning for it.
Sickly children are never killed on account of their bad health, and are allowed to die naturally."
Western Australia.
In 1937, a reverend in the Kimberley offered a "baby bonus" to Aboriginal families as a deterrent against infanticide and to increase the birthrate of the local Indigenous population.
Australian Capital Territory.
A Canberran journalist in 1927 wrote of the 'cheapness of life' to the Aboriginal people local to the Canberra area 100 years before. "If drought or bush fires had devastated the country and curtailed food supplies, babies got short shift. Ailing babies, too would not be kept" he wrote.
New South Wales.
A bishop wrote in 1928 that it was common for Aboriginal Australians to restrict the size of their tribal groups, including by infanticide, so that the food resources of the tribal area may be sufficient for them. See also.
Northern Territory.
Annette Hamilton, a professor of anthropology at Macquarie University who carried out research in the Aboriginal community of Maningrida in Arnhem Land during the 1960s wrote that prior to that time part-European babies born to Aboriginal mothers had not been allowed to live, and that 'mixed-unions are frowned on by men and women alike as a matter of principle'.
North America.
Inuit.
There is no agreement about the actual estimates of the frequency of newborn female infanticide in the Inuit population. Carmel Schrire mentions diverse studies ranging from 15-50% to 80%.
Polar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, "The Unwanted Child", where a mother throws her child into the fjord.
The Yukon and the Mahlemuit tribes of Alaska exposed the female newborns by first stuffing their mouths with grass before leaving them to die. In Arctic Canada the Inuit exposed their babies on the ice and left them to die.
Female Inuit infanticide disappeared in the 1930s and 1940s after contact with the Western cultures from the South.
Canada.
The "Handbook of North American Indians" reports infanticide among the Dene Natives and those of the Mackenzie Mountains.
Native Americans.
In the Eastern Shoshone there was a scarcity of Indian women as a result of female infanticide. For the Maidu native Americans twins were so dangerous that they not only killed them, but the mother as well. In the region known today as southern Texas, the Mariame Indians practiced infanticide of females on a large scale. Wives had to be obtained from neighboring groups.
Mexico.
Bernal Díaz recounted that, after landing on the Veracruz coast, they came across a temple dedicated to Tezcatlipoca. "That day they had sacrificed two boys, cutting open their chests and offering their blood and hearts to that accursed idol". In "The Conquest of New Spain" Díaz describes more child sacrifices in the towns before the Spaniards reached the large Aztec city Tenochtitlan.
South America.
Although academic data of infanticides among the indigenous people in South America is not as abundant as that of North America, the estimates seem to be similar.
Brazil.
The Tapirapé indigenous people of Brazil allowed no more than three children per woman. Furthermore, no more than two had to be of the same sex. If the rule was broken infanticide was practiced. The people in the Bororo tribe killed all the newborns that did not appear healthy enough. Infanticide is also documented in the case of the Korubo people in the Amazon.
The Yanomami men killed children while raiding enemy villages. Helena Valero, a Brazilian woman kidnapped by Yanomami warriors in the 1930s, witnessed a Karawetari raid on her tribe:
"They killed so many. I was weeping for fear and for pity but there was nothing I could do. They snatched the children from their mothers to kill them, while the others held the mothers tightly by the arms and wrists as they stood up in a line. All the women wept. ... The men began to kill the children; little ones, bigger ones, they killed many of them.”.
Peru, Paraguay and Bolivia.
While Capacocha was practiced in the Peruvian large cities, child sacrifice in the pre-Columbian tribes of the region is less documented. However, even today studies on the Aymara Indians reveal high incidences of mortality among the newborn, especially female deaths, suggesting infanticide. The Abipones, a small tribe of Guaycuran stock, of about 5,000 by the end of the 18th century in Paraguay, practiced systematic infanticide; with never more than two children being reared in one family. The Machigenga killed their disabled children. Infanticide among the Chaco in Paraguay was estimated as high as 50% of all newborns in that tribe, who were usually buried. The infanticidal custom had such roots among the Ayoreo in Bolivia and Paraguay that it persisted until the late 20th century.
Modern times.
Infanticide has become less common in the Western world. The frequency has been estimated to be approximately 1 in 3000-5000 children of all ages and 2.1 per 100,000 newborns per year. It is thought that infanticide today continues at a much higher rate in areas of extremely high poverty and overpopulation, such as parts of China and India. Female infants, then and even now, are particularly vulnerable, a factor in sex-selective infanticide. Recent estimates suggest that over 100 million girls and women are 'missing' in Asia.
Benin.
In spite of the fact that it is illegal, in Benin, West Africa, parents secretly continue with infanticidal customs.
North Korea.
According to "The Hidden Gulag" published by the Committee for Human Rights in North Korea, the People's Republic of China returns all illegal immigrants from North Korea which usually imprisons them in a short term facility. Women who are suspected of being impregnated by Chinese fathers are subjected to forced abortions; babies born alive are killed, sometimes by exposure or being buried alive.
China.
There have been some accusations that infanticide occurs in the People's Republic of China due to the one-child policy. In the 1990s, a certain stretch of the Yangtze River was known to be a common site of infanticide by drowning, until government projects made access to it more difficult. Others assert that China has twenty-five million fewer girl children than expected, but this can partially be explained by sex selective abortion. The illegal use of ultrasound is widespread in China, and itinerant sonographers with plain vans in parking lots offer inexpensive sonographs to determine the sex of a fetus. Recent studies suggest that over 40 million girls and women are 'missing' in China (Klasen and Wink 2003).
India.
India has been ranked the most dangerous place to be born a girl according to UN figures.
The practice has continued in some rural areas of India. Infanticide is illegal in India.
According to a recent report by the United Nations Children's Fund (UNICEF) up to 50 million girls and women are missing in India's population as a result of systematic sex discrimination and sex selective abortions.
Pakistan.
Killings of newborn babies have been on the rise in Pakistan, corresponding to an increase in poverty across the country. More than 1,000 infants, mostly girls, have been killed or abandoned to die in Pakistan in 2009 according to a Pakistani charity organization.
The Edhi Foundation found 1,210 dead babies in 2010. Many more are abandoned and left at the doorsteps of mosques. As a result, Edhi centers feature signs "Do not murder, lay them here." Though female infanticide is punishable by life in prison, such crimes are rarely prosecuted.
Oceania.
In November 2008 it was reported that in Agibu and Amosa villages of Gimi region of Eastern Highlands province of Papua New Guinea where tribal fighting in the region of Gimi has been going on since 1986 (many of the clashes arising over claims of sorcery) women had agreed that if they stopped producing males, allowing only female babies to survive, their tribe's stock of boys would go down and there would be no men in the future to fight. They agreed to have all newborn male babies killed. It is not known how many male babies were killed by being smothered, but it had reportedly happened to all males over a 10-year period and probably was still happening.
England.
In England and Wales there were typically 30 to 50 homicides per million children less than 1 year old between 1982 and 1996. The younger the infant, the higher the risk. The rate for children 1 to 5 years was around 10 per million children. The homicide rate of infants less than 1 year is significantly higher than for the general population.
United States of America.
In 1983, the United States ranked eleventh for infants under 1 year killed, and fourth for those killed from 1 through 14 years (the latter case not necessarily involving filicide). In the U.S. over six hundred children were killed by their parents in 1983.
In the United States the infanticide rate during the first hour of life dropped from 1.41 per 100,000 during 1963 to 1972 to 0.44 per 100,000 for 1974 to 1983; the rates during the first month of life also declined, whereas those for older infants rose during this time. The legalization of abortion, which was completed in 1973, was the most important factor in the decline in neonatal mortality during the period from 1964 to 1977, according to a study by economists associated with the National Bureau of Economic Research.
Canada.
In Canada 114 cases of infanticide by a parent were reported during 1964-1968. Some of the cases that made news were those of Amy Grossberg and Brian Peterson, Genene Jones, Marybeth Tinning, Melissa Drexler, Dena Schlosser and Waneta Hoyt. There is ongoing debate in the Canadian legal and political fields about whether Section 233 of the Criminal Code of Canada, which creates the specific offence and partial defence of infanticide in Canadian law, should be amended or abolished altogether.
South Africa.
Between Johannesburg and Soweto (South Africa’s largest township) 200 babies a month are left for dead. Only 60 of those are found alive and taken to a place of safety. Babies are thrown into the street, the garbage, the river, sewers, and toilets, dropped from buildings and left in fields. In South Africa there is no Infanticide Act. The Children’s Act does not protect the lives of children. Therefore infanticide is rife and increasing rapidly due to overpopulation, poverty, teenage pregnancy and the alarming rape statistic in South Africa.
Modern proposals.
In a 2012 article in the "Journal of Medical Ethics", a philosopher and a bioethicist jointly proposed that infanticide be legalized, calling it "after-birth abortion", and claiming that both "the fetus and the newborn are potential persons". Many replies were published to this article.
Child euthanasia.
Euthanasia applied to children that are gravely ill or that suffer from significant birth defects is legal in the Netherlands under rigidly controlled conditions, but controversial. Some critics have compared child euthanasia to infanticide.
Explanations for the practice.
There are various reasons for infanticide. Neonaticide typically has different patterns and causes than for killing of older infants. Traditional neonaticide is often related to economic necessity - inability to provide for the infant.
In England and the United States, Older infants are typically killed for reasons related to child abuse, domestic violence or mental illness. For infants older than one day, younger infants are more at risk, and boys are more at risk than girls. Risk factors for the parent include: Family history of violence, violence in current relationship, history of abuse or neglect of children, and personality disorder and/or depression.
Religious.
In the late 17th and early 18th centuries, "loopholes" were invented by those who wanted to avoid the damnation that was promised by most Christian doctrine as a penalty of suicide. One famous example of someone who wished to end their life but avoid the eternity in hell was Christina Johansdotter (died 1740). She was a Swedish murderer who killed a child in Stockholm with the sole purpose of being executed. She is an example of those who seek suicide through execution by committing a murder. It was a common act, frequently targeting young children or infants as they were believed to be free from sin, thus going straight to heaven.
Economic.
Many historians believe the reason to be primarily economic, with more children born than the family is prepared to support. In societies that are patrilineal and patrilocal, the family may choose to allow more sons to live and kill some daughters, as the former will support their birth family until they die, whereas the latter will leave economically and geographically to join their husband's family, possibly only after the payment of a burdensome dowry price. Thus the decision to bring up a boy is more economically rewarding to the parents. However, this does not explain why infanticide would occur equally among rich and poor, nor why it would be as frequent during decadent periods of the Roman Empire as during earlier, less affluent, periods.
Before the appearance of effective contraception, infanticide was a common occurrence in ancient brothels. Unlike usual infanticide - where historically girls have been more likely to be killed - prostitutes in certain areas preferred to kill their male offspring.
UK 18th and 19th century.
Instances of infanticide in Britain in 18th and 19th century is often attributed to the economic position of the women, with juries committing pious perjury in many subsequent murder cases. The knowledge of the difficulties faced in the 18th century by those women who attempted to keep their children can be seen as reason for juries to show compassion. If the woman chose to keep the child, society was not set up to ease the pressure placed upon the woman, legally, socially or economically.
In mid-18th century Britain there was assistance available for women who were not able to raise their children. The Foundling Hospital opened in 1756 and was able to take in some of the illegitimate children. However, the conditions within the hospital caused Parliament to withdraw funding and the governors to live off of their own incomes. This resulted in a stringent entrance policy, with the committee requiring that the hospital:
'Will not receive a child that is more than a year old, nor the child of a domestic servant, nor any child whose father can be compelled to maintain it'.
Once a mother had admitted her child to the hospital, the hospital did all it could to ensure that the parent and child were not re-united.
Macfarlane argues in Illegitimacy and Illegitimates in Britain (1980) that English society greatly concerned itself with the burden that a bastard child places upon its communities and had gone to some lengths to ensure that the father of the child is identified in order to maintain its wellbeing. Assistance could be gained through maintenance payments from the father, however, this was capped ‘at a miserable 2s and 6d a week’. If the father got into arrears with the payments he could only be asked ‘to pay a maximum of 13 weeks arrears’.
Despite the accusations of some that women were getting a free hand-out there is evidence that many women were far from receiving adequate assistance from their parish. ‘Within Leeds in 1822 … relief was limited to 1s per week’. Sheffield required women to enter the workhouse, whereas Halifax gave no relief to the women who required it. The prospect of entering the workhouse was certainly something to be avoided. Lionel Rose quotes Dr Joseph Rogers in "Massacre of the Innocents …" (1986). Rogers, who was employed by a London workhouse in 1856 stated that conditions in the nursery were ‘wretchedly damp and miserable … [and] … overcrowded with young mothers and their infants’.
The loss of social standing for a servant girl was a particular problem in respect of producing a bastard child as they relied upon a good character reference in order to maintain their job and more importantly, to get a new or better job. In a large number of trials for the crime of infanticide, it is the servant girl that stood accused. The disadvantage of being a servant girl is that they had to live to the social standards of their superiors or risk dismissal and no references. Whereas within other professions, such as in the factory, the relationship between employer and employee was much more anonymous and the mother would be better able to make other provisions, such as employing a minder. The result of the lack of basic social care in Britain in the 18th and 19th century is the numerous accounts in court records of women, particularly servant girls, standing trial for the murder of their child.
There may have been no specific offence of infanticide in England before about 1623 because infanticide was a matter for the by ecclesiastical courts, possibly because Infant mortality from natural causes was high (about 15% or one in six).
Thereafter the accusation of the suppression of bastard children by lewd mothers was a crime incurring the presumption of guilt.
The Infanticide Acts are several laws. That of 1922 made the killing of an infant child by its mother during the early months of life as a lesser crime than murder. The acts of 1938 and 1939 abolished the earlier act, but introduced the idea that Postpartum depression was legally to be regarded as a form of diminished responsibility.
Population control.
Marvin Harris estimated that among Paleolithic hunters 23-50% of newborn children were killed. He argued that the goal was to preserve the 0.001% population growth of that time. He also wrote that female infanticide may be a form of population control. Population control is achieved not only by limiting the number of potential mothers; increased fighting among men for access to relatively scarce wives would also lead to a decline in population. For example, on the Melanesian island of Tikopia infanticide was used to keep a stable population in line with its resource base. Research by Marvin Harris and William Divale supports this argument, it has been cited as an example of environmental determinism.
Customs and taboos.
In 1888, Lieut. F. Elton reported that Ugi beach people in the Solomon Islands killed their infants at birth by burying them, and women were also said to practice abortion. They reported that it was too much trouble to raise a child, and instead preferred to buy one from the bush people. Larry S. Milner, author of "Hardness of Heart/Hardness of Life", a treatise on infanticide, believes that superstition has always reigned supreme in tribal religion. In chapters 9 through 21 Milner explores diverse customs and taboos as possible causes of infanticide, from punishment and shame to poverty, famine, revenge, depression and insanity and superstitious omens.
Psychological.
Evolutionary psychology.
Evolutionary psychology has proposed several theories for different forms of infanticide. Infanticide by stepfathers, as well as child abuse in general by stepfathers, has been explained by spending resources on not genetically related children reducing reproductive success (See the Cinderella effect and Infanticide (zoology)). Infanticide is one of the few forms of violence more often done by women than men. Cross-cultural research have found that this is more likely to occur when the child has deformities or illnesses as well as when there are lacking resources due to factors such as poverty, other children requiring resources, and no male support. Such a child may have a low chance of reproductive success in which case it would decrease the mother's inclusive fitness, in particular since women generally have a greater parental investment than men, to spend resources on the child. .
"Early infanticidal childrearing".
A minority of academics subscribe to an alternate school of thought, considering the practice as "early infanticidal childrearing". They attribute parental infanticidal wishes to massive projection or displacement of the parents' unconscious onto the child, because of intergenerational, ancestral abuse by their own parents. Clearly, an infanticidal parent may have multiple motivations, conflicts, emotions, and thoughts about their baby and their relationship with their baby, which are often colored both by their individual psychology, current relational context and attachment history, and, perhaps most saliently, their psychopathology (See also Psychiatric section below) Almeida, Merminod, and Schechter suggest that parents with fantasies, projections, and delusions involving infanticide need to be taken seriously and assessed carefully, whenever possible, by an interdisciplinary team that includes infant mental health specialists or mental health practitioners who have experience in working with parents, children, and families.
Wider effects.
In addition to debates over the morality of infanticide itself, there is some debate over the effects of infanticide on surviving children, and the effects of childrearing in societies that also sanction infanticide. Some argue that the practice of infanticide in any widespread form causes enormous psychological damage in children. Conversely, studying societies that practice infanticide Géza Róheim reported that even infanticidal mothers in New Guinea, who ate a child, did not affect the personality development of the surviving children; that "these are good mothers who eat their own children". Harris and Divale's work on the relationship between female infanticide and warfare suggests that there are, however, extensive negative effects.
Psychiatric.
Postpartum psychosis is also a causative factor of infanticide. Stuart S. Asch, MD, a Professor of Psychiatry at Cornell University established the connections between some cases of infanticide and post-partum depression., The books, "From Cradle to Grave", and "The Death of Innocents", describe selected cases of maternal infanticide and the investigative research of Professor Asch working in concert with the New York City Medical Examiner's Office.
Stanley Hopwood wrote that childbirth and lactation entail severe stress on the female sex, and that under certain circumstances attempts at infanticide and suicide are common. A study published in the "American Journal of Psychiatry" revealed that 44% of filicidal fathers had a diagnosis of psychosis. In addition to postpartum psychosis, dissociative psychopathology and sociopathy have also been found to be associated with neonaticide in some cases
In addition, severe postpartum depression can lead to infanticide.
Genetic.
Larry Milner writes in the concluding chapter of his study of infanticide:
So with this strata of support, I have concluded that it is a normal — a "natural"— trait for a human being to be willing to kill his or her own child, especially during the first year of life, and that there are genetic factors which are determinative of this compulsion.
However, Milner's treatise includes at the same time cultural hypotheses for the practice, and his approach to the subject has been criticized as both scholarly and an idealized view of infanticide.
Sex selection.
Sex selection may be one of the contributing factors of infanticide. In the absence of sex-selective abortion, sex-selective infanticide can be deduced from very skewed birth statistics. The biologically normal sex ratio for humans at birth is approximately 105 males per 100 females; normal ratios hardly ranging beyond 102-108. When a society has an infant male to female ratio which is significantly higher or lower than the biological norm, and biased data can be ruled out, sex selection can usually be inferred.
Current law.
India.
Infanticide is illegal in India, but rarely enforced in the rural parts of India.
Canada.
In Canada, a mother commits infanticide, a lesser offence than homicide, if she killed her child while "not fully recovered from the effects of giving birth to the child and by reason thereof or of the effect of lactation consequent on the birth of the child her mind is then disturbed".
England and Wales.
In England and Wales, the Infanticide Act 1938 describes the offence of infanticide as one which would otherwise amount to murder (by his/her mother) if the victim was older than 12 months and the mother was not suffering from an imbalance of mind due to the effects of childbirth or lactation. Where a mother who has killed such an infant has been charged with murder rather than infanticide s.1(3) of the Act confirms that a jury has the power to find alternative verdicts of Manslaughter in English law or guilty but insane.
Romania.
The Romanian Penal Code defines infanticide ("pruncucidere") as a distinct criminal offence, providing for a maximum punishment of 7 years imprisonment, recognizing the fact that a mother's judgement may be impaired immediately after birth. Romanian legislation, however, does not define the term "infant", and this has led to debates regarding the precise moment when infanticide becomes homicide. This issue is to be resolved by a new Penal Code, scheduled to come into force in 2014, which states that infanticide can only occur in the first 24 hours after birth. The new Code will also reduce the maximum sentence to 5 years imprisonment.
United States.
In 2009, Texas state representative Jessica Farrar proposed legislation that would define infanticide as a distinct and lesser crime than homicide. Under the terms of the proposed legislation, if jurors concluded that a mother's "judgment was impaired as a result of the effects of giving birth or the effects of lactation following the birth," they would be allowed to convict her of the crime of infanticide, rather than murder. The maximum penalty for infanticide would be two years in prison. Farrar's introduction of this bill prompted liberal bioethics scholar Jacob M. Appel to call her "the bravest politician in America."
Prevention.
Since infanticide, especially neonaticide, is often a response to an unwanted birth, preventing unwanted pregnancies through improved sex education and increased contraceptive access are advocated as ways of preventing infanticide. Increased use of contraceptives and access to safe legal abortions have greatly reduced neonaticide in many developed nations. Some say that where abortion is illegal, as in Pakistan, infanticide would decline if safer legal abortions were available.
Screening for psychiatric disorders or risk factors, and providing treatment or assistance to those at risk may help prevent infanticide. However in developed world significant proportions of neonaticides that are detected occur in young women who deny their pregnancy, and avoid outside contacts, so they may have limited contact with health care services.
In some areas baby hatches, safe places for a mother to anonymously leave an infant, are offered, in part to reduce the rate of infanticide. In other places, like the United States, safe-haven laws allow mothers to anonymously give infants to designated officials. Typically such babies are put up for adoption, or cared for in orphanages.
Granting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. As a result, the infant mortality rate will decrease and economic development will increase.
In animals.
Although human infanticide has been widely studied, the practice has been observed in many other species of the animal kingdom since it was first seriously studied by . These include from microscopic rotifers and insects, to fish, amphibians, birds and mammals, including primates such as chacma baboons. Infanticide can be practiced by both males and females.

</doc>
<doc id="15476" url="http://en.wikipedia.org/wiki?curid=15476" title="Internet protocol suite">
Internet protocol suite

The Internet protocol suite is the computer networking model and set of communications protocols used on the Internet and similar computer networks. It is commonly known as TCP/IP, because its most important protocols, the Transmission Control Protocol (TCP) and the Internet Protocol (IP), were the first networking protocols defined in this standard. Often also called the "Internet model", it was originally also known as the DoD model, because the development of the networking model was funded by DARPA, an agency of the United States Department of Defense.
TCP/IP provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed and received at the destination. Indeed, this functionality is organized into four abstraction layers which are used to sort all related protocols according to the scope of networking involved. From lowest to highest, the layers are the link layer, containing communication technologies for a single network segment (link); the internet layer, connecting hosts across independent networks, thus establishing internetworking; the transport layer handling host-to-host communication; and the application layer, which provides process-to-process application data exchange.
The TCP/IP model and related protocol models are maintained by the Internet Engineering Task Force (IETF).
History.
Early research.
The Internet protocol suite resulted from research and development conducted by the Defense Advanced Research Projects Agency (DARPA) in the late 1960s. After initiating the pioneering ARPANET in 1969, DARPA started work on a number of other data transmission technologies. In 1972, Robert E. Kahn joined the DARPA Information Processing Technology Office, where he worked on both satellite packet networks and ground-based radio packet networks, and recognized the value of being able to communicate across both. In the spring of 1973, Vinton Cerf, the developer of the existing ARPANET Network Control Program (NCP) protocol, joined Kahn to work on open-architecture interconnection models with the goal of designing the next protocol generation for the ARPANET.
By the summer of 1973, Kahn and Cerf had worked out a fundamental reformulation, in which the differences between network protocols were hidden by using a common internetwork protocol, and, instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann and Louis Pouzin, designer of the CYCLADES network, with important influences on this design.
The design of the network included the recognition that it should provide only the functions of efficiently transmitting and routing traffic between end nodes and that all other intelligence should be located at the edge of the network, in the end nodes. Using a simple design, it became possible to connect almost any network to the ARPANET, irrespective of the local characteristics, thereby solving Kahn's initial problem. One popular expression is that TCP/IP, the eventual product of Cerf and Kahn's work, will run over "two tin cans and a string." (Years later, as a joke, the IP over Avian Carriers formal protocol specification was created and successfully tested.)
A computer called a router is provided with an interface to each network. It forwards packets back and forth between them. Originally a router was called "gateway", but the term was changed to avoid confusion with other types of gateways.
Specification.
From 1973 to 1974, Cerf's networking research group at Stanford worked out details of the idea, resulting in the first TCP specification. A significant technical influence was the early networking work at Xerox PARC, which produced the PARC Universal Packet protocol suite, much of which existed around that time.
DARPA then contracted with BBN Technologies, Stanford University, and the University College London to develop operational versions of the protocol on different hardware platforms. Four versions were developed: TCP v1, TCP v2, TCP v3 and IP v3, and TCP/IP v4. The last protocol is still in use today.
In 1975, a two-network TCP/IP communications test was performed between Stanford and University College London (UCL). In November, 1977, a three-network TCP/IP test was conducted between sites in the US, the UK, and Norway. Several other TCP/IP prototypes were developed at multiple research centers between 1978 and 1983. The migration of the ARPANET to TCP/IP was officially completed on flag day January 1, 1983, when the new protocols were permanently activated.
Adoption.
In March 1982, the US Department of Defense declared TCP/IP as the standard for all military computer networking. In 1985, the Internet Advisory Board (later renamed the Internet Architecture Board) held a three-day workshop on TCP/IP for the computer industry, attended by 250 vendor representatives, promoting the protocol and leading to its increasing commercial use.
In 1985, the first Interop conference focused on network interoperability by broader adoption of TCP/IP. The conference was founded by Dan Lynch, an early Internet activist. From the beginning, large corporations, such as IBM and DEC, attended the meeting. Interoperability conferences have been held every year since then. Every year from 1985 through 1993, the number of attendees tripled.
IBM, AT&T and DEC were the first major corporations to adopt TCP/IP, despite having competing internal protocols (SNA, XNS, etc.). In IBM, from 1984, Barry Appelman's group did TCP/IP development. (Appelman later moved to AOL to be the head of all its development efforts.) They navigated the corporate politics to get a stream of TCP/IP products for various IBM systems, including MVS, VM, and OS/2. At the same time, several smaller companies began offering TCP/IP stacks for DOS and MS Windows, such as the company FTP Software, and the Wollongong Group. The first VM/CMS TCP/IP stack came from the University of Wisconsin.
Back then, most of these TCP/IP stacks were written single-handedly by a few talented programmers. For example, John Romkey of FTP Software was the author of the MIT PC/IP package. John Romkey's PC/IP implementation was the first IBM PC TCP/IP stack. Jay Elinsky and Oleg Vishnepolsky of IBM Research wrote TCP/IP stacks for VM/CMS and OS/2, respectively.
The spread of TCP/IP was fueled further in June 1989, when AT&T agreed to place the TCP/IP code developed for UNIX into the public domain. Various vendors, including IBM, included this code in their own TCP/IP stacks. Many companies sold TCP/IP stacks for Windows until Microsoft released a native TCP/IP stack in Windows 95. This event was a little late in the evolution of the Internet, but it cemented TCP/IP's dominance over other protocols, which eventually disappeared. These protocols included IBM Systems Network Architecture (SNA), Open Systems Interconnection (OSI), Microsoft's native NetBIOS, and Xerox Network Systems (XNS).
Key architectural principles.
An early architectural document, RFC 1122, emphasizes architectural principles over layering.
Abstraction layers.
The Internet protocol suite uses encapsulation to provide abstraction of protocols and services. Encapsulation is usually aligned with the division of the protocol suite into layers of general functionality. In general, an application (the highest level of the model) uses a set of protocols to send its data down the layers, being further encapsulated at each level.
The layers of the protocol suite near the top are logically closer to the user application, while those near the bottom are logically closer to the physical transmission of the data. Viewing layers as providing or consuming a service is a method of abstraction to isolate upper layer protocols from the details of transmitting bits over, for example, Ethernet and collision detection, while the lower layers avoid having to know the details of each and every application and its protocol.
Even when the layers are examined, the assorted architectural documents—there is no single architectural model such as ISO 7498, the Open Systems Interconnection (OSI) model—have fewer and less rigidly defined layers than the OSI model, and thus provide an easier fit for real-world protocols. One frequently referenced document, RFC 1958, does not contain a stack of layers. The lack of emphasis on layering is a major difference between the IETF and OSI approaches. It only refers to the existence of the internetworking layer and generally to "upper layers"; this document was intended as a 1996 snapshot of the architecture: "The Internet and its architecture have grown in evolutionary fashion from modest beginnings, rather than from a Grand Plan. While this process of evolution is one of the main reasons for the technology's success, it nevertheless seems useful to record a snapshot of the current principles of the Internet architecture."
RFC 1122, entitled "Host Requirements", is structured in paragraphs referring to layers, but the document refers to many other architectural principles not emphasizing layering. It loosely defines a four-layer model, with the layers having names, not numbers, as follows:
The Internet protocol suite and the layered protocol stack design were in use before the OSI model was established. Since then, the TCP/IP model has been compared with the OSI model in books and classrooms, which often results in confusion because the two models use different assumptions and goals, including the relative importance of strict layering.
This abstraction also allows upper layers to provide services that the lower layers do not provide. While the original OSI model was extended to include connectionless services (OSIRM CL), IP is not designed to be reliable and is a best effort delivery protocol. This means that all transport layer implementations must choose whether or how to provide reliability. UDP provides data integrity via a checksum but does not guarantee delivery; TCP provides both data integrity and delivery guarantee by retransmitting until the receiver acknowledges the reception of the packet.
This model lacks the formalism of the OSI model and associated documents, but the IETF does not use a formal model and does not consider this a limitation, as illustrated in the comment by David D. Clark, "We reject: kings, presidents and voting. We believe in: rough consensus and running code." Criticisms of this model, which have been made with respect to the OSI model, often do not consider ISO's later extensions to that model.
For multiaccess links with their own addressing systems (e.g. Ethernet) an address mapping protocol is needed. Such protocols can be considered to be below IP but above the existing link system. While the IETF does not use the terminology, this is a subnetwork dependent convergence facility according to an extension to the OSI model, the internal organization of the network layer (IONL).
ICMP & IGMP operate on top of IP but do not transport data like UDP or TCP. Again, this functionality exists as layer management extensions to the OSI model, in its "Management Framework" (OSIRM MF)
The SSL/TLS library operates above the transport layer (uses TCP) but below application protocols. Again, there was no intention, on the part of the designers of these protocols, to comply with OSI architecture.
The link is treated like a black box. The IETF explicitly does not intend to discuss transmission systems, which is a less academic but practical alternative to the OSI model.
The following is a description of each layer in the TCP/IP networking model starting from the lowest level.
Link layer.
The link layer has the networking scope of the local network connection to which a host is attached. This regime is called the "link" in TCP/IP literature. It is the lowest component layer of the Internet protocols, as TCP/IP is designed to be hardware independent. As a result TCP/IP may be implemented on top of virtually any hardware networking technology.
The link layer is used to move packets between the Internet layer interfaces of two different hosts on the same link. The processes of transmitting and receiving packets on a given link can be controlled both in the software device driver for the network card, as well as on firmware or specialized chipsets. These perform data link functions such as adding a packet header to prepare it for transmission, then actually transmit the frame over a physical medium. The TCP/IP model includes specifications of translating the network addressing methods used in the Internet Protocol to data link addressing, such as Media Access Control (MAC). All other aspects below that level, however, are implicitly assumed to exist in the link layer, but are not explicitly defined.
This is also the layer where packets may be selected to be sent over a virtual private network or other networking tunnel. In this scenario, the link layer data may be considered application data which traverses another instantiation of the IP stack for transmission or reception over another IP connection. Such a connection, or virtual link, may be established with a transport protocol or even an application scope protocol that serves as a tunnel in the link layer of the protocol stack. Thus, the TCP/IP model does not dictate a strict hierarchical encapsulation sequence.
The TCP/IP model's link layer corresponds to the Open Systems Interconnection (OSI) model physical and data link layers, layers one and two of the OSI model.
Internet layer.
The internet layer has the responsibility of sending packets across potentially multiple networks. Internetworking requires sending data from the source network to the destination network. This process is called routing.
The Internet Protocol performs two basic functions:
The internet layer is not only agnostic of data structures at the transport layer, but it also does not distinguish between operation of the various transport layer protocols. IP carries data for a variety of different upper layer protocols. These protocols are each identified by a unique protocol number: for example, Internet Control Message Protocol (ICMP) and Internet Group Management Protocol (IGMP) are protocols 1 and 2, respectively.
Some of the protocols carried by IP, such as ICMP which is used to transmit diagnostic information, and IGMP which is used to manage IP Multicast data, are layered on top of IP but perform internetworking functions. This illustrates the differences in the architecture of the TCP/IP stack of the Internet and the OSI model. The TCP/IP model's internet layer corresponds to layer three of the Open Systems Interconnection (OSI) model, where it is referred to as the network layer.
The internet layer provides only an unreliable datagram transmission facility between hosts located on potentially different IP networks by forwarding the transport layer datagrams to an appropriate next-hop router for further relaying to its destination. With this functionality, the internet layer makes possible internetworking, the interworking of different IP networks, and it essentially establishes the Internet. The Internet Protocol is the principal component of the internet layer, and it defines two addressing systems to identify network hosts' computers, and to locate them on the network. The original address system of the ARPANET and its successor, the Internet, is Internet Protocol version 4 (IPv4). It uses a 32-bit IP address and is therefore capable of identifying approximately four billion hosts. This limitation was eliminated by the standardization of Internet Protocol version 6 (IPv6) in 1998, and beginning production implementations in approximately 2006.
Transport layer.
The transport layer establishes a basic data channel that an application uses in its task-specific data exchange. The layer establishes process-to-process connectivity, meaning it provides end-to-end services that are independent of the structure of user data and the logistics of exchanging information for any particular specific purpose. Its responsibility includes end-to-end message transfer independent of the underlying network, along with error control, segmentation, flow control, congestion control, and application addressing (port numbers). End-to-end message transmission or connecting applications at the transport layer can be categorized as either connection-oriented, implemented in TCP, or connectionless, implemented in UDP.
For the purpose of providing process-specific transmission channels for applications, the layer establishes the concept of the port. This is a numbered logical construct allocated specifically for each of the communication channels an application needs. For many types of services, these port numbers have been standardized so that client computers may address specific services of a server computer without the involvement of service announcements or directory services.
Because IP provides only a best effort delivery, some transport layer protocols offer reliability. However, IP can run over a reliable data link protocol such as the High-Level Data Link Control (HDLC).
For example, the TCP is a connection-oriented protocol that addresses numerous reliability issues in providing a reliable byte stream:
The newer Stream Control Transmission Protocol (SCTP) is also a reliable, connection-oriented transport mechanism. It is message-stream-oriented—not byte-stream-oriented like TCP—and provides multiple streams multiplexed over a single connection. It also provides multi-homing support, in which a connection end can be represented by multiple IP addresses (representing multiple physical interfaces), such that if one fails, the connection is not interrupted. It was developed initially for telephony applications (to transport SS7 over IP), but can also be used for other applications.
The User Datagram Protocol is a connectionless datagram protocol. Like IP, it is a best effort, "unreliable" protocol. Reliability is addressed through error detection using a weak checksum algorithm. UDP is typically used for applications such as streaming media (audio, video, Voice over IP etc.) where on-time arrival is more important than reliability, or for simple query/response applications like DNS lookups, where the overhead of setting up a reliable connection is disproportionately large. Real-time Transport Protocol (RTP) is a datagram protocol that is designed for real-time data such as streaming audio and video.
The applications at any given network address are distinguished by their TCP or UDP port. By convention certain "well known ports" are associated with specific applications.
The TCP/IP model's transport or host-to-host layer corresponds to the fourth layer in the Open Systems Interconnection (OSI) model, also called the transport layer.
Application layer.
The application layer includes the protocols used by most applications for providing user services or exchanging application data over the network connections established by the lower level protocols, but this may include some basic network support services, such as many routing protocols, and host configuration protocols. Examples of application layer protocols include the Hypertext Transfer Protocol (HTTP), the File Transfer Protocol (FTP), the Simple Mail Transfer Protocol (SMTP), and the Dynamic Host Configuration Protocol (DHCP). Data coded according to application layer protocols are encapsulated into transport layer protocol units (such as TCP or UDP messages), which in turn use lower layer protocols to effect actual data transfer.
The IP model does not consider the specifics of formatting and presenting data, and does not define additional layers between the application and transport layers as in the OSI model (presentation and session layers). Such functions are the realm of libraries and application programming interfaces.
Application layer protocols generally treat the transport layer (and lower) protocols as black boxes which provide a stable network connection across which to communicate, although the applications are usually aware of key qualities of the transport layer connection such as the end point IP addresses and port numbers. Application layer protocols are often associated with particular client–server applications, and common services have "well-known" port numbers reserved by the Internet Assigned Numbers Authority (IANA). For example, the HyperText Transfer Protocol uses server port 80 and Telnet uses server port 23. Clients connecting to a service usually use ephemeral ports, i.e., port numbers assigned only for the duration of the transaction at random or from a specific range configured in the application.
The transport layer and lower-level layers are unconcerned with the specifics of application layer protocols. Routers and switches do not typically examine the encapsulated traffic, rather they just provide a conduit for it. However, some firewall and bandwidth throttling applications must interpret application data. An example is the Resource Reservation Protocol (RSVP). It is also sometimes necessary for network address translator (NAT) traversal to consider the application payload.
The application layer in the TCP/IP model is often compared as equivalent to a combination of the fifth (Session), sixth (Presentation), and the seventh (Application) layers of the Open Systems Interconnection (OSI) model.
Furthermore, the TCP/IP reference model distinguishes between "user protocols" and "support protocols". Support protocols provide services to a system. User protocols are used for actual user applications. For example, FTP is a user protocol and DNS is a system protocol.
Layer names and number of layers in the literature.
The following table shows various networking models. The number of layers varies between three and seven.
Some of the networking models are from textbooks, which are secondary sources that may conflict with the intent of RFC 1122 and other IETF primary sources.
Comparison of TCP/IP and OSI layering.
The three top layers in the OSI model—the application layer, the presentation layer and the session layer—are not distinguished separately in the TCP/IP model where it is just the application layer. While some pure OSI protocol applications, such as X.400, also combined them, there is no requirement that a TCP/IP protocol stack must impose monolithic architecture above the transport layer. For example, the NFS application protocol runs over the eXternal Data Representation (XDR) presentation protocol, which, in turn, runs over a protocol called Remote Procedure Call (RPC). RPC provides reliable record transmission, so it can safely use the best-effort UDP transport.
Different authors have interpreted the RFCs differently, about whether the link layer (and the TCP/IP model) covers OSI model layer 1 (physical layer) issues, or whether a hardware layer is assumed below the link layer. In order to address this ambiguity, several authors have attempted to incorporate the OSI model's layers 1 and 2 into the TCP/IP model, since these are commonly referred to in modern standards (for example, by IEEE and ITU). This often results in a model with five layers, where the link layer or network access layer is split into the OSI model's layers 1 and 2.
The IETF protocol development effort is not concerned with strict layering. Some of its protocols may not fit cleanly into the OSI model, although RFCs sometimes refer to it and often use the old OSI layer numbers. The IETF has repeatedly stated that Internet protocol and architecture development is not intended to be OSI-compliant. RFC 3439, addressing Internet architecture, contains a section entitled: "Layering Considered Harmful".
For example, the session and presentation layers of the OSI suite are considered to be included to the application layer of the TCP/IP suite. The functionality of the session layer can be found in traces in protocols like HTTP and SMTP and is more evident in protocols like Telnet and SIP. Session layer functionality is also realized with the port numbering of the TCP and UDP protocols, which cover the transport layer in the TCP/IP suite. Concerning the presentation layer, application layer protocols such as HTTP and SMTP can exercise its functionality by utilizing the MIME standard in data exchange.
Conflicts are apparent also in the original OSI model, ISO 7498, when not considering the annexes to this model (e.g., ISO 7498/4 Management Framework), or the ISO 8648 Internal Organization of the Network layer (IONL). When the IONL and Management Framework documents are considered, the ICMP and IGMP are neatly defined as layer management protocols for the network layer. In like manner, the IONL provides a structure for "subnetwork dependent convergence facilities" such as ARP and RARP.
IETF protocols can be encapsulated recursively, as demonstrated by tunneling protocols such as Generic Routing Encapsulation (GRE). GRE uses the same mechanism that OSI uses for tunneling at the network layer.
Implementations.
The Internet protocol suite does not presume any specific hardware or software environment. It only requires that hardware and a software layer exists that is capable of sending and receiving packets on a computer network. As a result, the suite has been implemented on essentially every computing platform. A minimal implementation of TCP/IP includes the following: Internet Protocol (IP), Address Resolution Protocol (ARP), Internet Control Message Protocol (ICMP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and IGMP. In addition to IP, ICMP, TCP, UDP, Internet Protocol version 6 requires Neighbor Discovery Protocol (NDP), ICMPv6, and IGMPv6 and is often accompanied by an integrated IPSec security layer.
Application programmers are typically concerned only with interfaces in the application layer and often also in the transport layer, while the layers below are services provided by the TCP/IP stack in the operating system. Most IP implementations are accessible to programmers through sockets and APIs.
Unique implementations include Lightweight TCP/IP, an open source stack designed for embedded systems, and KA9Q NOS, a stack and associated protocols for amateur packet radio systems and personal computers connected via serial lines.
Microcontroller firmware in the network adapter typically handles link issues, supported by driver software in the operating system. Non-programmable analog and digital electronics are normally in charge of the physical components below the link layer, typically using an application-specific integrated circuit (ASIC) chipset for each network interface or other physical standard. High-performance routers are to a large extent based on fast non-programmable digital electronics, carrying out link level switching.

</doc>
<doc id="15477" url="http://en.wikipedia.org/wiki?curid=15477" title="Ibn al-Shaykh al-Libi">
Ibn al-Shaykh al-Libi

Ibn al-Shaykh al-Libi (Arabic: إبْنُ ٱلشَّيْخِ اللّيبي‎; Ḁbnʋ ălŞɑỉƈ alLibi; born Ali Mohamed Abdul Aziz al-Fakheri, 1963 – May 10, 2009) was a Libyan national captured in Afghanistan in November 2001 after the fall of the Taliban; he was interrogated by the American and Egyptian forces. The information he gave under torture to Egyptian authorities was cited by the George W. Bush Administration in the months preceding its 2003 invasion of Iraq as evidence of a connection between Saddam Hussein and al-Qaeda. That information was frequently repeated by members of the Bush Administration, although reports from both the Central Intelligence Agency (CIA) and the Defense Intelligence Agency (DIA) strongly questioned its credibility, suggesting that al-Libi was "intentionally misleading" interrogators.
In 2006, the United States transferred al-Libi to Libya, where he was imprisoned by the government. He was reported to have tuberculosis. On May 19, 2009, the government reported that he had recently committed suicide in prison. Human Rights Watch, which had a couple of representatives who had recently visited him, called for an investigation into the circumstances of his death; "The New York Times" reported that Ayman al-Zawahiri had asserted that Libya had tortured al-Libi to death.
Training camp director.
In Afghanistan, al-Libi led the Al Khaldan training camp, where Zacarias Moussaoui and Ahmed Ressam trained for attacks in the United States. An associate of Abu Zubaydah, al-Libi had his assets frozen by the U.S. government following the September 11 attacks; it published a list of terrorists on September 26, 2002 who were covered by this restriction.
Al-Libi was captured by Pakistani officials in November 2001, as he attempted to flee Afghanistan following the collapse of the Taliban after the 2001 U.S. invasion of Afghanistan.
Department of Defense spokesmen used to routinely described the Khaldan training camp as an al-Qaeda training camp, and Al-Libi and Abu Zubaydah as senior members of al-Qaeda. But, during testimony at their Combatant Status Review Tribunals, several Guantanamo captives, including Zubaydah, described the Khaldan camp as having been run by a rival jihadist organization – one that did not support attacking civilians.
Cooperation with the FBI.
Al-Libi was turned over to the FBI and held at Bagram Air Base. When talking to the FBI interrogators Russell Fincher and Marty Mahon, he seemed "genuinely friendly" and spoke chiefly in English, calling for a translator only when necessary. He seemed to bond with Fincher, a devout Christian, and the two prayed together and discussed religion at length.
Al-Libi told the interrogators details about Richard Reid, a British citizen who had joined al-Qaeda and trained to carry out a suicide bombing of an airliner, which he unsuccessfully attempted on December 22, 2001. Al-Libi agreed to continue cooperating if the United States would allow his wife and her family to emigrate, while he was prosecuted within the American legal system.
In CIA custody.
The CIA asked President Bush for permission to take al-Libi into their own custody and rendition him to a foreign country for more "tough guy" questioning, and were granted permission. They "simply came and took al-Libi away from the FBI." One CIA officer was heard telling their new prisoner that "You know where you are going. Before you get there, I am going to find your mother and fuck her".
In the second week of January 2002, al-Libi was flown to the USS "Bataan" in the northern Arabian Sea, a ship being used to hold eight other notable prisoners, including John Walker Lindh. He was subsequently transferred to Egyptian interrogators.
Information provided.
According to the "Washington Post",
Under questioning, al-Libi provided the CIA with intelligence about an alleged plot to blow up the U.S. Embassy in Yemen with a truck bomb and pointed officials in the direction of Abu Zubaydah, a top al Qaeda leader known to have been involved in the Sept. 11 plot.
On September 15, 2002, "Time" published an article that detailed the CIA interrogations of Omar al-Faruq. It said,
"On Sept. 9, according to a secret CIA summary of the interview, al-Faruq confessed that he was, in fact, al-Qaeda's senior representative in Southeast Asia. Then came an even more shocking confession: according to the CIA document, al-Faruq said two senior al-Qaeda officials, Abu Zubaydah and Ibn al-Shaykh al-Libi, had ordered him to 'plan large-scale attacks against U.S. interests in Indonesia, Malaysia, (the) Philippines, Singapore, Thailand, Taiwan, Vietnam and Cambodia.'"
Al-Libi has been identified as a principal source of faulty prewar intelligence regarding chemical weapons training between Iraq and al-Qaeda that was used by the Bush Administration to justify the invasion of Iraq. Specifically, he told interrogators that Iraq provided training to al-Qaeda in the area of "chemical and biological weapons". In Cincinnati in October 2002, Bush informed the public: "Iraq has trained al Qaeda members in bomb making and poisons and gases."
This claim was repeated several times in the run-up to the war, including in then-Secretary of State Colin Powell's speech to the U.N Security Council on February 5, 2003, which concluded with a long recitation of the information provided by al-Libi. Powell's speech was made less than a month after a then-classified CIA report concluded that the information provided by al-Libi was unreliable, and about a year after a DIA report concluded the same thing.
Al-Libi recanted these claims in January 2004 after U.S. interrogators presented "new evidence from other detainees that cast doubt on his claims", according to "Newsweek". The DIA concluded in February 2002 that al-Libi deliberately misled interrogators, in what the CIA called an "attempt to exaggerate his importance". Some speculate that his reason for giving disinformation was in order to draw the U.S. into an attack on Iraq—Islam's "weakest" state; a remark attributed to al-Libi—which al-Qaeda believes will lead to a global jihad. Others, including al-Libi himself, have insisted that he gave false information due to the use of torture (so-called "enhanced interrogation techniques").
An article published in the November 5, 2005 "New York Times" quoted two paragraphs of a Defense Intelligence Agency report, declassified upon request by Senator Carl Levin, that expressed doubts about the results of al-Libi's interrogation in February 2002.
Al-Libi told a foreign intelligence service that:
"Iraq — acting on the request of al-Qa'ida militant Abu Abdullah, who was Muhammad Atif's emissary — agreed to provide unspecified chemical or biological weapons training for two al-Qa'ida associates beginning in December 2000. The two individuals departed for Iraq but did not return, so al-Libi was not in a position to know if any training had taken place." 
The September 2002 version of "Iraqi Support for Terrorism" stated that al-Libi said Iraq had "provided" chemical and biological weapons training for two al-Qaeda associates in 2000, but also stated that al-Libi "did not know the results of the training."
The 2006 Senate Report on Pre-war Intelligence on Iraq stated that "Although DIA coordinated on CIA's "Iraqi Support for Terrorism" paper, DIA analysis preceding that assessment was more skeptical of the al-Libi reporting." In July 2002, DIA assessed "It is plausible al-Qa'ida attempted to obtain CB assistance from Iraq and Ibn al-Shaykh is sufficiently senior to have access to such sensitive information. However, Ibn al-Shaykh's information lacks details concerning the individual Iraqis involved, the specific CB materials associated with the assistance and the location where the alleged training occurred. The information is also second hand, and not derived from Ibn al-Shaykh's personal experience."
The Senate report also states "According to al-Libi, after his decision to fabricate information for debriefers, he 'lied about being a member of al-Qa'ida. Although he considered himself close to, but not a member of, al-Qa'ida, he knew enough about the senior members, organization and operations to claim to be a member.'"
Senate Reports on Pre-war Intelligence on Iraq.
On September 8, 2006, the United States Senate Select Committee on Intelligence released "Phase II" of its report on prewar intelligence on Iraq. Conclusion 3 of the report states the following:
Postwar findings support the DIA February 2002 assessment that Ibn al-Shaykh al-Libi was likely intentionally misleading his debriefers when he said that Iraq provided two al-Qa'ida associates with chemical and biological weapons (CBW) training in 2000… Postwar findings do not support the CIA's assessment that his reporting was credible… No postwar information has been found that indicates CBW training occurred and the detainee who provided the key prewar reporting about this training recanted his claims after the war… CIA's January 2003 version of Iraqi Support for Terrorism described al-Libi's reporting for CBW training "credible", but noted that the individuals who traveled to Iraq for CBW training had not returned, so al-Libi was not in position to know if the training had taken place… In January 2004, al-Libi recanted his allegations about CBW training and many of his other claims about Iraq's links to al Qa'ida. He told debriefers that, to the best of his knowledge, al-Qa'ida never sent any individuals into Iraq for any kind of support in chemical or biological weapons. Al-libi told debriefers that he fabricated information while in U.S. custody to receive better treatment and in response to threats of being transferred to a foreign intelligence service which he believed would torture him… He said that later, while he was being debriefed by a (REDACTED) foreign intelligence service, he fabricated more information in response to physical abuse and threats of torture. The foreign government service denies using any pressure during al-Libi's interrogation. In February 2004, the CIA reissued the debriefing reports from al-Libi to note that he had recanted information. A CIA officer explained that while CIA believes al-Libi fabricated information, the CIA cannot determine whether, or what portions of, the original statements or the later recants are true or false.
On June 11, 2008 "Newsweek" published an account of material from a "previously undisclosed CIA report written in the summer of 2002". The article reported that on August 7, 2002 CIA analysts had drafted a high-level report that expressed serious doubts about the information flowing from al-Libi's interrogation. The information that al-Libi acknowledged being a member of al-Qaeda's executive council was not supported by other sources. According to al-Libi, in Egypt he was locked in a tiny box less than 20 inches high and held for 17 hours and after being let out he was thrown to the floor and punched for 15 minutes. According to CIA operational cables, only then did he tell his "fabricated" story about al-Qaeda members being dispatched to Iraq.
Book: "Inside the Jihad".
In November 2006, a Moroccan using the pseudonym Omar Nasiri, having infiltrated al-Qaeda in the 1990s, wrote the book, "". In the book, Nasiri claims that al-Libi deliberately planted information to encourage the U.S. to invade Iraq. In an interview with BBC2's "Newsnight", Nasiri said Libi "needed the conflict in Iraq because months before I heard him telling us when a question was asked in the mosque after the prayer in the evening, where is the best country to fight the jihad?" Nasiri said that Libi had identified Iraq as the "weakest" Muslim country. He suggested to "Newsnight" that al-Libi wanted to overthrow Saddam and use Iraq as a jihadist base. Nasiri describes al-Libi as one of the leaders at the Afghan camp, and characterizes him as "brilliant in every way." He said that learning how to withstand interrogations and supply false information was a key part of the training in the camps. Al-Libi "knew what his interrogators wanted, and he was happy to give it to them. He wanted to see Saddam toppled even more than the Americans did."
Book: "At the Center of the Storm".
In April 2007 former Director of Central Intelligence George Tenet released his memoir titled "". With regard to al-Libi, Tenet writes the following:
We believed that al-Libi was withholding critical threat information at the time, so we transferred him to a third country for further debriefing. Allegations were made that we did so knowing that he would be tortured, but this is false. The country in question understood and agreed that they would hold al-Libi for a limited period. In the course of questioning while he was in U.S. custody in Afghanistan, al-Libi made initial references to possible al-Qa'ida training in Iraq. He offered up information that a militant known as Abu Abdullah had told him that at least three times between 1997 and 2000, the now-deceased al-Qa'ida leader Mohammad Atef had sent Abu Abdullah to Iraq to seek training in poisons and mustard gas. Another senior al-Qa'ida detainee told us that Mohammad Atef was interested in expanding al-Qa-ida's ties to Iraq, which, in our eyes, added credibility to the reporting. Then, shortly after the Iraq war got under way, al-Libi recanted his story. Now, suddenly, he was saying that there was no such cooperative training. Inside the CIA, there was sharp division on his recantation. It led us to recall his reporting, and here is where the mystery begins. Al-Libi's story will no doubt be that he decided to fabricate in order to get better treatment and avoid harsh punishment. He clearly lied. We just don't know when. Did he lie when he first said that al-Qa'ida members received training in Iraq or did he lie when he said they did not? In my mind, either case might still be true. Perhaps, early on, he was under pressure, assumed his interrogators already knew the story, and sang away. After time passed and it became clear that he would not be harmed, he might have changed his story to butt the minds of his captors. Al-Qa'ida operatives are trained to do just that. A recantation would restore his stature as someone who had successfully confounded the enemy. The fact is, we don't know which story is true, and since we don't know, we can assume nothing.
Repatriation to Libya and death.
In 2006 the Bush Administration announced that it was transferring high-value al-Qaeda detainees from CIA secret prisons so they could be put on trial by military commissions.
But the Administration was conspicuously silent about al-Libi.
Noman Benotman, a former Mujahideen who knew Libi, told "Newsweek" that during a recent trip to Tripoli, he met with a senior Libyan government official who confirmed to him that Libi had been transferred to Libya and was being held in prison there. He was suffering from tuberculosis.
On May 10, 2009, the English language edition of the Libyan newspaper "Ennahar" reported that the government said that Al-Libi had been repatriated to Libyan custody in 2006, and had recently committed suicide by hanging. It attributed the information to another newspaper, "Oea". "Ennahar" reported Al-Libi's real name was Ali Mohamed Abdul Aziz Al-Fakheri. It stated he was 46 years old, and had been allowed visits with international human rights workers from Human Rights Watch. The story was widely reported by other media outlets.
Al-Libi had been visited in April 2009 by a team from Human Rights Watch. His sudden death so soon after this visit has led human rights organisations and Islamic groups to question whether it was truly a suicide. Clive Stafford Smith, Legal Director of the UK branch of the human rights group Reprieve, said, "We are told that al-Libi committed suicide in his Libyan prison. If this is true it would be because of his torture and abuse. If false, it may reflect a desire to silence one of the greatest embarrassments to the Bush administration." Hafed Al-Ghwell, a Libya expert and director of communications at the Dubai campus of Harvard's Kennedy School of Government, commented,
"This is a regime with a long history of killing people in jail and then claiming it was suicide. My guess is Libya has seen the winds of change in America and wanted to bury this man before international organisations start demanding access to him."
On June 19, 2009, Andy Worthington published new information on al-Libi's death. Worthington gave a detailed timeline of Al Libi's last years.
The head of the Washington office of Human Rights Watch said al-Libi was "Exhibit A" in hearings on the relationship between pre-Iraq War false intelligence and torture. Confirmation of al-Libi's location came two weeks prior to his death. An independent investigation of his death has been requested by Human Rights Watch.
On October 4, 2009 the "Reuters" reported that Ayman Al Zawahiri, the head of al-Qaeda, had asserted that Libya had caused al-Libi's death through torture.

</doc>
<doc id="15478" url="http://en.wikipedia.org/wiki?curid=15478" title="IDF">
IDF

IDF or idf may refer to:

</doc>
<doc id="15487" url="http://en.wikipedia.org/wiki?curid=15487" title="International Red Cross and Red Crescent Movement">
International Red Cross and Red Crescent Movement

The International Red Cross and Red Crescent Movement is an international humanitarian movement with approximately 97 million volunteers, members and staff worldwide which was founded to protect human life and health, to ensure respect for all human beings, and to prevent and alleviate human suffering.
The movement consists of several distinct organizations that are legally independent from each other, but are united within the movement through common basic principles, objectives, symbols, statutes and governing organisations. The movement's parts are:
History of Movement.
The International Committee of the Red Cross.
Solferino, Jean-Henri Dunant and the foundation of the ICRC.
Until the middle of the 19th century, there were no organized and/or well-established army nursing systems for casualties and no safe and protected institutions to accommodate and treat those who were wounded on the battlefield. In June 1859, the Swiss businessman Jean-Henri Dunant traveled to Italy to meet French emperor Napoléon III with the intention of discussing difficulties in conducting business in Algeria, at that time occupied by France. When he arrived in the small town of Solferino on the evening of June 24, he toured the field of the Battle of Solferino, an engagement in the Austro-Sardinian War. In a single day, about 40,000 soldiers on both sides died or were left wounded on the field. Jean-Henri Dunant was shocked by the terrible aftermath of the battle, the suffering of the wounded soldiers, and the near-total lack of medical attendance and basic care. He completely abandoned the original intent of his trip and for several days he devoted himself to helping with the treatment and care for the wounded. He succeeded in organizing an overwhelming level of relief assistance by motivating the local villagers to aid without discrimination.
Back in his home in Geneva, he decided to write a book entitled "A Memory of Solferino" which he published with his own money in 1862. He sent copies of the book to leading political and military figures throughout Europe. In addition to penning a vivid description of his experiences in Solferino in 1859, he explicitly advocated the formation of national voluntary relief organizations to help nurse wounded soldiers in the case of war. In addition, he called for the development of international treaties to guarantee the protection of neutral medics and field hospitals for soldiers wounded on the battlefield.
In 1863, Gustave Moynier, a Geneva lawyer and president of the Geneva Society for Public Welfare, received a copy of Dunant's book and introduced it for discussion at a meeting of that society. As a result of this initial discussion the society established an investigatory commission to examine the feasibility of Dunant's suggestions and eventually to organize an international conference about their possible implementation. The members of this committee, which has subsequently been referred to as the "Committee of the Five," aside from Dunant and Moynier were physician Louis Appia, who had significant experience working as a field surgeon; Appia's friend and colleague Théodore Maunoir, from the Geneva Hygiene and Health Commission; and Guillaume-Henri Dufour, a Swiss Army general of great renown. Eight days later, the five men decided to rename the committee to the "International Committee for Relief to the Wounded". In October (26–29) 1863, the international conference organized by the committee was held in Geneva to develop possible measures to improve medical services on the battlefield. The conference was attended by 36 individuals: eighteen official delegates from national governments, six delegates from other non-governmental organizations, seven non-official foreign delegates, and the five members of the International Committee. The states and kingdoms represented by official delegates were:
 France
Among the proposals written in the final resolutions of the conference, adopted on October 29, 1863, were:
Only one year later, the Swiss government invited the governments of all European countries, as well as the United States, Brazil, and Mexico, to attend an official diplomatic conference. Sixteen countries sent a total of twenty-six delegates to Geneva. On August 22, 1864, the conference adopted the first Geneva Convention "for the Amelioration of the Condition of the Wounded in Armies in the Field". Representatives of 12 states and kingdoms signed the convention: Baden, Belgium, Denmark, France, Hesse, Italy, the Netherlands, Portugal, Prussia, Switzerland, Spain, and Württemberg. The convention contained ten articles, establishing for the first time legally binding rules guaranteeing neutrality and protection for wounded soldiers, field medical personnel, and specific humanitarian institutions in an armed conflict. Furthermore, the convention defined two specific requirements for recognition of a national relief society by the International Committee:
Directly following the establishment of the Geneva Convention, the first national societies were founded in Belgium, Denmark, France, Oldenburg, Prussia, Spain, and Württemberg. Also in 1864, Louis Appia and Charles van de Velde, a captain of the Dutch Army, became the first independent and neutral delegates to work under the symbol of the Red Cross in an armed conflict. Three years later in 1867, the first International Conference of National Aid Societies for the Nursing of the War Wounded was convened.
Also in 1867, Jean-Henri Dunant was forced to declare bankruptcy due to business failures in Algeria, partly because he had neglected his business interests during his tireless activities for the International Committee. Controversy surrounding Dunant's business dealings and the resulting negative public opinion, combined with an ongoing conflict with Gustave Moynier, led to Dunant's expulsion from his position as a member and secretary. He was charged with fraudulent bankruptcy and a warrant for his arrest was issued. Thus, he was forced to leave Geneva and never returned to his home city.
In the following years, national societies were founded in nearly every country in Europe. In 1876, the committee adopted the name "International Committee of the Red Cross" (ICRC), which is still its official designation today. Five years later, the American Red Cross was founded through the efforts of Clara Barton. More and more countries signed the Geneva Convention and began to respect it in practice during armed conflicts. In a rather short period of time, the Red Cross gained huge momentum as an internationally respected movement, and the national societies became increasingly popular as a venue for volunteer work.
When the first Nobel Peace Prize was awarded in 1901, the Norwegian Nobel Committee opted to give it jointly to Jean-Henri Dunant and Frédéric Passy, a leading international pacifist. More significant than the honor of the prize itself, the official congratulation from the International Committee of the Red Cross marked the overdue rehabilitation of Jean-Henri Dunant and represented a tribute to his key role in the formation of the Red Cross. Dunant died nine years later in the small Swiss health resort of Heiden. Only two months earlier his long-standing adversary Gustave Moynier had also died, leaving a mark in the history of the Committee as its longest-serving president ever.
In 1906, the 1864 Geneva Convention was revised for the first time. One year later, the Hague Convention X, adopted at the Second International Peace Conference in The Hague, extended the scope of the Geneva Convention to naval warfare. Shortly before the beginning of the First World War in 1914, 50 years after the foundation of the ICRC and the adoption of the first Geneva Convention, there were already 45 national relief societies throughout the world. The movement had extended itself beyond Europe and North America to Central and South America (Argentina, Brazil, Chile, Cuba, Mexico, Peru, El Salvador, Uruguay, Venezuela), Asia (the Republic of China, Japan, Korea, Siam), and Africa (Union of South Africa).
The ICRC during World War I.
With the outbreak of World War I, the ICRC found itself confronted with enormous challenges that it could handle only by working closely with the national Red Cross societies. Red Cross nurses from around the world, including the United States and Japan, came to support the medical services of the armed forces of the European countries involved in the war. On October 15, 1914, immediately after the start of the war, the ICRC set up its International Prisoners-of-War (POW) Agency, which had about 1,200 mostly volunteer staff members by the end of 1914. By the end of the war, the Agency had transferred about 20 million letters and messages, 1.9 million parcels, and about 18 million Swiss francs in monetary donations to POWs of all affected countries. Furthermore, due to the intervention of the Agency, about 200,000 prisoners were exchanged between the warring parties, released from captivity and returned to their home country. The organizational card index of the Agency accumulated about 7 million records from 1914 to 1923, each card representing an individual prisoner or missing person. The card index led to the identification of about 2 million POWs and the ability to contact their families. The complete index is on loan today from the ICRC to the International Red Cross and Red Crescent Museum in Geneva. The right to access the index is still strictly restricted to the ICRC.
During the entire war, the ICRC monitored warring parties’ compliance with the Geneva Conventions of the 1907 revision and forwarded complaints about violations to the respective country. When chemical weapons were used in this war for the first time in history, the ICRC vigorously protested against this new type of warfare. Even without having a mandate from the Geneva Conventions, the ICRC tried to ameliorate the suffering of civil populations. In territories that were officially designated as "occupied territories," the ICRC could assist the civilian population on the basis of the Hague Convention's "Laws and Customs of War on Land" of 1907. This convention was also the legal basis for the ICRC's work for prisoners of war. In addition to the work of the International Prisoner-of-War Agency as described above this included inspection visits to POW camps. A total of 524 camps throughout Europe were visited by 41 delegates from the ICRC until the end of the war.
Between 1916 and 1918, the ICRC published a number of postcards with scenes from the POW camps. The pictures showed the prisoners in day-to-day activities such as the distribution of letters from home. The intention of the ICRC was to provide the families of the prisoners with some hope and solace and to alleviate their uncertainties about the fate of their loved ones. After the end of the war, the ICRC organized the return of about 420,000 prisoners to their home countries. In 1920, the task of repatriation was handed over to the newly founded League of Nations, which appointed the Norwegian diplomat and scientist Fridtjof Nansen as its "High Commissioner for Repatriation of the War Prisoners." His legal mandate was later extended to support and care for war refugees and displaced persons when his office became that of the League of Nations "High Commissioner for Refugees." Nansen, who invented the "Nansen passport" for stateless refugees and was awarded the Nobel Peace Prize in 1922, appointed two delegates from the ICRC as his deputies.
A year before the end of the war, the ICRC received the 1917 Nobel Peace Prize for its outstanding wartime work. It was the only Nobel Peace Prize awarded in the period from 1914 to 1918. In 1923, the International Committee of the Red Cross adopted a change in its policy regarding the selection of new members. Until then, only citizens from the city of Geneva could serve in the Committee. This limitation was expanded to include Swiss citizens. As a direct consequence of World War I, an additional protocol to the Geneva Convention was adopted in 1925 which outlawed the use of suffocating or poisonous gases and biological agents as weapons. Four years later, the original Convention was revised and the second Geneva Convention "relative to the Treatment of Prisoners of War" was established. The events of World War I and the respective activities of the ICRC significantly increased the reputation and authority of the Committee among the international community and led to an extension of its competencies.
As early as in 1934, a draft proposal for an additional convention for the protection of the civil population during an armed conflict was adopted by the International Red Cross Conference. Unfortunately, most governments had little interest in implementing this convention, and it was thus prevented from entering into force before the beginning of World War II.
The ICRC and World War II.
The legal basis of the work of the ICRC during World War II were the Geneva Conventions in their 1929 revision. The activities of the Committee were similar to those during World War I: visiting and monitoring POW camps, organizing relief assistance for civilian populations, and administering the exchange of messages regarding prisoners and missing persons. By the end of the war, 179 delegates had conducted 12,750 visits to POW camps in 41 countries. The Central Information Agency on Prisoners-of-War ("Zentralauskunftsstelle für Kriegsgefangene") had a staff of 3,000, the card index tracking prisoners contained 45 million cards, and 120 million messages were exchanged by the Agency. One major obstacle was that the Nazi-controlled German Red Cross refused to cooperate with the Geneva statutes including blatant violations such as the deportation of Jews from Germany and the mass murders conducted in the Nazi concentration camps. Moreover, two other main parties to the conflict, the Soviet Union and Japan, were not party to the 1929 Geneva Conventions and were not legally required to follow the rules of the conventions.
During the war, the ICRC was unable to obtain an agreement with Nazi Germany about the treatment of detainees in concentration camps, and it eventually abandoned applying pressure in order to avoid disrupting its work with POWs. The ICRC was also unable to obtain a response to reliable information about the extermination camps and the mass killing of European Jews, Roma, et al. After November 1943, the ICRC achieved permission to send parcels to concentration camp detainees with known names and locations. Because the notices of receipt for these parcels were often signed by other inmates, the ICRC managed to register the identities of about 105,000 detainees in the concentration camps and delivered about 1.1 million parcels, primarily to the camps Dachau, Buchenwald, Ravensbrück, and Sachsenhausen.
It is known that Swiss army officer Maurice Rossel during World War II had been sent to Berlin as a delegate of the International Red Cross, as such he visited Auschwitz 1943 and Theresienstadt 1944. Claude Lanzmann recorded his experiences in 1979, producing a documentary entitled "Visitor from the living".
On March 12, 1945, ICRC president Jacob Burckhardt received a message from SS General Ernst Kaltenbrunner accepting the ICRC's demand to allow delegates to visit the concentration camps. This agreement was bound by the condition that these delegates would have to stay in the camps until the end of the war. Ten delegates, among them Louis Haefliger (Camp Mauthausen), Paul Dunant (Camp Theresienstadt) and Victor Maurer (Camp Dachau), accepted the assignment and visited the camps. Louis Haefliger prevented the forceful eviction or blasting of Mauthausen-Gusen by alerting American troops, thereby saving the lives of about 60,000 inmates. His actions were condemned by the ICRC because they were deemed as acting unduly on his own authority and risking the ICRC's neutrality. Only in 1990, his reputation was finally rehabilitated by ICRC president Cornelio Sommaruga.
Another example of great humanitarian spirit was Friedrich Born (1903–1963), an ICRC delegate in Budapest who saved the lives of about 11,000 to 15,000 Jewish people in Hungary. Marcel Junod (1904–1961), a physician from Geneva, was another famous delegate during the Second World War. An account of his experiences, which included being one of the first foreigners to visit Hiroshima after the atomic bomb was dropped, can be found in the book "Warrior without Weapons".
In 1944, the ICRC received its second Nobel Peace Prize. As in World War I, it received the only Peace Prize awarded during the main period of war, 1939 to 1945. At the end of the war, the ICRC worked with national Red Cross societies to organize relief assistance to those countries most severely affected. In 1948, the Committee published a report reviewing its war-era activities from September 1, 1939 to June 30, 1947. Since January 1996, the ICRC archive for this period has been open to academic and public research.
The ICRC after World War II.
On August 12, 1949, further revisions to the existing two Geneva Conventions were adopted. An additional convention "for the Amelioration of the Condition of Wounded, Sick and Shipwrecked Members of Armed Forces at Sea", now called the second Geneva Convention, was brought under the Geneva Convention umbrella as a successor to the 1907 Hague Convention X. The 1929 Geneva convention "relative to the Treatment of Prisoners of War" may have been the second Geneva Convention from a historical point of view (because it was actually formulated in Geneva), but after 1949 it came to be called the third Convention because it came later chronologically than the Hague Convention. Reacting to the experience of World War II, the Fourth Geneva Convention, a new Convention "relative to the Protection of Civilian Persons in Time of War," was established. Also, the additional protocols of June 8, 1977 were intended to make the conventions apply to internal conflicts such as civil wars. Today, the four conventions and their added protocols contain more than 600 articles, a remarkable expansion when compared to the mere 10 articles in the first 1864 convention.
In celebration of its centennial in 1963, the ICRC, together with the League of Red Cross Societies, received its third Nobel Peace Prize. Since 1993, non-Swiss individuals have been allowed to serve as Committee delegates abroad, a task which was previously restricted to Swiss citizens. Indeed, since then, the share of staff without Swiss citizenship has increased to about 35%.
On October 16, 1990, the UN General Assembly decided to grant the ICRC observer status for its assembly sessions and sub-committee meetings, the first observer status given to a private organization. The resolution was jointly proposed by 138 member states and introduced by the Italian ambassador, Vieri Traxler, in memory of the organization's origins in the Battle of Solferino. An agreement with the Swiss government signed on March 19, 1993, affirmed the already long-standing policy of full independence of the Committee from any possible interference by Switzerland. The agreement protects the full sanctity of all ICRC property in Switzerland including its headquarters and archive, grants members and staff legal immunity, exempts the ICRC from all taxes and fees, guarantees the protected and duty-free transfer of goods, services, and money, provides the ICRC with secure communication privileges at the same level as foreign embassies, and simplifies Committee travel in and out of Switzerland.
At the end of the Cold War, the ICRC's work actually became more dangerous. In the 1990s, more delegates lost their lives than at any point in its history, especially when working in local and internal armed conflicts. These incidents often demonstrated a lack of respect for the rules of the Geneva Conventions and their protection symbols. Among the slain delegates were:
Afghanistan.
ICRC is active in the Afghanistan conflict areas and has set up six physical rehabilitation centers to help landmine victims. Their support extends to the national and international armed forces, civilians and the armed opposition. They regularly visit detainees under the custody of the Afghan government and the international armed forces, but have also occasionally had access since 2009 to people detained by the Taliban. They have provided basic first aid training and aid kits to both the Afghan security forces and Taliban members because, according to an ICRC spokesperson, "ICRC's constitution stipulates that all parties harmed by warfare will be treated as fairly as possible".
The International Federation of Red Cross and Red Crescent Societies (IFRC).
History.
In 1919, representatives from the national Red Cross societies of Britain, France, Italy, Japan, and the US came together in Paris to found the "League of Red Cross Societies". The original idea was Henry Davison's, then president of the American Red Cross. This move, led by the American Red Cross, expanded the international activities of the Red Cross movement beyond the strict mission of the ICRC to include relief assistance in response to emergency situations which were not caused by war (such as man-made or natural disasters). The ARC already had great disaster relief mission experience extending back to its foundation.
The formation of the League, as an additional international Red Cross organization alongside the ICRC, was not without controversy for a number of reasons. The ICRC had, to some extent, valid concerns about a possible rivalry between both organizations. The foundation of the League was seen as an attempt to undermine the leadership position of the ICRC within the movement and to gradually transfer most of its tasks and competencies to a multilateral institution. In addition to that, all founding members of the League were national societies from countries of the Entente or from associated partners of the Entente. The original statutes of the League from May 1919 contained further regulations which gave the five founding societies a privileged status and, due to the efforts of Henry P. Davison, the right to permanently exclude the national Red Cross societies from the countries of the Central Powers, namely Germany, Austria, Hungary, Bulgaria and Turkey, and in addition to that the national Red Cross society of Russia. These rules were contrary to the Red Cross principles of universality and equality among all national societies, a situation which furthered the concerns of the ICRC.
The first relief assistance mission organized by the League was an aid mission for the victims of a famine and subsequent typhus epidemic in Poland. Only five years after its foundation, the League had already issued 47 donation appeals for missions in 34 countries, an impressive indication of the need for this type of Red Cross work. The total sum raised by these appeals reached 685 million Swiss francs, which were used to bring emergency supplies to the victims of famines in Russia, Germany, and Albania; earthquakes in Chile, Persia, Japan, Colombia, Ecuador, Costa Rica, and Turkey; and refugee flows in Greece and Turkey. The first large-scale disaster mission of the League came after the 1923 earthquake in Japan which killed about 200,000 people and left countless more wounded and without shelter. Due to the League's coordination, the Red Cross society of Japan received goods from its sister societies reaching a total worth of about $100 million. Another important new field initiated by the League was the creation of youth Red Cross organizations within the national societies.
A joint mission of the ICRC and the League in the Russian Civil War from 1917 to 1922 marked the first time the movement was involved in an internal conflict, although still without an explicit mandate from the Geneva Conventions. The League, with support from more than 25 national societies, organized assistance missions and the distribution of food and other aid goods for civil populations affected by hunger and disease. The ICRC worked with the Russian Red Cross society and later the society of the Soviet Union, constantly emphasizing the ICRC's neutrality. In 1928, the "International Council" was founded to coordinate cooperation between the ICRC and the League, a task which was later taken over by the "Standing Commission". In the same year, a common statute for the movement was adopted for the first time, defining the respective roles of the ICRC and the League within the movement.
During the Abyssinian war between Ethiopia and Italy from 1935 to 1936, the League contributed aid supplies worth about 1.7 million Swiss francs. Because the Italian fascist regime under Benito Mussolini refused any cooperation with the Red Cross, these goods were delivered solely to Ethiopia. During the war, an estimated 29 people lost their lives while being under explicit protection of the Red Cross symbol, most of them due to attacks by the Italian Army. During the Civil War in Spain from 1936 to 1939 the League once again joined forces with the ICRC with the support of 41 national societies. In 1939 on the brink of the Second World War, the League relocated its headquarters from Paris to Geneva to take advantage of Swiss neutrality.
In 1952, the 1928 common statute of the movement was revised for the first time. Also, the period of decolonization from 1960 to 1970 was marked by a huge jump in the number of recognized national Red Cross and Red Crescent societies. By the end of the 1960s, there were more than 100 societies around the world. On December 10, 1963, the Federation and the ICRC received the Nobel Peace Prize. In 1983, the League was renamed to the "League of Red Cross and Red Crescent Societies" to reflect the growing number of national societies operating under the Red Crescent symbol. Three years later, the seven basic principles of the movement as adopted in 1965 were incorporated into its statutes. The name of the League was changed again in 1991 to its current official designation the "International Federation of Red Cross and Red Crescent Societies". In 1997, the ICRC and the IFRC signed the Seville Agreement which further defined the responsibilities of both organizations within the movement. In 2004, the IFRC began its largest mission to date after the tsunami disaster in South Asia. More than 40 national societies have worked with more than 22,000 volunteers to bring relief to the countless victims left without food and shelter and endangered by the risk of epidemics.
Presidents of the IFRC.
As of November 2009, the president of the IFRC is Tadateru Konoe (Japanese Red Cross). The vice presidents are Paul Bierch (Kenya), Jaslin Uriah Salmon (Jamaica), Mohamed El Maadid (Qatar) and Bengt Westerberg (Sweden).
Former presidents (until 1977 titled "Chairman") have been:
Activities.
Organization of the Movement.
Altogether, there are about 97 million people worldwide who serve with the ICRC, the International Federation, and the National Societies, the majority with the latter.
The 1965 International Conference in Vienna adopted seven basic principles which should be shared by all parts of the Movement, and they were added to the official statutes of the Movement in 1986.
Fundamental Principles of the International Red Cross and Red Crescent Movement.
At the 20th International Conference in Neue Hofburg, Vienna, from 2nd – 9th October 1965, "proclaimed" seven fundamental principles which are shared by all components of the Movement, and they were added to the official statutes of the Movement in 1986. The durability and universal acceptance is a result of the process through which they came into being in the form they have. Rather than an effort to arrive at agreement, it was an attempt to answer the question of what did they have in common, over the past 100 years, those operations and organisational units that were successful? As a result, the Fundamental Principles of the Red Cross and Red Crescent were not revealed, but "found" - through a deliberate and participative process of discovery.
That makes it even more important to note that the text that appears under each "heading" is an integral part of the Principle in question and not an interpretation that can vary with time and place.
Humanity
The International Red Cross and Red Crescent Movement, born of a desire to bring assistance without discrimination to the wounded on the battlefield, endeavours, in its international and national capacity, to prevent and alleviate human suffering wherever it may be found. Its purpose is to protect life and health and to ensure respect for the human being. It promotes mutual understanding, friendship, cooperation and lasting peace amongst all peoples.
Impartiality
It makes no discrimination as to nationality, race, religious beliefs, class or political opinions. It endeavours to relieve the suffering of individuals, being guided solely by their needs, and to give priority to the most urgent cases of distress.
Neutrality
In order to continue to enjoy the confidence of all, the Movement may not take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.
Independence
The Movement is independent. The National Societies, while auxiliaries in the humanitarian services of their governments and subject to the laws of their respective countries, must always maintain their autonomy so that they may be able at all times to act in accordance with the principles of the Movement.
Voluntary Service
It is a voluntary relief movement not prompted in any manner by desire for gain. 
Unity
There can be only one Red Cross or one Red Crescent Society in any one country. It must be open to all. It must carry on its humanitarian work throughout its territory.
Universality
The International Red Cross and Red Crescent Movement, in which all Societies have equal status and share equal responsibilities and duties in helping each other, is worldwide.
Activities and organization of the International Conference and the Standing Commission.
The International Conference of the Red Cross and Red Crescent, which occurs once every four years, is the highest institutional body of the Movement. It gathers delegations from all of the national societies as well as from the ICRC, the IFRC and the signatory states to the Geneva Conventions. In between the conferences, the Standing Commission of the Red Cross and Red Crescent acts as the supreme body and supervises implementation of and compliance with the resolutions of the conference. In addition, the Standing Commission coordinates the cooperation between the ICRC and the IFRC. It consists of two representatives from the ICRC (including its president), two from the IFRC (including its president), and five individuals who are elected by the International Conference. The Standing Commission convenes every six months on average. Moreover, a convention of the Council of Delegates of the Movement takes place every two years in the course of the conferences of the General Assembly of the International Federation. The Council of Delegates plans and coordinates joint activities for the Movement.
Activities and Organization of the ICRC.
The mission of the ICRC and its responsibilities within the Movement.
The official mission of the ICRC as an impartial, neutral, and independent organization is to stand for the protection of the life and dignity of victims of international and internal armed conflicts. According to the 1997 Seville Agreement, it is the "Lead Agency" of the Movement in conflicts. The core tasks of the Committee, which are derived from the Geneva Conventions and its own statutes, are the following:
Legal status and organization.
The ICRC is headquartered in the Swiss city of Geneva and has external offices in about 80 countries. It has about 12,000 staff members worldwide, about 800 of them working in its Geneva headquarters, 1,200 expatriates with about half of them serving as delegates managing its international missions and the other half being specialists like doctors, agronomists, engineers or interpreters, and about 10,000 members of individual national societies working on site. Contrary to popular belief, the ICRC is not a non-governmental organization in the most common sense of the term, nor is it an international organization. As it limits its members (a process called cooptation) to Swiss nationals only, it does not have a policy of open and unrestricted membership for individuals like other legally defined NGOs. The word "international" in its name does not refer to its membership but to the worldwide scope of its activities as defined by the Geneva Conventions. The ICRC has special privileges and legal immunities in many countries, based on national law in these countries or through agreements between the Committee and respective national governments. According to Swiss law, the ICRC is defined as a private association. According to its statutes it consists of 15 to 25 Swiss-citizen members, which it coopts for a period of four years. There is no limit to the number of terms an individual member can have although a three-quarters majority of all members is required for re-election after the third term.
The leading organs of the ICRC are the Directorate and the Assembly. The Directorate is the executive body of the Committee. It consists of a General Director and five directors in the areas of "Operations", "Human Resources", "Resources and Operational Support", "Communication", and "International Law and Cooperation within the Movement". The members of the Directorate are appointed by the Assembly to serve for four years. The Assembly, consisting of all of the members of the Committee, convenes on a regular basis and is responsible for defining aims, guidelines, and strategies and for supervising the financial matters of the Committee. The president of the Assembly is also the president of the Committee as a whole. Furthermore, the Assembly elects a five-member Assembly Council which has the authority to decide on behalf of the full Assembly in some matters. The Council is also responsible for organizing the Assembly meetings and for facilitating communication between the Assembly and the Directorate.
Due to Geneva's location in the French-speaking part of Switzerland, the ICRC usually acts under its French name "Comité international de la Croix-Rouge" (CICR). The official symbol of the ICRC is the Red Cross on white background with the words "COMITE INTERNATIONAL GENEVE" circling the cross.
Funding and financial matters.
The 2009 budget of the ICRC amounts more than 1 billion Swiss francs. Most of that money comes from the States, including Switzerland in its capacity as the depositary state of the Geneva Conventions, from national Red Cross societies, the signatory states of the Geneva Conventions, and from international organizations like the European Union. All payments to the ICRC are voluntary and are received as donations based on two types of appeals issued by the Committee: an annual "Headquarters Appeal" to cover its internal costs and "Emergency Appeals" for its individual missions.
The ICRC is asking donors for more than 1.1 billion Swiss francs to fund its work in 2010. Afghanistan is projected to become the ICRC’s biggest humanitarian operation (at 86 million Swiss francs, an 18% increase over the initial 2009 budget), followed by Iraq (85 million francs) and Sudan (76 million francs). The initial 2010 field budget for medical activities of 132 million francs represents an increase of 12 million francs over 2009.
Activities and organization of the International Federation.
The Mission of the IFRC and its responsibilities within the Movement.
The IFRC coordinates cooperation between national Red Cross and Red Crescent societies throughout the world and supports the foundation of new national societies in countries where no official society exists. On the international stage, the IFRC organizes and leads relief assistance missions after emergencies such as natural disasters, manmade disasters, epidemics, mass refugee flights, and other emergencies. As per the 1997 Seville Agreement, the IFRC is the Lead Agency of the Movement in any emergency situation which does not take place as part of an armed conflict. The IFRC cooperates with the national societies of those countries affected – each called the "Operating National Society" (ONS) – as well as the national societies of other countries willing to offer assistance – called "Participating National Societies" (PNS). Among the 187 national societies admitted to the General Assembly of the International Federation as full members or observers, about 25–30 regularly work as PNS in other countries. The most active of those are the American Red Cross, the British Red Cross, the German Red Cross, and the Red Cross societies of Sweden and Norway. Another major mission of the IFRC which has gained attention in recent years is its commitment to work towards a codified, worldwide ban on the use of land mines and to bring medical, psychological, and social support for people injured by land mines.
The tasks of the IFRC can therefore be summarized as follows:
Legal status and organization.
The IFRC has its headquarters in Geneva. It also runs five zone offices (Africa, Americas, Asia Pacific, Europe, Middle East-North Africa), 14 permanent regional offices and has about 350 delegates in more than 60 delegations around the world. The legal basis for the work of the IFRC is its constitution. The executive body of the IFRC is a secretariat, led by a Secretary General. The secretariat is supported by five divisions including "Programme Services", "Humanitarian values and humanitarian diplomacy", "National Society and Knowledge Development" and "Governance and Management Services".
The highest decision making body of the IFRC is its General Assembly, which convenes every two years with delegates from all of the national societies. Among other tasks, the General Assembly elects the Secretary General. Between the convening of General Assemblies, the Governing Board is the leading body of the IFRC. It has the authority to make decisions for the IFRC in a number of areas. The Governing Board consists of the president and the vice presidents of the IFRC, the chairpersons of the Finance and Youth Commissions, and twenty elected representatives from national societies.
The symbol of the IFRC is the combination of the Red Cross (left) and Red Crescent (right) on a white background surrounded by a red rectangular frame.
Funding and financial matters.
The main parts of the budget of the IFRC are funded by contributions from the national societies which are members of the IFRC and through revenues from its investments. The exact amount of contributions from each member society is established by the Finance Commission and approved by the General Assembly. Any additional funding, especially for unforeseen expenses for relief assistance missions, is raised by "appeals" published by the IFRC and comes for voluntary donations by national societies, governments, other organizations, corporations, and individuals.
National societies within the Movement.
Official recognition of a national society.
National Red Cross and Red Crescent societies exist in nearly every country in the world. Within their home country, they take on the duties and responsibilities of a national relief society as defined by International Humanitarian Law. Within the Movement, the ICRC is responsible for legally recognizing a relief society as an official national Red Cross or Red Crescent society. The exact rules for recognition are defined in the statutes of the Movement. Article 4 of these statutes contains the "Conditions for recognition of National Societies."
Once a National Society has been recognized by the ICRC as a component of the International Red Cross and Red Crescent Movement (the Movement), it is in principle admitted to the International Federation of Red Cross and Red Crescent Societies in accordance with the terms defined in the Constitution and Rules of Procedure of the International Federation.
There are today 189 National Societies recognized within the Movement and which are members of the International Federation.
The most recent National Societies to have been recognized within the Movement are the Maldives Red Crescent Society (9 November 2011), the Cyprus Red Cross Society and the South Sudan Red Cross Society (12 November 2013).
Activities of national societies on a national and international stage.
Despite formal independence regarding its organizational structure and work, each national society is still bound by the laws of its home country. In many countries, national Red Cross and Red Crescent societies enjoy exceptional privileges due to agreements with their governments or specific "Red Cross Laws" granting full independence as required by the International Movement. The duties and responsibilities of a national society as defined by International Humanitarian Law and the statutes of the Movement include humanitarian aid in armed conflicts and emergency crises such as natural disasters through activities such as Restoring Family Links.
Depending on their respective human, technical, financial, and organizational resources, many national societies take on additional humanitarian tasks within their home countries such as blood donation services or acting as civilian Emergency Medical Service (EMS) providers. The ICRC and the International Federation cooperate with the national societies in their international missions, especially with human, material, and financial resources and organizing on-site logistics.
History of the emblems.
Emblems in use.
The Red Cross.
The Red Cross emblem was officially approved in Geneva in 1863.
The Red Cross flag is not to be confused with the St George's Cross which is on the flag of England, Barcelona, Freiburg, and several other places. In order to avoid this confusion the protected symbol is sometimes referred to as the "Greek Red Cross"; that term is also used in United States law to describe the Red Cross. The red cross of the St George cross extends to the edge of the flag, whereas the red cross on the Red Cross flag does not.
The Red Cross flag is the colour-switched version of the Flag of Switzerland. In 1906, to put an end to the argument of Turkey that the flag took its roots from Christianity, it was decided to promote officially the idea that the Red Cross flag had been formed by reversing the federal colours of Switzerland, although no clear evidence of this origin had ever been found.
The Red Crescent.
The Red Crescent emblem was first used by ICRC volunteers during the armed conflict between the Ottoman Empire and Russia (1877–1878). The symbol was officially adopted in 1929, and so far 33 Islamic states have recognized it.
The Red Crystal.
On December 8, 2005, in response to growing pressure to accommodate Magen David Adom, Israel's national emergency medical, disaster, ambulance and blood bank service, as a full member of the Red Cross and Red Crescent movement, a new emblem (officially the Third Protocol Emblem, but more commonly known as the Red Crystal) was adopted by an amendment of the Geneva Conventions known as Protocol III.
Recognized emblems in disuse.
The Red Lion and Sun.
The Red Lion and Sun Society of Iran was established in 1922 and admitted to the Red Cross and Red Crescent movement in 1923. However, some report the symbol was introduced at Geneva in 1864 as a counter example to the crescent and cross used by two of Iran's rivals, the Ottoman and the Russian empires. Though that claim is inconsistent with the Red Crescent's history, that history also suggests that the Red Lion and Sun, like the Red Crescent, may have been conceived during the 1877–1878 war between Russia and Turkey.
In 1980, Islamic Republic of Iran replaced the Red Lion and Sun with the Red Crescent, consistent with two existing Red Cross and Red Crescent symbols. Though the Red Lion and Sun has now fallen into disuse, Iran has in the past reserved the right to take it up again at any time; the Geneva Conventions continue to recognize it as an official emblem, and that status was confirmed by Protocol III in 2005 even as it added the Red Crystal.
Unrecognized emblems.
The Red Star of David (Magen David Adom).
For over 50 years, Israel requested the addition of a red Star of David, arguing that since Christian and Muslim emblems were recognized, the corresponding Jewish emblem should be as well. This emblem has been used since 1935 by Magen David Adom (MDA), or Red Star of David, the national first-aid society of Israel, but it is not recognized by the Geneva Conventions as a protected symbol.
The Red Cross and Red Crescent movement repeatedly rejected Israel's request over the years, stating that the Red Cross emblem was not meant to represent Christianity but was a colour reversal of the Swiss flag, and also that if Jews (or another group) were to be given another emblem, there would be no end to the number of religious or other groups claiming an emblem for themselves, although the movement recognised the Muslim Red Crescent (Which is actually a colour reversal of the Ottoman flag). They reasoned that a proliferation of red symbols would detract from the original intention of the Red Cross emblem, which was to be a single emblem to mark vehicles and buildings protected on humanitarian grounds.
Certain Arab nations, such as Syria, also protested against the entry of MDA into the Red Cross movement, making consensus impossible for a time.
However, from 2000 to 2006 the American Red Cross withheld its dues (a total of $42 million) to the International Federation of Red Cross and Red Crescent Societies (IFRC) because of IFRC's refusal to admit MDA; this ultimately led to the creation of the Red Crystal emblem and the admission of MDA on June 22, 2006.
The Red Star of David is not recognized as a protected symbol outside Israel; instead the MDA uses the Red Crystal emblem during international operations in order to ensure protection. Depending on the circumstances, it may place the Red Star of David inside the Red Crystal, or use the Red Crystal alone.
1996 hostage crisis allegations.
The Australian TV network ABC and the indigenous rights group Friends of Peoples Close to Nature released a documentary called "Blood on the Cross" in 1999. It alleged the involvement of the Red Cross with the British and Indonesian military in a massacre in the Southern Highlands of West Papua during the World Wildlife Fund hostage crisis of May 1996, when Western and Indonesian activists were held hostage by separatists.
Following the broadcast of the documentary, the Red Cross announced publicly that it would appoint an individual outside the organization to investigate the allegations made in the film and any responsibility on its part. Piotr Obuchowicz was appointed to investigate the matter. The report categorically states that the Red Cross personnel accused of involvement were proven not to have been present; that a white helicopter was probably used in a military operation, but the helicopter was not a Red Cross helicopter, and must have been painted by one of several military organizations operating in the region at the time. Perhaps the Red Cross logo itself was also used, although no hard evidence was found for this; that this was part of the military operation to free the hostages, but was clearly intended to achieve surprise by deceiving the local people into thinking that a Red Cross helicopter was landing; and that the Red Cross should have responded more quickly and thoroughly to investigate the allegations than it did.

</doc>
<doc id="15489" url="http://en.wikipedia.org/wiki?curid=15489" title="Ira Gershwin">
Ira Gershwin

Ira Gershwin (December 6, 1896 – August 17, 1983) was an American lyricist who collaborated with his younger brother, composer George Gershwin, to create some of the most memorable songs of the 20th century.
With George he wrote more than a dozen Broadway shows, featuring songs such as "I Got Rhythm", "Embraceable You", "The Man I Love" and "Someone to Watch Over Me". He was also responsible, along with DuBose Heyward, for the libretto to George's opera "Porgy and Bess".
The success the brothers had with their collaborative works has often overshadowed the creative role that Ira played. However, his mastery of songwriting continued after the early death of George. He wrote additional hit songs with composers Jerome Kern ("Long Ago (and Far Away)"), Kurt Weill and Harold Arlen.
His critically acclaimed book "Lyrics on Several Occasions" of 1959, an amalgam of autobiography and annotated anthology, is an important source for studying the art of the lyricist in the golden age of American popular song.
Biography.
Gershwin was born Israel Gershowitz in New York City to Morris (Moishe) and Rose Gershovitz who changed the family name to Gershvin well before their children rose to fame (it was not spelled "Gershwin" until later). Shy in his youth, he spent much of his time at home reading, but from grammar school through college he played a prominent part in several school newspapers and magazines. He graduated from Townsend Harris High School in 1914, where he met Yip Harburg, with whom he enjoyed a lifelong friendship, and a love of Gilbert and Sullivan. He attended the City College of New York but dropped out.
The childhood home of Ira and George Gershwin was in the center of the Yiddish Theater District, on the second floor at 91 Second Avenue, between East 5th Street and East 6th Street. They frequented the local Yiddish theaters.
While his younger brother began composing and "plugging" in Tin Pan Alley from the age of eighteen, Ira worked as a cashier in his father's Turkish baths. It was not until 1921 that Ira became involved in the music business. Alex Aarons signed Ira to write the songs for his next show, "Two Little Girls in Blue" (written under the pseudonym "Arthur Francis"), ultimately produced by Abraham Erlanger, along with co-composers Vincent Youmans and Paul Lannin. Gershwin's lyrics were well received, and allowed him to successfully enter the show-business world with just one show. Later the same year the Gershwins collaborated for the first time on a score, for "A Dangerous Maid", which played in Atlantic City and on tour.
It was not until 1924 that Ira and George Gershwin teamed up to write the music for their first Broadway hit "Lady, Be Good". Once the brothers joined forces, their combined talents became one of the most influential forces in the history of American Musical Theatre. "When the Gershwins teamed up to write songs for "Lady, Be Good", the American musical found its native idiom." Together, they wrote the music for more than twelve shows and four films. Some of their more famous works include "The Man I Love", "Fascinating Rhythm", "Someone to Watch Over Me", "I Got Rhythm" and "They Can't Take That Away from Me". Their partnership continued until George's sudden death from a brain tumor in 1937. Following his brother's death, Ira waited nearly three years before writing again.
After this temporary retirement, he teamed up with such accomplished composers as Jerome Kern ("Cover Girl"); Kurt Weill ("Where Do We Go from Here?" and "Lady in the Dark"); and Harold Arlen (""; "A Star Is Born"). Over the next fourteen years, Gershwin continued to write the lyrics for many film scores and a few Broadway shows. But the failure of "Park Avenue" in 1946, a "smart" show about divorce, co-written with composer Arthur Schwartz, was his farewell to Broadway. As he wrote at the time, "Am reading a couple of stories for possible musicalization (if there is such a word) but I hope I don't like them as I think I deserve a long rest." In 1947, he took eleven songs George had written but never used, provided them with new lyrics, and incorporated them into the Betty Grable film "The Shocking Miss Pilgrim" and he later wrote comic lyrics for Billy Wilder's movie "Kiss Me, Stupid" (although most critics believe his final major work was for the 1954 Judy Garland film "A Star Is Born").
American singer, pianist and musical historian Michael Feinstein worked for Gershwin in the lyricist's latter years, helping him with his archive. Several lost musical treasures were unearthed during this period, and Feinstein performed some of the material. Feinstein's book "The Gershwins and Me: A Personal History in Twelve Songs" about working for Ira, and George and Ira's music was published in 2012.
According to a 1999 story in Vanity Fair, Ira Gershwin’s love for loud music was as great as his wife’s loathing of it. When Debby Boone—daughter-in-law of his neighbor Rosemary Clooney—returned from Japan with one of the first Sony Walkmans (utilizing cassette tape), Clooney gave it to Michael Feinstein to give to Ira, "so he could crank it in his ears, you know. And he said, ‘This is absolutely wonderful!’ And he called his broker and bought Sony stock!"
Awards and honors.
Three of Gershwin's songs ("They Can't Take That Away From Me" (1937), "Long Ago (And Far Away)" (1944) and "The Man That Got Away" (1954)) were nominated for an Academy Award for Best Original Song, though none won.
Gershwin, along with George S Kaufman and Morrie Ryskind, was a recipient of the 1932 Pulitzer Prize for Drama for "Of Thee I Sing".
The George and Ira Gershwin Lifetime Musical Achievement Award was established in 1988 by UCLA to honor the brothers for their contribution to music and for their gift to UCLA of the fight song "Strike Up the Band for UCLA". Past winners have included Angela Lansbury (1988), Ray Charles (1991), Mel Tormé (1994), Bernadette Peters (1995), Frank Sinatra (2000), Stevie Wonder (2002), k.d. lang (2003), James Taylor (2004), Babyface (2005), Burt Bacharach (2006), Quincy Jones (2007), Lionel Richie (2008) and Julie Andrews (2009).
Legacy.
Ira Gershwin was a joyous listener to the sounds of the modern world. "He had a sharp eye and ear for the minutiae of living." He noted in a diary: "Heard in a day: An elevator's purr, telephone's ring, telephone's buzz, a baby's moans, a shout of delight, a screech from a 'flat wheel', hoarse honks, a hoarse voice, a tinkle, a match scratch on sandpaper, a deep resounding boom of dynamiting in the impending subway, iron hooks on the gutter."
In 1987, Ira's widow, Leonore Gershwin, established the Ira Gershwin Literacy Center at University Settlement, a century-old institution at 185 Eldridge Street on the Lower East Side, New York City. The Center is designed to give English-language programs to primarily Hispanic and Chinese Americans. Ira and his younger brother George spent many after-school hours at the Settlement.
The George and Ira Gershwin Collection is at the Library of Congress Music Division. The Edward Jablonski and Lawrence D. Stewart Gershwin Collection at the Harry Ransom Humanities Research Center at the University of Texas at Austin holds a number of Ira's manuscripts and other material.
In 2007, the United States Library of Congress named its Prize for Popular Song after him and his brother George. Recognizing the profound and positive effect of American popular music on the world's culture, the prize will be given annually to a composer or performer whose lifetime contributions exemplify the standard of excellence associated with the Gershwins.
Personal life.
He married Leonore (née Strunsky) in 1926. He died in Beverly Hills, California, on August 17, 1983 at the age of 86. He is interred at Westchester Hills Cemetery, Hastings-on-Hudson, New York.

</doc>
<doc id="15490" url="http://en.wikipedia.org/wiki?curid=15490" title="Indus River">
Indus River

The Indus River also called Sindhu River is one of the longest rivers in Asia. It flows through Pakistan, the Indian states of Jammu and Kashmir and Gujarat, and western Tibet (China). Originating in the Tibetan Plateau in the vicinity of Lake Mansarovar, the river runs a course through the Ladakh region of Jammu and Kashmir, towards Gilgit-Baltistan and then flows in a southerly direction along the entire length of Pakistan to merge into the Arabian Sea near the port city of Karachi in Sindh. The total length of the river is 3,180 km. It is Pakistan's longest river.
The river has a total drainage area exceeding 1,165,000 km2. Its estimated annual flow stands at around 207 km3, making it the twenty-first largest river in the world in terms of annual flow. The Zanskar is its left bank tributary in Ladakh. In the plains, its left bank tributary is the Chenab which itself has four major tributaries, namely, the Jhelum, the Ravi, the Beas and the Sutlej. Its principal right bank tributaries are the Shyok, the Gilgit, the Kabul, the Gomal and the Kurram. Beginning in a mountain spring and fed with glaciers and rivers in the Himalayas, the river supports ecosystems of temperate forests, plains and arid countryside.
The Indus forms the delta of present-day Pakistan mentioned in the Vedic Rigveda as "Sapta Sindhu" and the Iranian Zend Avesta as "Hapta Hindu" (both terms meaning "seven rivers"). The river has been a source of wonder since the Classical Period, with King Darius of Persia sending his Greek subject Scylax of Caryanda to explore the river as early as 510 BC.
Etymology and names.
The word "Indus" is the romanised form of the ancient Greek word "Indós" ("Ἰνδός"), borrowed from the old Persian word "Hinduš", which in turn was derived from the Sanskrit word "Sindhu" (सिन्धु ]). The word "Sindhu" or "Sindh" is still the local appellation of the Indus River. The original Sanskrit word "Sindhu" is an amalgamation of two words, "sim" (region or entirety or border) and "dhu" (to tremble or shake) and means "a body of trembling water, river, stream or ocean".
Megasthenes's book "Indica" derives its name from the river's Greek name, "Indós" ("Ἰνδός"), and describes Nearchus's contemporaneous account of how Alexander the Great crossed the river. The ancient Greeks referred to the Indians (people of present-day India and Pakistan) as "Indói" ("Ἰνδοί"), literally meaning "the people of the Indus". The country of India and the Pakistani province of Sindh owe their names to the river.
Rigveda and the Indus.
Rigveda also describes several mythical rivers, including one named "Sindhu". The Rigvedic "Sindhu" is thought to be the present-day Indus river and is attested 176 times in its text – 95 times in the plural, more often used in the generic meaning. In the Rigveda, notably in the later hymns, the meaning of the word is narrowed to refer to the Indus river in particular, as in the list of rivers mentioned in the hymn of "Nadistuti sukta". The Rigvedic hymns apply a feminine gender to all the rivers mentioned therein but "Sindhu" is the only river attributed with a masculine gender. Sindhu is seen as a strong warrior amongst other rivers which are seen as goddesses and compared to cows and mares yielding milk and butter.
Other names.
In Urdu, the official language of Pakistan, the Indus is known as درياۓ سِندھ "(Daryā-e Sindh)". In other languages of the region, the river is known as सिन्धु नदी "(Sindhu Nadī)" in Hindi, سنڌو "(Sindhu)" in Sindhi, سندھ "(Sindh)" in Shahmukhi alphabet, ਸਿੰਧ ਨਦੀ "(Sindh Nadī)" in Gurmukhī alphabet, સિંધુ નદી "(Sindhu)" in Gujarati; اباسين "(Abāsin," lit. "Father of Rivers") in Pashto, رود سند "(Rūd-e Sind)" in Persian, نهر السند "(Nahar al-Sind)" in Arabic, སེང་གེ།་གཙང་པོ "(Sênggê Zangbo", lit. "Lion River") in Tibetan, 印度 ("Yìndù") in Chinese, and "Nilab" in Turki.
Description.
The Indus River provides key water resources for Pakistan's economy – especially the "breadbasket" of Punjab province, which accounts for most of the nation's agricultural production, and Sindh. The word Punjab means "land of five rivers" and the five rivers are Jhelum, Chenab, Ravi, Beas and Sutlej, all of which finally flow into the Indus. The Indus also supports many heavy industries and provides the main supply of potable water in Pakistan.
The ultimate source of the Indus is in Tibet; the river begins at the confluence of the Sengge and Gar rivers that drain the Nganglong Kangri and Gangdise Shan (Gang Rinpoche, Mt. Kailas) mountain ranges. The Indus then flows northwest through Ladakh and Baltistan into Gilgit, just south of the Karakoram range. The Shyok, Shigar and Gilgit rivers carry glacial waters into the main river. It gradually bends to the south, coming out of the hills between Peshawar and Rawalpindi. The Indus passes gigantic gorges 4500 - deep near the Nanga Parbat massif. It flows swiftly across Hazara and is dammed at the Tarbela Reservoir. The Kabul River joins it near Attock. The remainder of its route to the sea is in the plains of the Punjab and Sindh, where the flow of the river becomes slow and highly braided. It is joined by the Panjnad at Mithankot. Beyond this confluence, the river, at one time, was named the Satnad River ("sat" = "seven", "nadī" = "river"), as the river now carried the waters of the Kabul River, the Indus River and the five Punjab rivers. Passing by Jamshoro, it ends in a large delta to the east of Thatta.
The Indus is one of the few rivers in the world to exhibit a tidal bore. The Indus system is largely fed by the snows and glaciers of the Himalayas, Karakoram and the Hindu Kush ranges of Tibet, the Indian states of Jammu and Kashmir and Himachal Pradesh and Gilgit-Baltistan of Pakistan. The flow of the river is also determined by the seasons – it diminishes greatly in the winter, while flooding its banks in the monsoon months from July to September. There is also evidence of a steady shift in the course of the river since prehistoric times – it deviated westwards from flowing into the Rann of Kutch and adjoining Banni grasslands after the 1816 earthquake.
The traditional source of the river is the "Senge Khabab" or "Lion's Mouth", a perennial spring, not far from the sacred Mount Kailash marked by a long low line of Tibetan chortens. There are several other tributaries nearby, which may possibly form a longer stream than Senge Khabab, but unlike the Senger Khabab, are all dependent on snowmelt. The Zanskar River, which flows into the Indus in Ladakh, has a greater volume of water than the Indus itself before that point.
History.
Paleolithic sites have been discovered in Pothohar near Pakistan's capital Islamabad, with the stone tools of the Soan Culture. In ancient Gandhara, near Islamabad, evidence of cave dwellers dated 15,000 years ago has been discovered at Mardan.
The major cities of the Indus Valley Civilization, such as Harappa and Mohenjo-daro, date back to around 3300 BC, and represent some of the largest human habitations of the ancient world. The Indus Valley Civilization extended from across Pakistan and northwest India, with an upward reach from east of Jhelum River to Ropar on the upper Sutlej. The coastal settlements extended from Sutkagan Dor at the Pakistan, Iran border to Kutch in modern Gujarat, India. There is an Indus site on the Amu Darya at Shortughai in northern Afghanistan, and the Indus site Alamgirpur at the Hindon River is located only 28 km from Delhi. To date, over 1,052 cities and settlements have been found, mainly in the general region of the Ghaggar-Hakra River and its tributaries. Among the settlements were the major urban centers of Harappa and Mohenjo-daro, as well as Lothal, Dholavira, Ganeriwala, and Rakhigarhi. Only 90–96 of more than 800 known Indus Valley sites have been discovered on the Indus and its tributaries. The Sutlej, now a tributary of the Indus, in Harappan times flowed into the Ghaggar-Hakra River, in the watershed of which were more Harappan sites than along the Indus.
Most scholars believe that settlements of Gandhara grave culture of the early Indo-Aryans flourished in Gandhara from 1700 BC to 600 BC, when Mohenjo-daro and Harappa had already been abandoned.
The word "India" is derived from the Indus River. In ancient times, "India" initially referred to those regions immediately along the east bank of the Indus, but by 300 BC, Greek writers including Megasthenes were applying the term to the entire subcontinent that extends much farther eastward.
The lower basin of the Indus forms a natural boundary between the Iranian Plateau and the Indian subcontinent; this region embraces all or parts of the Pakistani provinces Balochistan, Khyber Pakhtunkhwa, Punjab and Sindh and the countries Afghanistan and India. It was crossed by the invading armies of Alexander, but after his Macedonians conquered the west bank—joining it to the Hellenic Empire, they elected to retreat along the southern course of the river, ending Alexander's Asian campaign . The Indus plains were later dominated by the Persian empire and then the Kushan empire. Over several centuries Muslim armies of Muhammad bin Qasim, Mahmud of Ghazni, Mohammed Ghori, Tamerlane and Babur crossed the river to invade the inner regions of the Punjab and points farther south and east.
Geography.
The Indus River near Leh, Ladakh, India
Geology.
The Indus river feeds the Indus submarine fan, which is the second largest sediment body on the Earth at around 5 million cubic kilometres of material eroded from the mountains. Studies of the sediment in the modern river indicate that the Karakoram Mountains in northern Pakistan and India are the single most important source of material, with the Himalayas providing the next largest contribution, mostly via the large rivers of the Punjab (Jhelum, Ravi, Chenab, Beas and Sutlej). Analysis of sediments from the Arabian Sea has demonstrated that prior to five million years ago the Indus was not connected to these Punjab rivers which instead flowed east into the Ganges and were captured after that time. Earlier work showed that sand and silt from western Tibet was reaching the Arabian Sea by 45 million years ago, implying the existence of an ancient Indus River by that time. The delta of this proto-Indus river has subsequently been found in the Katawaz Basin, on the Afghan-Pakistan border.
In the Nanga Parbat region, the massive amounts of erosion due to the Indus river following the capture and rerouting through that area is thought to bring middle and lower crustal rocks to the surface.
In November 2011, satellite images showed that Indus river has re-entered India feeding Great Rann of Kutch , Little Rann of Kutch and a lake near Ahmedabad known as Nal Sarovar. Heavy rains had left the river basin along with the Lake Manchar, Lake Hemal and Kalri Lake (all in modern day Pakistan) inundated. This incident happened after two centuries, when Indus river majorly shifted its course westwards after 1819 Rann of Kutch earthquake.
Wildlife.
Accounts of the Indus valley from the times of Alexander's campaign indicate a healthy forest cover in the region, which has now considerably receded. The Mughal Emperor Babur writes of encountering rhinoceroses along its bank in his memoirs (the Baburnama). Extensive deforestation and human interference in the ecology of the Shivalik Hills has led to a marked deterioration in vegetation and growing conditions. The Indus valley regions are arid with poor vegetation. Agriculture is sustained largely due to irrigation works.
Indus river and its watershed has a rich biodiversity. It is home to around 25 amphibian species and 147 species, 22 of which are only found in the Indus.
Mammals.
The blind Indus River Dolphin ("Platanista indicus minor") is a sub-species of dolphin found only in the Indus River. It formerly also occurred in the tributaries of the Indus river. According to the World Wildlife Fund claims it is one of the most threatened cetaceans with only about 1000 still existing.
Fish.
Palla fish Tenualosa ilisha of the river is a delicacy for people living along the river. The population of fish in the river is moderately high, with Sukkur, Thatta and Kotri being the major fishing centres – all in the lower Sindh course. But damming and irrigation has made fish farming an important economic activity. Located southeast of Karachi, the large delta has been recognised by conservationists as one of the world's most important ecological regions. Here the river turns into many marshes, streams and creeks and meets the sea at shallow levels. Here marine fishes are found in abundance, including pomfret and prawns.
Economy.
The Indus is the most important supplier of water resources to the Punjab and Sindh plains – it forms the backbone of agriculture and food production in Pakistan. The river is especially critical since rainfall is meager in the lower Indus valley. Irrigation canals were first built by the people of the Indus valley civilization, and later by the engineers of the Kushan Empire and the Mughal Empire. Modern irrigation was introduced by the British East India Company in 1850 – the construction of modern canals accompanied with the restoration of old canals. The British supervised the construction of one of the most complex irrigation networks in the world. The Guddu Barrage is 1350 m long – irrigating Sukkur, Jacobabad, Larkana and Kalat. The Sukkur Barrage serves over 20000 km2.
After Pakistan came into existence, a water control treaty signed between India and Pakistan in 1960 guaranteed that Pakistan would receive water from the Indus River and its two tributaries the Jhelum River & the Chenab River independently of upstream control by India.
The Indus Basin Project consisted primarily of the construction of two main dams, the Mangla Dam built on the Jhelum River and the Tarbela Dam constructed on the Indus River, together with their subsidiary dams. The Pakistan Water and Power Development Authority undertook the construction of the Chashma-Jhelum link canal – linking the waters of the Indus and Jhelum rivers – extending water supplies to the regions of Bahawalpur and Multan. Pakistan constructed the Tarbela Dam near Rawalpindi – standing 2743 m long and 143 m high, with an 80 km long reservoir. The Kotri Barrage near Hyderabad is 915 m long and provides additional supplies for Karachi. It support the Chashma barrage near Dera Ismail Khan use for irrigation and flood control. for The Taunsa Barrage near Dera Ghazi Khan produces 100,000 kilowatts of electricity. The extensive linking of tributaries with the Indus has helped spread water resources to the valley of Peshawar, in the Khyber Pakhtunkhwa. The extensive irrigation and dam projects provide the basis for Pakistan's large production of crops such as cotton, sugarcane and wheat. The dams also generate electricity for heavy industries and urban centres.
People.
The inhabitants of the regions through which the Indus river passes and forms a major natural feature and resource are diverse in ethnicity, religion, national and linguistic backgrounds. On the northern course of the river in the state of Jammu and Kashmir in India, live the Buddhist people of Ladakh, of Tibetan stock, and the Dards of Indo-Aryan or Dardic stock and practising Buddhism and Islam. Then it descends into Baltistan, northern Pakistan passing the main Balti city of Skardu. On its course river from Dubair Bala also drains into it at Dubair Bazar. People living at this area are mainly Kohistani and speak Kohistani language. Major areas through which Indus river pass through in Kohistan are Dasu, Pattan and Dubair. As it continues through Pakistan, the Indus river forms a distinctive boundary of ethnicity and cultures – upon the western banks the population is largely Pashtun, Baloch, and of other Iranian stock, with close cultural, economic and ethnic ties to eastern Afghanistan. The eastern banks are largely populated by people of Indo-Aryan stock, such as the Punjabis and the Sindhis. In northern Punjab and the Khyber Pakhtunkhwa, ethnic Pashtun tribes live alongside Dardic people in the hills (Khowar, Kalash, Shina, etc.), Burushos (in Hunza), and Punjabi people.
Through its course in Punjab the people living along the Indus river are distinct from Punjabi and Pustoon. This distinction is not only based on language (Saraiki dialect) but these people also have a different genealogy. They are tall and slender, distinctively different from either pushtoon or Punjabi which have a sturdy built. These people live in Mianwali and Dera Ismail Khan, Dera Ghazi Khan, Rahim Yar Khan and Rajan Pur in Punjab. In the province of Sindh, upper third of River indus is again inhabited by Saraiki speaking people up to Shikapur. The rest of the indus river valley is inhabited by Sindhis and Baloch of Sindhi language. Upon the western banks of the river live the Baloch and Pashtun people of Balochistan.
Modern issues.
The Indus is a strategically vital resource for Pakistan's economy and society. After Pakistan and India declared Independence from the British Raj, also known as Indian Empire, the use of the waters of the Indus and its five eastern tributaries became a major dispute between India and Pakistan. The irrigation canals of the Sutlej valley and the Bari Doab were split – with the canals lying primarily in Pakistan and the headwork dams in India disrupting supply in some parts of Pakistan. The concern over India building large dams over various Punjab rivers that could undercut the supply flowing to Pakistan, as well as the possibility that India could divert rivers in the time of war, caused political consternation in Pakistan. Holding diplomatic talks brokered by the World Bank, India and Pakistan signed the Indus Waters Treaty in 1960. The treaty gave India control of the three easternmost rivers of the Punjab, the Sutlej, the Beas and the Ravi, while Pakistan gained control of the three western rivers, the Jhelum, the Chenab and the Indus. India retained the right to use of the western rivers for non-irrigation projects. (See discussion regarding a recent dispute about a hydroelectric project on the Chenab (not Indus) known as the Baglihar Project).
There are concerns that extensive deforestation, industrial pollution and global warming are affecting the vegetation and wildlife of the Indus delta, while affecting agricultural production as well. There are also concerns that the Indus river may be shifting its course westwards – although the progression spans centuries. On numerous occasions, sediment clogging owing to poor maintenance of canals has affected agricultural production and vegetation. In addition, extreme heat has caused water to evaporate, leaving salt deposits that render lands useless for cultivation.
Recently, India's construction of dams on the river, which Pakistan claims is in violation of the Indus Waters Treaty reducing water flow into Pakistan, has caused Pakistan to take the issue to the international courts for arbitration.
Effects of climate change on the river.
The Tibetan Plateau contains the world's third-largest store of ice. Qin Dahe, the former head of the China Meteorological Administration, said the recent fast pace of melting and warmer temperatures will be good for agriculture and tourism in the short term, but issued a strong warning:
"There is insufficient data to say what will happen to the Indus," says David Grey, the World Bank's senior water advisor in South Asia. "But we all have very nasty fears that the flows of the Indus could be severely, severely affected by glacier melt as a consequence of climate change," and reduced by perhaps as much as 50 percent. "Now what does that mean to a population that lives in a desert [where], without the river, there would be no life? I don't know the answer to that question," he says. "But we need to be concerned about that. Deeply, deeply concerned."
Pollution.
Over the years factories on the banks of the Indus River have increased levels of water pollution in the river and the atmosphere around it. High levels of pollutants in the river have led to the deaths of endangered Indus River Dolphin. The Sindh Environmental Protection Agency has ordered polluting factories around the river to shut down under the Pakistan Environmental Protection Act, 1997. Death of the Indus River Dolphin has also been attributed to fishermen using poison to kill fish and scooping them up. As a result, the government banned fishing from Guddu Barrage to Sukkur.
2010 floods.
In July 2010, following abnormally heavy monsoon rains, the Indus River rose above its banks and started flooding. The rain continued for the next two months, devastating large areas of Pakistan. In Sindh, the Indus burst its banks near Sukkur on 8 August, submerging the village of Mor Khan Jatoi. In early August, the heaviest flooding moved southward along the Indus River from severely affected northern regions toward western Punjab, where at least 1,400,000 acres of cropland was destroyed, and the southern province of Sindh. s of September 2010[ [update]], over two thousand people had died and over a million homes had been destroyed since the flooding began.
2011 floods.
The 2011 Sindh floods began during the Pakistani monsoon season in mid-August 2011, resulting from heavy monsoon rains in Sindh, eastern Balochistan, and southern Punjab. The floods caused considerable damage; an estimated 434 civilians were killed, with 5.3 million people and 1,524,773 homes affected. Sindh is a fertile region and often called the "breadbasket" of the country; the damage and toll of the floods on the local agrarian economy was said to be extensive. At least 1.7 e6acre of arable land were inundated. The flooding followed the previous year's floods, which devastated a large part of the country. Unprecedented torrential monsoon rains caused severe flooding in 16 districts of Sindh.

</doc>
<doc id="15491" url="http://en.wikipedia.org/wiki?curid=15491" title="Integer factorization">
Integer factorization

In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.
Prime decomposition.
By the fundamental theorem of arithmetic, every positive integer greater than one has a unique prime factorization. A special case for one can be avoided using an appropriate notion of the empty product. However, the fundamental theorem of arithmetic gives no insight into how to obtain an integer's prime factorization; it only guarantees its existence.
Given a general algorithm for integer factorization, one can factor any integer down to its constituent prime factors by repeated application of this algorithm. However, this is not the case with a special-purpose factorization algorithm, since it may not apply to the smaller factors that occur during decomposition, or may execute very slowly on these values. For example, if "N" is the number (2521 − 1) × (2607 − 1), then trial division will quickly factor 10 × "N" as 2 × 5 × "N", but will not quickly factor "N" into its factors.
Current state of the art.
Among the "b"-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. For this reason, these are the integers used in cryptographic applications. The largest such semiprime yet factored was RSA-768, a 768-bit number with 232 decimal digits, on December 12, 2009. This factorization was a collaboration of several research institutions, spanning two years and taking the equivalent of almost 2000 years of computing on a single-core 2.2 GHz AMD Opteron. Like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines.
Difficulty and complexity.
No algorithm has been published that can factor all integers in polynomial time, i.e., that can factor "b"-bit numbers in time O("b""k") for some constant "k". There are published algorithms that are faster than O((1+ε)"b") for all positive ε, i.e., sub-exponential.
The best published asymptotic running time is for the general number field sieve (GNFS) algorithm, which, for a "b"-bit number "n", is:
For current computers, GNFS is the best published algorithm for large "n" (more than about 100 digits). For a quantum computer, however, Peter Shor discovered an algorithm in 1994 that solves it in polynomial time. This will have significant implications for cryptography if quantum computation is possible. Shor's algorithm takes only O("b"3) time and O("b") space on "b"-bit number inputs. In 2001, the first seven-qubit quantum computer became the first to run Shor's algorithm. It factored the number 15.
When discussing what complexity classes the integer factorization problem falls into, it's necessary to distinguish two slightly different versions of the problem:
For formula_2, the decision problem is equivalent to asking if "N" is prime. 
An algorithm for either version provides one for the other. Repeated application of the function problem (applied to "d" and "N"/"d", and their factors, if needed) will eventually provide either a factor of "N" no larger than "M" or a factorization into primes all greater than "M". All known algorithms for the decision problem work in this way. Hence it is only of theoretical interest that, with at most formula_3 queries using an algorithm for the decision problem, one would isolate a factor of "N" (or prove it prime) by binary search. 
It is not known exactly which complexity classes contain the decision version of the integer factorization problem. It is known to be in both NP and co-NP. This is because both YES and NO answers can be verified in polynomial time. An answer of YES can be certified by exhibiting a factorization "N" = "d"("N"/"d") with "d" ≤ "M". An answer of NO can be certified by exhibiting the factorization of "N" into distinct primes, all larger than "M". We can verify their primality using the AKS primality test, and that their product is "N" by multiplication. The fundamental theorem of arithmetic guarantees that there is only one possible string that will be accepted (providing the factors are required to be listed in order), which shows that the problem is in both UP and co-UP. It is known to be in BQP because of Shor's algorithm. It is suspected to be outside of all three of the complexity classes P, NP-complete, and co-NP-complete. It is therefore a candidate for the NP-intermediate complexity class. If it could be proved that it is in either NP-Complete or co-NP-Complete, that would imply NP = co-NP. That would be a very surprising result, and therefore integer factorization is widely suspected to be outside both of those classes. Many people have tried to find classical polynomial-time algorithms for it and failed, and therefore it is widely suspected to be outside P.
In contrast, the decision problem "is "N" a composite number?" (or equivalently: "is "N" a prime number?") appears to be much easier than the problem of actually finding the factors of "N". Specifically, the former can be solved in polynomial time (in the number "n" of digits of "N") with the AKS primality test. In addition, there are a number of probabilistic algorithms that can test primality very quickly in practice if one is willing to accept the vanishingly small possibility of error. The ease of primality testing is a crucial part of the RSA algorithm, as it is necessary to find large prime numbers to start with.
Factoring algorithms.
Special-purpose.
A special-purpose factoring algorithm's running time depends on the properties of the number to be factored or on one of its unknown factors: size, special form, etc. Exactly what the running time depends on varies between algorithms.
An important subclass of special-purpose factoring algorithms is the "Category 1" or "First Category" algorithms, whose running time depends on the size of smallest prime factor. Given an integer of unknown form, these methods are usually applied before general-purpose methods to remove small factors. For example, trial division is a Category 1 algorithm.
General-purpose.
A general-purpose factoring algorithm, also known as a "Category 2", "Second Category", or "Kraitchik family" algorithm (after Maurice Kraitchik), has a running time which depends solely on the size of the integer to be factored. This is the type of algorithm used to factor RSA numbers. Most general-purpose factoring algorithms are based on the congruence of squares method.
Heuristic running time.
In number theory, there are many integer factoring algorithms that heuristically have expected running time
in o and L-notation.
Some examples of those algorithms are the elliptic curve method and the quadratic sieve.
Another such algorithm is the class group relations method proposed by Schnorr, Seysen, and Lenstra that is proved under of the Generalized Riemann Hypothesis (GRH).
Rigorous running time.
The Schnorr-Seysen-Lenstra probabilistic algorithm has been rigorously proven by Lenstra and Pomerance to have expected running time formula_5 by replacing the GRH assumption with the use of multipliers.
The algorithm uses the class group of positive binary quadratic forms of discriminant Δ denoted by "G"Δ.
"G"Δ is the set of triples of integers ("a", "b", "c") in which those integers are relative prime.
Schnorr-Seysen-Lenstra Algorithm.
Given is an integer "n" that will be factored, where "n" is an odd positive integer greater than a certain constant. In this factoring algorithm the discriminant Δ is chosen as a multiple of "n", Δ= -"dn", where "d" is some positive multiplier. The algorithm expects that for one "d" there exist enough smooth forms in "G"Δ. Lenstra and Pomerance show that the choice of "d" can be restricted to a small set to guarantee the smoothness result.
Denote by "P"Δ the set of all primes "q" with Kronecker symbol formula_6. By constructing a set of generators of "G"Δ and prime forms "f"q of "G"Δ with "q" in "P"Δ a sequence of relations between the set of generators and "f"q are produced.
The size of "q" can be bounded by formula_7 for some constant formula_8.
The relation that will be used is a relation between the product of powers that is equal to the neutral element of "G"Δ. These relations will be used to construct a so-called ambiguous form of "G"Δ, which is an element of "G"Δ of order dividing 2. By calculating the corresponding factorization of Δ and by taking a gcd, this ambiguous form provides the complete prime factorization of "n". This algorithm has these main steps:
Let "n" be the number to be factored.
To obtain an algorithm for factoring any positive integer, it is necessary to add a few steps to this algorithm such as trial division, and the Jacobi sum test.
Expected running time.
The algorithm as stated is a probabilistic algorithm as it makes random choices. Its expected running time is at most formula_5.
Applications.
Fast Fourier Transforms.
The best-known fast Fourier transform algorithms depend upon integer factorization.

</doc>
<doc id="15492" url="http://en.wikipedia.org/wiki?curid=15492" title="Imperial units">
Imperial units

The system of imperial units or the imperial system (also known as British Imperial) is the system of units first defined in the British Weights and Measures Act of 1824, which was later refined and reduced. The system came into official use across the British Empire. By the late 20th century, most nations of the former empire had officially adopted the metric system as their main system of measurement; however some imperial units are still used in the United Kingdom, Canada and other countries formerly part of the British Empire. The imperial system developed from what were first known as English units, as did the separate system of United States customary units.
Implementation.
The Weights and Measures Act of 1824 was initially scheduled to go into effect on 1 May 1825. However, the Weights and Measures Act of 1825 pushed back the date to 1 January 1826. The 1824 Act allowed the continued use of pre-imperial units provided that they were customary, widely known, and clearly marked with imperial equivalents.
Apothecaries' units.
Apothecaries' units are mentioned neither in the act of 1824 nor 1825. At the time, apothecaries' weights and measures were regulated "in England, Wales, and Berwick-upon-Tweed" by the London College of Physicians, and in Ireland by the Dublin College of Physicians. In Scotland, apothecaries' units were unofficially regulated by the Edinburgh College of Physicians. The three colleges published, at infrequent intervals, pharmacopoeiae, the London and Dublin editions having the force of law.
Imperial apothecaries' measures, based on the imperial pint of 20 fluid ounces, were introduced by the publication of the London Pharmacopoeia of 1836, the Edinburgh Pharmacopoeia of 1839, and the Dublin Pharmacopoeia of 1850. The Medical Act of 1858 transferred to the The Crown the right to publish the official pharmacopoeia and to regulate apothecaries' weights and measures.
Units.
Length.
Metric equivalents in this article usually assume the latest official definition. Before this date, the most precise measurement of the imperial Standard Yard was metres.
Volume.
In 1824, the various different gallons in use in the British Empire were replaced by the imperial gallon, a unit close in volume to the ale gallon. It was originally defined as the volume of 10 lb of distilled water weighed in air with brass weights with the barometer standing at 30 inHg at a temperature of 62 °F. In 1963, the gallon was redefined as the volume of 10 pounds of distilled water of density  g/mL weighed in air of density against weights of density 8.136 g/mL, which works out to or . The Weights and Measures Act of 1985 switched to a gallon of exactly (approximately <span id="FormattingError" />).
British apothecaries' volume measures.
These measurements were in use from 1824, when the new imperial gallon was defined, but were officially abolished in the United Kingdom on 1 January 1971. In the USA, though no longer recommended, the apothecaries' system is still used occasionally in medicine, especially in prescriptions for older medications.
Mass and weight.
In the 19th and 20th centuries, the UK used three different systems for mass and weight:
The troy pound () was made the primary unit of mass by the 1824 Act; however, its use was abolished in the UK on 1 January 1879, with only the troy ounce () and its decimal subdivisions retained. The "Weights and Measures Act 1855" (18 & 19 Victoria C72) made the avoirdupois pound the primary unit of mass. In all the systems, the fundamental unit is the pound, and all other units are defined as fractions or multiples of it.
Natural equivalents.
Although the 1824 act defined the yard and pound by reference to the prototype standards, it also defined the values of certain physical constants, to make provision for re-creation of the standards if they were to be damaged. For the yard, the length of a pendulum beating seconds at the latitude of Greenwich at Mean Sea Level "in vacuo" was defined as 39.013 93 inches, and, for the pound, the mass of a cubic inch of distilled water at an atmospheric pressure of 30 inches of mercury and a temperature of 62° Fahrenheit was defined as 252.458 grains. However, following the destruction of the original prototypes in the 1834 Houses of Parliament fire, it proved impossible to recreate the standards from these definitions, and a new Weights and Measures Act (18 & 19 Victoria. Cap. 72) was passed in 1855 which permitted the recreation of the prototypes from recognized secondary standards.
Relation to other systems.
The imperial system is one of many systems of English units. Although most of the units are defined in more than one system, some subsidiary units were used to a much greater extent, or for different purposes, in one area rather than the other. The distinctions between these systems are often not drawn precisely.
One such distinction is that between these systems and older British/English units/systems or newer additions. The term "imperial" should not be applied to English units that were outlawed in the Weights and Measures Act 1824 or earlier, or which had fallen out of use by that time, nor to post-imperial inventions, such as the slug or poundal.
The US customary system is historically derived from the English units that were in use at the time of settlement. Because the United States was already independent at the time, these units were unaffected by the introduction of the imperial system.
Current use of imperial units.
United Kingdom.
British law now defines each imperial unit in terms of the metric equivalent. The metric system is in official use within the United Kingdom for most applications; however, use of imperial units is still widespread amongst the public and all UK roads still primarily use the imperial system except for tonnage on main roads.
The Units of Measurement Regulations 1995 require that all measuring devices used in trade or retail shall display measurements in metric quantities. This has been proven in court against the so-called "Metric Martyrs", a small group of market traders who insisted on trading in imperial units only. Contrary to the impression given by some press reports, these regulations do not currently place any obstacle in the way of using imperial units alongside metric units. Almost all traders in the UK will accept requests from customers specified in imperial units, and scales which display in both unit systems are commonplace in the retail trade. Metric price signs may be accompanied by imperial price signs (known as supplementary indicators) provided that the imperial signs are no larger and no more prominent than the official metric ones. The EU units of measurement directive (directive 80/181/EEC) had previously permitted the use of "supplementary indicators" (imperial measurements) until 31 December 2009, but a revision of the directive published on 11 March 2009 permitted their use indefinitely.
The United Kingdom completed its legal partial transition to the metric system (sometimes referred to as "SI" from the French Système International d'Unités) in 1995, with some imperial units still legally mandated for certain applications; draught beer and cider "must" be sold in pints, road-sign distances "must" be in yards and miles, length and width (but not weight) restrictions "must" be in feet and inches on road signs (although an equivalent in metres may be shown as well), and road speed limits "must" be in miles per hour, therefore instruments in vehicles sold in the UK must be capable of displaying miles per hour. Foreign vehicles, such as all post-2005 Irish vehicles, may legally have instruments displayed only in kilometres per hour. Even though the troy pound was outlawed in the UK in the Weights and Measures Act of 1878, the "troy ounce" still "may" be used for the weight of precious stones and metals. The original railways (many built in the Victorian era) are a big user of imperial units, with distances officially measured in miles and yards or miles and chains, and also feet and inches, and speeds are in miles per hour, although many modern metro and tram systems are entirely metric, and London Underground uses both metric (for distances) and imperial (for speeds). Metric is also used for the Channel Tunnel and on High Speed 1. Adjacent to Ashford International railway station and Dollands Moor Freight Yard, railway speeds are given in both metric and imperial units.
Most British people still use imperial units in everyday life for distance (miles, yards, feet and inches), body weight (stones and pounds for adults, pounds and ounces for babies though use of kilogrammes is increasing) and volume in some cases (especially pints of milk, beer, and rational fractions thereof but rarely for canned or bottled soft drinks or petrol). Regardless of how people measure their weight or height, these must be recorded in metric officially, for example in medical records. Petrol is occasionally quoted as being so much per gallon (despite having been sold exclusively in litres for nearly three decades). Fuel consumption for vehicles is often discussed in miles per gallon, though official figures always include litres per 100 km equivalents. When sold "draught" in licensed premises, beer and cider is measured out and sold in pints and half-pints. Cow's milk is available in both litre- and pint-based containers. Non-metric nuts and bolts etc., are available, but usually only from specialist suppliers. Areas of land associated with farming, forestry and real estate are often advertised measured in acres and square feet, but for official government purposes the unit is always hectares and square metres. Office space and industrial units are often advertised in square feet, despite carpet and flooring products being sold in square metres with equivalents in square yards. Steel pipe sizes are sold in increments of inches, while copper pipe is sold in increments of millimetres. Road bicycles have their frames measured in centimetres, while off-road bicycles have their frames measured in inches. The size (diagonal) of television and computer monitor screens is denominated in inches.
India.
India's conversion to the metric system from the imperial system occurred in stages between 1955 and 1962. The metric system in weights and measures was adopted by the Indian Parliament in December 1956 with the "Standards of Weights and Measures Act", which took effect beginning 1 October 1958. The "Indian Coinage Act" was passed in 1955 by the Government of India to introduce decimal coinage in the country. The new system of coins became legal tender on April 1957, where the rupee consists of 100 paise. For the next five years, both the old and new systems were legal. In April 1962, all other systems were banned. This process of metrication is called "big-bang" route, which is to simultaneously outlaw the use of pre-metric measurement, metricise, reissue all government publications and laws, and change education systems to metric
Today all official measurements are made in the metric system. However, in common usage some older Indians may still refer to imperial units. Some measurements, such as the heights of mountains, are still recorded in feet. Additionally, the Indian numbering system of crores and lacs is used alongside otherwise metricated currency units, while tyre rim diameters are still measured in inches, as used worldwide. Road widths are popularly measured in feet but official documents use metres. Body temperature is still sometimes measured in degrees Fahrenheit. Industries like the construction and the real estate industry still use both the metric and the imperial system though it is more common for sizes of homes to be given in square feet and land in acres. Bulk cotton is sold by the "candy" (0.35 imperial tons, or 355.62 kg) or the "bale" (170 kg)
In Standard Indian English, as in Australian, Singaporean, and British English, metric units such as the litre (liter), metre (meter), and metric tonne (ton) utilise the traditional spellings brought over from French, which differ from those used in the United States and the Philippines. The imperial long ton is invariably spelt with one 'n'. (See English in the Commonwealth of Nations for more information)
Hong Kong.
Hong Kong has three main systems of units of measurement in current use:
In 1976 the Hong Kong Government started the conversion to the metric system, and as of 2012 measurements for government purposes, such as road signs, are almost always in metric units. However, all three systems are officially permitted for trade, and in the wider society a mixture of all three systems prevails.
The Chinese system's most commonly used units for length are 里 (li), 丈 (tseung/cheung), 尺 (tsek/chek), 寸 (tsun/chun), 分 (fen/fan) in descending scale order. These units are now rarely used in daily life, the imperial and metric systems being preferred. The imperial equivalents are written with the same basic Chinese characters as the Chinese system. In order to distinguish between the units of the two systems, the units can be prefixed with "Ying" () for the Imperial system and "Wa" () for the Chinese system. In writing, derived characters are often used, with an additional 口(mouth) radical to the left of the original Chinese character, for writing Iimperial units. The most commonly used units are the mile or "li" (), the yard or "ma" (), the foot or "chek" (), and the inch or "tsun" ().
The traditional measure of flat area is the square foot () of the imperial system, which is still in common use for real estate purposes. The measurement of agricultural plots and fields, however, is traditionally conducted in 畝 (mau) of the Chinese system.
For the measurement of volume, Hong Kong officially uses the metric system, though the gallon (加侖, ka-lun) is also occasionally used.
Canada.
During the 1970s, the metric system and SI units were introduced in Canada to replace the imperial system. Within the government, efforts to implement the metric system were extensive; almost any agency, institution, or function provided by the government uses SI units exclusively. Imperial units were eliminated from all road signs, although both systems of measurement will still be found on privately owned signs, such as the height warnings at the entrance of a parkade. In the 1980s, momentum to fully convert to the metric system stalled when the government of Brian Mulroney was elected. There was heavy opposition to metrication and as a compromise the government maintains legal definitions for and allows use of imperial units as long as metric units are shown as well. The law requires that measured products (such as fuel and meat) be priced in metric units, although an imperial price can be shown if a metric price is present. However, there tends to be leniency in regards to fruits and vegetables being priced in imperial units only.
Environment Canada still offers an imperial unit option beside metric units, even though weather is typically measured and reported in metric units in the Canadian media. However, some radio stations near the United States border (such as CIMX and CIDR) primarily use imperial units to report the weather. Railways in Canada also continue to use Imperial units.
Imperial units are still used in ordinary conversation. Today, Canadians typically use a mix of metric and imperial measurements in their daily lives. However, the use of the metric and imperial systems varies by age. The older generation mostly uses the imperial system, while the younger generation more often uses the metric system. Newborns are measured in SI at hospitals, but the birth weight and length is also announced to family and friends in imperial units. Drivers' licences use SI units. In livestock auction markets, cattle are sold in dollars per hundredweight (short), whereas hogs are sold in dollars per hundred kilograms. Imperial units still dominate in recipes, construction, house renovation and gardening. Land is now surveyed and registered in metric units, although initial surveys used imperial units. For example, partitioning of farm land on the prairies in the late 19th and early 20th centuries was done in imperial units; this accounts for imperial units of distance and area retaining wide use in the Prairie Provinces. The size of most apartments, condominiums and houses continues to be described in square feet rather than square metres, and carpet or flooring tile is purchased by the square foot. Motor-vehicle fuel consumption is reported in both litres per 100 km and statute miles per imperial gallon, leading to the erroneous impression that Canadian vehicles are 20% more fuel-efficient than their apparently identical American counterparts for which fuel economy is reported in statute miles per US gallon (neither country specifies which gallon is used). Canadian railways maintain exclusive use of imperial measurements to describe train length (feet), train height (feet), capacity (tons), speed (mph), and trackage (miles).
Imperial units also retain common use in firearms and ammunition. Imperial measures are still used in the description of cartridge types, even when the cartridge is of relatively recent invention (e.g., 0.204 Ruger, 0.17 HMR, where the calibre is expressed in decimal fractions of an inch). However, ammunition that is already classified in metric is still kept metric (e.g., 9 mm). In the manufacture of ammunition, bullet and powder weights are expressed in terms of grains for both metric and imperial cartridges.
As in most of the western world, air navigation is based on "nautical" units, e.g., the nautical mile, which is neither imperial nor metric, though altitude is still measured in imperial feet in keeping with the international standard.
Australia.
Metrication in Australia has largely ended the use of imperial units, though for particular measurements (such as flight altitudes and nominal sizes of computer and television screens) international usage of imperial units is still followed.
New Zealand.
Although New Zealand completed metrication in the 1970s, a study of university students undertaken in 1992 found a continued use of imperial units for birth weight and human height alongside metric units.
The aviation industry is one of the last major users of the old imperial system: altitude and airport elevation is measured in feet. All other aspects (fuel quantity, aircraft weight, runway length, etc.) use metric.
Ireland.
Ireland has officially changed over to the metric system since entering the European Union, with distances on new road signs being metric since 1997 and speed limits being metric since 2005. The imperial system remains in limited use – for sales of beer in pubs (traditionally sold by the pint). All other goods are required by law to be sold in metric units, although old quantities are retained for some goods like butter and sausages, which are sold in 454-gram (1 lb) packaging. The majority of cars sold pre-2005 feature speedometers with miles per hour as the primary unit, but with a kilometres per hour display as well.
Other countries.
Some imperial measurements remain in limited use in Canada, India, Malaysia, Sri Lanka and Hong Kong. Real estate agents continue to use acres and square feet to describe area, rarely in conjunction with hectares and square metres. Measurements in feet and inches, especially for a person's height, are frequently met in conversation and non-governmental publications. In India, inches, feet, yards and degrees Fahrenheit are often used in conjunction with their metric counterparts, while area is often still measured in acres though hectares are used in government documents; the Celsius scale is used for weather readings and forecasts, but the Fahrenheit scale is often used for body temperatures.
Towns and villages in Malaysia with no proper names had adopted the Malay word "batu" (meaning "rock") to indicate their locations along a main road before the use of metric system (for example, "batu enam" means "6th mile" or "mile 6"). Many of their names remain unchanged even after the adoption of the metric system for distance in the country.
Petrol is still sold by the imperial gallon in Anguilla, Antigua and Barbuda, Burma, the Cayman Islands, Dominica, Grenada, Montserrat, St Kitts and Nevis and St. Vincent and the Grenadines. The United Arab Emirates Cabinet in 2009 issued the Decree No. (270 / 3) specifying that, from 1 January 2010, the new unit sale price for petrol will be the litre and not the gallon. This in line with the UAE Cabinet Decision No. 31 of 2006 on the national system of measurement, which mandates the use of International System of units as a basis for the legal units of measurement in the country. Sierra Leone switched to selling fuel by the litre in May 2011.
In October 2011, the Antigua and Barbuda government announced the re-launch of the Metrication Programme in accordance with the Metrology Act 2007, which established the International System of Units as the legal system of units. The Antigua and Barbuda government has committed to a full conversion from the imperial system by the first quarter of 2015.

</doc>
<doc id="15494" url="http://en.wikipedia.org/wiki?curid=15494" title="Incompatible-properties argument">
Incompatible-properties argument

The incompatible-properties argument is the idea that no description of God is consistent with reality. For example, if one takes the definition of God to be described fully from the Bible, then the claims of what properties God has described therein might be argued to lead to a contradiction.
Evil vs. good and omnipotence.
The problem of evil is the argument that the existence of evil is incompatible with the concept of an omnipotent and perfectly good God.
A variation does not depend on the existence of evil. A truly omnipotent God could create all possible worlds. A "good" God can create only "good" worlds. A God that created all possible worlds would have no moral qualities whatsoever, and could be replaced by a random generator. The standard response is to argue a distinction between "could create" and "would create." In other words, God "could" create all possible worlds but that is simply not in God's nature. This has been argued by theologians for centuries. However, the result is that a "good" God is incompatible with some possible worlds, thus incapable of creating them without losing the property of being a totally different God.
Purpose vs. timelessness.
One argument based on incompatible properties rests on a definition of God that includes a will, plan or purpose and an existence outside of time. To say that a being possesses a purpose implies an inclination or tendency to steer events toward some state that does not yet exist. This, in turn, implies a privileged direction, which we may call "time". It may be one direction of causality, the direction of increasing entropy, or some other emergent property of a world. These are not identical, but one must exist in order to progress toward a goal. 
In general, God's time would not be related to our time. God might be able to operate within our time without being constrained to do so. However, God could then step outside this game for any purpose. Thus God's time must be aligned with our time if human activities are relevant to God's purpose. (In a relativistic universe, presumably this means—at any point in spacetime—time measured from t=0 at the Big Bang or end of inflation.)
A God existing outside of any sort of time could not create anything because creation substitutes one thing for another, or for nothing. Creation requires a creator that existed, by definition, prior to the thing created.
Omniscience vs. indeterminacy or free will.
Another pair of alleged incompatible properties is omniscience and either indeterminacy or free will. Omniscience concerning the past and present (properly defined relative to Earth) is not a problem, but some people argue that omniscience regarding the future implies it has been determined, what seems possible only in a deterministic world.
Simplicity vs. omniscience.
Another pair is simplicity and omniscience. God's memory alone vastly exceeds the terabytes in our computers, and bits (or bytes) are the fundamental mathematical units of information. Information is not "ineffable" and cannot be reduced to something simpler. Furthermore, God must live forever and therefore must have a deterministic processing unit or infinite error correction mechanisms. The simplest implementation is deterministic and quite unconscious, seemingly incompatible with an intelligent being.

</doc>
<doc id="15495" url="http://en.wikipedia.org/wiki?curid=15495" title="International Society of Olympic Historians">
International Society of Olympic Historians

"For the parody hardcore punk band" International Superheroes of Hardcore, "see ISHC."
The International Society of Olympic Historians (ISOH) is a non-profit organization founded in 1991 with the purpose of promoting and studying the Olympic Movement and the Olympic Games. The majority of recent books on the Olympic Games have been written by ISOH members. The ISOH publishes the Journal of Olympic History (JOH, formerly "Citius, Altius, Fortius") three times a year.
History.
The International Society of Olympic Historians (ISOH) was formed as the result of a meeting in London, England in December 1991. The idea of forming an Olympic historical society had been the subject of correspondence – mainly between Bill Mallon (United States) and Ture Widlund (Sweden) – for many years. On Thursday, 5 December 1991, a group of potential members met at the Duke of Clarence, a small pub in the Kensington section of London. Those present were Ian Buchanan (Great Britain), Stan Greenberg (Great Britain), Ove Karlsson (Sweden), Bill Mallon (United States), Peter Matthews (Great Britain), David Wallechinsky (United States), and Ture Widlund (Sweden). The invited guests who sent regrets were: Anthony Bijkerk (Netherlands), Peter Diamond (United States), Pim Huurman (Netherlands), Erich Kamper (Austria), Volker Kluge (Germany), John Lucas (United States), and Wolf Lyberg (Sweden).
ISOH was formed with the purpose of promoting and studying the Olympic Movement and the Olympic Games. This purpose is achieved primarily through research into their history, through the gathering of historical and statistical data concerning the Olympic Movement and Olympic Games, through the publication of the research via journals and other publications, and through the cooperation of the membership.
From its inception to 2000, Ian Buchanan has been the president of the ISOH. In 2000, this function was taken over by Bill Mallon. From 2004 to 2012 Dr. Karl Lennartz (Germany) served as president and since 2012 David Wallechinsky (United States) has been president.
Journal of Olympic History.
The ISOH publishes the "Journal of Olympic History" (formerly "Citius, Altius, Fortius").
Membership.
s of 2007[ [update]], the ISOH has about 340 members from 48 nations.<REF NAME="ISOH-2007-ORG">ISOH 2007, cited.</REF> The membership includes well-known Olympic historians and researchers on Olympic topics. The majority of recent books on the Olympic Games have been written by ISOH members. Over 20 ISOH members have received the Olympic Order for their contributions to the Olympic Movement, and several members of the IOC and several Olympians are members.<REF NAME="ISOH-2007-ORG">ISOH 2007, cited.</REF> Other members are collectors of Olympic memorabilia, such as Raleigh DeGeer Amyx.

</doc>
<doc id="15496" url="http://en.wikipedia.org/wiki?curid=15496" title="Serie A">
Serie A

Serie A (]), also called Serie A TIM due to sponsorship by Telecom Italia, is a professional league competition for soccer clubs located at the top of the Italian football league system and has been operating for over eighty years since the 1929–30 season. It had been organized by Lega Calcio until 2010, but a new league, the Lega Serie A, was created for the 2010–11 season. Serie A is regarded as one of the best football leagues in the world. Serie A was considered the best league in the world in the '90s, and has produced the highest number of European Cup finalists: Italian clubs have reached the final of the competition on a record twenty-six different occasions, winning the title twelve times. Serie A is ranked 4th among European leagues according to UEFA's league coefficient behind La Liga, Premier League and the Bundesliga, which is based on the performance of Italian clubs in the Champions League and the Europa League during the last five years. It also ranked 5th in world according to the first trends of the 2011 IFFHS rating.
In its current format, the Italian Football Championship was revised from having regional and interregional rounds, to a single-tier league from the 1929–30 season onwards. The championship titles won prior to 1929 are officially recognised by FIGC with the same weighting as titles that were subsequently awarded. However, the 1945–46 season, when the league was played over two geographical groups due to the ravages of WWII, is not statistically considered, even if its title is fully official.
The league hosts three of the world's most famous clubs as Juventus, Milan and Internazionale, all founding members of the G-14, a group which represented the largest and most prestigious European football clubs; Serie A was the only league to produce three founding members. More players have won the coveted "Ballon d'Or" award while playing at a Serie A club than any other league in the world. - ahead of Spain's La Liga, although the actual number of Ballon d'Or won by players in these two leagues is equal at 18 each if including the FIFA Ballon d'Or. Milan is the second club with the most official international titles in the world (18). Juventus, Italy's most successful club of the 20th century and the most successful Italian team, is tied for fourth in Europe and eighth in the world in the same ranking.
The club is the only one in the world to have won all possible official continental competitions and the world title. Internazionale, following their achievements in the 2009–10 season, became the first Italian team to have achieved The Treble.
Format.
For most of Serie A's history there were 16 or 18 clubs competing at the top level; however, since 2004–05 there have been 20 clubs altogether. A season (1947–1948) was played with 21 teams for political reasons. Below is a complete record of how many teams played in each season throughout the league's history;
During the league, from August to May, each club plays each of the other teams twice; once at home and once away, totaling 38 games for each team by the end of the season. Therefore, in Italian football a true round-robin format is used. In the first half of the season, called the "andata", each team plays once against each league opponent, for a total of 19 games. In the second half of the season, called the "ritorno", the teams play in exactly the same order that they did in the first half of the season, the only difference being that home and away situations are switched. Since the 1994-1995 season, teams were awarded three points for a win, one point for a draw, and no points for a loss.
Since Italy is currently rated as the fourth European countries in terms of club football ratings, the top three teams in the Serie A qualified for the UEFA Champions League (from the 2012-13 season). The top two teams qualify directly to the group phase, while the third-placed team enters the competition at the playoff qualifying round and must win a two-legged knockout tie in order to enter the group phase. Teams finishing 4th and 5th qualify for the UEFA Europa League Tournament. A third UEFA Europa League spot is reserved for the winner of the Coppa Italia. If the Coppa Italia champion has already qualified for the major European tournament by placing in the top three of Serie A, the third UEFA Europa League spot goes to the losing finalist. If both Coppa Italia finalists finish among the top five teams in Serie A, the 6th classified team in Serie A is awarded the UEFA Europa League spot. The three lowest placed teams are relegated to Serie B.
Before the 2005-06 season, if two or more teams were tied in points for first place, for only one spot in a European tournament or in relegation zone, this teams would play tie-breaking games after the league's end, to determine which team would get the best place (useful to be champion, to be awarder a European tournament sport, or to be saved). From 2005-06 season, if two or more teams end the league with the same number of points, the deciding tie-breakers used are (in order):
History.
Serie A, as it is structured today, began in 1929. From 1898 to 1922 the competition was organised into regional groups. Because of ever growing teams attending regional championships, FIGC split the CCI (Italian Football Confederation) in 1921. When CCI teams rejoined the FIGC created two interregional divisions renaming Categories into Divisions and splitting FIGC sections into two North-South leagues. In 1926 due to internal crises FIGC changed internal settings adding southern teams to the national divisions which lead to 1929-30 final settlement. No title was awarded in 1927 after Torino were stripped of the championship by the Italian Football Federation (FIGC). Torino were declared champions in the 1948-49 season following a plane crash near the end of the season in which the entire team was killed.
The Serie A Championship title is often referred to as the "scudetto" (small shield) because since the 1924–25 season the winning team will bear a small coat of arms with the Italian tricolour on their strip in the following season. The most successful club is Juventus with 31 championships, followed by both Milan and Internazionale (18), and Genoa (9). From 2004–05 onwards an actual trophy was awarded to club on the pitch after the last turn of the championship. The trophy, called "Coppa Campioni d'Italia", is official since the 1960–61 season, but between 1961 and 2004 it was consigned to the winning clubs at the head office of the "Lega Nazionale Professionisti".
On 30 April 2009, Serie A announced a split from Serie B. Nineteen of the twenty clubs voted in favour of the move in an argument over television rights. Relegation-threatened Lecce voted against. Maurizio Beretta, the former head of Italy's employers' association, became president of the new league.
Television rights.
In the past individual clubs competing in the league had the rights to sell their broadcast rights to specific channels in Italy, unlike in most other European countries. Currently, the two broadcasters in Italy are the satellite broadcaster SKY Italia, along with terrestrial broadcaster Mediaset Premium for its own pay television networks; RAI is allowed to broadcast only highlights (in exclusive from 13:30 to 22:30 CET).
This is a list of television rights in Italy (until 2009–2010):
For the 2010–11 and 2011–12 seasons, Serie A clubs negotiating club TV rights collectively rather than individually for the first time since 1998–99. The domestic rights for those two seasons were sold for €1.149bn to Sky Italia.
International.
Global rights for the 2010–11 and 2011–12 seasons were sold for €181.5M to MP & Silva.
In countries and territories outside of Italy, the league is broadcast on adventure time Raitalia (numerous countries in several continents), BT Sport (United Kingdom), Setanta Sports (Ireland), Canal+ (Spain), Teleclub (Switzerland), beIN Sports USA (United States as well as Canada while not having a broadcast channel), TV Esporte Interativo, Fox Sports (Brazil), ESPN Latin America (Latin America (except Brazil)), CCTV5 (China), KBS Sports (South Korea), beIN Sports (Middle East and North Africa), Astro SuperSport (Malaysia), Kompas TV (Indonesia), Neo Sports, TrueVisions (Thailand), mio TV (Singapore), TEN Sports (India), ESPN Star Sports (India), Fox Sports (Netherlands), OTE Sport (Greece), Canal+ Poland (Poland), NTV Turkey (Turkey), bTV Action and RING.BG (Bulgaria), Digi Sport (Romania), Arena Sport, Sport Klub (Serbia), SuperSport (Albania), Supersport Kosova (Kosovo), In televizija (Montenegro), Telelatino, Sport 5 (Israel), HiTV (Nigeria), Sport1 (Lithuania), Canal9 (Denmark and Norway) Sportbox (Mongolia) and beIN Sports (Australia) .
In the 1990s Serie A was at its most popular in the UK when it was shown on Channel 4, although it has actually appeared on more UK channels than any other league, rarely staying in one place for long since 2002. Serie A has appeared in the UK on BSB Sports Channel (1990–91), Sky Sports (1991–92), Channel 4 (1992–2002), Eurosport (2002–04), Setanta Sports and Bravo (2004–07), Channel 5 (2007–08), ESPN (2009–13) and BT Sport (since 2013). In Mexico, Televisa Deportes Network HD two games delay in the week.
Champions.
Bold indicates clubs currently playing in the top division.
Serie A clubs.
Prior to 1929, many clubs competed in the top level of Italian football as the earlier rounds were competed up to 1922 on a regional basis then interregional up to 1929. Below is a list of Serie A clubs who have competed in the competition when it has been a league format (63 in total).
Seasons in Serie A.
The teams in bold compete in Serie A currently. Internazionale is the only team that has played Serie A football in every season.
Serie A Members for 2014–15.
The following twenty clubs will complete in Serie A during the 2014–15 season.
1 Promoted as playoff winner.
Players.
Non-EU players.
Unlike La Liga, which imposed a quota on the number of non-EU players on each club, Serie A clubs could sign as many non-EU players as available on domestic transfer. But since the 2003–04 season a quota has been imposed on each of the clubs limiting the number of non-EU, non-EFTA and non-Swiss players who may be signed from abroad each season, following provisional measures were had been introduced in the 2002–03 season, which allowed Serie A & B clubs to sign only one non-EU player in the 2002 summer transfer window.
In the middle of the 2000–01 season, the old quota system was abolished, which no longer limited each team to having more than 5 non-EU players and using no more than 3 in each match. Concurrent with the abolishment of the quota, FIGC had investigated footballers that used fake passports. Alberto and Warley, Alejandro Da Silva and Jorginho Paulista of Udinese, Fábio Júnior and Gustavo Bartelt of Roma, Dida of Milan, Álvaro Recoba of Inter, Thomas Job, Francis Zé, Jean Ondoa of Sampdoria, Jeda and Dede of Vicenza were banned in July 2001, for 6 months to 1 year. However, most of the bans were subsequently reduced.
The number of non-EU players was reduced from 265 in 2002–03 season to 166 in 2006–07 season. It also included players got EU status after their countries joined the EU (see 2004 and 2007 enlargement), which made players such as Adrian Mutu, Valeri Bojinov, Marek Jankulovski and Marius Stankevičius no longer non-EU players.
The rule underwent minor changes in August 2004, June 2005, June 2006. and June 2007.
Since the 2008–09 season, 3 quotas have been awarded to clubs that do not have non-EU players in their squad (previously only newly promoted clubs could had 3 quota); clubs that have one non-EU player have 2 quotas. Those clubs that have 2 non-EU players, are awarded 1 quota and 1 conditional quota, which is awarded after: 1) Transferred 1 non-EU player abroad, or 2) Release 1 non-EU player as free agent, or 3) A non-EU player received EU nationality. Clubs with 3 or more non-EU players, have 2 conditional quotas, but releasing two non-EU players as free agent, will only have 1 quota instead of 2. Serie B and Lega Pro clubs cannot sign non-EU player from abroad, except those followed the club promoted from Serie D.
Big clubs with many foreigners, usually borrow quotas from other clubs that have few foreigners or no foreigners, in order to sign more non-EU players. Adrian Mutu joined Juventus via Livorno in 2005, at that time Romania was not a member of the EU. Other cases include Júlio César, Victor Obinna and Maxwell who joined Internazionale from Chievo (first two) and Empoli respectively.
On 2 July 2010, the above conditional quota reduced back to 1, however if a team did not have any non-EU players, that team could still sign up to 3 non-EU players.
FIFA World Players of the Year.
1Player was a member of the club for the first half of the calendar year (The second part of a finished season - January to May)
2Player was a member of the club for the second half of the calendar year (The first part of a new season - August to December)
UEFA ranking.
UEFA Country Ranking at the end of the 2013-14 season: "Last updated 5 May 2014"
Hourlies.
Until 1993, Serie A matches were played at same hour: on Sunday afternoon, at 2:30 or 4:30 (according to sunlight). In 1993–94 season, Lega Calcio made a notable edit: a deferred match, scheduled for Sunday evening at 8:30 (8:45 from 2009–10). This format was changed again in 1999–2000, due to emergence of pay television in Italian football:
In 2004, due to presence of 20 teams, is also possible to play in midweek: on Wednesday evening, with some matches on Tuesday and others on Thursday (at 8:45). In 2010, "lunch match" was introduced: a match played on Sunday at 12:30. Finally, in few weeks, matches can be played on Friday or on Monday (in evening hourly).

</doc>
<doc id="15501" url="http://en.wikipedia.org/wiki?curid=15501" title="Intoxicative inhalant">
Intoxicative inhalant

Intoxicative inhalants are a broad range of intoxicative drugs whose volatile vapors or gases are taken in via the nose and trachea. They are taken by room temperature volatilization or from a pressurized container (e.g., nitrous oxide), and do not include drugs that are sniffed after burning or heating. For example, amyl nitrite and toluene are considered inhalants, but tobacco, cannabis, and crack are not, even though the latter are also inhaled (as smoke).
While some inhalant drugs are used for medical purposes, as in the case of nitrous oxide (a dental anxiolytic), this article focuses on inhalant abuse of household and industrial chemicals, in a manner not intended by the manufacturer. These products are used as recreational drugs for their intoxicating effect. Inhaling volatile substances because of their intoxicating effect is called huffing, sniffing, or bagging. According to a 1995 report by the National Institute on Drug Abuse, the most serious inhalant abuse occurs among children and teens who "...live on the streets completely without family ties." Inhalant users inhale vapor or aerosol propellant gases using plastic bags held over the mouth or by breathing from a solvent-soaked rag or an open container.
The effects of inhalants range from an alcohol-like intoxication and intense euphoria to vivid hallucinations, depending on the substance and the dose. Some inhalant users are injured due to the harmful effects of the solvents or gases or due to other chemicals used in the products that they are inhaling. As with any recreational drug, users can be injured due to dangerous behavior while they are intoxicated, such as driving under the influence. In some cases, users have died from hypoxia (lack of oxygen), pneumonia, cardiac failure or arrest, or aspiration of vomit. Brain damage is typically seen with chronic long-term use as opposed to short-term exposure.
Even though many inhalants are legal, there have been legal actions taken in some jurisdictions to limit access by minors. With solvent glue, normally a legal product, there is a Scottish case where a court has ruled that supplying glue to children is illegal if the store knows the children intend to abuse the glue. In the US, thirty-eight of 50 states have enacted laws making various inhalants unavailable to those under the age of 18, or making inhalant use illegal.
Classification.
Inhalants can be classified by the intended function. Most inhalant drugs that are used non-medically are ingredients in household or industrial chemical products that are not intended to be concentrated and inhaled. A small number of recreational inhalant drugs are pharmaceutical products that are used illicitly.
Chemical structure.
Inhalants can also be classified by chemical structure. Classes include:
Product category.
Another way to categorize inhalants is by their product category. There are three main product categories: solvents; gases; and medical drugs which are used illicitly.
Solvents.
A wide range of volatile solvents intended for household or industrial use are inhaled as recreational drugs. This includes petroleum products (gasoline and kerosene), toluene (used in paint thinner and model glue), and acetone (used in nail polish remover).
Gases.
A number of gases intended for household or industrial use are inhaled as recreational drugs. This includes chlorofluorocarbons used in aerosols and propellants (e.g., hair spray). A gas used as a propellant in whipped cream aerosol containers, nitrous oxide, is used as a recreational drug.
Medical anesthetics.
Several medical anesthetics are misused as recreational drugs, including diethyl ether and nitrous oxide. Diethyl ether has a long history of use as a recreational drug. The effects of ether intoxication are similar to those of alcohol intoxication, but more potent. Also, due to NMDA antagonism, the user may experience all the psychedelic effects present in classical dissociatives such as ketamine in forms of thought loops and feeling of mind being disconnected from one's body. Nitrous oxide is a dental anesthetic which is used as a recreational drug, either by users who have access to medical grade gas canisters (e.g., dental hygienists or dentists) or by using the gas contained in whipped cream aerosol containers. Nitrous oxide inhalation can cause analgesia, depersonalisation, derealisation, dizziness, euphoria, and some sound distortion.
Classification by effect.
It is also possible to classify inhalants by the effect they have on the body. Many inhalants act 
primarily as asphyxiant gases, with their primary effect due to oxygen deprivation. Other agents may have more direct effects at receptors. Inhalants exhibit a variety of mechanisms of action. The mechanisms of action of many non-medical inhalants have not been well elucidated. Anesthetic gases used for surgery, such as nitrous oxide or enflurane, are believed to induce anesthesia primarily by acting as NMDA receptor antagonists, open channel blockers that bind to the inside of the calcium channels on the outer surface of the neuron, and provide high levels of NMDA receptor blockade for a short period of time.
This makes inhaled anesthetic gases different from other NMDA antagonists, such as ketamine, which bind to a regulatory site on the NMDA-sensitive calcium transporter complex and provide slightly lower levels of NMDA blockade, but for a longer and much more predictable duration. This makes a deeper level of anesthesia achievable more easily using anesthetic gases but can also make them more dangerous than other drugs used for this purpose.
Administration and effects.
Inhalant users inhale vapors or aerosol propellant gases using plastic bags held over the mouth or by breathing from an open container of solvents, such as gasoline or paint thinner. Nitrous oxide gases from whipped cream aerosol cans, aerosol hairspray or non-stick frying spray are sprayed into plastic bags. When inhaling non-stick cooking spray or other aerosol products, some users may filter the aerosolized particles out with a rag. Some gases, such as propane and butane gases, are inhaled directly from the canister. Once these solvents or gases are inhaled, the extensive capillary surface of the lungs rapidly absorb the solvent or gas, and blood levels peak rapidly. The intoxication effects occur so quickly that the effects of inhalation can resemble the intensity of effects produced by intravenous injection of other psychoactive drugs.
The effects of solvent intoxication can vary widely depending on the dose and what type of solvent or gas is inhaled. A person who has inhaled a small amount of rubber cement or paint thinner vapor may be impaired in a manner resembling alcohol inebriation. A person who has inhaled a larger quantity of solvents or gases, or a stronger chemical, may experience stronger effects such as distortion in perceptions of time and space, hallucinations, and emotional disturbances.
In the short term, many users experience headache, nausea and vomiting, slurred speech, loss of motor coordination, and wheezing. A characteristic "glue sniffer's rash" around the nose and mouth is sometimes seen after prolonged use. An odor of paint or solvents on clothes, skin, and breath is sometimes a sign of inhalant abuse, and paint or solvent residues can sometimes emerge in sweat.
According to NIH, even a single session of inhalant abuse "can disrupt heart rhythms and lower oxygen levels," which can lead to death. "Regular abuse can result in serious harm to the brain, heart, kidneys and liver."
Dangers and health problems.
Statistics on deaths caused by inhalant abuse are difficult to determine. It may be severely under-reported, because death is often attributed to a discrete event such as a stroke or a heart attack, even if the event happened because of inhalant abuse. Inhalant use or abuse was mentioned on 144 death certificates in Texas during the period 1988–1998 and was reported in 39 deaths in Virginia between 1987 and 1996 from acute voluntary exposure to abused inhalants.
General risks.
Regardless of which inhalant is used, administration can lead to injury or death. One major risk is hypoxia, which can occur due to inhaling fumes from a plastic bag, or from using proper equipment but not adding oxygen or room air. When a gas that was stored under high pressure is released, it cools abruptly and can cause frostbite if it is inhaled directly from the container (when nitrous oxide is used as an automotive power adder, its cooling effect is used to make the fuel-air charge denser. In a person, this effect is potentially lethal). Many inhalants are volatile organic chemicals and can catch fire or explode, especially when combined with smoking. As with many other drugs, users may also injure themselves due to loss of coordination or impaired judgment, especially if they attempt to drive.
Solvents have many potential risks in common, including pneumonia, cardiac failure or arrest, and aspiration of vomit. The inhaling of some solvents can cause hearing loss, limb spasms, and damage to the central nervous system and brain. Serious but potentially reversible effects include liver and kidney damage and blood-oxygen depletion. Death from inhalants is generally caused by a very high concentration of fumes. Deliberately inhaling solvents from an attached paper or plastic bag or in a closed area greatly increases the chances of suffocation. Brain damage is typically seen with chronic long-term use as opposed to short-term exposure. Parkinsonism (see: Signs and symptoms of Parkinson's disease) has been associated with huffing.
Female inhalant users who are pregnant may have adverse effects on the fetus, and the baby may be smaller when it is born and may need additional health care (similar to those seen with alcohol - fetal alcohol syndrome). There is some evidence of birth defects and disabilities in babies born to women who sniffed solvents such as gasoline.
In the short term, death from solvent abuse occurs most commonly from aspiration of vomit while unconscious or from a combination of respiratory depression and hypoxia, the second cause being especially a risk with heavier-than-air vapors such as butane or gasoline vapor. Deaths typically occur from complications related to excessive sedation and vomiting. Actual overdose from the drug does occur, however, and inhaled solvent abuse is statistically more likely to result in life-threatening respiratory depression than intravenous use of opiates such as heroin. Most deaths from solvent abuse could be prevented if individuals were resuscitated quickly when they stopped breathing and their airway cleared if they vomited. However, most inhalant abuse takes place when people inhale solvents by themselves or in groups of people who are intoxicated. Certain solvents are more hazardous than others, such as gasoline.
In contrast, a few inhalants like amyl nitrate and diethyl ether have medical applications and are less harmful, though they are still dangerous when used recreationally. Nitrous oxide is thought to be particularly non-toxic, though long-term use can lead to a variety of serious health problems linked to destruction of vitamin B12 and folic acid.
Risks of specific agents.
The hypoxic effect of inhalants can cause damage to many organ systems (particularly the brain, which has a very low tolerance for oxygen deprivation), but there can also be additional toxicity resulting from either the physical properties of the compound itself or additional ingredients present in a product.
Toxicity may also result from the pharmacological properties of the drug; excess NMDA antagonism can completely block calcium influx into neurons and provoke cell death through apoptosis, although this is more likely to be a long-term result of chronic solvent abuse than a consequence of short-term use.
Sudden sniffing death syndrome.
Inhaling butane gas can cause drowsiness, narcosis, asphyxia, cardiac arrhythmia and frostbite. Butane is the most commonly misused volatile solvent in the UK and caused 52% of solvent-related deaths in 2000. When butane is sprayed directly into the throat, the jet of fluid can cool rapidly to −20 °C by adiabatic expansion, causing prolonged laryngospasm. Sudden sniffing death syndrome is commonly known as SSDS.
Some inhalants can also indirectly cause sudden death by cardiac arrest, in a syndrome known as "sudden sniffing death". The anaesthetic gases present in the inhalants appear to sensitize the user to adrenaline and, in this state, a sudden surge of adrenaline (e.g., from a frightening hallucination or run-in with aggressors), may cause fatal cardiac arrhythmia.
Furthermore, the inhalation of any gas that is capable of displacing oxygen in the lungs (especially gasses heavier than oxygen) carries the risk of hypoxia as a result of the very mechanism by which breathing is triggered. Since reflexive breathing is prompted by elevated carbon dioxide levels (rather than diminished blood oxygen levels), breathing a concentrated, relatively inert gas (such as computer-duster tetrafluoroethane or nitrous oxide) that removes carbon dioxide from the blood without replacing it with oxygen will produce no outward signs of suffocation even when the brain is experiencing hypoxia. Once full symptoms of hypoxia appear, it may be too late to breathe without assistance, especially if the gas is heavy enough to lodge in the lungs for extended periods. Even completely inert gases, such as argon, can have this effect if oxygen is largely excluded.
Legal aspects.
Solvent glue.
Even though solvent glue is normally a legal product, there is a case where a court has ruled that supplying glue to children is illegal. Khaliq v HM Advocate was a Scottish criminal case decided by the High Court of Justiciary on appeal, in which it was decided that it was an offence at common law to supply glue sniffing materials that were otherwise legal in the knowledge that they would be used for self-harm. Two shopkeepers in Glasgow were arrested and charged with supplying to children ‘glue-sniffing kits’ consisting of a quantity of petroleum-based glue in a plastic bag. They argued there was nothing illegal about the items that they had supplied. On appeal, the High Court took the view that, even though glue and plastic bags might be perfectly legal, everyday items, the two shopkeepers knew perfectly well that the children were going to use the articles as inhalants and the charge on the indictment should stand. When the case came to trial at Glasgow High Court the two were sentenced to three years’ imprisonment.
"Thirty-eight of 50 states have enacted laws making various inhalants unavailable to those under the age of 18. Other states prohibit the sale of these items to anyone without recognition of purpose for purchase. Some states mandate laws against using these products for purposes of getting high, while some states have laws about possessing certain inhalants. Nearly every state imposes fines and jail terms for violation of their specific laws."
"Connecticut law bans the unauthorized manufacture or compounding, possession, control, sale, delivery, or administration of any “restricted substance.” It defines restricted substances as... specific volatile substances if they are sold, compounded, possessed or controlled, or delivered or administered to another person for breathing, inhaling, sniffing, or drinking to induce a stimulant, depressant, or hallucinogenic effect. Violators can be fined up to $100." As well, 24 states "ban the use, possession, or sale or other distribution of inhalants... like glue and solvents."
"Louisiana prohibits the sale, transfer, or possession of model glue and inhalable toluene substances to minors. In Ohio, it is illegal to inhale certain compounds for intoxication — a common, general prohibition other states have enacted.
Some states draw their prohibitions more narrowly...In Massachusetts, retailers must ask minors for identification before selling them glue or cement that contains a solvent that can release toxic vapors."
Propellant gases.
"New Jersey... prohibits selling or offering to sell minors products containing chlorofluorocarbon that is used in refrigerant."
Poppers.
The sale of alkyl nitrite-based poppers was banned in Canada in 2013. Although not considered a narcotic and not illegal to possess or use, they are considered a drug. Sales that are not authorized can now be punished with fines and prison. Since 2007, reformulated poppers containing isopropyl nitrite are sold in Europe because only isobutyl nitrite is prohibited. In France, the sale of products containing butyl nitrite, pentyl nitrite, or isomers thereof, has been prohibited since 1990 on grounds of danger to consumers. In 2007, the government extended this prohibition to all alkyl nitrites that were not authorized for sale as drugs. After litigation by sex shop owners, this extension was quashed by the Council of State on the grounds that the government had failed to justify such a blanket prohibition: according to the court, the risks cited, concerning rare accidents often following abnormal usage, rather justified compulsory warnings on the packaging.
In the United Kingdom, poppers are widely available and frequently (legally) sold in gay clubs/bars, sex shops, drug paraphernalia head shops, over the Internet and on markets. It is illegal under Medicines Act 1968 to sell them advertised for human consumption, and in order to bypass this, they are usually sold as odorizers. In the U.S., originally marketed as a prescription drug in 1937, amyl nitrite remained so until 1960, when the Food and Drug Administration removed the prescription requirement due to its safety record. This requirement was reinstated in 1969, after observation of an increase in recreational use. Other alkyl nitrites were outlawed in the U.S. by Congress through the Anti-Drug Abuse Act of 1988. The law includes an exception for commercial purposes. The term "commercial purpose" is defined to mean any use other than for the production of consumer products containing volatile alkyl nitrites meant for inhaling or otherwise introducing volatile alkyl nitrites into the human body for euphoric or physical effects. The law came into effect in 1990. Visits to retail outlets selling these products reveal that some manufacturers have since reformulated their products to abide by the regulations, through the use of the legal cyclohexyl nitrite as the primary ingredient in their products, which are sold as video head cleaners, polish removers, or room odorants.
Nitrous oxide.
In the United States, possession of nitrous oxide is legal under federal law and is not subject to DEA purview. It is, however, regulated by the Food and Drug Administration under the Food Drug and Cosmetics Act; prosecution is possible under its "misbranding" clauses, prohibiting the sale or distribution of nitrous oxide for the purpose of human consumption. Many states have laws regulating the possession, sale, and distribution of nitrous oxide. Such laws usually ban distribution to minors or limit the amount of nitrous oxide that may be sold without special license. For example, in the state of California, possession for recreational use is prohibited and qualifies as a misdemeanour. In New Zealand, the Ministry of Health has warned that nitrous oxide is a prescription medicine, and its sale or possession without a prescription is an offence under the Medicines Act. This statement would seemingly prohibit all non-medicinal uses of the chemical, though it is implied that only recreational use will be legally targeted. In India, for general anaesthesia purposes, nitrous oxide is available as Nitrous Oxide IP. India's gas cylinder rules (1985) permit the transfer of gas from one cylinder to another for breathing purposes. Because India's Food & Drug Authority (FDA-India) rules state that transferring a drug from one container to another (refilling) is equivalent to manufacturing, anyone found doing so must possess a drug manufacturing license.
Patterns of non-medical use.
Inhalant drugs are often used by children, teenagers, incarcerated or institutionalized people, and impoverished people, because these solvents and gases are ingredients in hundreds of legally available, inexpensive products, such as deodorant sprays, hair spray, and aerosol air fresheners. However, most users tend to be "...adolescents (between the ages of 12 and 17)." In some countries, chronic, heavy inhalant use is concentrated in marginalized, impoverished communities. Young people who become chronic, heavy inhalant abusers are also more likely to be those who are isolated from their families and community. The article "Epidemiology of Inhalant Abuse: An International Perspective" notes that "[t]he most serious form of obsession with inhalant use probably occurs in countries other than the United States where young children live on the streets completely without family ties. These groups almost always use inhalants at very high levels (Leal et al. 1978). This isolation can make it harder to keep in touch with the sniffer and encourage him or her to stop sniffing."
The article also states that "...high [inhalant use] rates among barrio Hispanics almost undoubtedly are related to the poverty, lack of opportunity, and social dysfunction that occur in barrios" and states that the "...same general tendency appears for Native-American youth" because "...Indian reservations are among the most disadvantaged environments in the United States; there are high rates of unemployment, little opportunity, and high rates of alcoholism and other health problems." There are a wide range of social problems associated with inhalant use, such as feelings of distress, anxiety and grief for the community; violence and damage to property; violent crime; stresses on the juvenile justice system; and stresses on youth agencies and support services.
Africa and Asia.
Glue and gasoline sniffing is also a problem in parts of Africa, especially with street children, and South Asia. Three of the most widely abused inhalants are the Dendrite brand and other forms of contact adhesives and rubber cements manufactured in Kolkata, and toluenes in paint thinners. Genkem is a brand of glue which had become the generic name for all the glues used by glue-sniffing children in Africa before the manufacturer replaced n-hexane in its ingredients in 2000.
The United Nations Office on Drugs and Crime has reported that glue sniffing is at the core of “street culture” in Nairobi, Kenya, and that the majority of street children in the city are habitual solvent users. Research conducted by Cottrell-Boyce for the African Journal of Drug and Alcohol Studies found that glue sniffing amongst Kenyan street children was primarily functional – dulling the senses against the hardship of life on the street – but it also provided a link to the support structure of the ‘street family’ as a potent symbol of shared experience.
Similar incidents of glue sniffing among destitute youth in the Philippines have also been reported, most commonly from groups of street children and teenagers collectively known as "Rugby" boys, which were named after a brand of toluene-laden contact cement. Other toluene-containing substances have also been subject to abuse, most notably the Vulca Seal brand of roof sealants. Bostik Philippines, which currently owns the Rugby and Vulca Seal brands, has since responded to the issue by adding bitterants such as mustard oil to their Rugby line, as well as reformulating it by replacing toluene with xylene. Several other manufacturers have also followed suit.
Another very common inhalant is Erase-X, a correction fluid that contains toluene. It has become very common for school and college students to use it, because it is easily available in stationery shops in India. This fluid is also used by street and working children in Delhi.
Europe and North America.
In the UK, marginalized youth use a number of inhalants, such as solvents and propellants. In Russia and Eastern Europe, gasoline sniffing became common on Russian ships following attempts to limit the supply of alcohol to ship crews in the 1980s. The documentary "Children Underground" depicts the huffing of a solvent called Aurolac (a product used in chroming) by Romanian homeless children. During the Interbellum the inhalation of ether (etheromania) was widespread in some regions of Poland, especially in Upper Silesia - tens of thousands of people were affected by this problem.
In Canada, Native children in the isolated Northern Labrador community of Davis Inlet were the focus of national concern in 1993, when many were found to be sniffing gasoline. The Canadian and provincial Newfoundland and Labrador governments intervened on a number of occasions, sending many children away for treatment. Despite being moved to the new community of Natuashish in 2002, serious inhalant abuse problems have continued. Similar problems were reported in Sheshatshiu in 2000 and also in Pikangikum First Nation. In 2012, the issue once again made the news media in Canada. In Mexico, the inhaling of a mixture of gasoline and/or industrial solvents, known locally as "Activo" or "Chemo", has risen in popularity among the homeless and among the street children of Mexico City in recent years. The mixture is poured onto a handkerchief and inhaled while held in one's fist.
In the US, ether was used as a recreational drug during the 1930s Prohibition era, when alcohol was made illegal. Ether was either sniffed or drunk and, in some towns, replaced alcohol entirely. However, the risk of death from excessive sedation or overdose is greater than that with alcohol, and ether drinking is associated with damage to the stomach and gastrointestinal tract. Use of glue, paint and gasoline became more common after the 1950s. Abuse of aerosol sprays became more common in the 1980s, as older propellants such as CFCs were phased out and replaced by more environmentally friendly compounds such as propane and butane. Most inhalant solvents and gases are not regulated under drug laws such as the United States' Controlled Substances Act. However, many US states and Canadian cities have placed restrictions on the sale of some solvent-containing products to minors, particularly for products widely associated with sniffing, such as model cement. The practice of inhaling such substances is sometimes colloquially referred to as huffing, sniffing (or glue sniffing), dusting, or chroming.
Australia.
Australia has long faced a petrol (gasoline) sniffing problem in isolated and impoverished aboriginal communities. Although some sources argue that sniffing was introduced by United States servicemen stationed in the nation's Top End during World War II or through experimentation by 1940s-era Cobourg Peninsula sawmill workers, other sources claim that inhalant abuse (such as glue inhalation) emerged in Australia in the late 1960s. Chronic, heavy petrol sniffing appears to occur among remote, impoverished indigenous communities, where the ready accessibility of petrol has helped to make it a common substance for abuse.
In Australia, petrol sniffing now occurs widely throughout remote Aboriginal communities in the Northern Territory, Western Australia, northern parts of South Australia and Queensland. The number of people sniffing petrol goes up and down over time as young people experiment or sniff occasionally. "Boss", or chronic, sniffers may move in and out of communities; they are often responsible for encouraging young people to take it up.
A 1983 survey of 4,165 secondary students in New Lydiate showed that solvents and aerosols ranked just after analgesics (e.g., codeine pills) and alcohol for drugs that were abused. This 1983 study did not find any common usage patterns or social class factors. The causes of death for inhalant users in Australia included pneumonia, cardiac failure/arrest, aspiration of vomit, and burns. In 1985, there were 14 communities in Central Australia reporting young people sniffing. In July 1997, it was estimated that there were around 200 young people sniffing petrol across 10 communities in Central Australia. Approximately 40 were classified as chronic sniffers. There have been reports of young Aboriginal people sniffing petrol in the urban areas around Darwin and Alice Springs.
In 2005, the Government of Australia and BP Australia began the usage of opal fuel in remote areas prone to petrol sniffing. Opal is a non-sniffable fuel (which is much less likely to cause a high) and has made a difference in some indigenous communities.
In popular culture.
Music and musical culture.
One of the early musical references to inhalant use occurs in the 1974 Elton John song "The Bitch Is Back," in the line "I get high in the evening sniffing pots of glue." Inhalant use, especially glue sniffing, is widely associated with the late-1970s punk youth subculture in the UK and North America. Raymond Cochrane and Douglas Carroll claim that when glue sniffing became widespread in the late 1970s, it was "...adopted by punks because public [negative] perceptions of sniffing fitted in with their self-image" as rebels against societal values. While punks at first used inhalants "...experimentally and as a cheap high, adult disgust and hostility [to the practice] encouraged punks to use glue sniffing as a way of shocking society." As well, using inhalants was a way of expressing their anti-corporatist DIY (do it yourself) credo; by using inexpensive household products as inhalants, punks did not have to purchase industrially manufactured liquor or beer.
One history of the punk subculture argues that "substance abuse was often referred to in the music and did become synonymous with the genre, glue sniffing especially" because the youths' "...faith in the future had died and that the youth just didn't care anymore" due to the "awareness of the threat of nuclear war and a pervasive sense of doom." In a BBC interview with a person who was a punk in the late 1970s, they said that "there was a real fear of imminent nuclear war — people were sniffing glue knowing that it could kill them, but they didn't care because they believed that very soon everybody would be dead anyway."
A number of 1970s punk rock and 1980s hardcore punk songs refer to inhalant use. The Ramones, an influential early US punk band, referred to inhalant use in several of their songs. The song "Now I Wanna Sniff Some Glue" describes adolescent boredom, and the song "Carbona not Glue" states, "My brain is stuck from shooting glue." An influential punk fanzine about the subculture and music took its name ("Sniffin' Glue") from the Ramones song. The 1980s punk band The Dead Milkmen wrote a song, "Life is Shit" from their album "Beelzebubba", about two friends hallucinating after sniffing glue. Punk-band-turned-hip-hop group the Beastie Boys penned a song "Hold it Now – Hit It," which includes the line "cause I'm beer drinkin, breath stinkin, sniffing glue." Pop punk band Sum 41 wrote a song, "Fat Lip", which refers to a character who does not "... make sense from all the gas you be huffing..." The song "Lança-perfume", written and performed by Brazilian popstar Rita Lee, became a national hit in 1980. The song is about chloroethane and its widespread recreational sale and use during the rise of Brazil's carnivals.
Inhalants are referred to by bands from other genres, including several grunge bands—an early 1990s genre that was influenced by punk rock. The 1990s grunge band Nirvana, which was influenced by punk music, penned a song, "Dumb", in which Kurt Cobain sings "my heart is broke/But I have some glue/help me inhale /And mend it with you". L7, an all-female grunge band, penned a song titled "Scrap" about a skinhead who inhales spray-paint fumes until his mind "starts to gel". Also in the 1990s, the Britpop band Suede had a UK hit with their song "Animal Nitrate" whose title is a thinly veiled reference to amyl nitrite. The Beck song "Fume" from his "Fresh Meat and Old Slabs" release is about inhaling nitrous oxide. Another Beck song, "Cold Ass Fashion", contains the line "O.G. – Original Gluesniffer!" Primus's 1998 song "Lacquer Head" is about adolescents who use inhalants to get high. Hip hop performer Eminem wrote a song, "Bad Meets Evil", which refers to breathing "...ether in three lethal amounts." The Brian Jonestown Massacre, a retro-rock band from the 1990s, has a song "Hyperventilation", which is about sniffing model-airplane cement. Frank Zappa's song "Teenage Wind" from 1981 has a reference to glue sniffing:
"...Nothing left to do but get out the 'ol glue; Parents, parents; Sniff it good now..."
Films.
A number of films have depicted or referred to the use of solvent inhalants. In the 1980 comedy film "Airplane!", the character of McCroskey (Lloyd Bridges) refers to his inhalant use when he states, "I picked the wrong week to quit sniffing glue." In the 1996 film "Citizen Ruth", the character Ruth (Laura Dern), a homeless drifter, is depicted inhaling patio sealant from a paper bag in an alleyway. In the tragicomedy "Love Liza", the main character, played by Philip Seymour Hoffman, plays a man who takes up building remote-controlled airplanes as a hobby to give him an excuse to sniff the fuel in the wake of his wife's suicide.
Harmony Korine's 1997 "Gummo" depicts adolescent boys inhaling contact cement for a high. Edet Belzberg's 2001 documentary "Children Underground" chronicles the lives of Romanian street children addicted to inhaling paint. In "The Basketball Diaries", a group of boys are huffing carbona cleaning liquid at 3 minutes and 27 seconds into the movie; further on, a boy is reading a diary describing the experience of sniffing the cleaning liquid.
In the David Lynch film "Blue Velvet", the bizarre and manipulative character played by Dennis Hopper uses a mask to inhale amyl nitrite. In "Little Shop of Horrors", Steve Martin's character dies from nitrous oxide inhalation. The 1999 independent film "Boys Don't Cry" depicts two young low-income women inhaling aerosol computer cleaner (compressed gas) for a buzz. In "The Cider House Rules", Michael Caine's character is addicted to inhaling ether vapors.
In "Thirteen", the main character, a teen, uses a can of aerosol computer cleaner to get high. In the action movie "Shooter", an ex-serviceman on the run from the law (Mark Wahlberg) inhales nitrous oxide gas from a number of Whip-It! whipped cream canisters until he becomes unconscious. The film "Fear and Loathing in Las Vegas" describes how the two main characters inhale diethyl ether and amyl nitrite. The South African film "The Wooden Camera" also depicts the use of inhalants by one of the main characters, a homeless teen, and their use in terms of socio-economic stratification. The titular characters in "Samson and Delilah" sniff petrol; in Samson's case, possibly causing brain damage.
In the 2004 film "Taxi", Queen Latifah and Jimmy Fallon are trapped in a room with a burst tank containing nitrous oxide. Queen Latifah's character curses at Fallon while they both laugh hysterically. Fallon's character asks if it is possible to die from nitrous oxide, to which Queen Latifah's character responds with "It's laughing gas, stupid!" Neither of them suffered any side effects other than their voices becoming much deeper while in the room.
In the French horror film "Them", (2006) a French couple living in Romania are pursued by a gang of street children who break into their home at night. Olivia Bonamy's character is later tortured and forced to inhale aurolac from a silver-colored bag. During a flashback scene in the 2001 film "Hannibal", Hannibal Lecter gets Mason Verger high on amyl nitrite poppers, then convinces Verger to cut off his own face and feed it to his dogs.
Books.
The science fiction story "Waterspider" by Philip K. Dick (first published in January 1964 in "If" magazine) contains a scene in which characters from the future are discussing the culture of the early 1950s. One character says: "You mean he sniffed what they called 'airplane dope'? He was a 'glue-sniffer'?", to which another character replies: "Hardly. That was a mania among adolescents and did not become widespread in fact until a decade later. No, I am speaking about imbibing alcohol."
In the 1999 horror-thriller novel "Hannibal" by Thomas Harris, psychiatrist Hannibal Lecter gets his patient Mason Verger high on amyl nitrite poppers, then convinces the intoxicated Verger to cut off his own face and feed it to his dogs.
Television.
In the comedy series "Newman and Baddiel in Pieces", Rob Newman's inhaling gas from a foghorn was a running joke in the series. One episode of the "Jeremy Kyle Show" featured a woman with a 20-year butane gas addiction. In the series "It's Always Sunny in Philadelphia", Charlie Kelly has an addiction to huffing glue. Additionally, season nine episode 8 shows Dennis, Mac and Dee getting a can of gasoline to use as a solvent, but instead end up taking turns huffing from the canister.
A 2008 episode of the reality show "Intervention" (season 5, episode 9) featured Allison, who was addicted to huffing computer duster for the short-lived, psychoactive effects. Allison has since achieved a small but significant cult following among bloggers and YouTube users. Several remixes of scenes from Allison's episode can be found online. Since 2009, Allison has worked with drug and alcohol treatment centers in Los Angeles County. In the third episode of season 5 of "American Dad!", titled "Home Adrone", Roger asks an airline stewardess to bring him industrial adhesive and a plastic bag. In the seventh episode of the fourteenth season of South Park, Towelie, an anthropomorphic towel, develops an addiction to inhaling computer duster. In the show "Squidbilles", the main character Early Cuyler is often seen inhaling gas or other substances.

</doc>
<doc id="15505" url="http://en.wikipedia.org/wiki?curid=15505" title="Iceman (comics)">
Iceman (comics)

Iceman (Robert "Bobby" Drake) is a fictional superhero appearing in American comic books published by Marvel Comics and is a founding member of the X-Men. A mutant, Iceman has the ability to manipulate ice and cold, meaning he can freeze anything around him and can also turn his body into ice. Although he is an Omega-level mutant, Drake has yet to tap into his full mutant potential. Over the years he has taken more interest in developing his abilities. One of the original X-Men, Iceman has had a frequent presence in X-Men (and Spider-Man)-related comics, video games, animated series, and movies.
Shawn Ashmore portrays Iceman in the "X-Men" films, and voices the character in "The Super Hero Squad Show".
Publication history.
Created by writer Stan Lee and artist/co-writer Jack Kirby, the character first appeared in "X-Men" #1 (September 1963). Lee later admitted that Iceman was created essentially as a copy of the Human Torch, only using the opposite element for his power.
Iceman has been featured in two self-titled limited comic book miniseries, one in 1984-85 written by J. M. DeMatteis and another in the 2000s by Andy Lanning and Dan Abnett, with art by Karl Kerschl. DeMatteis said of the first series, "It was my idea, so there was no one to blame but myself. I'll just say that it was a mistake and if the series made any sense whatsoever it was due to [editor] Bob Budiansky. That was a case where the editor's input was really needed - and Bob was a big help."
A mainstay in most X-Men titles, Iceman has been a main character in both "Uncanny X-Men" and the second volume of "X-Men" and was also featured in the "Champions" and "New Defenders" as a member. He was a main character in the first volume of "X-Factor", and a star in flashback stories when he was a teenager in "X-Men: The Hidden Years" and "X-Men: First Class".
In April 2015, in issue 40 of "All-New X-Men" Iceman is "outed" as being gay.
Fictional character biography.
Early life.
Robert Louis "Bobby" Drake was born in Port Washington, Long Island, New York, to William Robert Drake and Madeline Beatrice Bass-Drake. His father is Irish Catholic, and his mother is Jewish. Bobby's powers first manifested when he was on a date with Judy Harmon, and a local bully by the name of Rocky Beasely tried to take Judy away for himself. Knowing Judy could not put up a good fight, Bobby pointed his hand at Beasely and encased him in a block of ice. Later, the local townspeople, having heard of the incident, came looking for him in the form of an angry mob. The local sheriff had no choice but to put Bobby in jail for his own "protection". While Bobby sat in his cell at the sheriff station, the outer wall was blown open, and a young man named Scott Summers walked in and offered to take Bobby with him. After Bobby turned him down, the two mutants got into a short battle, which was soon ended by the arrival of Professor Charles Xavier.
After Xavier spoke with Bobby and his parents, Bobby's parents suggested that he go with Professor Xavier to his "school for gifted youngsters". Bobby took the suggestion and left with Professor Xavier and Cyclops to become the second member of the X-Men. He is later joined by Henry "Hank" McCoy, Jean Grey, and Warren Worthington III as the founding members of the X-Men. Iceman quickly befriends Hank McCoy (Beast), and the two serve as comic relief for the team. Drake remains self-conscious regarding the fact that he is the youngest member of the group. Appearing in his original snow covered form, he first battles Magneto along with the rest of the team, and later the Brotherhood of Evil Mutants. Bobby Drake's first girlfriend is Zelda. Not long after, he takes on a new ice-covered form. He then teams up with the Human Torch for the first time. The two would become close friends as time went on. With the X-Men, he visits the Savage Land and meets Ka-Zar for the first time. He then battles the Juggernaut, and is badly injured in his first battle against the Sentinels. He next battles Magneto by himself. Later, he visits Subterranea for the first time. Then, he and Beast battle the Maha Yogi. During his original stint with the X-Men, Drake pursues a relationship with Lorna Dane, although the relationship does not last. Iceman is among the original X-Men captured by Krakoa, leading to a new incarnation of X-Men of which he is not a member. With most of the original team, he quits the X-Men.
Champions and Defenders.
Iceman moves to the American west coast to attend UCLA and becomes a founding member of The Champions of Los Angeles. However, the Champions soon dissolve.
Iceman is then abducted by Master Mold, and alongside Angel, he encounters the Hulk. Iceman next aids the Thing in battling the Circus of Crime. Drake retires from life as a superhero to earn a college degree in accounting - but apparently at a college on the east coast, not UCLA. While in college, he briefly rejoins the X-Men to rescue the captives of Arcade's henchman, Miss Locke.
Iceman is reunited with Beast, encounters Cloud, and then returns as a full-time superhero in an incarnation of the Defenders alongside his former teammates, Angel and Beast. He also battles Professor Power's Secret Empire while with the Defenders. After the Defenders disband, Drake embarks on his career as an accountant.
Some time later, Iceman encounters Mirage, the "daughter" of Oblivion. Iceman journeys back in time and meets his parents before he was born, and battles Oblivion and Mirage. He then reconciles with his parents.
X-Factor.
The original X-Men, including Iceman, reunite to form the superhero team X-Factor. With this new team, he encounters Apocalypse for the first time.
During his time with the team, Loki captures Bobby, hoping to use him to gain control over the Frost Giants. Loki enhances Bobby's powers and then extracts them to restore the size of the Frost Giants. Iceman is rescued by Thor. Loki's tampering increases Bobby's powers to such an extent that he begins to lose control of his abilities. During a later battle with the Right, he is fitted with a power-dampening belt which actually helps him control his abilities. Once able only to sheathe his own body in a protective coating of ice, Bobby finds he can encase the entirety of the Empire State Building. With time, Bobby gains sufficient control over his augmented powers and is able to stop using the inhibitor belt. Believing he has achieved his full potential, Bobby does not attempt to develop his abilities further.
With X-Factor, Bobby then defeats Apocalypse's Horsemen. Iceman helps watch over many of the younger superheroes, something he once was. Most notably, he and Beast help Boom Boom gain a more normal life. For a brief while, he also helps supervise the New Mutants and their sister team, the X-Terminators. They, in turn, save him from the deadly kiss of Infectia.
Bobby also develops a romantic relationship with Opal Tanaka. After a session of ice sledding, she discovers threatening mail in her mailbox, a precursor to harassment by her cybernetically-enhanced relatives of the Tatsu Clan of the Yakuza, which Bobby helps her out with.
After the Muir Island Saga, Iceman rejoins the X-Men along with the rest of X-Factor.
Back with the X-Men.
When he rejoined the X-Men it seemed that he learned to control his powers and no longer needed the inhibitor belt, this made him believe he reached the limit of his powers. The X-Men were separated into two groups, Iceman was placed in the gold team, led by Storm, along with fellow original X-Men Jean Grey (now sans codename) and Archangel.
One day he took Opal to eat with his parents, however his dad began humiliating her because of her Japanese heritage. The four are attacked by the Cyber-Samurai, which added to William Drake's prejudices about the girl. When Bobby came across Mikhail Rasputin he used his mutant abilities on him. Bobby discovered that his potential was still far from being reached as he converted his body into ice, not just covered by it. By turning his entire body to ice, instead of just wearing an icy exterior, Bobby now was capable of using his power in new, aggressive ways, adding spikes and padding to his ice structure.
Too busy with the many threats that the X-Men faced every day, Bobby let his relationship with Opal deteriorate and, when they finally saw each other again after weeks, it was only to save her from an attack by mutant haters. Annoyed that she could only gain his attention by nearly getting killed, Opal broke off their relationship.
Later, as he was checking on Emma Frost who was in a comatose state after the mutant Trevor Fitzroy unleashed the mutant-hunting Sentinels on Emma Frost and her students known as the Hellions, the mansion was hit by an electricity breakdown. Emma woke up disoriented, possesses Bobby's mind and used his powers in ways Bobby never had; she froze an entire river and traveled through water. She was looking for her pupils but after finding out they were dead, she left Bobby's body.
Bobby invites Rogue to tag along on a visit to his parents. Wrongly assuming a romantic relation, his father disapproves of Rogue, verbally attacking them with the same prejudices he expressed with Opal. This time, Bobby had enough and left after telling his father that he should just accept the fact that he is a mutant and he would never fit the definition he has of normal. He was upset that Emma exhibited greater control of his powers than he had. Since Rogue was having problems with Gambit, the two of them go on a road trip to ease their minds.
During the Legion Quest, Iceman was one of the X-Men who goes back in time to stop Legion from killing Magneto. They succeed, but only partially. Legion does not kill Magneto, but instead accidentally murdered Xavier, his own father, years before Legion had been conceived much less born. This paradox caused the events of the Age of Apocalypse. At the last moment before the original reality ended, Iceman's fellow X-Men, Rogue and Gambit shared a kiss. When reality resumed, Rogue's mind-absorbing touch renders Gambit comatose. Having absorbed some secret haunting memories, she needs to get away from the X-Men and Iceman volunteers to join her on a road trip, though at the same time he was starting to see visions of Emma Frost. When Gambit awoke from his coma, he tracked them down and confronted Rogue about what she saw in his mind. She broke off their relationship officially and left for good. Iceman and Gambit returned to the X-Men.
When the entity known as Onslaught first appeared, Iceman was chosen as one of his test subjects, along with Storm, Cyclops and Wolverine. They were pitted against a servant of Onslaught named Post, in a specific battle area of harsh environment to test the extent of their abilities. They won and were returned to the mansion. However, Iceman's chest had been shattered in his iceform during battle, making it impossible to change back to human form. He confronted Emma Frost and demanded to know what she did to his powers in his body and how to save himself. She refused to help since she knew that Bobby would have to do it himself. When he captured her with his ice powers, she telepathically showed him his insecurities. By confronting Opal and his father in her simulation, Iceman realized that Emma was right and managed to transform back to his human body with his chest fully intact.
When Graydon Creed was running for President (with a heavy anti mutant campaign) Bobby was chosen to infiltrate in the campaign, his father stand out in a crowd and spoke in favor of the mutants, this came as a surprise to Bobby. His father connection with Bobby was discovered, so the people that worked for Graydon captured him and almost beat him to death. Bobby decided to stay away from the X-Men for a while to be with his dad.
Zero Tolerance came across and Bobby found and helped Cecilia Reyes who was trying to keep a secret that she was a mutant, they also joined with Charlotte Jones and the morlock Marrow, after Bastion was defeated he took Cecilia and Marrow to the mansion, and soon left the X-Men again for a while to be with his parents.
Much later, the X-Men found evidence in one of Destiny's journals of a group known as the Twelve, including Xavier, Magneto, Cyclops, Phoenix, Iceman, Polaris, Storm, Cable, Bishop, Sunfire, Mikhail Rasputin and the Living Monolith. They also learned that the Apocalypse’s Horsemen had been kidnapping these mutants from around the globe. Iceman was captured in the woods near his home by Deathbird, who had become the Horseman War. Gathering the remaining Twelve, the X-Men traveled to Egypt and confronted Apocalypse and the Skrulls. Apocalypse and his forces captured all of the Twelve during the battle, using them in a ritual to give the Chaos-Bringer a new body and incredible power. Magneto and Polaris created opposite magnetic polarities, Iceman, Storm, and Sunfire provided elemental extremes, Cyclops, Phoenix, and Cable gave the sheer power of family, Xavier represented the power of mind and Bishop and Mikhail stood for time and space, while the Monolith linked all their energies together. Nate Grey was to be Apocalypse's new host, a powerhouse to store his massive lifeforce. The Twelve managed to free themselves and Cyclops sacrificed his own body and lifeforce to keep Apocalypse from getting Nate. Though the new Apocalypse was defeated, Cyclops seemed lost forever.
After that incident, Iceman helped the Beast to secretly enter Genosha, as he wanted to study the legacy-infected mutates. When the High Evolutionary released his anti-mutation wave, they were trapped in the war-ravaged country. With the aid of Magneto, they escaped and joined forces with the rest of the scattered X-Men. They raided the Evolutionary's satellite, disabled the mutation field and defeated Sinister, who had been manipulating the Evolutionary.
Iceman was recruited by the living ship Prosh, along with other mutants, like Jean Grey, Mystique, Toad and Juggernaut, to preserve evolution and save it. In this journey Iceman developed his powers even further, which led him to not be afraid anymore of the natural course of his powers and he returned to the X-Men. He joined a new team of X-Men, consisting of Angel, Nightcrawler, Wolverine and Chamber, he was using his powers in a whole new way now, he was just channeling it and not turning his body into ice.
Second mutation.
During a heated battle with a recently evolved Black Tom Cassidy, he received a chest wound. After returning to normal his chest did not fully recover and some parts of it remained icy, and he was unable to return them to normal. At first he became afraid of it but in time it made him gain a new attitude in life, even rude at some times.
Nobody seemed to realize it at the time, but evidently this was an early sign of Iceman undergoing a secondary mutation of his own. When he repeatedly tried to evade his regular medical check-ups, school nurse Annie Ghazikhanian recognized that something odd was going on with him and pressured him to show her the wound. Bobby made her promise not to tell anyone and showed her that parts of his chest are now made of ice and he is unable to change them back to flesh and blood. Iceman wonders if he will entirely turn into ice on a permanent basis.
He developed an attitude that, to some of the newer addition to the X-Men, like Stacy X, Juggernaut and Northstar, he comes across as rather arrogant, denouncing their status as team members, as they have not been around as long as he. Iceman even went as far as offending Nightcrawler by claiming that only the original five, and no one else, has the right to call themselves an X-Man.
Bobby was further frustrated when Havok and Polaris announced their imminent wedding, him having carried a torch for Lorna for quite some time. In his frustration he turned to Annie, who had problems with the wedding too, as she was secretly in love with Havok. The nurse surprised him with the accusation of him being a racist - feeling comfortable as a mutant who could pass like a human when needed, opposed to being a fully obvious mutant or a “mere” human. With his secondary mutation manifesting, though, Iceman was in danger of losing this status. Shocked by the truth of her words, Bobby fully opened up about his fears, that as a man fully made of ice he could never feel the warmth of a physical relationship again. Touched, Annie allowed him to kiss her, but when Havok called off the wedding, wanting to be with Annie instead, she quickly dumped Bobby.
Upon encountering Azazel and his followers, Iceman's body is shattered from the neck down. Afterward, he regains his entire ice form, but cannot change back to his human appearance. As a result, Bobby becomes both bitter and despondent because of this drastic change.
Decimation.
After the events of the "House of M" storyline, Iceman finds himself to be flesh and blood again and believes himself to have lost his powers. Shortly after, while being held at gunpoint by the Leper Queen, Emma Frost "pushes" something in Bobby's mind that forces him to turn to ice. Further examination reveals that Drake had not lost his powers, but rather had unconsciously repressed them on his own. After this, Bobby is once again able to revert from ice to flesh.
Rogue's Team.
Iceman joins Rogue's team after she tells him that she wants him as a member. Their first mission as a team is to fight a new threat, a powerful group known as the Children of the Vault. The team is successful and during this time, Bobby learns that he can be completely destroyed but then pull himself back together again. It was shown several times during the arc.
The next mission for the team was to locate a man called Pandemic and defeat him. The team was again successful, but Rogue was infected with a virus called Strain 88. Cable took the team, including Bobby, to his island so Rogue could get treatment.
While on Cable's island, the team and Iceman began working to defeat the Shi'ar weapon known as the Hecatomb. During the chaos, he shared a passionate kiss with Mystique. Even as he did so, he saved many lives by containing the explosion of the Conquistador, and, later, the Hecatomb itself.
Blinded by the Light.
As the team recovers from Hecatomb attack in Rogue's childhood home, it appears that Mystique and Iceman begin a romantic relationship. This was a front, as Mystique was using Iceman and the X-Men as a Marauder spy for Mister Sinister. Marauders soon infiltrated the house; they attempt to gain access to Destiny's Diaries on the order of Mr. Sinister (who has been gathering information about the future from anybody and anything that could foretell the future). Bobby and Cannonball escape from the Marauders in the X-Jet, with help from Emma Frost. They are pursued by Sunfire; they manage to get the better of him and take him prisoner, but not before he manages to cripple the jet. While Sunfire is unconscious, Iceman and Sam discuss the Mauraders' plan to eliminate all precognitive mutants and anyone with knowledge of the future as well as retrieving Destiny's Diaries before the Marauders can. During this time, Bobby displayed sub-atomic control of energy transfers when he prevented Sunfire from using his fire-based powers.
Cannonball and Bobby, telepathically prompted by Emma Frost, attempt to recover the diaries which are hidden in a dilapidated brewery. Mr. Sinister uses the reverse-engineered version of Xavier's Cerebro to track the pair of X-Men to the brewery. The Marauders attack Cannonball and Iceman and overtake them. Bobby, while in his ice form, suffers a gunshot wound from Mystique, which severs one of his arms above the elbow. Mister Sinister, who takes Cannonball prisoner, attempts to telepathically erase his mind so that the X-Men will find him as an empty shell. Iceman attacks Sinister, distracting him, which allows both of the X-Men to escape.
Messiah Complex.
The New X-Men team decided to raid the headquarters of the Purifiers in Washington, D.C., but were forced to retreat. Pixie teleported them back to the mansion in a rush, but the entire team was scattered between D.C. and Westchester. Iceman, after recovering from his injuries, volunteered to go look for them and was given telepathic directions by Emma Frost.
Iceman was successful in finding the New X-Men, most of them injured. On the way back, they found that the O*N*E* Sentinels guarding the Xavier Institute had become infected by nano-Sentinels and attacked the school. Iceman and New X-Man X-23 helped out in the battle with the O*N*E* Sentinels. With the help of Dust and X-23, the X-Men were able to survive this battle but the nano-Sentinel infected human escaped.
Soon, Iceman participated in the final battle against the Marauders, the Acolytes, and Predator X. He was one of the X-Men who came running in to fight Predator X after it swallowed Wolverine whole. Unfortunately, he also witnessed his mentor, Professor Xavier, "killed" by Bishop's bullet that was not meant for him.
Manifest Destiny.
According to writer Mike Carey, "the events leading up to "Messiah Complex," [when Iceman] got blindsided by Mystique" are "still preying on [Iceman] to a large extent" and "some of those events [will be revisited]. Bobby is questioning his role as a member of the X-Men and contemplating his trip west to San Francisco, but he's not entirely sure what his next move is going to be." "Carey's Iceman story in "Manifest Destiny" takes place before the recent "Uncanny X-Men" arc that saw Iceman arrive in San Francisco, and follows Iceman as he travels west on a cross-country trip to the X-Men's new home." Bobby was having problems controlling his powers once again, so he decided to visit Opal Tanaka, his ex-girlfriend. Opal advised him to call Beast and ask him about it, Beast said that it may be the neural inhibitor toxins that Mystique used on him months ago, but only by running tests on Bobby, Beast could have a straight answer. When he entered a plane with Opal a bomb detonates, and Opal reveals herself as Mystique. Iceman subsequently asked if she did so to mess with his head and she replies "No lover, your melting point." She shoots him out of the plane.
Mystique followed him to the hospital and injected him with more neural inhibitor, this time enough to kill him, Bobby managed to find a way to use his powers to counter the effect of the toxins and he rose "good as new". When he confronted her again, they exchanged a passionate conversation. Mystique cried and realized she actually did care about Bobby and threw herself off a bridge seemingly killing herself, Bobby realized what she meant with "kill or cure" and was grateful.
Divided We Stand.
Iceman arrives in San Francisco, where he was set to meet Hepzibah, Warpath, and Angel. All four are caught in the effects of a city-wide illusion created by Martinique Jason, who used her powers to transform the city into a hippie paradise. Now calling himself "Frosty", he and the others are sent by Martinique to confront Scott Summers and Emma Frost. Emma Frost is able to break up the illusion and free everyone. They eventually set up their base of operations in San Francisco as X-Men.
Secret Invasion and Utopia.
Iceman is one of the X-Men that assists in fighting the Skrull invasion in San Francisco.
Iceman rescues Colossus from Venom and is later made a part of a team to battle Emma Frost's Dark X-Men. During the final battle on Utopia, Iceman teams up with the X-Students to take on Mimic. Iceman had the labor of providing water to the population. He attempted to use humor to keep everyone's spirits up even though he believed that the situation was helpless and that the X-Men were living in the last days of mutantkind. Bobby then helped defeat Predator X and also helped stop Selene's resurrected army's invasion of Utopia.
Second Coming.
During the battle with Bastion's Nimrod Sentinels, Iceman is severely injured following an attack from one of them that reversed his ice form and left him with burns on his body. He is only there for a short time because his mutant powers help him minimize the injuries he suffered and he is seen back in battle alongside Psylocke and the other X-Men.
Five Lights.
When Cerebro picked up the new mutant signatures, Cyclops sent X-Men to watch them until Hope could reach them. Iceman and Angel were sent to watch over the Light in Canada, Laurie. The transformation into a mutant was a painful experience and Bobby could not stand to listen to the girl in such agony. It was when the girl was about to kill herself that Hope arrived and was able to fully activate her powers.
Bobby was sent to talk to a super hero public relations official. During the meeting she told him that she needed full disclosure of any of the X-Men secrets. Bobby hesitantly told her that Magneto was back and staying with the X-Men. The woman than asked if Worthington was funding them still, Bobby told her yes and asked why she wanted to know, she said because she was going to have a lot of overtime.
Curse of the Mutants.
During the "Curse of the Mutants" storyline, an all out war of mutants against vampires, Bobby was blessed by a priest so his powers can function as holy water, a plan conceived by Cyclops.
Regenesis.
Early solicitations show that after Wolverine and Cyclops have a major falling out, Wolverine decides to branch off and open "The Jean Grey School for Higher Learning" back in New York. Iceman is the first person Wolverine approaches and recruits to his new X-Men squad as both a professor and teammate. Iceman is chosen because Wolverine feels he has the kind of spirit his the new school needs. He has shared some kisses with Kitty Pryde.
All-New X-Men.
Following the war with the Phoenix Five, as Cyclops begins to lash out against government "oppression" of mutants, a chance comment by Bobby about how the old Cyclops wouldn't tolerate what he is currently doing inspires Beast to travel back in time and recruit the original five X-Men to stop Cyclops. The past and present Bobby are particularly shocked when they see each other. Later, the younger, time-displaced Bobby is forced to confront being gay by his teammate, Jean, who privately asks him why he calls women "hot," when she knows via her psychic abilities that he is gay. This causes the younger Bobby to speculate as to the complicated identity issues faced by his older self and the decisions his older self may have made in the time between them.
Powers and abilities.
Iceman possesses the power to instantly decrease the temperature of ambient water vapor in his immediate environment to below zero degrees Celsius, thereby freezing it into ice. He is able to make ice that will not break unless he wills it to. In this manner he is able to quickly form a great variety of ice structures, including projectiles, shields, ladders, baseball bats, etc. He often makes ice slides which form rapidly beneath and behind his feet, moving him along the slick surface at high speeds. He is also able to form exceedingly complicated structures within relative short time, such as miniature cities. Originally, Iceman's own body temperature would lower dramatically when his powers were active, reaching -105 °F within a few tenths of a second (now his body usually converts to organic ice; see below). Iceman is immune to sub-zero temperatures; he is also able to perceive the thermal energy level of objects around him. Because cold is the absence of heat, Iceman does not actually 'emanate' cold; rather, he decreases thermal energy. As mentioned by writer Mike Carey, Iceman is "an Omega level mutant...[and] has powers that can influence the ecosystem of the entire world."
In his early appearances, Iceman generally covered his body in a thick layer of what appeared to be snow; hence he looked more like a traditional snowman than an ice-man. Upon further training in the use of his powers, he was able to fashion an armor of solid ice around his body when using his powers, which afforded him some degree of protection against concussive force and projectiles. Later on, he manifested the ability to convert the tissue of his body into organic ice. He sometimes augments his organic ice form with razor sharp adornments to his shoulders, elbows, knees, and fists. Iceman has also been able to move rapidly to another distant location while in his organic ice form, being able to deposit his bodily mass into a river and reconstitute his entire mass a great distance away in a matter of minutes (by temporarily merging his molecules with those of the river). On one occasion, Iceman suffered a severe chest injury while in his ice form and was able to heal himself by converting back into his normal human form.
Iceman is also able to reconstitute his organic ice form if any part of it is damaged, or even if it is completely shattered, without permanently harming himself. He can temporarily add the mass of a body of water to his own, increasing his mass, size, and strength. He can survive not only as ice, but as liquid water and water vapor. He can also transform his body from a gaseous state back to a solid, although it is physically and mentally taxing. Iceman can also freeze sea water, as seen during the "Operation Zero Tolerance" story arc. While he usually does not use his powers in lethal ways his powers are so vast that it extends to the molecular level, to the point that he can freeze all of the molecules of an object/being with a thought; he once froze every single molecule of water within the body of David Haller.
Iceman is also able to dissolve his own icy constructs.
Iceman's powers were pushed to their limit while possessed by Emma Frost, who used Iceman to discover the fate of her Hellions. During this time Iceman was able to control all forms of moisture, freeze fluids inside people's bodies, travel as a liquid, solid or gas. Not even the combined might of the X-Men Gold team was able to stop Emma Frost in Iceman's body. Following this, Bobby confronted Emma about how she was able to use his powers so effectively. While together they made some initial progress, she refused to train him further. Instead he turned to Storm because they share similar elemental powers and she agreed to tutor him.
When Iceman was injected with Sinister's neuro-inhibiter by Mystique, he was able to save himself by drawing in all of the ambient moisture around him, rapidly replacing his poisoned cells with healthy material before the injection could kill him.
During the 2013 - 2014 "Battle of the Atom" storyline, Iceman's future self revealed that he has the ability to create semi-independent ice structures that can act on their own, although one of these structures - demonstrating a Hulk-like physique and intellect - has gone on to join the future version of the Brotherhood.
Aside from his superhuman powers, Iceman is also a fair hand-to-hand combatant, and received combat training at Xavier's School as well as coaching from the Black Widow and Hercules while serving with the Champions of Los Angeles. Iceman has as much combat training as Cyclops or Beast.
Characterization.
Personality.
According to writer Mike Carey "one of Iceman's best personality traits is that emotionally Bobby Drake is like the ice he manipulates -- not cold but transparent. 'He's devastatingly honest. He is very up-front with his emotions and his thoughts all the time.'" "Also, he's obviously incredibly brave both in terms of facing external, physical danger as well as facing up to unpleasant situations and admitting his own mistakes."
This emotional honesty can often complicate matters for Bobby, especially in matters of love. Bobby is sometimes looked at as being immature by his teammates, as well as outsiders.
Friendships and relationships.
Iceman is often the center point in dealing with the non-romantic friendships among the X-Men. While Bobby is often well liked by his teammates, his moodiness and occasional insecurities sometimes push them away. Bobby's best friends among the X-Men include Angel, Cannonball, and Rogue. Outside of the X-Men, he is good friends with Spider-Man for their shared sense of humor. He is also close friends with the Human Torch as their personalities complement each other and their respective powers are polar opposites. Originally, the Iceman/Beast friendship was one of the cornerstones of the X-Universe, but has recently been ignored by writers. With the other two original X-Men, Cyclops and Phoenix, Bobby has a little brother-big brother/sister relationship, with the latter being much more pleasant. Bobby is fiercely protective of those he considers friends, going as far as to threaten to kill his teammate Northstar for leaving Angel in a potentially dangerous situation, keeping constant vigil by Cannonball's bedside while Sam recovered from his injury from Sinister, and engaging in physical confrontation with Cyclops for questioning Rogue's judgment as a team leader.
Despite Iceman's often well-liked personality, he has always seemed unable to maintain any sort of long-time relationships, with one exception being Opal, although that too ended. Iceman also had strong feelings for his fellow X-Man Polaris, but she did not return those feelings, due to her feelings towards Havok instead. Northstar formed an unrequited crush for Iceman during their time on the same team, though Iceman never did find out about Northstar's feelings. Iceman also began a brief relationship with the Xavier School's nurse, Annie, however she ended up leaving him for Havok, who had just left Polaris at the altar. When Iceman attempted to rekindle his relationship with Polaris, that too ended abruptly and Polaris returned to Havok. Iceman's latest attempt at romance was with the X-Men's enemy Mystique, who later betrayed him. Oddly enough, she still seemed fixated on him, stating that she will either kill him or cure him of his personal uncertainty. Iceman began a relationship with Kitty Pryde. However, Kitty breaks up with him after the Battle of the Atom. After reading the younger Iceman's mind, the younger Jean Grey claims that Iceman is actually gay, which Bobby admits. They speculate that his future self either 'repressed' or 'got over' that part of his identity in the years between him and the future Iceman, given his numerous relationships with women, although he still speculates that he may just be bisexual despite Jean's statement that, having seen his thoughts, he is fully gay.
Physical appearance.
Iceman's appearance in ice form has changed significantly over the years. In early X-Men stories, his appearance was reminiscent of a snowman, apparently because frost formed on his skin when he used his abilities. Later, Iceman learned to cover his body with hard-but-flexible ice and adopted the crystalline appearance familiar to modern readers. Later still, he developed the ability to actually become organic ice, appearing almost translucent. He is virtually indestructible in this form, as he can reform his shattered body even if part of him is completely destroyed. As a result of an adventure in which his mind was "hacked" by Emma Frost, Iceman assumed a more grotesque transparent appearance, with organic bubbles, large veins, and spiky hair. Since this experience, Iceman has regained control over his powers and refined his appearance with jagged spikes on his limbs, back, and hair.
Other versions.
1602.
Iceman is Roberto Trefusis in the miniseries "Marvel 1602", a member of the group of "witchbreeds" founded by Carlos Javier and led by Scotius Summerisle. He is the nephew of naval commander Sir Francis Drake. As in the Marvel Universe, he generates ice and can assume a physical ice form.
Age of Apocalypse.
In the "Age of Apocalypse" crossover event, Bobby, along with the rest of the X-Men, is trained by Magneto. Because Magneto is harder on his students than Professor X, Bobby lacks his 616-counterpart's sense of humor. Instead, Bobby becomes very cold and inhuman, making his teammates feel uncomfortable. In addition to his normal abilities, Bobby is capable of breaking down his body and merging it with another body of water to travel great distances in a matter of seconds. He can bring others along through a process that he calls "moisture molecular inversion", though it is a painful process for the passengers. Bobby is also able to reconstitute his body from broken pieces. Just before Apocalypse's defeat, Colossus stormed right through Iceman, causing him to fall into pieces in an attempt to reach his sister. A couple of months later, Iceman, Exodus, Wild Child, and Morph were sent on a secret mission by Magneto; for a time, only Wild Child's fate was revealed.
Iceman returned in the "Uncanny X-Force" arc "The Dark Angel Saga," when the title team was forced to travel to the AoA reality. He abandoned the X-Men during a battle with the minions of Weapon X, who had by this time been transformed into that world's new Apocalypse, a defection later revealed to be the result of Iceman's having lost faith in his X-Men's ability to save their world. He was soon brought to Earth-616 by the Dark Beast, where he joined forces with Archangel on his quest to eradicate all life on Earth so he could create a new evolution process. Tired of seeing the people he cared about being killed fighting what he thought to be an unwinnable war, Drake agreed to help Archangel - despite the latter's now being the mainstream Marvel Universe's version of Drake's X-Men's arch-enemy, Apocalypse - in exchange for transport to the relative paradise that was Earth-616.
After X-Force defeated Archangel, Bobby managed to escape with McCoy and most of Archangel's other minions, but eventually broke away from them and went into hiding, living a life of hedonistic bliss in Madripoor. The "Age of Apocalypse" version of Nightcrawler - who, after being brought to Earth-616 along with the rest of his world's X-Men to help take down Archangel, decided to stay behind there in order to track down and kill the various villains who also came to the mainstream Marvel Universe - eventually tracked Bobby down to execute him for his treason. Nightcrawler is quickly overwhelmed by Drake's avatars, but eventually forces Iceman into a factory boiler room, where there's not enough moisture in the air for him to effectively use his powers. Nightcrawler quickly disables his former friend, and shoves him into an incinerator, killing the once noble hero.
Earth X.
During the series "Earth X", Bobby had become trapped in his ice form, making him vulnerable to melting. He moves to the Arctic regions of Earth, and made an ice city for himself and the Inuit. Due to a series of events where Earth's orbital path moves, Bobby is able to return to the United States to aid in the battle against the demon Mephisto.
House of M.
In the "House of M" reality, Iceman was seen in Magneto's army during his rise to power. Bobby later appears as one of the Horsemen of Apocalypse because Apocalypse rescued Bobby from a mutant internment camp that his parents had sent him to. Magneto sends Apocalypse to dispose of his rival Black Panther; when Apocalypse is attacked en route by Black Panther's allies, Iceman aids him by freezing Namor solid and attempting to attack Storm, but he is severely injured by Sunfire.
Marvel Zombies.
A zombified Iceman appears in "Marvel Zombies: Dead Days" alongside zombies Wolverine and Cyclops. Ultimately, he is seen attacking Magneto. But Iceman perished at the hands of the Master Of Magnetism himself when Iceman is cut apart.
Mutant X.
In the Mutant X universe, the Asgardian god Loki amplified Bobby's powers to a dangerous level, leaving him unable to touch any living thing without killing it. Despite this, he retains a jovial and optimistic personality. When Havok has a disagreement with Magneto and decides to leave the X-Men, Ice-Man is one of those who follow him, becoming a founding member of the Six, who eventually come to be considered the world's premiere mutant team.
New Exiles.
After the New Exiles land on the world of warring empires, they encounter Dame Emma Frost, head of Britain's Department X and founder of Force-X. This team includes Roberta "Bobby" Drake, a female version of Bobby who is codenamed Aurion and displays ice-based abilities.
Battle of the Atom.
In the alternate future witnessed in "Battle of the Atom", Iceman's future self is shown in a long brown robe with a long beard and ice staff, prompting the present Bobby to compare himself to a wizard. He has also developed the ability to create semi-independent ice structures that can act on their own, although one of these structures - demonstrating a Hulk-like physique and intellect - has gone on to join the future version of the Brotherhood, resulting in the Brotherhood using the duplicate to reinforce the illusion of themselves as the future X-Men. The duplicate Iceman was defeated by the O5 Iceman, the present Iceman, and the future Iceman during the battle in the past (Prompting the youngest Iceman to note that they only needed one more Iceman for a five-a-side basketball team).
Ronin.
In the alternate reality of "X-Men: Ronin" Iceman is a murderous ninja in the employ of the Hellfire Club. He works with Pyro and Avalanche as part of the 'Shadowcat Clan' and battles the X-Men.
Shadow-X.
New Excalibur battles an evil counterpart of Iceman, who is a member of the Shadow-X, the X-Men of an alternate reality in which Professor X was possessed by the Shadow King. They are brought to Earth-616 as a result of M-Day. He appeared to be mute and died during the final battle against Albion.
Spider-Verse.
In "Edge of Spider-verse: Web of Fear", a Spider-Man who is a member of the Captain Britain Corps witnesses Morlun about to kill Spider-Man. Later, a larger picture is shown of Firestar and Iceman lying dead, with Ms. Lion being left out, mourning her comrades. This universe is an alternate version of "Spider-Man and his Amazing Friends" designated as Earth-1983.
Marvel Noir.
Iceman appears in "X-Men Noir" as one of the X Men, a crew of talented criminals. He is depicted as being very short-tempered and paranoid. He is dubbed "Iceman", and angrily insists others refer to him that way, due to his custom of using an icepick as a weapon.
Ultimate Marvel.
In the Ultimate Marvel continuity, Bobby Drake is the youngest founding member of the X-Men. He ran away from his family at the peak of government-supported Sentinel attacks, fearing his family would be killed in such an attack.
Ultimate Iceman never had the snowman look of his counterpart, instead generating a flexible ice armor from the beginning. Bobby establishes himself as a valuable asset, singlehandedly taking out the Ultimates once with a gigantic ice wall ("see Ultimate War"), as well as singlehandedly halting an invasion by Colonel Wraith and Weapon X. He was only able to be stopped by Rogue, who was in temporary possession of Marvel Girl's telepathy. Professor X has stated that Bobby is one of the three most powerful X-Men. During the "World Tour" arc, after enlarging his armor to form a gigantic ice troll, Bobby is greatly injured by Proteus, which resulted in a lawsuit issued by his parents against Xavier. Bobby eventually rebels against his parents, and later returns to the X-Men.
While Bobby was away from the X-Men on a vacation, he had a girlfriend, but Professor Xavier erased all memories of her from Bobby's mind when he told her too much about the X-Men (he presumably also erased the girl's memories). Upon her acceptance into the X-Men, Bobby begins to date Rogue. The pair date for a considerable amount of time, but eventually break up due to Bobby's growing feelings for Shadowcat and Rogue's feelings for Gambit. Eventually Rogue leaves, and Bobby starts to date Kitty. After this, the two rekindle their relationship, but problems erupted.
In "Ultimate X-Men" #80, Bishop and Hammer, time-travellers from the future, comment on how powerful Bobby will one day become. Cyclops disbanded the X-Men in "Ultimate X-Men" #81 and Bishop and Storm created a new team. Iceman stayed at the Institute as a student only until Xavier returned and reformed his X-Men.
Professor X is later revealed to be alive and the X-Men return to the Xavier Institute, which is also when Iceman rejoins the X-Men line up. Jean Grey soon discovers that fellow X-Man, Colossus, is using a drug called Banshee to enhance his mutant abilities. The X-Men are highly against this, but Colossus manages to convince Rogue, Dazzler, Angel, Nightcrawler, and Cyclops to join him on his own X-Men team. Iceman remains on Professor X's drug-free X-Men and fights the Banshee enhanced X-Men. Xavier's X-Men win and the two teams combine again with nobody using drugs.
The Ultimatum Wave hits the X-Men next killing several of the X-Men (Beast, Dazzler, and Nightcrawler). Magneto and the Brotherhood attack the world and Iceman helps the world's heroes fight them off. Most of the X-Men die, but Iceman (alongside Rogue, Storm, Colossus, and Jean Grey) is able to survive Magneto's attack. He is last seen demolishing the X-Mansion alongside Rogue and Jean Grey and burying the deceased X-Men in its place. He finds it hard to destroy their home, but he feels it to be the right thing to do now that Professor Xavier is dead.
Later, Bobby Drake is kicked out of his home for being a mutant. With nowhere else to go, Kitty suggests to Peter Parkers' Aunt May that he move in with her, Peter, Gwen Stacy, and Johnny Storm (who also recently moved in their household). Aunt May agrees and enrolls Bobby at Midtown High under the guise of Bobby Parker, one of Peters' cousins and shaves his hair off to help keep his, Peter's and Johnny's secret identities safe.
Without knowing where to go after Peter's death, Bobby Drake asked Kitty Pride if Johhny could come with them searching for a place where to live and hide from authorities. They found the Morlock Tunnels where they live now and help mutants in danger. Firstly they rescued Rogue, who joined them, and later Jimmy Hudson (the son of Wolverine), came to them for help after escaping Stryker's imprisonment along with other mutants he freed.
X-Men Fairy Tales.
In "X-Men Fairy Tales" (issue #1), Iceman appears as a white wolf with icy breath named Kori (Japanese for ice). Before he is reached by , he appears to have lost faith in friendship.
X-Men: The End.
In "", Iceman appears as one of the instrumental characters in the defeat of Cassandra Nova and Khan and one of the few surviving X-Men.
In other media.
Film.
In the movies "X-Men", "X2", ', and ', he is played by Shawn Ashmore. Bobby is one of the first students to reach out to Rogue and begins a romantic relationship with her. In "X2", he has an uneasy friendship/rivalry with Pyro. His relationship with his family is also strained, and his brother actually turns Bobby in to the police out of jealousy. In "X-Men: The Last Stand", his relationship with Rogue appears to be deteriorating, strained by their inability to have physical contact and by his close friendship with Kitty Pryde. Seeing them both almost kiss prompts Rogue to seek out "the cure" so she can finally touch Bobby without fear of hurting him. He takes part in the X-Men's final confrontation with Magneto's army and shows his true power by fighting Pyro one-on-one. During this battle, Iceman's ability to transform his body into ice is finally shown, giving him the form of Iceman from the comics, being first implied in a previous confrontation between the two former best friends in which Bobby's fist ices over. After the battle, he finds Rogue in her room, having taken the cure. Bryan Singer posted on his Twitter account that Ashmore would return as Iceman in "". In film, he was shown with a greatly developed ability to transform completely into ice and to travel on ice slides. At the beginning of the film, Iceman attempts to stall the Sentinels so Kitty and Bishop could change the past, but is beheaded and crushed by the Sentinels in the process. Kitty and Bishop however change the past and Iceman survives. While stalling the Sentinels, so Wolverine could change the timeline, Iceman is melted in his ice form by the Sentinels. He is restored to life after the timeline is reset, along with him and Rogue being back together again.
Novels.
In the novelization for "X-Men: The Last Stand", Iceman saves Pyro from the destruction of Dark Phoenix.

</doc>
<doc id="15506" url="http://en.wikipedia.org/wiki?curid=15506" title="Isidore of Seville">
Isidore of Seville

Saint Isidore of Seville (Latin: "Isidorus Hispalensis"; c. 560 – 4 April 636) served as Archbishop of Seville for more than three decades and is considered, as the 19th-century historian Montalembert put it in an oft-quoted phrase, "The last scholar of the ancient world". 
At a time of disintegration of classical culture, and aristocratic violence and illiteracy, he was involved in the conversion of the royal Visigothic Arians to Catholicism, both assisting his brother Leander of Seville, and continuing after his brother's death. He was influential in the inner circle of Sisebut, Visigothic king of Hispania. Like Leander, he played a prominent role in the Councils of Toledo and Seville. The Visigothic legislation that resulted from these councils influenced the beginnings of representative government.
His fame after his death was based on his "Etymologiae", an etymological encyclopedia which assembled extracts of many books from classical antiquity that would have otherwise been lost.
Life.
Childhood and education.
Isidore was probably born in Cartagena, Spain to Severianus and Theodora. His father belonged to a Hispano-Roman family of high social rank while his mother was of Visigothic origin and apparently, was distantly related to Visigothic royalty. His parents were members of an influential family who were instrumental in the political-religious manoeuvring that converted the Visigothic kings from Arianism to Catholicism. The Catholic Church celebrates him and all his siblings as known saints:
Isidore received his elementary education in the Cathedral school of Seville. In this institution, the first of its kind in Iberia, a body of learned men including Archbishop Saint Leander of Seville taught the trivium and quadrivium, the classic liberal arts. Saint Isidore applied himself to study diligently enough that he quickly mastered at least a pedestrian level of Latin, a smattering of Greek, and some Hebrew.
Two centuries of Gothic control of Iberia incrementally suppressed the ancient institutions, classic learning, and manners of the Roman Empire. The associated culture entered a period of long-term decline. The ruling Visigoths nevertheless showed some respect for the outward trappings of Roman culture. Arianism meanwhile took deep root among the Visigoths as the form of Christianity that they received.
Scholars may debate whether Isidore ever personally embraced monastic life or affiliated with any religious order, but he undoubtedly esteemed the monks highly.
Bishop of Seville.
After the death of Saint Leander of Seville on 13 March 600 or 601, Isidore succeeded to the See of Seville. On his elevation to the episcopate, he immediately constituted himself as protector of monks.
Saint Isidore recognized that the spiritual and material welfare of the people of his See depended on assimilation of remnant Roman and ruling barbarian cultures, and consequently attempted to weld the peoples and subcultures of the Visigothic kingdom into a united nation. He used all available religious resources toward this end and succeeded. Isidore practically eradicated the heresy of Arianism and completely stifled the new heresy of Acephali at its very outset. Archbishop Isidore strengthened religious discipline throughout his See.
Archbishop Isidore also used resources of education to counteract increasingly influential Gothic barbarism throughout his episcopal jurisdiction. His quickening spirit animated the educational movement centered on Seville. Saint Isidore introduced Aristotle to his countrymen long before the Arabs studied Greek philosophy extensively.
In 619, Saint Isidore of Seville pronounced anathema against any ecclesiastic who in any way should molest the monasteries.
Second Synod of Seville (November 618 or 619).
Through the enlightened statecraft of his two brothers, the Councils of Seville and Toledo emanated Visigothic legislation which influenced the beginnings of representative government.
Saint Isidore presided over the Second Council of Seville, begun on 13 November 619, in the reign of King Sisebut. The bishops of Gaul and Narbonne and the Hispanic prelates all attended. The Acts of the Council fully set forth the nature of Christ, countering Arian conceptions.
Fourth National Council of Toledo.
All bishops of Hispania attended the Fourth National Council of Toledo, begun on 5 December 633. The aged Archbishop Saint Isidore presided over its deliberations and originated most enactments of the council.
Through Isidore's influence, this Council of Toledo promulgated a decree, commanding all bishops to establish seminaries in their cathedral cities along the lines of the cathedral school at Seville, which had educated Saint Isidore decades earlier. The decree prescribed the study of Greek, Hebrew, and the liberal arts and encouraged interest in law and medicine. The authority of the Council made this education policy obligatory upon all bishops of the Kingdom of the Visigoths. The council granted remarkable position and deference to the king of the Visigoths. The independent Church bound itself in allegiance to the acknowledged king; it said nothing of allegiance to the Bishop of Rome.
Death.
Saint Isidore of Seville died on 4 April 636 after serving more than 32 years as archbishop of Seville.
Work.
Isidore's Latin style in the "Etymologiae" and elsewhere, though simple and lucid, reveals increasing local Visigothic traditions. 
"Etymologiae".
Isidore was the first Christian writer to try to compile a "summa" of universal knowledge, in his most important work, the "Etymologiae" (taking its title from the method he uncritically used in the transcription of his era's knowledge). It is also known by classicists as the "Origines" (the standard abbreviation being "Orig"). This encyclopedia — the first such Christian epitome—formed a huge compilation of 448 chapters in 20 volumes. In it, as Isidore entered his own terse digest of Roman handbooks, miscellanies and compendia, he continued the trend towards abridgements and summaries that had characterised Roman learning in Late Antiquity. In the process, many fragments of classical learning are preserved which otherwise would have been hopelessly lost; "in fact, in the majority of his works, including the "Origines", he contributes little more than the mortar which connects excerpts from other authors, as if he was aware of his deficiencies and had more confidence in the "stilus maiorum" than his own" his translator Katherine Nell MacFarlane remarks; on the other hand, some of these fragments were lost in the first place because Isidore’s work was so highly regarded—Braulio called it "quecunque fere sciri debentur", "practically everything that it is necessary to know"— that it superseded the use of many individual works of the classics themselves, which were not recopied and have therefore been lost: "all secular knowledge that was of use to the Christian scholar had been winnowed out and contained in one handy volume; the scholar need search no further".
The fame of this work imparted a new impetus to encyclopedic writing, which bore abundant fruit in the subsequent centuries of the Middle Ages. It was the most popular compendium in medieval libraries. It was printed in at least ten editions between 1470 and 1530, showing Isidore's continued popularity in the Renaissance. Until the 12th century brought translations from Arabic sources, Isidore transmitted what western Europeans remembered of the works of Aristotle and other Greeks, although he understood only a limited amount of Greek. The "Etymologiae" was much copied, particularly into medieval bestiaries.
On the Catholic faith against the Jews.
Isidore's "De fide catholica contra Iudaeos" furthers Augustine of Hippo's ideas on the Jewish presence in Christian society. Like Augustine, Isidore accepted the necessity of the Jewish presence because of their expected role in the anticipated Second Coming of Christ. In "De fide catholica contra Iudaeos", Isidore exceeds the anti-rabbinic polemics of earlier theologians by criticizing Jewish practice as deliberately disingenuous.
He contributed two decisions to the Fourth Council of Toledo: Canon 60 calling for the forced removal of children from parents practicing Crypto-Judaism and their education by Christians and Canon 65 forbidding Jews and Christians of Jewish origin from holding public office.
Other works.
Isidore's other works, all in Latin, include:
Veneration.
Isidore was one of the last of the ancient Christian philosophers; he was the last of the great Latin Church Fathers and was contemporary with Maximus the Confessor. Some consider him to be the most learned man of his age, and he exercised a far-reaching and immeasurable influence on the educational life of the Middle Ages. His contemporary and friend, Braulio of Zaragoza, regarded him as a man raised up by God to save the Iberian peoples from the tidal wave of barbarism that threatened to inundate the ancient civilization of Hispania. The Eighth Council of Toledo (653) recorded its admiration of his character in these glowing terms: "The extraordinary doctor, the latest ornament of the Catholic Church, the most learned man of the latter ages, always to be named with reverence, Isidore". This tribute was endorsed by the Fifteenth Council of Toledo, held in 688.
Isidore was interred in Seville. His tomb represented an important place of veneration for the Mozarabs during the centuries after the Arab conquest of Visigothic Hispania. In the middle of the 11th century, with the division of Al Andalus into taifas and the strengthening of the Christian holdings in the Iberian peninsula, Fernando I of León found himself in a position to extract tribute from the fractured Arab states. In addition to money, Abbad II al-Mu'tadid, the Abbasid ruler of Seville (1042–1069), agreed to turn over St. Isidore's remains to Fernando I. A Catholic poet described al-Mutatid placing a brocaded cover over Isidore's sarcophagus, and remarked, "Now you are leaving here, revered Isidore. You know well how much your fame was mine!" Fernando had Isidore's remains reinterred in the then-recently constructed Basilica of San Isidoro in León.
Isidore was canonised a saint by the Roman Catholic Church in 1598 by Pope Clement VIII and declared a Doctor of the Church in 1722 by Pope Innocent XIII. Many of his bones are buried in the cathedral of Murcia, Spain.
Legacy.
In Dante's "Paradiso" (X.130), Isidore is mentioned among theologians and Doctors of the Church alongside the Scot Richard of St. Victor and the Englishman Bede the Venerable.
The University of Dayton has named their implementation of the Sakai Project in honour of Saint Isidore.
His likeness, along with that of Leander of Sevile and Ferdinand III of Castile, are depicted on the crest badge of Sevilla FC.
The is a chivalric order formed on January 1, 2000. An international organisation, the order aims to honour Saint Isidore as patron saint of the Internet, alongside promoting Christian chivalry online. Members, who may be men or women, receive a modern-day knighthood.

</doc>
<doc id="15507" url="http://en.wikipedia.org/wiki?curid=15507" title="Compounds of carbon">
Compounds of carbon

Compounds of carbon are defined as chemical substances containing carbon. More compounds of carbon exist than any other chemical element except for hydrogen. Organic carbon compounds are far more numerous than inorganic carbon compounds. In general bonds of carbon with other elements are covalent bonds. Carbon is tetravalent but carbon free radicals and carbenes occur as short-lived intermediates. Ions of carbon are carbocations and carbanions and are also short-lived. An important carbon property is catenation as the ability to form long carbon chains and rings.
Allotropes of carbon.
The known inorganic chemistry of the allotropes of carbon (diamond, graphite, and the fullerenes) blossomed with the discovery of buckminsterfullerene in 1985, as additional fullerenes and their various derivatives were discovered. One such class of derivatives is inclusion compounds, in which an ion is enclosed by the all-carbon shell of the fullerene. This inclusion is denoted by the "@" symbol in endohedral fullerenes. For example, an ion consisting of a lithium ion trapped within buckminsterfullerene would be denoted Li+@C60. As with any other ionic compound, this complex ion could in principle pair with a counterion to form a salt. Other elements are also incorporated in so-called graphite intercalation compounds.
Carbides.
Carbides are binary compounds of carbon with an element that is less electronegative than it. The most important are
Al4C3,
B4C,
CaC2,
Fe3C,
HfC,
SiC,
TaC,
TiC, and
WC.
Organic compounds.
It was once thought that organic compounds could only be created by living organisms. Over time, however, scientists learned how to synthesize organic compounds in the lab. The number of organic compounds is immense and the known number of defined compounds is close to 10 million. However, an indefinitely large number of such compounds are theoretically possible. 
By definition, an organic compound must contain at least one atom of carbon, but this criterion is not generally regarded as sufficient. Indeed, the distinction between organic and inorganic compounds is ultimately a matter of convention, and there are several compounds that have been classified either way, such as:
COCl2,
CSCl2,
CS(NH2)2,
CO(NH2)2.
With carbon bonded to metals the field of organic chemistry crosses over into organometallic chemistry.
Inorganic compounds.
There is a rich variety of carbon chemistry that does not fall within the realm of organic chemistry and is thus called inorganic carbon chemistry.
Carbon-oxygen compounds.
There are many oxides of carbon (oxocarbons), of which the most common are carbon dioxide (CO2) and carbon monoxide (CO). Other less known oxides include carbon suboxide (C3O2) and mellitic anhydride (C12O9). There are also numerous unstable or elusive oxides, such as dicarbon monoxide (C2O), oxalic anhydride (C2O4), and carbon trioxide (CO3). 
There are several oxocarbon anions, negative ions that consist solely of oxygen and carbon. The most common are the carbonate (CO32−) and oxalate (C2O42−). The corresponding acids are the highly unstable carbonic acid (H2CO3) and the quite stable oxalic acid (H2C2O4), respectively. These anions can be partially deprotonated to give the bicarbonate (HCO3−) and hydrogenoxalate (HC2O4−). Other more exotic carbon–oxygen anions exist, such as acetylenedicarboxylate (O2C–C≡C–CO22−), mellitate (C12O96−), squarate (C4O42−), and rhodizonate (C6O62−). The anhydrides of some of these acids are oxides of carbon; carbon dioxide, for instance, can be seen as the anhydride of carbonic acid. 
Some important carbonates are
Ag2CO3,
BaCO3,
CaCO3,
CdCO3,
Ce2(CO3)3,
CoCO3,
Cs2CO3,
CuCO3,
FeCO3,
K2CO3,
La2(CO3)3,
Li2CO3,
MgCO3,
MnCO3,
(NH4)2CO3,
Na2CO3,
NiCO3,
PbCO3,
SrCO3, and
ZnCO3.
The most important bicarbonates include 
NH4HCO3,
Ca(HCO3)3,
KHCO3, and
NaHCO3.
The most important oxalates include
Ag2C2O4,
BaC2O4,
CaC2O4, 
Ce2(C2O4)3,
K2C2O4, and
Na2C2O4.
Carbonyls are coordination complexes between transition metals and carbonyl ligands. Metal carbonyls are complexes that are formed with the neutral ligand CO. These complexes are covalent. Here is a list of some carbonyls:
Cr(CO)6,
Co2(CO)8,
Fe(CO)5,
Mn2(CO)10,
Mo(CO)6,
Ni(CO)4,
W(CO)6.
Carbon-sulfur compounds.
Important inorganic carbon-sulfur compounds are the carbon sulfides carbon disulfide (CS2) and carbonyl sulfide (OCS). Carbon monosulfide (CS) unlike carbon monoxide is very unstable. Important compound classes are thiocarbonates, thiocarbamates, dithiocarbamates and trithiocarbonates. 
Carbon-nitrogen compounds.
Small inorganic carbon - nitrogen compounds are cyanogen, hydrogen cyanide, cyanamide, isocyanic acid and cyanogen chloride.
Paracyanogen is the polymerization product of cyanogen. Cyanuric chloride is the trimer of cyanogen chloride and 2-cyanoguanidine is the dimer of cyanamide. 
Other types of inorganic compounds include the inorganic salts and complexes of the carbon-containing cyanide, cyanate, fulminate, thiocyanate and cyanamide ions. Examples of cyanides are copper cyanide (CuCN) and potassium cyanide (KCN), examples of cyanates are potassium cyanate (KNCO) and silver cyanate (AgNCO), examples of fulminates are silver fulminate (AgOCN) and mercury fulminate (HgOCN) and an example of a thiocyanate is potassium thiocyanate (KSCN).
Carbon halides.
The common carbon halides are carbon tetrafluoride (CF4), carbon tetrachloride (CCl4), carbon tetrabromide (CBr4), carbon tetraiodide (CI4), and a large number of other carbon-halogen compounds.
Carboranes.
A carborane is a cluster composed of boron and carbon atoms such as H2C2B10H10.
Its use in alloys.
There are hundreds of alloys that contain carbon. The most common of these alloys is steel, sometimes called "carbon steel" (see ). All kinds of steel contain some amount of carbon, by definition, and all ferrous alloys contain some carbon. 
Some other common alloys that are based on iron and carbon include anthracite iron, cast iron, pig iron, and wrought iron. 
In more technical uses, there are also spiegeleisen, an alloy of iron, manganese, and carbon; and stellite, an alloy of cobalt, chromium, tungsten, and carbon. 
Whether it was placed there deliberately or not, some traces of carbon is also found in these common metals and their alloys: aluminum, chromium, magnesium, molybdenum, niobium, thorium, titanium, tungsten, uranium, vanadium, zinc, and zirconium. For example, many of these metals are smelted with coke, a form of carbon; and aluminum and magnesium are made in electrolytic cells with carbon electrodes. Some distribution of carbon into all of these metals is inevitable.

</doc>
<doc id="15508" url="http://en.wikipedia.org/wiki?curid=15508" title="Industrial espionage">
Industrial espionage

Industrial espionage, economic espionage or corporate espionage is a form of espionage conducted for commercial purposes instead of purely national security. Economic espionage is conducted or orchestrated by governments and is international in scope, while industrial or corporate espionage is more often national and occurs between companies or corporations.
Competitive intelligence and economic or industrial espionage.
"Competitive intelligence" levels out two scenarios of description as the legal and ethical activity of systematically gathering, analyzing and managing information on industrial competitors becomes beneficial. It may include activities such as examining newspaper articles, corporate publications, websites, patent filings, specialised databases, information at trade shows and the like to determine information on a corporation. The compilation of these crucial elements is sometimes termed CIS or CRS, a Competitive Intelligence Solution or Competitive Response Solution. With its roots in market research, "competitive intelligence" has been described as the "application of principles and practices from military and national intelligence to the domain of global business"; it is the business equivalent of open-source intelligence.
The difference between competitive intelligence and economic or industrial espionage is not clear; one needs to understand the legal basics to recognize how to draw the line between the two. Others maintain it is sometimes quite difficult to tell the difference between legal and illegal methods, especially if considering the ethical side of information gathering, making the definition even more elusive.
Forms of economic and industrial espionage.
Economic or industrial espionage takes place in two main forms. In short, the purpose of espionage is to gather knowledge about (an) organization(s). It may include the acquisition of intellectual property, such as information on industrial manufacture, ideas, techniques and processes, recipes and formulas. Or it could include sequestration of proprietary or operational information, such as that on customer datasets, pricing, sales, marketing, research and development, policies, prospective bids, planning or marketing strategies or the changing compositions and locations of production. It may describe activities such as theft of trade secrets, bribery, blackmail and technological surveillance. As well as orchestrating espionage on commercial organizations, governments can also be targets — for example, to determine the terms of a tender for a government contract so that another tenderer can underbid.
Target industries.
Economic and industrial espionage is most commonly associated with technology-heavy industries, including computer software and hardware, biotechnology, aerospace, telecommunications, transportation and engine technology, automobiles, machine tools, energy, materials and coatings and so on. Silicon Valley is known to be one of the world's most targeted areas for espionage, though any industry with information of use to competitors may be a target.
Information theft and sabotage.
Information can make the difference between success and failure; if a trade secret is stolen, the competitive playing field is leveled or even tipped in favor of a competitor.
Although a lot of information-gathering is accomplished legally through competitive intelligence, at times corporations feel the best way to get information is to take it. Economic or industrial espionage is a threat to any business whose livelihood depends on information.
In recent years, economic or industrial espionage has taken on an expanded definition. For instance, attempts to sabotage a corporation may be considered industrial espionage; in this sense, the term takes on the wider connotations of its parent word. That espionage and sabotage (corporate or otherwise) have become more clearly associated with each other is also demonstrated by a number of profiling studies, some government, some corporate. The United States government currently has a polygraph examination entitled the "Test of Espionage and Sabotage" (TES), contributing to the increasingly popular, though not consensus, notion, by those studying espionage and sabotage countermeasures, of the interrelationship between the two. In practice, particularly by "trusted insiders," they are generally considered functionally identical for the purpose of informing countermeasures.
Agents and the process of collection.
Economic or industrial espionage commonly occurs in one of two ways. Firstly, a dissatisfied employee appropriates information to advance their own interests or to damage the company or, secondly, a competitor or foreign government seeks information to advance its own technological or financial interest. "Moles" or trusted insiders are generally considered the best sources for economic or industrial espionage. Historically known as a "patsy," an insider can be induced, willingly or under duress to provide information. A patsy may be initially asked to hand over inconsequential information and once compromised by committing a crime, bribed into handing over material which is more sensitive. Individuals may leave one company to take up employment with another and take sensitive information with them. Such apparent behavior has been the focus of numerous industrial espionage cases that have resulted in legal battles. Some countries hire individuals to do spying rather than make use of their own intelligence agencies. Academics, business delegates and students are often thought to be utilized by governments in gathering information. Some countries, such as Japan, have been reported to expect students be debriefed on returning home. A spy may follow a guided tour of a factory then get "lost". A spy could be an engineer, a maintenance man, a cleaner, an insurance salesman or an inspector - basically anyone who has legitimate access to the premises.
A spy may break into the premises to steal data. They may search through waste paper and refuse, known as "dumpster diving". Information may be compromised via unsolicited requests for information, marketing surveys or use of technical support, research or software facilities. Outsourced industrial producers may ask for information outside of the agreed-upon contract.
Computers have facilitated the process of collecting information, due to the ease of access to large amounts of information, through physical contact or via the internet.
Use of computers and the Internet.
Personal computers.
Computers have become key in exercising industrial espionage due to the enormous amount of information they contain and its ease of being copied and transmitted. The use of computers for espionage increased rapidly in the 1990s. Information has been commonly stolen by being copied from unattended computers in offices, those gaining unsupervised access doing so through subsidiary jobs, such as cleaners or repairmen. Laptops were, and still are, a prime target, with those traveling abroad on business being warned not to leave them for any period of time. Perpetrators of espionage have been known to find many ways of conning unsuspecting individuals into parting, often only temporarily, from their possessions, enabling others to access and steal information. A "bag-op" refers to the use of hotel staff to access data, such as through laptops, in hotel rooms. Information may be stolen in transit, in taxis, at airport baggage counters, baggage carousels, on trains and so on.
The Internet.
The rise of the internet and computer networks has expanded the range and detail of information available and the ease of access for the purpose of industrial espionage. Worldwide, around 50,000 companies a day are thought to come under cyberattack with the rate estimated as doubling each year. This type of operation is generally identified as state backed or sponsored, because the "access to personal, financial or analytic resources" identified exceed that which could be accessed by cybercriminals or individual hackers. Sensitive military or defense engineering or other industrial information may not have immediate monetary value to criminals, compared with, say, bank details. Analysis of cyberattacks suggests deep knowledge of networks, with targeted attacks, obtained by numerous individuals operating in a sustained organized way.
Opportunities for sabotage.
The rising use of the internet has also extended opportunities for industrial espionage with the aim of sabotage. In the early 2000s, it was noticed that energy companies were increasingly coming under attack from hackers. Energy power systems, doing jobs like monitoring power grids or water flow, once isolated from the other computer networks, were now being connected to the internet, leaving them more vulnerable, having historically few built-in security features. The use of these methods of industrial espionage have increasingly become a concern for governments, due to potential attacks by terrorist groups or hostile foreign governments.
Malware.
One of the means of perpetrators conducting industrial espionage is by exploiting vulnerabilities in computer software. Malware and spyware as "a tool for industrial espionage", in "transmitting digital copies of trade secrets, customer plans, future plans and contacts". Newer forms of malware include devices which surreptitiously switch on mobile phones camera and recording devices. In attempts to tackle such attacks on their intellectual property, companies are increasingly keeping important information off network, leaving an "air gap", with some companies building "Faraday cages" to shield from electromagnetic or cellphone transmissions.
Distributed denial of service (DDoS) attack.
The distributed denial of service (DDoS) attack uses compromised computer systems to orchestrate a flood of requests on the target system, causing it to shut down and deny service to other users. It could potentially be used for economic or industrial espionage with the purpose of sabotage. This method was allegedly utilized by Russian secret services, over a period of two weeks on a cyberattack on Estonia in May 2007, in response to the removal of a Soviet era war memorial.
History.
Origins of industrial espionage.
Economic and industrial espionage has a long history. The work of Father Francois Xavier d'Entrecolles in Jingdezhen, China to reveal to Europe the manufacturing methods of Chinese porcelain in 1712 is sometimes considered an early case of industrial espionage.
Historical accounts have been written of industrial espionage between Britain and France. Attributed to Britain's emergence as an "industrial creditor," the second decade of the 18th century saw the emergence of a large-scale state-sponsored effort to surreptitiously take British industrial technology to France. Witnesses confirmed both the inveigling of tradespersons abroad and the placing of apprentices in England. Protests by those such as iron workers in Sheffield and steel workers in Newcastle, about skilled industrial workers being enticed abroad, led to the first English legislation aimed at preventing this method of economic and industrial espionage.
The 20th Century.
East-West commercial development opportunities after World War I saw a rise in Soviet interest in American and European manufacturing know-how, exploited by Amtorg Corporation. Later, with Western restrictions on the export of items thought likely to increase military capabilities to the USSR, Soviet industrial espionage was a well known adjunct to other spying activities up until the 1980s. "BYTE" reported in April 1984, for example, that although the Soviets sought to develop their own microelectronics, their technology appeared to be several years behind the West's. Soviet CPUs required multiple chips and appeared to be close or exact copies of American products such as the Intel 3000 and DEC LSI-11/2.
"Operation Brunnhilde".
Some of these activities were directed via the East German Stasi (Ministry for State Security). One such operation, known as "Operation Brunnhilde" operated from the mid-1950s until early 1966 and made use of spies from many Communist Bloc countries. Through at least 20 forays, many western European industrial secrets were compromised. One member of the "Brunnhilde" ring was a Swiss chemical engineer, Dr. Jean Paul Soupert (also known as "Air Bubble"), living in Brussels. He was described by Peter Wright in Spycatcher as having been "doubled" by the Belgian Sûreté de l'État. He revealed information about industrial espionage conducted by the ring, including the fact that Russian agents had obtained details of Concorde's advanced electronics system. He testified against two Kodak employees, living and working in Britain, during a trial in which they were accused of passing information on industrial processes to him, though they were eventually acquitted.
Soviet "spetsinformatsiya" system.
A secret report from the Military-Industrial Commission of the USSR (VPK), from 1979–80, detailed how "spetsinformatsiya" (Russian: специнформация i.e. "special records") could be utilised in twelve different military industrial areas. Writing in the Bulletin of the Atomic Scientists, Philip Hanson detailed a "spetsinformatsiya" system in which 12 industrial branch ministries formulated requests for information to aid technological development in their military programs. Acquisition plans were described as operating on 2 year and 5 year cycles with about 3000 tasks under way each year. Efforts were aimed at civilian as well as military industrial targets, such as in the petrochemical industries. Some information was garnered so as to compare levels of competitor to Soviet technological advancement. Much unclassified information was also gathered, blurring the boundary with "competitive intelligence".
The Soviet military was recognised as making much better use of acquired information, compared to civilian industry, where their record in replicating and developing industrial technology was poor.
The legacy of Cold War espionage.
Following the demise of the Soviet Union and the end of the Cold War, commentators, including the US Congressional Intelligence Committee, noted a redirection amongst the espionage community from military to industrial targets, with Western and former communist countries making use of "underemployed" spies and expanding programs directed at stealing such information.
The legacy of Cold War spying included not just the redirection of personnel but the use of spying apparatus such as computer databases, scanners for eavesdropping, spy satellites, bugs and wires.
Notable cases.
France and the United States.
Between 1987 and 1989, IBM and Texas Instruments were thought to have been targeted by French spies with the intention of helping France's Groupe Bull. In 1993, US aerospace companies were also thought to have been targeted by French interests. During the early 1990s, France was described as one of the most aggressive pursuers of espionage to garner foreign industrial and technological secrets. France accused the U.S. of attempting to sabotage its high tech industrial base. The government of France has been alleged to have conducted ongoing industrial espionage against American aerodynamics and satellite companies.
Volkswagen.
In 1993, car manufacturer Opel, the German division of General Motors, accused Volkswagen of industrial espionage after Opel's chief of production, Jose Ignacio Lopez, and seven other executives moved to Volkswagen. Volkswagen subsequently threatened to sue for defamation, resulting in a four-year legal battle. The case, which was finally settled in 1997, resulted in one of the largest settlements in the history of industrial espionage, with Volkswagen agreeing to pay General Motors $100 million and to buy at least $1 billion of car parts from the company over 7 years, although it did not explicitly apologize for Lopez's behavior.
Hilton and Starwood.
In April 2009 the US based hospitality company Starwood accused its rival Hilton of a "massive" case of industrial espionage. After being purchased by private equity group Blackstone, Hilton employed 10 managers and executives from Starwood. Under intense pressure to improve profits, Starwood accused Hilton of stealing corporate information relating to its luxury brand concepts, used in setting up its own Denizen hotels. Specifically, former head of its luxury brands group, Ron Klein, was accused of downloading "truckloads of documents" from a laptop to his personal email account.
GhostNet.
GhostNet was a "vast surveillance system" reported by Canadian researchers based at the University of Toronto in March 2009. Using targeted emails it compromised thousands of computers in governmental organisations, enabling attackers to scan for information and transfer this back to a "digital storage facility in China".
Google and Operation Aurora.
On 13 January 2010, Google Inc. announced that operators, from within China, had hacked into their Google China operation, stealing intellectual property and, in particular, accessing the email accounts of human rights activists. The attack was thought to have been part of a more widespread cyber attack on companies within China which has become known as Operation Aurora. Intruders were thought to have launched a zero-day attack, exploiting a weakness in the Microsoft Internet Explorer browser, the malware used being a modification of the trojan "Hydraq". Concerned about the possibility of hackers taking advantage of this previously unknown weakness in Internet Explorer, the governments of Germany and, subsequently France, issued warnings not to use the browser.
There was speculation that "insiders" had been involved in the attack, with some Google China employees being denied access to the company's internal networks after the company's announcement. In February 2010, computer experts from the U.S. National Security Agency claimed that the attacks on Google probably originated from two Chinese universities associated with expertise in computer science, Shanghai Jiao Tong University and the Shandong Lanxiang Vocational School, the latter having close links to the Chinese military.
Google claimed at least 20 other companies had also been targeted in the cyber attack, said by the "London Times", to have been part of an "ambitious and sophisticated attempt to steal secrets from unwitting corporate victims" including "defence contractors, finance and technology companies". Rather than being the work of individuals or organised criminals, the level of sophistication of the attack was thought to have been "more typical of a nation state". Some commentators speculated as to whether the attack was part of what is thought to be a concerted Chinese industrial espionage operation aimed at getting "high-tech information to jump-start China's economy". Critics pointed to what was alleged to be a lax attitude to the intellectual property of foreign businesses in China, letting them operate but then seeking to copy or reverse engineer their technology for the benefit of Chinese "national champions". In Google's case, they may have (also) been concerned about the possible misappropriation of source code or other technology for the benefit of Chinese rival Baidu. In March 2010 Google subsequently decided to cease offering censored results in China, leading to the closing of its Chinese operation.
CyberSitter and Green Dam.
The US based firm CyberSitter announced in January 2010 that it was suing the Chinese government, and other US companies, for stealing its anti pornography software, with the accusation that it had been incorporated into China's Green Dam program, used by the state to censor children's internet access. CyberSitter accused Green Dam creators as having copied around 3000 lines of code. They were described as having done 'a sloppy job of copying,' with some lines of the copied code continuing to direct people to the CyberSitter website. The attorney acting for CyberSitter maintained "I don't think I have ever seen such clear-cut stealing".
"USA v. Lan Lee, et al.".
The United States charged two former NetLogic Inc. engineers, Lan Lee and Yuefei Ge, of committing economic espionage against TSMC and NetLogic, Inc. A jury acquitted the defendants of the charges with regard to TSMC and deadlocked on the charges with regard to NetLogic. In May 2010, a federal judge dismissed all the espionage charges against the two defendants. The judge ruled that the U.S. Government presented no evidence of espionage.
Dongxiao Yue and Chordiant Software, Inc..
In May 2010, the federal jury convicted Chordiant Software, Inc., a U.S. corporation, of stealing Dongxiao Yue's JRPC technologies and used them in a product called Chordiant Marketing Director. Yue previously filed lawsuits against Symantec Corporation for a similar theft.
Concerns of national governments.
Brazil.
Revelations from the Snowden documents have provided information to the effect that the United States, notably vis-à-vis the NSA, has been conducting aggressive economic espionage against Brazil. Canadian intelligence has apparently supported U.S. economic espionage efforts.
United States.
According to Edward Snowden, The National Security Agency spies on foreign companies. A recent report to the US government, by aerospace and defense company Northrop Grumman, describes Chinese economic espionage as comprising "the single greatest threat to U.S. technology". Joe Stewart, of SecureWorks, blogging on the 2009 cyber attack on Google, referred to a "persistent campaign of 'espionage-by-malware' emanating from the People’s Republic of China (PRC)" with both corporate and state secrets being "Shanghaied" over the past 5 or 6 years. The Northrop Grumann report states that the collection of US defense engineering data through cyberattack is regarded as having "saved the recipient of the information years of R&D and significant amounts of funding". Concerns about the extent of cyberattacks on the US emanating from China has led to the situation being described as the dawn of a "new cold cyberwar". In response to these and other reports, Amitai Etzioni of the Institute for Communitarian Policy Studies has suggested that China and the United States should agree to a policy of mutually assured restraint with respect to cyberspace. This would involve allowing both states to take the measures they deem necessary for their self-defense while simultaneously agreeing to refrain from taking offensive steps; it would also entail vetting these commitments.
United Kingdom.
In December 2007, it was revealed that Jonathan Evans, head of the United Kingdom's MI5, had sent out confidential letters to 300 chief executives and security chiefs at the country's banks, accountants and legal firms warning of attacks from Chinese 'state organisations'. A summary was also posted on the secure website of the Centre for the Protection of the National Infrastructure, accessed by some of the nation's 'critical infrastructure' companies, including 'telecoms firms, banks and water and electricity companies'. One security expert warned about the use of 'custom trojans,' software specifically designed to hack into a particular firm and feed back data. Whilst China was identified as the country most active in the use of internet spying, up to 120 other countries were said to be using similar techniques. The Chinese government responded to UK accusations of economic espionage by saying that the report of such activities was 'slanderous' and that the government opposed hacking which is prohibited by law.
Germany.
German counter-intelligence experts have maintained the German economy is losing around €53 billion or the equivalent of 30,000 jobs to economic espionage yearly.

</doc>
<doc id="15511" url="http://en.wikipedia.org/wiki?curid=15511" title="Isaac Bashevis Singer">
Isaac Bashevis Singer

Isaac Bashevis Singer (Yiddish: יצחק באַשעװיס זינגער; November 21, 1902 – July 24, 1991) was a Polish-born Jewish-American author. The Polish form of his birth name was "Izaak Zynger" and he used his mother's first name in an initial pseudonym, "Izaak Baszewis", which he later expanded to the form under which he is now known. He was a leading figure in the Yiddish literary movement, writing and publishing only in Yiddish, and was awarded the Nobel Prize in Literature in 1978. He also was awarded two U.S. National Book Awards, one in Children's Literature for his memoir "A Day Of Pleasure: Stories of a Boy Growing Up in Warsaw" (1970) and one in Fiction for his collection, "A Crown of Feathers and Other Stories" (1974).
Personal life.
Early life.
Isaac Bashevis Singer was born in 1902 in Leoncin village near Warsaw, Poland, then part of the Russian Empire. A few years later, the family moved to a nearby Polish town of Radzymin, which is often and erroneously given as his birthplace. The exact date of his birth is uncertain, but most probably it was November 21, 1902, a date that Singer gave both to his official biographer Paul Kresh, and his secretary Dvorah Telushkin. It is also consistent with the historical events he and his brother refer to in their childhood memoirs. The often-quoted birth date, July 14, 1904 was made up by the author in his youth, most probably to make himself younger to avoid the draft.
His father was a Hasidic rabbi and his mother, Bathsheba, was the daughter of the rabbi of Biłgoraj. Singer later used her name in his pen name "Bashevis" (Bathsheba's). Both his older siblings, sister Esther Kreitman (1891–1954) and brother Israel Joshua Singer (1893–1944), became writers as well. Esther was the first of the family to write stories.
The family moved to the court of the Rabbi of Radzymin in 1907, where his father became head of the Yeshiva. After the Yeshiva building burned down in 1908, the family moved to a flat at ul. Krochmalna 10. In the spring of 1914, the Singers moved to No. 12.
The street where Singer grew up was located in the impoverished, Yiddish-speaking Jewish quarter of Warsaw. There his father served as a rabbi, and was called on to be a judge, arbitrator, religious authority and spiritual leader in the Jewish community. The unique atmosphere of pre-war Krochmalna Street can be found both in the collection of "Varshavsky-stories", which tell stories from Singer's childhood, as well as in those novels and stories which take place in pre-war Warsaw.
World War I.
In 1917, because of the hardships of World War I, the family split up. Singer moved with his mother and younger brother Moshe to his mother's hometown of Biłgoraj, a traditional "shtetl," where his mother's brothers had followed his grandfather as rabbis. When his father became a village rabbi again in 1921, Singer returned to Warsaw. He entered the Tachkemoni Rabbinical Seminary and soon decided that neither the school nor the profession suited him. He returned to Biłgoraj, where he tried to support himself by giving Hebrew lessons, but soon gave up and joined his parents, considering himself a failure. In 1923 his older brother Israel Joshua arranged for him to move to Warsaw to work as a proofreader for the "Literarische Bleter," of which the brother was an editor.
United States.
In 1935, four years before the German invasion and the start of the Holocaust, Singer emigrated from Poland to the United States. He was fearful of the growing Nazi threat in neighboring Germany. The move separated the author from his common-law first wife Runia Pontsch and son Israel Zamir (b. 1929); they emigrated to Moscow and then Palestine. (The three met again in 1955).
Singer settled in New York City, where he took up work as a journalist and columnist for "The Forward" (פֿאָרװערטס), a Yiddish-language newspaper. After a promising start, he became despondent and felt for some years "Lost in America" (title of his 1974 novel published in Yiddish; he published it in English in 1981).
In 1938, he met Alma Wassermann (born Haimann) {b. 1907 – d. 1996}, a German-Jewish refugee from Munich. They married in 1940, and their union seemed to release energy in him; he returned to prolific writing and to contributing to the "Forward." In addition to his pen name of "Bashevis," he published under the pen names of "Varshavsky" and "D. Segal." They lived for many years in the Belnord apartment building on Manhattan's Upper West Side.
In 1981, Singer delivered a commencement address at the University at Albany, and was presented with an honorary doctorate.
Singer died on July 24, 1991 in Surfside, Florida, after suffering a series of strokes. He was buried in Cedar Park Cemetery, Emerson, New Jersey. A street in Surfside, Florida is named Isaac Singer Boulevard in his honor. The full academic scholarship for undergraduate students at the University of Miami is named in his honor.
Literary career.
Singer's first published story won the literary competition of the "literarishe bletter" and garnered him a reputation as a promising talent. A reflection of his formative years in "the kitchen of literature" can be found in many of his later works. IB Singer published his first novel, "Satan in Goray," in installments in the literary magazine "Globus", which he had co-founded with his life-long friend, the Yiddish poet Aaron Zeitlin in 1935. The book recounts events of 1648 in the village of Goraj (close to Biłgoraj), where the Jews of Poland lost a third of their population in a wholesale attack by Cossacks. It explores the effects of the seventeenth-century faraway false messiah, Shabbatai Zvi, on the local population. Its last chapter imitates the style of a medieval Yiddish chronicle. With a stark depiction of innocence crushed by circumstance, the novel appears to foreshadow coming danger. In his later work, "The Slave" (1962), Singer returns to the aftermath of 1648, in a love story between a Jewish man and a Gentile woman. He portrays the traumatized and desperate survivors of the historic catastrophe with even deeper understanding.
"The Family Moskat".
Singer became a literary contributor to the "Forward" only after his older brother Israel died in 1945. That year, Singer published "The Family Moskat" in his brother's honor. His own style showed in the daring turns of his action and characters, with (and this in the Jewish family-newspaper in 1945!) double adultery during the holiest of nights of Judaism, the evening of Yom Kippur. He was almost forced to stop writing the novel by his legendary editor-in-chief, Abraham Cahan, but was saved by readers who wanted the story to go on. After this, his stories—which he had published in Yiddish literary newspapers before—were printed in the "Forward" as well. Throughout the 1940s, Singer's reputation grew.
Singer believed in the power of his native language and thought that there was still a large audience, including in New York, who longed to read in Yiddish. In an interview in "Encounter" (February 1979), he claimed that although the Jews of Poland had died, "something—call it spirit or whatever—is still somewhere in the universe. This is a mystical kind of feeling, but I feel there is truth in it."
Some of his colleagues and readers were shocked by his all-encompassing view of human nature. He wrote about female homosexuality ("Zeitl and Rickel", "Tseytl un Rikl"), published in "The Seance and Other Stories"), transvestism ("Yentl the Yeshiva Boy" in "Short Friday"), and of rabbis corrupted by demons ("Zeidlus the Pope" in "Short Friday"). In those novels and stories which refer to events in his own life, he portrays himself unflatteringly (with some degree of accuracy) as an artist who is self-centered yet has a keen eye for the sufferings and tribulations of others.
Literary influences.
Singer had many literary influences; besides the religious texts he studied, he grew up with a rich array of Jewish folktales and worldly Yiddish detective-stories about "Max Spitzkopf" and his assistant "Fuchs.; He read Russian, including Dostoyevsky's "Crime and Punishment" at the age of fourteen;. He wrote in memoirs about the importance of the Yiddish translations donated in book-crates from America, which he studied as a teenager in Bilgoraj: "I read everything: Stories, novels, plays, essays... I read Rajsen, Strindberg, Don Kaplanowitsch, Turgenev, Tolstoy, Maupassant and Chekhov." He studied many philosophers, among them Spinoza, Arthur Schopenhauer, and Otto Weininger. Among his Yiddish contemporaries, Singer considered his older brother to be his greatest artistic example; he was also life-long friend and admirer of the author and poet Aaron Zeitlin.
Of his non-Yiddish-contemporaries, he was strongly influenced by the writings of Knut Hamsun, many of whose works he later translated, while he had a more critical attitude towards Thomas Mann, whose approach to writing he considered opposed to his own. Contrary to Hamsun's approach, Singer shaped his world not only with the egos of his characters, but also using the moral commitments of the Jewish tradition known from his youth and embodied by his father in the stories about Singer's youth. There was a dichotomy between the life his heroes lead and the life they feel they should lead — which gives his art a modernity his predecessors did not express. His themes of witchcraft, mystery and legend draw on traditional sources, but they are contrasted with a modern and ironic consciousness. They are also concerned with the bizarre and the grotesque.
Another important strand of his art is intra-familial strife, which he experienced firsthand when taking refuge with his mother and younger brother at his uncle's home in Biłgoraj. This is the central theme in Singer's big family chronicles, such as "The Family Moskat" (1950), "The Manor" (1967), and "The Estate" (1969). Some critics believe these show the influence of Thomas Mann's novel "Buddenbrooks"; Singer had translated Mann's "Der Zauberberg" ("The Magic Mountain") into Yiddish as a young writer.
Language.
Singer always wrote and published in Yiddish. His novels were serialized in newspapers, which also published his short stories. He edited his novels and stories for their publication in English in the United States; these versions were used as the basis for translation into other languages. He referred to his English version as his "second original". This has led to an ongoing controversy whether the "real Singer" can be found in the Yiddish original, with its finely tuned language and sometimes rambling construction, or in the more tightly edited American versions, where the language is usually simpler and more direct. Many of Singer's stories and novels have not yet been translated.
In the short story form, in which many critics feel he made his most lasting contributions, his greatest influences were writers Anton Chekhov and Guy de Maupassant, Russian and French, respectively. From Maupassant, Singer developed a finely grained sense of drama. Like those of the French master, Singer's stories can pack enormous visceral excitement in the space of a few pages. From Chekhov, Singer developed his ability to draw characters of enormous complexity and dignity in the briefest of spaces. In the foreword to his personally selected volume of his finest short stories he describes the two aforementioned writers as the greatest masters of the short story form.
Illustrators.
Several respected artists have illustrated Singer’s novels, short stories, and children’s books, including Raphael Soyer, Maurice Sendak, Larry Rivers, and Irene Lieblich. Singer personally selected Lieblich to illustrate some of his books for children, including "A Tale of Three Wishes" and "The Power of Light: Eight Stories for Hanukkah," after seeing her work in an exhibition at an Artists Equity exhibit in New York. A Holocaust survivor, Lieblich was from Zamosc, Poland, a town adjacent to the area where Singer grew up. As their memories of shtetl life were so similar, Singer found Lieblich’s images ideally suited to illustrate his texts. Of her style, Singer wrote that “her works are rooted in Jewish folklore and are faithful to Jewish life and the Jewish spirit.”
Summary.
Singer published at least 18 novels, 14 children's books, a number of memoirs, essays and articles. He is best known as a writer of short stories, which have been published in more than a dozen collections. The first collection of Singer's short stories in English, "Gimpel the Fool", was published in 1957. The title story was translated by Saul Bellow and published in May 1953 in the "Partisan Review". Selections from Singer's "Varshavsky-stories" in the "Daily Forward" were later published in anthologies such as "My Father's Court" (1966). Later collections include "A Crown of Feathers" (1973), with notable masterpieces in between, such as "The Spinoza of Market Street" (1961) and "A Friend of Kafka" (1970). His stories and novels reflect the world of the East European Jewry in which he grew up. After his many years in America, his stories also portrayed the world of the immigrants and their pursuit of an elusive American dream, which seems always beyond reach.
Prior to Singer's winning the Nobel Prize, English translations of dozens of his stories were frequently published in popular magazines such as "Playboy" and "Esquire." They were publishing literary works and included his stories among their best; in turn, he found them to be appropriate outlets for his work.
Throughout the 1960s, Singer continued to write on questions of personal morality. Because of the controversial aspects of his plots, he was a target of scathing criticism from many quarters, some of it for not being "moral" enough, some for writing stories that no one wanted to hear. To his critics he replied, "Literature must spring from the past, from the love of the uniform force that wrote it, and not from the uncertainty of the future." 
Singer was awarded the Nobel Prize in 1978.
Film adaptations.
His novel "Enemies, a Love Story" was adapted as a film by the same name (1989) and was quite popular, bringing new readers to his work. He featured a Holocaust survivor who deals with varying desires, complex family relationships, and a loss of faith.
Singer's story, "Yentl," was adapted into a film by that name (1983) starring singer Barbra Streisand.
Perhaps the most fascinating Singer-inspired film is 1974's "Mr. Singer's Nightmare or Mrs. Pupkos Beard," directed by Bruce Davidson, a renowned photographer who became Singer's neighbor. This unique film is a half-hour mixture of documentary and fantasy for which Singer wrote the script and played the leading role.
The 2007 film "Love Comes Lately", starring Otto Tausig, was adapted from several of Singer's stories.
Beliefs.
Judaism.
Singer's relationship to Judaism was complex and unconventional. He identified as a skeptic and a loner, though he felt a connection to his Orthodox roots. Ultimately, he developed a view of religion and philosophy, which he called "private mysticism: Since God was completely unknown and eternally silent, He could be endowed with whatever traits one elected to hang upon Him."
Singer was raised Orthodox and learned all the Jewish prayers, studied Hebrew, and learned Torah and Talmud. As he recounted in the autobiographical, "In My Father's Court", he broke away from his parents in his early twenties. Influenced by his older brother, who had done the same, he began spending time with non-religious Bohemian artists in Warsaw. Although Singer believed in a God, as in traditional Judaism, he stopped attending Jewish religious services of any kind, even on the High Holy Days. He struggled throughout his life with the feeling that a kind and compassionate God would never support the great suffering he saw around him, especially the Holocaust deaths of so many of the Polish Jews from his childhood. In one interview with the photographer Richard Kaplan, he said, "I am angry at God because of what happened to my brothers": Singer's older brother died suddenly in February 1944, in New York, of a thrombosis; his younger brother perished in Soviet Russia around 1945, after being deported with his mother and wife to Southern Kazakhstan in Stalin's purges.
Despite the complexities of his religious outlook, Singer lived in the midst of the Jewish community throughout his life. He did not seem to be comfortable unless he was surrounded by Jews; particularly Jews born in Europe. Although he spoke English, Hebrew, and Polish fluently, he always considered Yiddish his natural tongue. He always wrote in Yiddish and he was the last notable American author to be writing in this language. After he had achieved success as a writer in New York, Singer and his wife began spending time during the winters in Miami with its Jewish community, many of them New Yorkers.
Eventually, as senior citizens, they moved to Miami. They identified closely with the European Jewish community. After his death, Singer was buried in a traditional Jewish ceremony in a Jewish cemetery.
Especially in his short fiction, Singer often wrote about various Jews having religious struggles; sometimes these struggles became violent, bringing death or mental illness. In one story his narrator meets a young woman in New York whom he knew from an Orthodox family in Poland. She has become a kind of hippie, sings American folk music with a guitar, and rejects Judaism, although the narrator comments that in many ways she seems typically Jewish. The narrator says that he often meets Jews who think they are anything but Jewish, and yet still are.
In the end, Singer remains an unquestionably Jewish writer, yet his precise views about Jews, Judaism, and the Jewish God are open to interpretation. Whatever they were, they lay at the center of his literary art.
Vegetarianism.
Singer was a prominent Jewish vegetarian for the last 35 years of his life and often included vegetarian themes in his works. In his short story, "The Slaughterer", he described the anguish of an appointed slaughterer trying to reconcile his compassion for animals with his job of killing them. He felt that the ingestion of meat was a denial of all ideals and all religions: "How can we speak of right and justice if we take an innocent creature and shed its blood?" When asked if he had become a vegetarian for health reasons, he replied: "I did it for the health of the chickens."
In "The Letter Writer", he wrote "In relation to [animals], all people are Nazis; for the animals, it is an eternal Treblinka." which became a classical reference in the discussions about the legitimacy of the comparison of animal exploitation with the holocaust.
In the preface to Steven Rosen's "Food for Spirit: Vegetarianism and the World Religions" (1986), Singer wrote, "When a human kills an animal for food, he is neglecting his own hunger for justice. Man prays for mercy, but is unwilling to extend it to others. Why should man then expect mercy from God? It's unfair to expect something that you are not willing to give. It is inconsistent. I can never accept inconsistency or injustice. Even if it comes from God. If there would come a voice from God saying, "I'm against vegetarianism!" I would say, "Well, I am for it!" This is how strongly I feel in this regard."
Politics.
Singer described himself as "conservative," adding that "I don't believe by flattering the masses all the time we really achieve much." His conservative side was most apparent in his Yiddish writing and journalism, where he was openly hostile to Marxist sociopolitical agendas. In Forverts he once wrote, "It may seem like terrible apikorses [heresy], but conservative governments in America, England, France, have handled Jews no worse than liberal governments... The Jew's worst enemies were always those elements that the modern Jew convinced himself (really hypnotized himself) were his friends."
Published works.
Note: Publication dates refer to English editions, not the Yiddish originals, which often predate the versions in translation by 10 to 20 years.

</doc>
<doc id="15513" url="http://en.wikipedia.org/wiki?curid=15513" title="Islamic eschatology">
Islamic eschatology

Islamic eschatology is the branch of Islamic scholarship that studies Yawm al-Qiyāmah (pronounced "yome-ul-key-ah-mah"; Arabic: يوم القيامة‎ "the Day of Resurrection") or Yawm ad-Dīn (pronounced "yome-ud-dean"; يوم الدين "the Day of Judgment"). This is believed to be the final assessment of humanity by Allah, consisting of the annihilation of all life, resurrection and judgment.
Judgment day can happen whenever Allah wills it. 
The time of the event is not specified, although there are major and minor signs which have been foretold to happen with "Qiyamah" at the end of time. Many verses of Qur'anic Sura contain the motif of the impending Day of Resurrection.
The 75th Sura of the Qur'an, "al-Qiyama", has as its main subject the resurrection. Its tribulation is also described in the hadith, and commentaries of Islamic expositors such as al-Ghazali, Ibn Kathir, Ibn Majah, al-Bukhari, and Ibn Khuzaymah. The Day of Judgment is also known as the Day of Reckoning, the Last Day and "al-sā'ah", or the Hour.
The hadith describe the end time with more specificity than the Qur'an, describing the events of al-Qiyamah through twelve major signs. At the time of judgment, terrible corruption and chaos will rule. The Mahdi will be sent and with the help of Isa, will battle Masih ad-Dajjal. They will triumph, liberating Islam from cruelty, and this will be followed by a time of serenity with people living true to religious values. However there is no mention of the advent of Mahdi and Isa in one era in any of the hadith. Some Muslim scholars translate the Arabic word "Imam" as "Mahdi" to prove the advent of Mahdi and Isa in a single era.
Like other Abrahamic religions, Islam also teaches resurrection of the dead, a final tribulation and eternal division of the righteous and wicked. Islamic apocalyptic literature describing Armageddon is often known as "fitnah", "malāhim", or "ghaybah" in Shī‘a Islam. The righteous are rewarded with pleasures of Jannah, while the unrighteous are tortured in Jahannam.
Six articles of faith.
The Day of Judgment or Resurrection, al-Qiyāmah, is one of the six articles of faith in Islam. The tribulation associated with it is described in the Qur'an and hadith, and commentaries of Islamic expositors like al-Ghazali, Ibn Kathir, Ibn Majah, al-Bukhari, and Ibn Khuzaymah. The Day of Judgment is also known as the Day of Reckoning, the Hour, and the Last Day. The Day of Judgment or Resurrection, al-Qiyāmah, relates to one of the six "aqīdah" in Sunni Islam, and seven "aqidah" in Shī‘a belief.
Sources.
There are two main sources in Islamic scripture that discuss the Last Judgment: the Qur'an, which is viewed in Islam as infallible, and the hadith, or sayings of the prophet. Hadith are viewed with more flexibility due to the late compilation of the traditions in written form, two hundred years after the death of Muhammad. The concept has also been discussed in commentaries of Islamic scholars such as al-Ghazali, Ibn Kathir, and Muhammad al-Bukhari.
Last Judgment in the Qur'an.
The Qur'an describes the Last Judgment, with a number of interpretations of its verses. There are specific aspects:
Three periods.
There are three periods before the Day of Judgment, also known as "ashratu's-sa'ah" or "alamatu qiyami's-sa'ah," with some debate as to whether the periods could overlap.
Major and minor signs.
There are a number of major and minor signs of the end of days in Islam. There is debate over whether they could occur concurrently or must be at different points in times, although Islamic scholars typically divide them into three major periods.
Minor signs.
With regards to the minor signs of Qiyamah. These can be derived from the Hadith (sayings) of Muhammad. The majority of these have been collected, summarized and listed below to the best of knowledge:
Also Ibn Umar reported: The Messenger of Allah, peace be upon him, turned to us and said, “O emigrants, there are five things with which you will be tested, and I seek refuge with Allah lest you live to see them:
1. Sexual immorality never appears among people to such an extent that they commit it openly except that they will be afflicted by plagues and diseases unknown to their forefathers;
2. They do not cheat in weights and measures (business, trades, etc.) except that they will be stricken with famine, calamity, and the oppression of rulers;
3. They do not withhold charity from their wealth except that rain will be withheld from the sky, and were it not for the animals there would be no rain at all;
4. They do not break their covenant with Allah and His Messenger except that Allah will enable their enemies to overpower them and take some of what is in their hands;
5. And unless their leaders rule according to the Book of Allah and seek every good from that which Allah has revealed, then Allah will cause them to fight one another.”
Major signs.
Following the second period, the third will be marked by the major signs known as "alamatu's-sa'ah al- kubra" (The major signs of the end). They are as follows:
Major figures.
Mahdi.
Mahdi (Arabic: مهدي‎) translates to 'guided one', with hadith being the primary source of his descriptions. His appearance will be the first sign of the third period. Hadith write that he will be a descendant of Muhammad through his daughter Fatimah and cousin Ali. The Mahdi will be looked upon to kill Al-Dajjal and end the prevalent disintegration of the Muslim community to prepare for the reign of Jesus who will rule for a time after. The Mahdi will similarly kill all enemies of the Prophet and fulfill the prophetic mission as a vision of justice and peace before following Jesus’ rule. The physical features of Mahdi are described in the hadith—he will be of Arab complexion and average height with a large belly, large eyes and a sharp nose. He will have a mole on his cheek, the sign of the prophet on his shoulder, and be recognized by the caliphate while he sits at his own home. As written by Abu Dawud:
Our Mahdi will have a broad forehead and a pointed (prominent) nose. He will fill the earth with justice as it is filled with injustice and tyranny. He will rule for seven years.—Abu Dawud, Sahih, 2.208 and Fusul al-muhimma, 275
Though the duration of his rule differs, hadith are consistent in describing that Allah will perfect him in a single night with inspiration and wisdom, and his name will be announced from the sky. He will bring back worship of true Islamic values, and bring the Ark of the Covenant to light. He will conquer Istanbul and Mount Daylam. And will Eye Jerusalem and the Dome as his Home. His banner will be that of the prophet Muhammad: black and unstitched, with a halo. Unopened since the death of Muhammad, the banner will unfurl when the Mahdi appears. He will be helped by angels and others that will prepare the way for him. He will understand the secrets of abjad.
Sunni and Shi'ite perspectives on the Mahdi.
Sunni and Shi'ite Islam have different beliefs on the identity of Mahdi. Historically, Sunni Islam has derived religious authority from the caliphate, who was in turn appointed by the companions of Muhammad at his death. The Sunnis view the Mahdi as the successor of Mohammad, the Mahdi is expected to arrive to rule the world and reestablish righteousness. Various Sunnis also share a parallel belief that though there may be no actual Mahdi, the existence of mujaddid will instead lead the Islamic revolution of a renewal in faith and avoidance of deviation from God’s path. Such an intellectual and spiritual figure of Sunni tradition has been attributed to numerous Muslims at the end of each Muslim century from the origin of Islam through present day. This classical interpretation is favored by Sunni scholars like Ghazali and Ibn Taymiyyah.
Shi'a Islam, in distinction, followed the bloodline of Muhammad, favoring his cousin and son by marriage, Ali. Ali was appointed the first Imam, and following him there were eleven more. Muhammad al-Mahdi, otherwise known as the twelfth imam, went into hiding in 873 AD at the age of four. His father was al-`Askari, and had been murdered, and so he was hidden from the authorities of the Abbasid Caliphate. He maintained contact with his followers until 940 AD, when he was hidden. Twelver Shia Islam believes that al-Mahdi is the current Imam, and will emerge at the end of the current age. Some scholars say that, although unnoticed by others present, the Mahdi of Twelver Shi'a Islam continues to make an annual pilgrimage while he resides outside of Mecca. In distinction, Sunni Islam foresees him as a separate and new person. The present Ayatollahs of Iran see themselves as joint caretakers of the office of the Imam until he returns.
The Mahdi is not described in the Qurʾān, only in hadith, with scholars suggesting he arose when Arabian tribes were settling in Syria under Muawiya. “They anticipated 'the Mahdi who will lead the rising people of the Yemen back to their country’ in order to restore the glory of their lost Himyarite kingdom. It was believed that he would eventually conquer Constantinople.”
Claimants of the Mahdi.
Throughout history, there have been multiple claimants to the role of Mahdi that had come into existence through their pious deeds and by subsequently acquiring their own following. One of these men, Muhammad al-Hanifiyya was said to have judgment and character over rival caliphs; and mysteries of his death arose in the 8th century. It was believed he had in fact not died and would one day return as the Mahdi. The sect of Mahdavis arose as followers of another claimant, Muhammad Mahdi of Janpur in the 15th century. Furthermore, a potential Mahdi, Muhammad Ahmad of Sudan, was believed to hold the title following his self-proclamation in 1881 and stand against the Turco-Egyptian government as well as the British. Additionally, Mīrzā Ghulām Aḥmad of Punjab claimed to be the Mahdi during the same period as Muhammad Ahmad and considered a heretic by Orthodox Muslims, though he amassed a substantial following of 10 to 20 million and is credited with founding the sect of Ahmadiyya, Today, this sect is established in over 200 countries and territories of the world. It should not be forgotten that two linked Shi'i movements, that of the Bábís and that of the Bahá'ís believed (and believe) that their prophets, Sayyid "Ali Muhammad, the Bab" (d. 1850) and Mirza Husayn "Ali Nuri, Bahá'u'lláh were fulfillers of prophecy. The Bab is thought to be the return of the Twelfth Imam and Bahá'u'lláh the Mahdi. Since the Baha'is now preach a fairly successful international religion with possibly 6 million followers, their concept of a fulfillment of Islamic prophecy is now currently well outside the Islamic world.
Isa.
Isa is the Arabic name for Jesus of Nazareth, and his return is considered the third major sign of last days, while the second is the appearance of Masih ad-Dajjal. Although Muhammad is the preeminent Prophet in Islam, Jesus is the only Prophet who is said not to have died but rather raised up by Allah other than Idris (Enoch) mentioned in the Quran. Thus, in accordance with post-Quranic hadiths, he will conceivably return to Earth as a just judge before the Day of Judgment. As written in hadith:
Abu Hurayrah narrates that the Messenger of Allah said, "By Him in whose hands my soul rests! It is definitely close in that time that Isa, Son of Maryam descends amongst you as a just ruler. He will break the cross, kill the swine and abolish jaziya. And money will abound in such excess that no one will accept it.—Ahmad bin Hambal, al-Musnad, vol 2, p. 240
Hadith reference both the Mahdi and Isa simultaneously and the return of the Mahdi will coincide with the return of Isa. He will descend from the heavens in "al-Quds" at dawn. The two will meet, and Mahdi will lead the people in fajr prayer. After the prayer, they will open a gate to the west and encounter Masih ad-Dajjal. After the defeat of ad-Dajjal, Isa will lead a peaceful forty-year reign until his death. He will be buried in a tomb beside Muhammad in Medina. Though the two most certainly differ regarding their role and persona in Islamic eschatology, the figures of the Mahdi and Isa are ultimately inseparable for according to the Prophet. Though Isa is said to descend upon the world once again, the Mahdi will already be present.
Al-Dajjal.
Al-Dajjal or the Antichrist or False Messiah does not appear in the Quran but is a prominent figure in the Hadiths and Islamic eschatology as a whole. He appears gruesome and is blind in his right eye. His one eye is thought to be a symbol that correlates with how single minded he is in achieving his goal of converting Muslims to his side. Al-Dajjal has the intention of gaining followers through his miracle working abilities and apparent wealth and generosity. These abilities are a test for true believers of Islam, who have been warned about his power and must resist his material temptations. He is thought to appear prior to the Day of Judgment, where he will engage in an epic battle with and be killed by either Jesus (according to Sunni tradition) or the Mahdi (according to the Shia tradition). Al-Dajjal functions symbolically as a key cog in overall Islamic eschatological picture, which emphasizes the world coming to an end, of good finally triumphing over evil, and of the remarkable events that will prefigure the replacement of the mortal world with a more authentic form of existence in the afterlife. Various Muslim political movements use the concept of Al-Dajjal to comment on contemporary events, and often identify him with opposing regimes or other worldly forces that they consider as harmful to Islam.
Ya'juj and Ma'juj.
The fourth major sign of end time will be that the wall which imprisons the nations of Ya'juj and Ma'juj will break, and they will surge forth. Some Islamic scholars, such as Imran Nazar Hosein, believe the wall began to crack during the life of Muhammad. This is supported in the hadith when the prophet mentions that "a hole has been made in the wall containing the Ya'juj and Ma'juj", indicating the size of the hole with his thumb and index finger. Their release will occur forty years prior to the Last Judgment:
But when Ya'jooj and Ma'jooj are let loose and they rush headlong down every hill and mountain—Qur'an 21:96
They will ravage the earth. Ultimately, Allah will send worms and insects to destroy them.
Major events.
Desertion of Medina / Destruction of Mecca / Beast of the Earth.
The fifth sign is that Medinah will be deserted, and all that remains in the city will be date palms. The just will have gone to join Mahdi, and the evil to Dajjal. Medinah will have been depopulated for forty years by the time of al-Qiyama. The sixth sign is that a thin ruler with short legs from Ethiopia will attack Mecca and destroy the Kabah.
The seventh sign is written in the Ahadith, and is the appearance of the "da'ba-tul-ard", or the "Beast of the Earth", who will populate the entire world and judge the wicked:
And when the Word is fulfilled against [the unjust], We shall produce from the earth a Beast to [face] them: he will speak to them, for that mankind did not believe with assurance in our Signs.—Qur'an 27:82
The entire world will be engulfed by "dukhan" or smoke, for forty days and there will be three huge earthquakes. The Qur'an will be taken to the heavens and even the huffaz will not recall its verses. Finally, a pleasant breeze will blow that shall cause all believers to die, but infidels and sinners will remain alive. A fire will start from Hadramawt in Yemen that shall gather all the people of the world in the land of Mahshar, and al-Qiyamah will commence.
Resurrection of the dead.
In the Qur'an, "barzakh" (Arabic: برزخ‎) is the intermediate state in which "nafs" of the deceased are held between realities to rest with loved ones until "Qiyamah".
The eighth sign is a breeze bearing a pleasant scent will emanate from Yemen, causing the "awliya", "sulaha" and the pious to die peacefully once they inhale it. After the believers die, there will be a period of 120 years during which the world will hold only "kafirs", sinners, oppressors, liars and adulterers, and there would be a reversion to idolatry.
The ninth sign is the rising of the sun from the West after a long night, which after midday will set again. According to Hadith:
Abu Hurayrah states that the Messenger of Allah (saw) as said, “The Hour will not be established until the sun rises from the West and when the people see it they will have faith. But that will be (the time) when believing of the soul, that will have not believed before that time, will not benefit it.—Ibn Maja, as-Sunan, vol. 2 p 1352-53
The final signs will be "nafkhatu'l-ula", when the trumpet will be sounded for the first time, and which will result in the death of the remaining sinners. Then there will be a period of forty years. The eleventh sign is the sounding of a second trumpet to signal the resurrection as "ba'as ba'da'l-mawt". As written in the Qur'an:
The Trumpet will (just) be sounded, when all that are in the heavens and on earth will swoon, except such as it will please Allah (to exempt). Then will a second one be sounded, when, behold, they will be standing and looking on!—Sura 39 (Az-Zumar), ayah 68
 All will be naked and running to the Place of Gathering, while the enemies of Allah will be travelling on their faces with their legs upright.
Finally, there will be no more injustice: Surely God does not do injustice to the weight of an ant, and if it is a good deed He multiplies it and gives from Himself a great reward.—Sura 4 An-Nisa, ayah 40
Separation of the righteous and the damned at al-Qiyamah.
At divine judgment, each person's "Book of Deeds" will be read, in which 'every small and great thing is recorded', will be read, with actions before adolescence not written. Records shall be given in the right hand if they are good, and the left if they are evil. Even the smallest acts will not be ignored: Then shall anyone who has done an atom's weight of good, see it!And anyone who has done an atom's weight of evil, shall see it.—Qur'an, sura 99 Az-Zalzala, ayat 7-8
 This will be followed by perfect, divine and merciful justice. The age of the hereafter, or rest of eternity, is the final stage after the Day of Judgment, when all will receive their judgment from God.
Those who believe in that which is revealed unto thee, Muhammad, and those who are Jews, and Christians, and Sabians - whoever believeth in Allah and the Last Day and doeth right - surely their reward is with their Lord, and there shall no fear come upon them neither shall they grieve.—Qur'an, sura 2 Al-Baqara, ayah 62 Muhammad will be the first to be resurrected.
If one did good deeds, one would go to "Jannah", and if unrighteous would go to "Jahannam". Punishments will include "adhab", or severe pain, and "khizy" or shame. There will also be a punishment of the grave (for those who disbelieved) between death and the resurrection.
Islamic eschatology in literature.
Ibn al-Nafis wrote of Islamic eschatology in "Theologus Autodidactus" (circa 1270 CE), where he used reason, science, and early Islamic philosophy to explain how he believed al-Qiyamah would unfold, told in the form of a theological fiction novel.
Imran Nazar Hosein wrote numerous books that deals with Islamic eschatology ("Ilmu Ākhir al-Zamān" - Knowledge of the later days), among which the most famous is "Jerusalem in the Qur'an".

</doc>
<doc id="15516" url="http://en.wikipedia.org/wiki?curid=15516" title="Intelsat">
Intelsat

Intelsat, S.A. is a communications satellite services provider.
Originally formed as International Telecommunications Satellite Organization (INTELSAT), it was—from 1964 to 2001—an intergovernmental consortium owning and managing a constellation of communications satellites providing international broadcast services.
s of 2011[ [update]], Intelsat operates a fleet of 52 communications satellites, which is one of the world's largest fleet of commercial satellites.
History.
The Inter-Governmental Organization (IGO) began on 20 August, 1964, with 11 participating countries. On 6 April, 1965, Intelsat’s first satellite, the Intelsat I (nicknamed "Early Bird"), was placed in geostationary orbit above the Atlantic Ocean by a Delta D rocket.
In 1973, the name was changed and there were 80 signatories. Intelsat provides service to over 600 Earth stations in more than 149 countries, territories and dependencies. By 2001, INTELSAT had over 100 members. It was also this year that INTELSAT privatized and changed its name to Intelsat.
Since its inception, Intelsat has used several versions (blocks) of its dedicated Intelsat satellites. INTELSAT completes each block of spacecraft independently, leading to a variety of contractors over the years. Intelsat’s largest spacecraft supplier is Space Systems/Loral, having built 31 spacecraft (as of 2003), or nearly half of the fleet.
The network in its early years was not as robust as it is now. A failure of the Atlantic satellite in the spring of 1969 threatened to stop the "Apollo 11" mission; a replacement satellite went into a bad orbit and could not be recovered in time; NASA had to resort to using undersea cable telephone circuits to bring Apollo's communications to NASA during the mission. Fortunately, during the Apollo 11 moonwalk, the moon was over the Pacific Ocean, and so other antennas were used, as well as INTELSAT III, which was in geostationary orbit over the Pacific.
Commercialization.
By the 1990s, building and launching satellites was no longer exclusively a government domain and as country-specific telecommunications systems were privatized, several private satellite operators arose to meet the growing demand. In the U.S., satellite operators such as Panamsat, Orion Communications, Columbia Communications, Iridium, Globalstar, TRW and others formed under the umbrella of the Alliance for Competitive International Satellite Services (ACISS) to press for an end to the IGOs and the monopoly position of COMSAT the US signatory to Intelsat and Inmarsat. In March 2001, the US Congress passed the Open Market Reorganization for the Betterment of International Telecommunications (ORBIT) Act to privatize COMSAT and reform the role of the international organizations. In April 1998, to address US government concerns about market power, Intelsat's senior management spun off five of its older satellites to a private Dutch entity, New Skies Satellites, which became a direct competitor to INTELSAT. To avert the US government's interference with Intelsat, Intelsat's senior management unsuccessfully considered relocating the IGO to another country.
Privatization.
On 18 July, 2001, Intelsat became a private company, 37 years after formation. Prior to Intelsat's privatization in 2001, ownership and investment in INTELSAT (measured in shares) was distributed among INTELSAT members according to their use of services. Investment shares determined each member’s percentage of the total contribution needed to finance capital expenditures. The organization’s primary source of revenue was satellite usage fees which, after deduction of operating costs, was redistributed to INTELSAT members in proportion to their shares as repayment of capital and compensation for use of capital. Satellite services were available to any organization (both INTELSAT members and non-members), and all users paid the same rates.
Today, the number of Intelsat satellites, as well as ocean-spanning fibre-optic lines, allows rapid rerouting of traffic when one satellite fails. Modern satellites are more robust, lasting longer with much larger capacity.
Intelsat Americas-7 (known formerly as Telstar 7 and now known as Galaxy 27) experienced a several-day power failure on 29 November, 2004. The satellite returned to service with reduced capacity.
Intelsat was sold for U.S. $3.1bn in January 2005 to four private equity firms: Madison Dearborn Partners, Apax Partners, Permira and Apollo Global Management. The company acquired PanAmSat on 3 July, 2006, and is now the world's largest provider of fixed satellite services, operating a fleet of 52 satellites in prime orbital locations. In June 2007 BC Partners announced they had acquired 76 percent of Intelsat for about 3.75 billion euros.
In April 2013 the renamed Intelsat S.A. undertook an initial public offering on the New York Stock Exchange, raising a net $550 million USD, of which $492 million was paid immediately to reduce outstanding company debts of $15.9 billion USD. In May the company announced it would be purchasing four new high-performance Boeing EpicNG 702 MP satellites.
Current operation.
Intelsat maintains its corporate headquarters in Luxembourg, with a majority of staff and satellite functions — administrative headquarters — located at the Intelsat Corporation offices in Washington, DC. In 2012, Intelsat announced that they would relocate their US headquarters from Washington to nearby Tysons Corner, Virginia by mid-2014. A highly international business, Intelsat sources the majority of its revenue from non-U.S. located customers. Intelsat's biggest teleport is the Teleport Fuchsstadt in Germany.
Spacecraft operations are controlled through ground stations in Hagerstown, Maryland (USA), Riverside, California (USA), and Fuchsstadt, Germany.
In-space refueling demonstration project.
s of 2011[ [update]], Intelsat has agreed to purchase one-half of the 2000 kg propellant payload that an MDA Corporation spacecraft satellite-servicing demonstration project would take to geostationary orbit. Catching up in orbit with four or five Intelsat communication satellites, a fuel load of 200 kg of fuel delivered to each satellite would add somewhere between two and four years of additional service life.
A near-end-of-life Intelsat satellite will be moved to a graveyard orbit 200 to(-) above the geostationary belt where the refueling will be done, "without consequence" to the Intelsat business.
s of 2010[ [update]], the business model was still evolving. MDA "could ask customers to pay per kilogram of fuel successfully added to [each] satellite, with the per-kilogram price being a function of the additional revenue the operator can expect to generate from the spacecraft’s extended operational life."
The plan is that the fuel-depot vehicle would maneuver to several satellites, dock at the target satellite’s apogee-kick motor, remove a small part of the target spacecraft’s thermal protection blanket, connect to a fuel-pressure line and deliver the propellant. "MDA officials estimate the docking maneuver would take the communications satellite out of service for about 20 minutes."
Satellites.
Renaming.
On February 1, 2007, Intelsat changed the names of 16 of its satellites formerly known under the Intelsat Americas and PanAmSat brands to Galaxy and Intelsat, respectively.
Launch vehicle.
On May 30, 2012 Intelsat signed a contract with Space Exploration technologies Corporation for the first Falcon Heavy launch vehicle.

</doc>
<doc id="15517" url="http://en.wikipedia.org/wiki?curid=15517" title="ITSO">
ITSO

ITSO may stand for:

</doc>
<doc id="15521" url="http://en.wikipedia.org/wiki?curid=15521" title="Indian numerals">
Indian numerals

Indian numerals are the symbols representing numbers in India. These numerals are generally used in the context of the decimal Hindu–Arabic numeral system, and are distinct from, though related by descent to Arabic numerals.
Devanagari numerals and their Hindi names.
Below is a list of the Indian numerals in their modern Devanagari form, the corresponding Hindu-Arabic (European) equivalents, their Hindi and Sanskrit pronunciation, and translations in some languages.
Since Sanskrit is an Indo-European language, it is obvious (as also seen from the table) that the words for numerals closely resemble those of Greek and Latin. The word "Shunya" for zero was translated into Arabic as "صفر" "sifr", meaning 'nothing' which became the term "zero" in many European languages from Medieval Latin, "zephirum".
South Indian language(s).
"For numerals in Tamil language see Tamil numerals."
"For numerals in Telugu language see Telugu numerals."
Other modern Indian languages.
The five Indian languages (Hindi, Marathi, Konkani, Nepali and Sanskrit itself) that have adapted the Devanagari script to their use also naturally employ the numeral symbols above; of course, the names for the numbers vary by language. The table below presents a listing of the symbols used in various modern Indian scripts for the numbers from zero to nine:
"For numerals in Bengali language and Assamese languages see Bengali-Assamese numerals."
History.
A decimal place system has been traced back to ca. 500 in India. Before that epoch, the Brahmi numeral system was in use; that system did not encompass the concept of the place-value of numbers. Instead, Brahmi numerals included additional symbols for the tens, as well as separate symbols for "hundred" and "thousand".
The Indian place-system numerals spread to neighboring Persia, where they were picked up by the conquering Arabs. In 662, a Nestorian bishop living in what is modern day Iraq said:
I will omit all discussion of the science of the Indians ... of their subtle discoveries in astronomy — discoveries that are more ingenious than those of the Greeks and the Babylonians - and of their valuable methods of calculation which surpass description. I wish only to say that this computation is done by means of nine signs. If those who believe that because they speak Greek they have arrived at the limits of science would read the Indian texts they would be convinced even if a little late in the day that there are others who know something of value.
The addition of zero as a tenth positional digit is documented from the 7th century by Brahmagupta, though the earlier Bakhshali Manuscript, written sometime before the 5th century, also included zero. But it is in Khmer numerals of modern Cambodia where the first extant material evidence of zero as a numerical figure, dating its use back to the seventh century, is found.
As it was from the Arabs that the Europeans learned this system, the Europeans called them "Arabic numerals;" the Arabs refer to their numerals as "Indian numerals". In academic circles they are called the "Hindu–Arabic" or "Indo–Arabic" numerals.
The significance of the development of the positional number system is probably best described by the French mathematician Pierre Simon Laplace (1749–1827) who wrote:
It is India that gave us the ingenious method of expressing all numbers by the means of ten symbols, each symbol receiving a value of position, as well as an absolute value; a profound and important idea which appears so simple to us now that we ignore its true merit, but its very simplicity, the great ease which it has lent to all computations, puts our arithmetic in the first rank of useful inventions, and we shall appreciate the grandeur of this achievement when we remember that it escaped the genius of Archimedes and Apollonius, two of the greatest minds produced by antiquity.
Tobias Dantzig had this to say in "Number":
This long period of nearly five thousand years saw the rise and fall of many civilizations, each leaving behind a heritage of literature, art, philosophy, and religion. But what was the net achievement in the field of reckoning, the earliest art practiced by man? An inflexible numeration so crude as to make progress well nigh impossible, and a calculating device so limited in scope that even elementary calculations called for the services of an expert. [...] man used these devices for thousands of years [...] without contributing a single important idea to the system!
 [...] even when compared with the slow growth of ideas during the Dark Ages, the history of reckoning presents a peculiar picture of desolate stagnation.
 When viewed in this light, the achievements of the unknown Hindu, who some time in the first centuries of our era discovered the "principle of position" assumes the importance of a world event.

</doc>
<doc id="15524" url="http://en.wikipedia.org/wiki?curid=15524" title="Ian Botham">
Ian Botham

 
Sir Ian Terence Botham, OBE (born 24 November 1955) is a former England Test cricketer and Test team captain, and current cricket commentator. He was a genuine all-rounder with 14 centuries and 383 wickets in Test cricket, and remains well known by his nickname "Beefy". While at times a controversial player both on and off the field, Botham also held a number of Test cricket records, and until 17th April 2015 held the record for the highest number of wickets taken by an England bowler, when surpassed by James Anderson.
He is generally regarded as being England's greatest ever all-rounder, particularly in Test cricket, although having earned celebrity status, his award of a knighthood was in recognition of his services to charity.
Just like fellow cricketers Denis Compton, Chris Balderstone and Arnold Sidebottom, Botham was also a talented footballer, and made 11 appearances in the Football League.
Botham was knighted by Queen Elizabeth II in the 2007 New Years Honours List and on 8 August 2009, was inducted into the ICC Cricket Hall of Fame.
Early life.
Botham was born in Heswall on the Wirral, to Herbert Leslie Botham (who worked for Westland) and Violet Marie, née Collett (a nurse). Both his parents played cricket. He went to Milford Junior School in Yeovil, Somerset, where his "love affair" with sport began, and played for Somerset Under-15s. He left Bucklers Mead Comprehensive School at the age of 15, intent on playing cricket for the Somerset County Cricket Club, although he also had an offer to play football for Crystal Palace F.C. From an early age he was always single-minded. When informed that Botham wanted to be a sportsman, the Careers Mistress at his school said to him, "Fine, everyone wants to play sport, but what are you really going to do?".
Domestic career.
In his non-first-class appearances for Somerset, his bowling figures did not stand out, but there were some sizeable scores, namely 91 for the Under-25s v Glamorgan Under-25s, 82 and 42 v Cornwall, 51 v Gloucester Under-25s, 50 v Glamorgan 2nd XI and in his last game (before his 1986 comeback match) 100 against Glamorgan 2nd XI.
In first-class cricket, he scored 19,399 runs at 33.97, took 1,172 wickets at 27.22 and held 354 catches. He played for Durham, Somerset and Worcestershire, as well as a season (1987–88) in Australia playing for the Queensland Bulls.
Botham began his first-class career in 1974 with Somerset. In that year, when playing against Hampshire and facing the West Indian fast bowler Andy Roberts, a bouncer hit him straight in the mouth. He spat out teeth and simply carried on batting. In 1986 he resigned from Somerset, in protest against the sacking of his friends Sir Viv Richards and Joel Garner, and joined Worcestershire, playing for that county between 1987 and 1991. In 1992, he joined County Championship newcomers Durham before retiring midway through the 1993 season, his last match being Durham's match against the visiting Australian XI.
Grade cricket in Australia 1976/1977.
Only months before announcing his presence on the international scene, Botham played Grade cricket for the University of Melbourne Cricket Club during the 1976/77 Australian Domestic Season. In a season where 5 of the 15 rounds were abandoned because of adverse weather, Botham joined up for the second half as a result of sponsorship arranged through the TCCB by Whitbread's Brewery. He was joined by Yorkshire's Graham Stevenson. Botham played 4 matches, the first of which was against Northcote in a one-day game on 8 January 1977. Brought on as first change, he finished with figures of 10.5-0-83-0 (8-ball overs). He batted at number 4 being run out for 0. The opposition wicket-keeper Richie Robinson would be an opponent in the Test arena only months later.
Botham's second match was against St Kilda, scheduled as a 2-day game over consecutive weeks. It became a one day game after the first Saturday was washed out. His analysis was 10–1–39–2. He scored a hard hit 41 in 33 minutes with five 4s. His third appearance was against Essendon CC in another 2-day game. He was the side's most successful bowler with analyses of 22.7–2–92–4 but fell for another 0 caught off leg-spinner Keith Kirby. His last match was against North Melbourne CC. He was promoted to opening the batting but was caught for 3 off Neil Majewski. His bowling analysis was 27–4–86–0 against a side that included Rohan Kanhai and Ian Chappell. Prior to this game, the match against Richmond was abandoned because of rain and the last game was also abandoned. His complete analysis was 44 runs in 4 matches at a batting average of 11, and 6 wickets at a bowling average of 51.16. Botham took one catch.
International career.
Botham made his Test début for England on 28 July 1977 in the Third Test against Australia, where he took five wickets for 74 runs in the first innings. He went on to enjoy a Test career spanning 15 years, in which he played in 102 matches.
Botham finished his Test career with 5,200 runs at an average of 33.54, taking 383 wickets at an average of 28.40, and holding 120 catches. He is generally regarded as one of England's greatest Test players. He was also England's captain for 12 Tests in 1980 and 1981. As captain of the England XI, Botham is generally considered to have been unsuccessful. His tenure was brief and under his captaincy the team achieved no wins, 8 draws and 4 losses. In his defence, 9 of his matches as captain were against the best team of that era, the West Indies, who won 12 out of the next 13 Tests played against England.
He was renowned as a big-hitting batsman, though with a classical technique of playing straight, and as a fast-medium paced swing bowler who could be very effective when atmospheric conditions favoured his style.
Records.
Botham holds a number of Test records as an all-rounder, including being the fastest (in terms of matches) to achieve the "doubles" of 1,000 runs and 100 wickets, 2,000 runs and 200 wickets, and 3,000 runs and 300 wickets. He briefly held the world record for the greatest number of Test wickets, although his tally has subsequently been passed by several specialist bowlers.
Botham scored a century and took 5 wickets in an innings in the same Test match on 5 occasions; no-one else has managed this feat more than twice. In 1980, playing against India, he became the first player to score a century and take ten wickets in a Test match (Alan Davidson was the first to score 100 runs and take 10 wickets in a Test but that did not include a century).
During the 1981 Ashes, Botham set a record of six sixes in a single Ashes Test Match at Old Trafford. That record remained unbroken until 7 August 2005 when Andrew Flintoff scored five in the first innings and four in the second innings of the second Test at Edgbaston, and again until 12 September 2005, when Kevin Pietersen hit seven sixes in the second innings of the last Test at The Oval.
One Day Internationals.
Botham's ODI career included 116 games from 1976 to 1992. He made his debut on 26 August against the West Indies at Scarborough. He finished with a batting average of 23.21 (nine 50s, no 100s, cumulative score of 2113 runs), and a bowling average of 28.5 (strike rate 43.24, 145 wickets in total, best figures 4/31).
1981 Ashes Tour: "Botham's Ashes".
In 1980 Botham had been appointed captain of the England team. However, his captaincy proved to be an unhappy one; he lost form and the team did not do well.
He resigned the captaincy after a loss and a draw in the first two Tests of the 1981 Ashes series. The resignation itself was the cause of controversy, with Sir Alec Bedser, Chairman of the TCCB selectors, making it clear after media questioning that Botham would have been fired in any event. Botham himself refers to the event as his "dismissal" in his autobiography. In this Test, the second played at Lord's and his last as England captain, Botham was dismissed for a pair. He returned to an embarrassed silence in the pavilion and after the previous year's events at the centenary Test, this possibly was the final straw. For the remainder of his cricket-playing career, Botham refused to acknowledge MCC members in the pavilion when playing at Lord's. However, Botham subsequently accepted an Honorary Life membership of the MCC and his portrait (depicting him enjoying a cigar) now hangs prominently in the Long Room Bar at Lord's.
Mike Brearley, the captain whom Botham had replaced, took up the reins again for the Third Test scheduled for 16 to 21 July, at Headingley. Australia won the toss and elected to bat. They batted all day Thursday and most of Friday, declaring after tea at 401 for 9, John Dyson making 102 and Botham taking 6 for 95. The England openers Graham Gooch and Geoff Boycott survived the remaining few overs, and England finished the day on 7 for no wicket.
The next day, Saturday, was a disaster for England: Gooch was out in the first over of the day, and although Boycott and Brearley then attempted to dig in, they were both out before lunch. None of the other batsmen got going with the exception of Botham who top-scored with 50 — his first half century since his first Test as captain 13 matches earlier. England were all out in the third session for 174. Australia enforced the follow-on and piled on the pressure; Gooch was out for 0 on his third ball of the first over caught by Terry Alderman off the bowling of Dennis Lillee. By the close, England had struggled to just 6 for 1, still 221 behind Australia.
By all accounts, both teams' players thought Australia would win the match; indeed the England team had enjoyed a raucous barbecue "chez" Botham on the Saturday evening, such was their lack of faith in a positive result. Sunday 19 July was a rest-day and the newspapers roasted the lamentable England performance. Morale was not improved by Ladbrokes offering odds of 500–1 against England winning the match, as displayed on the Headingley electronic scoreboard. Controversially, the Australian wicket-keeper Rod Marsh and opening bowler Dennis Lillee both placed bets on England to win, later claiming that 500–1 were silly (if not incredible) odds on any two-horse race.
On the Monday morning 500–1 odds began to look somewhat more ungenerous as first Brearley, then David Gower and Mike Gatting all fell cheaply reducing England to 41 for 4. Boycott was still anchored at the other end however, and he and Peter Willey added 50 runs before lunch. In the afternoon, Willey was out for 33 and England were still in deep trouble at 105 for 5 when Botham went in to bat. Matters did not improve as first Geoff Boycott and then Bob Taylor were quickly dismissed. At 135 for 7 an innings defeat looked almost certain.
When Graham Dilley joined him at the crease, Botham reportedly said, "Right then, let's have a bit of fun...". With able support from Dilley (56) and Chris Old (29), Botham hit out and by the close of play was 145 not out with Bob Willis hanging on at the other end on 1 not out. England's lead was just 124 but there remained some glimmer of hope. On the final day's play there was time for just four more runs from Botham before Willis was out and Botham was left on 149 not out. Wisden rated this innings as the 4th best of all time.
Willis' far greater contribution was with the ball. After Botham took the first wicket, Willis skittled Australia out for just 111, finishing with figures of 8 for 43 – rated by Wisden as the 7th best bowling performance of all time. England had won by just 18 runs. It was only the second time in history that a team following-on had won a Test match.
The next Test match, at Edgbaston, looked almost as hopeless, if not hapless, from England's point of view. In a low scoring match (no-one made a score over 48), Australia needed 151 to win. At 105–5, things looked a little worrying for them, but an Australian win still seemed the most likely result. Botham then took 5 wickets for only 1 run in 28 balls to give England victory by 29 runs. Later, Brearley said that Botham had not wanted to bowl and had to be persuaded to do so.
The Old Trafford Test was less of a turnaround and more of a team effort than the previous two Tests, but Botham again was England's hero hitting yet another century in what Lillee claimed to be a better innings than his Headingley heroics. Botham had joined Chris Tavaré with the score at 104–5. Botham then scored 118 in a partnership of 149 before he was dismissed. He hit six sixes in this innings, three off Lillee's bowling, two of them in the same over. Remarkably, even though he seemed to take his eye off the ball while hooking some fearsome Lillee bouncers, his sheer power and strength carried the ball over the boundary rope. In total Botham batted for 5 hours shorter than Tavaré and yet scored 40 more runs. England won that match, then drew the last one at The Oval (Botham taking 6 wickets in the first innings), and thereby winning the series 3–1. Hardly surprisingly, Botham was named Man of the Series, scoring 399 runs and taking 34 wickets.
Football career.
An occasional professional footballer as well as cricketer, Botham had to choose very early in his career whether to play football or cricket. At one point during his career, in an effort to get fit after an injury, in March 1980 he joined the football club Scunthorpe United, where he played as a centre half and made 11 appearances in the Football League.
Botham also had a spell at Yeovil Town. Whilst with Yeovil, Botham made an appearance for the Football Association XI (a representative side for non-league footballers) against the Northern Football League at Croft Park during the 1984-85 season.
Controversies.
Botham was suspended for two months by the England and Wales Cricket Board in 1986 for smoking cannabis. In 1994, Imran Khan accused Botham and fellow England player Allan Lamb of bringing the game into disrepute in an article for "India Today"; Botham and Lamb instigated a libel action in response. The case was heard at the High Court in 1996 with the court choosing to hear on the second day a separate action brought solely by Botham against Khan who had suggested in a Sun newspaper article that Botham had been involved in ball-tampering. This would become the subject of a court case later on, one that Imran Khan would go on to win. Botham was liable for all expenses in the court case in the ruling, including those incurred by Khan.
Botham also fell out publicly with other players, including fellow England player, opener Geoff Boycott, Somerset captain Peter Roebuck, and Australian batsman Ian Chappell, with whom he had an altercation in an Adelaide Oval car park during the 2010–11 Ashes series.
Botham's private life has also made occasional dramatic appearances in Britain's tabloid newspapers, with at least one extramarital affair prompting a public apology to his wife Kathy. Botham was also sacked from the Queensland team after being arrested for assault of a fellow airline passenger.
In August, 2014, the image of an erect penis was sent from Botham's Twitter account. Botham denied that the member was his and claimed that his account had been hacked.
Charity walks.
Botham has been a prodigious fundraiser for charitable causes, undertaking a total of 12 long-distance charity walks. His first, in 1985, was a 900-mile trek from John o' Groats to Land's End. His efforts were inspired after a visit to Taunton's Musgrove Park Hospital whilst receiving treatment for a broken toe; when he took a wrong turn into a children's ward, he was devastated to learn that some of the children had only weeks to live, and why. Since then, his efforts have raised more than £12 million for charity, with Leukaemia Research among the causes to benefit.
Test centuries and five-wicket innings.
Botham achieved the double of making a century and taking 5 wickets in an innings in the same Test match 5 times. Only three other players have achieved this feat more than once: Sir Gary Sobers, Mushtaq Mohammad and Jacques Kallis, who have each done it twice.
He is the only man to have made a century and take 8 wickets in an innings in the same Test match, 108 and 8/34 against Pakistan at Lord's in 1978.
Botham was also the first of only two men to make a century and take 10 wickets in the same Test match, the other being Imran Khan. Botham did this in the Centenary Test in Bombay in 1979–80 (114, 6/58 and 7/48), the last match before he became England captain. In the 25 Tests he played before he became captain he made 6 centuries and took 5 wickets in an innings 14 times, including 10 in a match 3 times, an astonishing record.
Personal life.
In 1976, in the Borough of Doncaster, Botham married Kathryn Waller (now Lady Botham) whom he first met in June 1974. After their marriage, they lived until the late 1980s in Epworth, near Scunthorpe. They have one son, Liam (born August 1977), and two daughters, Becky (born November 1985) and Sarah. Sarah works for Sky as a production assistant, and Liam is a former professional cricketer and rugby player. Viv Richards is godfather to Liam.
Botham is an enthusiastic football fan and supports, Chelsea. He is also a vice president at Scunthorpe United Football Club. Botham is also passionate about playing golf. Ian Botham is also an avid trout and salmon angler, and presented a TV series "Botham on the Fly" with guests such as Eric Clapton, Mike Atherton and Chris Tarrant.
He has a tattoo on his right shoulder which is dedicated to his wife. He is partially colour-blind.
He is a supporter of the Conservative Party

</doc>
<doc id="15526" url="http://en.wikipedia.org/wiki?curid=15526" title="Id Software">
Id Software

Id Software (; see Company name, sometimes erroneously called "ID Software") is an American video game development company with its headquarters in Richardson, Texas. The company was founded in 1991 by four members of the computer company Softdisk: programmers John Carmack and John Romero, game designer Tom Hall, and artist Adrian Carmack (no relation to John Carmack). Business manager Jay Wilbur was also involved.
Id made important technological developments in video game technologies for the PC (running MS-DOS and Windows), including work done for "Wolfenstein 3D", "Doom" and "Quake" franchises. Id's work was particularly important in 3D computer graphics technology and in game engines that are heavily used throughout the video game industry.
The company was also heavily involved in the creation of the first-person shooter genre. "Wolfenstein 3D" is often considered as the first true FPS, "Doom" was a game that popularized the genre and PC gaming in general, and "Quake" is the first shooter to have online multi-player, which is an essential feature of a shooter today.
On June 24, 2009, ZeniMax Media acquired the company.
History.
The founders of Id Software met in the offices of Softdisk developing multiple games for Softdisk's monthly publishing. These included "Dangerous Dave" and other titles. In September 1990, John Carmack developed an efficient way that would perform rapid side-scrolling graphics on the PC. Upon making this breakthrough, Carmack and Hall stayed up late into the night making a replica of the first level of the popular 1988 NES game "Super Mario Bros. 3", inserting stock graphics of Romero's Dangerous Dave character in lieu of Mario. When Romero saw the demo, entitled "Dangerous Dave in Copyright Infringement", he realized that Carmack's breakthrough could have potential, the team that would later form Id Software immediately began moonlighting, going so far as to "borrow" company computers that were not being used over the weekends and at nights while they designed their own remake of "Super Mario Bros. 3".
Despite their work, Nintendo turned them down, saying they had no interest in expanding to the PC market, and that Mario games were to remain exclusive to Nintendo consoles. Around this time, Scott Miller of Apogee Software learned of the group and their exceptional talent, having played one of John Romero's Softdisk games, "Dangerous Dave", and contacted Romero under the guise of multiple fan letters that Romero came to realize all originated from the same address. When he confronted Miller, Miller explained that the deception was necessary since companies at that time were very protective of their talent and it was the only way he could get Romero to initiate contact with him. Miller suggested that they develop shareware games that he would distribute. As a result, the Id Software team began the development of "Commander Keen", a Mario-style side-scrolling game for the PC, once again "borrowing" company computers to work on it at odd hours at the lake house at which they lived in Shreveport, Louisiana. On December 14, 1990, the first episode was released as shareware by Miller's company, Apogee, and orders began rolling in. Shortly after this, Softdisk management learned of the team's deception and suggested that they form a new company together, but the administrative staff at Softdisk threatened to resign if such an arrangement were made. In a legal settlement, the team was required to provide a game to Softdisk every two months for a certain period of time, but they would do so on their own. On February 1, 1991, Id Software was founded.
The shareware distribution method was initially employed by Id Software through Apogee Software to sell their products, such as the "Commander Keen", "Wolfenstein" and "Doom" games. They would release the first part of their trilogy as shareware, then sell the other two installments by mail order. Only later (about the time of the release of "Doom II") did Id Software release their games via more traditional shrink-wrapped boxes in stores (through other game publishers).
Id Software has moved from the "cube-shaped" Mesquite office, to a newly built location in Richardson, Texas.
On June 24, 2009, it was announced that Id Software had been acquired by ZeniMax Media (owner of Bethesda Softworks). The deal would eventually affect publishing deals Id Software had before the acquisition, namely "Rage", which was being published through Electronic Arts.
On June 26, 2013, Id Software president Todd Hollenshead quit after 17 years of service.
On November 22, 2013, it was announced Id co-founder and Technical Director John Carmack had fully resigned from the company to work full-time at Oculus VR of which he joined as CTO in August 2013. He was the last of the original founders to leave the company.
Company name.
The company writes its name with a lowercase "id", which is pronounced as in "did" or "kid", and
in the book, "Masters of Doom", it is said that the group identified itself as "Ideas from the Deep" in the early days of Softdisk, but in the end the name 'id' came from the phrase, "in demand."
It is presented by the company as a reference to the id, a psychological concept introduced by Sigmund Freud. Evidence of the reference can be found as early as "Wolfenstein 3D" with the statement "that's id, as in the id, ego, and superego in the psyche" appearing in the game's documentation. Prior to an update to the website, Id's History page made a direct reference to Freud.
Game development.
Technology.
Starting with their first shareware game series, "Commander Keen", Id Software has licensed the core source code for the game, or what is more commonly known as the engine. Brainstormed by John Romero, Id Software held a weekend session titled "The Id Summer Seminar" in the summer of 1991 with prospective buyers including Scott Miller, George Broussard, Ken Rogoway, Jim Norwood and Todd Replogle. One of the nights, Id Software put together an impromptu game known as "Wac-Man" to demonstrate not only the technical prowess of the "Keen" engine, but also how it worked internally.
Id Software has developed their own game engine for each of their titles when moving to the next technological milestone, including "Commander Keen", "Wolfenstein 3D", "ShadowCaster", "Doom", "Quake", "Quake II", and "Quake III", as well as the technology used in making "Doom 3". After being used first for Id Software's in-house game, the engines are licensed out to other developers. According to "Eurogamer.net", "Id Software has been synonymous with PC game engines since the concept of a detached game engine was first popularised". During the mid to late 1990s, "the launch of each successive round of technology it's been expected to occupy a headlining position", with the "Quake III" engine being most widely adopted of their engines. However Id Tech 4 had far fewer licensees than the Unreal Engine from Epic Games, due to the long development time that went into "Doom 3" which Id had to release before licensing out that engine to others.
In conjunction with his self-professed affinity for sharing source code, John Carmack has open-sourced most of the major Id Software engines under the General Public License. Historically, the source code for each engine has been released once the code base is 5 years old. Consequently, many home grown projects have sprung up porting the code to different platforms, cleaning up the source code, or providing major modifications to the core engine. "Wolfenstein 3D", "DOOM" and "Quake" engine ports are ubiquitous to nearly all platforms capable of running games, such as hand-held PCs, iPods, the PSP, the Nintendo DS and more. Impressive core modifications include DarkPlaces which adds stencil shadow volumes into the original "Quake" engine along with a more efficient network protocol. Another such project is ioquake3, which maintains a goal of cleaning up the source code, adding features and fixing bugs. Even earlier id Software code, namely for "Hovertank 3D" and "Catacomb 3D", was released in June 2014 by Flat Rock Software.
The GPL release of the "Quake III" engine's source code was moved from the end of 2004 to August 2005 as the engine was still being licensed to commercial customers who would otherwise be concerned over the sudden loss in value of their recent investment.
On August 4, 2011, John Carmack revealed during his QuakeCon 2011 keynote that they will be releasing the source code of the "Doom 3" engine (Id Tech 4) during the year.
Id Software publicly stated they would not support the Wii console (possibly due to technical limitations), although they have since indicated that they may release titles on that platform (although it would be limited to their games released during the 1990s).
Since Id Software revealed their engine Id Tech 5, they call their engines "Id Tech", followed by a version number. Older engines have retroactively been renamed to fit this scheme, with the "Doom" engine as Id Tech 1.
Linux gaming.
Id Software was an early pioneer in the Linux gaming market, and Id Software's Linux games have been some of the most popular of the platform. Many Id Software games won the Readers' and Editors' Choice awards of Linux Journal. Some Id Software titles ported to Linux are "Doom" (the first Id Software game to be ported), "Quake", "Quake II", "Quake III Arena", "Return to Castle Wolfenstein", ', "Doom 3", "Quake 4", and '. Since Id Software and some of its licensees released the source code for some of their previous games, several games which were not ported (such as "Wolfenstein 3D", "Spear of Destiny", "Heretic", "Hexen", "Hexen II", and "Strife") can run on Linux and other operating systems natively through the use of source ports. "Quake Live" also launched with Linux support, although this, alongside OS X support, was later removed when changed to a standalone title.
The tradition of porting to Linux was first started by Dave D. Taylor, with David Kirsch doing some later porting. Since "Quake III Arena", Linux porting had been handled by Timothee Besset. The majority of all Id Tech 4 games, including those made by other developers, have a Linux client available, the only current exceptions being "Wolfenstein" and "Brink". Similarly, almost all of the games utilizing the Id tech 2 engine have Linux ports, the only exceptions being those created by Ion Storm. Despite fears by the Linux gaming community that Id Tech 5 would not be ported to that platform, Timothee Besset in his blog has stated "I'll be damned if we don't find the time to get Linux builds done". Besset has stated that Id Software's primary justification for releasing Linux builds is better code quality, along with a technical interest for the platform. On January 26, 2012 Besset announced that he had left id.
John Carmack has expressed his stance with regard to Linux builds in the past. In December 2000 Todd Hollenshead has expressed support for Linux: "All said, we will continue to be a leading supporter of the Linux platform because we believe it is a technically sound OS and is the OS of choice for many server ops." However, on 25 April 2012 Carmack revealed that "there are no plans for a native Linux client" of id's most recent game, "Rage". In February 2013, Carmack argued for improving emulation as the "proper technical direction for gaming on Linux", though this was also due to Zenimax's refusal to support "unofficial binaries", given all prior ports (except for "Quake III Arena", via Loki Software, and earlier versions of "Quake Live") having only ever been unofficial. Carmack didn't mention official games "Quake: The Offering" and "Quake II: Colossus" ported by Id Software to Linux and published by Macmillan Computer Publishing USA.
Games.
"Commander Keen".
The "Commander Keen" series, a platform game introducing one of the first smooth side-scrolling game engines for MS-DOS, brought Id Software into the gaming mainstream. The game was very successful and spawned a whole series of titles. It was also the series of Id Software that designer Tom Hall was most affiliated with. The first Commander Keen trilogy was released on December 14, 1990.
"Wolfenstein".
The company's breakout product was released on May 5, 1992: "Wolfenstein 3D", a first person shooter (FPS) with smooth 3D graphics that were unprecedented in computer games, and with violent gameplay that many gamers found engaging. After essentially founding an entire genre with this game, Id Software created "Doom", "Doom II", "Quake", "Quake II", "Quake III Arena", "Quake 4" and "Doom 3". Each of these first person shooters featured progressively higher levels of graphical technology. "Wolfenstein 3D" spawned a prequel and a sequel, the prequel called "Spear of Destiny", and the second, "Return to Castle Wolfenstein", used the Id Tech 3 engine. A third "Wolfenstein" sequel, simply titled "Wolfenstein", has been released by Raven Software, using the Id tech 4 engine. The and reboot was developed using the Id tech 5 engine and released by MachineGames.
"Doom".
Eighteen months after their release of "Wolfenstein 3D", on December 10, 1993 Id Software released "Doom" which would again set new standards for graphic quality and graphic violence in computer gaming. "Doom" featured a sci-fi/horror setting with graphic quality that had never been seen on personal computers or even video game consoles. "Doom" became a cultural phenomenon and its violent theme would eventually launch a new wave of criticism decrying the dangers of violence in video games. "Doom" was ported to numerous platforms, inspired many knock-offs and was eventually followed by the technically similar "Doom II". Id Software made its mark in video game history with the shareware release of "Doom", and eventually revisited the theme of this game in 2004 with their release of "Doom 3". John Carmack said in an interview at QuakeCon 2007 that there will be a "Doom 4". It has been in development since May 7, 2008.
"Quake".
On June 22, 1996, the release of "Quake" marked the second milestone in Id Software history. "Quake" combined a cutting edge fully 3D engine with a distinctive art style to create critically acclaimed graphics for its time. Audio was not neglected either, having recruited Nine Inch Nails frontman Trent Reznor to facilitate unique sound effects and ambient music for the game. (A small homage was paid to Nine Inch Nails in the form of the band's logo appearing on an ammunition box.) It also included the work of Michael Abrash. Furthermore, "Quake"'s main innovation—the capability to play a deathmatch (competitive gameplay between living opponents instead of against computer-run characters) over the Internet (especially through the add-on "QuakeWorld") seared the title into the minds of gamers as another smash hit.
In 2008, Id Software was honored at the 59th Annual Technology & Engineering Emmy Awards for the pioneering work "Quake" represented in user modifiable games. Id Software is the only game development company ever honored twice by the National Academy of Television Arts & Sciences, having been given an Emmy Award in 2007 for creation of the 3D technology that underlies modern shooter video games.
The "Quake" series continued with "Quake II" in 1997. However, the game is not a storyline sequel, and instead focuses on an assault on an alien planet, Stroggos, in retaliation for Strogg attacks on Earth. Most of the subsequent entries in the "Quake" franchise follow this storyline. "Quake III Arena" (1999), the next title in the series, has minimal plot, but centers around the "Arena Eternal", a gladiatorial setting created by an alien race known as the Vadrigar and populated by combatants plucked from various points in time and space. Among these combatants are some characters either drawn from or based on those in "Doom" ("Doomguy"), "Quake" (Ranger, Wrack) and "Quake II" (Bitterman, Tank Jr., Grunt, Stripe). "Quake IV" (2005) picks up where "Quake II" left off — finishing the war between the humans and Strogg. The spin-off "" acts as a prequel to "Quake II", when the Strogg first invade Earth. It should be noted that "Quake IV" and "Enemy Territory: Quake Wars" were made by outside developers and not Id.
There have also been a few other spin off games such as Quake Mobile in 2005 and "Quake Live", an internet browser based modification of "Quake III". A game called "Quake Arena DS" is planned for the Nintendo DS. John Carmack stated, at QuakeCon 2007, that the "Id Tech 5" engine would be used for a new "Quake" game.
"Rage".
Todd Hollenshead announced in May 2007 that Id Software had begun working on an all new series that would be using a new engine. Hollenshead also mentioned that the title would be completely developed in-house, marking the first game since 2004's "Doom 3" to be done so. At 2007's WWDC, John Carmack showed the new engine called Id Tech 5. Later that year, at QuakeCon 2007, the title of the new game was revealed as "Rage".
On July 14, 2008, Id Software announced at the 2008 E3 event that they would be publishing "Rage" through Electronic Arts, and not Id's longtime publisher Activision. However, since then Zenimax has also announced that they are publishing "Rage" through Bethesda Softworks.
On August 12, 2010, during Quakecon 2010, Id Software announced "Rage" US ship date of September 13, 2011, and a European ship date of September 15, 2011. During the keynote, Id also demonstrated a "Rage" spin-off title running on the iPhone. This technology demo later became "Rage HD".
Other games.
During its early days, Id Software produced much more varied games; these include the early 3D first person shooter experiments that led to "Wolfenstein 3D" and "Doom" — "Hovertank 3D" and "Catacomb 3D". There was also the "Rescue Rover" series, which had two games — "Rescue Rover" and "Rescue Rover 2". Also there was John Romero's "Dangerous Dave" series, which included such notables as the tech demo ("In Copyright Infringement") which led to the "Commander Keen" engine, and the decently popular "Dangerous Dave in the Haunted Mansion". "In the Haunted Mansion" was powered by the same engine as the earlier Id Software game "Shadow Knights", which was one of the several games written by Id Software to fulfill their contractual obligation to produce games for Softdisk, where the Id Software founders formerly were employed. Id Software has also overseen several games using its technology that were not made in one of their IPs such as "Shadowcaster", (early-Id Tech 1), "Heretic", "Hexen" (Id Tech 1), "Hexen II" ("Quake" engine), and "Orcs and Elves" ("Doom RPG" engine).
Other media.
Id Software has also been associated with novels since the publication of the original "Doom" novels. This has been restarted from 2008 onward with Matthew J. Costello's (a story consultant for "Doom 3" and now "Rage") new "Doom 3" novels: ' and '.
Id Software became involved in film development when they were in the production team of the film adaption of their "Doom" franchise in 2005. In August 2007, Todd Hollenshead stated at QuakeCon 2007 that a "Return to Castle Wolfenstein" movie is in development which re-teams the "Silent Hill" writer/producer team, Roger Avary as writer and director and Samuel Hadida as producer.
Controversy.
Id Software was the target of controversy over two of their most popular games, "Doom" and the earlier "Wolfenstein 3D":
"Doom".
"Doom" was notorious for its high levels of graphic violence and satanic imagery, which generated controversy from a broad range of groups. Yahoo! Games listed it as one of the top ten most controversial games of all time.
The game again sparked controversy throughout a period of school shootings in the United States when it was found that Eric Harris and Dylan Klebold, who committed the Columbine High School massacre in 1999, were avid players of the game. While planning for the massacre, Harris said that the killing would be "like playing "Doom"", and "it'll be like the LA riots, the Oklahoma bombing, WWII, Vietnam, "Duke Nukem" and "Doom" all mixed together", and that his shotgun was "straight out of the game". A rumor spread afterwards that Harris had designed a "Doom" level that looked like the high school, populated with representations of Harris's classmates and teachers, and that Harris practiced for his role in the shootings by playing the level over and over. Although Harris did design "Doom" levels, none of them were based on Columbine High School.
While "Doom" and other violent video games have been blamed for nationally covered school shootings, 2008 research featured by Greater Good Science Center shows that the two are not closely related. Harvard medical school researchers Cheryl Olson and Lawrence Kutner found that violent video games did not correlate to school shootings. The U.S. Secret Service and Department of Education analyzed 37 incidents of school violence and sought to develop a profile of school shooters, they discovered that the most common traits among shooters were that they were male and had histories of depression and attempted suicide. While many of the killers—like the vast majority of young teenage boys—did play video games, this study did not find a relationship between game play and school shootings. In fact, only one eighth of the shooters showed any special interest in violent video games; far less than the number of shooters who seemed attracted to books and movies with violent content.
"Wolfenstein 3D".
As for "Wolfenstein 3D", due to its use of Nazi symbols such as the Swastika and the anthem of the Nazi Party, "Horst-Wessel-Lied", as theme music, the PC version of the game was withdrawn from circulation in Germany in 1994, following a verdict by the Amtsgericht München on January 25, 1994. Despite the fact that Nazis are portrayed as the enemy in "Wolfenstein", the use of those symbols is a federal offense in Germany unless certain circumstances apply. Similarly, the Atari Jaguar version was confiscated following a verdict by the Amtsgericht Berlin Tiergarten on December 7, 1994.
Due to concerns from Nintendo of America, the Super NES version was modified to not include any swastikas or Nazi references; furthermore, blood was replaced with sweat to make the game seem less violent, and the attack dogs in the game were replaced by giant mutant rats. Employees of Id Software are quoted in "The Official DOOM Player Guide" about the reaction to "Wolfenstein", claiming it to be ironic that it was morally acceptable to shoot people and rats, but not dogs. Two new weapons were added as well. The Super NES version was not as successful as the PC version.
People.
In 2003, the book "Masters of Doom" chronicled the development of Id Software, concentrating on the personalities and interaction of John Carmack and John Romero. Below are the key people involved with Id's success.
John Carmack.
Carmack's skill at 3D programming is widely recognized in the software industry and from its inception, he was Id's lead programmer. On the 7th of August 2013 he joined Oculus VR, a company developing a virtual reality head-mounted display, and left id Software on the 22nd of November, 2013.
John Romero.
John Romero, who was forced to resign after the release of "Quake", later formed the ill-fated company Ion Storm. There, he became infamous through the development of "Daikatana", which received generally negative reception from reviewers and gamers alike upon release.
Both Tom Hall and John Romero have reputations as designers and idea men who have helped shape some of the key PC gaming titles of the 1990s.
Tom Hall.
Tom Hall was forced to resign by Id Software during the early days of "Doom" development, but not before he had some impact; for example, he was responsible for the inclusion of teleporters in the game. He was let go before the shareware release of "Doom" and then went to work for Apogee, developing "Rise of the Triad" with the "Developers of Incredible Power". When he finished work on that game, he found he was not compatible with the "Prey" development team at Apogee, and therefore left to join his ex-Id compatriot John Romero at Ion Storm. Hall has frequently commented that if he could obtain the rights to "Commander Keen", he would immediately develop another Keen title.
Sandy Petersen.
Sandy Petersen was a level designer for 19 of the 27 levels in the original "Doom" title as well as 17 of the 32 levels of "Doom II". As a fan of H.P. Lovecraft, his influence is apparent in the Lovecraftian feel of the monsters for "Quake", and he created "Inferno", the third "episode" of the first DOOM. He left Id Software during the production of "Quake II" and most of his work was scrapped before the title was released.
American McGee.
American McGee was a level designer for "Doom II", "The Ultimate Doom", "Quake", and "Quake II". He was asked to resign after the release of "Quake II", then moved to Electronic Arts where he gained industry notoriety with the development of his own game "American McGee's Alice". After leaving Electronic Arts, he became an independent entrepreneur and game developer. McGee now heads independent development house Spicy Horse in Shanghai, where he works on various projects.

</doc>
<doc id="15531" url="http://en.wikipedia.org/wiki?curid=15531" title="Isaac Stern">
Isaac Stern

Isaac Stern (Ukrainian: Ісаак Штерн, Russian: Исаа́к Штерн; 21 July 1920 – 22 September 2001) was an American violinist and conductor. 
Biography.
The son of Solomon and Clara Stern, Isaac Stern was born into a Volhynian-Jewish family in Kremenets (Krzemieniec), then in the Soviet Ukraine (the year after his birth it again became part of Poland). He was fourteen months old when his family moved to San Francisco. He received his first music lessons from his mother. In 1928, he enrolled at the San Francisco Conservatory of Music, where he studied until 1931 before going on to study privately with Louis Persinger. He returned to the San Francisco Conservatory to study for five years with Naoum Blinder, to whom he said he owed the most. At his public début on 18 February 1936, aged 15, he played Saint-Saëns' Violin Concerto No. 3 in B minor with the San Francisco Symphony under the direction of Pierre Monteux. Reflecting on his background, Stern once memorably quipped that cultural exchanges between the US and Soviet Russia were simple affairs:
Stern toured the Soviet Union in 1951, the first American violinist to do so. In 1967, Stern stated his refusal to return to the USSR until the Soviet regime allowed artists to enter and leave the country freely. His only visit to Germany was in 1999, for a series of master classes, but he never performed publicly in Germany.
Stern was married three times. His first marriage, in 1948 to ballerina Nora Kaye, ended in divorce after 18 months, but the two of them subsequently remained friends. On 17 August 1951, he married Vera Lindenblit. They had three children together, including the conductor Michael Stern. Their marriage ended in divorce in 1994 after 43 years. In 1996, Stern married his third wife, Linda Reynolds. His third wife, his three children, and five grandchildren survived him.
Music career.
In 1940, Stern began performing with Russian-born pianist Alexander Zakin, collaborating until 1977. Within musical circles, Stern became renowned both for his recordings and for championing certain younger players. Among his discoveries were cellists Yo-Yo Ma and Jian Wang, and violinists Itzhak Perlman and Pinchas Zukerman. In the 1960s, he also played a major role in saving New York City's Carnegie Hall from demolition, by organising the Citizens' Committee to Save Carnegie Hall. Following the purchase of Carnegie Hall by New York City, the Carnegie Hall Corporation was formed, and Stern was chosen as its first president, a title he held until his death. Carnegie Hall later named its main auditorium in his honor.
Among Stern's many recordings are concertos by Brahms, Bach, Beethoven, Mendelssohn, Tchaikovsky, and Vivaldi and modern works by Barber, Bartók, Stravinsky, Bernstein, Rochberg, and Dutilleux. The Dutilleux concerto, entitled "L'arbre des songes" ["The Tree of Dreams"] was a 1985 commission by Stern himself. He also dubbed actors' violin-playing in several films, such as "Fiddler on the Roof".
Stern served as musical advisor for the 1946 film, "Humoresque", about a rising violin star and his patron, played respectively by John Garfield and Joan Crawford. He was also the featured violin soloist on the soundtrack for the 1971 film of Fiddler on the Roof. In 1999, he appeared in the film "Music of the Heart", along with Itzhak Perlman and several other famed violinists, with a youth orchestra led by Meryl Streep (the film was based on the true story of a gifted violin teacher in Harlem who eventually took her musicians to play a concert in Carnegie Hall).
In his autobiography, co-authored with Chaim Potok, "My First 79 Years", Stern cited Nathan Milstein and Arthur Grumiaux as major influences on his style of playing.
He won Grammys for his work with Eugene Istomin and Leonard Rose in their famous chamber music trio in the 1960s and '70s, while also continuing his duo work with Alexander Zakin during this time. Stern recorded a series of piano quartets in the 1980s and 1990s with Emanuel Ax, Jaime Laredo and Yo-Yo Ma, including those of Mozart, Beethoven, Schumann and Fauré, winning another Grammy in 1992 for the Brahms quartets Opp. 25 and 26.
In 1979, seven years after Richard Nixon made the first official visit by a US President to the country, the People's Republic of China offered Stern and pianist David Golub an unprecedented invitation to tour the country. While there, he collaborated with the China Central Symphony Society (now China National Symphony) under the direction of conductor Li Delun. Their visit was filmed and resulted in the Oscar-winning documentary, "".
Ties to Israel.
Stern maintained close ties with Israel. Stern began performing in the country in 1949. In 1973, he performed for wounded Israeli soldiers during the Yom Kippur War. During the 1991 Gulf War and Iraq's Scud missile attacks on Israel, he played in the Jerusalem Theater. During his performance, an air raid siren sounded, causing the audience to panic. Stern then stepped onto the stage and began playing a movement of Bach. The audience then calmed down, donned gas masks, and sat throughout the rest of his performance. Stern was a supporter of several educational projects in Israel, among them the America-Israel Foundation and the Jerusalem Music Center.
Instruments.
Stern's favorite instrument was the Ysaÿe Guarnerius, one of the violins produced by the Cremonese luthier Giuseppe Guarneri del Gesù. It had previously been played by the violin virtuoso and composer Eugène Ysaÿe.
Among other instruments, Stern played the "Kruse-Vormbaum" Stradivarius (1728), the "ex-Stern" Bergonzi (1733), the "Stern-Alard" Guarneri del Gesù (1737), a Michele Angelo Bergonzi (1739–1757), the "Arma Senkrah" Guadagnini (1750), a Giovanni Guadagnini (1754), a J. B. Vuillaume copy of the "Panette" Guarneri del Gesu of 1737 (c.1850), and the "ex-Nicolas I" J.B. Vuillaume (1840). He also owned two contemporary instruments by Samuel Zygmuntowicz.
In 2001, Stern's collection of instruments, bows and musical ephemera was sold through Tarisio Auctions. The May 2003 auction set a number of world records and was at the time the second highest grossing violin auction of all time, with total sales of over $3.3M.
Awards and commemoration.
In 2012, a street in Tel Aviv was named for Stern.
Discography.
 

</doc>
<doc id="15532" url="http://en.wikipedia.org/wiki?curid=15532" title="Integral">
Integral

The integral is an important concept in mathematics. Integration is one of the two main operations in calculus, with its inverse, differentiation, being the other. Given a function f of a real variable x and an interval ["a", "b"] of the real line, the definite integral
is defined informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines and . The area above the x-axis adds to the total and that below the x-axis subtracts from the total.
Roughly speaking, the operation of integration is the reverse of differentiation. For this reason, the term "integral" may also refer to the related notion of the antiderivative, a function F whose derivative is the given function f. In this case, it is called an "indefinite integral" and is written:
The integrals discussed in this article are those termed "definite integrals". It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval ["a", "b"], then, once an antiderivative F of f is known, the definite integral of over that interval is given by
The principles of integration were formulated independently by Isaac Newton and Gottfried Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. A rigorous mathematical definition of the integral was given by Bernhard Riemann. It is based on a limiting procedure which approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or three variables, and the interval of integration ["a", "b"] is replaced by a certain curve connecting two points on the plane or in the space. In a surface integral, the curve is replaced by a piece of a surface in the three-dimensional space.
Integrals of differential forms play a fundamental role in modern differential geometry. These generalizations of integrals first arose from the needs of physics, and they play an important role in the formulation of many physical laws, notably those of electrodynamics. There are many modern concepts of integration, among these, the most common is based on the abstract mathematical theory known as Lebesgue integration, developed by Henri Lebesgue.
History.
Pre-calculus integration.
The first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient Greek astronomer Eudoxus ("ca." 370 BC), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known. This method was further developed and employed by Archimedes in the 3rd century BC and used to calculate areas for parabolas and an approximation to the area of a circle. Similar methods were independently developed in China around the 3rd century AD by Liu Hui, who used it to find the area of the circle. This method was later used in the 5th century by Chinese father-and-son mathematicians Zu Chongzhi and Zu Geng to find the volume of a sphere (; ).
The next significant advances in integral calculus did not begin to appear until the 16th century. At this time the work of Cavalieri with his "method of indivisibles", and work by Fermat, began to lay the foundations of modern calculus, with Cavalieri computing the integrals of "x""n" up to degree in Cavalieri's quadrature formula. Further steps were made in the early 17th century by Barrow and Torricelli, who provided the first hints of a connection between integration and differentiation. Barrow provided the first proof of the fundamental theorem of calculus. Wallis generalized Cavalieri's method, computing integrals of x to a general power, including negative powers and fractional powers.
Newton and Leibniz.
The major advance in integration came in the 17th century with the independent discovery of the fundamental theorem of calculus by Newton and Leibniz. The theorem demonstrates a connection between integration and differentiation. This connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. In particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. Equal in importance is the comprehensive mathematical framework that both Newton and Leibniz developed. Given the name infinitesimal calculus, it allowed for precise analysis of functions within continuous domains. This framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of Leibniz.
Formalization.
While Newton and Leibniz provided a systematic approach to integration, their work lacked a degree of rigour. Bishop Berkeley memorably attacked the vanishing increments used by Newton, calling them "ghosts of departed quantities". Calculus acquired a firmer footing with the development of limits. Integration was first rigorously formalized, using limits, by Riemann. Although all bounded piecewise continuous functions are Riemann integrable on a bounded interval, subsequently more general functions were considered—particularly in the context of Fourier analysis—to which Riemann's definition does not apply, and Lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). Other definitions of integral, extending Riemann's and Lebesgue's approaches, were proposed. These approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite Riemann sum, based on the hyperreal number system.
Historical notation.
Isaac Newton used a small vertical bar above a variable to indicate integration, or placed the variable inside a box. The vertical bar was easily confused with ."x" or "x"′, which Newton used to indicate differentiation, and the box notation was difficult for printers to reproduce, so these notations were not widely adopted.
The modern notation for the indefinite integral was introduced by Gottfried Leibniz in 1675 (; ). He adapted the integral symbol, ∫, from the letter "ſ" (long s), standing for "summa" (written as "ſumma"; Latin for "sum" or "total"). The modern notation for the definite integral, with limits above and below the integral sign, was first used by Joseph Fourier in "Mémoires" of the French Academy around 1819–20, reprinted in his book of 1822 (; ).
Terminology and notation.
The simplest case, the integral with respect to x of a real-valued function "f"("x"), is written as
The integral sign ∫ represents integration. The symbol dx (explained below) indicates the variable of integration, x. The function "f"("x") which is to be integrated is called the "integrand". In correct mathematical typography, the dx is separated from the integrand by a space (as shown). Some authors use an upright "d" (that is, d"x" instead of "dx"). Also, some authors place the symbol dx before "f"("x") rather than after it. Because there is no domain specified, the above integral is called an "indefinite integral".
When integrating over a specified domain, we speak of a "definite integral". Integrating over a domain D is written as formula_5. If "D" is an interval ["a", "b"] of the real line, the integral is usually written formula_6. The domain D or the interval ["a", "b"] is called the "domain of integration".
If a function has an integral, it is said to be "integrable". In general, the integrand may be a function of more than one variable, and the domain of integration may be an area, volume, a higher-dimensional region, or even an abstract space that does not have a geometric structure in any usual sense (such as a sample space in probability theory).
In modern Arabic mathematical notation, a reflected integral symbol is used .
The symbol dx has different interpretations depending on the theory being used. In Leibniz's notation, dx is interpreted as an infinitesimal change in x. Although Leibniz's interpretation lacks rigour, his integration notation is the most common one in use today. If the underlying theory of integration is not important, dx can be seen as strictly a notation indicating that x is a dummy variable of integration; if the integral is seen as a Riemann integral, dx indicates that the sum is over subintervals in the domain of x; in a Riemann–Stieltjes integral, it indicates the weight applied to a subinterval in the sum; in Lebesgue integration and its extensions, dx is a measure, a type of function which assigns sizes to sets; in non-standard analysis, it is an infinitesimal; and in the theory of differentiable manifolds, it is often a differential form, a quantity which assigns numbers to tangent vectors. Depending on the situation, the notation may vary slightly to capture the important features of the situation. For instance, when integrating a variable x with respect to a measure μ, the notation dμ("x") is sometimes used to emphasize the dependence on "x".
Introduction.
Integrals appear in many practical situations. If a swimming pool is rectangular with a flat bottom, then from its length, width, and depth we can easily determine the volume of water it can contain (to fill it), the area of its surface (to cover it), and the length of its edge (to rope it). But if it is oval with a rounded bottom, all of these quantities call for integrals. Practical approximations may suffice for such trivial examples, but precision engineering (of any discipline) requires exact and rigorous values for these elements.
To start off, consider the curve between and with . We ask:
and call this (yet unknown) area the integral of f. The notation for this integral will be
As a first approximation, look at the unit square given by the sides to and and . Its area is exactly 1. As it is, the true value of the integral must be somewhat less. Decreasing the width of the approximation rectangles shall give a better result; so cross the interval in five steps, using the approximation points 0, 1/5, 2/5, and so on to 1. Fit a box for each step using the right end height of each curve piece, thus √1/5, √2/5, and so on to √1 = 1. Summing the areas of these rectangles, we get a better approximation for the sought integral, namely
We are taking a sum of finitely many function values of f, multiplied with the differences of two subsequent approximation points. We can easily see that the approximation is still too large. Using more steps produces a closer approximation, but will never be exact: replacing the 5 subintervals by twelve in the same way, but with the left end height of each piece, we will get an approximate value for the area of 0.6203, which is too small. The key idea is the transition from adding "finitely many" differences of approximation points multiplied by their respective function values to using infinitely many fine, or "infinitesimal" steps.
As for the "actual calculation of integrals", the fundamental theorem of calculus, due to Newton and Leibniz, is the fundamental link between the operations of differentiating and integrating. Applied to the square root curve, , it says to look at the antiderivative , and simply take "F"(1) − "F"(0), where 0 and 1 are the boundaries of the interval [0, 1]. So the "exact" value of the area under the curve is computed formally as
The notation
conceives the integral as a weighted sum, denoted by the elongated s, of function values, "f"("x"), multiplied by infinitesimal step widths, the so-called "differentials", denoted by dx. The multiplication sign is usually omitted.
Historically, after the failure of early efforts to rigorously interpret infinitesimals, Riemann formally defined integrals as a limit of weighted sums, so that the dx suggested the limit of a difference (namely, the interval width). Shortcomings of Riemann's dependence on intervals and continuity motivated newer definitions, especially the Lebesgue integral, which is founded on an ability to extend the idea of "measure" in much more flexible ways. Thus the notation
refers to a weighted sum in which the function values are partitioned, with μ measuring the weight to be assigned to each value. Here A denotes the region of integration.
Differential geometry, with its "calculus on manifolds", gives the familiar notation yet another interpretation. Now "f"("x") and dx become a differential form, , a new differential operator "d", known as the exterior derivative is introduced, and the fundamental theorem becomes the more general Stokes' theorem,
from which Green's theorem, the divergence theorem, and the fundamental theorem of calculus follow.
More recently, infinitesimals have reappeared with rigor, through modern innovations such as non-standard analysis. Not only do these methods vindicate the intuitions of the pioneers; they also lead to new mathematics.
Although there are differences between these conceptions of integral, there is considerable overlap. Thus, the area of the surface of the oval swimming pool can be handled as a geometric ellipse, a sum of infinitesimals, a Riemann integral, a Lebesgue integral, or as a manifold with a differential form. The calculated result will be the same for all.
Formal definitions.
There are many ways of formally defining an integral, not all of which are equivalent. The differences exist mostly to deal with differing special cases which may not be integrable under other definitions, but also occasionally for pedagogical reasons. The most commonly used definitions of integral are Riemann integrals and Lebesgue integrals.
Riemann integral.
The Riemann integral is defined in terms of Riemann sums of functions with respect to "tagged partitions" of an interval. Let ["a", "b"] be a closed interval of the real line; then a "tagged partition" of ["a", "b"] is a finite sequence
This partitions the interval ["a", "b"] into n sub-intervals ["x""i"−1, "x""i"] indexed by i, each of which is "tagged" with a distinguished point "t""i" ∈ ["x""i"−1, "x""i"]. A "Riemann sum" of a function f with respect to such a tagged partition is defined as
thus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let be the width of sub-interval i; then the "mesh" of such a tagged partition is the width of the largest sub-interval formed by the partition, . The "Riemann integral" of a function f over the interval ["a", "b"] is equal to S if:
When the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.
Lebesgue integral.
It is often of interest, both in theory and applications, to be able to pass to the limit under the integral. For instance, a sequence of functions can frequently be constructed that approximate, in a suitable sense, the solution to a problem. Then the integral of the solution function should be the limit of the integrals of the approximations. However, many functions that can be obtained as limits are not Riemann integrable, and so such limit theorems do not hold with the Riemann integral. Therefore it is of great importance to have a definition of the integral that allows a wider class of functions to be integrated .
Such an integral is the Lebesgue integral, that exploits the following fact to enlarge the class of integrable functions: if the values of a function are rearranged over the domain, the integral of a function should remain the same. Thus Henri Lebesgue introduced the integral bearing his name, explaining this integral thus in a letter to Paul Montel:
I have to pay a certain sum, which I have collected in my pocket. I take the bills and coins out of my pocket and give them to the creditor in the order I find them until I have reached the total sum. This is the Riemann integral. But I can proceed differently. After I have taken all the money out of my pocket I order the bills and coins according to identical values and then I pay the several heaps one after the other to the creditor. This is my integral.
As puts it, "To compute the Riemann integral of f, one partitions the domain ["a", "b"] into subintervals", while in the Lebesgue integral, "one is in effect partitioning the range of f". The definition of the Lebesgue integral thus begins with a measure, μ. In the simplest case, the Lebesgue measure "μ"("A") of an interval is its width, "b" − "a", so that the Lebesgue integral agrees with the (proper) Riemann integral when both exist. In more complicated cases, the sets being measured can be highly fragmented, with no continuity and no resemblance to intervals.
Using the "partitioning the range of f" philosophy, the integral of a non-negative function "f" : R → R should be the sum over t of the areas between a thin horizontal strip between "y" = "t" and "y" = "t" + "dt". This area is just "μ"{ "x" : "f"("x") > "t"} "dt". Let "f"∗("t") = "μ"{ "x" : "f"("x") > "t"}. The Lebesgue integral of f is then defined by 
where the integral on the right is an ordinary improper Riemann integral ("f"∗ is a strictly decreasing positive function, and therefore has a well-defined improper Riemann integral). For a suitable class of functions (the measurable functions) this defines the Lebesgue integral.
A general measurable function f is Lebesgue integrable if the area between the graph of f and the x-axis is finite:
In that case, the integral is, as in the Riemannian case, the difference between the area above the x-axis and the area below the x-axis:
where
Other integrals.
Although the Riemann and Lebesgue integrals are the most widely used definitions of the integral, a number of others exist, including:
Properties.
Linearity.
The collection of Riemann integrable functions on a closed interval ["a", "b"] forms a vector space under the operations of pointwise addition and multiplication by a scalar, and the operation of integration
is a linear functional on this vector space. Thus, firstly, the collection of integrable functions is closed under taking linear combinations; and, secondly, the integral of a linear combination is the linear combination of the integrals,
Similarly, the set of real-valued Lebesgue integrable functions on a given measure space E with measure μ is closed under taking linear combinations and hence form a vector space, and the Lebesgue integral
is a linear functional on this vector space, so that
More generally, consider the vector space of all measurable functions on a measure space ("E","μ"), taking values in a locally compact complete topological vector space V over a locally compact topological field "K", "f" : "E" → "V". Then one may define an abstract integration map assigning to each function f an element of V or the symbol "∞",
that is compatible with linear combinations. In this situation the linearity holds for the subspace of functions whose integral is an element of V (i.e. "finite"). The most important special cases arise when K is R, C, or a finite extension of the field Q"p" of p-adic numbers, and V is a finite-dimensional vector space over K, and when and V is a complex Hilbert space.
Linearity, together with some natural continuity properties and normalisation for a certain class of "simple" functions, may be used to give an alternative definition of the integral. This is the approach of Daniell for the case of real-valued functions on a set X, generalized by Nicolas Bourbaki to functions with values in a locally compact topological vector space. See for an axiomatic characterisation of the integral.
Inequalities.
A number of general inequalities hold for Riemann-integrable functions defined on a closed and bounded interval ["a", "b"] and can be generalized to other notions of integral (Lebesgue and Daniell).
Conventions.
In this section f is a real-valued Riemann-integrable function. The integral
over an interval ["a", "b"] is defined if "a" < "b". This means that the upper and lower sums of the function f are evaluated on a partition whose values "x""i" are increasing. Geometrically, this signifies that integration takes place "left to right", evaluating f within intervals ["x" "i" , "x" "i" +1] where an interval with a higher index lies to the right of one with a lower index. The values a and b, the end-points of the interval, are called the limits of integration of f. Integrals can also be defined if "a" > "b":
This, with , implies:
The first convention is necessary in consideration of taking integrals over subintervals of ["a", "b"]; the second says that an integral taken over a degenerate interval, or a point, should be zero. One reason for the first convention is that the integrability of f on an interval ["a", "b"] implies that f is integrable on any subinterval ["c", "d"], but in particular integrals have the property that:
With the first convention the resulting relation
is then well-defined for any cyclic permutation of a, b, and c.
Instead of viewing the above as conventions, one can also adopt the point of view that integration is performed of differential forms on "oriented" manifolds only. If M is such an oriented m-dimensional manifold, and "M"′ is the same manifold with opposed orientation and ω is an m-form, then one has:
These conventions correspond to interpreting the integrand as a differential form, integrated over a chain. In measure theory, by contrast, one interprets the integrand as a function f with respect to a measure μ and integrates over a subset A, without any notion of orientation; one writes formula_40 to indicate integration over a subset A. This is a minor distinction in one dimension, but becomes subtler on higher-dimensional manifolds; see Differential form: Relation with measures for details.
Fundamental theorem of calculus.
The "fundamental theorem of calculus" is the statement that differentiation and integration are inverse operations: if a continuous function is first integrated and then differentiated, the original function is retrieved. An important consequence, sometimes called the "second fundamental theorem of calculus", allows one to compute integrals by using an antiderivative of the function to be integrated.
Statements of theorems.
Fundamental theorem of calculus.
Let f be a continuous real-valued function defined on a closed interval ["a", "b"]. Let F be the function defined, for all x in ["a", "b"], by
Then, F is continuous on ["a", "b"], differentiable on the open interval ("a", "b"), and
for all x in ("a", "b").
Second fundamental theorem of calculus.
Let f be a real-valued function defined on a closed interval ["a", "b"] that admits an antiderivative F on ["a", "b"]. That is, f and F are functions such that for all x in ["a", "b"],
If f is integrable on ["a", "b"] then
Extensions.
Improper integrals.
A "proper" Riemann integral assumes the integrand is defined and finite on a closed and bounded interval, bracketed by the limits of integration. An improper integral occurs when one or more of these conditions is not satisfied. In some cases such integrals may be defined by considering the limit of a sequence of proper Riemann integrals on progressively larger intervals.
If the interval is unbounded, for instance at its upper end, then the improper integral is the limit as that endpoint goes to infinity.
If the integrand is only defined or finite on a half-open interval, for instance ("a", "b"], then again a limit may provide a finite result.
That is, the improper integral is the limit of proper integrals as one endpoint of the interval of integration approaches either a specified real number, or ∞, or −∞. In more complicated cases, limits are required at both endpoints, or at interior points.
Consider, for example, the function 1/(("x" + 1)√"x") integrated from 0 to ∞ (shown right). At the lower bound, as x goes to 0 the function goes to ∞, and the upper bound is itself ∞, though the function goes to 0. Thus this is a doubly improper integral. Integrated, say, from 1 to 3, an ordinary Riemann sum suffices to produce a result of π/6. To integrate from 1 to ∞, a Riemann sum is not possible. However, any finite upper bound, say t (with "t" > 1), gives a well-defined result, 2 arctan(√"t") − π/2. This has a finite limit as t goes to infinity, namely π/2. Similarly, the integral from 1/3 to 1 allows a Riemann sum as well, coincidentally again producing π/6. Replacing 1/3 by an arbitrary positive value s (with "s" < 1) is equally safe, giving π/2 − 2 arctan(√"s"). This, too, has a finite limit as s goes to zero, namely π/2. Combining the limits of the two fragments, the result of this improper integral is
This process does not guarantee success; a limit might fail to exist, or might be unbounded. For example, over the bounded interval from 0 to 1 the integral of 1/x does not converge; and over the unbounded interval from 1 to ∞ the integral of 1/√"x" does not converge.
It might also happen that an integrand is unbounded at an interior point, in which case the integral must be split at that point. For the integral as a whole to converge, the limit integrals on both sides must exist and must be bounded. For example:
But the similar integral
cannot be assigned a value in this way, as the integrals above and below zero do not independently converge. (However, see Cauchy principal value.)
Multiple integration.
Integrals can be taken over regions other than intervals. In general, an integral over a set E of a function f is written:
Here x need not be a real number, but can be another suitable quantity, for instance, a vector in R3. Fubini's theorem shows that such integrals can be rewritten as an "iterated integral". In other words, the integral can be calculated by integrating one coordinate at a time.
Just as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the "x"-axis, the "double integral" of a positive function of two variables represents the volume of the region between the surface defined by the function and the plane which contains its domain. (The same volume can be obtained via the "triple integral" — the integral of a function in three variables — of the constant function "f"("x", "y", "z") = 1 over the above-mentioned region between the surface and the plane.) If the number of variables is higher, then the integral represents a hypervolume, a volume of a solid of more than three dimensions that cannot be graphed.
For example, the volume of the cuboid of sides 4 × 6 × 5 may be obtained in two ways:
Line integrals.
The concept of an integral can be extended to more general domains of integration, such as curved lines and surfaces. Such integrals are known as line integrals and surface integrals respectively. These have important applications in physics, as when dealing with vector fields.
A "line integral" (sometimes called a "path integral") is an integral where the function to be integrated is evaluated along a curve. Various different line integrals are in use. In the case of a closed curve it is also called a "contour integral".
The function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulas in physics have natural continuous analogs in terms of line integrals; for example, the fact that work is equal to force, F, multiplied by displacement, s, may be expressed (in terms of vector quantities) as:
For an object moving along a path "C" in a vector field F such as an electric field or gravitational field, the total work done by the field on the object is obtained by summing up the differential work done in moving from s to s + "d"s. This gives the line integral
Surface integrals.
A "surface integral" is a definite integral taken over a surface (which may be a curved set in space); it can be thought of as the double integral analog of the line integral. The function to be integrated may be a scalar field or a vector field. The value of the surface integral is the sum of the field at all points on the surface. This can be achieved by splitting the surface into surface elements, which provide the partitioning for Riemann sums.
For an example of applications of surface integrals, consider a vector field v on a surface "S"; that is, for each point "x" in "S", v("x") is a vector. Imagine that we have a fluid flowing through "S", such that v("x") determines the velocity of the fluid at x. The flux is defined as the quantity of fluid flowing through "S" in unit amount of time. To find the flux, we need to take the dot product of v with the unit surface normal to "S" at each point, which will give us a scalar field, which we integrate over the surface:
The fluid flux in this example may be from a physical fluid such as water or air, or from electrical or magnetic flux. Thus surface integrals have applications in physics, particularly with the classical theory of electromagnetism.
Integrals of differential forms.
A differential form is a mathematical concept in the fields of multivariable calculus, differential topology and tensors. The modern notation for the differential form, as well as the idea of the differential forms as being the wedge products of exterior derivatives forming an exterior algebra, was introduced by Élie Cartan.
We initially work in an open set in R"n".
A 0-form is defined to be a smooth function f.
When we integrate a function f over an m-dimensional subspace "S" of R"n", we write it as
(The superscripts are indices, not exponents.) We can consider "dx"1 through "dx""n" to be formal objects themselves, rather than tags appended to make integrals look like Riemann sums. Alternatively, we can view them as covectors, and thus a measure of "density" (hence integrable in a general sense). We call the "dx"1, …, "dxn" "basic" 1-"forms".
We define the wedge product, "∧", a bilinear "multiplication" operator on these elements, with the "alternating" property that
for all indices a. Alternation along with linearity and associativity implies "dx""b" ∧ "dx""a" = −"dx""a" ∧ "dx""b". This also ensures that the result of the wedge product has an orientation.
We define the set of all these products to be "basic" 2-"forms", and similarly we define the set of products of the form "dx""a" ∧ "dx""b" ∧ "dx""c" to be "basic" 3-"forms". A general "k"-form is then a weighted sum of basic "k-"forms, where the weights are the smooth functions f. Together these form a vector space with basic "k"-forms as the basis vectors, and 0-forms (smooth functions) as the field of scalars. The wedge product then extends to "k"-forms in the natural way. Over R"n" at most n covectors can be linearly independent, thus a "k-"form with "k" > "n" will always be zero, by the alternating property.
In addition to the wedge product, there is also the exterior derivative operator "d". This operator maps "k"-forms to ("k"+1)-forms. For a "k"-form over R"n", we define the action of "d" by:
with extension to general "k"-forms occurring linearly.
This more general approach allows for a more natural coordinate-free approach to integration on manifolds. It also allows for a natural generalisation of the fundamental theorem of calculus, called Stokes' theorem, which we may state as
where ω is a general "k"-form, and ∂Ω denotes the boundary of the region Ω. Thus, in the case that ω is a 0-form and Ω is a closed interval of the real line, this reduces to the fundamental theorem of calculus. In the case that ω is a 1-form and Ω is a two-dimensional region in the plane, the theorem reduces to Green's theorem. Similarly, using 2-forms, and 3-forms and Hodge duality, we can arrive at Stokes' theorem and the divergence theorem. In this way we can see that differential forms provide a powerful unifying view of integration.
Summations.
The discrete equivalent of integration is summation. Summations and integrals can be put on the same foundations using the theory of Lebesgue integrals or time scale calculus.
Computation.
Analytical.
The most basic technique for computing definite integrals of one real variable is based on the fundamental theorem of calculus. Let "f"("x") be the function of x to be integrated over a given interval ["a", "b"]. Then, find an antiderivative of f; that is, a function F such that on the interval. Provided the integrand and integral have no singularities on the path of integration, by the fundamental theorem of calculus,
The integral is not actually the antiderivative, but the fundamental theorem provides a way to use antiderivatives to evaluate definite integrals.
The most difficult step is usually to find the antiderivative of f. It is rarely possible to glance at a function and write down its antiderivative. More often, it is necessary to use one of the many techniques that have been developed to evaluate integrals. Most of these techniques rewrite one integral as a different one which is hopefully more tractable. Techniques include:
Alternative methods exist to compute more complex integrals. Many nonelementary integrals can be expanded in a Taylor series and integrated term by term. Occasionally, the resulting infinite series can be summed analytically. The method of convolution using Meijer G-functions can also be used, assuming that the integrand can be written as a product of Meijer G-functions. There are also many less common ways of calculating definite integrals; for instance, Parseval's identity can be used to transform an integral over a rectangular region into an infinite sum. Occasionally, an integral can be evaluated by a trick; for an example of this, see Gaussian integral.
Computations of volumes of solids of revolution can usually be done with disk integration or shell integration.
Specific results which have been worked out by various techniques are collected in the list of integrals.
Symbolic.
Many problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma.
A major mathematical difficulty in symbolic integration is that in many cases, a closed formula for the antiderivative of a rather simple-looking function does not exist. For instance, it is known that the antiderivatives of the functions exp("x"2), "x""x" and (sin "x")/"x" cannot be expressed in the closed form involving only rational and exponential functions, logarithm, trigonometric and inverse trigonometric functions, and the operations of multiplication and composition; in other words, none of the three given functions is integrable in elementary functions, which are the functions which may be built from rational functions, roots of a polynomial, logarithm, and exponential functions. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary, and, if it is, to compute it. Unfortunately, it turns out that functions with closed expressions of antiderivatives are the exception rather than the rule. Consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may be still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition, and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.
Some special integrands occur often enough to warrant special study. In particular, it may be useful to have, in the set of antiderivatives, the special functions of physics (like the Legendre functions, the hypergeometric function, the Gamma function, the Incomplete Gamma function and so on — see Symbolic integration for more details). Extending the Risch's algorithm to include such functions is possible but challenging and has been an active research subject.
More recently a new approach has emerged, using "D"-finite function, which are the solutions of linear differential equations with polynomial coefficients. Most of the elementary and special functions are "D"-finite and the integral of a "D"-finite function is also a "D"-finite function. This provide an algorithm to express the antiderivative of a "D"-finite function as the solution of a differential equation.
This theory allows also to compute a definite integrals of a "D"-function as the sum of a series given by the first coefficients and an algorithm to compute any coefficient.
Numerical.
The integrals encountered in a basic calculus course are deliberately chosen for simplicity; those found in real applications are not always so accommodating. Some integrals cannot be found exactly, some require special functions which themselves are a challenge to compute, and others are so complex that finding the exact answer is too slow. This motivates the study and application of numerical methods for approximating integrals, which today use floating-point arithmetic on digital electronic computers. Many of the ideas arose much earlier, for hand calculations; but the speed of general-purpose computers like the ENIAC created a need for improvements.
The goals of numerical integration are accuracy, reliability, efficiency, and generality. Sophisticated methods can vastly outperform a naive method by all four measures (; ; ). Consider, for example, the integral
which has the exact answer . (In ordinary practice the answer is not known in advance, so an important task — not explored here — is to decide when an approximation is good enough.) A “calculus book” approach divides the integration range into, say, 16 equal pieces, and computes function values.
Using the left end of each piece, the rectangle method sums 16 function values and multiplies by the step width, h, here 0.25, to get an approximate value of 3.94325 for the integral. The accuracy is not impressive, but calculus formally uses pieces of infinitesimal width, so initially this may seem little cause for concern. Indeed, repeatedly doubling the number of steps eventually produces an approximation of 3.76001. However, 218 pieces are required, a great computational expense for such little accuracy; and a reach for greater accuracy can force steps so small that arithmetic precision becomes an obstacle.
A better approach replaces the horizontal tops of the rectangles with slanted tops touching the function at the ends of each piece. This trapezium rule is almost as easy to calculate; it sums all 17 function values, but weights the first and last by one half, and again multiplies by the step width. This immediately improves the approximation to 3.76925, which is noticeably more accurate. Furthermore, only 210 pieces are needed to achieve 3.76000, substantially less computation than the rectangle method for comparable accuracy.
Romberg's method builds on the trapezoid method to great effect. First, the step lengths are halved incrementally, giving trapezoid approximations denoted by "T"("h"0), "T"("h"1), and so on, where "h""k"+1 is half of "h""k". For each new step size, only half the new function values need to be computed; the others carry over from the previous size (as shown in the table above). But the really powerful idea is to interpolate a polynomial through the approximations, and extrapolate to "T"(0). With this method a numerically "exact" answer here requires only four pieces (five function values). The Lagrange polynomial interpolating {(4.00,6.128), (2.00,4.352), (1.00,3.908)} is 3.76 + 0.148"h"2, producing the extrapolated value 3.76 at .
Gaussian quadrature often requires noticeably less work for superior accuracy. In this example, it can compute the function values at just two x positions, ±2 ⁄ √3, then double each value and sum to get the numerically exact answer. The explanation for this dramatic success lies in error analysis, and a little luck. An n-point Gaussian method is exact for polynomials of degree up to 2"n" − 1. The function in this example is a degree 3 polynomial, plus a term that cancels because the chosen endpoints are symmetric around zero. (Cancellation also benefits the Romberg method.)
Shifting the range left a little, so the integral is from −2.25 to 1.75, removes the symmetry. Nevertheless, the trapezoid method is rather slow, the polynomial interpolation method of Romberg is acceptable, and the Gaussian method requires the least work — if the number of points is known in advance. As well, rational interpolation can use the same trapezoid evaluations as the Romberg method to greater effect.
In practice, each method must use extra evaluations to ensure an error bound on an unknown function; this tends to offset some of the advantage of the pure Gaussian method, and motivates the popular Gauss–Kronrod quadrature formulae. Symmetry can still be exploited by splitting this integral into two ranges, from −2.25 to −1.75 (no symmetry), and from −1.75 to 1.75 (symmetry). More broadly, adaptive quadrature partitions a range into pieces based on function properties, so that data points are concentrated where they are needed most.
Simpson's rule, named for Thomas Simpson (1710–1761), uses a parabolic curve to approximate integrals. In many cases, it is more accurate than the trapezoidal rule and others. The rule states that
with an error of
The computation of higher-dimensional integrals (for example, volume calculations) makes important use of such alternatives as Monte Carlo integration.
A calculus text is no substitute for numerical analysis, but the reverse is also true. Even the best adaptive numerical code sometimes requires a user to help with the more demanding integrals. For example, improper integrals may require a change of variable or methods that can avoid infinite function values, and known properties like symmetry and periodicity may provide critical leverage.
Mechanical.
The area of an arbitrary two-dimensional shape can be determined using a measuring instrument called planimeter. The volume of irregular objects can be measured with precision by the fluid displaced as the object is submerged.
Geometrical.
Area can be found via geometrical compass-and-straightedge constructions of an equivalent square, e.g., squaring the circle.
Some important definite integrals.
Mathematicians have used definite integrals as a tool to define identities. Among these identities is the definition of the Euler–Mascheroni constant:
the Gamma function:
the Fourier transform which is widely used in physics:
Laplace transform which is widely used in engineering:
and the Gaussian Integral fundamental to the Normal Distribution used in probability and statistics:
References.
</dl>

</doc>
